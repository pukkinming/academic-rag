{
  "paper_id": "2405.02151v3",
  "title": "Gmp-Tl: Gender-Augmented Multi-Scale Pseudo-Label Enhanced Transfer Learning For Speech Emotion Recognition",
  "published": "2024-05-03T14:58:46Z",
  "authors": [
    "Yu Pan",
    "Yuguang Yang",
    "Heng Lu",
    "Lei Ma",
    "Jianjun Zhao"
  ],
  "keywords": [
    "Speech emotion recognition",
    "model fine-tuning",
    "gender-augmented",
    "multi-scale pseudo-label",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The continuous evolution of pre-trained speech models has greatly advanced Speech Emotion Recognition (SER). However, current research typically relies on utterancelevel emotion labels, inadequately capturing the complexity of emotions within a single utterance. In this paper, we introduce GMP-TL, a novel SER framework that employs gender-augmented multi-scale pseudo-label (GMP) based transfer learning to mitigate this gap. Specifically, GMP-TL initially uses the pre-trained HuBERT, implementing multi-task learning and multi-scale k-means clustering to acquire frame-level GMPs. Subsequently, to fully leverage frame-level GMPs and utterance-level emotion labels, a two-stage model fine-tuning approach is presented to further optimize GMP-TL. Experiments on IEMOCAP show that our GMP-TL attains a WAR of 80.0% and an UAR of 82.0%, achieving superior performance compared to state-of-the-art unimodal SER methods while also yielding comparable results to multimodal SER approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As one of the crucial elements in realizing human-computer interaction, speech emotion recognition (SER) aims to categorize emotions conveyed through human speech, which has been extensively applied in diverse practical domains  [1] .\n\nIn recent years, the remarkable advancements in machine and deep learning technologies have significantly accelerated the progress of the field of SER. By constructing robust deep neural networks such as convolutional neural networks or recurrent neural networks  [2] [3] [4]  based models, these methods can achieve improved performance compared to traditional SER approaches  [5, 6] . Nonetheless, due to the factors such as the inherent variability and complexity of speech signals  [7] [8] [9]  and the scarcity of available large-scale labeled speech emotion data resulting from difficulties in data collection and annotation  [10, 11] , achieving accurate and reliable emotion recognition continues to pose challenges.\n\nTo tackle these issues, numerous researchers  [12] [13] [14] [15]  have attempted to leverage transfer learning by incorporating large-scale pre-trained speech models such as HuBERT  [16] , Wav2vec 2.0  [17] , and WavLM  [18]  into speech emotion modeling. For instance, Morais et al.  [12]  presented a modular end-to-end Upstream + Downstream SER architecture, which facilitates the integration of the pretrained, fine-tuned, and averaged Upstream models. Hu et al.  [15]  introduced a joint network combining the pretrained Wav2Vec 2.0 model and a separate spectrum-based model to achieve SER. Nevertheless, despite achieving commendable results, these methods predominantly depend on utterance-level emotion labels as training objectives, which contradicts the fact that the emotional expression within a single utterance cannot be adequately conveyed with just one emotion label. Additionally, though some methods  [19] [20] [21]  attempted to incorporate local information by constructing emotional pseudo-labels, there is still room for improvement in both the quality of pseudolabels and their recognition performance.\n\nHence in this study, we propose a novel HuBERTbased GMP-TL (Gender-augmented Multi-scale Pseudolabel Transfer Learning) framework for SER, as shown in Fig.  1 . Overall, our primary emphasis is on two critical dimensions: the meticulous acquisition of high-quality frame-level emotional pseudo-labels and the comprehensive utilization of both frame-level and utterancelevel emotional labels. The key insight behind is that the captured frame-level pseudo-labels of each speech signal could provide valuable complementary local details for arXiv:2405.02151v3 [cs.SD] 23 Sep 2024 modeling speech emotion. To achieve this goal, we design three continuous stages in the proposed GMP-TL workflow. First, with the guidance of utterance-level emotional labels, we train a pretrained-HuBERT based SER model implemented with multi-task learning (emotion and gender classification) and unsupervised multi-scale k-means clustering on different HuBERT layers to acquire the high-quality gender-augmented multi-scale pseudo-labels (GMPs). The training loss function is cross-entropy (CE). Subsequently, to better leverage the frame-level GMPs and utterance-level emotion labels, we present an effective two-stage hybrid loss based fine-tuning (Hybrid-FT) strategy to further optimize the GMP-TL SER workflow. To elaborate, we first employ the obtained GMPs to guide the pretrained-HuBERT based SER model using CE loss as well, in order to incorporate the beneficial information of GMPs for emotion identification. Afterwards, we utilize the AM-Softmax (AMS) loss  [22]  to fine-tune the proposed SER model based on the trained HuBERT of stage 2, under the supervision of utterance-level emotion labels. In this way, our proposed GMP-TL framework excels in capturing fine-grained contextualized representations of speech emotion, leading to a substantial performance improvements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we provide a brief overview of relevant works, including the HuBERT-based SER methods, attribute-based multi-task learning for SER, and pseudolabel based SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hubert-Based Ser Models",
      "text": "HuBERT stands out as a prominent pre-trained speech model renowned for its effectiveness in mastering speech representation learning. Adopting a self-supervised learning framework, HuBERT undergoes pre-training on extensive speech datasets and employs iterative unsupervised clustering to generate pseudo-labels for each training phase. This process enables the HuBERT model to learn efficient and robust feature representations. Leveraging HuBERT's advanced capabilities in speech representation learning, numerous HuBERT-based SER methods have been proposed. For instance,  [23]  presented a block architecture search strategy to explore downstream transfer of HuBERT-based features for emotion recognition.  [24]  introduced an optimal transport approach for cross-domain SER using the pre-trained HuBERT model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attribute-Based Multi-Task Learning For Ser",
      "text": "Due to the multitude of attributes present in speech signals, numerous recent works  [2, 3, 8, 14, 25]  aim to leverage these attributes to enhance SER. Among them, attributebased multi-task learning emerges as an effective approach. It enhances the recognition performance of SER models by integrating auxiliary tasks, including gender classification, automatic speech recognition, and so forth. For instance, Li et al.  [2]  introduced a multitask learning (emotion and gender classification) based SER method and obtained great recognition results. Ghriss et al.  [3]  advocated an end-to-end multi-task approach which employed a sentiment-aware automatic speech recognition pre-training for emotion recognition. Zhang et al.  [25]  investigated an effective combination approaches on the basis of multi-task learning, with a focus on the style attribute.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pseudo-Label Based Ser",
      "text": "Constrained by the availability of labeled datasets, the majority of current methods normally rely on utterance-level emotion labels to achieve SER. Nonetheless, according to existing research  [19] [20] [21] , utterance-level emotion labels of speech utterances may not be as accurate, and they suggest that introducing frame-level pseudo-labels can be beneficial in modeling speech emotion. In  [20] , the authors presented Seg-FT, which investigated the significance of temporal context for SER and advocated a segment-based learning objective to leverage local features. In  [19] , a dynamic frame based formulation was presented to achieve emotion recognition. In  [21] , the researchers designed Semi-FedSER, a semi-supervised federated learning approach for SER using multi-view pseudo-labeling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "This section introduces the overall architecture of our proposed GMP-TL workflow, as depicted in Fig.  1 . First, we provide a detailed exposition of the frame-level GMP extraction phase. Subsequently, the principles and specific implementation of the proposed Hybrid-FT approach are elucidated.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Frame-Level Gmps Extraction",
      "text": "Intuitively, human speech is not likely to be consistently characterized by one single emotion, especially in a long utterance. Therefore, to bridge this gap, we believe that it is necessary to extract and incorporate high-quality framelevel pseudo-labels to train the SER model, whose overall schematic diagram is outlined in Fig.  1 (a) .\n\nConcretely, we initiate the process by employing the pre-trained HuBERT model as a feature extractor. The derived features are subsequently fed into a bi-directional LSTM network, and the impact of the temporal dimension is alleviated through an average pooling operation. Following this, the acquired features undergo a linear projection module that consists of two linear layers and one ReLU activation layer. Furthermore, drawing inspiration from relevant literature  [2, 14, 26] , we incorporate a multitask learning strategy into the training process to introduce gender information within speech signals. In this way, the overall model is capable of capturing more feasible emotional representations. Ultimately, the entire framework is trained using the cross-entropy (CE) loss function, guided by utterance-level emotion and gender categorical labels. Consequently, the final loss L T otal of this stage is formu-lated as:\n\nwhere L Emo is the CE-loss of emotion attribute, L Gender is the CE-loss of gender attribute, α e is a parameter to adjust L Emo and L Gender . In our case, α e is set to 0.9. Afterward, in order to generate frame-level GMPs of higher quality, we attempt to use features from different layers of HuBERT and perform multi-scale unsupervised k-means clustering, a departure from the conventional practice of using features from the final layer. In our case, the features from the third-to-last layer gains the best performance. The number of cluster centers is empirically set to 64, 512, and 4096, respectively.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ce-Loss Based Fine-Tuning Using Gmps",
      "text": "In the second stage, we adopt the CE-loss based model fine-tuning strategy that uses the obtained frame-level GMPs to retrain and optimize the HuBERT-based SER model. The concrete components of which are illustrated in the left side of the Fig.  1 (b) .\n\nAs depicted in the figure, we adhere to the conventional masking operation of HuBERT and implement a linear pro-jection module, which comprises two linear layers and one ReLU activation layer following the HuBERT model, to accurately harness and align the GMPs. The entire model is trained using CE loss as well. In light of this means, the frame-level GMP-based fine-tuning method also aligns well with the original pre-training objectives of the Hu-BERT model, ensuring an effective utilization of information from both the original pre-trained HuBERT model and the obtained frame-level GMPs of previous phase.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ams-Loss Based Fine-Tuning Via Emotion Labels",
      "text": "Due to many factors such as the inherent ambiguity in speech emotion, an issue arises where speech emotion categories show similarity between different classes and differences within the same classes.\n\nTo alleviate this problem, in the last fine-tuning stage, we employ the AMS loss to fine-tune the HuBERT-based SER model under the guidance of utterance-level emotion labels. By incorporating a constraint vector margin, this approach is able to increase the inter-class distance between different emotion categories and reduce the intraclass distance within the same emotion category, thus enhancing the model's recognition performance. Moreover, it is worth noting that we utilize the HuBERT model trained in the second stage as the feature extractor for this phase, with its comprehensive system layout presented in the right side of the Fig.  1 (b) .\n\nAs a result, the final loss function for the fine-tuning stage can be defined as:\n\ne s(cos(θ y i ,i )-m) + j̸ =y i e s(cos(θ j,i ))\n\n(2)\n\nwhere L represents the ultimate loss, x i and y i represent the feature vector and label of the ith sample, w j corresponds to the feature vector of class j, and θ j,i signifies the angle between x i and w j . N denotes the batch size, while s is the scaling factor, and m is the additive margin. In our specific scenario, the values for m and s are set at 0.2 and 30.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first outline the experimental setups of this study, encompassing the experimental database and implementation details. Second, a comprehensive comparison and analysis of the proposed approach with existing SOTA unimodal and multimodal SER methods is given. Finally, to examine the validity of our proposed GMP-TL framework, we conduct an ablation study.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setups",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Datasets",
      "text": "In this work, we conduct extensive experiments using the most challenging IEMOCAP  [27]  corpus, which has been widely recognized for evaluating SER systems. IEMOCAP consists of scripted and improvised interactions recorded in five sessions with the participation of 10 actors (5 male and 5 female), and the dataset is labeled by three annotators.\n\nTo facilitate a fair comparison with other SOTA methods, we merge \"excited\" and \"happy\" into the \"happy\" category, resulting in four emotion categories: angry, happy, neutral, and sad.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "In all experiments, we use Adam to optimize the proposed GMT-TL framework with an initial learning rate of 1e-4 and a batch size of 64. Besides, due to the limitations of computational resources, we employ the pre-trained HuBERT-base model, which is accessible on an opensource website  1  .\n\nAs for evaluating metrics, we adopt weighted average recall (WAR) and unweighted average recall (UAR) in the context of the speaker-independent setting. Additionally, we perform a standard 5-fold cross-validation, aligning with current SOTA SER methods, to evaluate our GMT-TL. Within each fold, one session is reserved as the test set, and we calculate the average of the acquired UAR and WAR across all folds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Main Results",
      "text": "To examine the effectiveness of the proposed GMP-TL method, we first compare its recognition performance with SOTA unimodal and multimodal SER approaches, with the results summarized in Table  1 .\n\nAs illustrated in the above table, it is apparent that our GMP-TL achieves superior recognition results. To be specific, when compared with SOTA unimodal SER methods including the Pseudo-label based Seg-FT model  [20] , the proposed GMP-TL attains the secondary best UAR and WAR of 82.0% and 80.0%. In addition, the proposed GMP-TL also demonstrates competitive recognition results as opposed to SOTA multimodal SER approaches, showcasing the effectiveness of our method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "To comprehensively evaluate the validity of our design, ablation studies are performed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Effect Of Key Components In Gmt-Tl.",
      "text": "We first validate the effectiveness of the two crucial components, i.e., GMPs and Hybrid-FT, of the proposed GMT-TL in Table  2 . From the table, we can easily observe that without the proposed frame-level GMPs, the performance of our GMP-TL framework has dropped by at least 2% when just using the MPs, which proves our strategy that incorporating GMPs into the speech emotion modeling are beneficial. Additionally, without the Hybrid-FT strategy, the perfor-mance of GMP-TL drops as well compared to using CE-FT, indicating the effectiveness of the proposed Hybrid-FT approach.\n\n4.3.2. Effect of GMPs Using Different Feature Layers.\n\nMoreover, we study the effect of GMPs based on different feature layers of HuBERT within the proposed GMP-TL framework. Results are shown in Table  3 . It is evident that under the proposed GMP-TL workflow, the performance of GMPs based on the final layer features of HuBERT is suboptimal. Conversely, GMPs relying on the features from the third-to-last and fourth-to-last layers demonstrate superior results, which may be attributed to the final layer features of the HuBERT model containing more semantic information rather than emotion-related details.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we propose GMP-TL, an effective GMPsbased transfer learning framework for SER. Initially, GMP-TL employs a pre-trained HuBERT model, implemented with multi-task learning and multi-scale k-means clustering strategies, to obtain high-quality frame-level GMPs. Moreover, an efficient two-stage hybrid loss based fine-tuning approach is introduced to further optimize the GMP-TL by comprehensively leveraging frame-level GMPs and utterance-level emotion labels. Experiments on IEMOCAP show that our GMP-TL not only attains superior performance compared to SOTA unimodal SER methods but achieves competitive results compared to multimodal SER approaches, demonstrating the effectiveness of the proposed approach. In future work, we aim to develop more accurate frame-level emotional pseudo-labels by integrating additional relevant speech attributes, thereby further enhancing the proposed GMP-TL framework.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall, our primary emphasis is on two critical",
      "page": 1
    },
    {
      "caption": "Figure 1: First, we",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "1Kyushu University, Japan\n2University of Alberta, Canada\n3Ximalaya Inc., ShangHai, China"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "ABSTRACT\nnetworks or recurrent neural networks [2–4] based models,"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "these methods can achieve improved performance com-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "The\ncontinuous\nevolution of pre-trained speech models"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "pared to traditional SER approaches [5, 6]. Nonetheless,"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "has greatly advanced Speech Emotion Recognition (SER)."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "due to the factors such as the inherent variability and com-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "However,\ncurrent\nresearch typically relies on utterance-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "plexity of speech signals [7–9] and the scarcity of available"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "level emotion labels,\ninadequately capturing the complex-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "large-scale labeled speech emotion data resulting from dif-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "ity of emotions within a single utterance. In this paper, we"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "ficulties in data collection and annotation [10, 11], achiev-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "introduce GMP-TL, a novel SER framework that employs"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "ing accurate and reliable emotion recognition continues to"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "gender-augmented multi-scale pseudo-label (GMP) based"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "pose challenges."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "transfer learning to mitigate this gap. Specifically, GMP-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "To tackle these issues, numerous researchers [12–15]"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "TL initially uses the pre-trained HuBERT,\nimplementing"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "have\nattempted\nto\nleverage\ntransfer\nlearning\nby\nincor-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "multi-task learning and multi-scale k-means clustering to"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "porating\nlarge-scale\npre-trained\nspeech models\nsuch\nas"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "acquire frame-level GMPs. Subsequently, to fully leverage"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "HuBERT [16], Wav2vec 2.0 [17], and WavLM [18]\ninto"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "frame-level GMPs and utterance-level emotion labels,\na"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "speech emotion modeling. For instance, Morais et al. [12]"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "two-stage model fine-tuning approach is presented to fur-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "presented a modular end-to-end Upstream + Downstream"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "ther optimize GMP-TL. Experiments on IEMOCAP show"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "SER architecture, which facilitates the integration of\nthe"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "that our GMP-TL attains a WAR of 80.0% and an UAR"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "pretrained, fine-tuned, and averaged Upstream models. Hu"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "of 82.0%,\nachieving superior performance\ncompared to"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "et al.\n[15]\nintroduced a joint network combining the pre-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "state-of-the-art unimodal SER methods while also yielding"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "trained Wav2Vec 2.0 model and a separate spectrum-based"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "comparable results to multimodal SER approaches."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "model\nto achieve SER. Nevertheless,\ndespite\nachieving"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "Index Terms— Speech emotion recognition, model"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "commendable\nresults,\nthese methods predominantly de-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "fine-tuning, gender-augmented, multi-scale pseudo-label,"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "pend on utterance-level emotion labels as training objec-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "transfer learning"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "tives, which contradicts the fact that the emotional expres-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "sion within a single utterance cannot be adequately con-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "veyed with just one emotion label. Additionally,\nthough\n1.\nINTRODUCTION"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "some methods [19–21] attempted to incorporate local\nin-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "As one of the crucial elements in realizing human-computer\nformation by constructing emotional pseudo-labels,\nthere"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "interaction,\nspeech\nemotion\nrecognition\n(SER)\naims\nto\nis still room for improvement in both the quality of pseudo-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "categorize\nemotions\nconveyed\nthrough\nhuman\nspeech,\nlabels and their recognition performance."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "which has been extensively applied in diverse practical"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "Hence\nin this\nstudy, we propose\na novel HuBERT-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "domains [1]."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "based GMP-TL (Gender-augmented Multi-scale Pseudo-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "label Transfer Learning) framework for SER, as shown in"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "In recent years,\nthe remarkable advancements in ma-\nFig.\n1. Overall, our primary emphasis is on two critical"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "chine and deep learning technologies have significantly ac-\nthe meticulous acquisition of high-quality\ndimensions:"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "celerated the progress of the field of SER. By constructing\nframe-level\nemotional pseudo-labels\ncompre-\nand the"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "robust deep neural networks such as convolutional neural\nhensive utilization of both frame-level and utterance-"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "level emotional\nlabels. The key insight behind is that\nthe"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "∗ denotes equal contributing."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "captured frame-level pseudo-labels of each speech signal"
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "† denotes the corresponding author."
        },
        {
          "Yu Pan1*, Yuguang Yang3*, Heng Lu3†, Lei Ma2†, Jianjun Zhao1": "Work done at Ximalaya Inc.\ncould provide valuable\ncomplementary local details\nfor"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modeling speech emotion. To achieve this goal, we design": "three continuous\nstages\nin the proposed GMP-TL work-",
          "introduced an optimal transport approach for cross-domain": "SER using the pre-trained HuBERT model."
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "flow. First, with the guidance of utterance-level emotional",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "labels, we train a pretrained-HuBERT based SER model",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "2.2. Attribute-based Multi-task Learning for SER"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "implemented with multi-task learning (emotion and gen-",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "der classification) and unsupervised multi-scale k-means",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "Due to the multitude of attributes present\nin speech sig-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "clustering\non\ndifferent HuBERT layers\nto\nacquire\nthe",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "nals, numerous recent works [2,3,8,14,25] aim to leverage"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "high-quality gender-augmented multi-scale pseudo-labels",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "these attributes to enhance SER. Among them, attribute-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "(GMPs). The training loss function is cross-entropy (CE).",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "based multi-task learning emerges as an effective approach."
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "Subsequently,\nto better\nleverage\nthe\nframe-level GMPs",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "It enhances the recognition performance of SER models"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "and utterance-level emotion labels, we present an effective",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "by integrating auxiliary tasks,\nincluding gender classifica-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "two-stage hybrid loss based fine-tuning (Hybrid-FT) strat-",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "tion, automatic speech recognition, and so forth.\nFor\nin-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "egy to further optimize the GMP-TL SER workflow.\nTo",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "stance, Li et al. [2] introduced a multitask learning (emo-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "elaborate, we first employ the obtained GMPs to guide the",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "tion and gender classification) based SER method and ob-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "pretrained-HuBERT based SER model using CE loss as",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "tained great\nrecognition results.\nGhriss et al.\n[3] advo-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "well,\nin order to incorporate the beneficial\ninformation of",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "cated an end-to-end multi-task approach which employed a"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "GMPs for emotion identification. Afterwards, we utilize",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "sentiment-aware automatic speech recognition pre-training"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "the AM-Softmax (AMS)\nloss\n[22]\nto fine-tune the pro-",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "for emotion recognition. Zhang et al. [25] investigated an"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "posed SER model based on the trained HuBERT of stage",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "effective combination approaches on the basis of multi-task"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "2,\nunder\nthe\nsupervision of utterance-level\nemotion la-",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "learning, with a focus on the style attribute."
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "bels. In this way, our proposed GMP-TL framework excels",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "in\ncapturing fine-grained\ncontextualized\nrepresentations",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "of\nspeech emotion,\nleading to a substantial performance",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "2.3. Pseudo-label based SER"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "improvements.",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "Constrained by the availability of labeled datasets, the ma-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "jority of current methods normally rely on utterance-level"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "2. RELATED WORK",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "emotion labels\nto achieve SER. Nonetheless,\naccording"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "to existing research [19–21], utterance-level emotion la-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "In\nthis\nsection, we\nprovide\na\nbrief\noverview of\nrele-",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "bels of speech utterances may not be as accurate, and they"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "vant works,\nincluding the HuBERT-based SER methods,",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "suggest\nthat\nintroducing frame-level pseudo-labels can be"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "attribute-based multi-task learning for SER, and pseudo-",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "beneficial in modeling speech emotion. In [20], the authors"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "label based SER.",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "presented Seg-FT, which investigated the significance of"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "temporal context for SER and advocated a segment-based"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "learning objective to leverage local features.\nIn [19], a dy-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "2.1. HuBERT-based SER Models",
          "introduced an optimal transport approach for cross-domain": ""
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "",
          "introduced an optimal transport approach for cross-domain": "namic frame based formulation was presented to achieve"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "HuBERT stands out\nas\na prominent pre-trained speech",
          "introduced an optimal transport approach for cross-domain": "emotion recognition.\nIn [21],\nthe\nresearchers designed"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "model renowned for its effectiveness in mastering speech",
          "introduced an optimal transport approach for cross-domain": "Semi-FedSER, a semi-supervised federated learning ap-"
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "representation learning. Adopting a self-supervised learn-",
          "introduced an optimal transport approach for cross-domain": "proach for SER using multi-view pseudo-labeling."
        },
        {
          "modeling speech emotion. To achieve this goal, we design": "ing framework, HuBERT undergoes pre-training on ex-",
          "introduced an optimal transport approach for cross-domain": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "pooling, LinearP represents linear projection module, Emo. and Gend. are the abbreviations of emotion and gender."
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "3.1. Frame-level GMPs Extraction"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "Intuitively, human speech is not\nlikely to be consistently"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "characterized by one single emotion, especially in a long"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "utterance. Therefore,\nto bridge this gap, we believe that\nit"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "is necessary to extract and incorporate high-quality frame-"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "level pseudo-labels to train the SER model, whose overall"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "schematic diagram is outlined in Fig. 1 (a)."
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "Concretely, we initiate the process by employing the"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "pre-trained HuBERT model as a feature extractor.\nThe"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "derived features are subsequently fed into a bi-directional"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "LSTM network, and the impact of the temporal dimension"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "is alleviated through an average pooling operation.\nFol-"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "lowing this,\nthe acquired features undergo a linear pro-"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "jection module that consists of\ntwo linear\nlayers and one"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "ReLU activation layer.\nFurthermore, drawing inspiration"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": ""
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "from relevant literature [2, 14, 26], we incorporate a multi-"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "task learning strategy into the training process to introduce"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "gender information within speech signals.\nIn this way,\nthe"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "overall model\nis capable of capturing more feasible emo-"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "tional representations. Ultimately,\nthe entire framework is"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "trained using the cross-entropy (CE) loss function, guided"
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "by utterance-level emotion and gender categorical\nlabels."
        },
        {
          "Fig. 1: Overview of the proposed GMP-TL framework. BiLSTM is bi-directional LSTM, MAP indicates mean average": "Consequently,\nthe final\nloss LT otal of this stage is formu-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "jection module, which comprises two linear layers and one": "ReLU activation layer\nfollowing the HuBERT model,\nto",
          "implementation details. Second, a comprehensive compar-": "ison and analysis of\nthe proposed approach with existing"
        },
        {
          "jection module, which comprises two linear layers and one": "accurately harness and align the GMPs. The entire model",
          "implementation details. Second, a comprehensive compar-": "SOTA unimodal and multimodal SER methods\nis given."
        },
        {
          "jection module, which comprises two linear layers and one": "is trained using CE loss as well.\nIn light of\nthis means,",
          "implementation details. Second, a comprehensive compar-": "Finally,\nto examine the validity of our proposed GMP-TL"
        },
        {
          "jection module, which comprises two linear layers and one": "the frame-level GMP-based fine-tuning method also aligns",
          "implementation details. Second, a comprehensive compar-": "framework, we conduct an ablation study."
        },
        {
          "jection module, which comprises two linear layers and one": "well with the original pre-training objectives of\nthe Hu-",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "BERT model, ensuring an effective utilization of informa-",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "4.1. Experimental Setups"
        },
        {
          "jection module, which comprises two linear layers and one": "tion from both the original pre-trained HuBERT model and",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "4.1.1. Experimental Datasets"
        },
        {
          "jection module, which comprises two linear layers and one": "the obtained frame-level GMPs of previous phase.",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "In this work, we conduct extensive experiments using the"
        },
        {
          "jection module, which comprises two linear layers and one": "3.3. AMS-loss based Fine-tuning via Emotion Labels",
          "implementation details. Second, a comprehensive compar-": "most challenging IEMOCAP [27] corpus, which has been"
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "widely recognized for evaluating SER systems. IEMOCAP"
        },
        {
          "jection module, which comprises two linear layers and one": "Due\nto many factors\nsuch as\nthe\ninherent\nambiguity in",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "consists of scripted and improvised interactions recorded in"
        },
        {
          "jection module, which comprises two linear layers and one": "speech emotion,\nan issue\narises where\nspeech emotion",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "five sessions with the participation of 10 actors (5 male and"
        },
        {
          "jection module, which comprises two linear layers and one": "categories\nshow similarity between different classes and",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "5 female), and the dataset is labeled by three annotators."
        },
        {
          "jection module, which comprises two linear layers and one": "differences within the same classes.",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "To facilitate a fair comparison with other SOTA meth-"
        },
        {
          "jection module, which comprises two linear layers and one": "To alleviate this problem,\nin the last fine-tuning stage,",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "ods, we merge ”excited” and ”happy” into the ”happy” cat-"
        },
        {
          "jection module, which comprises two linear layers and one": "we employ the AMS loss to fine-tune the HuBERT-based",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "egory, resulting in four emotion categories: angry, happy,"
        },
        {
          "jection module, which comprises two linear layers and one": "SER model under the guidance of utterance-level emotion",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "neutral, and sad."
        },
        {
          "jection module, which comprises two linear layers and one": "labels. By incorporating a constraint vector margin,\nthis",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "approach is able to increase the inter-class distance be-",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "4.1.2.\nImplementation Details"
        },
        {
          "jection module, which comprises two linear layers and one": "tween different emotion categories and reduce the intra-",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "class distance within the same emotion category,\nthus en-",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "In all experiments, we use Adam to optimize the proposed"
        },
        {
          "jection module, which comprises two linear layers and one": "hancing the model’s recognition performance. Moreover, it",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "GMT-TL framework with an initial\nlearning rate of 1e-4"
        },
        {
          "jection module, which comprises two linear layers and one": "is worth noting that we utilize the HuBERT model trained",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "and a batch size of 64.\nBesides, due to the limitations"
        },
        {
          "jection module, which comprises two linear layers and one": "in the second stage as the feature extractor for this phase,",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "of\ncomputational\nresources, we\nemploy the pre-trained"
        },
        {
          "jection module, which comprises two linear layers and one": "with its comprehensive system layout presented in the right",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "HuBERT-base model, which is\naccessible on an open-"
        },
        {
          "jection module, which comprises two linear layers and one": "side of the Fig. 1 (b).",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "source website1."
        },
        {
          "jection module, which comprises two linear layers and one": "As a result,\nthe final\nloss function for\nthe fine-tuning",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "As for evaluating metrics, we adopt weighted average"
        },
        {
          "jection module, which comprises two linear layers and one": "stage can be defined as:",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "recall (WAR) and unweighted average recall (UAR) in the"
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "context of\nthe speaker-independent setting. Additionally,"
        },
        {
          "jection module, which comprises two linear layers and one": "es(cos(θyi,i)−m)",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "we perform a\nstandard 5-fold cross-validation,\naligning"
        },
        {
          "jection module, which comprises two linear layers and one": "1 N\nN(cid:88) i\n(2)\nL = −\nlog",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "es(cos(θj,i))\nes(cos(θyi,i)−m) + (cid:80)",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "=1\nj̸=yi",
          "implementation details. Second, a comprehensive compar-": "with current SOTA SER methods,\nto evaluate our GMT-"
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "TL. Within each fold, one session is reserved as the test"
        },
        {
          "jection module, which comprises two linear layers and one": "xT",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "i wj",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "set, and we calculate the average of the acquired UAR and"
        },
        {
          "jection module, which comprises two linear layers and one": "(3)\ncos(θj,i) =",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "∥xi∥ ∥wj∥",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "WAR across all folds."
        },
        {
          "jection module, which comprises two linear layers and one": "represent\nwhere L represents the ultimate loss, xi and yi",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "the feature vector and label of\ncorre-\nthe ith sample, wj",
          "implementation details. Second, a comprehensive compar-": "4.2. Main Results"
        },
        {
          "jection module, which comprises two linear layers and one": "sponds to the feature vector of class j, and θj,i signifies the",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "To examine\nthe\neffectiveness of\nthe proposed GMP-TL"
        },
        {
          "jection module, which comprises two linear layers and one": "angle between xi and wj. N denotes the batch size, while",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "method, we first compare its recognition performance with"
        },
        {
          "jection module, which comprises two linear layers and one": "s is the scaling factor, and m is the additive margin.\nIn our",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "SOTA unimodal and multimodal SER approaches, with"
        },
        {
          "jection module, which comprises two linear layers and one": "specific scenario,\nthe values for m and s are set at 0.2 and",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "the results summarized in Table 1."
        },
        {
          "jection module, which comprises two linear layers and one": "30.",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "As\nillustrated in the above table,\nit\nis apparent\nthat"
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "our GMP-TL achieves superior recognition results. To be"
        },
        {
          "jection module, which comprises two linear layers and one": "4. EXPERIMENTS",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "specific, when compared with SOTA unimodal SER meth-"
        },
        {
          "jection module, which comprises two linear layers and one": "",
          "implementation details. Second, a comprehensive compar-": "ods including the Pseudo-label based Seg-FT model [20],"
        },
        {
          "jection module, which comprises two linear layers and one": "In this section, we first outline the experimental setups of",
          "implementation details. Second, a comprehensive compar-": ""
        },
        {
          "jection module, which comprises two linear layers and one": "this\nstudy,\nencompassing the experimental database and",
          "implementation details. Second, a comprehensive compar-": "1https://huggingface.co/facebook/hubert-base-ls960"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Recognition comparison of SOTA SER methods mance of GMP-TL drops as well compared to using CE-",
      "data": [
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "on IEMOCAP. A and T are audio and text modalities, re-"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "spectively."
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "Model"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "Seg-FT [20]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "Joint Network [15]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "TAP [11]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "P-TAPT [10]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "BAS [23]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "UDA [12]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "SA-CNN-BLSTM [2]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "UATMF [28]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "MMER [29]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "GEmo-CLAP [14]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "DC-BVM [30]"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "GMP-TL (Ours)"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "the proposed GMP-TL attains"
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": ""
        },
        {
          "Table 1: Recognition comparison of SOTA SER methods": "and WAR of 82.0% and 80.0%."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Recognition comparison of SOTA SER methods mance of GMP-TL drops as well compared to using CE-",
      "data": [
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": "Table 2:"
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": "mance"
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": "tuning method, MPs denote multi-scale pesudo-labels."
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": "MPs"
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": "✓"
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": "✓"
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": ""
        },
        {
          "TL in Table 2.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "2023, pp. 1715–1720."
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "et\n[16] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai,\nal.,\n“Hubert:\nSelf-"
        },
        {
          "6. REFERENCES": "[1]\nS. Zhang, S. Zhang, T. Huang, et al., “Speech emotion recog-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "supervised speech representation learning by masked prediction"
        },
        {
          "6. REFERENCES": "nition using deep convolutional neural network and discriminant",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "of hidden units,” IEEE/ACM Transactions on Audio, Speech, and"
        },
        {
          "6. REFERENCES": "temporal pyramid matching,” IEEE Transactions on Multimedia,",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Language Processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "6. REFERENCES": "vol. 20, no. 6, pp. 1576–1590, 2017.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[17] A. Baevski, Y. Zhou, A. Mohamed, et al., “wav2vec 2.0: A frame-"
        },
        {
          "6. REFERENCES": "[2] Y. Li, T. Zhao, T. Kawahara, et al., “Improved end-to-end speech",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "work for self-supervised learning of speech representations,” Ad-"
        },
        {
          "6. REFERENCES": "emotion recognition using self attention mechanism and multitask",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "vances\nin neural\ninformation processing systems,\nvol. 33,\npp."
        },
        {
          "6. REFERENCES": "learning.” in Interspeech, 2019, pp. 2803–2807.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "12 449–12 460, 2020."
        },
        {
          "6. REFERENCES": "[3] A. Ghriss, B. Yang, V. Rozgic, et al.,\n“Sentiment-aware auto-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[18]\nS. Chen, C. Wang, Z. Chen, et al., “Wavlm: Large-scale self-"
        },
        {
          "6. REFERENCES": "matic speech recognition pre-training for enhanced speech emo-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "supervised pre-training for\nfull\nstack speech processing,” IEEE"
        },
        {
          "6. REFERENCES": "tion recognition,” in ICASSP 2022-2022 IEEE International Con-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Journal of Selected Topics in Signal Processing, vol. 16, no. 6,"
        },
        {
          "6. REFERENCES": "ference on Acoustics, Speech and Signal Processing (ICASSP).",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "pp. 1505–1518, 2022."
        },
        {
          "6. REFERENCES": "IEEE, 2022, pp. 7347–7351.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[19] H. M. Fayek, M. Lech, and L. Cavedon, “Evaluating deep learning"
        },
        {
          "6. REFERENCES": "[4]\nJ. Ye, X. cheng Wen, Y. Wei, et al., “Temporal modeling matters:",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "architectures for speech emotion recognition,” Neural Networks,"
        },
        {
          "6. REFERENCES": "A novel\ntemporal emotional modeling approach for speech emo-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "vol. 92, pp. 60–68, 2017."
        },
        {
          "6. REFERENCES": "tion recognition,” 2023.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[20] Y. Xia, L.-W. Chen, A. Rudnicky, et al., “Temporal context\nin"
        },
        {
          "6. REFERENCES": "[5] B. Schuller, G. Rigoll, and M. Lang, “Speech emotion recognition",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "speech emotion recognition.” in Interspeech, vol. 2021, 2021, pp."
        },
        {
          "6. REFERENCES": "combining acoustic features and linguistic information in a hy-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "3370–3374."
        },
        {
          "6. REFERENCES": "brid support vector machine-belief network architecture,” in 2004",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[21] T. Feng and S. Narayanan, “Semi-fedser: Semi-supervised learn-"
        },
        {
          "6. REFERENCES": "IEEE International Conference on Acoustics, Speech, and Signal",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "ing for speech emotion recognition on federated learning using"
        },
        {
          "6. REFERENCES": "Processing, vol. 1, 2004, pp. I–577.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "arXiv\npreprint\nmultiview pseudo-labeling,”\narXiv:2203.08810,"
        },
        {
          "6. REFERENCES": "[6] C.-H. Wu and W.-B. Liang,\n“Emotion recognition of affective",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "2022."
        },
        {
          "6. REFERENCES": "speech based on multiple classifiers using acoustic-prosodic in-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[22]\nF. Wang, J. Cheng, W. Liu, et al., “Additive margin softmax for"
        },
        {
          "6. REFERENCES": "formation and semantic labels,” IEEE Transactions on Affective",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "face verification,” IEEE Signal Processing Letters, vol. 25, no. 7,"
        },
        {
          "6. REFERENCES": "Computing, vol. 2, no. 1, pp. 10–21, 2011.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "pp. 926–930, 2018."
        },
        {
          "6. REFERENCES": "[7]\nJ. Kim, G. Englebienne, K. P. Truong, et al., “Towards speech",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[23] Y. Fang, X. Xing, X. Xu, et al., “Exploring Downstream Transfer"
        },
        {
          "6. REFERENCES": "emotion\nrecognition”\nin\nthe wild”\nusing\naggregated\ncorpora",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "of Self-Supervised Features for Speech Emotion Recognition,” in"
        },
        {
          "6. REFERENCES": "and deep multi-task learning,” arXiv preprint arXiv:1708.03920,",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Proc. INTERSPEECH 2023, 2023, pp. 3627–3631."
        },
        {
          "6. REFERENCES": "2017.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[24] R. Zhang, J. Wei, X. Lu, et al., “SOT: Self-supervised Learning-"
        },
        {
          "6. REFERENCES": "[8] Y. Pan, Y. Yang, Y. Huang, et al., “Msac: Multiple speech attribute",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Assisted Optimal Transport\nfor Unsupervised Adaptive Speech"
        },
        {
          "6. REFERENCES": "control method for\nreliable speech emotion recognition,” arXiv",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Emotion Recognition,” in Proc. INTERSPEECH 2023, 2023, pp."
        },
        {
          "6. REFERENCES": "preprint arXiv:2308.04025, 2023.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "1858–1862."
        },
        {
          "6. REFERENCES": "[9] Y.\nPan,\nL. Ma,\nand\nJ.\nZhao,\n“Promptcodec:\nHigh-fidelity",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[25] H. Zhang, M. Mimura, T. Kawahara, et al., “Selective multi-task"
        },
        {
          "6. REFERENCES": "neural\nspeech codec using disentangled representation learning",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "learning for speech emotion recognition using corpora of different"
        },
        {
          "6. REFERENCES": "based adaptive feature-aware prompt encoders,” arXiv preprint",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "styles,” in ICASSP 2022-2022 IEEE International Conference on"
        },
        {
          "6. REFERENCES": "arXiv:2404.02702, 2024.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2022,"
        },
        {
          "6. REFERENCES": "[10] L.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine tuning",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "pp. 7707–7711."
        },
        {
          "6. REFERENCES": "for improved speech emotion recognition,” in ICASSP 2023-2023",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[26] A. Nediyanchath, P. Paramasivam, and P. Yenigalla, “Multi-head"
        },
        {
          "6. REFERENCES": "IEEE International Conference on Acoustics, Speech and Signal",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "attention for\nspeech emotion recognition with auxiliary learn-"
        },
        {
          "6. REFERENCES": "Processing (ICASSP).\nIEEE, 2023, pp. 1–5.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "ing of gender\nrecognition,”\nin ICASSP 2020-2020 IEEE Inter-"
        },
        {
          "6. REFERENCES": "[11]\nI. Gat, H. Aronowitz, W. Zhu, et al., “Speaker normalization for",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "national Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "6. REFERENCES": "self-supervised speech emotion recognition,”\nin ICASSP 2022-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "(ICASSP).\nIEEE, 2020, pp. 7179–7183."
        },
        {
          "6. REFERENCES": "2022 IEEE International Conference on Acoustics, Speech and",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[27] C. Busso, M. Bulut, C.-C. Lee, et al., “Iemocap: Interactive emo-"
        },
        {
          "6. REFERENCES": "Signal Processing (ICASSP).\nIEEE, 2022, pp. 7342–7346.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "tional dyadic motion capture database,” Language resources and"
        },
        {
          "6. REFERENCES": "[12] E. Morais, R. Hoory, W. Zhu, et al.,\nin ICASSP 2022-2022 IEEE",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "6. REFERENCES": "International Conference on Acoustics, Speech and Signal Pro-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[28] D. Sun, Y. He, and J. Han, “Using auxiliary tasks in multimodal"
        },
        {
          "6. REFERENCES": "cessing (ICASSP).\nIEEE, 2022, pp. 6922–6926.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "fusion of wav2vec 2.0 and bert for multimodal emotion recogni-"
        },
        {
          "6. REFERENCES": "[13] G. Yi, Y. Yang, Y. Pan, et al., “Exploring the power of cross-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "tion,” in ICASSP 2023 - 2023 IEEE International Conference on"
        },
        {
          "6. REFERENCES": "contextual large language model in mimic emotion prediction,” in",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1–"
        },
        {
          "6. REFERENCES": "Proceedings of\nthe 4th on Multimodal Sentiment Analysis Chal-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "5."
        },
        {
          "6. REFERENCES": "lenge and Workshop: Mimicked Emotions, Humour and Person-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[29]\nS. Ghosh, U. Tyagi, S. Ramaneswaran, et al.,\n“Mmer: Multi-"
        },
        {
          "6. REFERENCES": "alisation, 2023, pp. 19–26.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "modal multi-task learning for speech emotion recognition,” arXiv"
        },
        {
          "6. REFERENCES": "[14] Y. Pan, Y. Hu, Y. Yang, et al.,\n“Gemo-clap: Gender-attribute-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "preprint arXiv:2203.16794, 2022."
        },
        {
          "6. REFERENCES": "enhanced contrastive language-audio pretraining for speech emo-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "[30] B. Miao, Y. Xu, J. Wang, et al., “Dc-bvm: Dual-channel\ninfor-"
        },
        {
          "6. REFERENCES": "tion recognition,” arXiv preprint arXiv:2306.07848, 2023.",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "mation fusion network based on voting mechanism,” Biomedical"
        },
        {
          "6. REFERENCES": "[15] Y. Hu, S. Hou, H. Yang, et al., “A joint network based on interac-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        },
        {
          "6. REFERENCES": "",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": "Signal Processing and Control, vol. 94, p. 106248, 2024."
        },
        {
          "6. REFERENCES": "tive attention for speech emotion recognition,” in 2023 IEEE In-",
          "ternational Conference on Multimedia and Expo (ICME).\nIEEE,": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Sentiment-aware automatic speech recognition pre-training for enhanced speech emotion recognition",
      "authors": [
        "A Ghriss",
        "B Yang",
        "V Rozgic"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Cheng Wen",
        "Y Wei"
      ],
      "year": "2023",
      "venue": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels",
      "authors": [
        "C.-H Wu",
        "W.-B Liang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong"
      ],
      "year": "2017",
      "venue": "Towards speech emotion recognition\" in the wild\" using aggregated corpora and deep multi-task learning",
      "arxiv": "arXiv:1708.03920"
    },
    {
      "citation_id": "9",
      "title": "Msac: Multiple speech attribute control method for reliable speech emotion recognition",
      "authors": [
        "Y Pan",
        "Y Yang",
        "Y Huang"
      ],
      "year": "2023",
      "venue": "Msac: Multiple speech attribute control method for reliable speech emotion recognition",
      "arxiv": "arXiv:2308.04025"
    },
    {
      "citation_id": "10",
      "title": "Promptcodec: High-fidelity neural speech codec using disentangled representation learning based adaptive feature-aware prompt encoders",
      "authors": [
        "Y Pan",
        "L Ma",
        "J Zhao"
      ],
      "year": "2024",
      "venue": "Promptcodec: High-fidelity neural speech codec using disentangled representation learning based adaptive feature-aware prompt encoders",
      "arxiv": "arXiv:2404.02702"
    },
    {
      "citation_id": "11",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "I Gat",
        "H Aronowitz",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Exploring the power of crosscontextual large language model in mimic emotion prediction",
      "authors": [
        "G Yi",
        "Y Yang",
        "Y Pan"
      ],
      "year": "2023",
      "venue": "Proceedings of the 4th on Multimodal Sentiment Analysis Challenge and Workshop: Mimicked Emotions, Humour and Personalisation"
    },
    {
      "citation_id": "15",
      "title": "Gemo-clap: Gender-attributeenhanced contrastive language-audio pretraining for speech emotion recognition",
      "authors": [
        "Y Pan",
        "Y Hu",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "Gemo-clap: Gender-attributeenhanced contrastive language-audio pretraining for speech emotion recognition",
      "arxiv": "arXiv:2306.07848"
    },
    {
      "citation_id": "16",
      "title": "A joint network based on interactive attention for speech emotion recognition",
      "authors": [
        "Y Hu",
        "S Hou",
        "H Yang"
      ],
      "year": "2023",
      "venue": "2023 IEEE In-ternational Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "17",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "19",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "21",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Y Xia",
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Semi-fedser: Semi-supervised learning for speech emotion recognition on federated learning using multiview pseudo-labeling",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2022",
      "venue": "Semi-fedser: Semi-supervised learning for speech emotion recognition on federated learning using multiview pseudo-labeling",
      "arxiv": "arXiv:2203.08810"
    },
    {
      "citation_id": "23",
      "title": "Additive margin softmax for face verification",
      "authors": [
        "F Wang",
        "J Cheng",
        "W Liu"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "24",
      "title": "Exploring Downstream Transfer of Self-Supervised Features for Speech Emotion Recognition",
      "authors": [
        "Y Fang",
        "X Xing",
        "X Xu"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "25",
      "title": "SOT: Self-supervised Learning-Assisted Optimal Transport for Unsupervised Adaptive Speech Emotion Recognition",
      "authors": [
        "R Zhang",
        "J Wei",
        "X Lu"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "26",
      "title": "Selective multi-task learning for speech emotion recognition using corpora of different styles",
      "authors": [
        "H Zhang",
        "M Mimura",
        "T Kawahara"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "29",
      "title": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "authors": [
        "D Sun",
        "Y He",
        "J Han"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "31",
      "title": "Dc-bvm: Dual-channel information fusion network based on voting mechanism",
      "authors": [
        "B Miao",
        "Y Xu",
        "J Wang"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}