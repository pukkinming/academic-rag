{
  "paper_id": "2207.05691v2",
  "title": "The Muse 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, And Stress",
  "published": "2022-06-23T13:34:33Z",
  "authors": [
    "Lukas Christ",
    "Shahin Amiriparian",
    "Alice Baird",
    "Panagiotis Tzirakis",
    "Alexander Kathan",
    "Niklas M√ºller",
    "Lukas Stappen",
    "Eva-Maria Me√üner",
    "Andreas K√∂nig",
    "Alan Cowen",
    "Erik Cambria",
    "Bj√∂rn W. Schuller"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computing methodologies ‚Üí Neural networks",
    "Artificial intelligence",
    "Computer vision",
    "Natural language processing Multimodal Sentiment Analysis",
    "Affective Computing",
    "Humor Detection",
    "Emotion Recognition",
    "Multimodal Fusion",
    "Challenge",
    "Benchmark"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is dedicated to multimodal sentiment and emotion recognition. For this year's challenge, we feature three datasets: (i) the Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset that contains audio-visual recordings of German football coaches, labelled for the presence of humour; (ii) the Hume-Reaction dataset in which reactions of individuals to emotional stimuli have been annotated with respect to seven emotional expression intensities, and (iii) the Ulm-Trier Social Stress Test (Ulm-TSST) dataset comprising of audio-visual data labelled with continuous emotion values (arousal and valence) of people in stressful dispositions. Using the introduced datasets, MuSe 2022 addresses three contemporary affective computing problems: in the Humor Detection Sub-Challenge (MuSe-Humor), spontaneous humour has to be recognised; in the Emotional Reactions Sub-Challenge (MuSe-Reaction), seven fine-grained 'in-the-wild' emotions have to be predicted; and in the Emotional Stress Sub-Challenge (MuSe-Stress), a continuous prediction of stressed emotion values is featured. The challenge is designed to attract different research communities, encouraging a fusion of their disciplines. Mainly, MuSe 2022 targets the communities of audio-visual emotion recognition, health informatics, and symbolic sentiment analysis. This baseline paper describes the datasets as well as the feature sets extracted from them. A recurrent neural network with LSTM cells is used to set competitive baseline results on the test partitions for each sub-challenge. We",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The 3rd edition of the Multimodal Sentiment Analysis (MuSe) Challenge addresses three tasks: humour detection and categorical as well as dimensional emotion recognition. Each corresponding sub-challenge utilises a different dataset. In the Humor Detection Sub-Challenge (MuSe-Humor), participants will detect the presence of humour in football press conference recordings. For MuSe-Humor, the novel Passau Spontaneous Football Coach Humor (Passau-SFCH) dataset is introduced. It features press conference recordings of 10 German Bundesliga football coaches, recorded between August 2017 and November 2017. Initially, the dataset comprises about 18 hours of video, where each of the 10 coaches accounts for at least 90 minutes of data. The subset provided in the challenge still features 11 hours of video. Originally, the data is annotated for direction as well as sentiment of humour following the two-dimensional model of humour proposed in  [36] . In the challenge, only the presence of humour is to be predicted.\n\nFor the Emotional Reactions Sub-Challenge (MuSe-Reaction), emotional reactions are explored by introducing a first of its kind, large-scale (2,222 subjects, 70+ hours), multi-modal (audio and video) dataset: Hume-Reaction . The data was gathered in the wild, with subjects recording their own facial and vocal reactions to a wide range of emotionally evocative videos via their webcam, in a wide variety of at-home recording settings with varying noise conditions. Subjects selected the emotions they experienced in response to each video out of 48 provided categories and rated each selected emotion on a 0-100 intensity scale. In this sub-challenge, participants will apply a multi-output regression to predict the intensities of seven self-reported emotions from the subjects' multi-modal recorded responses: Adoration, Amusement, Anxiety, Disgust, Empathic Pain, Fear, Surprise .\n\nThe Emotional Stress Sub-Challenge (MuSe-Stress) is a regression task on continuous signals for valence and arousal. It is based on the Ulm-Trier Social Stress Test dataset (Ulm-TSST), comprising individuals in a stress-inducing scenario following the Trier Social Stress Test (TSST). This sub-challenge is motivated by the prevalence of stress and its harmful impacts in modern societies  [12] . In addition to audio, video and textual features, Ulm-TSST includes four biological signals captured at a sampling rate of 1 kHz; EDA, Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM). MuSe-Stress was already part of MuSe 2021  [42] , where it attracted considerable interest. Due to some participants reporting challenges generalising to the test set  [20, 27] , we rerun the challenge, allowing participants to submit more predictions than in the previous iteration. We thereby hope to encourage participants to thoroughly explore the robustness of their proposed approaches. Moreover, for this year's MuSe-Stress sub-challenge, we use the labels of last year's MuSe-Physio sub-challenge as the arousal gold standard.\n\nBy providing the mentioned tasks in the 2022 edition of MuSe, we aim for addressing research questions that are of interest to affective computing, machine learning and multimodal signal processing communities and encourage a fusion of their disciplines. Further, we hope that our multimodal challenge can yield new insights into the merits of each of the core modalities, as well as various multimodal fusion approaches. Participants are allowed to use the provided feature sets in the challenge packages and integrate them into their own machine learning frameworks.\n\nThe paper is structured as follows: Section 2 introduces the three sub-challenges alongside with the datasets they are based on, and outlines the challenge protocol. Then, pre-processing, provided features, their alignment, and our baseline models are described in Section 3. In Section 4, we present and discuss our baseline results before concluding the paper in Section 5.\n\nA summary of the challenge results can be found in  [1] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Three Sub-Challenges",
      "text": "In what follows, each sub-challenge and dataset is described in detail, as well as the participation guidelines.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Muse-Humor Sub-Challenge",
      "text": "Humour is one of the richest and most consequential elements of human behaviour and cognition  [25]  and thus of high relevance in the context of affective computing and human-computer interaction. As humour can be expressed both verbally and non-verbally, multimodal approaches are especially suited for detecting humour. However, while humour detection is a very active field of research in Natural Language Processing (e. g.,  [16, 52] ), only a few multimodal datasets for humour detection exist  [28, 38, 51] . Especially, to the best of our knowledge, there are no datasets for detecting humour in spontaneous, non-staged situations. With MuSe-Humor, we intend to address this research gap.\n\nIn this challenge, the Passau-SFCH dataset is utilised. It features video and audio recordings of press conferences of 10 German Bundesliga football coaches, during which the coaches occasionally express humour. The press conferences present natural, live, semistaged communication of the coaches to and with journalists in the audience. All subjects are male and aged between 30 and 53 years. The dataset is split into speaker independent partitions. The training set includes the videos of 4 coaches, while the development and test partition both comprise the videos of 3 coaches. We only include segments in which the coach is speaking to ensure that humour is detected from the behaviour of the coach and not the audience, e. g., laughter. Participants are provided with video, audio, and ASR transcripts of said segments. To obtain the transcripts, we utilise a Wav2Vec2-Large-XLSR  [17]  model fine-tuned on the German data in Common Voice  [6]   1  . Moreover, manually corrected transcripts are included.\n\nEvery video was originally labelled by 9 annotators at a 2 Hz rate indicating sentiment and direction of the humour expressed, as defined by the two-dimensional humour model proposed by Martin et al.  [36]  in the Humor Style Questionnaire (HSQ). For the challenge, we only build upon binary humour labels, i. e., indicating if the coach's communication is humorous or not. We obtain a binary label referring to presence or absence of humour using the following three steps. First, we only consider the humour dimension label of sentiment. Second, based on the sentiment labels, we filter out annotators displaying low agreement with other annotators. In order to account for slight lags in annotation signals, we choose to compute the target humour labels for frames of two seconds using a step size of one second. Finally, such a frame is considered as containing humour if at least 3 of the remaining annotators indicate humour within this frame. As a result, 4.9 % of the training partition frames, 2.9 % of the development partition frames, and 3.9 % of the test partition frames are labelled as humorous. We deliberately opted for a split in which the humour label is over-represented in the training partition in order to help participants' models with learning. The provided features are extracted at 2 Hz rates. They can easily be mapped to the 2 s segments they belong to.\n\nFor evaluation, the AUC metric is utilised, indicating how well a model can separate humorous from non-humorous frames.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Muse-Reaction Sub-Challenge",
      "text": "Computational approaches for understanding human emotional reactions are of growing interest to researchers  [32, 46] , with emerging applications ranging from pedagogy  [15]  to medicine  [41] . A person's reaction to a given stimulus can be informative about both the stimulus itself, e. g., whether educational material is interesting to a given audience, and about the person, e. g., their level of empathy  [48]  and well-being  [57] . However, progress in developing computational approaches to understand human emotional reactions has been hampered by the limited availability of large-scale datasets of spontaneous emotional reactions. Thus, for the MuSe-Reaction sub-challenge, we introduce the Hume-Reaction (Hume-Reaction) dataset, which consists of more than 70 hours of audio and video data, from 2,222 subjects from the United States  (1, 138)  and South Africa (1,084), aged from 18.5 -49.0 years old.\n\nThe subjects within the dataset are reacting to a wide range of emotionally evocative stimuli (2,185 stimuli in total  [18] ). Each sample within the dataset has been self-annotated by the subjects themselves for the intensity of 7 emotional expressions in a range from 1-100: Adoration, Amusement, Anxiety, Disgust, Empathic Pain, Fear, Surprise.\n\nThe data is self-recorded via subjects' own webcams in an environment of their choosing, including a wide variety of background, noise, and lighting conditions. Furthermore, different subjects spontaneously reacted with their faces and voices to varying degrees, such that the audio and multi-modal aspects of this sub-challenge will be particularly interesting to incorporate. The organisers also provide labels for detected (energy-based) vocalisations to aid participants in incorporating audio, with a total of 8,064 multi-modal recordings found to include vocalisations.\n\nFor the MuSe-Reaction sub-challenge the aim is to perform a multi-output regression from features extracted from the multimodal (audio and video) data for the intensity of 7 emotional reaction classes. For this sub-challenge's evaluation, the Pearson's correlations coefficient (ùúå) is reported as the primary baseline.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Muse-Stress Sub-Challenge",
      "text": "The MuSe-Stress task is based on the multimodal Ulm-TSST database, for which subjects were recorded in a stress-inducing, free speech scenario, following the TSST protocol  [31] . In the TSST, a job interview situation is simulated. Following a short period of preparation, a five-minute free speech oral presentation is given by the subjects. This presentation is supervised by two interviewers, who do not communicate with the subjects during the five minutes. Ulm-TSST comprises recordings of such TSST presentations of 69 participants (49 of them female), aged between 18 and 39 years. Overall, Ulm-TSST includes about 6 hours of data (cf. Table  1 ). On the one hand, the dataset features the audio, video, and text modalities. On the other hand, the physiological signals ECG, RESP, and BPM are provided. For extensive experiments on multimodal emotion recognition in TSST-based multimodal datasets see  [9] .\n\nUlm-TSST has been annotated by three raters continuously for the emotional dimensions of valence and arousal, at a 2 Hz sampling rate. Regarding valence, a gold standard is created by fusing the three corresponding annotator ratings, utilising the Rater Aligned Annotation Weighting (RAAW) method from the MuSe-Toolbox  [45] . RAAW addresses the difficulties arising when emotion annotations -subjective in their nature -are to be combined into a gold standard signal. In short, RAAW first tackles the inherent rater lag by aligning the (per annotator) standardised signals via generalised Canonical Time Warping (CTW)  [56] . After that, the Evaluator Weighted Estimator (EWE)  [26]  is applied to the aligned signals. EWE fuses the individual signals using a weighting based on each rater's inter-rater agreement to the mean of all others. A detailed description of RAAW can be found in  [45] . We obtain a mean inter-rater agreement of 0.204 (¬± 0.200) for valence.\n\nAs for the arousal gold standard, a different approach is employed. Instead of fusing the three annotators' arousal ratings, we take the labels of last year's MuSe-Physio sub-challenge as the arousal gold standard. Here, the annotator with lowest inter-rater agreement is discarded and replaced with the subject's electrodermal activity signal (EDA) which is known to indicate emotional arousal  [14] . This signal is downsampled to 2 Hz and smoothed using a Savitzky-Golay filtering approach (window size of 26 steps) in advance. Then, the two remaining annotators and the preprocessed EDA signal are again fused via RAAW, resulting in a mean inter annotator agreement of 0.233 (¬±0.289). This signal is called physiological arousal in the following. The motivation to employ this kind of gold standard is to obtain a more objective arousal signal. Considering such an objective criterion for arousal in addition to subjective annotations is especially relevant given the task at hand: in the job interview setting, individuals can be expected to try to hide their arousal, making it more difficult for annotators to recognise it. Detailed experiments on combining subjective annotations with objective physiological signals are provided in  [8] .\n\nUlm-TSST is split into train, development, and test partitions containing 41, 14, and 14 videos, respectively. The split is identical to the split used in last year's challenge. Figure  1  shows the distributions of the valence and physiological arousal signals for the dataset.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Challenge Protocol",
      "text": "All challenge participants are required to complete the End User License Agreement (EULA) which is available on the MuSe 2022 homepage  2  . Further, the participants must hold an academic affiliation. Each challenge contribution should be followed by a paper that describes the applied methods and provides the obtained results. The peer review process is double-blind. To obtain results on the test set, participants upload their predictions for unknown test labels on CodaLab 3  . The number of prediction uploads depends on the sub-challenge: for MuSe-Humor and MuSe-Reaction, up to 5 prediction uploads can be submitted, while for MuSe-Stress, up to 20 prediction uploads are allowed. We want to stress that the organisers themselves do not participate as competitors in the challenge.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Features And Model",
      "text": "To enable the participants to get started quickly, we provide a set of features extracted from each sub-challenge's video data. More precisely, the provided features include of up to five model-ready video, audio, and linguistic feature sets, depending on the subchallenge  4  . Regarding the label sampling rate, labels refer to 2 s windows in MuSe-Humor. The MuSe-Stress data is labelled at a 2 Hz rate. For MuSe-Reaction, there is one label vector of 7 classes per sample.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pre-Processing",
      "text": "All datasets are split into training, development, and test sets. For all partitions, ratings, speaker independence, and duration are taken into consideration (cf. Table  1 ). The videos in Passau-SFCH are cut to only include segments in which the respective coach is actually speaking. As the press conference setting can be seen as a dialogue between journalists and the coach, the answers given by each coach provide a natural segmentation of the Passau-SFCH data. For MuSe-Reaction -as can be seen in Table  1  -, a 60-20-20% split strategy is applied. There is no additional segmentation applied to clean the data further, each sample contains a single reaction to an emotional stimulus, and labels were normalised per sample to range from [0 :1]. For further exploration, the participants are also provided with voice activity segments from the samples, which show to contain audio of substantial energy. In the Ulm-TSST dataset, we make sure to exclude scenes which are not a part of the TSST setting, e. g., the instructor speaking. Moreover, we cut segments in which TSST participants reveal their names. The Ulm-TSST dataset is not segmented any further.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio",
      "text": "All audio files are first normalised to -3 decibels and then converted from stereo to mono, at 16 kHz, 16 bit. Afterwards, we make use of the two well-established machine learning toolkits openSMILE  [23]  and DeepSpectrum  [3]  for expert-designed and deep feature extraction from the audio recordings. Both systems have proved valuable in audio-based Speech Emotion Recognition (SER) tasks  [5, 10, 24] .\n\n3.2.1 eGeMAPS. The openSMILE toolkit  [23]   5  is used for the extraction of the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [22] . This feature set which is proven valuable for SER tasks  [7] , also in past MuSe challenges (e. g.,  [50] ), includes 88 acoustic features that can capture affective physiological changes in voice production. In MuSe-Humor, we use the default configuration to extract the 88 eGeMAPS functionals for each two second audio frame. For the audio of MuSe-Reaction, the 88 eGeMAPS functionals are extracted with a step size of 100 ms and window size of 1 second. Regarding MuSe-Stress, the functionals are obtained with a 2 Hz rate, using a window size of 5 seconds.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Deepspectrum.",
      "text": "The principle of DeepSpectrum  [3]   6  is to utilise pre-trained image Convolutional Neural Networks (CNNs) for the extraction of deep features from visual representations (e. g., Mel-spectrograms) of audio signals. The efficacy of DeepSpectrum features has been demonstrated for SER  [39] , sentiment analysis  [2] , and general audio processing tasks  [4] . For our DeepSpectrum baseline experiments, we use DenseNet121  [30]  pre-trained on ImageNet  [40]  as the CNN backbone. The audio is represented as a Mel-spectrogram with 128 bands employing the viridis colour mapping. Subsequently, the spectrogram representation is fed into DenseNet121, and the output of the last pooling layer is taken as a 1 024-dimensional feature vector. The window size is set to one second, the hop-size to 500 ms.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video",
      "text": "To extract specific image descriptors related to facial expressions, we make use of two CNN architectures: Multi-task Cascaded Convolutional Networks (MTCNN) and VGGface 2. We also provide a set of Facial Action Units (FAUs) obtained from faces of individuals in the datasets. Further, participants are also given the set of extracted faces from the raw frames. In the videos of MuSe-Humor, typically more than one face is visible. As this sub-challenge's objective is to predict the expression of humour of the coach, we only provide the faces of the respective coach and the features computed for them.  7  , pre-trained on the datasets WIDER FACE  [53]  and CelebA  [33] , is used to detect faces in the videos. Two steps are carried out to filter extracted faces that do not show the coach in Passau-SFCH: first, we automatically detect the respective coach's faces using FaceNet 8 embeddings of reference pictures showing the coach. The results of this procedure are then corrected manually. Ulm-TSST, in contrast, has a simple, static setting. The camera position is fixed and videos only show the TSST subjects who typically do not move much. Similarly, for MuSe-Reaction, the video is captured from a fixed webcam. Hence, the performance of MTCNN is almost flawless for both Hume-Reaction and Ulm-TSST. The extracted faces then serve as inputs of the feature extractors VGGface 2 and Py-Feat .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Mtcnn. The Mtcnn [54] Model",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vggface 2.",
      "text": "The purpose of VGGface 2 is to compute general facial features for the previously extracted faces. VGGface 2  [13]  is a dataset for the task of face recognition. It contains 3.3 million faces of about 9,000 different persons. As the dataset is originally intended for supervised facial recognition purposes, models trained on it compute face encodings not directly related to emotion and sentiment. We use a ResNet50  [29]  trained on VGGface 2  9  and detach its classification layer, resulting in a 512-dimensional feature vector output referred to as VGGface 2 in the following.  [21] , are closely related to the expression of emotions. Hence, detecting FAUs is a promising and popular approach to the visual prediction of affect-related targets (e. g.,  [35] ). We employ Py-Feat 10  to obtain predictions for the presence of 20 different FAUs. We do not change Py-Feat 's default configuration, so that a pre-trained random forest model is used to predict the FAUs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fau. Faus As Originally Proposed By Ekman And Friesen",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Language: Bert",
      "text": "In recent years, pre-trained Transformer language models account for state-of-the-art results in numerous Natural Language Processing tasks, also in tasks related to affect (e. g.,  [10] ). In general, these models are pretrained in a self-supervised way utilising large amounts of text data. Subsequently, they can be fine-tuned for specific downstream tasks. For the transcripts of MuSe-Humor and MuSe-Stress, we employ a German version of the BERT (Bidirectional Encoder Representations from Transformers  [19] ) model  11  . No further fine-tuning is applied. For both Passau-SFCH and MuSe-Stress, we extract the BERT token embeddings. Additionally, we obtain 768 dimensional sentence embeddings for all texts in Passau-SFCH by using the encodings of BERT 's [ùê∂ùêøùëÜ] token. In all cases, we average the embeddings provided by the last 4 layers of the BERT model, following  [47] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Alignment",
      "text": "For each task, at least two different modalities are available. Typically, sampling rates per modality may differ. We sample the visual features with a rate of 2 Hz in all sub-challenges. The only exception is the FAUs in MuSe-Reaction, which are sampled at a 4 Hz rate. Regarding the audio features (DeepSpectrum and eGeMAPS), we apply the same frequency in MuSe-Humor and MuSe-Stress, while eGeMAPS features are obtained using a step size of 100 ms in MuSe-Reaction. As VGGish and FAUs are only meaningful if the respective frame actually includes a face, we impute frames without a face with zeros.\n\nFor MuSe-Humor, the binary humour label refers to frames of at most 2 seconds length. Hence, each label in MuSe-Humor corresponds to at most 4 facial and acoustic feature vectors. 2 Hz sentence embedding vectors are constructed by assigning every sentence to the 500 ms frames it corresponds to. If two sentences fall into the same frame, their embeddings are averaged to form the feature for that frame.\n\nRegarding MuSe-Reaction, there is no alignment needed with labels, as each file is associated to a single vector of 7 emotional reaction labels.\n\nFor the MuSe-Stress sub-challenge, we provide label-aligned features. Hence, these features exactly align with the labels. We apply zero-padding to the frames, where the feature type is absent. Moreover, we downsample the biosignals in Ulm-TSST to 2 Hz, followed by a smoothing utilising a Savitzky-Golay filter. Participants are provided with both the raw signals and the downsampled ones.\n\nIn both Ulm-TSST and Passau-SFCH, manual transcripts are available. However, they lack timestamps. Hence, we reconstruct word level timestamps utilising the Montreal Forced Aligner (MFA)  [37]  tool. Here, we employ the German (Prosodylab) model and the German Prosodylab dictionary. The text features are then aligned to the 2 Hz label signal by repeating each word embedding throughout the determined interval of the corresponding word. In case a 500 ms frame comprises more than one word, we average over the word embeddings. Zero imputing is applied to parts where subjects do not speak. For the sentence embeddings in Passau-SFCH we choose an analogous approach, repeating and, if applicable, averaging the embeddings.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Baseline Model: Lstm-Rnn",
      "text": "The sequential nature of the tasks makes recurrent neural networks (RNNs) a natural choice for a fairly simple baseline system. More specifically, we employ a Long Short-Term Memory (LSTM)-RNN. Initially, we train a single model on each of the available feature sets. Regarding MuSe-Stress, we separately train a model for both labels, valence and physiological arousal. We conduct an extensive hyperparameter search for each prediction target and feature. We thus optimise the number of RNN layers, the dimensionality of the LSTM's hidden vectors and the learning rate. Of note, we also experiment with both unidirectional and bidirectional LSTMs. The code as well as the configurations found in the hyperparameter search are available in the baseline GitHub repository 12 .\n\nEach label in MuSe-Humor is predicted based on all feature vectors belonging to the corresponding 2 s window. Hence, the sequence length in the MuSe-Humor training process is at most 4 steps.\n\nIn both MuSe-Reaction and MuSe-Stress, we make use of a segmentation approach which showed to improve results in previous works  [43, 44, 47] . We find that a segmentation of the training data with a window size of 50 s (i. e., 200 steps) and a hop size of 25 s (i.e., 100 steps) leads to good results for MuSe-Stress. For MuSe-Reaction a slightly larger size of 500 steps and a hop size of 250, lead to more robust results.\n\nFollowing the unimodal experiments, in order to combine different modalities, for MuSe-Humor and MuSe-Stress, we implement a simple late fusion approach. We apply the exact same training procedure as before, now treating the predictions of previously trained unimodal models as input features. In these experiments, we use one configuration per task, without performing a hyperparameter search for every possible modality combination in MuSe-Stress. As this approach for late fusion is less suited to a multi-label strategy, we apply an early fusion strategy for MuSe-Reaction. For early fusion, we simply concatenate the best performing feature sets for each modality (audio and video), and then train a new model with the same hyperparameters from the uni-modal experiments. The code and configuration for the two fusion methods are also part of the baseline GitHub repository 13 . Moreover, the repository also includes links to the best model weight files in order to ease reproducibility.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Baseline Results",
      "text": "We apply the model described above for every sub-challenge. In what follows, we discuss the baseline results in more detail.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Muse-Humor",
      "text": "The results for MuSe-Humor are given in Table  2 . Each result is obtained from running the LSTM using the specified features with 5 different fixed seeds, consistent with the challenge setting.\n\nTable  2 : Results for MuSe-Humor. We report the AUC-Scores for the best among 5 fixed seeds, as well as the mean AUC-Scores over these seeds and the corresponding standard deviations.\n\n[ Evaluating audio and video features for the MuSe-Humor subchallenge shows a clear pattern. The video-based features, FAU and VGGish, clearly outperform the audio-based features with VGGish accounting for an AUC of .8480 on the test set while eGeMAPS only achieves .6952 AUC. This comes as no surprise, given that the expression of humour is often accompanied by smile or laughter and thus recognisable from facial expressions features. A manual inspection of the humorous segments confirms this intuition. Nevertheless, audio features are able to detect humour, too. Partly, this may be due to the presence of laughter. The performance of text features (.7888 on the test set) is slightly worse than for the features 13 https://github.com/EIHW/MuSe2022 based on the video modality, but also better than the performance of the audio features. We find that the sentence-level BERT features outperform the token-level features. With the simple fusion of modalities, the performance is not improved. Specifically, the late fusion approach typically shows worse generalisation to the test data than the unimodal experiments. e. g., there is a discrepancy of about .16 between mean AUC on the development (.8219) and test (.6633) sets for the combination of audio and video.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Muse-Reaction",
      "text": "Table  3  shows the results for the MuSe-Reaction baseline. As expected, the audio results are substantially lower than those from the video modality. Of particular note, as it pertains to audio, we see that the emotion-tailored feature-set of eGeMAPS performs poorly, almost 0.05 ùúå lower on the development set than the DeepSpectrum features. Given that there is limited speech in the data set, this may be why the DeepSpectrum features perform better, as due to being spectrogram-based, they can potentially capture a more general acoustic scene and non-speech verbalisations potentially better.\n\nFor the video features, the FAUs are performing much better on the test set than VGGface 2 (although both are derived from faces), given the nature of the data being 'reactions', it may be that the facial action units are much more dynamic generally, and these features model more accurately the emotional expression occurring within the scene.\n\nInterestingly, when we observe the individual class scores, we see that Amusement is consistently performing better than all other classes, a finding which is consistent for audio and video features (eGeMAPS: .148 ùúå, and FAU: .405 ùúå). As well as being the most likely class to contain non-verbal communication e. g., laughter, this performance may be due to the known ease of modelling highly aroused states of emotional expression  [49] . However, it may also relate to the valence of the emotions as we can see from Figure  2 , the Disgust class is the worst performing for FAU.\n\nIt is worth noting that in this case, the early-fusion of the two best-performing feature sets in each modality does not yield any beneficial results. This holds, although we do consider that through the use of a knowledge-based audio approach, we may see more improvement for audio, which may result in stronger performance via fusion.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Muse-Stress",
      "text": "Table  4  reports the results obtained for MuSe-Stress. Consistent with results reported by some of last year's participants  [20, 27] , the results for MuSe-Stress partly fail to generalise to the test data. With respect to single modality experiments, this observation is particularly significant for the video features. For example, the best seed for predicting physiological arousal based on Facial Action Units yields a CCC of .5191 on the development set, but only results in a CCC of .0785 on the test partition. The audio feature sets, in comparison, achieve better generalisation with the most extreme difference between development and test CCC being about .12 (for eGeMAPS on physiological arousal). Moreover, for both prediction targets, the DeepSpectrum audio features perform best among the unimodal approaches with CCC values of .4239 and .4931 on the test sets for physiological arousal and valence, respectively.   3 . A surprising aspect of the unimodal results is that audio features yield better results for valence than for arousal, contrary to previous results in the domain of multimodal emotion recognition. For the visual features, no such tendency to work better for one of the two dimensions can be observed: FAUs lead to better results for predicting valence (mean CCC of .3878 on the test set) than for physiological arousal (.1135); the same is true for the VGGface 2 features (.1968 and .1576 mean CCC on the test set for valence and arousal, respectively). The textual BERT features account for higher CCCs on the development partition for valence (mean CCC of .3221) than for physiological arousal (.2828). Surprisingly, however, for arousal, they generalise better to the test data, while for valence, the mean BERT CCCs drops from .3221 to .1872 when evaluating on the test set. These partly counterintuitive results may be attributed to the job interview setting. Job interviewees typically suppress nervousness in an attempt to give a relaxed, sovereign impression. This might make the detection of arousal from audio and video difficult. The comparably stable performance of textual features for physiological arousal may be due to correlations between participants pausing their speech for a longer time -or hardly at all -and arousal. We find such correlations to exist for several participants.\n\nWe also experiment with the downsampled biosignals, motivated by some of last year's approaches (  [11, 34, 55] ) to the task which used these signals as a feature. To do so, we concatenate the three signals (BPM, ECG, and respiratory rate) into a three-dimensional feature vector and normalise them. Here, severe generalisation and stability problems can be observed. To give an example, for arousal, the mean CCC performance of biosignal features on the development set is .2793, but for the test set, it drops to .1095. What is more, the standard deviations obtained with the biosignal results are consistently higher than those of any other modality. Because of these issues and in order not to inflate the number of experiments, we exclude the physiological modality from the late fusion experiments.\n\nWhile valence prediction could not be improved by late fusion, the late fusion of the audio and text modality accounts for the best result on the test set for physiological arousal prediction (.4761 CCC), slightly surpassing the late fusion of audio and text (.4413) as well as DeepSpectrum (.4239). For valence, a generalisation issue for late fusion is apparent. To give an example, the late fusion of acoustic and visual features yields by far the best result on the development set (.6914) but only achieves a CCC of .4906 on the test set.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "This baseline paper introduced MuSe 2022 -the 3rd Multimodal Sentiment Analysis challenge. MuSe 2022 features three multimodal datasets: Passau-SFCH with press conference recordings The provided baselines give a first impression on which features and modalities may be suited best for the different tasks. We believe that more refined methods of combining different modalities and features may lead to significant improvements over the reported baseline results. We hope that MuSe 2022 serves as a stimulating environment for developing and evaluating such novel approaches.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Frequency distribution in the partitions train,",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the dis-",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Erik Cambria\nAlan Cowen": "Nanyang Technological University\nHume AI",
          "Bj√∂rn W. Schuller": "Imperial College London"
        },
        {
          "Erik Cambria\nAlan Cowen": "Singapore\nNew York, USA",
          "Bj√∂rn W. Schuller": "London, United Kingdom"
        },
        {
          "Erik Cambria\nAlan Cowen": "ABSTRACT",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "report an Area Under the Curve (AUC) of .8480 for MuSe-Humor;"
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": ".2801 mean (from 7-classes) Pearson‚Äôs Correlations Coefficient (ùúå)"
        },
        {
          "Erik Cambria\nAlan Cowen": "The Multimodal Sentiment Analysis Challenge (MuSe) 2022 is ded-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "for MuSe-Reaction, as well as .4931 Concordance Correlation Co-"
        },
        {
          "Erik Cambria\nAlan Cowen": "icated to multimodal sentiment and emotion recognition. For this",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "efficient (CCC) and .4761 for valence and arousal in MuSe-Stress,"
        },
        {
          "Erik Cambria\nAlan Cowen": "year‚Äôs challenge, we feature three datasets:\n(i)\nthe Passau Spon-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "respectively."
        },
        {
          "Erik Cambria\nAlan Cowen": "taneous Football Coach Humor (Passau-SFCH) dataset that con-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "tains audio-visual recordings of German football coaches, labelled",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "CCS CONCEPTS"
        },
        {
          "Erik Cambria\nAlan Cowen": "for the presence of humour;\n(ii)\nthe Hume-Reaction dataset\nin",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "which reactions of individuals to emotional stimuli have been an-",
          "Bj√∂rn W. Schuller": "‚Ä¢ Computing methodologies ‚Üí Neural networks; Artifi-"
        },
        {
          "Erik Cambria\nAlan Cowen": "notated with respect\nto seven emotional expression intensities,",
          "Bj√∂rn W. Schuller": "cial intelligence; Computer vision; Natural language processing."
        },
        {
          "Erik Cambria\nAlan Cowen": "and (iii) the Ulm-Trier Social Stress Test (Ulm-TSST) dataset com-",
          "Bj√∂rn W. Schuller": "KEYWORDS"
        },
        {
          "Erik Cambria\nAlan Cowen": "prising of audio-visual data labelled with continuous emotion val-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "Multimodal Sentiment Analysis; Affective Computing; Humor"
        },
        {
          "Erik Cambria\nAlan Cowen": "ues (arousal and valence) of people in stressful dispositions. Us-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "Detection; Emotion Recognition; Multimodal Fusion; Challenge;"
        },
        {
          "Erik Cambria\nAlan Cowen": "ing the introduced datasets, MuSe 2022 addresses three contempo-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "Benchmark"
        },
        {
          "Erik Cambria\nAlan Cowen": "rary affective computing problems: in the Humor Detection Sub-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "ACM Reference Format:"
        },
        {
          "Erik Cambria\nAlan Cowen": "Challenge (MuSe-Humor), spontaneous humour has to be recog-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "Lukas Christ, Shahin Amiriparian, Alice Baird, Panagiotis Tzirakis, Alexan-"
        },
        {
          "Erik Cambria\nAlan Cowen": "nised; in the Emotional Reactions Sub-Challenge (MuSe-Reaction),",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "der Kathan, Niklas M√ºller, Lukas Stappen, Eva-Maria Me√üner, Andreas"
        },
        {
          "Erik Cambria\nAlan Cowen": "seven fine-grained ‚Äòin-the-wild‚Äô emotions have to be predicted; and",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "K√∂nig, Alan Cowen, Erik Cambria, and Bj√∂rn W. Schuller. 2022. The MuSe"
        },
        {
          "Erik Cambria\nAlan Cowen": "in the Emotional Stress Sub-Challenge (MuSe-Stress), a continu-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reac-"
        },
        {
          "Erik Cambria\nAlan Cowen": "ous prediction of stressed emotion values is featured. The challenge",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "tions, and Stress. In Proceedings of the 3rd International Multimodal Sentiment"
        },
        {
          "Erik Cambria\nAlan Cowen": "is designed to attract different research communities, encouraging",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "Analysis Workshop and Challenge (MuSe‚Äô 22), October 10, 2022, Lisboa, Portu-"
        },
        {
          "Erik Cambria\nAlan Cowen": "a fusion of their disciplines. Mainly, MuSe 2022 targets the com-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "gal. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3551876."
        },
        {
          "Erik Cambria\nAlan Cowen": "munities of audio-visual emotion recognition, health informatics,",
          "Bj√∂rn W. Schuller": "3554817"
        },
        {
          "Erik Cambria\nAlan Cowen": "and symbolic sentiment analysis. This baseline paper describes the",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "datasets as well as the feature sets extracted from them. A recur-",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "1\nINTRODUCTION"
        },
        {
          "Erik Cambria\nAlan Cowen": "rent neural network with LSTM cells is used to set competitive",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "The 3rd edition of\nthe Multimodal Sentiment Analysis (MuSe)"
        },
        {
          "Erik Cambria\nAlan Cowen": "baseline results on the test partitions for each sub-challenge. We",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "Challenge addresses three tasks: humour detection and categori-"
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "cal as well as dimensional emotion recognition. Each correspond-"
        },
        {
          "Erik Cambria\nAlan Cowen": "Permission to make digital or hard copies of all or part of this work for personal or",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "ing sub-challenge utilises a different dataset.\nIn the Humor De-"
        },
        {
          "Erik Cambria\nAlan Cowen": "classroom use is granted without fee provided that copies are not made or distributed",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "for profit or commercial advantage and that copies bear this notice and the full citation",
          "Bj√∂rn W. Schuller": "tection Sub-Challenge (MuSe-Humor), participants will detect"
        },
        {
          "Erik Cambria\nAlan Cowen": "on the first page. Copyrights for components of this work owned by others than ACM",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "the presence of humour in football press conference recordings."
        },
        {
          "Erik Cambria\nAlan Cowen": "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "to post on servers or to redistribute to lists, requires prior specific permission and/or a",
          "Bj√∂rn W. Schuller": "For MuSe-Humor, the novel Passau Spontaneous Football Coach"
        },
        {
          "Erik Cambria\nAlan Cowen": "fee. Request permissions from permissions@acm.org.",
          "Bj√∂rn W. Schuller": "Humor (Passau-SFCH) dataset\nis introduced.\nIt\nfeatures press"
        },
        {
          "Erik Cambria\nAlan Cowen": "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "conference recordings of 10 German Bundesliga football coaches,"
        },
        {
          "Erik Cambria\nAlan Cowen": "¬© 2022 Association for Computing Machinery.",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "",
          "Bj√∂rn W. Schuller": "recorded between August 2017 and November 2017. Initially, the"
        },
        {
          "Erik Cambria\nAlan Cowen": "ACM ISBN 978-1-4503-9484-0/22/10. . . $15.00",
          "Bj√∂rn W. Schuller": ""
        },
        {
          "Erik Cambria\nAlan Cowen": "https://doi.org/10.1145/3551876.3554817",
          "Bj√∂rn W. Schuller": "dataset comprises about 18 hours of video, where each of the 10"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the duration for each sub-challenge hh :mm :ss.",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "#",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "4",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "3",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "3",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "10",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "is described in"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "detail, as well as the participation guidelines.": ""
        },
        {
          "detail, as well as the participation guidelines.": "3.0"
        },
        {
          "detail, as well as the participation guidelines.": ""
        },
        {
          "detail, as well as the participation guidelines.": ""
        },
        {
          "detail, as well as the participation guidelines.": "2.5"
        },
        {
          "detail, as well as the participation guidelines.": "2.0"
        },
        {
          "detail, as well as the participation guidelines.": ""
        },
        {
          "detail, as well as the participation guidelines.": "1.5"
        },
        {
          "detail, as well as the participation guidelines.": "1.0"
        },
        {
          "detail, as well as the participation guidelines.": "0.5"
        },
        {
          "detail, as well as the participation guidelines.": "0.0"
        },
        {
          "detail, as well as the participation guidelines.": ""
        },
        {
          "detail, as well as the participation guidelines.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe 2022: Baseline Paper": "Every video was originally labelled by 9 annotators at a 2 Hz",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "For the MuSe-Reaction sub-challenge the aim is to perform a"
        },
        {
          "MuSe 2022: Baseline Paper": "rate indicating sentiment and direction of the humour expressed, as",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "multi-output regression from features extracted from the multi-"
        },
        {
          "MuSe 2022: Baseline Paper": "defined by the two-dimensional humour model proposed by Martin",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "modal (audio and video) data for the intensity of 7 emotional re-"
        },
        {
          "MuSe 2022: Baseline Paper": "et al. [36] in the Humor Style Questionnaire (HSQ). For the chal-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "action classes. For this sub-challenge‚Äôs evaluation, the Pearson‚Äôs"
        },
        {
          "MuSe 2022: Baseline Paper": "lenge, we only build upon binary humour labels, i. e., indicating if",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "correlations coefficient (ùúå) is reported as the primary baseline."
        },
        {
          "MuSe 2022: Baseline Paper": "the coach‚Äôs communication is humorous or not. We obtain a binary",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "label referring to presence or absence of humour using the follow-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "ing three steps. First, we only consider the humour dimension label",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2.3\nThe MuSe-Stress Sub-challenge"
        },
        {
          "MuSe 2022: Baseline Paper": "of sentiment. Second, based on the sentiment labels, we filter out",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "annotators displaying low agreement with other annotators.\nIn",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "The MuSe-Stress task is based on the multimodal Ulm-TSST data-"
        },
        {
          "MuSe 2022: Baseline Paper": "order to account for slight lags in annotation signals, we choose to",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "base, for which subjects were recorded in a stress-inducing, free"
        },
        {
          "MuSe 2022: Baseline Paper": "compute the target humour labels for frames of two seconds using",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "speech scenario, following the TSST protocol [31]. In the TSST, a"
        },
        {
          "MuSe 2022: Baseline Paper": "a step size of one second. Finally, such a frame is considered as",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "job interview situation is simulated. Following a short period of"
        },
        {
          "MuSe 2022: Baseline Paper": "containing humour if at least 3 of the remaining annotators indicate",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "preparation, a five-minute free speech oral presentation is given by"
        },
        {
          "MuSe 2022: Baseline Paper": "humour within this frame. As a result, 4.9 % of the training parti-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the subjects. This presentation is supervised by two interviewers,"
        },
        {
          "MuSe 2022: Baseline Paper": "tion frames, 2.9 % of the development partition frames, and 3.9 % of",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "who do not communicate with the subjects during the five minutes."
        },
        {
          "MuSe 2022: Baseline Paper": "the test partition frames are labelled as humorous. We deliberately",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Ulm-TSST comprises recordings of such TSST presentations of 69"
        },
        {
          "MuSe 2022: Baseline Paper": "opted for a split in which the humour label is over-represented in",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "participants (49 of them female), aged between 18 and 39 years."
        },
        {
          "MuSe 2022: Baseline Paper": "the training partition in order to help participants‚Äô models with",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Overall, Ulm-TSST includes about 6 hours of data (cf. Table 1)."
        },
        {
          "MuSe 2022: Baseline Paper": "learning. The provided features are extracted at 2 Hz rates. They",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "On the one hand, the dataset features the audio, video, and text"
        },
        {
          "MuSe 2022: Baseline Paper": "can easily be mapped to the 2 s segments they belong to.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "modalities. On the other hand, the physiological signals ECG, RESP,"
        },
        {
          "MuSe 2022: Baseline Paper": "For evaluation, the AUC metric is utilised, indicating how well a",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "and BPM are provided. For extensive experiments on multimodal"
        },
        {
          "MuSe 2022: Baseline Paper": "model can separate humorous from non-humorous frames.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "emotion recognition in TSST-based multimodal datasets see [9]."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Ulm-TSST has been annotated by three raters continuously for"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the emotional dimensions of valence and arousal, at a 2 Hz sam-"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "pling rate. Regarding valence, a gold standard is created by fus-"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ing the three corresponding annotator ratings, utilising the Rater"
        },
        {
          "MuSe 2022: Baseline Paper": "2.2\nThe MuSe-Reaction Sub-Challenge",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Aligned Annotation Weighting (RAAW) method from the MuSe-"
        },
        {
          "MuSe 2022: Baseline Paper": "Computational approaches for understanding human emotional",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Toolbox [45]. RAAW addresses the difficulties arising when emotion"
        },
        {
          "MuSe 2022: Baseline Paper": "reactions are of growing interest to researchers [32, 46], with emerg-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "annotations ‚Äì subjective in their nature ‚Äì are to be combined into"
        },
        {
          "MuSe 2022: Baseline Paper": "ing applications ranging from pedagogy [15] to medicine [41]. A",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "a gold standard signal. In short, RAAW first tackles the inherent"
        },
        {
          "MuSe 2022: Baseline Paper": "person‚Äôs reaction to a given stimulus can be informative about both",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "rater lag by aligning the (per annotator) standardised signals via"
        },
        {
          "MuSe 2022: Baseline Paper": "the stimulus itself, e. g., whether educational material is interesting",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "generalised Canonical Time Warping (CTW) [56]. After that, the"
        },
        {
          "MuSe 2022: Baseline Paper": "to a given audience, and about the person, e. g., their level of empa-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Evaluator Weighted Estimator (EWE) [26] is applied to the aligned"
        },
        {
          "MuSe 2022: Baseline Paper": "thy [48] and well-being [57]. However, progress in developing com-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "signals. EWE fuses the individual signals using a weighting based"
        },
        {
          "MuSe 2022: Baseline Paper": "putational approaches to understand human emotional reactions",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "on each rater‚Äôs inter-rater agreement to the mean of all others. A"
        },
        {
          "MuSe 2022: Baseline Paper": "has been hampered by the limited availability of large-scale datasets",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "detailed description of RAAW can be found in [45]. We obtain a"
        },
        {
          "MuSe 2022: Baseline Paper": "of spontaneous emotional reactions. Thus, for the MuSe-Reaction",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "mean inter-rater agreement of 0.204 (¬± 0.200) for valence."
        },
        {
          "MuSe 2022: Baseline Paper": "sub-challenge, we introduce the Hume-Reaction (Hume-Reaction)",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "As for the arousal gold standard, a different approach is em-"
        },
        {
          "MuSe 2022: Baseline Paper": "dataset, which consists of more than 70 hours of audio and video",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ployed. Instead of fusing the three annotators‚Äô arousal ratings, we"
        },
        {
          "MuSe 2022: Baseline Paper": "data, from 2,222 subjects from the United States (1,138) and South",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "take the labels of\nlast year‚Äôs MuSe-Physio sub-challenge as the"
        },
        {
          "MuSe 2022: Baseline Paper": "Africa (1,084), aged from 18.5 ‚Äì 49.0 years old.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "arousal gold standard. Here, the annotator with lowest inter-rater"
        },
        {
          "MuSe 2022: Baseline Paper": "The subjects within the dataset are reacting to a wide range",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "agreement is discarded and replaced with the subject‚Äôs electroder-"
        },
        {
          "MuSe 2022: Baseline Paper": "of emotionally evocative stimuli (2,185 stimuli in total [18]). Each",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "mal activity signal (EDA) which is known to indicate emotional"
        },
        {
          "MuSe 2022: Baseline Paper": "sample within the dataset has been self-annotated by the subjects",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "arousal [14]. This signal is downsampled to 2 Hz and smoothed us-"
        },
        {
          "MuSe 2022: Baseline Paper": "themselves for the intensity of 7 emotional expressions in a range",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ing a Savitzky‚ÄìGolay filtering approach (window size of 26 steps) in"
        },
        {
          "MuSe 2022: Baseline Paper": "from 1-100: Adoration, Amusement, Anxiety, Disgust, Empathic",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "advance. Then, the two remaining annotators and the preprocessed"
        },
        {
          "MuSe 2022: Baseline Paper": "Pain, Fear, Surprise.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "EDA signal are again fused via RAAW, resulting in a mean inter"
        },
        {
          "MuSe 2022: Baseline Paper": "The data is self-recorded via subjects‚Äô own webcams in an envi-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "annotator agreement of 0.233 (¬±0.289). This signal is called phys-"
        },
        {
          "MuSe 2022: Baseline Paper": "ronment of their choosing, including a wide variety of background,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "in the following. The motivation to employ this\niological arousal"
        },
        {
          "MuSe 2022: Baseline Paper": "noise, and lighting conditions. Furthermore, different subjects spon-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "kind of gold standard is to obtain a more objective arousal signal."
        },
        {
          "MuSe 2022: Baseline Paper": "taneously reacted with their faces and voices to varying degrees,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Considering such an objective criterion for arousal in addition to"
        },
        {
          "MuSe 2022: Baseline Paper": "such that the audio and multi-modal aspects of this sub-challenge",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "subjective annotations is especially relevant given the task at hand:"
        },
        {
          "MuSe 2022: Baseline Paper": "will be particularly interesting to incorporate. The organisers also",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "in the job interview setting, individuals can be expected to try to"
        },
        {
          "MuSe 2022: Baseline Paper": "provide labels for detected (energy-based) vocalisations to aid par-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "hide their arousal, making it more difficult for annotators to recog-"
        },
        {
          "MuSe 2022: Baseline Paper": "ticipants in incorporating audio, with a total of 8,064 multi-modal",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "nise it. Detailed experiments on combining subjective annotations"
        },
        {
          "MuSe 2022: Baseline Paper": "recordings found to include vocalisations.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "with objective physiological signals are provided in [8]."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "3.2\nAudio"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Ulm-TSST is split into train, development, and test partitions",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "containing 41, 14, and 14 videos, respectively. The split is identical",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "All audio files are first normalised to -3 decibels and then converted"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "to the split used in last year‚Äôs challenge. Figure 1 shows the dis-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "from stereo to mono, at 16 kHz, 16 bit. Afterwards, we make use of"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "tributions of the valence and physiological arousal signals for the",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "the two well-established machine learning toolkits openSMILE [23]"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "dataset.",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "and DeepSpectrum [3] for expert-designed and deep feature extrac-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "tion from the audio recordings. Both systems have proved valuable"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2.4\nChallenge Protocol",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "in audio-based Speech Emotion Recognition (SER) tasks [5, 10, 24]."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "All challenge participants are required to complete the End User",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "eGeMAPS. The openSMILE toolkit [23]5 is used for the ex-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "License Agreement (EULA) which is available on the MuSe 2022",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "traction of the extended Geneva Minimalistic Acoustic Parameter"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "homepage2. Further, the participants must hold an academic affili-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "Set (eGeMAPS) [22]. This feature set which is proven valuable for"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ation. Each challenge contribution should be followed by a paper",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "SER tasks [7], also in past MuSe challenges (e. g., [50]), includes 88"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "that describes the applied methods and provides the obtained re-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "acoustic features that can capture affective physiological changes"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "sults. The peer review process is double-blind. To obtain results on",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "in voice production. In MuSe-Humor, we use the default configu-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the test set, participants upload their predictions for unknown test",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "ration to extract the 88 eGeMAPS functionals for each two second"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "labels on CodaLab3. The number of prediction uploads depends",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "audio frame. For the audio of MuSe-Reaction, the 88 eGeMAPS"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "on the sub-challenge: for MuSe-Humor and MuSe-Reaction, up",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "functionals are extracted with a step size of 100 ms and window size"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "to 5 prediction uploads can be submitted, while for MuSe-Stress,",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "of 1 second. Regarding MuSe-Stress, the functionals are obtained"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "up to 20 prediction uploads are allowed. We want to stress that",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "with a 2 Hz rate, using a window size of 5 seconds."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the organisers themselves do not participate as competitors in the",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "challenge.",
          "Lukas Christ et al.": "DeepSpectrum. The principle of DeepSpectrum [3]6 is to"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "utilise pre-trained image Convolutional Neural Networks (CNNs)"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "3\nBASELINE FEATURES AND MODEL",
          "Lukas Christ et al.": "for the extraction of deep features from visual representations (e. g.,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "Mel-spectrograms) of audio signals. The efficacy of DeepSpectrum"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "To enable the participants to get started quickly, we provide a set",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "features has been demonstrated for SER [39], sentiment analysis [2],"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "of features extracted from each sub-challenge‚Äôs video data. More",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "and general audio processing tasks [4]. For our DeepSpectrum"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "precisely, the provided features include of up to five model-ready",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "baseline experiments, we use DenseNet121 [30] pre-trained on"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "video, audio, and linguistic feature sets, depending on the sub-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "ImageNet [40] as the CNN backbone. The audio is represented as"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "challenge4. Regarding the label sampling rate,\nlabels refer to 2 s",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "a Mel-spectrogram with 128 bands employing the viridis colour"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "windows in MuSe-Humor. The MuSe-Stress data is labelled at a",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "mapping. Subsequently, the spectrogram representation is fed into"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2 Hz rate. For MuSe-Reaction, there is one label vector of 7 classes",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "DenseNet121, and the output of the last pooling layer is taken as"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "per sample.",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "a 1 024-dimensional feature vector. The window size is set to one"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "second, the hop-size to 500 ms."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "3.1\nPre-processing",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "All datasets are split into training, development, and test sets. For all",
          "Lukas Christ et al.": "3.3\nVideo"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "partitions, ratings, speaker independence, and duration are taken",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "To extract specific image descriptors related to facial expressions,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "into consideration (cf. Table 1). The videos in Passau-SFCH are cut",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "we make use of two CNN architectures: Multi-task Cascaded Convo-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "to only include segments in which the respective coach is actually",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "lutional Networks (MTCNN) and VGGface 2. We also provide a set"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "speaking. As the press conference setting can be seen as a dia-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "of Facial Action Units (FAUs) obtained from faces of individuals in"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "logue between journalists and the coach, the answers given by each",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "the datasets. Further, participants are also given the set of extracted"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "coach provide a natural segmentation of the Passau-SFCH data.",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "faces from the raw frames. In the videos of MuSe-Humor, typically"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "For MuSe-Reaction ‚Äì as can be seen in Table 1 ‚Äì, a 60-20-20% split",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "more than one face is visible. As this sub-challenge‚Äôs objective is to"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "strategy is applied. There is no additional segmentation applied to",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "predict the expression of humour of the coach, we only provide the"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "clean the data further, each sample contains a single reaction to an",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "faces of the respective coach and the features computed for them."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "emotional stimulus, and labels were normalised per sample to range",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "from [0 :1]. For further exploration, the participants are also pro-",
          "Lukas Christ et al.": "MTCNN. The MTCNN [54] model7, pre-trained on the data-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "vided with voice activity segments from the samples, which show",
          "Lukas Christ et al.": "sets WIDER FACE [53] and CelebA [33], is used to detect faces in"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "to contain audio of substantial energy. In the Ulm-TSST dataset,",
          "Lukas Christ et al.": "the videos. Two steps are carried out to filter extracted faces that"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "we make sure to exclude scenes which are not a part of the TSST",
          "Lukas Christ et al.": "do not show the coach in Passau-SFCH: first, we automatically"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "setting, e. g., the instructor speaking. Moreover, we cut segments in",
          "Lukas Christ et al.": "detect the respective coach‚Äôs faces using FaceNet8 embeddings of"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "which TSST participants reveal their names. The Ulm-TSST dataset",
          "Lukas Christ et al.": "reference pictures showing the coach. The results of this proce-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "is not segmented any further.",
          "Lukas Christ et al.": "dure are then corrected manually. Ulm-TSST,\nin contrast, has a"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "simple, static setting. The camera position is fixed and videos only"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "show the TSST subjects who typically do not move much. Similarly,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2https://www.muse-challenge.org",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "3https://codalab.lisn.upsaclay.fr/",
          "Lukas Christ et al.": "5https://github.com/audeering/opensmile"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "4Note: Participants are free to use other external resources such as features, datasets,",
          "Lukas Christ et al.": "6https://github.com/DeepSpectrum/DeepSpectrum"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "or pretrained networks. The accompanying paper is expected to clearly state and",
          "Lukas Christ et al.": "7https://github.com/ipazc/mtcnn"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "explain the sources and tools used.",
          "Lukas Christ et al.": "8https://github.com/timesler/facenet-pytorch"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe 2022: Baseline Paper": "for MuSe-Reaction, the video is captured from a fixed webcam.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "sentence to the 500 ms frames it corresponds to. If two sentences"
        },
        {
          "MuSe 2022: Baseline Paper": "Hence,\nthe performance of MTCNN is almost flawless for both",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "fall into the same frame, their embeddings are averaged to form the"
        },
        {
          "MuSe 2022: Baseline Paper": "Hume-Reaction and Ulm-TSST. The extracted faces then serve as",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "feature for that frame."
        },
        {
          "MuSe 2022: Baseline Paper": "inputs of the feature extractors VGGface 2 and Py-Feat .",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Regarding MuSe-Reaction, there is no alignment needed with"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "labels, as each file is associated to a single vector of 7 emotional"
        },
        {
          "MuSe 2022: Baseline Paper": "VGGface 2. The purpose of VGGface 2 is to compute gen-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "reaction labels."
        },
        {
          "MuSe 2022: Baseline Paper": "eral\nfacial\nfeatures for the previously extracted faces. VGGface",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "For the MuSe-Stress sub-challenge, we provide label-aligned"
        },
        {
          "MuSe 2022: Baseline Paper": "2 [13] is a dataset for the task of face recognition. It contains 3.3",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "features. Hence, these features exactly align with the labels. We"
        },
        {
          "MuSe 2022: Baseline Paper": "million faces of about 9,000 different persons. As the dataset is orig-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "apply zero-padding to the frames, where the feature type is absent."
        },
        {
          "MuSe 2022: Baseline Paper": "inally intended for supervised facial recognition purposes, models",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Moreover, we downsample the biosignals in Ulm-TSST to 2 Hz, fol-"
        },
        {
          "MuSe 2022: Baseline Paper": "trained on it compute face encodings not directly related to emotion",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "lowed by a smoothing utilising a Savitzky-Golay filter. Participants"
        },
        {
          "MuSe 2022: Baseline Paper": "and sentiment. We use a ResNet50 [29] trained on VGGface 29 and",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "are provided with both the raw signals and the downsampled ones."
        },
        {
          "MuSe 2022: Baseline Paper": "detach its classification layer, resulting in a 512-dimensional feature",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "In both Ulm-TSST and Passau-SFCH, manual\ntranscripts are"
        },
        {
          "MuSe 2022: Baseline Paper": "vector output referred to as VGGface 2 in the following.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "available. However, they lack timestamps. Hence, we reconstruct"
        },
        {
          "MuSe 2022: Baseline Paper": "3.3.3",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "FAU. FAUs as originally proposed by Ekman and Friesen [21],",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "word level\ntimestamps utilising the Montreal\nForced Aligner"
        },
        {
          "MuSe 2022: Baseline Paper": "are closely related to the expression of emotions. Hence, detecting",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "(MFA) [37] tool. Here, we employ the German (Prosodylab) model"
        },
        {
          "MuSe 2022: Baseline Paper": "FAUs is a promising and popular approach to the visual prediction",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "and the German Prosodylab dictionary. The text features are then"
        },
        {
          "MuSe 2022: Baseline Paper": "of affect-related targets (e. g., [35]). We employ Py-Feat 10 to obtain",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "aligned to the 2 Hz label signal by repeating each word embedding"
        },
        {
          "MuSe 2022: Baseline Paper": "predictions for the presence of 20 different FAUs. We do not change",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "throughout the determined interval of the corresponding word. In"
        },
        {
          "MuSe 2022: Baseline Paper": "Py-Feat ‚Äôs default configuration, so that a pre-trained random forest",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "case a 500 ms frame comprises more than one word, we average over"
        },
        {
          "MuSe 2022: Baseline Paper": "model is used to predict the FAUs.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the word embeddings. Zero imputing is applied to parts where sub-"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "jects do not speak. For the sentence embeddings in Passau-SFCH"
        },
        {
          "MuSe 2022: Baseline Paper": "3.4\nLanguage: Bert",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "we choose an analogous approach, repeating and,\nif applicable,"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "averaging the embeddings."
        },
        {
          "MuSe 2022: Baseline Paper": "In recent years, pre-trained Transformer\nlanguage models ac-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "count for state-of-the-art results in numerous Natural Language",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "3.6\nBaseline Model: LSTM-RNN"
        },
        {
          "MuSe 2022: Baseline Paper": "Processing tasks, also in tasks\nrelated to affect\n(e. g.,\n[10]).\nIn",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "general,\nthese models are pretrained in a self-supervised way",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "The sequential nature of the tasks makes recurrent neural networks"
        },
        {
          "MuSe 2022: Baseline Paper": "utilising large amounts of\ntext data. Subsequently,\nthey can be",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "(RNNs) a natural choice for a fairly simple baseline system. More"
        },
        {
          "MuSe 2022: Baseline Paper": "fine-tuned for specific downstream tasks. For the transcripts of",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "specifically, we employ a Long Short-Term Memory (LSTM)-RNN."
        },
        {
          "MuSe 2022: Baseline Paper": "MuSe-Humor and MuSe-Stress, we employ a German version",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Initially, we train a single model on each of the available feature"
        },
        {
          "MuSe 2022: Baseline Paper": "of the BERT (Bidirectional Encoder Representations from Trans-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "sets. Regarding MuSe-Stress, we separately train a model for both"
        },
        {
          "MuSe 2022: Baseline Paper": "formers [19]) model11. No further fine-tuning is applied. For both",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "labels, valence and physiological arousal. We conduct an extensive"
        },
        {
          "MuSe 2022: Baseline Paper": "Passau-SFCH and MuSe-Stress, we extract the BERT token em-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "hyperparameter search for each prediction target and feature. We"
        },
        {
          "MuSe 2022: Baseline Paper": "beddings. Additionally, we obtain 768 dimensional sentence em-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "thus optimise the number of RNN layers,\nthe dimensionality of"
        },
        {
          "MuSe 2022: Baseline Paper": "beddings for all texts in Passau-SFCH by using the encodings of",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the LSTM‚Äôs hidden vectors and the learning rate. Of note, we also"
        },
        {
          "MuSe 2022: Baseline Paper": "BERT ‚Äôs [ùê∂ùêøùëÜ] token. In all cases, we average the embeddings pro-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "experiment with both unidirectional and bidirectional LSTMs. The"
        },
        {
          "MuSe 2022: Baseline Paper": "vided by the last 4 layers of the BERT model, following [47].",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "code as well as the configurations found in the hyperparameter"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "search are available in the baseline GitHub repository12."
        },
        {
          "MuSe 2022: Baseline Paper": "3.5\nAlignment",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Each label\nin MuSe-Humor is predicted based on all\nfeature"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "vectors belonging to the corresponding 2 s window. Hence,\nthe"
        },
        {
          "MuSe 2022: Baseline Paper": "For each task, at least two different modalities are available. Typi-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "sequence length in the MuSe-Humor training process is at most 4"
        },
        {
          "MuSe 2022: Baseline Paper": "cally, sampling rates per modality may differ. We sample the visual",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "steps."
        },
        {
          "MuSe 2022: Baseline Paper": "features with a rate of 2 Hz in all sub-challenges. The only excep-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "In both MuSe-Reaction and MuSe-Stress, we make use of a"
        },
        {
          "MuSe 2022: Baseline Paper": "tion is the FAUs in MuSe-Reaction, which are sampled at a 4 Hz",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "segmentation approach which showed to improve results in previ-"
        },
        {
          "MuSe 2022: Baseline Paper": "rate. Regarding the audio features (DeepSpectrum and eGeMAPS),",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ous works [43, 44, 47]. We find that a segmentation of the training"
        },
        {
          "MuSe 2022: Baseline Paper": "we apply the same frequency in MuSe-Humor and MuSe-Stress,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "data with a window size of 50 s (i. e., 200 steps) and a hop size"
        },
        {
          "MuSe 2022: Baseline Paper": "while eGeMAPS features are obtained using a step size of 100 ms",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "of 25 s (i.e., 100 steps) leads to good results for MuSe-Stress. For"
        },
        {
          "MuSe 2022: Baseline Paper": "in MuSe-Reaction. As VGGish and FAUs are only meaningful if",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "MuSe-Reaction a slightly larger size of 500 steps and a hop size"
        },
        {
          "MuSe 2022: Baseline Paper": "the respective frame actually includes a face, we impute frames",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "of 250, lead to more robust results."
        },
        {
          "MuSe 2022: Baseline Paper": "without a face with zeros.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Following the unimodal experiments, in order to combine differ-"
        },
        {
          "MuSe 2022: Baseline Paper": "For MuSe-Humor,\nthe binary humour label refers to frames",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ent modalities, for MuSe-Humor and MuSe-Stress, we implement"
        },
        {
          "MuSe 2022: Baseline Paper": "of at most 2 seconds length. Hence, each label\nin MuSe-Humor",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "a simple late fusion approach. We apply the exact same training pro-"
        },
        {
          "MuSe 2022: Baseline Paper": "corresponds to at most 4 facial and acoustic feature vectors. 2 Hz",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "cedure as before, now treating the predictions of previously trained"
        },
        {
          "MuSe 2022: Baseline Paper": "sentence embedding vectors are constructed by assigning every",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "unimodal models as input features. In these experiments, we use"
        },
        {
          "MuSe 2022: Baseline Paper": "9https://github.com/WeidiXie/Keras-VGGFace2-ResNet50",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "one configuration per task, without performing a hyperparameter"
        },
        {
          "MuSe 2022: Baseline Paper": "10https://py-feat.org",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "11https://huggingface.co/bert-base-german-cased",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "12https://github.com/EIHW/MuSe2022"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: shows the results for the MuSe-Reaction baseline. As",
      "data": [
        {
          "AUC-Scores over these seeds and the corresponding stan-": "dard deviations.",
          "facial action units are much more dynamic generally, and these": "features model more accurately the emotional expression occurring"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "Interestingly, when we observe the individual class scores, we"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "see that Amusement is consistently performing better than all other"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "Features",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "classes, a finding which is consistent for audio and video features"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "Audio",
          "facial action units are much more dynamic generally, and these": ".148 ùúå, and FAU:"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "eGeMAPS",
          "facial action units are much more dynamic generally, and these": "likely class to contain non-verbal communication e. g.,"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "DeepSpectrum",
          "facial action units are much more dynamic generally, and these": "this performance may be due to the known ease of modelling highly"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "aroused states of emotional expression [49]. However, it may also"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "Video",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "relate to the valence of the emotions as we can see from Figure 2,"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "FAU",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "the Disgust class is the worst performing for FAU."
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "VGGface 2",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "It is worth noting that in this case, the early-fusion of the two"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "Text",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "best-performing feature sets in each modality does not yield any"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "BERT",
          "facial action units are much more dynamic generally, and these": "beneficial results. This holds, although we do consider that through"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "the use of a knowledge-based audio approach, we may see more"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "Late Fusion",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "improvement for audio, which may result in stronger performance"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "A+T",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "A+V",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "T+V",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "MuSe-Stress"
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "A+T+V",
          "facial action units are much more dynamic generally, and these": ""
        },
        {
          "AUC-Scores over these seeds and the corresponding stan-": "",
          "facial action units are much more dynamic generally, and these": "Table 4 reports the results obtained for MuSe-Stress. Consistent"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: shows the results for the MuSe-Reaction baseline. As",
      "data": [
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "search for every possible modality combination in MuSe-Stress.",
          "Lukas Christ et al.": "based on the video modality, but also better than the performance"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "As this approach for late fusion is less suited to a multi-label strat-",
          "Lukas Christ et al.": "of the audio features. We find that the sentence-level BERT fea-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "egy, we apply an early fusion strategy for MuSe-Reaction. For",
          "Lukas Christ et al.": "tures outperform the token-level features. With the simple fusion"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "early fusion, we simply concatenate the best performing feature",
          "Lukas Christ et al.": "of modalities, the performance is not improved. Specifically, the late"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "sets for each modality (audio and video), and then train a new model",
          "Lukas Christ et al.": "fusion approach typically shows worse generalisation to the test"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "with the same hyperparameters from the uni-modal experiments.",
          "Lukas Christ et al.": "data than the unimodal experiments. e. g., there is a discrepancy of"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "The code and configuration for the two fusion methods are also",
          "Lukas Christ et al.": "about .16 between mean AUC on the development (.8219) and test"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "part of the baseline GitHub repository13. Moreover, the repository",
          "Lukas Christ et al.": "(.6633) sets for the combination of audio and video."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "also includes links to the best model weight files in order to ease",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "reproducibility.",
          "Lukas Christ et al.": "4.2\nMuSe-Reaction"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "Table 3 shows the results for the MuSe-Reaction baseline. As"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "4\nEXPERIMENTS AND BASELINE RESULTS",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "expected, the audio results are substantially lower than those from"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "We apply the model described above for every sub-challenge. In",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "the video modality. Of particular note, as it pertains to audio, we see"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "what follows, we discuss the baseline results in more detail.",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "that the emotion-tailored feature-set of eGeMAPS performs poorly,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "almost 0.05 ùúå lower on the development set than the DeepSpectrum"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "4.1\nMuSe-Humor",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "features. Given that there is limited speech in the data set, this may"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "The results for MuSe-Humor are given in Table 2. Each result is",
          "Lukas Christ et al.": "be why the DeepSpectrum features perform better, as due to being"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "obtained from running the LSTM using the specified features with",
          "Lukas Christ et al.": "spectrogram-based, they can potentially capture a more general"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "5 different fixed seeds, consistent with the challenge setting.",
          "Lukas Christ et al.": "acoustic scene and non-speech verbalisations potentially better."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "For the video features, the FAUs are performing much better on"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Table\n2: Results\nfor MuSe-Humor. We\nreport\nthe AUC-",
          "Lukas Christ et al.": "the test set than VGGface 2 (although both are derived from faces),"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Scores for the best among 5 fixed seeds, as well as the mean",
          "Lukas Christ et al.": "given the nature of the data being ‚Äòreactions‚Äô,\nit may be that the"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "AUC-Scores over these seeds and the corresponding stan-",
          "Lukas Christ et al.": "facial action units are much more dynamic generally, and these"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "dard deviations.",
          "Lukas Christ et al.": "features model more accurately the emotional expression occurring"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "within the scene."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "Interestingly, when we observe the individual class scores, we"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[AUC]",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "see that Amusement is consistently performing better than all other"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Features\nDevelopment\nTest",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "classes, a finding which is consistent for audio and video features"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Audio",
          "Lukas Christ et al.": "(eGeMAPS:\n.148 ùúå, and FAU:\n.405 ùúå). As well as being the most"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "eGeMAPS\n.6861 (.6731 ¬± .0172)\n.6952 (.6979 ¬± .0098)",
          "Lukas Christ et al.": "likely class to contain non-verbal communication e. g.,\nlaughter,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "DeepSpectrum\n.7149 (.7100 ¬± .0030)\n.6547 (.6497 ¬± .0102)",
          "Lukas Christ et al.": "this performance may be due to the known ease of modelling highly"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "aroused states of emotional expression [49]. However, it may also"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Video",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "relate to the valence of the emotions as we can see from Figure 2,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "FAU\n.9071 (.9030 ¬± .0028)\n.7960 (.7952 ¬± .0077)",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "the Disgust class is the worst performing for FAU."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "VGGface 2\n.9253 (.9225 ¬± .0024)\n.8480 (.8412 ¬± .0027)",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "It is worth noting that in this case, the early-fusion of the two"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Text",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "best-performing feature sets in each modality does not yield any"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "BERT\n.8270 (.8216 ¬± 0045)\n.7888 (.7905 ¬± 0035)",
          "Lukas Christ et al.": "beneficial results. This holds, although we do consider that through"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "the use of a knowledge-based audio approach, we may see more"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Late Fusion",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "improvement for audio, which may result in stronger performance"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "A+T\n.8901 (.8895 ¬± .0005)\n.7804 (.7843 ¬± .0037)",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "via fusion."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "A+V\n.8252 (.8219 ¬± .0038)\n.6643 (.6633 ¬± .0027)",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "T+V\n.8908 (.8893 ¬± .0015)\n.8232 (.8212 ¬± .0017)",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "4.3\nMuSe-Stress"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "A+T+V\n.9033 (.9026 ¬± .0006)\n.7973 (.7910 ¬± .0057)",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "Table 4 reports the results obtained for MuSe-Stress. Consistent"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "with results reported by some of last year‚Äôs participants [20, 27],"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Evaluating audio and video features for the MuSe-Humor sub-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "the results for MuSe-Stress partly fail to generalise to the test data."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "challenge shows a clear pattern. The video-based features, FAU and",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "With respect to single modality experiments, this observation is"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "VGGish, clearly outperform the audio-based features with VGGish",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "particularly significant for the video features. For example, the best"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "accounting for an AUC of\n.8480 on the test set while eGeMAPS",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "seed for predicting physiological arousal based on Facial Action"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "only achieves .6952 AUC. This comes as no surprise, given that the",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "Units yields a CCC of .5191 on the development set, but only results"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "expression of humour is often accompanied by smile or laughter",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "in a CCC of .0785 on the test partition. The audio feature sets, in"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "and thus recognisable from facial expressions features. A manual",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "comparison, achieve better generalisation with the most extreme"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "inspection of the humorous segments confirms this intuition. Nev-",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "difference between development and test CCC being about .12 (for"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ertheless, audio features are able to detect humour, too. Partly, this",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "eGeMAPS on physiological arousal). Moreover, for both prediction"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "may be due to the presence of laughter. The performance of text",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "targets, the DeepSpectrum audio features perform best among the"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "features (.7888 on the test set) is slightly worse than for the features",
          "Lukas Christ et al.": ""
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "unimodal approaches with CCC values of .4239 and .4931 on the"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "13https://github.com/EIHW/MuSe2022",
          "Lukas Christ et al.": "test sets for physiological arousal and valence, respectively."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "to the job interview setting. Job interviewees typically suppress"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "nervousness in an attempt to give a relaxed, sovereign impression."
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "This might make the detection of arousal from audio and video"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "difficult. The comparably stable performance of textual features for"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "physiological arousal may be due to correlations between partici-"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "pants pausing their speech for a longer time ‚Äì or hardly at all ‚Äì and"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "arousal. We find such correlations to exist for several participants."
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "We also experiment with the downsampled biosignals, motivated"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "by some of last year‚Äôs approaches ([11, 34, 55]) to the task which"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "used these signals as a feature. To do so, we concatenate the three"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "signals (BPM, ECG, and respiratory rate) into a three-dimensional"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "feature vector and normalise them. Here, severe generalisation"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "and stability problems can be observed. To give an example, for"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "arousal, the mean CCC performance of biosignal features on the"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "development set\nis .2793, but\nfor the test set,\nit drops to .1095."
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "What is more, the standard deviations obtained with the biosignal"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "results are consistently higher than those of any other modality."
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "Because of these issues and in order not to inflate the number of"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "experiments, we exclude the physiological modality from the late"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "fusion experiments."
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "While valence prediction could not be improved by late fusion,"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "the late fusion of the audio and text modality accounts for the best"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "result on the test set\nfor physiological arousal prediction (.4761"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "CCC), slightly surpassing the late fusion of audio and text (.4413) as"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "well as DeepSpectrum (.4239). For valence, a generalisation issue"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "for late fusion is apparent. To give an example, the late fusion of"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "acoustic and visual\nfeatures yields by far the best result on the"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "development set (.6914) but only achieves a CCC of .4906 on the"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "test set."
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "5\nCONCLUSIONS"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": ""
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "This baseline paper introduced MuSe 2022 ‚Äì the 3rd Multimodal"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "Sentiment Analysis challenge. MuSe 2022 features three multi-"
        },
        {
          "Figure 2: Confusion matrices for the best (Amusement) and worst (Disgust) performing classes for the best test set configura-": "modal datasets: Passau-SFCH with press conference recordings"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": ""
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": ""
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": ""
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": ""
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "Features"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "Audio"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "eGeMAPS"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "DeepSpectrum"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "Video"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "FAU"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "VGGface 2"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "Text"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "BERT"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "Physiological"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "BPM + ECG + resp."
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "Late Fusion"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "A+T"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "A+V"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "T+V"
        },
        {
          "Table 4: Results for MuSe-Stress. Reported are the CCC values for valence, and physiological arousal. For each feature and": "A+T+V"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T+V": "A+T+V\n.5056 (.4940 ¬± .0070)",
          ".2883": ".4058"
        },
        {
          "T+V": "",
          ".2883": ""
        },
        {
          "T+V": "of football coaches annotated for humour, MuSe-Reaction con-",
          ".2883": ""
        },
        {
          "T+V": "taining emotional reactions to stimuli, and Ulm-TSST consisting of",
          ".2883": ""
        },
        {
          "T+V": "",
          ".2883": ""
        },
        {
          "T+V": "recordings of the stress-inducing TSST. The challenge offers three",
          ".2883": ""
        },
        {
          "T+V": "",
          ".2883": ""
        },
        {
          "T+V": "sub-challenges accounting for a wide range of different prediction",
          ".2883": ""
        },
        {
          "T+V": "",
          ".2883": ""
        },
        {
          "T+V": "targets: i) in MuSe-Humor, humour in press conferences is to be",
          ".2883": ""
        },
        {
          "T+V": "detected; ii) in MuSe-Reaction, the intensities of 7 emotion classes",
          ".2883": ""
        },
        {
          "T+V": "are to be predicted; and iii) MuSe-Stress is a regression task on",
          ".2883": ""
        },
        {
          "T+V": "the levels of continuous valence and arousal values in a stressful",
          ".2883": ""
        },
        {
          "T+V": "situation. Similar to previous iterations ([42, 43]), we employed",
          ".2883": ""
        },
        {
          "T+V": "open-source software to provide participants with an array of ex-",
          ".2883": ""
        },
        {
          "T+V": "tracted features in order to facilitate fast development of novel",
          ".2883": ""
        },
        {
          "T+V": "methods. Based on these features, we set transparent and realistic",
          ".2883": ""
        },
        {
          "T+V": "baseline results. Features, code, and raw data are made publicly",
          ".2883": ""
        },
        {
          "T+V": "available. The official baselines on the test sets are as follows:\n.8480",
          ".2883": ""
        },
        {
          "T+V": "AUC for MuSe-Humor as achieved using VGGface 2 features; a",
          ".2883": ""
        },
        {
          "T+V": "mean ùúå over all classes of .2801 for MuSe-Reaction is obtained",
          ".2883": ""
        },
        {
          "T+V": "utilising FAU, and a CCCs of\n.4761 and .4931 for physiological",
          ".2883": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe 2022: Baseline Paper": "REFERENCES",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Intelligent Tutoring Systems. Springer, Jhongli, Taiwan, 3."
        },
        {
          "MuSe 2022: Baseline Paper": "[1]\nShahin Amiriparian, Lukas Christ, Andreas K√∂nig, Eva-Maria Me√üner, Alan",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[16]\nPeng-Yu Chen and Von-Wun Soo. 2018. Humor recognition using deep learning."
        },
        {
          "MuSe 2022: Baseline Paper": "Cowen, Erik Cambria, and Bj√∂rn W. Schuller. 2022. MuSe 2022 Challenge: Mul-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "In Proceedings of the 2018 conference of the north american chapter of the association"
        },
        {
          "MuSe 2022: Baseline Paper": "timodal Humour, Emotional Reactions, and Stress.\nIn Proceedings of\nthe 30th",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "for computational linguistics: Human language technologies, volume 2 (short papers)."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Association for Computational Linguistics, New Orleans, Louisiana, 113‚Äì117."
        },
        {
          "MuSe 2022: Baseline Paper": "ACM International Conference on Multimedia (MM‚Äô22), October 10-14, 2022, Lisbon,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[17] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and"
        },
        {
          "MuSe 2022: Baseline Paper": "Portugal. Association for Computing Machinery, Lisbon, Portugal. 3 pages, to",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "appear.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Michael Auli. 2020. Unsupervised cross-lingual representation learning for speech"
        },
        {
          "MuSe 2022: Baseline Paper": "[2]\nShahin Amiriparian, Nicholas Cummins, Sandra Ottl, Maurice Gerczuk, and Bj√∂rn",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "recognition.\narXiv:arXiv preprint arXiv:2006.13979"
        },
        {
          "MuSe 2022: Baseline Paper": "Schuller. 2017. Sentiment Analysis Using Image-based Deep Spectrum Features.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[18] Alan S Cowen and Dacher Keltner. 2017. Self-report captures 27 distinct categories"
        },
        {
          "MuSe 2022: Baseline Paper": "In Proceedings 2nd International Workshop on Automatic Sentiment Analysis in",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "of emotion bridged by continuous gradients. Proceedings of the National Academy"
        },
        {
          "MuSe 2022: Baseline Paper": "the Wild (WASA 2017) held in conjunction with the 7th biannual Conference on",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "of Sciences 114, 38 (2017), E7900‚ÄìE7909."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[19]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:"
        },
        {
          "MuSe 2022: Baseline Paper": "Affective Computing and Intelligent\nInteraction (ACII 2017). AAAC,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "Antonio, TX, 26‚Äì29.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Pre-training of Deep Bidirectional Transformers for Language Understanding. In"
        },
        {
          "MuSe 2022: Baseline Paper": "[3]\nShahin Amiriparian, Maurice Gerczuk, Sandra Ottl, Nicholas Cummins, Michael",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Proceedings of the 2019 Conference of the North American Chapter of the Association"
        },
        {
          "MuSe 2022: Baseline Paper": "Freitag, Sergey Pugachevskiy, and Bj√∂rn Schuller. 2017. Snore Sound Classifica-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "for Computational Linguistics: Human Language Technologies. Association for"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Computational Linguistics, Minneapolis, Minnesota, 4171‚Äì4186."
        },
        {
          "MuSe 2022: Baseline Paper": "tion Using Image-based Deep Spectrum Features. In Proceedings INTERSPEECH",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[20] Anh-Quang Duong, Ngoc-Huynh Ho, Hyung-Jeong Yang, Guee-Sang Lee, and"
        },
        {
          "MuSe 2022: Baseline Paper": "2017, 18th Annual Conference of the International Speech Communication Associa-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Soo-Hyung Kim. 2021. Multi-modal Stress Recognition Using Temporal Con-"
        },
        {
          "MuSe 2022: Baseline Paper": "tion. ISCA, ISCA, Stockholm, Sweden, 3512‚Äì3516.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "Shahin Amiriparian, Maurice Gerczuk, Lukas Stappen, Alice Baird, Lukas Koebe,\n[4]",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "volution and Recurrent Network with Positional Embedding.\nIn Proceedings of"
        },
        {
          "MuSe 2022: Baseline Paper": "Sandra Ottl, and Bj√∂rn Schuller. 2020. Towards Cross-Modal Pre-Training and",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "the 2nd on Multimodal Sentiment Analysis Challenge. Association for Computing"
        },
        {
          "MuSe 2022: Baseline Paper": "Learning Tempo-Spatial Characteristics for Audio Recognition with Convolu-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Machinery, New York, NY, USA, 37‚Äì42."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[21]\nPaul Ekman and Wallace V Friesen. 1978. Facial action coding system. Environ-"
        },
        {
          "MuSe 2022: Baseline Paper": "tional and Recurrent Neural Networks. EURASIP Journal on Audio, Speech, and",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "Music Processing 2020, 19 (2020), 1‚Äì11.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "mental Psychology & Nonverbal Behavior (1978)."
        },
        {
          "MuSe 2022: Baseline Paper": "Shahin Amiriparian, Tobias H√ºbner, Vincent Karas, Maurice Gerczuk, Sandra\n[5]",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[22]\nFlorian Eyben, Klaus R Scherer, Bj√∂rn W Schuller, Johan Sundberg, Elisabeth"
        },
        {
          "MuSe 2022: Baseline Paper": "Ottl, and Bj√∂rn W. Schuller. 2022. DeepSpectrumLite: A Power-Efficient Trans-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Andr√©, Carlos Busso, Laurence Y Devillers, Julien Epps, Petri Laukka, Shrikanth S"
        },
        {
          "MuSe 2022: Baseline Paper": "fer Learning Framework for Embedded Speech and Audio Processing From",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Narayanan, et al. 2015. The Geneva minimalistic acoustic parameter set (GeMAPS)"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "for voice research and affective computing.\nIEEE Transactions on Affective Com-"
        },
        {
          "MuSe 2022: Baseline Paper": "Frontiers\nin Artificial\nIntelligence 5 (2022), 10.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "//doi.org/10.3389/frai.2022.856232",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "puting 7, 2 (2015), 190‚Äì202."
        },
        {
          "MuSe 2022: Baseline Paper": "[6] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[23]\nFlorian Eyben, Martin W√∂llmer, and Bj√∂rn Schuller. 2010. Opensmile: the mu-"
        },
        {
          "MuSe 2022: Baseline Paper": "L. Saunders, F. M. Tyers, and G. Weber. 2020. Common Voice: A Massively-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "nich versatile and fast open-source audio feature extractor. In Proceedings of the"
        },
        {
          "MuSe 2022: Baseline Paper": "Multilingual Speech Corpus. In Proceedings of the 12th Conference on Language",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "18th ACM International Conference on Multimedia. Association for Computing"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Machinery, Firenze, Italy, 1459‚Äì1462."
        },
        {
          "MuSe 2022: Baseline Paper": "Resources and Evaluation (LREC 2020). European Language Resources Association",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "(ELRA), Marseille, France, 4211‚Äì4215.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[24] Maurice Gerczuk, Shahin Amiriparian, Sandra Ottl, and Bj√∂rn Schuller. 2022."
        },
        {
          "MuSe 2022: Baseline Paper": "[7] Alice Baird, Shahin Amiriparian, and Bj√∂rn Schuller. 2019. Can deep generative",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion"
        },
        {
          "MuSe 2022: Baseline Paper": "audio be emotional? Towards an approach for personalised emotional audio gen-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Recognition.\nIEEE Transactions on Affective Computing 13 (2022)."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[25]\nPanagiotis Gkorezis, Eugenia Petridou, and Panteleimon Xanthiakos. 2014. Leader"
        },
        {
          "MuSe 2022: Baseline Paper": "eration. In 2019 IEEE 21st International Workshop on Multimedia Signal Processing",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "(MMSP). IEEE, IEEE, Kuala Lumpur, Malaysia, 1‚Äì5.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "positive humor and organizational cynicism: LMX as a mediator. Leadership &"
        },
        {
          "MuSe 2022: Baseline Paper": "[8] Alice Baird, Lukas Stappen, Lukas Christ, Lea Schumann, Eva-Maria Me√üner, and",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Organization Development Journal 35 (2014), 305 ‚Äì 315."
        },
        {
          "MuSe 2022: Baseline Paper": "Bj√∂rn W Schuller. 2021. A Physiologically-adapted Gold Standard for Arousal",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[26] Michael Grimm and Kristian Kroschel. 2005. Evaluation of natural emotions using"
        },
        {
          "MuSe 2022: Baseline Paper": "During a Stress Induced Scenario. In Proceedings of the 2nd Multimodal Sentiment",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "self assessment manikins. In IEEE Workshop on Automatic Speech Recognition and"
        },
        {
          "MuSe 2022: Baseline Paper": "Analysis Challenge, co-located with the 29th ACM International Conference on",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Understanding, 2005. IEEE, IEEE, Canc√∫n, Mexico, 381‚Äì385."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[27]\nSalam Hamieh, Vincent Heiries, Hussein Al Osman, and Christelle Godin."
        },
        {
          "MuSe 2022: Baseline Paper": "Multimedia (ACMMM). ACM, Association for Computing Machinery, Changu,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "China, 69‚Äì73.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2021. Multi-modal Fusion for Continuous Emotion Recognition by Using Auto-"
        },
        {
          "MuSe 2022: Baseline Paper": "[9] Alice Baird, Andreas Triantafyllopoulos, Sandra Z√§nkert, Sandra Ottl, Lukas",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Encoders.\nIn Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge."
        },
        {
          "MuSe 2022: Baseline Paper": "Christ, Lukas Stappen, Julian Konzok, Sarah Sturmbauer, Eva-Maria Me√üner,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Association for Computing Machinery, New York, NY, USA, 21‚Äì27."
        },
        {
          "MuSe 2022: Baseline Paper": "Brigitte M. Kudielka, Nicolas Rohleder, Harald Baumeister, and Bj√∂rn W. Schuller.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[28] Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar"
        },
        {
          "MuSe 2022: Baseline Paper": "2021. An Evaluation of Speech-Based Recognition of Emotional and Physiological",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Tanveer, Louis-Philippe Morency, et al. 2019. Ur-funny: A multimodal language"
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "dataset for understanding humor.\narXiv:arXiv preprint arXiv:1904.06618"
        },
        {
          "MuSe 2022: Baseline Paper": "Markers of Stress. Frontiers in Computer Science 3 (2021), 19.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "3389/fcomp.2021.750284",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual"
        },
        {
          "MuSe 2022: Baseline Paper": "[10] Bj√∂rn W. Schuller and Anton Batliner and Christian Bergler and Cecilia Mas-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "learning for image recognition. In Proceedings of the IEEE conference on computer"
        },
        {
          "MuSe 2022: Baseline Paper": "colo and Jing Han and Iulia Lefter and Heysem Kaya and Shahin Amiriparian",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "vision and pattern recognition. IEEE, Las Vegas, Nevada, 770‚Äì778."
        },
        {
          "MuSe 2022: Baseline Paper": "and Alice Baird and Lukas Stappen and Sandra Ottl and Maurice Gerczuk and",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[30] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger."
        },
        {
          "MuSe 2022: Baseline Paper": "Panaguiotis Tzirakis and Chlo√´ Brown and Jagmohan Chauhan and Andreas",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2017. Densely connected convolutional networks.\nIn Proceedings of the IEEE"
        },
        {
          "MuSe 2022: Baseline Paper": "Grammenos and Apinan Hasthanasombat and Dimitris Spathis and Tong Xia",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "conference on computer vision and pattern recognition. IEEE, Honolulu, Hawaii,"
        },
        {
          "MuSe 2022: Baseline Paper": "and Pietro Cicuta and Leon J. M. Rothkrantz and Joeri Zwerts and Jelle Treep",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "4700‚Äì4708."
        },
        {
          "MuSe 2022: Baseline Paper": "and Casper Kaandorp. 2021. The INTERSPEECH 2021 Computational Paralin-",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[31] Clemens Kirschbaum, Karl-Martin Pirke, and Dirk H Hellhammer. 1993. The"
        },
        {
          "MuSe 2022: Baseline Paper": "guistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "‚ÄòTrier Social Stress Test‚Äô‚Äìa tool for investigating psychobiological stress responses"
        },
        {
          "MuSe 2022: Baseline Paper": "In Proceedings INTERSPEECH 2021, 22nd Annual Conference of the International",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "in a laboratory setting. Neuropsychobiology 28, 1-2 (1993), 76‚Äì81."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[32] T Senthil Kumar and T Senthil. 2021. Construction of hybrid deep learning model"
        },
        {
          "MuSe 2022: Baseline Paper": "Speech Communication Association. ISCA, ISCA, Brno, Czechia, 431‚Äì435.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "[11] Cong Cai, Yu He, Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao, Mingyu Xu, and",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "for predicting children behavior based on their emotional reaction. Journal of"
        },
        {
          "MuSe 2022: Baseline Paper": "Kexin Wang. 2021. Multimodal Sentiment Analysis based on Recurrent Neural",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Information Technology 3, 01 (2021), 29‚Äì43."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face"
        },
        {
          "MuSe 2022: Baseline Paper": "In Proceedings of the 2nd on Multimodal",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "Sentiment Analysis Challenge. Association for Computing Machinery, New York,",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Attributes in the Wild. In Proceedings of International Conference on Computer"
        },
        {
          "MuSe 2022: Baseline Paper": "NY, USA, 61‚Äì67.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Vision (ICCV). IEEE, Santiago, Chile, 3730 ‚Äì 3738."
        },
        {
          "MuSe 2022: Baseline Paper": "[12] Yekta Said Can, Bert Arnrich, and Cem Ersoy. 2019.\nStress detection in daily",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[34] Ziyu Ma, Fuyan Ma, Bin Sun, and Shutao Li. 2021. Hybrid Mutimodal Fusion"
        },
        {
          "MuSe 2022: Baseline Paper": "life scenarios using smart phones and wearable sensors: A survey.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "for Dimensional Emotion Recognition.\nIn Proceedings of the 2nd on Multimodal"
        },
        {
          "MuSe 2022: Baseline Paper": "Journal of",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "Biomedical Informatics 92 (2019), 22.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Sentiment Analysis Challenge. Association for Computing Machinery, New York,"
        },
        {
          "MuSe 2022: Baseline Paper": "[13] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. 2018.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "NY, USA, 29‚Äì36."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[35] Adria Mallol-Ragolta, Nicholas Cummins, and Bj√∂rn W Schuller. 2020. An In-"
        },
        {
          "MuSe 2022: Baseline Paper": "VGGFace2: A Dataset for Recognising Faces across Pose and Age. In 2018 13th",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "vestigation of Cross-Cultural Semi-Supervised Learning for Continuous Affect"
        },
        {
          "MuSe 2022: Baseline Paper": "IEEE International Conference on Automatic Face Gesture Recognition (FG 2018).",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "IEEE, New York, NY, 67‚Äì74.\nhttps://doi.org/10.1109/FG.2018.00020",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Recognition.. In INTERSPEECH. International Speech Communication Associa-"
        },
        {
          "MuSe 2022: Baseline Paper": "[14] Delphine Caruelle, Anders Gustafsson, Poja Shams, and Line Lervik-Olsen. 2019.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "tion (ISCA), Shanghai, China, 511‚Äì515."
        },
        {
          "MuSe 2022: Baseline Paper": "The use of electrodermal activity (EDA) measurement to understand consumer",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[36] Rod A Martin, Patricia Puhlik-Doris, Gwen Larsen, Jeanette Gray, and Kelly Weir."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2003.\nIndividual differences in uses of humor and their relation to psychological"
        },
        {
          "MuSe 2022: Baseline Paper": "emotions‚Äìa literature review and a call for action. Journal of Business Research",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "104 (2019), 146‚Äì160.",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "well-being: Development of the Humor Styles Questionnaire. Journal of research"
        },
        {
          "MuSe 2022: Baseline Paper": "[15]\nPierre Chalfoun, Soumaya Chaffar, and Claude Frasson. 2006. Predicting the",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "in personality 37, 1 (2003), 48‚Äì75."
        },
        {
          "MuSe 2022: Baseline Paper": "",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[37] Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan"
        },
        {
          "MuSe 2022: Baseline Paper": "emotional reaction of the learner with a machine learning technique. In Workshop",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": ""
        },
        {
          "MuSe 2022: Baseline Paper": "on Motivaional and Affective Issues in ITS,\nITS‚Äô06,\nInternational Conference on",
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Sonderegger. 2017. Montreal Forced Aligner: Trainable Text-Speech Alignment"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Using Kaldi.. In Proceedings of INTERSPEECH, Vol. 2017. International Speech",
          "Lukas Christ et al.": "[46]\nJennifer J Sun, Ting Liu, Alan S Cowen, Florian Schroff, Hartwig Adam, and"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Communication Association (ISCA), Stockholm, Sweden, 498‚Äì502.",
          "Lukas Christ et al.": "Gautam Prasad. 2020. EEV: A large-scale dataset for studying evoked expressions"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[38] Anirudh Mittal, Pranav Jeevan, Prerak Gandhi, Diptesh Kanojia, and Pushpak",
          "Lukas Christ et al.": "from video.\narXiv:arXiv preprint arXiv:2001.05488"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Bhattacharyya. 2021. \" So You Think You‚Äôre Funny?\": Rating the Humour Quotient",
          "Lukas Christ et al.": "Licai Sun, Zheng Lian, Jianhua Tao, Bin Liu, and Mingyue Niu. 2020. Multi-\n[47]"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "in Standup Comedy.\narXiv:arXiv preprint arXiv:2110.12765",
          "Lukas Christ et al.": "modal Continuous Dimensional Emotion Recognition Using Recurrent Neural"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[39]\nSandra Ottl, Shahin Amiriparian, Maurice Gerczuk, Vincent Karas, and Bj√∂rn",
          "Lukas Christ et al.": "Network and Self-Attention Mechanism. In Proceedings of the 1st International"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Schuller. 2020. Group-level Speech Emotion Recognition Utilising Deep Spectrum",
          "Lukas Christ et al.": "on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Features. In Proceedings of the 8th ICMI 2020 EmotiW ‚Äì Emotion Recognition In The",
          "Lukas Christ et al.": "Association for Computing Machinery, New York, NY, USA, 27‚Äì34."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Wild Challenge (EmotiW 2020), 22nd ACM International Conference on Multimodal",
          "Lukas Christ et al.": "[48] Ron Tamborini, James Stiff, and Carl Heidel. 1990. Reacting to graphic horror: A"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Interaction (ICMI 2020). ACM, ACM, Utrecht, The Netherlands, 821‚Äì826.",
          "Lukas Christ et al.": "model of empathy and emotional behavior. Communication Research 17, 5 (1990),"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean",
          "Lukas Christ et al.": "616‚Äì640."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.",
          "Lukas Christ et al.": "[49]\nPanagiotis Tzirakis, Stefanos Zafeiriou, and Bjorn W Schuller. 2018. End2You‚ÄìThe"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "2015.\nImagenet large scale visual recognition challenge.\nInternational journal of",
          "Lukas Christ et al.": "Imperial Toolkit for Multimodal Profiling by End-to-End Learning.\narXiv:arXiv"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "computer vision 115, 3 (2015), 211‚Äì252.",
          "Lukas Christ et al.": "preprint arXiv:1802.01115"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[41] G Skoraczy≈Ñski, P Dittwald, B Miasojedow, S Szymkuƒá, EP Gajewska, Bartosz A",
          "Lukas Christ et al.": "[50] Bogdan Vlasenko, RaviShankar Prasad, and Mathew Magimai.-Doss. 2021. Fusion"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Grzybowski, and A Gambin. 2017. Predicting the outcomes of organic reactions",
          "Lukas Christ et al.": "of Acoustic and Linguistic Information using Supervised Autoencoder for Im-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "via machine learning: are current descriptors sufficient? Scientific reports 7, 1",
          "Lukas Christ et al.": "proved Emotion Recognition.\nIn Proceedings of the 2nd on Multimodal Sentiment"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "(2017), 1‚Äì9.",
          "Lukas Christ et al.": "Analysis Challenge. Association for Computing Machinery, New York, NY, USA,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[42]\nLukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin Sertolli,",
          "Lukas Christ et al.": "51‚Äì59."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Eva-Maria Messner, Erik Cambria, Guoying Zhao, and Bj√∂rn W Schuller. 2021.",
          "Lukas Christ et al.": "[51]\nJiaming Wu, Hongfei Lin, Liang Yang, and Bo Xu. 2021. MUMOR: A Multimodal"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "The MuSe 2021 multimodal sentiment analysis challenge: sentiment, emotion,",
          "Lukas Christ et al.": "Dataset for Humor Detection in Conversations. In CCF International Conference on"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "physiological-emotion, and stress.\nIn Proceedings of\nthe 2nd on Multimodal",
          "Lukas Christ et al.": "Natural Language Processing and Chinese Computing. Springer, Springer, Qingdao,"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Sentiment Analysis Challenge. Association for Computing Machinery, New York,",
          "Lukas Christ et al.": "China, 619‚Äì627."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "NY, USA, 5‚Äì14.",
          "Lukas Christ et al.": "[52] Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. 2015. Humor recognition"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[43]\nLukas Stappen, Alice Baird, Georgios Rizos, Panagiotis Tzirakis, Xinchen Du,",
          "Lukas Christ et al.": "and humor anchor extraction. In Proceedings of the 2015 conference on empirical"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Felix Hafner, Lea Schumann, Adria Mallol-Ragolta, Bjoern W. Schuller,\nIulia",
          "Lukas Christ et al.": "methods in natural language processing. Association for Computational Linguis-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Lefter, Erik Cambria, and Ioannis Kompatsiaris. 2020. MuSe 2020 Challenge",
          "Lukas Christ et al.": "tics, Lisbon, Portugal, 2367‚Äì2376."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "and Workshop: Multimodal Sentiment Analysis, Emotion-Target Engagement",
          "Lukas Christ et al.": "Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2016. WIDER FACE:\n[53]"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "and Trustworthiness Detection in Real-Life Media.\nIn Proceedings of\nthe 1st",
          "Lukas Christ et al.": "A Face Detection Benchmark. In IEEE Conference on Computer Vision and Pattern"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "International on Multimodal Sentiment Analysis in Real-Life Media Challenge and",
          "Lukas Christ et al.": "Recognition (CVPR). IEEE, Las Vegas, NV, USA, 9."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Workshop. ACM, Association for Computing Machinery, New York, NY, USA,",
          "Lukas Christ et al.": "[54] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016.\nJoint Face"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "35‚Äì44.",
          "Lukas Christ et al.": "Detection and Alignment Using Multitask Cascaded Convolutional Networks."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "[44]\nLukas Stappen, Alice Baird, Lea Schumann, and Bj√∂rn Schuller. 2021. The Mul-",
          "Lukas Christ et al.": "IEEE Signal Processing Letters 23 (04 2016)."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "timodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection,",
          "Lukas Christ et al.": "[55] Tenggan Zhang, Zhaopei Huang, Ruichen Li, Jinming Zhao, and Qin Jin. 2021."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Insights and Improvements.\nIEEE Transactions on Affective Computing (Early",
          "Lukas Christ et al.": "Multimodal Fusion Strategies for Physiological-emotion Analysis.\nIn Proceedings"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "https://doi.org/10.1109/TAFFC.2021.3097002\nAccess) (June 2021).",
          "Lukas Christ et al.": "of the 2nd on Multimodal Sentiment Analysis Challenge. Association for Computing"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Lukas Stappen, Lea Schumann, Benjamin Sertolli, Alice Baird, Benjamin Weigel,\n[45]",
          "Lukas Christ et al.": "Machinery, New York, NY, USA, 43‚Äì50."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Erik Cambria, and Bj√∂rn W Schuller. 2021. MuSe-Toolbox: The Multimodal Senti-",
          "Lukas Christ et al.": "Feng Zhou and Fernando De la Torre. 2015. Generalized canonical time warping.\n[56]"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ment Analysis Continuous Annotation Fusion and Discrete Class Transformation",
          "Lukas Christ et al.": "IEEE Transactions on Pattern Analysis and Machine Intelligence 38, 2 (2015), 279‚Äì"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "Toolbox. In Proceedings of the 2nd Multimodal Sentiment Analysis Challenge, co-",
          "Lukas Christ et al.": "294."
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "located with the 29th ACM International Conference on Multimedia (ACMMM).",
          "Lukas Christ et al.": "[57] Dov Zohar, Orna Tzischinsky, Rachel Epstein, and Peretz Lavie. 2005. The effects"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "ACM, Association for Computing Machinery, Changu, China, 75‚Äì82.",
          "Lukas Christ et al.": "of sleep loss on medical residents‚Äô emotional reactions to work events: a cognitive-"
        },
        {
          "MuSe‚Äô 22, October 10, 2022, Lisboa, Portugal": "",
          "Lukas Christ et al.": "energy model. Sleep 28, 1 (2005), 47‚Äì54."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "MuSe 2022 Challenge: Multimodal Humour, Emotional Reactions, and Stress",
      "authors": [
        "Shahin Amiriparian",
        "Lukas Christ",
        "Andreas K√∂nig",
        "Eva-Maria Me√üner",
        "Alan Cowen",
        "Erik Cambria",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia (MM'22)"
    },
    {
      "citation_id": "2",
      "title": "Sentiment Analysis Using Image-based Deep Spectrum Features",
      "authors": [
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Sandra Ottl",
        "Maurice Gerczuk",
        "Bj√∂rn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings 2nd International Workshop on Automatic Sentiment Analysis in the Wild (WASA 2017) held in conjunction with the 7th biannual Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "3",
      "title": "Snore Sound Classification Using Image-based Deep Spectrum Features",
      "authors": [
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Sandra Ottl",
        "Nicholas Cummins",
        "Michael Freitag",
        "Sergey Pugachevskiy",
        "Bj√∂rn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings INTERSPEECH 2017, 18th Annual Conference of the International Speech Communication Association. ISCA, ISCA"
    },
    {
      "citation_id": "4",
      "title": "Towards Cross-Modal Pre-Training and Learning Tempo-Spatial Characteristics for Audio Recognition with Convolutional and Recurrent Neural Networks",
      "authors": [
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Koebe",
        "Sandra Ottl",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "5",
      "title": "DeepSpectrumLite: A Power-Efficient Transfer Learning Framework for Embedded Speech and Audio Processing From Decentralized Data",
      "authors": [
        "Shahin Amiriparian",
        "Tobias H√ºbner",
        "Vincent Karas",
        "Maurice Gerczuk",
        "Sandra Ottl",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "Frontiers in Artificial Intelligence",
      "doi": "10.3389/frai.2022.856232"
    },
    {
      "citation_id": "6",
      "title": "Common Voice: A Massively-Multilingual Speech Corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "7",
      "title": "Can deep generative audio be emotional? Towards an approach for personalised emotional audio generation",
      "authors": [
        "Alice Baird",
        "Shahin Amiriparian",
        "Bj√∂rn Schuller"
      ],
      "year": "2019",
      "venue": "2019 IEEE 21st International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "8",
      "title": "A Physiologically-adapted Gold Standard for Arousal During a Stress Induced Scenario",
      "authors": [
        "Alice Baird",
        "Lukas Stappen",
        "Lukas Christ",
        "Lea Schumann",
        "Eva-Maria Me√üner",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd Multimodal Sentiment Analysis Challenge, co-located with the 29th ACM International Conference on Multimedia (ACMMM)"
    },
    {
      "citation_id": "9",
      "title": "An Evaluation of Speech-Based Recognition of Emotional and Physiological Markers of Stress",
      "authors": [
        "Alice Baird",
        "Andreas Triantafyllopoulos",
        "Sandra Z√§nkert",
        "Sandra Ottl",
        "Lukas Christ",
        "Lukas Stappen",
        "Julian Konzok",
        "Sarah Sturmbauer",
        "Eva-Maria Me√üner",
        "Brigitte Kudielka",
        "Nicolas Rohleder",
        "Harald Baumeister",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Frontiers in Computer Science",
      "doi": "10.3389/fcomp.2021.750284"
    },
    {
      "citation_id": "10",
      "title": "The INTERSPEECH 2021 Computational Paralinguistics Challenge: COVID-19 Cough, COVID-19 Speech, Escalation & Primates",
      "authors": [
        "Bj√∂rn Schuller",
        "Anton Batliner",
        "Christian Bergler",
        "Cecilia Mascolo",
        "Jing Han",
        "Iulia Lefter",
        "Heysem Kaya",
        "Shahin Amiriparian",
        "Alice Baird",
        "Lukas Stappen",
        "Sandra Ottl",
        "Maurice Gerczuk",
        "Panaguiotis Tzirakis",
        "Chlo√´ Brown",
        "Jagmohan Chauhan",
        "Andreas Grammenos",
        "Apinan Hasthanasombat",
        "Dimitris Spathis",
        "Tong Xia",
        "Pietro Cicuta",
        "Leon Rothkrantz",
        "Joeri Zwerts",
        "Jelle Treep",
        "Casper Kaandorp"
      ],
      "year": "2021",
      "venue": "Proceedings INTERSPEECH 2021, 22nd Annual Conference of the International Speech Communication Association. ISCA, ISCA"
    },
    {
      "citation_id": "11",
      "title": "Multimodal Sentiment Analysis based on Recurrent Neural Network and Multimodal Attention",
      "authors": [
        "Cong Cai",
        "Yu He",
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao",
        "Mingyu Xu",
        "Kexin Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "12",
      "title": "Stress detection in daily life scenarios using smart phones and wearable sensors: A survey",
      "authors": [
        "Yekta Said Can",
        "Bert Arnrich",
        "Cem Ersoy"
      ],
      "year": "2019",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "13",
      "title": "VGGFace2: A Dataset for Recognising Faces across Pose and Age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "M Omkar",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face Gesture Recognition",
      "doi": "10.1109/FG.2018.00020"
    },
    {
      "citation_id": "14",
      "title": "The use of electrodermal activity (EDA) measurement to understand consumer emotions-a literature review and a call for action",
      "authors": [
        "Delphine Caruelle",
        "Anders Gustafsson",
        "Poja Shams",
        "Line Lervik-Olsen"
      ],
      "year": "2019",
      "venue": "Journal of Business Research"
    },
    {
      "citation_id": "15",
      "title": "Predicting the emotional reaction of the learner with a machine learning technique",
      "authors": [
        "Pierre Chalfoun",
        "Soumaya Chaffar",
        "Claude Frasson"
      ],
      "year": "2006",
      "venue": "Workshop on Motivaional and Affective Issues in ITS, ITS'06, International Conference on Intelligent Tutoring Systems"
    },
    {
      "citation_id": "16",
      "title": "Humor recognition using deep learning",
      "authors": [
        "Peng-Yu Chen",
        "Von-Wun Soo"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference of the north american chapter of the association for computational linguistics: Human language technologies"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "Alexis Conneau",
        "Alexei Baevski",
        "Ronan Collobert",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition",
      "arxiv": "arXiv:arXivpreprintarXiv:2006.13979"
    },
    {
      "citation_id": "18",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "19",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal Stress Recognition Using Temporal Convolution and Recurrent Network with Positional Embedding",
      "authors": [
        "Anh-Quang Duong",
        "Ngoc-Huynh Ho",
        "Hyung-Jeong Yang",
        "Guee-Sang Lee",
        "Soo-Hyung Kim"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "21",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "22",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Bj√∂rn Schuller",
        "Johan Sundberg",
        "Elisabeth Andr√©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition",
      "authors": [
        "Maurice Gerczuk",
        "Shahin Amiriparian",
        "Sandra Ottl",
        "Bj√∂rn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Leader positive humor and organizational cynicism: LMX as a mediator",
      "authors": [
        "Panagiotis Gkorezis",
        "Eugenia Petridou",
        "Panteleimon Xanthiakos"
      ],
      "year": "2014",
      "venue": "Leadership & Organization Development Journal"
    },
    {
      "citation_id": "26",
      "title": "Evaluation of natural emotions using self assessment manikins",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel"
      ],
      "year": "2005",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "27",
      "title": "Multi-modal Fusion for Continuous Emotion Recognition by Using Auto-Encoders",
      "authors": [
        "Salam Hamieh",
        "Vincent Heiries",
        "Hussein Al Osman",
        "Christelle Godin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "28",
      "title": "A multimodal language dataset for understanding humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Jianyuan Zhong",
        "Md Iftekhar Tanveer",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "A multimodal language dataset for understanding humor",
      "arxiv": "arXiv:arXivpreprintarXiv:1904.06618"
    },
    {
      "citation_id": "29",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "The 'Trier Social Stress Test'-a tool for investigating psychobiological stress responses in a laboratory setting",
      "authors": [
        "Clemens Kirschbaum",
        "Karl-Martin Pirke",
        "Dirk Hellhammer"
      ],
      "year": "1993",
      "venue": "Neuropsychobiology"
    },
    {
      "citation_id": "32",
      "title": "Construction of hybrid deep learning model for predicting children behavior based on their emotional reaction",
      "authors": [
        "Senthil Kumar",
        "T Senthil"
      ],
      "year": "2021",
      "venue": "Journal of Information Technology"
    },
    {
      "citation_id": "33",
      "title": "Deep Learning Face Attributes in the Wild",
      "authors": [
        "Ziwei Liu",
        "Ping Luo"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "34",
      "title": "Hybrid Mutimodal Fusion for Dimensional Emotion Recognition",
      "authors": [
        "Ziyu Ma",
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "35",
      "title": "An Investigation of Cross-Cultural Semi-Supervised Learning for Continuous Affect Recognition",
      "authors": [
        "Adria Mallol-Ragolta",
        "Nicholas Cummins",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "INTERSPEECH. International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "36",
      "title": "Individual differences in uses of humor and their relation to psychological well-being: Development of the Humor Styles Questionnaire",
      "authors": [
        "Rod Martin",
        "Patricia Puhlik-Doris",
        "Gwen Larsen",
        "Jeanette Gray",
        "Kelly Weir"
      ],
      "year": "2003",
      "venue": "Journal of research in personality"
    },
    {
      "citation_id": "37",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment",
      "authors": [
        "Michael Mcauliffe",
        "Michaela Socolof",
        "Sarah Mihuc",
        "Michael Wagner",
        "Morgan Sonderegger"
      ],
      "year": "2017",
      "venue": "Montreal Forced Aligner: Trainable Text-Speech Alignment"
    },
    {
      "citation_id": "38",
      "title": "International Speech Communication Association (ISCA)",
      "authors": [
        "Using Kaldi"
      ],
      "year": "2017",
      "venue": "International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "39",
      "title": "Diptesh Kanojia, and Pushpak Bhattacharyya. 2021",
      "authors": [
        "Anirudh Mittal",
        "Pranav Jeevan",
        "Prerak Gandhi"
      ],
      "venue": "Rating the Humour Quotient in Standup Comedy",
      "arxiv": "arXiv:arXivpreprintarXiv:2110.12765"
    },
    {
      "citation_id": "40",
      "title": "Group-level Speech Emotion Recognition Utilising Deep Spectrum Features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 8th ICMI 2020 EmotiW -Emotion Recognition In The Wild Challenge"
    },
    {
      "citation_id": "41",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "Olga Russakovsky",
        "Jia Deng",
        "Hao Su",
        "Jonathan Krause",
        "Sanjeev Satheesh",
        "Sean Ma",
        "Zhiheng Huang",
        "Andrej Karpathy",
        "Aditya Khosla",
        "Michael Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "42",
      "title": "Predicting the outcomes of organic reactions via machine learning: are current descriptors sufficient?",
      "authors": [
        "Skoraczy≈Ñski",
        "B Dittwald",
        "S Miasojedow",
        "Szymkuƒá",
        "Bartosz Gajewska",
        "Grzybowski",
        "Gambin"
      ],
      "year": "2017",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "43",
      "title": "The MuSe 2021 multimodal sentiment analysis challenge: sentiment, emotion, physiological-emotion, and stress",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Christ",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Eva-Maria Messner",
        "Erik Cambria",
        "Guoying Zhao",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "44",
      "title": "MuSe 2020 Challenge and Workshop: Multimodal Sentiment Analysis, Emotion-Target Engagement and Trustworthiness Detection in Real-Life Media",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Georgios Rizos",
        "Panagiotis Tzirakis",
        "Xinchen Du",
        "Felix Hafner",
        "Lea Schumann",
        "Adria Mallol-Ragolta",
        "Bjoern Schuller",
        "Iulia Lefter"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-Life Media Challenge and Workshop"
    },
    {
      "citation_id": "45",
      "title": "The Multimodal Sentiment Analysis in Car Reviews (MuSe-CaR) Dataset: Collection, Insights and Improvements",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lea Schumann",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing (Early Access)",
      "doi": "10.1109/TAFFC.2021.3097002"
    },
    {
      "citation_id": "46",
      "title": "MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox",
      "authors": [
        "Lukas Stappen",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Alice Baird",
        "Benjamin Weigel",
        "Erik Cambria",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd Multimodal Sentiment Analysis Challenge, colocated with the 29th ACM International Conference on Multimedia (ACMMM)"
    },
    {
      "citation_id": "47",
      "title": "EEV: A large-scale dataset for studying evoked expressions from video",
      "authors": [
        "Jennifer J Sun",
        "Ting Liu",
        "Alan Cowen",
        "Florian Schroff",
        "Hartwig Adam",
        "Gautam Prasad"
      ],
      "year": "2020",
      "venue": "EEV: A large-scale dataset for studying evoked expressions from video",
      "arxiv": "arXiv:arXivpreprintarXiv:2001.05488"
    },
    {
      "citation_id": "48",
      "title": "Multimodal Continuous Dimensional Emotion Recognition Using Recurrent Neural Network and Self-Attention Mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "49",
      "title": "Reacting to graphic horror: A model of empathy and emotional behavior",
      "authors": [
        "Ron Tamborini",
        "James Stiff",
        "Carl Heidel"
      ],
      "year": "1990",
      "venue": "Communication Research"
    },
    {
      "citation_id": "50",
      "title": "End2You-The Imperial Toolkit for Multimodal Profiling by End-to-End Learning",
      "authors": [
        "Panagiotis Tzirakis",
        "Stefanos Zafeiriou",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "End2You-The Imperial Toolkit for Multimodal Profiling by End-to-End Learning",
      "arxiv": "arXiv:arXivpreprintarXiv:1802.01115"
    },
    {
      "citation_id": "51",
      "title": "Fusion of Acoustic and Linguistic Information using Supervised Autoencoder for Improved Emotion Recognition",
      "authors": [
        "Bogdan Vlasenko",
        "Ravishankar Prasad",
        "Mathew Magimai",
        ". Doss"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "52",
      "title": "MUMOR: A Multimodal Dataset for Humor Detection in Conversations",
      "authors": [
        "Jiaming Wu",
        "Hongfei Lin",
        "Liang Yang",
        "Bo Xu"
      ],
      "year": "2021",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "53",
      "title": "Humor recognition and humor anchor extraction",
      "authors": [
        "Diyi Yang",
        "Alon Lavie",
        "Chris Dyer",
        "Eduard Hovy"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing. Association for Computational Linguistics"
    },
    {
      "citation_id": "54",
      "title": "WIDER FACE: A Face Detection Benchmark",
      "authors": [
        "Shuo Yang",
        "Ping Luo",
        "Chen Loy",
        "Xiaoou Tang"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "55",
      "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "56",
      "title": "Multimodal Fusion Strategies for Physiological-emotion Analysis",
      "authors": [
        "Tenggan Zhang",
        "Zhaopei Huang",
        "Ruichen Li",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "57",
      "title": "Generalized canonical time warping",
      "authors": [
        "Feng Zhou",
        "Fernando De La"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "58",
      "title": "The effects of sleep loss on medical residents' emotional reactions to work events: a cognitiveenergy model",
      "authors": [
        "Dov Zohar",
        "Orna Tzischinsky",
        "Rachel Epstein",
        "Peretz Lavie"
      ],
      "year": "2005",
      "venue": "Sleep"
    }
  ]
}