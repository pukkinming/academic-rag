{
  "paper_id": "2407.18552v3",
  "title": "Multimodal Emotion Recognition Using Audio-Video Transformer Fusion With Cross Attention",
  "published": "2024-07-26T07:05:04Z",
  "authors": [
    "Joe Dhanith P R",
    "Shravan Venkatraman",
    "Vigya Sharma",
    "Santhosh Malarvannan",
    "Modigari Narendra"
  ],
  "keywords": [
    "Cross Attention",
    "Channel Attention",
    "Spatial Attention",
    "Multimodal Emotion Recognition",
    "Self-Attention",
    "Transformer Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding emotions is a fundamental aspect of human communication. Integrating audio and video signals offers a more comprehensive understanding of emotional states compared to traditional methods that rely on a single data source, such as speech or facial expressions. Despite its potential, multimodal emotion recognition faces significant challenges, particularly in synchronization, feature extraction, and fusion of diverse data sources. To address these issues, this paper introduces a novel transformer-based model named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA model employs a transformer fusion approach to effectively capture and synchronize interlinked features from both audio and video inputs, thereby resolving synchronization problems. Additionally, the Cross Attention mechanism within AVT-CA selectively extracts and emphasizes critical features while discarding irrelevant ones from both modalities, addressing feature extraction and fusion challenges. Extensive experimental analysis conducted on the CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the proposed model. The results underscore the importance of AVT-CA in developing precise and reliable multimodal emotion recognition systems for practical applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Effective human communication hinges on the fundamental skill of emotional awareness, enabling individuals to comprehend and respond to the feelings of others. Conventional emotion recognition methods  [1]  often rely on data from a single source, such as speech patterns or facial expressions. However, human emotional communication is inherently multimodal, involving a complex interplay of verbal and non-verbal cues. To address this complexity, recent research has increasingly focused on multimodal emotion recognition tasks that combine audio and video sources. By leveraging both verbal intonations and facial expressions  [2] , these approaches aim to provide a deeper and more nuanced understanding of an individual's ğ‘’ğ‘šğ‘œğ‘¡ğ‘–ğ‘œğ‘›ğ‘ğ‘™ state.\n\nConsider the example: \"I'm really happy for you!\" Determining whether genuine happiness is being expressed based solely on the tone of voice can be challenging. Visual cues, such as a sincere smile, wrinkled eyes, and an overall positive demeanor, offer additional insights. However, relying solely on visual data has its drawbacks, as a smile can be faked to mask true feelings. Therefore, combining audio and visual elements is essential to capture a more accurate emotional context  [3] ,  [4] . Accurately recognizing emotions from multiple sources requires effective integration of audio and video information. This task is challenging due to three primary concerns: (i) synchronization, (ii) feature extraction, and (iii) fusion. Synchronizing audio and video data is crucial but technically demanding, especially when dealing with temporal inconsistencies and differing sample rates. Combining data from these sources in a way that maximizes their respective strengths is also challenging. Existing deep learning models struggle to determine the appropriate weight for each modality during fusion, and they must account for individual variations in emotional expression to avoid bias, necessitating careful feature extraction.\n\nEarly attempts at multimodal emotion recognition, which either integrated features from each source (early fusion) or combined final predictions (late fusion), yielded limited success. Newer techniques leveraging deep learning, particularly those involving attention mechanisms and transformers, have shown promise in exploring more sophisticated fusion methods. Extracting necessary features in unrestricted environments remains difficult, introducing uncertainty into the processing pipeline. This is especially problematic for language-based approaches, such as text transcriptions of ğ‘ğ‘¢ğ‘‘ğ‘–ğ‘œ ğ‘ ğ‘–ğ‘”ğ‘›ğ‘ğ‘™ğ‘ , which are rarely available in real-world scenarios and require separate estimation. Pre-extracted features  [5] ,  [6] ,  [7] ,\n\ncommonly used in transformer-based designs, have become popular for multimodal emotion recognition, as they eliminate the need for end-to-end feature learning  [8] .\n\nTo address these challenges, this work proposes a novel method for multimodal emotion recognition called Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA introduces a unique feature extraction mechanism for video sources, comprising (i) channel attention, (ii) spatial attention, and (iii) a local feature extractor. Channel attention assesses the importance of each ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ ğ‘â„ğ‘ğ‘›ğ‘›ğ‘’ğ‘™ in the input video, spatial attention identifies critical regions for emotion recognition, and the local feature extractor refines the frames selected by both channel and spatial attention for efficient feature representation. This approach eliminates the need for end-to-end feature extraction.\n\nBy utilizing the proposed feature representation module, the AVT-CA model can learn complex relationships between features through an intermediate-level fusion technique called Transformer Fusion.\n\nThis approach addresses synchronization issues by capturing interlinked features between audio and video sources. The cross-attention mechanism in AVT-CA extracts significant features while discarding insignificant ones from both audio and video inputs, thereby resolving feature extraction and fusion challenges in multimodal emotion recognition tasks.\n\nThe key contributions of the proposed work are as follows:\n\nï‚· The AVT-CA model introduces an innovative approach to audio-video emotion recognition by employing a novel feature representation strategy before learning from raw facial and speech data.\n\nThis strategy encompasses channel attention, spatial attention, and a local feature extractor, which collectively enhance the model's ability to discern intrinsic correlations within the data while mitigating the impact of preprocessing inaccuracies.\n\nï‚· The AVT-CA model presents a novel intermediate transformer fusion method that integrates inputs from complementary modalities post-initial feature extraction. This integration facilitates early learning of significant features from both modalities. Incorporating transformer blocks within each branch captures complex interrelations in the data, yielding robust and discriminative feature representations for precise emotion recognition. This early fusion strategy, augmented by the capabilities of transformers, holds promise for substantially enhancing the efficacy of contemporary multi-modal emotion recognition models. The rest of the paper is organized as follows: section 2 reviews the literature in multimodal emotion recognition tasks, section 3 introduces the proposed AVT-CA model, section 4 discusses the experimental setup, section 5 analyzes the results and section 6 concludes the paper.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Related Works",
      "text": "The machine learning community has placed a significant emphasis on emotion recognition tasks, and various approaches have been introduced to tackle this challenge. The concept of multimodal learning is fundamental to machine learning research, which includes fields like multimodal multi-task learning  [9] , cross-modal generation  [10] ,  [11] , vision-language learning  [12] ,  [13] , vision-audio learning  [14] ,  [15] ,  [16] ,  [17] ,  [18] , and zero-shot learning  [19] ,  [20] . Multi-task learning leverages different data types to accomplish various tasks, capitalizing on how these data types can complement each other. Cross-modal creation involves using one type of data to create another, such as generating pictures from words or vice versa. Vision-language learning is essential for understanding and creating sentences that vividly depict visual experiences. Vision-audio learning is about integrating visual information with auditory input, aiding in locating sounds and lip-reading. Zero-shot learning enables models to recognize new items without specific preparation by leveraging the relationships between different topics. The progress of emotion detection can be enhanced by combining insights from different sources. Researchers aim to develop emotion-understanding systems that perform better in real-life situations and can be applied in a wider range of scenarios. Their objective is to create versions that integrate information from images, text, and other data to provide a comprehensive understanding of emotions.\n\nNaturally, video includes various types of information, and it allows us to learn from large amounts of data that would be difficult to annotate quickly. In this context, Miech et al.  [21]  introduced a method that shows the potential of ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘– -ğ‘šğ‘œğ‘‘ğ‘ğ‘™ learning for creating a space where videos and texts are linked through comparative loss. They also provided the 100M dataset of narrated movies, consisting of YouTube instructional videos with text descriptions generated by Automatic Speech Recognition (ASR), along with the corresponding audio and subtitles. Since this data may be considered noisier compared to carefully curated vision-text datasets, Amrani et al.  [22]  suggested using multi-modal density estimation to estimate the noise in multi-modal data.\n\nBy combining ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ -ğ‘ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘ ğ‘¡ğ‘–ğ‘£ğ‘’ estimating with a multiple-instance learning approach, Miech et al.  [23]  developed MIL-NCE. Alwassel et al.  [24]  proposed unsupervised clustering as a supervisory signal across different types of data. While some studies  [25]  trained their models using only two types of data, other studies  [26] ,  [27] ,  [28] ,  [29]  focused on the challenge of learning from ğ‘£ğ‘–ğ‘ ğ‘–ğ‘œğ‘›, ğ‘ğ‘¢ğ‘‘ğ‘–ğ‘œ, and ğ‘¡ğ‘’ğ‘¥ğ‘¡ simultaneously. In a related work, Alayrac et al.  [26]  introduced Multi-Modal Versatile Networks, which explore the concept of distinct embedding spaces for different combinations of data types. Rouditchenko et al.  [29]  introduced a combined embedding space that aligns all three modalities into a single unified space.\n\nBuilding on this idea, Chen et al.  [27]  have enhanced the concept with the addition of clustering and reconstruction loss.\n\nThe following research studies were conducted to explore emotional representations in various modalities: Dai et al.  [30]  studied the transfer of emotion embeddings from textual modality to visual and auditory modalities and investigated adaptation in few-shot learning. Hazarika et al.  [6]  introduced MISA, a ğ‘šğ‘œğ‘‘ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¦ -ğ‘–ğ‘›ğ‘£ğ‘ğ‘Ÿğ‘–ğ‘ğ‘›ğ‘¡ and ğ‘šğ‘œğ‘‘ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¦ -ğ‘ ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ representation learning algorithm for multimodal sentiment analysis, with a focus on enhancing multimodal characteristics before the fusion process. Han et al.  [5]  concentrated on developing a fusion strategy for multimodal data, with text as the primary modality.\n\nThey designed a Transformer-based fusion network to combine complementary data from pairs of texts, images, and sounds.\n\nVu et al.  [31]  proposed a solution to the issue of insufficient emotion data in facial recognition.\n\nThey introduced a knowledge distillation-based multi-task learning model that performs two tasks. First, it classifies facial expressions (e.g., happy, furious) and second, it predicts the intensity and positive/negative nature of an emotion (valence-arousal estimate). Notably, not every data sample has labels for both tasks.\n\nTo overcome this, they employ knowledge distillation. In this approach, a simpler model (student) is guided by a more complex model (teacher) trained on complete data (with labels for both tasks). Even when the original data lacks labels for a specific task, the student model uses the soft labels (probabilities) from the teacher model in addition to learning from the original data. This combined method improves the accuracy of the student model's emotion identification, even when certain labels are missing in the data.\n\nIn addition, Han et al.  [7]  introduced multi-modal ğ¼ğ‘›ğ‘“ğ‘œğ‘Ÿğ‘€ğ‘ğ‘¥ (MMIM) for multimodal sentiment analysis, which utilizes mutual information concepts to combine multimodal features. They also provided hand-crafted input features to other techniques  [32]  for multimodal sentiment analysis and emotion identification. Hazarika et al.  [36]  dynamically weighted the contribution of each feature using ğ‘ ğ‘’ğ‘™ğ‘“ -ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› mechanism, allowing the model to focus on the most relevant emotional cues.\n\nCheng et al.  [25]  proposed a co-attention module to learn connections between audio and visual data. Luo et al.  [37]  expanded this by suggesting a combined cross-modal encoding for video-text pairings, similar to the approach used by Uniter  [38]  for vision-language tasks. On the other hand, Bain et al.  [39]  focused on integrating temporal and spatial information in the video backbone, achieving this by adding a linear mapping layer on top of two distinct transformer backbones handling the text and video modalities.\n\nAdditionally, Nagrani et al.  [40]  recently introduced a ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘– -ğ‘šğ‘œğ‘‘ğ‘ğ‘™ bottleneck transformer designed for effective audio-visual fusion in a supervised learning environment. Wu et al.  [41]  leveraged self-supervised learning to integrate and interpret various data modalities (e.g., text, audio, video) without extensive labeled datasets.\n\nIn 2023, Wen et al.  [42]  introduced the Distract Your Attention Network (DAN) to address the challenge of identifying subtle facial emotions that require intentional facial movements. The DAN approach is inspired by the understanding that facial expressions can be similar and that emotions are expressed through coordination across multiple areas of the face. DAN employs a three-part strategy: first, it identifies distinctive characteristics of facial expressions. Then, it utilizes a unique network with multiple \"heads\" to focus on different areas of the face simultaneously. Finally, it combines data from various sources to create a comprehensive interpretation of the expression. This method has achieved cutting-edge results in facial expression recognition tasks. Huang et al.  [43]  utilized a combination of vector quantization and masked autoencoding to process and learn from multimodal data (e.g., text, audio, video).\n\nAkbari et al.'s transformer-based approach  [44]  incorporates all three modalities. In their model, each modality is processed separately by a single backbone transformer, but with shared attention. The model first computes video-audio matching and then video-text matching for training, following the concept introduced by  [45] . This effectively pairs the modalities together, resembling a part of the suggested loss function. Zadeh et al.  [46]  integrated multiple data modalities (e.g., text, audio, and video) by capturing both intra-modality and inter-modality dynamics using ğ‘‡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿ ğ¹ğ‘¢ğ‘ ğ‘–ğ‘œğ‘› ğ‘ğ‘’ğ‘¡ğ‘¤ğ‘œğ‘Ÿğ‘˜ (ğ‘‡ğ¹ğ‘). Liu et al.  [47]  combined multiple modalities (such as text, audio, and video) by leveraging low-rank tensor approximations.\n\nOther methods also utilize temporal elements in the context of multi-modal transformer learning. The key issues identified from the literature are, (i) Existing models often struggle with temporal misalignment, where audio and video inputs are not perfectly synchronized. This misalignment can lead to inaccurate emotion detection as the model might miss critical cues that are time sensitive.\n\n(ii) Some models fail to extract features that are sufficiently discriminative for different emotional states, leading to poor performance, especially in recognizing subtle or complex emotions.\n\n(iii) Current fusion techniques sometimes fail to effectively integrate multimodal data.\n\nSimple concatenation or early fusion methods may not capture the complex interdependencies between audio and video features, leading to suboptimal performance.",
      "page_start": 4,
      "page_end": 7
    },
    {
      "section_name": "Proposed Work",
      "text": "This section presents a comprehensive overview of the proposed model's architecture. The model uses two separate channels to process audio and visual data. The knowledge that emotions frequently appear in both visual and audible signals is reflected in this design. We implement blocks in each channel that are specifically intended to pull relevant information from the corresponding modalities. These blocks have been meticulously designed to precisely capture the details necessary for identifying emotional shifts.\n\nFigure  2  shows the proposed AVT-CA model architecture.\n\nInitially, the 1D audio tensor goes through two convolutional blocks, each with a 3x3",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Figure 2. Audio Video Transformer Fusion With Cross Attention",
      "text": "After the convolutional block processes the video frames, the output is enhanced through a series of specialized blocks: channel attention, spatial attention, and a local feature extractor. The channel attention block recalibrates the importance of different feature channels, focusing on those that carry more relevant information for emotion recognition, such as distinctive facial features or body postures.\n\nSimultaneously, the spatial attention block emphasizes specific regions within each frame, like the eyes or mouth, which are crucial for interpreting emotional expressions. The local feature extractor further refines these frames by pulling out detailed features that contribute to understanding emotions through visual cues.\n\nAlgorithms 2, 3 and 4 show the workflow of channel attention, spatial attention and local feature extractor. The outputs from the channel and spatial attention blocks are then combined by dot product multiplication, creating a focused emphasis on the most relevant features across both channels and spatial locations. This combined output is subsequently added to the output from the local feature extractor block, integrating detailed local features with the globally emphasized features. This process enriches the video data by highlighting the most informative aspects for emotion recognition.\n\nFollowing this enhancement, the refined video data is passed through two consecutive inverse residual blocks. These blocks are designed to deepen the network efficiently, allowing for more complex representations of the video data without significantly increasing computational cost. Through a series of convolutions and shortcut connections, these blocks enable the model to learn intricate patterns and dependencies within the video frames. The video output undergoes further processing through two convolutional blocks, each equipped with a 3x3 convolution, batch normalization, and ReLU activation. This step continues to enhance the video data by extracting more detailed spatial features and adjusting the activations for improved learning efficiency. Algorithm 5 shows the workflow of inverted residual block.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "ğ’†ğ’ğ’… ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ’†ğ’ğ’…",
      "text": "The outputs from the first transformer block, which has processed the video and audio inputs, are then combined with the original audio output. Similarly, the outputs from the second transformer block are combined with the original video output. This integration method allows for a dynamic exchange of information between the modalities, enriching the representation of each by incorporating insights from the other. Essentially, this process enables the model to leverage the strengths of both ğ‘ğ‘¢ğ‘‘ğ‘–ğ‘œ and ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ data, leading to a more comprehensive and accurate understanding of human emotions expressed through these modalities.\n\nIn the next phase, on the audio side, the output is enhanced through two convolutional blocks, each featuring a 3x3 convolution, batch normalization (bn), ReLU activation, and max pooling. This sequence of operations serves to extract and highlight key audio features, such as spectral and temporal patterns, while also reducing the dimensionality of the data to focus on the most salient aspects. Simultaneously, the video output is processed through two similar convolutional blocks but without max pooling, allowing for the retention of more detailed spatial information pertinent to visual emotion cues. Following these enhancements, the refined audio and video outputs are then channeled into two separate self-attention blocks. These blocks are designed to facilitate a deeper interaction between the audio and video modalities by allowing each modality to attend to the other. In the first self-attention block, the audio output acts as the key, and the video output serves as the query. This configuration enables the model to align and relate the audio features with corresponding visual features, enhancing the model's understanding of how audio and visual cues interplay in conveying emotions. Conversely, in the second self-attention block, the roles are reversed, with the video output acting as the key and the audio output as the query. This reversal allows the model to explore the relationship from the perspective of visual features attending to audio features, further enriching the cross-modal learning experience. Algorithm 8 shows the workflow of self-attention.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "ğ’“ğ’†ğ’•ğ’–ğ’“ğ’ ğ‘¥, ğ‘ğ‘˜ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥ ğ’†ğ’ğ’… ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ’†ğ’ğ’…",
      "text": "The outputs from these self-attention blocks are then reintegrated with their respective original inputs. Specifically, the output from the first self-attention block, where audio was the key, is added back to the original audio output. Similarly, the output from the second self-attention block, where video was the key, is added back to the original video output. This reintegration process effectively embeds the crossmodal contextual information into each modality's representation, strengthening the model's ability to recognize emotions by leveraging the complementary nature of audio and visual data.\n\nIn the concluding stages of the multimodal emotion recognition process, both the audio and video streams are subjected to cross-attention mechanisms, specifically implemented as multi-head attention blocks. This step is crucial for integrating the learned representations from both modalities into a unified understanding of the emotional content. The ğ‘šğ‘¢ğ‘™ğ‘¡ğ‘– -â„ğ‘’ğ‘ğ‘‘ attention mechanism allows the model to focus on different parts of the input sequences simultaneously, capturing various aspects of the emotional expressions encoded in the audio and video data. By dividing the input into multiple heads, the model can attend to information from different representation subspaces in parallel, enhancing its capacity to understand complex emotional cues that may be distributed across the modalities. Algorithm 9 shows the workflow of cross attention.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "ğ’†ğ’ğ’…",
      "text": "After processing through the multi-head attention blocks, the outputs from both the audio and video sides are pooled using max pooling. This operation reduces the dimensionality of the data by selecting the maximum value from each segment, effectively distilling the most significant features from the attended information. The max-pooled outputs from both modalities are then combined by addition, merging the independently processed audio and video features into a single, comprehensive representation of the emotional content. This merged representation is finally passed through a ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ activation layer. The ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ function converts the combined features into a probability distribution over the possible emotion categories, ensuring that the sum of probabilities equals one. This transformation allows the model to output a normalized prediction, indicating the likelihood of each potential emotion being expressed in the input audio-video sequence. Through this series of operations, the system culminates in a final prediction of the emotional state conveyed through the multimodal input, leveraging the strengths of both audio and video data to achieve a refined understanding of human emotions.  Algorithm\n\nwhere ğ¶ is the total number of emotions, ğ‘ is the total instances in a batch, ğ‘¦ , is the true emotion, and ğ‘ , is the predicted emotion.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experimental Setup",
      "text": "This section describes the experimental setup and the data utilized to assess the effectiveness of the   Figure  3  shows the statistics of all the three datasets (RAVDESS, CMU-MOSEI and RAVDESS) used for experimental analysis.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Baseline Models For Cmu-Mosei Dataset",
      "text": "To thoroughly validate AVT-CA's performance on the CMU-MOSEI dataset, this paper conducted fair comparisons with various baseline models in multimodal emotion recognition.\n\n(i) Low-rank Multimodal Fusion (LMF)  [47]  for emotion recognition combines audio, video, and text features by constructing a high-dimensional tensor and applying low-rank tensor factorization. This method efficiently reduces the tensor's dimensionality while capturing critical inter-modality interactions with reduced computational complexity.\n\n(ii) The Memory Fusion Network (MFN)  [54]  for multimodal emotion recognition utilizes a memory matrix to encode long-term inter-modality interactions across sequences. It independently processes each modality and employs a multi-view gated memory mechanism to integrate and retain these interactions effectively.\n\n(iii) The Multimodal Factorization Model (MFM)  [55]  for emotion recognition decomposes audio, video, and text input features into shared and modality-specific components using tensor factorization. This approach effectively captures both common characteristics and unique aspects of each modality, facilitating robust fusion for emotion recognition.\n\n(iv) The Multimodal Transformer (MulT)  [56]  for emotion recognition utilizes transformer architectures to process temporal sequences of multimodal data, including audio, video, and text. It incorporates crossmodal attention mechanisms to capture interactions between modalities by aligning and integrating their respective features. This approach enhances contextual understanding as each modality attends to and benefits from the others' information.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Baseline Models For Ravdess Dataset",
      "text": "To thoroughly validate AVT-CA's performance on the RAVDESS dataset, this paper conducted fair comparisons with various baseline models in multimodal emotion recognition.\n\n(i) The Vector Quantized Masked Autoencoder (VQ-MAE)  [43]  is a model crafted for multimodal emotion recognition, merging vector quantization with masked autoencoding techniques. It learns discrete latent representations from multimodal inputs, such as audio, video, and text, by masking sections of the input data and reconstructing them. This process enhances the robustness and informativeness of the features. The vector quantization discretizes continuous inputs, effectively capturing diverse emotional cues across various modalities. Utilizing self-supervised learning, VQ-MAE improves performance in emotion recognition tasks while minimizing the need for labeled data.\n\n(ii) The Multimodal Transfer Module (MMTM)  [60]  is a framework designed for multimodal emotion recognition that enables information exchange between various modalities, including audio, video, and text.\n\nIt dynamically refines the feature representations of each modality through cross-modal attention mechanisms, allowing the model to effectively leverage complementary information from different sources.\n\n(iii) Multimodal Self-Attention Fusion (MSAF)  [36]  is a technique for multimodal emotion recognition that incorporates self-attention mechanisms across various modalities, such as audio, video, and text. By (vi) The Multimodal Masked Autoencoder for Dynamic Emotion Recognition (MultiMAE-DER)  [62]  leverages spatiotemporal correlations across visual and audio modalities. Utilizing a pre-trained masked autoencoder model, it is fine-tuned for enhanced performance. MultiMAE-DER optimizes six fusion strategies for multimodal input sequences, addressing dynamic feature correlations across spatial, temporal, and spatiotemporal data.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Baseline Models For Crema-D Dataset",
      "text": "To thoroughly validate AVT-CA's performance on the CREMA-D dataset, this paper conducted fair comparisons with various baseline models in multimodal emotion recognition. each modality. This approach demonstrated superior performance in ablation studies. Additionally, a novel temporal embedding scheme, termed block embedding, is introduced to incorporate temporal information into visual features derived from multiple video frames.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Result Analysis",
      "text": "",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Result Analysis On Cmu-Mosei Dataset",
      "text": "This section provides a detailed analysis of the performance of the proposed AVT-CA model compared to baseline models on the CMU-MOSEI dataset. As demonstrated in       The proposed AVT-CA model outperforms all baseline models, achieving ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ -ğ‘œğ‘“ -ğ‘¡â„ğ‘’ -ğ‘ğ‘Ÿğ‘¡ results on the RAVDESS dataset. The superior performance is due to the cross-attention module, which effectively captures key information between audio and video modalities. This novel approach addresses the fusion problems in multimodal emotion recognition tasks, leading to enhanced accuracy and F1-scores.\n\nThis cross-attention mechanism helps to solve the synchronization issues between the audio and video data.   In conclusion, the proposed AVT-CA model demonstrates significant improvements over existing baseline models in multimodal emotion recognition tasks on the RAVDESS dataset. The cross-attention mechanism and efficient feature fusion strategy of AVT-CA contribute to its superior performance, effectively addressing the challenges in multimodal emotion recognition.   The AVT-CA model accurately identifies key features from both audio and video inputs that are critical for emotion recognition. These advantages allow the AVT-CA model to achieve superior ğ‘ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ and ğ¹1 -ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ , as shown in Table  6 .     The results indicate that the IT-1 configuration shows moderate performance on both datasets.\n\nHowever, IT-4 demonstrates improved performance, highlighting the benefits of using multiple attention heads in the transformer fusion process. Multiple attention heads enable the model to focus on different parts of the input sequence simultaneously, which is advantageous for capturing diverse aspects of emotional cues. Each head can attend to distinct features or patterns relevant to emotion identification.\n\nSimilarly, the CT-1 configuration shows moderate performance, while CT-4 shows enhanced performance on both datasets. These findings reinforce the advantage of employing multiple attention heads in the cross-attention mechanism, as they improve the model's ability to extract significant features from both audio and video inputs.\n\nThe full AVT-CA model, combining IT-4 and CT-4, achieves ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ -ğ‘œğ‘“ -ğ‘¡â„ğ‘’ -ğ‘ğ‘Ÿğ‘¡ results on both CMU-MOSEI and RAVDESS datasets. The superior performance is attributed to the synergistic effects of IT-4 and CT-4: IT-4 effectively captures interlinked features between audio and video, while CT-4 isolates and emphasizes critical features for accurate emotion recognition.\n\nIn summary, the ablation study clearly demonstrates that the use of multiple attention heads in both intermediate transformer fusion and cross attention mechanisms significantly enhances the performance of the AVT-CA model in emotion recognition tasks. This comprehensive approach enables the model to capture a wide range of emotional cues, leading to more precise and reliable emotion recognition. The results shown in Table  8  and 9 clearly prove that all the components in the proposed AVT-CA model is important to achieve ğ‘ ğ‘¡ğ‘ğ‘¡ğ‘’ -ğ‘œğ‘“ -ğ‘¡â„ğ‘’ -ğ‘ğ‘Ÿğ‘¡ results. The highlights of the proposed AVT-CA model are as follows:\n\n1. The cross-attention mechanism in the proposed AVT-CA model effectively learns the key information from the different modalities, which helps to solve the synchronization issues by handling temporal inconsistency and different sampling rates.\n\n2. The feature enhancement strategy in the proposed AVT-CA model incorporates spatial attention, channel attention, and a local feature extractor, which concentrates on subtle and complex emotions from the video.\n\n3. The transformer fusion mechanism in the proposed AVT-CA model integrates the enhanced video features with the audio features, effectively fusing information from diverse modalities.",
      "page_start": 22,
      "page_end": 33
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents the AVT-CA model, a novel approach to multimodal emotion recognition that",
      "page_start": 33,
      "page_end": 33
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Various emotions in CMU-MOSEI dataset",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the proposed AVT-CA model architecture.",
      "page": 8
    },
    {
      "caption": "Figure 2: Audio Video Transformer Fusion with Cross Attention",
      "page": 9
    },
    {
      "caption": "Figure 3: shows the statistics of all the three datasets (RAVDESS, CMU-MOSEI and RAVDESS) used for",
      "page": 18
    },
    {
      "caption": "Figure 3: Dataset Statistics",
      "page": 19
    },
    {
      "caption": "Figure 4: displays the training and validation accuracy curves of the AVT-CA model on the CMU-",
      "page": 23
    },
    {
      "caption": "Figure 5: displays the training and validation F1-score curves of the AVT-CA model on the CMU-MOSEI dataset.",
      "page": 23
    },
    {
      "caption": "Figure 6: presents the training and validation loss curves of the AVT-CA model, reflecting the model's",
      "page": 23
    },
    {
      "caption": "Figure 4: Accuracy of AVT-CA in CMU-MOSEI dataset",
      "page": 24
    },
    {
      "caption": "Figure 5: F1-Score of AVT-CA in CMU-MOSEI dataset",
      "page": 24
    },
    {
      "caption": "Figure 6: Loss of AVT-CA in CMU-MOSEI dataset",
      "page": 25
    },
    {
      "caption": "Figure 7: displays the training and validation accuracy curves of the AVT-CA model on the",
      "page": 26
    },
    {
      "caption": "Figure 8: displays the training and validation F1-score curves of the AVT-CA model on the RAVDESS dataset.",
      "page": 26
    },
    {
      "caption": "Figure 9: presents the training and validation loss curves of the AVT-CA model, reflecting the model's",
      "page": 26
    },
    {
      "caption": "Figure 7: Accuracy of AVT-CA in RAVDESS dataset",
      "page": 27
    },
    {
      "caption": "Figure 8: F1-Score of AVT-CA in RAVDESS dataset",
      "page": 27
    },
    {
      "caption": "Figure 9: Loss of AVT-CA in RAVDESS dataset",
      "page": 28
    },
    {
      "caption": "Figure 10: displays the training and validation accuracy curves of the AVT-CA model on the",
      "page": 29
    },
    {
      "caption": "Figure 11: displays the training and validation accuracy curves of the AVT-CA model on the CREMA-D dataset.",
      "page": 29
    },
    {
      "caption": "Figure 12: presents the training and validation loss curves of the AVT-CA model, reflecting the model's",
      "page": 29
    },
    {
      "caption": "Figure 10: Accuracy of AVT-CA in CREMA-D dataset",
      "page": 30
    },
    {
      "caption": "Figure 11: F1-Score of AVT-CA in CREMA-D dataset",
      "page": 30
    },
    {
      "caption": "Figure 12: Loss of AVT-CA in CREMA-D dataset",
      "page": 31
    }
  ],
  "tables": [
    {
      "caption": "Table 1: displays the statistics of RAVDESS dataset.",
      "data": [
        {
          "Emotion": "Neutral",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Calm",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Happy",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Sad",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Angry",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Fearful",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Disgust",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Surprised",
          "Total Samples": "920",
          "Training Samples (80%)": "736",
          "Validation Samples (20%)": "184"
        },
        {
          "Emotion": "Total",
          "Total Samples": "7360",
          "Training Samples (80%)": "5888",
          "Validation Samples (20%)": "1472"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 1: displays the statistics of RAVDESS dataset.",
      "data": [
        {
          "Emotion": "Happy",
          "Total Samples": "3909",
          "Training Samples (80%)": "3127",
          "Validation Samples (20%)": "782"
        },
        {
          "Emotion": "Sad",
          "Total Samples": "3909",
          "Training Samples (80%)": "3127",
          "Validation Samples (20%)": "782"
        },
        {
          "Emotion": "Angry",
          "Total Samples": "3909",
          "Training Samples (80%)": "3127",
          "Validation Samples (20%)": "782"
        },
        {
          "Emotion": "Fearful",
          "Total Samples": "3909",
          "Training Samples (80%)": "3127",
          "Validation Samples (20%)": "782"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 3: displays the",
      "data": [
        {
          "Disgusted": "Surprised",
          "3909": "3909",
          "3127": "3127",
          "782": "782"
        },
        {
          "Disgusted": "Total",
          "3909": "23454",
          "3127": "18763",
          "782": "4691"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 3: displays the",
      "data": [
        {
          "Emotion": "Neutral",
          "Total Samples": "1240",
          "Training Samples (80%)": "992",
          "Validation Samples (20%)": "248"
        },
        {
          "Emotion": "Happy",
          "Total Samples": "1240",
          "Training Samples (80%)": "992",
          "Validation Samples (20%)": "248"
        },
        {
          "Emotion": "Sad",
          "Total Samples": "1240",
          "Training Samples (80%)": "992",
          "Validation Samples (20%)": "248"
        },
        {
          "Emotion": "Angry",
          "Total Samples": "1240",
          "Training Samples (80%)": "992",
          "Validation Samples (20%)": "248"
        },
        {
          "Emotion": "Fearful",
          "Total Samples": "1240",
          "Training Samples (80%)": "992",
          "Validation Samples (20%)": "248"
        },
        {
          "Emotion": "Disgust",
          "Total Samples": "1240",
          "Training Samples (80%)": "992",
          "Validation Samples (20%)": "248"
        },
        {
          "Emotion": "Total",
          "Total Samples": "7440",
          "Training Samples (80%)": "5952",
          "Validation Samples (20%)": "1488"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 4: Results of baseline and the AVT-CA model on CMU-MOSEI dataset",
      "data": [
        {
          "Model": "ConCluGen",
          "Accuracy": "66.01",
          "F1-Score": "66.30"
        },
        {
          "Model": "MFN",
          "Accuracy": "76.03",
          "F1-Score": "76.07"
        },
        {
          "Model": "LMF",
          "Accuracy": "82.01",
          "F1-Score": "82.13"
        },
        {
          "Model": "MuIT",
          "Accuracy": "82.51",
          "F1-Score": "82.31"
        },
        {
          "Model": "COGMEN",
          "Accuracy": "83.81",
          "F1-Score": "83.51"
        },
        {
          "Model": "MFM",
          "Accuracy": "84.41",
          "F1-Score": "84.36"
        },
        {
          "Model": "MAG-BERT",
          "Accuracy": "84.71",
          "F1-Score": "84.51"
        },
        {
          "Model": "AVT-CA (Proposed)",
          "Accuracy": "95.84",
          "F1-Score": "94.13"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 5: Results of baseline and the AVT-CA model on RAVDESS dataset",
      "data": [
        {
          "Model": "MultiMAE-DER",
          "Accuracy": "69.17",
          "F1-Score": "69.03"
        },
        {
          "Model": "MMTM",
          "Accuracy": "73.12",
          "F1-Score": "72.7"
        },
        {
          "Model": "MSAF",
          "Accuracy": "74.86",
          "F1-Score": "74.31"
        },
        {
          "Model": "CFN-SR",
          "Accuracy": "75.76",
          "F1-Score": "75.27"
        },
        {
          "Model": "VQ-MAE",
          "Accuracy": "84.1",
          "F1-Score": "84.4"
        },
        {
          "Model": "CNN",
          "Accuracy": "95.95",
          "F1-Score": "92.17"
        },
        {
          "Model": "AVT-CA (Proposed)",
          "Accuracy": "96.11",
          "F1-Score": "93.78"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table 6: Figure 10 displays the training and validation accuracy curves of the AVT-CA model on the",
      "data": [
        {
          "Model": "Multi-Modal Transformer",
          "Accuracy": "72.6",
          "F1-Score": "71.4"
        },
        {
          "Model": "AuxFormer",
          "Accuracy": "76.3",
          "F1-Score": "69.8"
        },
        {
          "Model": "VAVL",
          "Accuracy": "82.6",
          "F1-Score": "77.9"
        },
        {
          "Model": "LADDER",
          "Accuracy": "80.3",
          "F1-Score": "80.2"
        },
        {
          "Model": "DE III",
          "Accuracy": "83.7",
          "F1-Score": "79.5"
        },
        {
          "Model": "AVT-CA (Proposed)",
          "Accuracy": "94.13",
          "F1-Score": "94.67"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table 8: and 9 clearly prove that all the components in the proposed AVT-CA model is",
      "data": [
        {
          "Model": "IT-1",
          "Accuracy": "67.72",
          "F1-Score": "67.13"
        },
        {
          "Model": "IT-4",
          "Accuracy": "64.91",
          "F1-Score": "63.97"
        },
        {
          "Model": "CT-1",
          "Accuracy": "64.94",
          "F1-Score": "64.33"
        },
        {
          "Model": "CT-4",
          "Accuracy": "67.72",
          "F1-Score": "67.91"
        },
        {
          "Model": "AVT-CA (Proposed)",
          "Accuracy": "95.84",
          "F1-Score": "94.13"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 8: and 9 clearly prove that all the components in the proposed AVT-CA model is",
      "data": [
        {
          "Model": "IT-1",
          "Accuracy": "76.41",
          "F1-Score": "76.31"
        },
        {
          "Model": "IT-4",
          "Accuracy": "78.50",
          "F1-Score": "78.31"
        },
        {
          "Model": "CT-1",
          "Accuracy": "76.0",
          "F1-Score": "75.9"
        },
        {
          "Model": "CT-4",
          "Accuracy": "77.41",
          "F1-Score": "77.13"
        },
        {
          "Model": "AVT-CA (Proposed)",
          "Accuracy": "96.11",
          "F1-Score": "93.78"
        }
      ],
      "page": 32
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal Emotion Recognition with Deep Learning: Advancements, challenges, and future directions",
      "year": "2024",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.102218"
    },
    {
      "citation_id": "2",
      "title": "Multimodal Emotion Detection via Attention-Based Fusion of Extracted Facial and Speech Features",
      "authors": [
        "D Mamieva",
        "A Abdusalomov",
        "A Kutlimuratov",
        "B Muminov",
        "T Whangbo"
      ],
      "year": "2023",
      "venue": "Sensors",
      "doi": "10.3390/s23125475"
    },
    {
      "citation_id": "3",
      "title": "Deep spatio-temporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Computer Vision and Image Understanding",
      "doi": "10.1016/j.cviu.2018.06.005"
    },
    {
      "citation_id": "4",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "K Chumachenko",
        "A Iosifidis",
        "M Gabbouj"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)",
      "doi": "10.1109/ICPR56361.2022.9956592"
    },
    {
      "citation_id": "5",
      "title": "Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis",
      "authors": [
        "W Han",
        "H Chen",
        "A Gelbukh",
        "A Zadeh",
        "L Morency",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis"
    },
    {
      "citation_id": "6",
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis"
    },
    {
      "citation_id": "7",
      "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis"
    },
    {
      "citation_id": "8",
      "title": "EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection",
      "authors": [
        "G Wang",
        "J Li",
        "Z Wu",
        "J Xu",
        "J Shen",
        "W Yang"
      ],
      "year": "2023",
      "venue": "EfficientFace: An Efficient Deep Network with Feature Enhancement for Accurate Face Detection"
    },
    {
      "citation_id": "9",
      "title": "Multimodal Sentiment Analysis With Two-Phase Multi-Task Learning",
      "authors": [
        "B Yang",
        "L Wu",
        "J Zhu",
        "B Shao",
        "X Lin",
        "T.-Y Liu"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Trans Audio Speech Lang Process",
      "doi": "10.1109/TASLP.2022.3178204"
    },
    {
      "citation_id": "10",
      "title": "Audio-to-Image Cross-Modal Generation",
      "authors": [
        "M Å»elaszczyk",
        "J MaÅ„dziuk"
      ],
      "year": "2021",
      "venue": "Audio-to-Image Cross-Modal Generation"
    },
    {
      "citation_id": "11",
      "title": "Cross-modal text and visual generation: A systematic review. Part 1: Image to text",
      "authors": [
        "M Å»elaszczyk",
        "J MaÅ„dziuk"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.01.008"
    },
    {
      "citation_id": "12",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "A Radford"
      ],
      "year": "2021",
      "venue": "Learning Transferable Visual Models From Natural Language Supervision"
    },
    {
      "citation_id": "13",
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "authors": [
        "P Zhang"
      ],
      "year": "2021",
      "venue": "VinVL: Revisiting Visual Representations in Vision-Language Models"
    },
    {
      "citation_id": "14",
      "title": "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset",
      "authors": [
        "S Chen"
      ],
      "year": "2023",
      "venue": "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset"
    },
    {
      "citation_id": "15",
      "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
      "authors": [
        "Q Zhu"
      ],
      "year": "2022",
      "venue": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
      "doi": "10.1109/TMM.2023.3275873"
    },
    {
      "citation_id": "16",
      "title": "VGGSound: A Large-scale Audio-Visual Dataset",
      "authors": [
        "H Chen",
        "W Xie",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "VGGSound: A Large-scale Audio-Visual Dataset"
    },
    {
      "citation_id": "17",
      "title": "STAViS: Spatio-Temporal AudioVisual Saliency Network",
      "authors": [
        "A Tsiami",
        "P Koutras",
        "P Maragos"
      ],
      "year": "2020",
      "venue": "STAViS: Spatio-Temporal AudioVisual Saliency Network"
    },
    {
      "citation_id": "18",
      "title": "Audiovisual SlowFast Networks for Video Recognition",
      "authors": [
        "F Xiao",
        "Y Lee",
        "K Grauman",
        "J Malik",
        "C Feichtenhofer"
      ],
      "year": "2020",
      "venue": "Audiovisual SlowFast Networks for Video Recognition"
    },
    {
      "citation_id": "19",
      "title": "Open World Compositional Zero-Shot Learning",
      "authors": [
        "M Mancini",
        "M Naeem",
        "Y Xian",
        "Z Akata"
      ],
      "year": "2021",
      "venue": "Open World Compositional Zero-Shot Learning"
    },
    {
      "citation_id": "20",
      "title": "A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning",
      "authors": [
        "D Huynh",
        "E Elhamifar"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00880"
    },
    {
      "citation_id": "21",
      "title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips",
      "authors": [
        "A Miech",
        "D Zhukov",
        "J.-B Alayrac",
        "M Tapaswi",
        "I Laptev",
        "J Sivic"
      ],
      "year": "2019",
      "venue": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips"
    },
    {
      "citation_id": "22",
      "title": "Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning",
      "authors": [
        "E Amrani",
        "R Ben-Ari",
        "D Rotman",
        "A Bronstein"
      ],
      "year": "2020",
      "venue": "Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning"
    },
    {
      "citation_id": "23",
      "title": "End-to-End Learning of Visual Representations from Uncurated Instructional Videos",
      "authors": [
        "A Miech",
        "J.-B Alayrac",
        "L Smaira",
        "I Laptev",
        "J Sivic",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "End-to-End Learning of Visual Representations from Uncurated Instructional Videos"
    },
    {
      "citation_id": "24",
      "title": "Self-Supervised Learning by Cross-Modal Audio-Video Clustering",
      "authors": [
        "H Alwassel",
        "D Mahajan",
        "B Korbar",
        "L Torresani",
        "B Ghanem",
        "D Tran"
      ],
      "year": "2019",
      "venue": "Self-Supervised Learning by Cross-Modal Audio-Video Clustering"
    },
    {
      "citation_id": "25",
      "title": "Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning",
      "authors": [
        "Y Cheng",
        "R Wang",
        "Z Pan",
        "R Feng",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning",
      "doi": "10.1145/3394171.3413869"
    },
    {
      "citation_id": "26",
      "title": "Self-Supervised MultiModal Versatile Networks",
      "authors": [
        "J.-B Alayrac"
      ],
      "year": "2020",
      "venue": "Self-Supervised MultiModal Versatile Networks"
    },
    {
      "citation_id": "27",
      "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos",
      "authors": [
        "B Chen"
      ],
      "year": "2021",
      "venue": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos"
    },
    {
      "citation_id": "28",
      "title": "Routing with Self-Attention for Multimodal Capsule Networks",
      "authors": [
        "K Duarte"
      ],
      "year": "2021",
      "venue": "Routing with Self-Attention for Multimodal Capsule Networks"
    },
    {
      "citation_id": "29",
      "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos",
      "authors": [
        "A Rouditchenko"
      ],
      "year": "2020",
      "venue": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos"
    },
    {
      "citation_id": "30",
      "title": "Modality-Transferable Emotion Embeddings for Low-Resource Multimodal Emotion Recognition",
      "authors": [
        "W Dai",
        "Z Liu",
        "T Yu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Modality-Transferable Emotion Embeddings for Low-Resource Multimodal Emotion Recognition"
    },
    {
      "citation_id": "31",
      "title": "Multitask Multi-database Emotion Recognition",
      "authors": [
        "M Vu",
        "M Beurton-Aimar",
        "S Marchand"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/ICCVW54120.2021.00406"
    },
    {
      "citation_id": "32",
      "title": "Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation",
      "authors": [
        "A Shenoy",
        "A Sardana"
      ],
      "year": "2020",
      "venue": "Multilogue-Net: A Context Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation",
      "doi": "10.18653/v1/2020.challengehml-1.3"
    },
    {
      "citation_id": "33",
      "title": "Deep Auto-Encoders With Sequential Learning for Multimodal Dimensional Emotion Recognition",
      "authors": [
        "D Nguyen"
      ],
      "year": "2022",
      "venue": "IEEE Trans Multimedia",
      "doi": "10.1109/TMM.2021.3063612"
    },
    {
      "citation_id": "34",
      "title": "Multimodal End-to-End Sparse Model for Emotion Recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Multimodal End-to-End Sparse Model for Emotion Recognition"
    },
    {
      "citation_id": "35",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Detecting expressions with multimodal transformers"
    },
    {
      "citation_id": "36",
      "title": "Self-Attentive Feature-Level Fusion for Multimodal Emotion Detection",
      "authors": [
        "D Hazarika",
        "S Gorantla",
        "S Poria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)",
      "doi": "10.1109/MIPR.2018.00043"
    },
    {
      "citation_id": "37",
      "title": "UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation",
      "authors": [
        "H Luo"
      ],
      "year": "2020",
      "venue": "UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"
    },
    {
      "citation_id": "38",
      "title": "UNITER: UNiversal Image-TExt Representation Learning",
      "authors": [
        "Y.-C Chen"
      ],
      "year": "2019",
      "venue": "UNITER: UNiversal Image-TExt Representation Learning"
    },
    {
      "citation_id": "39",
      "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
      "authors": [
        "M Bain",
        "A Nagrani",
        "G Varol",
        "A Zisserman"
      ],
      "year": "2021",
      "venue": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"
    },
    {
      "citation_id": "40",
      "title": "Attention Bottlenecks for Multimodal Fusion",
      "authors": [
        "A Nagrani",
        "S Yang",
        "A Arnab",
        "A Jansen",
        "C Schmid",
        "C Sun"
      ],
      "year": "2021",
      "venue": "Attention Bottlenecks for Multimodal Fusion"
    },
    {
      "citation_id": "41",
      "title": "Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition",
      "authors": [
        "Y Wu",
        "M Daoudi",
        "A Amad"
      ],
      "year": "2023",
      "venue": "Transformer-based Self-supervised Multimodal Representation Learning for Wearable Emotion Recognition"
    },
    {
      "citation_id": "42",
      "title": "Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition",
      "authors": [
        "Z Wen",
        "W Lin",
        "T Wang",
        "G Xu"
      ],
      "year": "2021",
      "venue": "Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition",
      "doi": "10.3390/biomimetics8020199"
    },
    {
      "citation_id": "43",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "authors": [
        "S Sadok",
        "S Leglaive",
        "R SÃ©guier"
      ],
      "year": "2023",
      "venue": "A vector quantized masked autoencoder for audiovisual speech emotion recognition"
    },
    {
      "citation_id": "44",
      "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text",
      "authors": [
        "H Akbari"
      ],
      "year": "2021",
      "venue": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"
    },
    {
      "citation_id": "45",
      "title": "T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval",
      "authors": [
        "X Wang",
        "L Zhu",
        "Y Yang"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR46437.2021.00504"
    },
    {
      "citation_id": "46",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "47",
      "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors"
    },
    {
      "citation_id": "48",
      "title": "Multi-modal Transformer for Video Retrieval",
      "authors": [
        "V Gabeur",
        "C Sun",
        "K Alahari",
        "C Schmid"
      ],
      "year": "2020",
      "venue": "Multi-modal Transformer for Video Retrieval"
    },
    {
      "citation_id": "49",
      "title": "CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval and captioning",
      "authors": [
        "H Luo"
      ],
      "year": "2022",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2022.07.028"
    },
    {
      "citation_id": "50",
      "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "authors": [
        "Z Fu"
      ],
      "year": "2021",
      "venue": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition"
    },
    {
      "citation_id": "51",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS One",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "52",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "53",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Trans Affect Comput",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "54",
      "title": "Memory Fusion Network for Multi-view Sequential Learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Memory Fusion Network for Multi-view Sequential Learning"
    },
    {
      "citation_id": "55",
      "title": "Learning Factorized Multimodal Representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning Factorized Multimodal Representations"
    },
    {
      "citation_id": "56",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal Transformer for Unaligned Multimodal Language Sequences"
    },
    {
      "citation_id": "57",
      "title": "Integrating Multimodal Information in Large Pretrained Transformers",
      "authors": [
        "W Rahman"
      ],
      "year": "2019",
      "venue": "Integrating Multimodal Information in Large Pretrained Transformers"
    },
    {
      "citation_id": "58",
      "title": "Multi-Task Multi-Modal Self-Supervised Learning for Facial Expression Recognition",
      "authors": [
        "M Halawa",
        "F Blume",
        "P Bideau",
        "M Maier",
        "R Rahman",
        "O Hellwich"
      ],
      "year": "2024",
      "venue": "Multi-Task Multi-Modal Self-Supervised Learning for Facial Expression Recognition"
    },
    {
      "citation_id": "59",
      "title": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "COGMEN: COntextualized GNN based Multimodal Emotion recognitioN"
    },
    {
      "citation_id": "60",
      "title": "MMTM: Multimodal Transfer Module for CNN Fusion",
      "authors": [
        "H Joze",
        "A Shaban",
        "M Iuzzolino",
        "K Koishida"
      ],
      "year": "2019",
      "venue": "MMTM: Multimodal Transfer Module for CNN Fusion"
    },
    {
      "citation_id": "61",
      "title": "Multimodal Emotion Recognition via Convolutional Neural Networks: Comparison of different strategies on two multimodal datasets",
      "authors": [
        "U Bilotti",
        "C Bisogni",
        "M Marsico",
        "S Tramonte"
      ],
      "year": "2024",
      "venue": "Eng Appl Artif Intell",
      "doi": "10.1016/j.engappai.2023.107708"
    },
    {
      "citation_id": "62",
      "title": "MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion Recognition",
      "authors": [
        "P Xiang",
        "C Lin",
        "K Wu",
        "O Bai"
      ],
      "year": "2024",
      "venue": "MultiMAE-DER: Multimodal Masked Autoencoder for Dynamic Emotion Recognition"
    },
    {
      "citation_id": "63",
      "title": "AuxFormer: Robust Approach to Audiovisual Emotion Recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9747157"
    },
    {
      "citation_id": "64",
      "title": "Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks",
      "authors": [
        "L Goncalves",
        "S.-G Leem",
        "W.-C Lin",
        "B Sisman",
        "C Busso"
      ],
      "year": "2023",
      "venue": "Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks"
    },
    {
      "citation_id": "65",
      "title": "Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes",
      "doi": "10.21437/Interspeech.2018-1391"
    },
    {
      "citation_id": "66",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)",
      "doi": "10.1109/SLT.2018.8639583"
    },
    {
      "citation_id": "67",
      "title": "Audio and Video-based Emotion Recognition using Multimodal Transformers",
      "authors": [
        "V John",
        "Y Kawanishi"
      ],
      "year": "2022",
      "venue": "Proceedings -International Conference on Pattern Recognition",
      "doi": "10.1109/ICPR56361.2022.9956730"
    }
  ]
}