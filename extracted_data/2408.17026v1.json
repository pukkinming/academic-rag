{
  "paper_id": "2408.17026v1",
  "title": "From Text To Emotion: Unveiling The Emotion Annotation Capabilities Of Llms",
  "published": "2024-08-30T05:50:15Z",
  "authors": [
    "Minxue Niu",
    "Mimansa Jaiswal",
    "Emily Mower Provost"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Training emotion recognition models has relied heavily on human annotated data, which present diversity, quality, and cost challenges. In this paper, we explore the potential of Large Language Models (LLMs), specifically GPT-4, in automating or assisting emotion annotation. We compare GPT-4 with supervised models and/or humans in three aspects: agreement with human annotations, alignment with human perception, and impact on model training. We find that common metrics that use aggregated human annotations as ground truth can underestimate GPT-4's performance, and our human evaluation experiment reveals a consistent preference for GPT-4 annotations over humans across multiple datasets and evaluators. Further, we investigate the impact of using GPT-4 as an annotation filtering process to improve model training. Together, our findings highlight the great potential of LLMs in emotion annotation tasks and underscore the need for refined evaluation methodologies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding human emotions from written or spoken language is crucial is a key part of studying how computers can interact with us more like humans do. The field has attracted significant research efforts, ranging from word-level analysis  [1, 2]  to building sophisticated neural networks  [3, 4] . Currently, many models demonstrate impressive capabilities in recognizing various human emotions.\n\nThe training of emotion models has relied heavily on datasets with human annotations. However, obtaining emotion annotations is challenging due to the rich, ambiguous and subjective nature of emotions  [5] [6] [7] . The first challenge is to identify the emotion theory that will motivate a particular labeling schema. Common theories include basic emotion theory  [8] , assigning one or more predefined emotion classes to each sample (categorical labels), and the emotion circumplex theory  [9] , rating each sample on continuous scales, such as valence and arousal to reflect the emotion's positivity and intensity (dimensional labels). The process of collecting human annotations involves multiple annotators per sample to accommodate subjective interpretations and possible quality issues, with the final label often determined through aggregation methods like majority voting  [10]  or averaging  [11] . Given the large scale of modern datasets, such annotation processes can be both costly and timeconsuming. Moreover, the complexity of the label space and the difficulty of quality control further add to the challenges.\n\nRecently, the progress in LLMs brings new alternatives. With remarkable proficiency in language modeling across a wide range of scenarios, LLMs show emerging common sense reasoning capability  [12] : they can answer a wide range of nat-ural language reasoning questions through zero-or few-shot prompting, matching or even outperforming supervised models  [13] [14] [15] . What's more, LLMs display an understanding of human emotion and can respond differently to emotional content  [16, 17] . This has inspired research into leveraging LLMs as emotion models to aid emotion annotation processes.\n\nIn this work, we comprehensively assess GPT-4's potential to perform emotion annotations in a zero-shot manner. We first measure its emotion recognition performance and find that it performs comparably to established supervised models as baselines, using human annotations as the ground truth. We then reflect on the differences between GPT-perception and humanperception and evaluate how those differences are perceived by a separate set of human evaluators. Surprisingly, we find that human evaluators consistently prefer the GPT-4 annotations over human annotations. These findings raise important open questions about the suitability of conventional \"ground truth\" concepts and evaluation practices, especially as models begin to approach human-level performance. Further, we analyze how label formats (categorical vs. dimensional) affect GPT-4's performance, and we explore the feasibility of applying GPT-4 as a quality checker for existing annotations. We demonstrate that GPT-4 can identify potentially low-quality annotations and help with curating a cleaner and more efficient training set.\n\nIn summary, our research reveals the great potential of utilizing LLMs for emotion annotation tasks, offers new insights into their capabilities across label formats, and highlights the challenges involved in their evaluation. We also release the GPT-4 annotations on the entire GoEmotions dataset, along with our code and prompts 1  .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Affective capabilities of LLMs. Many evaluation studies have shown that LLMs are equipped with emotional intelligence: they are able to derive appraisals of given situations  [18] , identify the emotions and emotion causes in dialogues  [16] , and respond with emotional support  [16, 17, 19 ]. Yet, they are generally found to be inferior to humans: a few works that developed benchmarks for assessing emotional intelligence consistently indicate a notable gap in complex emotion reasoning between state-of-the-art LLMs and human performance  [17, 20, 21] . There have been a few works that evaluate GPT's zero-or fewshot capability of emotion recognition from text or speech input  [22] [23] [24] [25] . However, the diversity of emotion label spaces are rarely discussed. Besides, existing works adopt evaluation criteria that rely on automatic metrics against human annotations as the ground truth. In this work, we show that such metrics can LLMs as data annotators. Despite their remarkable capabilities in various language understanding tasks  [14, 15, 26] , the high operational costs and impracticality of deployment on edge devices have focus efforts towards using LLMs to enhance annotation processes for training more compact models. GPT has been recognized for its potential in sample annotation  [27]  and generation  [28, 29] . In a closer look, LLMs especially excel at tasks with limited and well-defined label sets  [28] .\n\nPrompting methods. It is widely acknowledged that LLMs are sensitive to the format and word choices in the prompts  [30] , making prompts the key factor in the successful application of LLMs. There are two common ways of prompting  [31] : cloze prompts, which involve a fill-in-the-blank approach (e.g.,\"I feel [X]. I finally got that promotion!\"), and prefix prompts, where the model extends a given prompt (e.g., \"'I finally got the promotion!' What is the speaker's emotion?\"). Given GPT-4's pretraining on generation tasks, our study employs prefix prompts. There have been a lot of work exploring different techniques of prompting that could bring a significant improvement in the models' responses  [31] [32] [33] . The efficiency of different prompting techniques is not the focus of this paper. We follow the common effective practices without dedicated prompt engineering (details in Section 4.1).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "We use four publicly available emotion recognition datasets for our analysis, encompassing a variety of label representations and diverse text domains (Table  1 ). Considering the substantial volume of these datasets, we first select a subset of 500 samples from each for GPT-4 annotation and subsequent analysis.\n\nInternational Survey on Emotion Antecedents and Reactions (ISEAR)  [34]  is an outcome of a psychological study aiming to understand the antecedents and reactions to seven basic emotions (joy, fear, anger, sadness, disgust, shame, guilt). It consists of 7.6k samples from firsthand emotional reports in text form. We randomly select 500 samples for our analysis.\n\nSemEval 2017 Task 4 (SemEval)  [10]  is part of the International Workshop on Semantic Evaluation. It consists of Twitter text samples, each annotated with one or more of 11 emotion classes. Since this dataset is very unbalanced, we conduct weighted sampling to select the 500 samples by applying log inverse frequency weighting to the labels, in order to include more emotion labels in our analysis. If a sample carries multiple emotions, the weighting is determined by the rarest label.\n\nGoEmotions  [35]  is a comprehensive dataset with 58k samples derived from Reddit comments, designed for finegrained emotion detection. It is characterized by its extensive range of 27 distinct emotion categories, including admiration, remorse, gratitude, etc. Each sample can be assigned one or more emotion labels, as well as an extra \"neutral\" option. We also apply log inverse frequency weighting in our selection of 500 samples, to address the label imbalance.\n\nEmobank  [11]  consists of 10k English sentences balancing multiple genres (newspapers, blogs, etc.). The samples are annotated with dimensional emotion labels in the Valence-Arousal-Dominance (VAD) space on a 5-point scale. We focus on the valence score in this study, as it is most commonly included in related literature  [25, 36] . Notably, EmoBank distinguishes between the emotional perceptions of writers and readers  [37] ; we use the reader's annotations, to be consistent with the perspective of GPT-4. We weight the samples by their log deviation from neutral score, to encourage the inclusion of stronger emotional content. I.e., wi = log|Vi -3|.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gpt-4 Prompting",
      "text": "For each of the three emotion classification datasets, we collect two sets of GPT-4 annotations. In the first set of annotations, we ask GPT-4 to conduct classification annotations by making selections from a pre-determined set of emotion classes. Informed by the common prompting techniques detailed in Section 2, we follow an instruction-based prompting method, which is consistent with the tasks given to human annotators. We try to give GPT-4 similar instructions as those given to humans, based on the descriptions in the GoEmotions paper  [35] . Additionally, we set up a persona in the beginning, which has been found to be effective in our preliminary experiments. As an example, the prompt we use for multi-label classification datasets (GoEmotions and SemEval) is shown below.\n\n\"You are an emotionally-intelligent and empathetic agent. You will be given a piece of text, and your task is to identify all the emotions expressed by the writer of the text. You are only allowed to make selections from the following emotions, and don't use any other words:  [Options] . Only select those ones for which you are reasonably confident that they are expressed in the text. If no emotion is clearly expressed, select 'neutral'. Reply with only the list of emotions, separated by comma.\"\n\nWe make minimal modifications as needed for other tasks/datasets and all prompts we use across datasets/tasks can be found in our released code.\n\nWe then ask GPT-4 to freely generate descriptors of the expressed emotion, without a given range of options. We compare the generated descriptors with the classification results to understand how the granularity of emotion labels affect GPT-4's performance (Section 5.1). For Emobank, we use a similar prompt with the expected response being a integer number from 1 to 5, indicating the perceived valence of the expressed emotion. Using these prompts, we obtain GPT-4 emotion annotations on the 2000 samples selected from four datasets. We additionally obtain classification annotations on all of the GoEmotions dataset using GPT-4 for our model training analysis (Section 5.2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Automatic Evaluation Metrics",
      "text": "Following common approaches in previous work, we evaluate GPT-4's performance on two aspects: 1) agreement with human annotations  [28, 36] , and 2) potential to improve model performance when GPT-4's annotations are used as training data  [24, 25]  to train smaller models, in this case implemented by fine-tuning BERT. For classification, we use Unweighted Average Recall (UAR) and Macro-averaged F-1 (Macro-F1) scores as the metrics. UAR measures a model's ability to correctly identify instances of each class with equal importance, while Macro-F1 assesses the balance between precision and recall for all classes. For regression, we use Pearson Correlation Coefficient (PCC) to measure the strength and direction of the linear correlation, and Mean Absolute Error (MAE) to reflect the average error magnitude.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Supervised Model: Finetuned Bert",
      "text": "We finetune BERT  [38]  models on the full training set of each dataset to serve as a supervised baseline. BERT is one of the most commonly used models for text classification tasks  [39]  and has been used as a benchmark model for the GoEmotions dataset  [35] . Besides, its smaller size means it can be run on a single GPU, so we also use it as our base model when comparing the training efficiency of different annotation sources.\n\nWe use the same finetuning settings across all experiments: we use the \"bert-base-uncased\" model implemented in the transformers library, which has 110M parameters. We add a linear layer on top of the base model, and finetune the whole model on a training set with an AdamW optimizer and learning rate = 1e-5. We optimize a Binary Cross Entropy loss for multilabel classification tasks, a Cross Entropy loss for the singlelabel classification task, and a Mean Squared Error loss for the regression task. We train the model for 10 epochs, and use the model with best performance on a validation set for testing. For the regression task, we find the model not yet converged after 10 epochs, so we train the model for 30 epochs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Human Evaluation",
      "text": "Human annotations often contain inaccuracies  [40] , thus metrics based solely on human annotations can be biased. Therefore, we conduct a human evaluation study on samples where GPT-4 and the human evaluators do not agree, aiming to incorporate human perspectives into our evaluation.\n\nWe recruited four students from the University of Michigan as our evaluators, aged between 19 to 28 and including two females. They were presented with annotations from two sources (i.e., human vs. GPT-4 classification or GPT-4 classification vs. generation) without identification, and were asked to choose the one which they thought \"better and more accurately describes the emotion expressed in the text\". Each sample was evaluated by two evaluators, who were given an option to indicate uncertainty on each sample. For the classification tasks (ISEAR, SemEval and GoEmotions), we first found all samples that were annotated with disjoint sets of labels by the two sources. Note that we did not adjudicate annotations if they contained overlapping emotion labels as the differences can be subtle (e.g., \"anger\" vs. \"anger, annoyance\"). The annotations were randomized and mixed from three different datasets to reduce the likelihood that evaluators could recognizing patterns associated with a specific source. For the regression task (Emobank), it is harder for evaluators to decide whether a given number is a more or less accurate valence rating for a given sample, especially when the ratings are close. Thus, we adopted a relative evaluation schema  [41] . We found pairs in disagreement where one annotation source assigns sample A a significantly (> 1 standard deviation) higher rating than sample B, while the other indicates reversed significance. We asked evaluators to indicate which of the two samples in each pair should have the higher valence. The order of the samples was randomized.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gpt-4 Zero-Shot Performance",
      "text": "We first compare GPT-4 and human classification annotations, with a focus on their disagreements. We visualize the disagree- ments on the ISEAR dataset as a confusion matrix in Figure  1  (we select ISEAR because it has the fewest number of classes and is the clearest to show). GPT-4 aligns well with human annotations on most samples, as indicated by the numbers on the diagonal. It's worth noting that confusions mostly happen among similar emotions, and the confusion between a positive emotion and a negative one is rare. Further, it shows that the confusion between classes is not symmetric, indicating some systematic differences between human and GPT-4 annotations. For example, GPT-4 tends to perceive more shame than guilt  (18) , but seldom marks human-perceived shame as guilt (3).\n\nWe then quantitatively evaluate the zero-shot efficacy of GPT-4, and compare its performance to a BERT model finetuned to predict the human evaluations. Our findings are in line with prior work in this space  [28]  that the two approaches perform comparably, and GPT-4 performs slightly better than BERT on the easier 7-class classification dataset ISEAR, but was more challenged on the multi-label classification datasets SemEval and GoEmotions (Table  2 ).\n\nHowever, the subsequent human evaluation reveals a different trend and suggests that the automatic metrics may have underestimated GPT-4 performances. As shown in Figure  2 (a)  with the colored bars representing the ratio of human preference obtained on each annotation source (Human vs. GPT-4 classification), human evaluators prefer labels from GPT-4 on more samples than those from human annotators, consistently across datasets: ISEAR 62.3%, SemEval 68.2%, GoEmotions 71.1%. This trend holds for each individual annotator, ranging from 64.1% to 71.2%.\n\nFurther, Figure  2  (b) shows that GPT-4 generated emotion descriptions are preferred to GPT-4 classification annotations by human evaluators, indicating that without the pre-defined classes as a restriction, GPT-4 generates emotion descriptions that were more often preferred by human evaluators. This trend is more significant when the label set is small, like ISEAR (7classes 65.4%) and SemEval (11 classes, 73.8%), compared to GoEmotions (28 classes, 55.2%). This comparison indicates that it's beneficial to have a larger label space, which is more likely to encompass the precise emotion labels needed for accurate annotation. The results in Figure  2  (a) highlight the proficiency of GPT-4 in navigating a wide range of labels, further demonstrating its utility in complex emotion recognition tasks.\n\nOn the valence regression task, GPT-4 significantly outperforms fine-tuned BERT when measured by PCC, but it has a larger MAE (Table  2 ). The large MAE can be explained by the highly centralized distribution of human annotations (standard deviation for human evaluations was 0.54, vs. 1.16 for GPT-4) and the fact that GPT-4 predicts integer-valued numbers while the human evaluations are continuous (e.g., averages of multiple evaluators). However, the large PCC value (0.764) indicates that GPT-4 can identify relative emotional valence. Human evalua-  tion also finds an overall 56% agreement with GPT-4 rather than the original human annotations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Impact On Model Training",
      "text": "We then investigate whether the labels resulting from GPT-4 classification annotations can be used to train emotion recognition models. We focus on the GoEmotions dataset for this study, using its original train/val/test split.\n\nWe compare the performance of a BERT model when it is fine-tuned on the whole training set (Ntrain = 42,278) with human annotations to one trained using the GPT-4 annotations. Additionally, we downsample the training data, retaining only data where the original human evaluations agree with the GPT-4 annotations. We refer to this set as the filtered human set (Human-F, Ntrain = 19,130). Note that this set potentially contains easier samples, compared to both the original human evaluations and GPT-4 annotation labels, because ambiguous samples are more likely to receive different annotations from human and GPT-4, and would thus be filtered out. We test the model on the original human evaluation data (Ntest = 5,283), the GPT-4 annotations (Ntest = 5,283), and the Human-F test set (Ntest = 2,409). We add an extra test set that we refer to as the \"adjudicated\" test set (Ntest = 405), which is a subset of the 500 samples used in the human preference evaluation experiment. The set is first populated with samples that have overlapping labels from the original human evaluations and GPT-4 (N = 217), and either the human or the GPT-4 label is selected by random. The remaining samples exhibit disagreement between the two sources. We select the subset of samples where humans exhibited a clear preference for either the original human evaluation or GPT-4 label 2 as the final label (N = 188). The performance on the adjudicated test set is our main metric, because the annotations have been adjudicated and are considered to be more reliable than the raw human or GPT-4 annotations.\n\nIn Table  3 , the models perform most accurately when trained and tested on the same type of annotation. When models are trained on human annotations and tested with GPT-4 annotations (and vice versa) there are notable performance decreases. This indicates that the models learn a systematic difference between human and GPT-4 annotations, which echos our findings in Section 5.1. On the adjudicated test set, we find that the model trained on GPT-4 annotations outperformed the model trained on human annotations by a large margin (0.524 vs. 0.392, respectively), again pointing to the systematic differ-2 Samples where the human evaluators did not agree on the preferred annotation were not included in this sample, 19% of the samples. ences between the two annotation sources. We find that models trained on the filtered subset of the original human evaluations estimate the labels of the adjudicated data more accurately than models trained on the full set of the original human evaluations (0.430 vs. 0.392, respectively). This result is notable given that the Human-F set is only 45% of the size of the original Human set (N = 19, 130 vs. N = 42, 278, respectively).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Discussion, Limitations And Conclusion",
      "text": "In this work, we evaluate GPT-4's emotion recognition capability and find that its zero-shot performance is comparable to supervised models. Our human evaluation study reveals that GPT-4 annotations are preferred to human annotations by our human evaluators, and GPT-4 is good at handling a wide range of options in emotion classification tasks. We also show that models trained on GPT-4 annotations are subsequently better at predicting the labels amongst the adjudicated subset of data. These results highlight the potential of LLMs to be applied in emotion recognition applications. Several factors may contribute to the observed preference for GPT-4 annotations. First, humans make mistakes, and the increased cognitive load on more complex label spaces could have increased the vulnerability  [42] . Additionally, given the inherent subjectivity and ambiguity of emotion annotations  [7] , different preferences could indicate variations in annotation perspectives or reflect a lack of diversity in the annotation process. Further exploration is needed to identify the underlying reason. Our findings emphasize the need to reconsider conventional notions of \"ground truth\" and explore novel evaluation metrics as LLMs approach and surpass human-level performance.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (we select ISEAR because it has the fewest number of classes",
      "page": 3
    },
    {
      "caption": "Figure 2: (b) shows that GPT-4 generated emotion",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) highlight the pro-",
      "page": 3
    },
    {
      "caption": "Figure 1: Disagreements between human",
      "page": 4
    },
    {
      "caption": "Figure 2: Human preference ratio comparing human annotations, GPT-4 classiﬁca-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: , the models perform most accurately when",
      "data": [
        {
          "68": "0",
          "0": "3",
          "1": "3",
          "2": "6"
        },
        {
          "68": "1",
          "0": "1",
          "1": "3",
          "2": "20"
        },
        {
          "68": "1",
          "0": "1",
          "1": "1",
          "2": "7"
        },
        {
          "68": "0",
          "0": "0",
          "1": "60",
          "2": "3"
        },
        {
          "68": "0",
          "0": "0",
          "1": "0",
          "2": "61"
        },
        {
          "68": "4",
          "0": "31",
          "1": "1",
          "2": "10"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: , the models perform most accurately when",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "hu": "gp",
          "man": "t cls.",
          "cls.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: , the models perform most accurately when",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "hu": "gp",
          "m": "t c",
          "an c": "ls.",
          "ls.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: , the models perform most accurately when",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "gpt c": "gpt g",
          "ls.": "en."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: , the models perform most accurately when",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "gpt\ngpt",
          "Column_5": "cl\nge",
          "Column_6": "s.\nn.",
          "Column_7": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
      "authors": [
        "S Mohammad"
      ],
      "year": "2018",
      "venue": "Proceed-ings of the 56th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "3",
      "title": "Beyond sentiment analysis: A review of recent trends in text based sentiment analysis and emotion detection",
      "authors": [
        "L Hung",
        "S Alias"
      ],
      "year": "2023",
      "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics"
    },
    {
      "citation_id": "4",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "5",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Estimating the uncertainty in emotion attributes using deep evidential regression",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2023",
      "venue": "Estimating the uncertainty in emotion attributes using deep evidential regression",
      "arxiv": "arXiv:2306.06760"
    },
    {
      "citation_id": "7",
      "title": "Do multimodal emotion recognition models tackle ambiguity",
      "authors": [
        "H Tran",
        "I Falih",
        "X Goblet",
        "E Nguifo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Workshop on People in Vision, Language, and the Mind"
    },
    {
      "citation_id": "8",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "10",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "11",
      "title": "Semeval-2017 task 4: Sentiment analysis in twitter",
      "authors": [
        "S Rosenthal",
        "N Farra",
        "P Nakov"
      ],
      "year": "2019",
      "venue": "Semeval-2017 task 4: Sentiment analysis in twitter",
      "arxiv": "arXiv:1912.00741"
    },
    {
      "citation_id": "12",
      "title": "EmoBank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of EACL"
    },
    {
      "citation_id": "13",
      "title": "Towards reasoning in large language models: A survey",
      "authors": [
        "J Huang",
        "-C Chang"
      ],
      "year": "2022",
      "venue": "Towards reasoning in large language models: A survey",
      "arxiv": "arXiv:2212.10403"
    },
    {
      "citation_id": "14",
      "title": "A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets",
      "authors": [
        "M Laskar",
        "M Bari",
        "M Rahman",
        "M Bhuiyan",
        "S Joty",
        "J Huang"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "15",
      "title": "Language models are Few-Shot learners",
      "authors": [
        "T Brown"
      ],
      "year": "2020",
      "venue": "Language models are Few-Shot learners"
    },
    {
      "citation_id": "16",
      "title": "Finetuned language models are Zero-Shot learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Zhao",
        "K Guu",
        "A Yu",
        "B Lester",
        "N Du",
        "A Dai",
        "Q Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are Zero-Shot learners"
    },
    {
      "citation_id": "17",
      "title": "Is Chat-GPT equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is Chat-GPT equipped with emotional dialogue capabilities?"
    },
    {
      "citation_id": "18",
      "title": "Emotionally numb or empathetic? evaluating how LLMs feel using EmotionBench",
      "authors": [
        "J.-T Huang",
        "M Lam",
        "E Li",
        "S Ren",
        "W Wang",
        "W Jiao",
        "Z Tu",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally numb or empathetic? evaluating how LLMs feel using EmotionBench"
    },
    {
      "citation_id": "19",
      "title": "Is GPT a computational model of emotion?",
      "authors": [
        "A Tak",
        "J Gratch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "20",
      "title": "Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus",
      "authors": [
        "C Li",
        "J Wang",
        "K Zhu",
        "Y Zhang",
        "W Hou",
        "J Lian",
        "X Xie"
      ],
      "year": "2023",
      "venue": "Emotionprompt: Leveraging psychology for large language models enhancement via emotional stimulus"
    },
    {
      "citation_id": "21",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "22",
      "title": "EmoBench: Evaluating the emotional intelligence of large language models",
      "authors": [
        "S Sabour",
        "S Liu",
        "Z Zhang",
        "J Liu",
        "J Zhou",
        "A Sunaryo",
        "J Li",
        "T Lee",
        "R Mihalcea",
        "M Huang"
      ],
      "year": "2024",
      "venue": "EmoBench: Evaluating the emotional intelligence of large language models"
    },
    {
      "citation_id": "23",
      "title": "Bias in emotion recognition with ChatGPT",
      "authors": [
        "N Wake",
        "A Kanehira",
        "K Sasabuchi",
        "J Takamatsu",
        "K Ikeuchi"
      ],
      "year": "2023",
      "venue": "Bias in emotion recognition with ChatGPT"
    },
    {
      "citation_id": "24",
      "title": "Affect recognition in conversations using large language models",
      "authors": [
        "S Feng",
        "G Sun",
        "N Lubis",
        "C Zhang",
        "M Gašić"
      ],
      "year": "2023",
      "venue": "Affect recognition in conversations using large language models"
    },
    {
      "citation_id": "25",
      "title": "Can large language models aid in annotating speech emotional data? uncovering new frontiers",
      "authors": [
        "S Latif",
        "M Usama",
        "M Malik",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Can large language models aid in annotating speech emotional data? uncovering new frontiers"
    },
    {
      "citation_id": "26",
      "title": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "authors": [
        "Z Zhang",
        "L Peng",
        "T Pang",
        "J Han",
        "H Zhao",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Refashioning emotion recognition modelling: The advent of generalised large models"
    },
    {
      "citation_id": "27",
      "title": "Text classification via large language models",
      "authors": [
        "X Sun",
        "X Li",
        "J Li",
        "F Wu",
        "S Guo",
        "T Zhang",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Text classification via large language models",
      "arxiv": "arXiv:2305.08377"
    },
    {
      "citation_id": "28",
      "title": "ChatGPT outperforms crowd workers for text-annotation tasks",
      "authors": [
        "F Gilardi",
        "M Alizadeh",
        "M Kubli"
      ],
      "year": "2023",
      "venue": "Proc. Natl. Acad. Sci. U. S. A"
    },
    {
      "citation_id": "29",
      "title": "Is GPT-3 a good data annotator",
      "authors": [
        "B Ding",
        "C Qin",
        "L Liu",
        "Y Chia",
        "B Li",
        "S Joty",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "From humans to machines: Can ChatGPT-like LLMs effectively replace human annotators in NLP tasks",
      "authors": [
        "S Thapa",
        "U Naseem",
        "M Nasim"
      ],
      "year": "2023",
      "venue": "From humans to machines: Can ChatGPT-like LLMs effectively replace human annotators in NLP tasks"
    },
    {
      "citation_id": "31",
      "title": "Exploring the sensitivity of llms' decision-making capabilities: Insights from prompt variation and hyperparameters",
      "authors": [
        "M Loya",
        "D Sinha",
        "R Futrell"
      ],
      "year": "2023",
      "venue": "Exploring the sensitivity of llms' decision-making capabilities: Insights from prompt variation and hyperparameters",
      "arxiv": "arXiv:2312.17476"
    },
    {
      "citation_id": "32",
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "P Liu",
        "W Yuan",
        "J Fu",
        "Z Jiang",
        "H Hayashi",
        "G Neubig"
      ],
      "year": "2023",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "33",
      "title": "Using cognitive psychology to understand GPT-3",
      "authors": [
        "M Binz",
        "E Schulz"
      ],
      "year": "2023",
      "venue": "Proc. Natl. Acad. Sci. U. S. A"
    },
    {
      "citation_id": "34",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "How universal and specific is emotional experience? evidence from 27 countries on five continents",
      "authors": [
        "H Wallbott",
        "K Scherer"
      ],
      "year": "1986",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "36",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting",
      "authors": [
        "T Feng",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "Foundation model assisted automatic speech emotion recognition: Transcribing, annotating, and augmenting"
    },
    {
      "citation_id": "38",
      "title": "Readers vs. writers vs. texts: Coping with different perspectives of text understanding in emotion annotation",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2017",
      "venue": "Proceedings of the 11th linguistic annotation workshop"
    },
    {
      "citation_id": "39",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "40",
      "title": "Comparing bert against traditional machine learning text classification",
      "authors": [
        "S González-Carvajal",
        "E Garrido-Merchán"
      ],
      "year": "2020",
      "venue": "Comparing bert against traditional machine learning text classification",
      "arxiv": "arXiv:2005.13012"
    },
    {
      "citation_id": "41",
      "title": "Crowd-sourcing (who, why and what)",
      "authors": [
        "H Ikediego",
        "M Ilkan",
        "A Abubakar",
        "F Bekun"
      ],
      "year": "2018",
      "venue": "International Journal of Crowd Science"
    },
    {
      "citation_id": "42",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "43",
      "title": "A comparison of emotion annotation approaches for text",
      "authors": [
        "I Wood",
        "J Mccrae",
        "V Andryushechkin",
        "P Buitelaar"
      ],
      "year": "2018",
      "venue": "Information"
    }
  ]
}