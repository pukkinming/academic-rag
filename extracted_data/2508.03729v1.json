{
  "paper_id": "2508.03729v1",
  "title": "Privileged Contrastive Pretraining For Multimodal Affect Modelling",
  "published": "2025-07-30T09:48:23Z",
  "authors": [
    "Kosmas Pinitas",
    "Konstantinos Makantasis",
    "Georgios N. Yannakakis"
  ],
  "keywords": [
    "affective computing",
    "arousal",
    "valence",
    "privileged information",
    "representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Figure 1: Visualising the proposed Privileged Contrastive Pretraining (PriCon) framework. The top figure illustrates the traditional end-to-end training of an affect model based on a set multimodal signals. The middle figure illustrates a LUPI paradigm for affect modelling, by which the student model learns affect labels from modalities available in the wild (e.g., visual frames in this example) while the teacher model uses additional multimodal signals available in the lab (e.g., audio and physiological signals in this example) during training. The bottom figure visualises the high-level concept of PriCon that trains a teacher model via SCL prior to transferring knowledge to the student via LUPI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing (AC) has made significant leaps forward in recent years due to advances in deep learning  [2, 9, 17, [35] [36] [37] , which directly enhanced the predictive capacity of affect models. A major AC challenge remains, however: transferring models trained in controlled (in-vitro) environments to real-world (in-vivo) settings. Although laboratory conditions enable precise data collection, realworld scenarios introduce noise, privacy concerns, and hardware constraints, widening the in-vitro to in-vivo gap.\n\nAs a response to the above challenges this paper introduces a unified framework that combines the advantages of Supervised Contrastive Learning (SCL) and the Learning Using Privileged Information (LUPI) paradigm  [38, 40] . The introduced Privileged Contrastive Pretraining (PriCon) framework both exploits additional modalities during training as privileged information for the model and improves the generalisability of derived affect models deployed in in-vivo scenarios (see Figure  1 ). While LUPI allows PriCon to leverage richer data during training than what is available at test time, SCL enhances the quality of learned representations by encouraging class-wise separation in the feature space. We integrate these two components within PriCon to address two key hypotheses. First, LUPI can enhance affect detection under real-world (in-vivo) conditions, producing performances comparable to in-vitro models (H1). Second, PriCon further enhances the effectiveness of in-vivo LUPI models (H2). Both hypotheses are evaluated on the RECOLA  [29]  and AGAIN  [21]  datasets, across different affective dimensions and modalities.\n\nFor testing H1, we incorporate fine-grained image features and fused data (i.e., image frames and features) as privileged information during in-vitro training, while using only frames for in-vivo testing. This approach reflects real-world limitations, where additional modalities (e.g., physiological signals) are impractical due to privacy and hardware constraints. For testing H2, PriCon is applied to in-vitro models, refining their representations and improving their transferability to in-vivo settings. Our results show that privileged information significantly enhances model performance (validating H1), while training privileged models via PriCon further improves in-vivo performance (validating H2). Notably, Pri-Con trained models achieve performances comparable to those obtained from models trained on all modalities.\n\nThis paper makes several key contributions towards advancing affect modelling in real-world settings. First, we evaluate the LUPI paradigm for affect modelling in games and dyadic interactions, demonstrating its potential in dynamic environments. Second, we introduce the PriCon framework that boosts the robustness and transferability of in-vitro LUPI models via SCL. Third, we validate the approach introduced on both RECOLA and AGAIN datasets, showing its robustness across affective dimensions (arousal and valence) and tasks (games vs. dyadic interactions). These findings underscore the benefits of privileged contrastive pretraining in creating more generalisable affective models for real-world applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "This section reviews related work on the two key learning paradigms featured in PriCon. Specifically, we survey learning using privileged information and contrastive learning under the lens of affect modelling.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Lupi For Affect Modelling",
      "text": "While both distillation and LUPI involve a teacher-student framework, they serve different purposes. Traditional knowledge distillation focuses on compressing knowledge by training a student model to mimic the outputs of a more complex teacher model. For instance, Sun et al.  [33]  proposed a novel technique for microexpression detection that distils knowledge from a pre-trained deep teacher neural network to a shallow student neural network via a teacher-student correlative framework enabling the student to capture fine-grained details necessary for accurate micro-expression recognition. Similarly, Liu et al.  [12]  developed a cross-modal consistency modelling-based knowledge distillation framework for image-text sentiment classification of social media data. By aligning the information between images and text, the student model can effectively capture the sentiment expressed in multimodal social media posts.\n\nSeveral affective computing problems have asymmetric train and test input distributions. This discrepancy means that the training phase might have access to richer or more diverse data than what is available during testing. Hence, it is not surprising that the LUPI paradigm has started to become popular for AC research. LUPI addresses this asymmetry by allowing the use of additional information during training that is not available during testing. Makantasis et al.  [16]  introduced a ranking model that treats additional training information as privileged information to rank affect states. This approach leverages richer data available during training, such as audio or contextual information, to improve the ranking model's accuracy in predicting affective states. In another study  [18] , the same authors predicted arousal from gameplay footage while treating telemetry and heart rate as privileged information. Using detailed physiological and telemetry data during training, the model could better capture the nuances of player arousal based solely on gameplay footage during testing. Zhang et al.  [46]  proposed a LUPI method to distil EEG representations via capsule-based architectures for both classification (positive, negarive, neutral emotion) and regression (vigilance) tasks. The capsule-based architectures captured spatial hierarchies in the data, making the distilled knowledge more effective for the student model.\n\nThis work extends LUPI-based affect modelling by applying it to affect classification, a previously unexplored area. Unlike prior studies that focus on ranking affect states or knowledge distillation for efficiency, our approach leverages multimodal privileged information to improve the discriminative power of the unimodal student model, enabling more accurate classification of affective states. Additionally, we enhance model robustness by pretaining the teacher models via SCL, improving generalisation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Contrastive Learning For Affect Modelling",
      "text": "Contrastive learning techniques are among the most widely applied methods for learning representation. Notably, Li et al.  [10]  investigated the impact of unsupervised representation learning for speech emotion recognition. Their study demonstrated that the proposed contrastive predictive coding method based on InfoNCE produced representations that achieved state-of-the-art performance across the activation, valence, and dominance dimensions.\n\nBuilding on these advances, Mai et al.  [15]  introduced a novel hybrid contrastive learning framework. This framework integrates intra-modal, inter-modal, and semi-contrastive learning to enable the model to explore cross-modal interactions and preserve interclass relationships, thereby reducing the modality gap. Their approach demonstrated significant improvements in capturing the complex relationships between different modalities, such as audio and visual data, in emotion recognition tasks. Pinitas et al.  [24]  employed supervised contrastive learning on fine-grained multimodal features to develop robust arousal-infused representations. Their work underscored the importance of incorporating supervised learning signals to enhance the quality of representations, leading to higher arousal classification accuracy than the end-to-end alternatives. Yang et al.  [43]  proposed an innovative low-dimensional supervised cluster-level contrastive learning method. This approach reduces the high-dimensional SCL space to a more manageable three-dimensional affect representation. By efficiently compressing the feature space, their method facilitates more effective and computationally efficient emotion recognition while maintaining high performance. Recently, Pinitas et al.  [25]  introduced a novel few-shot representation learning framework that fine-tuned self-supervised models for engagement state prediction across FPS games outperforming end-to-end baselines.\n\nUnlike the aforementioned works that focus on unsupervised, hybrid, or cluster-level contrastive learning for affect modelling, this paper integrates supervised contrastive learning within the LUPI paradigm with the aim to improve in-vitro to in-vivo generalisation. Additionally, the focus on affect classification in games and dyadic interactions, rather than general emotion recognition, sets this study apart by tackling dynamic and interactive environments where real-time adaptability is crucial and very much needed.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "This section first describes the use of privileged information and supervised contrastive learning in a concise way and then moves on to outline the model architectures employed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Learning Using Privileged Information",
      "text": "Learning using privileged information  [38, 40]  addresses problems characterised by an asymmetric distribution of information between training and test time. LUPI provides the means to transfer knowledge from all available modalities to a machine learning model that makes predictions using only a subset of these modalities  [13, 31] . As far as the RECOLA database is concerned, we treat as privileged the information that corresponds to physiology and ausiovisual features provided by the database creators  [29] . For AGAIN, we consider fine-grained gameplay features as privileged information  [21] . Our choice is justified by the fact that capturing physiology requires specialised sensors, while constructing physiology and audiovisual features implies the employment of specific software algorithms. In the same vein, information about the state of the game (e.g., number and actions of enemies) requires access to the game engine itself. On the contrary, information that comes from raw footage frames is considered prevalent information as it can be captured using conventional cameras that can be available at both training and test times.\n\nThis study explores the use of privileged information with neural network-based affect models. Following  [6, 13, 39] , we represent privileged knowledge-i.e., information available only during training, such as physiological signals or high-resolution feature embeddings-within the output of a neural network that has been trained to make predictions based on either all available modalities (privileged + standard) or on privileged information alone. This model is called teacher. A trained teacher model can transfer knowledge obtained through privileged information to another model called student. The transfer of knowledge can be achieved by feeding the model with only those modalities of information that are available in the wild and forcing it during training to balance (via hyperparameter ğ›¼) between learning the task (cross-entropy loss ğ¿ ğ¶ğ¸ ) and learning prediction distributions that match those of the teacher model (KL divergence loss ğ¿ ğ¾ğ¿ ):\n\nThe resulting optimisation objective ğ¿ ğ‘ is shown in Eq.  (1) . Where ğ‘“ (ğ‘¥) and ğ‘”( x) are the outputs of the student and teacher models, respectively, while x refers to the privileged modalities. It should be noted that after training, the student model makes predictions based only on the information that is available in the wild, without any dependence on the teacher model or privileged information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Privileged Contrastive Pretraining",
      "text": "A central component of the PriCon framework is the pretraining of teacher models using SCL, which lays the foundation for effective privileged supervision. By leveraging class label information, SCL guides the formation of semantically meaningful sample pairs, resulting in highly discriminative representations. This results in an embedding space where similar instances are clustered together, and dissimilar ones are pushed apart, enhancing the model's capacity to generalise.\n\nIn the PriCon framework, this teacher pretraining stage is not merely auxiliary but is instrumental in learning rich, class-consistent representation structure. Specifically, for each input ğ‘¥ ğ‘– (referred to as the anchor), SCL constructs a set of positives ğ‘ƒ ğ‘– , which includes all other instances in the batch sharing the same class label, and contrasts them against negatives-instances from different classes. This structured approach enhances the teacher's ability to model finegrained intra-class similarities and inter-class separability, which is later transferred to the student model through the LUPI paradigm.\n\nCompared to traditional supervised learning, which typically optimises for loss minimisation without explicitly shaping the latent space, SCL provides a more expressive training signal. By encouraging consistent semantic alignment in the representation space, it equips the teacher with a more nuanced understanding of class distributions-an essential property for the downstream knowledge transfer in PriCon. Formally, the supervised contrastive loss ğ¿ ğ‘†ğ¶ is defined as:\n\nwhere ğ¼ is a set that includes all samples and ğ‘ƒ ğ‘– is the set that includes only the samples that are assigned to the same class as ğ‘–. ğ´ ğ‘– is a set that contains any element of set ğ¼ besides element ğ‘–.\n\nWith ğ‘Ÿ ğ‘– , ğ‘Ÿ ğ‘ and ğ‘Ÿ ğ‘ we denote the latent representations of the model for the samples ğ‘–, ğ‘ and ğ‘, respectively. Finally, ğœ stands for a nonnegative temperature hyperparameter that controls the sharpness of the similarity distribution. It should be noted that ğ‘–, ğ‘ and ğ‘ correspond to the index of the current sample, a sample positive to the current sample, and a sample different from the current one, respectively. This formulation ensures that representations of samples from the same class are tightly clustered by maximising ğ‘Ÿ ğ‘– â€¢ ğ‘Ÿ ğ‘ , while dissimilar samples are discouraged from occupying nearby regions in the latent space via the denominator term. In the context of PriCon, SCL serves as a powerful mechanism to encode privileged information into the teacher model, thus laying a strong representational foundation for effective student training in the subsequent knowledge transfer phase.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model Architectures",
      "text": "In this section, we first present the main components used in LUPI, namely student and teacher models. Then, we present the baseline architectures that we compare against our method for assessing the effectiveness of the obtained models. Both student and teacher architectures are illustrated in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Student Model.",
      "text": "The student neural network ğ‘† is a predictive model that learns to estimate the target outcomes by utilising the information from frames. During the training phase, the student model leverages knowledge distilled from a teacher model ğ‘‡ , which has access to an additional set of modalities (i.e., privileged information). This privileged information, available only during training, enhances the learning process by providing auxiliary guidance. The goal of the student model is to effectively generalise to new, unseen data, where privileged information is not available, using only the primary feature set. In this work ğ‘† is a Convolutional Neural Network (CNN) comprising five convolutional layers of 6, 8, 12, 16 and 20 filters, respectively, followed by a dense layer. The first four convolutional layers are configured with a stride parameter of 2, while the fifth convolutional layer uses a stride of 1. Each convolutional layer has a kernel size of dimensions 3x3 and employs the ReLU activation function. Finally, a dense layer with 768 neurons, also activated using ReLU, is applied to produce the encoded frame representation. The output of this layer is fed to the decision layer, which is a simple 2-neuron dense SoftMax-activated layer. A dropout of 0.1 is applied before the last layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Teacher Models.",
      "text": "The teacher, ğ‘‡ , is a predictive model that is trained with access to both the primary feature set (i.e., prevalent information) and an additional set of privileged information. The teacher model is typically designed to achieve high performance by leveraging this richer information. Once trained, the teacher model's knowledge is distilled and transferred to the student model ğ‘†, guiding it to improve its predictions based on the primary feature set alone. The teacher model plays a crucial role in the LUPI framework by acting as a source of enhanced supervision during training. In this paper, we consider two teacher architectures. Privileged Teacher: The privileged teacher ğ‘‡ ğ‘ considers only feature (privileged) information. Consequently, it is a simple ANN consisting of two layers. The first layer has 30 neurons employing the logistic activation function. The output of this layer is fed to the decision layer which is a simple 2-neuron dense SoftMax-activated layer. Once again a dropout of 0.1 is applied before the last layer. Fusion Teacher: The fusion teacher ğ‘‡ ğ‘“ has access to both privileged and prevalent information (i.e., features and frames). In particular, it fuses the penultimate layers of the frame and feature models described above. To account for the difference in dimensionality, an additional ReLu-activated layer of 30 neurons is added to the frame encoder to match the dimensions of the feature encoder. In this case the feature encoder is a single 30-neuron layer activated by ReLU. The last layer of the fusion encoder is a linear layer of 60 neurons activated by ReLU. The purpose of this last layer is to fuse information from both frames and features in order to yield the final representation. The output of the fusion layer is fed to a 2-neuron dense layer which is activated by SoftMax. A dropout of 0.1 is applied before the output layer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baselines.",
      "text": "There are two baseline architectures employed in this paper corresponding to student models that do not consider privileged information at all. The first architecture, used for comparison with the conventional LUPI framework, performs end-to-end affect classification from frames, bypassing the representation learning process. The second baseline, used for comparison with the PriCon framework, first trains the latent dimension of the student model via SCL and then probes the frozen learned embeddings with a SoftMax activated layer as the one used in all models of this work.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "This section presents the datasets and preprocessing steps used to evaluate the performance of the proposed approach and test our hypotheses.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Recola Database",
      "text": "The RECOLA dataset (Figure  3a ) contains 9.5 hours of synchronised multimodal data from 46 French-speaking participants, including audio, visual, and physiological signals (electrocardiogram and electrodermal activity). It features annotations for arousal and valence provided by six experts (third-person annotation). The dataset includes raw footage (frames) along with fine-grained feature sets: 40 visual features (e.g., facial action units, head pose, optical flow), 130 audio features (e.g., voice intensity, pitch, mel-frequency spectral coefficients), and 116 physiological features extracted from cardiac and electrodermal activity. Raw data is also available for further processing. Of the 46 participants, 34 consented to data sharing, 23 of which are included in the public release.\n\n4.1.1 Processing RECOLA. As mentioned earlier, the RECOLA database offers annotation traces for both arousal and valence. The same preprocessing approach is followed for both affective dimensions. Specifically, we segment each participant's session (features and frames) into overlapping time windows using a sliding step of 400 ms and window lengths of 1, 2, and 3 s. These hyperparameters-the sliding step and window length-influence the size of the dataset and the temporal granularity of information contained in each window. Since the features and annotations are already synchronised, there is no need to account for the reaction time between the stimulus and the emotional response. After segmentation, each time window contains a sequence of feature vectors and a sequence of frames. To reduce computational complexity, we compute the average value for each feature within the window, representing the time window with a single feature vector. This ensures that the dimensionality of the feature vector is independent of the window length. For frame sequences, we retain 5 greyscale frames with dimensions 224 Ã— 224 per second. For example, the input for a 3 s time window consists of a single feature vector (via averaging) and a frame tensor with dimensions 15 Ã— 224 Ã— 224.\n\nWhen it comes to affect annotation, we use the median annotation values per time window in order to derive a single trace segment-one per affect dimension-that mitigates inter-annotator disagreement  [4] . The median affect trances are then averaged across time windows resulting in two scalar values per time window (ğ‘” ğ‘ for arousal and ğ‘” ğ‘£ for valence). To produce the corresponding class labels we binarise the corresponding affect values. In RECOLA, the binarisation criterion is determined by the median ground truth value of the entire set of affect annotation traces ( gğ‘  ) and a threshold ğœ–. Specifically, a time window ğ‘– is labelled as \"high\" if ğ‘” ğ‘  ğ‘– > gğ‘  + ğœ– and as \"low\" if ğ‘” ğ‘  ğ‘– < gğ‘  -ğœ–. The median is used for RECOLA because each video stimulus has been annotated by the same six expert annotators, making the median a robust measure of central tendency. For RECOLA, ğœ– is set to 0.1, ensuring a precise exclusion of ambiguous annotations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Again Dataset: Platformer Games",
      "text": "For the experiments reported in this paper, we focus on the three games of the platformer genre featured in the AGAIN dataset (Figure  3b ) as they offer sufficiently diverse gameplay properties without in need of excessive computation for experimental validation. The three games examined include Endless, an infinite runner where players must avoid obstacles while automatically moving ever rightward, Pirates!, a jumping platformer similar to Super Mario Bros (Nintendo, 1985), and Run'N'Gun!, a more complex game which requires players to move while aiming and shooting at enemies. All games have arcade-style controls of varying complexity, with Run'N'Gun! being the most complex of the three. Each game assigns a score to the player depending on their in-game performance. For the purpose of controlling the data collection process, we limit gameplay duration for all games to two minutes. The AGAIN-Platformers dataset consists of 120 participants that played and annotated their own gameplay in terms of arousal. Apart from raw gameplay footage the creators of AGAIN have also provided a high-level feature set for each game. The general feature set includes 14 features that describe player actions, environmental elements, and game events. Moreover, the dataset includes a set of game-specific extracted features for each game (33 for Endless!, 39 for Pirates! and 47 for Run'N'Gun!). The game-specific features correspond to player status (e.g., player health), gameplay events (e.g., bot aims at player), bot status and the proximal and general game context (e.g., bot-player distance and pickups visible).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Processing Again.",
      "text": "Each game in the AGAIN dataset provides self-reported arousal annotations, which are used in this work with a consistent preprocessing approach applied separately to each of the three games. Similarly to RECOLA, we segment each participant's session into overlapping time windows using a sliding step of 500 ms and window lengths of 1, 2, and 3 s. Unlike the RECOLA dataset, synchronisation is required for AGAIN. To account for the reaction time between stimulus and emotional response, the arousal annotations are shifted backward by 1 s  [22, 23] . Furthermore, the trace of each participant is normalised within [0, 1] due to the unbounded nature of the annotations in this dataset.\n\nAfter preprocessing, each time window contains a sequence of feature vectors and a sequence of frames. To reduce computational load, we compute the average value of each feature within the window, resulting in a single feature vector that represents the entire window. For frame sequences, we retain 5 greyscale frames with dimensions 224 Ã— 224 per second. In the case of arousal annotation, there is no need to mitigate the disagreement between annotators since each participant provides a single trace per game per session (i.e., self-reporting annotations). For each session, we compute the arousal score ğ‘” ğ‘ as the mean value of this trace, yielding a scalar arousal score per session. This preprocessing pipeline is repeated independently for each of the three games in the dataset. For the AGAIN dataset, the binarisation criterion is based on the mean value of the annotation traces ( á¸¡ğ‘  ) for each session, as it allows us to anchor binarisation relative to each participant's own reporting baseline, rather than applying a fixed global threshold that may not generalise across participants or game contexts  [19, 28] . For AGAIN, ğœ– is set to 0.2, accommodating the variability in self-reported annotations. These thresholds were empirically validated to ensure a balanced exclusion of ambiguous annotations while maintaining sufficient data  [17] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "This section presents the framework for evaluating the impact of contrastive teacher pretraining for privileged information on affect modelling and the experimental results obtained.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Framework",
      "text": "As mentioned earlier, we treat affect as a classification task. Hence, our models attempt to predict high and low affect (arousal or valence) states. We adopt a 5-fold cross-validation scheme to evaluate the models' performance across all experiments reported in the paper. When splitting a dataset, we make sure that data from the same participant is either in the training or the test set, not both. Before any train-test split we hold out 10% of the participants randomly as our validation set. This set is used for stopping training early so as to avoid model overfitting; i.e., training stops after 5 epochs without loss improvement on the validation set. The training, validation and test sets are the same for all models reported. We report models' performance in terms of binary classification accuracy, since after class splitting, the datasets are balanced. A key component of the LUPI training objective is the hyperparameter In this work, student models make predictions using solely information that is available in the wild; in our experiments that is the raw footage frames. The student models trained via LUPI are denoted as ğ‘† ğ‘ and ğ‘† ğ‘“ and have access to privileged information through the Privileged Teacher and the Fusion Teacher, respectively. We compare the student's performance against the performance achieved by the Fusion Teacher (ğ‘‡ ğ‘“ ) that uses all modalities (i.e., features & frames) for training and testing, and the Privileged Teacher (ğ‘‡ ğ‘ ) that makes predictions using only privileged information. ğ¸ refers to models that do not utilise privileged information during training. In Tables  1  and 2  the LUPI columns refer to the conventional approach where the teachers have been trained in an end-toend fashion in this case ğ¸ is also trained in the same manner. The PC column refers to LUPI with teachers that have been pretrained via SCL (proposed PriCon framework). The accuracy of the ğ‘‡ ğ‘ , ğ‘‡ ğ‘“ and ğ¸ models in the PC setting is obtained by applying a linear probe on top of the learned representations.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "The Importance Of Privileged Information",
      "text": "To test H1-which posits that LUPI can enhance affect detection under real-world (in-vivo) conditions, producing performances comparable to in-vitro models-we evaluate and compare the performance of baseline, teacher, and student models within the LUPI framework. Table  1  reports the average 5-fold validation accuracy for high-low The results are analysed across three different time windows (1s, 2s, 3s). The models evaluated include baseline models, teacher models (Fusion and Privileged Teachers), and student models trained via the LUPI framework (see LUPI column).\n\nBaseline models (ğ¸) consistently achieve low accuracy scores across all six experimental settings, indicating that models relying solely on raw frame data are unable to achieve competitive performance. This highlights the limitations of training models without incorporating privileged information or additional learning signals. The teacher models-namely the Fusion Teacher (ğ‘‡ ğ‘“ ) and the Privileged Teacher (ğ‘‡ ğ‘ )-yield significantly higher accuracy values for arousal compared to the baseline models. ğ‘‡ ğ‘ teachers, which consider only privileged features, consistently achieve the highest accuracy, establishing the upper bound for arousal classification. On the other hand, ğ‘‡ ğ‘“ teachers-trained on both privileged information and frames-achieve competitive results but typically perform worse than ğ‘‡ ğ‘ .\n\nLUPI-trained student models (ğ‘† ğ‘ , ğ‘† ğ‘“ ) demonstrate substantial improvements over baseline models. Specifically, ğ‘† ğ‘“ , which is trained under the supervision of the Fusion Teacher, performs on par with or even outperforms ğ‘† ğ‘ , which is guided by the Privileged Teacher. This finding suggests that the inclusion of frames in the ğ‘‡ ğ‘“ teacher can allow for better information transfer during the LUPI training process.\n\nWhile significant improvements are observed for arousal classification, valence classification results remain relatively stable across models, training strategies, and time window lengths. This can be attributed to the inherent challenges of valence prediction, which relies on subtle and ambiguous cues that are less dynamic and harder to capture than arousal. Valence-related expressions often appear visually similar across emotional contexts, making it difficult for models using raw frames to learn discriminative features  [1, 20] . Additionally, while privileged information provides strong supervisory signals for arousal, it may not capture sufficient valence-specific information, leading to weaker guidance from the ğ‘‡ ğ‘ and ğ‘‡ ğ‘“ teachers. This suggests that valence recognition might require additional contextual information, such as semantics or interaction dynamics  [32, 42] .\n\nTable  2  presents the arousal classification results for the AGAIN dataset, which includes three platformer games: Run'N'Gun!, Pirates!, and Endless. Each game is evaluated for time windows of 1s, 2s, and 3s. In Run'N'Gun!, ğ‘‡ ğ‘ achieves the highest performance, establishing an upper bound. Among the LUPI-trained student models, ğ‘† ğ‘ performs on par with or slightly better ğ‘† ğ‘“ , and both improve upon the baseline models (ğ¸).\n\nFor Pirates!, performance remains relatively stable across all time windows, with limited improvements as the temporal context increases. This stability suggests that arousal cues are less dynamic and harder to capture in this game due to subtler changes in play. In this case, both teachers perform on par; the same holds for ğ‘† ğ‘“ and ğ‘† ğ‘ among the student models. It is worth noting that here, teacher models perform on par with student models that do not use privileged information. Consequently, there is no information gain to be exploited by the LUPI framework. Finally, in Endless, the end-to-end teachers perform on par. Among the LUPI-trained students, ğ‘† ğ‘“ and ğ‘† ğ‘ also perform similarly across all time windows, consistently improving over the baseline models ğ¸.\n\nThe obtained results collectively across two different datasets and 3 different affective dimensions validate H1, confirming that the LUPI framework can enhance affect detection under real-world (in-vivo) conditions. LUPI-trained student models consistently outperform baseline models and, in several cases, closely approximate the performance of their privileged teacher counterparts. This demonstrates the potential of privileged supervision to guide the learning of compact, frame-only student models that generalise well in realistic settings where privileged modalities are unavailable at test time.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Privileged Contrastive Pretraining",
      "text": "To evaluate H2, we investigate the impact of PriCon-a framework that integrates SCL within LUPI-on the performance of both teacher and student models. The central question is whether teacher pretraining via SCL leads to more effective student learning and ultimately enhances binary affect classification. To evaluate this, we focus on the results reported in the 'PC' columns of Tables  1  and 2 , which isolate the influence of PriCon on model performance across different datasets, affective states, and time window configurations.\n\nFor the RECOLA dataset (Table  1 ), PriCon significantly improves model performance across the board. SCL-pretrained teacher models, ğ‘‡ ğ‘“ and ğ‘‡ ğ‘ , (PC column) not only match but often outperform their end-to-end LUPI counterparts. This indicates that contrastive pretraining enables the learning of more discriminative and generalisable affect representations, which are beneficial regardless of whether the teacher is trained on fused or privileged data. Importantly, these benefits are transferred downstream: student models ğ‘† ğ‘ and ğ‘† ğ‘“ consistently outperform the end-to-end baseline ğ¸, and in multiple cases even match or exceed the performance of SCLpretrained ğ¸. These results validate the effectiveness of the PriCon framework, allowing student models to benefit from the increased performace of the SCL pretrained teachers. Valence classification results, however, remain relatively stable. While PriCon provides modest gains, the improvements are less substantial due to the subtle and ambiguous nature of valence cues in visual data, as discussed in Section 5.2.\n\nIn the AGAIN dataset (Table  2 ), PriCon continues to show strong and consistent gains for student models. In Run'N'Gun!, both ğ‘† ğ‘ and ğ‘† ğ‘“ outperform not only the baseline ğ¸ but also their end-toend LUPI versions, reinforcing the utility of combining contrastive objectives (for teacher pretraining) with the conventional LUPI paradigm. This demonstrates PriCon's capacity to bridge the modality gap during student training and produce better generalising models. In Pirates!, although the overall gains are smaller, PriCon yields noticeable improvements in 2-second time windows. Here, ğ‘† ğ‘ and ğ‘† ğ‘“ significantly outperform ğ¸, even when teacher performance remains similar. In Endless, PriCon teachers train students (see PC column) that achieve higher accuracy than both their end-to-end counterparts (LUPI column) and the baseline models ğ¸. Notably, the performance gap is most pronounced for the 2s and 3s windows, suggesting that temporal context plays a role in enhancing the benefits of SCL teacher pretraining.\n\nIn summary, obtained results validate H2, demonstrating the effectiveness of the PriCon framework. By integrating SCL for teacher pretraining and LUPI for privileged information distillation, PriCon enables the development of robust affective models with improve generalisation across tasks (affect dimensions), input modalities, and temporal resolutions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "The findings presented in this paper underscore the significant potential of privileged contrastive pretraining for affect modelling. By focusing on binary classification (high vs. low affective state) our introduced method demonstrated improved robustness and performance across both RECOLA and AGAIN datasets. However, several methodological limitations and considerations must be acknowledged. Although arousal classification benefits substantially from PriCon, valence results show only marginal gains, indicating that valence remains a harder target for generalisation. Thus future work should explore the integration of user context  [34]  and MAMBA  [11]  architectures within the PriCon framework to further improve valence prediction.\n\nThe study employed an exhaustive search protocol to fine tune the ğ›¼ hyperparameter, which controls the influence of the teacher in LUPI. While this approach ensured optimal performance for the chosen datasets, it may not generalise well to other datasets or applications. The fixed hyperparameter tuning approach assumes a universal solution, potentially missing dataset-specific variations.\n\nTuning was done for 1-second time windows, assuming the optimal ğ›¼ value is consistent across all windows. While more efficient methods like Bayesian optimisation could improve tuning, their computational demands were deemed impractical for this study. Future work could explore adaptive optimisation techniques such as gradient-based hyperparameter learning  [3, 14]  for better efficiency and performance across datasets.\n\nBeyond hyperparameter tuning, the manual selection of privileged features represents another limitation of this study. While the selected features were thoughtfully aligned with domain knowledge and task-specific objectives, the ad-hoc approach followed may have overlooked complex, latent structures within the datasets that automated methods could exploit. Attention mechanisms and feature-ranking algorithms  [5, 45]  could help identify and prioritise the most relevant features, revealing patterns missed by manual selection. However, implementing these methods would require additional resources, potentially diverting attention from the study's main goal of evaluating the proposed framework. While manual selection ensured interpretability, future work could explore automated techniques to enhance scalability and effectiveness.\n\nWhile the binary classification paradigm is widely used to showcase the robustness of AC frameworks  [7, 8, 26, 30]  , a natural extension would be to explore the potential of PriCon in semi-supervised and self-supervised learning paradigms. Such extensions could unlock the ability to learn powerful general-purpose representations that are better suited for diverse affect modelling tasks, especially in scenarios with limited labelled data. These paradigms would be particularly beneficial for scaling affective computing systems in real-world applications where data labelling is often costly and time-consuming.\n\nAnother promising direction involves extending PriCon to ranking paradigms, which are essential for ordinal affect modelling tasks  [16, 44] . The incorporation of PriCon into these paradigms could enable models to better exploit privileged information for inferring subtle affective gradients, thereby improving their ability to model subjective human experiences. Furthermore, combining PriCon with few-shot learning strategies  [27, 41]  could facilitate robust affect modelling in low-data regimes, a common challenge in real-world scenarios. However, these extensions would require significant methodological adaptations. These adaptations-while outside the scope of the current study-represent exciting opportunities for advancing the field and addressing more complex affective computing challenges in future research. Finally, regarding evaluation, this study reports binary classification accuracy since the datasets are balanced. Nonetheless, future work could explore alternative metrics-such as F1 score, AUC, or mean absolute error-that may offer complementary insights, particularly in the presence of class imbalance or in regression-based affect modelling.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we introduced a methodology for affect modelling in real-world scenarios by leveraging privileged information and teacher pre-training via SCL. Our central hypothesis posits that privileged information can facilitate the reliable transfer of affect models from controlled environments, where abundant high-quality data is available, to real-world settings. We evaluated this hypothesis in the RECOLA and AGAIN datasets, which include both raw visual data and high-level handcrafted features. We considered all handcrafted features as privileged information that is accessible solely during model training, while also considering raw visual data such as video frames as available during both training and testing. As part of the PriCon framework, we pretrain teacher models via SCL and subsequently transfer their knowledge to student models, which operate without privileged information during testing. This approach assumes that teachers with a higher predictive power can produce more robust student models.\n\nOur findings for arousal and valence prediction highlight the combined advantages of leveraging privileged information and contrastive pretraining. Affect models trained via the PriCon framework not only match but often exceed the performance of their teacher models, while operating solely on frame-based features at test time. Remarkably, in many cases, PriCon models achieve performance comparable to models trained with access to all modalities during both training and testing. The proposed methodology has broad applicability to affective computing tasks involving multimodal data. It is especially valuable in scenarios where access to certain modalities is limited or unavailable, offering a scalable and effective approach to emotion recognition in the wild. The findings underscore the potential of PriCon as a paradigm towards further bridging the gap between in-vitro and in-vivo affective modelling, offering a scalable and practical solution for real-world applications.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Safe And Responsible Innovation Statement",
      "text": "The experiments presented in this paper were conducted using publicly available datasets containing frames, fine-grained features, and affect annotations. Any personally identifiable information was anonymised with untraceable IDs, ensuring that such data was not accessible to us. Participants consented to having their faces publicly available. Furthermore, the datasets used do not contain potentially offensive content, and the proposed framework is based on open-source methods, ensuring reproducibility. Finally, to the best of our knowledge, our work does not contribute to the development of deceptive applications or exacerbate existing privacy or discriminatory concerns.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Visualising the proposed Privileged Contrastive Pretraining (PriCon) framework. The top figure illustrates the",
      "page": 1
    },
    {
      "caption": "Figure 1: ). While LUPI allows PriCon to lever-",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of the affect models. â€˜Câ€™ and â€˜Dâ€™ denote",
      "page": 4
    },
    {
      "caption": "Figure 3: Sample frames from the datasets. (a) RECOLA:",
      "page": 4
    },
    {
      "caption": "Figure 3: a) contains 9.5 hours of synchronised",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: AGAIN: Average accuracy prediction for arousal",
      "data": [
        {
          "Model": "ğ¸",
          "LUPI": "65.52",
          "PC": "65.54"
        },
        {
          "Model": "ğ‘†ğ‘\nğ‘†ğ‘“",
          "LUPI": "66.36\n66.27",
          "PC": "69.51\n69.29"
        },
        {
          "Model": "ğ‘‡ğ‘\nğ‘‡ğ‘“",
          "LUPI": "72.77\n72.06",
          "PC": "81.33\n78.95"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: AGAIN: Average accuracy prediction for arousal",
      "data": [
        {
          "Model": "ğ¸",
          "LUPI": "58.22",
          "PC": "60.85"
        },
        {
          "Model": "ğ‘†ğ‘\nğ‘†ğ‘“",
          "LUPI": "60.96\n61.62",
          "PC": "63.07\n62.31"
        },
        {
          "Model": "ğ‘‡ğ‘\nğ‘‡ğ‘“",
          "LUPI": "73.11\n66.22",
          "PC": "75.45\n69.49"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: AGAIN: Average accuracy prediction for arousal",
      "data": [
        {
          "Model": "ğ¸",
          "LUPI": "66.54",
          "PC": "66.57"
        },
        {
          "Model": "ğ‘†ğ‘\nğ‘†ğ‘“",
          "LUPI": "66.42\n66.01",
          "PC": "66.86\n67.39"
        },
        {
          "Model": "ğ‘‡ğ‘\nğ‘‡ğ‘“",
          "LUPI": "65.66\n65.65",
          "PC": "66.72\n66.46"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: AGAIN: Average accuracy prediction for arousal",
      "data": [
        {
          "Model": "ğ¸",
          "LUPI": "62.36",
          "PC": "61.34"
        },
        {
          "Model": "ğ‘†ğ‘\nğ‘†ğ‘“",
          "LUPI": "64.90\n62.51",
          "PC": "62.42\n59.61"
        },
        {
          "Model": "ğ‘‡ğ‘\nğ‘‡ğ‘“",
          "LUPI": "65.15\n64.74",
          "PC": "62.80\n64.21"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: AGAIN: Average accuracy prediction for arousal",
      "data": [
        {
          "Model": "ğ¸",
          "LUPI": "68.07",
          "PC": "68.77"
        },
        {
          "Model": "ğ‘†ğ‘\nğ‘†ğ‘“",
          "LUPI": "69.73\n68.96",
          "PC": "71.11\n70.43"
        },
        {
          "Model": "ğ‘‡ğ‘\nğ‘‡ğ‘“",
          "LUPI": "73.25\n73.07",
          "PC": "73.68\n73.88"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Privileged knowledge distillation for dimensional emotion recognition in the wild",
      "authors": [
        "Muhammad Haseeb",
        "Muhammad Osama Zeeshan",
        "Marco Pedersoli",
        "Alessandro Koerich",
        "Simon Bacon",
        "Eric Granger"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Improving Sensor-Free Affect Detection Using Deep Learning",
      "authors": [
        "Anthony Botelho",
        "Ryan Baker",
        "Neil Heffernan"
      ],
      "year": "2017",
      "venue": "Artificial Intelligence in Education",
      "doi": "10.1007/978-3-319-61425-0_4"
    },
    {
      "citation_id": "3",
      "title": "Forward and reverse gradient-based hyperparameter optimization",
      "authors": [
        "Luca Franceschi",
        "Michele Donini",
        "Paolo Frasconi",
        "Massimiliano Pontil"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "4",
      "title": "Emotions over time: synchronicity and development of subjective, physiological, and facial affective reactions to music",
      "authors": [
        "Oliver Grewe",
        "Frederik Nagel",
        "Reinhard Kopiez",
        "Eckart AltenmÃ¼ller"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "5",
      "title": "AFS: An attention-based mechanism for supervised feature selection",
      "authors": [
        "Ning Gui",
        "Danni Ge",
        "Ziyin Hu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "6",
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Stat"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "Victoria Smith K Khare",
        "Esmaeil Blanes-Vidal",
        "U Nadimi",
        "Acharya Rajendra"
      ],
      "year": "2024",
      "venue": "Information fusion"
    },
    {
      "citation_id": "8",
      "title": "Classification of human emotional states based on valence-arousal scale using electroencephalogram",
      "authors": [
        "Gs Shashi Kumar",
        "Niranjana Sampathila",
        "Roshan Martis"
      ],
      "year": "2023",
      "venue": "Journal of Medical Signals & Sensors"
    },
    {
      "citation_id": "9",
      "title": "How textual quality of online reviews affect classification performance: a case of deep learning sentiment analysis",
      "authors": [
        "Lin Li",
        "Tiong-Thye Goh",
        "Dawei Jin"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-018-3865-7"
    },
    {
      "citation_id": "10",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "Mao Li",
        "Bo Yang",
        "Joshua Levy",
        "Andreas Stolcke",
        "Viktor Rozgic",
        "Spyros Matsoukas",
        "Constantinos Papayiannis",
        "Daniel Bone",
        "Chao Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Mambava: A mamba-based approach for continuous emotion recognition in valencearousal space",
      "authors": [
        "Yuheng Liang",
        "Zheyu Wang",
        "Feng Liu",
        "Mingzhou Liu",
        "Yu Yao"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "12",
      "title": "Social Image-text Sentiment Classification With Cross-Modal Consistency and Knowledge Distillation",
      "authors": [
        "Huan Liu",
        "Ke Li",
        "Jianping Fan",
        "Caixia Yan",
        "Tao Qin",
        "Qinghua Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Unifying distillation and privileged information",
      "authors": [
        "David Lopez-Paz",
        "LÃ©on Bottou",
        "Bernhard SchÃ¶lkopf",
        "Vladimir Vapnik"
      ],
      "year": "2016",
      "venue": "Unifying distillation and privileged information"
    },
    {
      "citation_id": "14",
      "title": "Gradient-based hyperparameter optimization through reversible learning",
      "authors": [
        "Dougal Maclaurin",
        "David Duvenaud",
        "Ryan Adams"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "15",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "Sijie Mai",
        "Ying Zeng",
        "Shuangjia Zheng",
        "Haifeng Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Affranknet+: ranking affect using privileged information",
      "authors": [
        "Konstantinos Makantasis"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "17",
      "title": "The Pixels and Sounds of Emotion: General-Purpose Representations of Arousal in Games",
      "authors": [
        "Konstantinos Makantasis",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2021",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Privileged Information for Modeling Affect In The Wild",
      "authors": [
        "Konstantinos Makantasis",
        "David Melhart",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2021",
      "venue": "Proc. of the IEEE Int. Conf. on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "19",
      "title": "The Invariant Ground Truth of Affect",
      "authors": [
        "Konstantinos Makantasis",
        "Kosmas Pinitas",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "20",
      "title": "From the lab to the wild: Affect modeling via privileged information",
      "authors": [
        "Konstantinos Makantasis",
        "Kosmas Pinitas",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "The Affect Game AnnotatIoN",
      "authors": [
        "David Melhart",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2021",
      "venue": "The Affect Game AnnotatIoN",
      "arxiv": "arXiv:2104.02643"
    },
    {
      "citation_id": "22",
      "title": "Towards general models of player experience: A study within genres",
      "authors": [
        "David Melhart",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "23",
      "title": "RankNEAT: outperforming stochastic gradient search in preference learning tasks",
      "authors": [
        "Kosmas Pinitas",
        "Konstantinos Makantasis",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2022",
      "venue": "Proceedings of the Genetic and Evolutionary Computation Conference"
    },
    {
      "citation_id": "24",
      "title": "Supervised contrastive learning for affect modelling",
      "authors": [
        "Kosmas Pinitas",
        "Konstantinos Makantasis",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "25",
      "title": "Across-game engagement modelling via few-shot learning",
      "authors": [
        "Kosmas Pinitas",
        "Konstantinos Makantasis",
        "Georgios Yannakakis"
      ],
      "year": "2025",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "",
      "authors": [
        "Kosmas Pinitas",
        "Nemanja Rasajski",
        "Matthew Barthet",
        "Maria Kaselimi",
        "Konstantinos Makantasis",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "venue": ""
    },
    {
      "citation_id": "27",
      "title": "Silhouette Distance Loss for Learning Few-Shot Contrastive Representations",
      "authors": [
        "Kosmas Pinitas",
        "Nemanja Rasajski",
        "Konstantinos Makantasis",
        "Georgios Yannakakis"
      ],
      "year": "2024",
      "venue": "Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "28",
      "title": "Predicting Player Engagement in Tom Clancy's The Division 2: A Multimodal Approach via Pixels and Gamepad Actions",
      "authors": [
        "Kosmas Pinitas",
        "David Renaudie",
        "Mike Thomsen",
        "Matthew Barthet",
        "Konstantinos Makantasis",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "29",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. of the IEEE Int. conf. and workshops on automatic face and gesture recognition"
    },
    {
      "citation_id": "30",
      "title": "Machine learning techniques for arousal classification from electrodermal activity: A systematic review",
      "authors": [
        "Roberto SÃ¡nchez-Reolid",
        "Francisco LÃ³pez De La Rosa",
        "Daniel SÃ¡nchez-Reolid",
        "MarÃ­a LÃ³pez",
        "Antonio FernÃ¡ndez-Caballero"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "31",
      "title": "Learning to rank using privileged information",
      "authors": [
        "Viktoriia Sharmanska",
        "Novi Quadrianto",
        "Christoph Lampert"
      ],
      "year": "2013",
      "venue": "Proc. of the IEEE Int. Conf. on computer vision"
    },
    {
      "citation_id": "32",
      "title": "How do valence and meaning interact? The contribution of semantic control",
      "authors": [
        "Ariyana Nicholas E Souter",
        "Jake Reddy",
        "JuliÃ¡n Walker",
        "Elizabeth Marino DÃ¡volos",
        "Jefferies"
      ],
      "year": "2023",
      "venue": "Journal of Neuropsychology"
    },
    {
      "citation_id": "33",
      "title": "Dynamic microexpression recognition using knowledge distillation",
      "authors": [
        "Bo Sun",
        "Siming Cao",
        "Dongliang Li",
        "Jun He",
        "Lejun Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Modeling user context for valence prediction from narratives",
      "authors": [
        "Aniruddha Tammewar",
        "Alessandra Cervone",
        "Eva-Maria Messner",
        "Giuseppe Riccardi"
      ],
      "year": "2019",
      "venue": "Modeling user context for valence prediction from narratives",
      "arxiv": "arXiv:1905.05701"
    },
    {
      "citation_id": "35",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence",
      "doi": "10.1038/s42256-020-00280-0"
    },
    {
      "citation_id": "36",
      "title": "Adieu features? End-toend speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "Mihalis Nicolaou",
        "BjÃ¶rn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. of the Int. Conf. on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2016.7472669"
    },
    {
      "citation_id": "37",
      "title": "Endto-end multimodal affect recognition in real-world environments",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiaxin Chen",
        "Stefanos Zafeiriou",
        "BjÃ¶rn Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2020.10.011"
    },
    {
      "citation_id": "38",
      "title": "Learning using privileged information: similarity control and knowledge transfer",
      "authors": [
        "Vladimir Vapnik",
        "Rauf Izmailov"
      ],
      "year": "2015",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "39",
      "title": "Knowledge transfer in SVM and neural networks",
      "authors": [
        "Vladimir Vapnik",
        "Rauf Izmailov"
      ],
      "year": "2017",
      "venue": "Annals of Mathematics and Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "A new learning paradigm: Learning using privileged information",
      "authors": [
        "Vladimir Vapnik",
        "Akshay Vashist"
      ],
      "year": "2009",
      "venue": "Neural networks"
    },
    {
      "citation_id": "41",
      "title": "Generalizing from a few examples: A survey on few-shot learning",
      "authors": [
        "Yaqing Wang",
        "Quanming Yao",
        "James Kwok",
        "Lionel Ni"
      ],
      "year": "2020",
      "venue": "ACM computing surveys (csur)"
    },
    {
      "citation_id": "42",
      "title": "Faces in context: A review and systematization of contextual influences on affective face processing",
      "authors": [
        "J Matthias",
        "Tobias Wieser",
        "Brosch"
      ],
      "year": "2012",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "43",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "Kailai Yang",
        "Tianlin Zhang",
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "Roddy Georgios N Yannakakis",
        "Carlos Cowie",
        "Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Feature subset selection and feature ranking for multivariate time series",
      "authors": [
        "Hyunjin Yoon",
        "Kiyoung Yang",
        "Cyrus Shahabi"
      ],
      "year": "2005",
      "venue": "IEEE transactions on knowledge and data engineering"
    },
    {
      "citation_id": "46",
      "title": "Distilling EEG representations via capsules for affective computing",
      "authors": [
        "Guangyi Zhang",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "Distilling EEG representations via capsules for affective computing",
      "arxiv": "arXiv:2105.00104"
    }
  ]
}