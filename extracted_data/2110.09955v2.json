{
  "paper_id": "2110.09955v2",
  "title": "Positional-Spectral-Temporal Attention In 3D Convolutional Neural Networks For Eeg Emotion Recognition",
  "published": "2021-10-13T12:03:36Z",
  "authors": [
    "Jiyao Liu",
    "Yanxi Zhao",
    "Hao Wu",
    "Dongmei Jiang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing the feelings of human beings plays a critical role in our daily communication. Neuroscience has demonstrated that different emotion states present different degrees of activation in different brain regions, EEG frequency bands and temporal stamps. In this paper, we propose a novel structure to explore the informative EEG features for emotion recognition. The proposed module, denoted by PST-Attention, consists of Positional, Spectral and Temporal Attention modules to explore more discriminative EEG features. Specifically, the Positional Attention module is to capture the activate regions stimulated by different emotions in the spatial dimension. The Spectral and Temporal Attention modules assign the weights of different frequency bands and temporal slices respectively. Our method is adaptive as well as efficient which can be fit into 3D Convolutional Neural Networks (3D-CNN) as a plugin module. We conduct experiments on two real-world datasets. 3D-CNN combined with our module achieves promising results and demonstrate that the PST-Attention is able to capture stable patterns for emotion recognition from EEG.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions play an important role in our daily life, including human communication, disease prediction, and so on. Even if emotions seem natural to us, we have little knowledge of the mechanisms behind the affective function of the brain  [1] . EEG signal is capable of objectively reflecting different emotions. With the development of deep learning, research on emotion recognition based on EEG has attracted great interest in different interdisciplinary fields, including psychology detection, affective computing, and stress monitor  [2, 3] . Although emotion recognition from EEG has achieved rapid development with machine learning, there are still many suspend problems to be solved and some negative factors in EEG signal to conduct emotion recognition  [3, 4] . For example, the uncontrollable label tagging and the great differences in individual variations of emotional states make emotion recognition hard. The EEG signal collected in the experimental environment is also influenced by many interference factors, e.g., time, context, space, races, language, culture  [5] , so there is much disturbing noise in the EEG signal. If we train the models based on the raw EEG samples indistinguishably, unnecessary noise will be introduced in the training processing. Therefore, the stability of EEG patterns, the validity of EEG features, and the rationality of model structure are vital to EEG emotion recognition and other real-world applications of EEG.\n\nAlthough EEG is sensitive to differences in environment variables  [7] , it is considered to have some stable patterns exhibiting consistency among different sessions of the same participants or different participants in the same experimental setup  [6] . Zheng et al.  [6]  investigate stable patterns of EEG over time for emotion recognition using a machine learning approach and show that power spectral density (PSD), differential entropy (DE), differential asymmetry (DASM), rational asymmetry (RASM), asymmetry (ASM) and differential caudality (DCAU) features are good indicators for EEG emotion recognition, which implies that taking different EEG features of different brain regions into consideration is beneficial to the task of emotion classification. Zheng et al.  [8]  show that using features on critical frequency bands and channels can get close or even better results than all EEG features without pruning. Wu et al.  [9]  indicate that combining the functional connectivity features from EEG and physiological signals can get promising results in emotion classification, which implies that brain regions' connectivity and topological structure are also key factors of emotion classification. On the other hand, as a time series, the time-variation of EEG signal is an important clue of emotion fluctuation. Although existing models have achieved high performance, most methods only use the raw EEG data or unprocessed EEG features and consider one or two factors of EEG emotion classification, but ignore the complementarity of the features among different dimensions, limiting the classification capability of the models to a certain extent.\n\nIn this paper, we propose the Positional-Spectral-Temporal Attention (PST-Attention) in 3D convolutional neural networks (3D-CNN) to exploit important information in critical spec-trum bands, brain regions, and time slots in the EEG signal for emotion classification. Specifically, the Positional Attention module learns the active regions induced by different emotions in the spatial dimension. The Spectral and Temporal Attention modules explore the different significance of frequency bands and temporal stamps, respectively. The main contributions of this paper can be summarized as follows:\n\n1) We recombine the EEG features to a 4D shape as the EEG feature representation. It is an adaptive fusion of the position-spectral-temporal information of the EEG signal and a suitable input of our proposed structure.\n\n2) We develop a parallel Positional-Spectral-Temporal Attention module to adaptively capture stable and discriminative patterns of spatial, spectral and temporal dimensions as well as suppress uninformative EEG features.\n\n3) Experiments are conducted on two benchmark datasets, and the results show that the 3D-CNN with PST-Attention Module outperforms most competitive methods.\n\nThe layout of the paper is as follows. In Section 2, we describe the related works on emotion recognition based on EEG. Section 3 presents the details of our proposed methods. Experimental evaluation and ablation experiments of the proposed methods are presented in section 4. Finally, we conclude our work in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion classification based on EEG signals is a meaningful research direction. As the electrophysiological manifestation of the central nervous system, EEG has the capability to reflect the real emotion states of humans objectively and precisely. With the development of artificial intelligence technology, emotion recognition has become a hot research topic in the field of human-computer interaction.\n\nIn recent years, traditional machine learning and deep learning methods have been widely used in EEG emotion recognition and other related applications. For traditional machine learning methods, Koelstra et al.  [10]  adopt Gaussian naive Bayes to classify the degree of arousal, valence and liking. Bahari et al.  [11]  use recurrence plot analysis and k-nearest neighbor classifier to recognize different emotions. Wang et al.  [12]  employ a support vector machine (SVM) classifier to discriminate different emotions.\n\nCompared with traditional methods, deep learning technologies have advantages in high-level representation and training in an end-to-end mode. Inspired by the achievements of deep learning in computer vision, natural language processing and other fields  [13, 14, 15] , deep learning methods are applied to EEG emotion classification. Zheng et.al  [16]  introduce deep belief networks to investigate critical frequency bands and channels, which is a pioneering work in EEG-Based emotion recognition with neural networks. Since then, many works have utilized deep learning models to exploit EEG data from the properties of spatial, spectral and temporal dimensions. Al-Nafjan et al.  [17]  adopt Deep Neural Networks (DNN) using PSD feature to identify human emotions. Yang et al.  [18]  propose a hierarchical network using DE features from five frequency bands to identify different emotions. Above works mainly utilize frequency features but less explore the spatial and temporal information.\n\nTo explore the temporal information, Fourati et al. present an Echo State Network (ESN), which uses recursive layer projects the raw EEG signals to the high-dimensional state space  [19] . Alhagry et al.  [20]  use a two-layer long-short term memory (LSTM) to get satisfactory emotion recognition results with the EEG signal as input. Ryan et al.  [21]  use a recurrent neural networks (RNNs) model to explain the time dependence of cognitive-related EEG signal. Bashivan et al.  [22]  propose a deep recursive convolutional neural network (R-CNN) for EEG-based cognitive and mental load classification tasks. Among diverse deep learning models, Convolutional Neural Network (CNN)  [23]  is the kernel of the current best architecture for image and video recognizing for their robust capabilities to learn feature representations of input data. CNN is also widely used for spatial features extraction of EEG signal, Li et al.  [24]  present a hierarchical CNN to capture spatial information among different channels based on 2D EEG maps. Zhang et al.  [25]  propose a deep CNN model to learn the spatio-temporal robust feature representation of the raw EEG data stream for motion intention classification. Lawhern et al.  [26]  propose an application of multi-layer pure CNN without full connection layer for P300-based oddball recognition task, finger motor task and motor imagination task. In order to capture the information of interaction between different brain regions, Salama et al.  [27]  use a 3D-CNN to recognize human emotions from multichannel EEG data of the DEAP dataset.\n\nHowever, CNN is capable of catching local visual field information but not the global information. To overcome the shortcoming, many studies introduce attention mechanism in CNN. Attention can not only learn global information in CNN, but also enhance important information. Even in other deep learning models, attention is used to enhance the vital information as well as suppress unnecessary noise information so as to improve the performance of models. Chen et al.  [28]  propose a hierarchical bidirectional Gated Recurrent Unit (GRU) network with attention for human emotion classification from continual EEG signal. Kim, et al.  [29]  propose a long short-term memory network and apply an attention mechanism to assign weights to the emotional states appearing at specific moments to conduct two-level (low and high) and three-level (low, middle, and high) classification on the valence and arousal emotion models. Tao et al.  [30]  propose an attentionbased convolutional recurrent neural network (ACRNN) to extract more discriminative features from EEG signal and improve the accuracy of emotion recognition on the DEAP and DREAMER databases. Liu et al.  [31]  propose a novel multichannel EEG emotion recognition method based on sparse graphic attention long short-term memory (SGA-LSTM) for EEG emotion recognition. Though existing emotion recognition methods have achieved promising results, most methods consider single feature or a combination of two features. Due to the complementarity among features of different dimensions and the different importance of features in different positions of the same dimension to emotion classification, we propose the PST-Attention, which takes into account spatial-spectraltemporal features and the most important features among them simultaneously.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Overview Of Model Structure",
      "text": "EEG-based emotion recognition is to classify the emotion categories according to the EEG signal. As illustrated in Fig.  2 , the overview of the recognition model in this paper consists of three sub-modules which are feature preparing and reorganization, the backbone of model and classification loss. The yellow parts in Fig.  2  are our contributions in this paper. The EEG emotion classification problem to learn a mapping function F that maps the EEG input to the corresponding emotion labels:\n\nwhere X denotes the representation of EEG signals, F denotes the mapping function i.e., neural network transformations. Y ∈ {y 1 , y 2 , ..., y n } denotes the emotion classification labels. In this paper, the classification cross entropy is adopted as loss function, which is defined as the formula:\n\nwhere L denotes the loss function of the task of EEG emotion recoginition, C denotes number of emotion classes, y c is the ground truth emotion label and y c is the predictors of neural networks. The innovation of our paper is the PST-Attention combined with 3D convolution operation (Conv3D) to extract informative feature from the 4D EEG representation. 3D-CNN can better capture the temporal and spatial feature information in EEG signals. The pooling layer in the model is to connect the convolution and full connected layer. The details of EEG representation organization and the PST-Attention module are illustrated in Fig.  1  and Fig.  3  respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. 4D Eeg Representation Organization",
      "text": "To better represent the features of different dimensions of EEG data, we re-organize EEG signal into a 4D representation. The processing of transforming the original EEG signal into 4D Representation is shown in Fig.  1 . We define X = (E 1 , E 2 , ..., E T ) ∈ R C×T as an EEG sample collected in T time stamps, where C is the number of electrodes. E t denotes the EEG signal of C electrodes collected at time stamp t. Here, we use the DE features denoted as F t = (D 1 , D 2 , ..., D S ) ∈ R C×S from E t as described in  [32] . We set (δ[1-4Hz], θ [4-8Hz], α [8-14HZ], β [14-31Hz], and γ [31-51 Hz]) as the spectral band set S. To explore the interactions among spatial, spectral and temporal dimensions, we re-organize F t of the sample X into 4D EEG representation. Specifically, the sth band feature D s from C channels is transformed into a 2D map D s ∈ R V ×H according to the EEG cap layout for EEG channels as Fig.  1 . In other words, we reshape the 1D tensor\n\nWhen we do the same operation in each band, F t will be transformed into a 3D map\n\nFinally we stack all the transformed 3D feature map along the temporal dimension to get the 4D EEG representation X = (F 1 , F 2 , ..., F T ) ∈ R T ×S×V ×H . It is also the form of input for our proposed model. 1) Positional Attention module (P-Attention): The Positional Attention module is to learn the spatial attention mask to enhance the valuable regions of EEG signal for emotion classification. The researches  [8]  have proved that the activation of different brain regions is dissimilar when subjects watching various videos to evoke emotion. Because there is evidence that the lateralization between the left and right hemispheres is associated with emotions  [33] , we investigate asymmetry features. Unlike previous spatial attention  [30] , which learns a 2D attention map within the spatial dimension, we concatenate the relationship on all sides to get global information in the spatial dimensions. Our positional attention is illustrated in the left branch in Fig.  3 . Given the input tensor X ∈ R B×T ×S×V ×H of the PST-Attention module and B is the batch size in training, the positional attention is formulated as follows:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Proposed Pst-Attention Module",
      "text": "where R v (X) and R h (X) is adaptive average pooling method in horizon and vertical, respectively.\n\n. f 2d is the attention learning method with 2D convolution operation.\n\nThe learning parameters of f 2d is W (V +H) which capture the global context information in horizon and vertical dimensions.\n\nThen with the sigmoid function θ normalizing the attention weight W (V +H) , the attention mask\n\n2) Spectral Attention module (S-Attention): The Spectral Attention module is to learn the spectrum attention mask to strengthen the informative spectral bands for emotion classification. When the brain is faced with different emotional stimuli, the energy spectrum of different bands will vary greatly. The Spectral Attention module aims to learn the most discriminative EEG frequency that can enhance the ability of\n\nwhere τ is the transpose operation to exchange the temporal and spectral dimensions. f 3d is a 3D convolution operation with kernel size (S × 1 × 1). The W S×1×1 is the learning weight represent the different importance of spectral-wise. θ is the sigmoid function to scale the spectral-wise attention weight W S .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3) Temporal Attention Module (T-Attention):",
      "text": "The Temporal Attention module is to learn the time dimension mask to reinforce the climax time slice of a particular emotion. The generation of emotions can't accomplish at one stroke, it may experience upsurge or downcast. So for the same emotion at different time points, the situation of brain waves is also different. We take advantage of the Temporal Attention module to learn changes of a certain emotion in the time axis. The Temporal Attention is illustrated in the right branch in Fig.  3 . The Temporal Attention formula is as following:\n\nwhere the f 3d is the convolution operation with kernel size (T × 1 × 1). The parameters weight of f 3d is W T , which represent the difference importance of different temporal. θ is sigmoid function.\n\nThe M vh , M s and M t are Positional Attention mask, Spectral Attention mask and Temporal Attention mask respectively. We call the whole attention module as Positional-Spectral-Temporal Attention (PST-Attention). After attention masks learning, the input tensor is recalibrated by the three attention masks. In fig.  3 , the * means the dot multiplication operation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "We validated our model on SEED  [8, 32]  and SEED-IV  [34]  databases.\n\nSEED contains three different categories of emotion, namely positive, negative, and neutral. Fifteen participants' EEG data of the dataset were collected while they were watching the stimulus videos. The videos are carefully selected and can elicit a single desired target emotion. With an interval of about one week, each subject participated in three experiments, and each session contained 15 film clips. The participants are asked to give feedback immediately after each experiment. second non-overlapping segments and down-sampled with 128 Hz. The DE feature is also pre-computed over five frequency bands in each channel.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "We train our model on NVIDIA RTX 2080 GPU. The Adam optimization is used to minimize the loss function. The learning rate is set to 0.001. The kernel size of the 3D-CNN is 5*5*3. The number of the Attention Module Block is set to 2 and the number of time stamps in each sample is set to 9, considering the longer temporal feature may have more meaningful Physiological significance, different from  [41] , which sets the time slice length to 0.5s, we set the time slice length to 1s. We conducted experiments on each subject. For each experiment, we randomly shuffle the samples. The ratio of the training set to test set is 9:6.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Compared Models",
      "text": "• SVM  [35] : A Least squares support vector machine classifier.\n\n• DBN  [16] : Deep Belief Networks investigate the critical frequency bands and channels.\n\n• DGCNN  [36] : Dynamical Graph Convolutional Neural Networks model the multichannel EEG features.  • GSCCA  [37] : Group sparse canonical correlation analysis, a group sparse extension of the conventional CCA method which models the linear correlationship between emotional EEG feature vectors and the corresponding EEG class label vectors.\n\n• BiDANN  [38] : Bi-hemispheres domain adversarial neural network maps the EEG feature data of both left and right hemispheres into discriminative feature spaces separately.\n\n• RGNN  [39] : Regularized graph neural network considers the biological topology among different brain regions to capture both local and global relations among different EEG channels.\n\n• BiHDM  [40] : Bi-hemispheric discrepancy model learns the asymmetric differences between two hemispheres for EEG emotion recognition.\n\n• 4D-aNN  [41] : Four-dimensional attention-based neural network fuses information on different domains and captures discriminative patterns in EEG signals based on the 4D spatialspectral-temporal representation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Experimental Results And Analysis",
      "text": "We compare our model with other models on SEED and SEED-IV dataset. TABLE I presents the average accuracy (Mean) and standard deviation (Std) of the compared models for EEG based emotion recognition on SEED and SEED-IV datasets. DGCNN  [36]  considers the spatial information of EEG signals collected from different channels and employs graph convolution to extract spatial information. BiHDM  [40]  uses two directional RNNs to extract spatial information of EEG signals. 4D-aNN  [41]  transforms DE and PSD features into 4D spatial-spectral-temporal representations as input and integrates the attention mechanisms into the CNN module and the bidirectional LSTM module for emotion classification. Our model takes positional, spectral, and temporal information into consideration simultaneously. In light of the asymmetry of brain regions that significantly influence emotion classification, we design the Positional Attention to concatenate the relationship on all sides to get global information in the spatial dimensions. It learns the critical information of anterior-posterior and left-right hemispheres simultaneously, enabling our model to capture informative features from EEG signals for emotion recognition comprehensively. In addition, with the Spectral Attention module to learn critical frequency bands and the Temporal Attention module to learn important time points, the accuracy of our model is further improved compared with the other competitive models.\n\nThe TABLE I presents the performance of all models on the SEED and the SEED-IV dataset. For the three-category classification task, 4D-aNN  [41]  transforms the combination of DE and PSD features to input, reaching 96.10% on classification accuracy. Xiao et al.  [41]  also conduct experiments by only taking DE features and PSD features as inputs, respectively. The 4D-aNN (DE) accuracy reaches 95.39% and exceeds 4D-aNN (PSD) 4.90%, and the result is very close to 4D-aNN's experiments using both of them. The performance of our method is better than that of 4D-aNN(DE), indicating our effectiveness. For the four-category classification task, DBN achieved an accuracy of 66.77%, and the graph-based network DGCNN  [36]  and RGNN  [39]  further improve the accuracy to 69.88% and 79.37%, respectively. 3D-CNN with PST-Attention achieves superior performance with an accuracy of 82.73% on SEED-IV datasets compared to other competitive models.\n\nTo verify the validity of 4D feature representation, we conduct ablation experiments with different EEG sample representations on SEED and SEED-IV datasets. TABLE II presents the experimental results on different EEG representations, where VHS stands for spatial (Horizontal and Vertical) and spectral dimensions. VHT stands for the spectral and temporal dimensions, as for spectral dimension, we take the best result from the feature of 5 different frequency bands and the mean value. PST means using spatial, spectral, and temporal dimensions without split the spatial dimension into Horizontal and Vertical dimensions. The results show that the 4D representation obtains better accuracy in terms of the same structure than other competitive EEG representations. As for the 3D EEG representation, we use a more suitable structure for 3D input, CNN, to conduct emotion recognition II.\n\nTo further validate the performance of the proposed method,",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Ablation Studies",
      "text": "To demonstrate the effectiveness of different modules in PST-Attention, we conduct ablation experiments on SEED-IV database. The proposed PST-Attention module includes three submodules, which are P-Attention, S-Attention, and T-Attention. TABLE III presents the experimental results on each sub-modules. The results of the three modules have different degrees of improvement in classification accuracy. In TABLE III, 3D-CNN with P-Attention achieves 3.27% improvements on Mean and 3.02% reductions on Std. 3D-CNN with S-Attention achieves 2.47% improvements on Mean and 3.84% reductions on Std. 3D-CNN with T-Attention achieves 0.4% improvements on Mean and 1.8% reductions on Std. Grouping these three attention modules into 3D-CNN achieves 9.81% improvements on Mean and 6.8% reductions on Std. These illustrate that different attention modules possess complementary in mining useful EEG information for emotion recognition. We also conclude that the effectiveness of the P-Attention or S-Attention module is a bit better than that of the T-Attention module, which indicates that the informative channels and frequency bands may have more importance on emotion recognition. The Positional Attention mask and Spectral Attention mask show that the features of different brain regions and different frequency bands play different roles in EEG emotion recognition, consistent with the literature observations  [6, 8] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we propose the PST-Attention in 3D-CNN for EEG emotion recognition. The 3D-CNN with PST-Attention model extracts positional features, spectral features, and temporal features with the proposed 4D EEG representation, which effectively utilizes the complementarity among features of different dimensions, enhances the informative features as well as suppress useless features. In addition, the PST-Attention mechanism is used to adaptively focus on attentive brain regions, brain asymmetry, frequency bands, time stamps. The experiments on SEED and SEED-IV datasets demonstrate better performance than all competitive methods. Meanwhile, ablation studies show the effectiveness of each attention module in the PST-Attention. The 3D-CNN with the proposed PST-Attention is a general framework based on multivariate physiological time series, which can be further applied to other fields in the future, such as pressure detection and sleep quality analysis.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: are our contributions in this paper.",
      "page": 3
    },
    {
      "caption": "Figure 1: and Fig. 3 respectively.",
      "page": 3
    },
    {
      "caption": "Figure 1: We deﬁne X",
      "page": 3
    },
    {
      "caption": "Figure 1: In other words, we reshape the 1D tensor",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the overview of the Positional-Spectral-",
      "page": 3
    },
    {
      "caption": "Figure 3: Given the input tensor",
      "page": 3
    },
    {
      "caption": "Figure 1: 4D EEG Representation Organization",
      "page": 4
    },
    {
      "caption": "Figure 3: The formula is as following:",
      "page": 4
    },
    {
      "caption": "Figure 3: The Temporal Attention formula is as following:",
      "page": 4
    },
    {
      "caption": "Figure 2: Overview of EEG-based emotion recognition.",
      "page": 5
    },
    {
      "caption": "Figure 3: Details of the PST-Attention Module.",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparing these four pictures, the distances of",
      "page": 6
    },
    {
      "caption": "Figure 4: Visualization of the high-level features extracted from the PST-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FP1": "AF3\nFPZ",
          "FPZ": "FP2",
          "FP2": "AF4"
        },
        {
          "FP1": "F1",
          "FPZ": "FCZ\nAF4",
          "FP2": "F2"
        },
        {
          "FP1": "FC1\nFCZ",
          "FPZ": "FCZ\nF2",
          "FP2": "FC2\nF4"
        },
        {
          "FP1": "C1\nFCZ",
          "FPZ": "CZ\nFC2",
          "FP2": "C2\nFC4"
        },
        {
          "FP1": "CP1\nCZ",
          "FPZ": "CPZ\nC2",
          "FP2": "CP2\nC4"
        },
        {
          "FP1": "P1\nCPZ",
          "FPZ": "PZ\nCP2",
          "FP2": "P2\nCP4"
        },
        {
          "FP1": "PO3\nPZ",
          "FPZ": "POZ PO4\nP2",
          "FP2": "P4"
        },
        {
          "FP1": "POZ PO4",
          "FPZ": "OZ",
          "FP2": "O2\nPO6"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F7\nFT7\nT7\nTP7\nP7": "",
          "...\nAF3\nAF4\nFP1\nFPZ\nFP2\nF7\nF5\nF3\nF1\nFCZ\nF2\nF4\nF6\nAF3\nAF4\nFT7\nFC5\nFC3\nFC1\nFCZ\nFC2\nFC4\nFC6\nF5\nF3\nF1\nFCZ\nF2\nF4\nF6\nF8\nT7\nC5\nC3\nC1\nCZ\nC2\nC4\nC6\nFC5\nFC3\nFC1\nFCZ\nFC2\nFC4\nFC6\nFT8\nTP7\nCP5\nCP3\nCP1\nCPZ\nCP2\nCP4\nCP6\nC5\nC3\nC1\nCZ\nC2\nC4\nC6\nT8\nP7\nP5\nP3\nP1\nPZ\nP2\nP4\nP6\nCP5\nCP3\nCP1\nCPZ\nCP2\nCP4\nCP6\nTP8\nPO7\nPO5\nPO3\nPO6\nPO8\nPOZ PO4\nP5\nP3\nP1\nPZ\nP2\nP4\nP6\nP8\nCB1\nOZ\nO2\nCB2\nPO7\nPO5\nPO3\nPO6\nPO8\nPOZ PO4": "CB1\nO1\nOZ\nO2\nCB2"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FP1": "AF3\nFPZ",
          "FPZ": "FP2",
          "FP2": "AF4"
        },
        {
          "FP1": "F1",
          "FPZ": "FCZ\nAF4",
          "FP2": "F2"
        },
        {
          "FP1": "FC1\nFCZ",
          "FPZ": "FCZ\nF2",
          "FP2": "FC2\nF4"
        },
        {
          "FP1": "C1\nFCZ",
          "FPZ": "CZ\nFC2",
          "FP2": "C2\nFC4"
        },
        {
          "FP1": "CP1\nCZ",
          "FPZ": "CPZ\nC2",
          "FP2": "CP2\nC4"
        },
        {
          "FP1": "P1\nCPZ",
          "FPZ": "PZ\nCP2",
          "FP2": "P2\nCP4"
        },
        {
          "FP1": "PO3\nPZ",
          "FPZ": "POZ PO4\nP2",
          "FP2": "P4"
        },
        {
          "FP1": "O1\nPOZ PO4",
          "FPZ": "OZ",
          "FP2": "O2\nPO6"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F7\nFT7\nT7\nTP7\nP7": "",
          "...\nAF3\nAF4\nFP1\nFPZ\nFP2\nF7\nF5\nF3\nF1\nFCZ\nF2\nF4\nF6\nAF3\nAF4\nFT7\nFC5\nFC3\nFC1\nFCZ\nFC2\nFC4\nFC6\nF5\nF3\nF1\nFCZ\nF2\nF4\nF6\nF8\nT7\nC5\nC3\nC1\nCZ\nC2\nC4\nC6\nFC5\nFC3\nFC1\nFCZ\nFC2\nFC4\nFC6\nFT8\nF't\nTP7\nCP5\nCP3\nCP1\nCPZ\nCP2\nCP4\nCP6\nC5\nC3\nC1\nCZ\nC2\nC4\nC6\nT8\nP7\nP5\nP3\nP1\nPZ\nP2\nP4\nP6\nCP5\nCP3\nCP1\nCPZ\nCP2\nCP4\nCP6\nTP8\nPO7\nPO5\nPO3\nPO6\nPO8\nPOZ PO4\nP5\nP3\nP1\nPZ\nP2\nP4\nP6\nP8\nCB1\nO1\nOZ\nO2\nCB2\nPO7\nPO5\nPO3\nPO6\nPO8\nPOZ PO4": "CB1\nO1\nOZ\nO2\nCB2"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The neural bases of emotion regulation",
      "authors": [
        "Amit Etkin",
        "Christian Büchel",
        "James Gross"
      ],
      "year": "2015",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Affective brain-computer interfaces: Psychophysiological markers of emotion in healthy persons and in persons with amyotrophic lateral sclerosis",
      "authors": [
        "Femke Nijboer"
      ],
      "year": "2009",
      "venue": "2009 3rd international conference on affective computing and intelligent interact ion and workshops"
    },
    {
      "citation_id": "3",
      "title": "A survey of affective brain computer interfaces: principles, state-of-the-art, and challenges",
      "authors": [
        "Christian Mühl"
      ],
      "year": "2014",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "4",
      "title": "Affective computing: challenges",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "Jonghwa Kim",
        "Elisabeth André"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "6",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "Wei-Long Zheng",
        "Jia-Yi Zhu",
        "Bao-Liang Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Test-retest reliability of cognitive EEG",
      "authors": [
        "L Mcevoy",
        "M Smith",
        "A Gevins"
      ],
      "year": "2000",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "8",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "9",
      "title": "Investigating EEG-based functional connectivity patterns for multimodal emotion recognition",
      "authors": [
        "Xun Wu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2020",
      "venue": "Investigating EEG-based functional connectivity patterns for multimodal emotion recognition",
      "arxiv": "arXiv:2004.01973"
    },
    {
      "citation_id": "10",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra"
      ],
      "year": "2011",
      "venue": "Deap: A database for emotion analysis; using physiological signals"
    },
    {
      "citation_id": "11",
      "title": "Eeg-based emotion recognition using recurrence plot analysis and k nearest neighbor classifier",
      "authors": [
        "Fatemeh Bahari",
        "Amin Janghorbani"
      ],
      "year": "2013",
      "venue": "20th Iranian Conference on Biomedical Engineering (ICBME)"
    },
    {
      "citation_id": "12",
      "title": "EEG-based emotion recognition using frequency domain features and support vector machines",
      "authors": [
        "Xiao Wang",
        "Dan Wei",
        "Bao-Liang Nie",
        "Lu"
      ],
      "year": "2011",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "13",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Generating sequences with recurrent neural networks",
      "authors": [
        "Alex Graves"
      ],
      "year": "2013",
      "venue": "Generating sequences with recurrent neural networks",
      "arxiv": "arXiv:1308.0850"
    },
    {
      "citation_id": "15",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "Andrej Karpathy"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "EEG-based emotion classification using deep belief networks",
      "authors": [
        "Wei-Long Zheng"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "17",
      "title": "Classification of human emotions from electroencephalogram (EEG) signal using deep neural network",
      "authors": [
        "Al-Nafjan",
        "Abeer"
      ],
      "year": "2017",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "18",
      "title": "EEG-based emotion recognition using hierarchical network with subnetwork nodes",
      "authors": [
        "Yimin Yang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "19",
      "title": "Optimized echo state network with intrinsic plasticity for eeg-based emotion recognition",
      "authors": [
        "Rahma Fourati"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition based on EEG using LSTM recurrent neural network",
      "authors": [
        "Salma Alhagry",
        "Aly Aly Fahmy",
        "Reda El-Khoribi"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "21",
      "title": "Deep long short-term memory structures model temporal dependencies improving cognitive workload estimation",
      "authors": [
        "Ryan Hefron"
      ],
      "year": "2017",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "22",
      "title": "Learning representations from EEG with deep recurrent-convolutional neural networks",
      "authors": [
        "Pouya Bashivan"
      ],
      "year": "2015",
      "venue": "Learning representations from EEG with deep recurrent-convolutional neural networks",
      "arxiv": "arXiv:1511.06448"
    },
    {
      "citation_id": "23",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Yann Lecun"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "24",
      "title": "Hierarchical convolutional neural networks for EEG-based emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Zhaoxiang Zhang",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "25",
      "title": "EEG-based intention recognition from spatiotemporal representations via cascade and parallel convolutional recurrent neural networks",
      "authors": [
        "Dalin Zhang"
      ],
      "year": "2017",
      "venue": "EEG-based intention recognition from spatiotemporal representations via cascade and parallel convolutional recurrent neural networks",
      "arxiv": "arXiv:1708.06578"
    },
    {
      "citation_id": "26",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "Vernon Lawhern"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "27",
      "title": "EEG-based emotion recognition using 3D convolutional neural networks",
      "authors": [
        "Elham Salama"
      ],
      "year": "2018",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "28",
      "title": "A hierarchical bidirectional GRU model with attention for EEG-based emotion classification",
      "authors": [
        "J Chen",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "29",
      "title": "EEG-Based Emotion Classification Using Long Short-Term Memory Network with Attention Mechanism[J]",
      "authors": [
        "Y Kim",
        "A Choi"
      ],
      "venue": "Sensors"
    },
    {
      "citation_id": "30",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Wei Tao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Sparse Graphic Attention LSTM for EEG Emotion Recognition",
      "authors": [
        "Suyuan Liu"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "32",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "Duan",
        "Jia-Yi Ruo-Nan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "33",
      "title": "Anterior cerebral asymmetry and the nature of emotion",
      "authors": [
        "Richard Davidson"
      ],
      "year": "1992",
      "venue": "Brain and cognition"
    },
    {
      "citation_id": "34",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "35",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "Johan Suykens",
        "Joos Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural processing letters"
    },
    {
      "citation_id": "36",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Multichannel EEG-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "Wenming Zheng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "38",
      "title": "A Novel Neural Network Model based on Cerebral Hemispheric Asymmetry for EEG Emotion Recognition",
      "authors": [
        "Yang Li"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "39",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Yang Li"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "41",
      "title": "4D Attention-based Neural Network for EEG Emotion Recognition",
      "authors": [
        "Guowen Xiao"
      ],
      "year": "2021",
      "venue": "4D Attention-based Neural Network for EEG Emotion Recognition",
      "arxiv": "arXiv:2101.05484"
    }
  ]
}