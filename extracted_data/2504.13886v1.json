{
  "paper_id": "2504.13886v1",
  "title": "Quantifying Emotional Arousal Through Pupillary Response: A Novel Approach For Isolating The Luminosity Effect And Predicting Affective States",
  "published": "2025-04-04T16:40:19Z",
  "authors": [
    "Zeel Pansara",
    "Gabriele Navyte",
    "Tatiana Freitas-Mendes",
    "Camila Bottger",
    "Edoardo Franco",
    "Luca Citi",
    "Erik S. Jacobi",
    "Giulia L. Poerio",
    "Helge Gillmeister",
    "Caterina Cinel",
    "Vito De Feo"
  ],
  "keywords": [
    "AI (Artificial Intelligence)",
    "Pupil Dilation",
    "Pupil Size",
    "Emotion Detection",
    "Arousal",
    "Luminosity Effect",
    "Linear Regression (LR)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Researchers have long recognized pupil response as a potential objective indicator of emotional arousal; however, confounding factors, particularly luminosity of stimuli and the ambient environment, have limited its usefulness in detecting emotions. This study presents a new approach to isolate and remove the effect of luminosity on pupil dilation, obtaining the component of pupil dilation due only to emotional arousal. Our model predicts the pupil size due to luminosity only as a function of the screen luminosity. Our model adapts to individual differences in pupil response to light and different types and configurations of monitors by using a calibration procedure before the experiment. The predicted pupil size has an average correlation with the measured pupil size of 0.76 ± 0.045, an R2 of 0.58 ± 0.680, and a normalized root mean square error (NRMSE) of 0.14 ± 0.014. Here, we demonstrate that our model can be used simply to calculate emotional arousal. We showed 32 video clips with different content and emotional intensity to 47 participants, who, after each video, reported their level of emotional arousal. We then calculated the pupil size due only to luminosity and subtracted it from the total recorded pupil size, obtaining the component due only to emotional arousal. From the latter, we predicted the arousal of each participant for each video. We obtained an average correlation between predicted and selfreported arousal of 0.65 ± 0.12, an R2 of 0.43 ± 0.12, and an NRMSE of 0.27 ± 0.036. Instead, using the measured pupil size, without subtracting the component due to luminosity, we obtained dramatically worse results: an average correlation between the predicted and self-reported arousal of 0.26 ± 0.15, an R2 of 0.09 ± 0.089, and an NRMSE of 0.42 ± 0.054. Our results highlight that separating the emotional and luminosity components from pupillary responses is critical to accurately and precisely predicting arousal. Our approach improves the reliability of pupil-based emotional arousal assessment. It opens new avenues for real-time emotion detection in various ap-plications, including human-computer interaction, mental health monitoring, neuromarketing, and affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are a central component of human life, influencing most, if not all, aspects of cognition and behavior  [1] . Arising from complex psychological and physiological processes, they shape how we perceive and interact with the world around us  [2] , modulating key cognitive functions  [3] , such as memory, attention, perception, and learning, and shaping our experiences  [4] ,  [5] . Emotions also play an integral role in decision-making  [4] ,  [6]  and in our social lives through their involvement in empathy, cooperation, and facilitating meaningful interpersonal interactions  [7] ; in workplaces, as they can enhance team dynamics and leadership, contributing to better collaboration and overall productivity  [8] ,  [9] ; in educational settings  [10] ,  [11]  and in mental health diagnosis and treatment  [12] .\n\nAccurate emotion detection is a crucial skill across multiple contexts. For example, teachers can observe students' facial expressions or vocal tones in education to identify emotions like confusion or disengagement  [10] . This helps them to adjust their teaching methods, ensuring that students remain engaged, especially in remote learning environments where non-verbal cues are more complex to detect  [11] . In mental health, emotion detection plays a key role in diagnosing and treating conditions such as depression, where recognizing patients' emotional responses is vital for effective care  [12] .\n\nEmotional changes manifest through various physiological and behavioral indicators, such as heart rate, blood pressure, facial expressions, body language, eye movements, and pupil dilation  [13] ,  [14] . Each of these indicators provides valuable insights into emotional states  [15] , contributing to the development of comprehensive models of human behavior (i.e., Arousal Detection Models)  [16] . Accurately detecting emotional arousal has essential applications in fields such as psychology  [17] , education  [18] , human-computer interaction  [19] , healthcare  [20] ,  [21] , and artificial intelligence  [22] . AI-powered emotion detection systems have proven helpful in optimizing emotion detection by analyzing facial expressions  [22] , vocal tones  [23] , and physiological signals  [24] , enabling more accurate and efficient identification of emotional states. However, AI-powered systems for using pupillary responses to measure emotional arousal have progressed more slowly than those using other physiological and behavioral markers due to the challenge of separating emotional arousal effects from luminosity effects, an issue we address in this study  [25] ,  [26] .\n\nPupil dilation in response to emotional stimuli is controlled by the autonomic nervous system (ANS) through the interplay of its two branches  [27] ,  [28] . The sympathetic nervous system (SNS) triggers pupil dilation (mydriasis) via norepinephrine (a neurotransmitter and hormone), enhancing visual processing during arousal or stress  [29] . The parasympathetic nervous system (PNS) regulates pupil constriction (miosis) by acting through the oculomotor nerve and the neurotransmitter acetylcholine. This function is most active during calm or restful states, reducing the amount of light entering the eye and adjusting pupil size for near vision  [30] . Indeed, previous research shows that pupils dilate within 200 milliseconds of the release of noradrenaline in response to emotional arousal  [31] -  [34] .\n\nIt has been well documented that emotional arousal triggers pupil dilation, and ambient luminosity levels also significantly influence pupil size  [26] ,  [35] -  [37] . Pupils typically constrict in bright conditions and dilate in darker environments  [38] , with size varying by up to 50%  [39] , hindering researchers' use of pupil dilation as a direct measure of an individual's emotional state  [36] ,  [40] . Research on emotion detection must account for the impact of luminosity on pupil size, especially in our digital world, where screen brightness and contrast may significantly influence pupil dilation  [41] . To reliably use pupil size as an emotional arousal marker, it is crucial to distinguish genuine emotional responses from luminosity-induced changes  [42] .\n\nPrevious research suggests that accounting for luminosityinduced pupil size changes can help to isolate the emotional responses with varying levels of success  [43] . Previous research used gray screens to measure baseline pupil diameter  [44] -  [46] , but this method has proven insufficient for complex stimuli such as videos with dynamic visual elements  [36] , where changing luminosity and visual elements continually affect pupil responses. In the case of a video, an ideal baseline would be a set of emotionally neutral stimuli of the same luminosity as each video frame, which is challenging to engineer.\n\nIn this paper, we propose a more realistic and scalable approach: we directly predicted the component of pupil size influenced by luminosity for each emotional frame instead of including an emotionally neutral frame with the same luminosity repeated for the entire video. This is not the first attempt to use such an approach, with previous studies attempting to disentangle the effects of emotional arousal and light exposure on pupil size for dynamic stimuli such as videos. Our research builds on previous attempts to use such an approach while addressing some of their shortcomings. For example, Nakayama et al.  [47]  developed a hyperbolic model to predict pupil size as a function of the luminosity of the computer screen in a dark laboratory. Other researchers have used simpler models. For example, Tarnowski et al.  [48]  hypothesized a linear relationship between luminosity and pupil size. However, as the relationship between pupil size and luminosity is exponentially decreasing  [37] , we developed an exponential model in our previous study  [49] .\n\nIn the present study, we extended and improved our previous work as follows:\n\n1) First, we tested the model we presented in  [49]  in a dark laboratory with 10 participants. 2) Second, we assessed the model's functionality in a welllit laboratory environment, testing its robustness across varying luminosity levels. 3) Third, we calculated the portion of pupil size due to emotional arousal in response to emotional videos, subtracting the portion of pupil size attributable to luminosity from the total pupil size. We asked participants to watch 32 video clips, which varied in emotional content, and report the level of arousal for each of them. 4) Finally, we tested hyperbolic and linear models, recording the pupil's size in response to different luminosity levels. We then compared the results with those obtained with our method, demonstrating that our exponential model better describes the relationship between pupil size and luminosity. Our approach can effectively remove luminosity-induced confounders in pupil-based emotion research, especially in low-light conditions, filling a significant gap. Nakayama and colleagues  [47]  also sought to separate pupil dilation related to emotional arousal from luminosity-induced changes in video content. They also applied their method to emotional images and demonstrated a luminosity and arousal effect on pupil size using ANOVA. We wanted to go one step further: predicting self-reported arousal using pupil size, which was not addressed in Nakayama and colleagues' work.\n\nRaiturkar et al.  [25]  developed a linear model to predict pupil size based on light intensity, allowing for the isolation of emotional arousal by subtracting the effects of light from actual pupil measurements, and applied it to the analysis of emotional videos. However, they did not record self-reported arousal while watching the movie clips.\n\nWhere possible, we have applied the existing models mentioned above to our data, with the advantage that we included a comparatively large sample of 47 participants (compared to a maximum of 10 in the studies above), a comparatively rich video collection of 32 emotional video clips, and the fact that we had recorded self-reported arousal for each participant and each video.\n\nIn some cases, it was not possible to apply methods from previous studies to our data due to insufficient details regarding the corresponding models. For example, Asano et al.  [50] ,  [51]  developed models that considered the temporal evolution of pupil size, which we did not do. Their first model consisted of a linear model and the second of a neural network. We only managed to evaluate the first model but could not assess the neural network model since we could not access their trained neural network. Asano and colleagues  [50]  wrote that the second method performed better than the first one, and they tested both methods with emotional video clips but did not detect self-reported arousal. A more detailed analysis of the differences between our method and those of the other researchers is reported in the Discussion.\n\nIn summary, by eliminating confounding luminosity effects, we aimed to improve the accuracy and reliability of pupilbased emotion detection compared to traditional approaches. In the following, we present a robust and easy-to-use algorithm to isolate the effects of brightness on pupil size.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Ii. Methodology",
      "text": "Our approach to developing a model for arousal detection consisted of two study phases:\n\n1) Development of the Luminosity Effect Prediction Model (LEPM): In this study phase, we developed a model to predict pupil size changes purely caused by luminosity. By subtracting the predicted pupil size from the observed pupil size, we isolated the pupillary response attributable to emotional stimuli.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2) Development Of The Arousal Detection Model (Adm):",
      "text": "To show that the LEPM developed in the previous study phase is accurate and has ecological validity, we trained a simple linear model on participants' pupil size data while watching emotional video clips and used it to predict arousal with and without the LEPM algorithm.\n\nWe developed a relatively complex model for the first phase (LEPM) and a very simple one (ADM) for the second phase. Our objective was to demonstrate that the result obtained using the LEPM, i.e., computing the component of pupil size due to arousal, is immediately applicable without knowledge of mathematics or advanced machine learning. As presented here, ADM is deliberately simple, as this study aimed to show the utility of LEPM. Ongoing developments in our lab will build on and extend ADM and its effectiveness in arousal state prediction. In this paper, we wanted to show that our LEPM model allowed us to obtain the component of pupil size due to arousal and that it correlates well with participants' subjective arousal state.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Development Of The Luminosity Effect Prediction Model",
      "text": "Here, we refer to a monochrome image as one in which all pixels have one color only. In contrast, a non-monochrome image may contain pixels of different colors. Additionally, by primary colors, we refer to red, green, and blue (RGB) colors. For example, when referring to a primary-color monochrome image, we will indicate an image with all pixels of one color only: red, green, or blue. Note that images used in real-world applications are commonly non-monochrome and non-primary colors. Finally, by gray, we mean the color resulting from combining red, green, and blue equally, e.g., 65% red, 65% green, and 65% blue.\n\nWe first developed the LEPM model to isolate emotional pupil responses and subtracted its predictions from the observed pupil sizes. The LEPM development process involved several steps. First, we created a model that predicted the luminosity of a computer screen. Second, we developed a model that predicted pupil size in a dark laboratory using monochrome images (red, green, blue, and gray) with varying luminosities, which were tested on monochrome and nonmonochrome emotionally neutral images. Third, we validated the model in a well-lit laboratory environment.\n\n1) Computing the luminosity of a monochrome image on a computer screen in a dark laboratory: The brightness of a monitor is a function of the RGB values of all pixels. In the case of a monochrome image, we can write:\n\nwhere L is the luminosity value, k is a constant (scaling factor) that depends on the brightness/contrast settings of the monitor, and (r, g, b) ∈ [0%, 100%] 3 are the RGB intensity values of a monochrome image displayed on the screen  [49] . The relationship between RGB intensity and luminosity values is non-linear for a commercial monitor. For example, considering a primary color, e.g., blue, the luminosity corresponding to an intensity value of 60% is not twice as high as that corresponding to a value of 30%. Instead, the increase in luminosity as a function of RGB intensity is smaller at lower intensity values and dramatically increases as the intensity approaches maximum  [49] . Furthermore, as more colors are used, this non-linear relationship changes. Therefore, the function f in Equation 1 cannot be easily described analytically; therefore, we tabulated its values in a look-up table to accurately calculate the luminosity of images or frames displayed on a computer screen. The input of our look-up table was an RGB value corresponding to three integers ranging from 0 to 100. Luminosity values were measured empirically. We created a set of images with different RGB values. The luminosity of each image was measured in a dark laboratory using an LX1010BS digital lux meter at a distance of 65 cm, following standard eye-tracking protocols to ensure controlled conditions for accurate gaze and pupil response measurements  [49] . If we wanted to consider all values, we would have 100 3  entries and would have to generate 100 3 images. We sub-sampled this space to make the search computationally less expensive, producing 1330 images. When querying the table, the corresponding luminosity value was returned if the input matched a point. Otherwise, a weighted average of the eight nearest points was calculated, with weights inversely proportional to their distances. In the case of images composed of only one of the fundamental colors, instead of eight nearest neighbors, we used only two  [49] .\n\nWe calculated the values in the lookup table using a Dell Precision M6500 (1920x1080) monitor set to 100% brightness and 100% contrast, which we will refer to as the \"reference monitor\"  [49] . Different monitor models have different luminosity capabilities. However, when testing five monitors of different sizes and brands, we consistently observed that the non-linear relationship described in equation 1 remains substantially valid, except for the scale factor k which must be recalculated every time the brightness and contrast settings are changed (see calibration procedure below). In particular, once the appropriate value of k was found, the maximum error between the monitors was 3 lux  [49] , which is tolerable for our purposes. This suggests that, regardless of the manufacturer and model of the monitor, the relationship between the brightness values and RGB intensity is highly consistent between different monitors. The calibration procedure described below allowed us to compensate for the differences between the possible monitor settings and between different monitors, provided that the monitor settings did not change during the experiment. For the reference monitor, the value of k was 1  [49] .\n\n2) Computing the luminosity of a non-monochrome image on a computer screen in a dark laboratory: When an image or video frame is not monochrome, one can calculate the luminosity of each pixel and average the values obtained. However, we observed that the luminosity of an image is similar to that of a single-color image, with the average RGB value of the original image calculated by averaging the RGB values of all its pixels. To validate this observation, we selected 100 images: 50 taken from the Internet and 50 generated by assigning a random RGB value to each pixel. We then generated the corresponding monochrome images using the average RGB value of each original image. The luminosity of the original images differed from the luminosity of the corresponding monochrome images by 1.5 lux on average, with a maximum difference of 3 lux, tolerable for our purpose. For computational reasons, using the corresponding monochrome images is preferable, as it is much faster than calculating the luminosity for each pixel by querying our lookup table and then averaging  [49] .\n\n3) Predicting pupil size due to luminosity at different wavelengths: The second step consisted of developing a model to predict the pupil size of a participant looking at a monochrome image on the screen 1  . This model incorporated the function f , defined in the previous step, which transforms RGB values into luminosity. This luminosity function then served as an input to another function responsible for mapping luminosity to pupil size. Notably, this second function is highly nonlinear and exhibits a decreasing trend  [37] .\n\nFor each primary color plus gray, the model can be described using the following equation:\n\nwhere PS is the pupil size, f (r, g, b) is part of the luminosity function that gives the predicted luminosity (see Equation  1 ), and a, b, c, and d are coefficients to be determined by fitting the model to the recorded data  [49] . The parameters b and c also absorb the parameter k in the Equation  1 . Our goal was to assess the pupil response either to one of the fundamental wavelengths (red, green, blue) or to their uniform combination (gray).\n\nTo fit the model shown in Equation  2 , for each of the four colors, we performed an initial estimation of the coefficients using data recorded from ten participants and then devised a method to recalibrate the coefficients a, b, c, and d to new experimental participants. We sequentially presented 101 monochrome frames (video) of each of the four colors to the 10 participants for the initial estimation. We started from the lowest intensity (0%, 0%, 0%) to the highest intensity (e.g., for red 100%, 0%, 0% and for gray 100%, 100%, 100%), increasing the brightness by 1% every second. Each of the four videos lasted 101 seconds and was presented full-screen on the reference monitor (1920x1080). We measured pupil size for each video frame at a data rate of approximately  2 60 Hz and at a distance of 65 cm between the screen and the participant, using a Tobii Nano Pro eye tracker and the iMotions software (https://imotions.com/) in a dark (unlit) laboratory. At the same time, we measured luminosity during each video frame using the lux meter  [49] . When an eye blink occurred (indicated by a value of -1 in the dataset output of the iMotions software), it was replaced with an estimated pupil size, which was calculated as the median pupil size for that specific frame. For each RGB intensity and participant, we considered the average pupil size recorded during the video frame presentation corresponding to that particular luminosity. Subsequently, we calculated the average pupil size data across all participants for each luminosity level. Figure  1  displays the average pupil size (aggregated across participants) plotted against luminosity for each color (scattered points). The dotted lines represent the fitted models 3 . As shown, pupil size diminished exponentially with rising luminosity  [37] ,  [49] . Figure  1  shows that the chosen model appropriately describes the data (R2 ≥ 0.98).  Table  I  shows the coefficient values of the model presented in Figure 1  [49] . These were then re-calibrated for each new participant. Indeed, it is essential to redetermine the coefficients before starting an experiment and tailoring them to the individual participant. To achieve this, each participant completed a calibration procedure before any experiment by watching a video of 27 monochrome frames  [49] . The frames consisted of four colors -red, blue, green, and gray-each displayed for 4 seconds, resulting in a total duration of 108 seconds. These images were generated using all possible combinations of three RGB intensity levels 0%, 50%, and 100%, covering a range from the darkest (0%, 0%, 0%) to the brightest (100%, 100%, 100%).\n\nOut of the 27 monochrome frames, nine were selected to calibrate the pupil size prediction model. These included three frames for each primary color (red, blue, and green) and gray. Since the black frame (0%, 0%, 0%) is common to all colors, only one instance of it was used for calibration across all colors 4 .\n\nOverall, the calibration process simultaneously accounts for individual variations in pupillary responses due to color, luminosity, and the difference between different monitors, establishing a reliable baseline for accurate subsequent measurements.\n\n4) Two different approaches to predict the pupil size of a participant looking at a non-primary color monochrome image in a dark laboratory: The model described by Equation  2 enabled us to predict the pupil size of an individual observing a primary color or gray image with brightness levels ranging from 0 to 100. Our next objective was to extend the capability to predict pupil size in response to monochrome non-primary color images of any arbitrary color and luminosity, such as, for example, an image with RGB values (100, 75, 80), which represents a dark pink hue, with red as the dominant component.\n\nWe pursued this goal with two approaches. The first approach, which we called \"gray-based,\" considered the pupil variation dependent only on the total luminosity of the image and not on the particular color. On the other hand, the second approach, called \"color-based,\" considered the luminosity of each primary color differently in the image. We expected that the first approach would work best for images where the three primary colors were fairly balanced, while the second approach would work best for images where one of the three primary colors was prevalent.\n\nThe \"gray-based\" approach involved calculating the image luminosity on the screen as a weighted average of the luminosities of the eight nearest images in the look-up table, as explained above. We then calculated the relative pupil size as if the image were equally composed of the three primary colors (according to the definition of gray used in this paper). In practice, we used the fourth equation of the set of Equations 2. To evaluate the \"gray-based\" approach and the calibration procedure, we tested 18 participants across five different monitors with varying resolutions, ranging from a minimum of 1920 × 1080 to a maximum of 3840 × 2160. The test measurements included 18 (27 -9 = 18) images from the calibration video and nine additional test images.\n\nThe \"color-based\" approach considered each image of a non-primary color (r, g, b) as the superposition of three images of a single fundamental color: (r, 0, 0), (0, g, 0) and (0, 0, b). This approach considers separately the effect of each wavelength on pupil size. For example, an image with RGB values  (64, 86, 45)  was treated as three separate images: (64, 0, 0), (0, 86, 0) e (0, 0, 45). We calculated the pupil size relative to each of the three images and assumed that the actual pupil 4 In the case of red, the three calibration points were (0%, 0%, 0%), (50%, 0%, 0%), and (100%, 0%, 0%), representing increasing intensity levels of red. Similarly, for green, the calibration points were (0%, 0%, 0%), (0%, 50%, 0%), and (0%, 100%, 0%). For blue, they were (0%, 0%, 0%), (0%, 0%, 50%), and (0%, 0%, 100%). For gray, the selected points were (0%, 0%, 0%), (50%, 50%, 50%), and (100%, 100%, 100%). The remaining frames were used for testing. size can be obtained as a linear combination of those three separate ones:\n\nwhere PS is predicted pupil size. The weights of each contribution to PS in Equation 3 are equal to the contribution of each fundamental color in the initial image  [49]    (45) .\n\n(4) The \"color-based\" approach was tested simultaneously with the \"gray-based\" approach (with the same 18 participants, the same screens and resolutions).\n\n5) Combining the \"gray-based\" and the \"color-based\" approaches: Since the \"gray-based\" method performed more accurately on images where the three primary colors had similar intensities, while the \"color-based\" method excelled when one color was dominant, we combined both approaches to leverage their respective strengths. Consequently, the pupil size was calculated as a linear combination of the values obtained with the \"gray-based\" approach and those obtained with the \"color-based\" approach:\n\nwith the constraint a gray + a red + a green + a blue = 1.\n\nWe named this approach the \"combined approach\" (see Figure  2 ). Given the constraint in Equation  6 , it is expected that, after fitting, the value of the multiplicative coefficient K is close to 1 and that of the intercept C is close to 0. This would mean that the value of the pupil size PS is given by the values PS gray , PS red , PS green , and PS blue added together in different percentages.\n\nThe \"combined\" approach was tested simultaneously with the \"gray-based\" and the \"color-based\" approaches (with the same 18 participants, the same five different screens with different resolutions, the same images, etc.).\n\nFor each participant, first, we calculated the values PS gray , PS red , PS green , and PS blue using the nine images of the calibration procedure, as described above. Then, we trained and tested the model in Equation  5 (see Figure  2 ).\n\nSince the best results were obtained with the \"combined\" method, we used only this approach from then on and throughout this paper.  6) Testing the Luminosity Effect Prediction Model in a well-lit laboratory and with non-monochrome images: Next, we tested our LEPM model on non-monochrome images in a dark and a light environment. We recruited an additional 10 participants and used the 27 monochrome test images mentioned above, plus 46 non-monochrome emotionally neutral images (see Figure  3 ). Each image was displayed for 4 seconds, resulting in a video of 4 minutes and 52 seconds plus 36 seconds for the calibration procedure (9 images). We considered the mean pupil size across the 4 seconds for each image. Having taken the average value over a relatively long period compared to the time constant of the pupil size variation, we did not develop a dynamic model in this study. As before (when testing the model in a dark laboratory), we used five different monitors and computers/laptops with different brightness levels and resolutions.\n\nThe results obtained with the non-monochrome images were not as promising as those obtained with monochrome images. In particular, images that contained a very bright region within a dark figure yielded much worse results than monochrome images. This was because an image could be dark on average, but the participant looked at the bright region, potentially inducing a more pronounced pupillary constriction than that elicited by an image with its average luminosity level. Therefore, it was necessary to determine where participants were looking through the recorded eye-tracking data. Specifically, we analyzed a region with a 300-pixel radius centered on where the gaze was directed at each instant. If the average luminosity of that specific region exceeded that of the average luminosity of the entire image, we used that region average instead. This method provided a more accurate representation of the subjective luminance and led to improved results, which we present in the Results Section III-A.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Development Of Arousal Detection Model",
      "text": "The second part of the study aimed to demonstrate that the pupil size value discounted from luminosity was immediately usable for detecting emotional arousal levels without the need for complex pre-processing, advanced machine learning, or even deep learning models.\n\nWe recruited 55 participants who, after the calibration procedure described above, watched 32 video clips with emotional content while pupil size was recorded. After each video clip, participants self-reported the experienced emotions on a scale of 0 to 9 for 12 emotions (more details in Data Collection part II-B1), which was used as the target value to predict selfreported arousal (more details in Arousal Ground Truth part II-B2).\n\n1) Data Collection: The categorization of emotions has long been debated, with two dominant approaches: basic emotion theory, which defines emotions as distinct categories, and multi-dimensional theory, which conceptualizes emotions as points along interconnected scales  [52] . This ongoing debate highlights the complexity of understanding human emotions. Our study adopted the multi-dimensional theory, utilizing Russell's widely recognized circumplex model  [53] , which maps emotions along two key dimensions: valence (pleasantness) and arousal (activation level). Valence represents emotions ranging from unpleasant to pleasant, while arousal captures the intensity of emotional states, from calm to highly energized. These two dimensions are widely recognized in affective computing and cognitive research as a robust framework for modeling emotional experiences  [54] ,  [55]  and have the advantage of providing a comprehensive yet efficient representation of complex emotional states.\n\nWe selected 32 video clips with emotional content. These were classified into four distinct groups based on Russell's circumplex model  [53] : high arousal with positive valence, high arousal with negative valence, low arousal with positive valence, and low arousal with negative valence (this classification was tested through a pilot study with 37 participants).\n\nAs part of the participant recruitment process, we conducted a comprehensive screening procedure to identify potential indicators of conditions such as alexithymia, anxiety, depression, and personality disorders, which are known to influence emotional processing and responses  [56] -  [58] . 130 participants were initially screened, with 55 meeting the inclusion criteria, by which eligible participants must be 18-65 years old, with sufficient English language proficiency to sign consent and follow questionnaires, but without severe psychiatric conditions affecting emotional recognition (e.g., schizophrenia, bipolar disorder, autism; we accepted mild anxiety, depression, or alexithymia as measure by GAD-7, PHQ-9, and TAS-20), without intellectual disabilities, significant cognitive impairments, or recent neurological disorders (e.g., epilepsy, stroke, TBI in the past year). The final sample enrolled in the study All participants provided informed consent before participation, and the study was approved by the University of Essex Ethics Committee (reference number: ETH2425-0176) as part of a more extensive study that included recording pupil size, facial expression data, galvanic skin response, photoplethysmogram, and electroencephalogram. However, we only report on measurement and data processing related to pupil size. The iMotions software (https://imotions.com/)  [59]  managed the experimental protocol and facilitated efficient data acquisition.\n\nThe experiment was conducted in a well-lit laboratory environment (see Figure  4 ). At the beginning of the experiment, and before displaying the video clips, participants were shown the calibration video described in section II-A. Participants were instructed to focus on the center of the screen during the calibration procedure.\n\nFollowing calibration, the main experiment started. Each trial began with a 5-second gray screen displaying a central cross to neutralize previous emotional states, followed by an emotional video clip. We used 32 clips with a duration variable between 10 and 100 seconds, with different durations distributed uniformly across valence and arousal values, and shown in a randomized order. To determine the emotional arousal, we only considered the salient intervals from each video (see below). After each clip, participants completed a questionnaire (extended from  [54]  to include a broader range of emotions) to assess 12 emotional states: positive, negative, happy, calm, content, amused, excited, angry, sad, disgusted, fearful, and bored. Participants rated their emotional responses on a scale from 0 (no emotion) to 9 (high emotion).\n\nThe final sample comprised 47 participants after excluding those based on three criteria: poor Tobii calibration that affected gaze and pupil size measurements, insufficient focus (e.g., participants consistently not looking at the screen or appearing to be asleep), and inconsistent responses, such as identical ratings for all questions. The excluded participants also showed low inter-rater reliability scores (Krippendorff's alpha < 0.4) as their answers were highly inconsistent with the other participants' answers. The pupil size data from these remaining participants were then pre-processed as explained in section II-A.\n\n2) Arousal Ground Truth: As ground truth for the ADM, we used the participants' questionnaire responses, mapping their reported emotion scales into the arousal/valence twodimensional space. Initially, we explored factor analysis for this purpose; however, this failed to distribute emotions effectively across valence and arousal dimensions, resulting in suboptimal model training outcomes (see results VII). Instead, we employed Individual Scaling (INDSCAL)  [54] , a multidimensional scaling approach that reduces high-dimensional data into interpretable lower-dimensional representations. This approach condensed participants' responses from 12 emotional dimensions into two key dimensions: valence (x-coordinate) and arousal (y-coordinate) (see Figure  5 ). INDSCAL creates a common multidimensional space, like multidimensional scaling, and an individual space for each subject. This methodology improved the precision of predicting emotional states by providing a robust ground truth for training the ADM.\n\n3) Pupil Data Pre-processing: The first pre-processing step involved identifying and marking blink-related artifacts. Data points identified as blinks by the eye tracker were substituted with null values. To account for the potential pre-and postblink effects on pupil size measurements, an additional window of two milliseconds before and after each blink was also marked as null. Following this, we performed data imputation to fill the null values. The missing data points were replaced using a linear interpolation approach that considers the trends in the data immediately before and after the null segments. By interpolating in this way, we maximized the maintenance of the natural trajectory of pupil size changes over time, preserving the physiological relevance of the data while minimizing noise introduced by blinking.\n\n4) Arousal Effect Prediction: After pre-processing the pupil size data, we applied the LEPM to predict the pupil size for each video frame across all video clips without influencing ambient and screen luminosity.\n\nWe trained the model described by the Equations 5 and 6 by calculating PS gray , PS red , PS green , and PS blue frame by frame and the coefficients a gray , a red , a green , a blue , K, and C for each video clip. We hypothesized that since there was also an arousal component (in addition to the light component), the fitting would be worse and that the error in the fitting would be due specifically to the effect of arousal, given that the model was designed to capture only the component due to luminosity. In other words, we hypothesized that the measured pupil size would be equal to the pupil size predicted by the LEPM model as an effect of luminosity plus a residual:\n\nwhere\n\nso that\n\nThe Residual was the portion of pupil size that cannot be explained by luminosity, i.e., the portion due to arousal:\n\nHence, to extract arousal-related information for each video, we subtracted the predicted pupil size due to luminosity from the pre-processed measured pupil size:\n\nThe resulting difference represents the emotional arousal level of participants watching the video clips, as measured by pupil size.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "5) Adm Testing:",
      "text": "To test the model, we compared its predictions of a participant's arousal while watching video clips -based on the measured pupil size -against the arousal ground truth. For each video and each participant, we had the self-reported arousal value (ground truth) Arousal self-reported , the recorded pupil size value PS measured , and the pupil size value corrected for luminosity and due only to arousal PS arousal . We then calculated the Pearson correlation between the pupil size corrected and not corrected for luminosity and the selfreported arousal value (see results section). One of the key challenges in this process was the considerable variation in the proportion of emotional content across different video clips. For example, some videos maintained a consistent emotional tone throughout their duration, while others contained emotional content only in specific segments, such as peaking toward the end while remaining neutral initially.\n\nWhen completing the questionnaire at the end of each video clip, participants were asked to report their emotion and arousal levels without specifying the exact moment in the video when they occurred. To identify the emotionally salient intervals that would allow us to predict arousal accurately at each relevant moment throughout the video duration, we randomly selected a sub-sample of 10 participants from the 55 who previously participated in the study. Participants were asked to indicate the salient intervals of each video. The final salient interval of a video was defined as the interval for which there was the highest degree of agreement among the 10 participants. The overlap between the chosen interval and that indicated by each participant was, on average, 74% ± 15%, considering all the videos, indicating a reasonable degree of agreement. For some videos, e.g., boring ones, the salient intervals corresponded to the entire video, while for other videos, e.g., scary ones, only some segments were salient. Finally, we only computed the average pupil size for each video and participant across the salient intervals, with and without our luminosity correction.\n\nWhile our results demonstrated the potential of pupil size as a reliable indicator of emotional arousal, we wanted to demonstrate that PS arousal , derived from our model, can be used without further processing or the use of complex machine learning techniques, becoming accessible to the entire scientific community, while still obtaining robust results. For this purpose we used the following procedure. We used a leaveone-participant-out cross-validation approach, which consisted of temporarily eliminating one participant at a time from our dataset as if they were a new participant.\n\nWe then calculated the pupil size corrected for luminosity PS arousal and self-reported arousal Arousal self-reported for each video and each participant remaining in our dataset. We then assumed a simple linear relationship between the size of the pupil corrected for luminosity (component due to arousal) and self-reported arousal:\n\nWe fitted the model 12 to all the videos and all the participants (except one). We obtained a good fit (see Results).\n\nBy inverting Equation  12 , we obtained an estimate of selfreported arousal Arousal self-reported as a function of pupil size corrected for luminosity:\n\nWe then repeated the same procedure for pupil size not corrected for luminosity. In this case, in Equations 12 and 13, PS arousal must be replaced with PS measured . We predicted the self-reported arousal value for the eliminated participant starting from the pupil size, using Equation  13 , and for all the videos. We compared the obtained values with the ground truth for the luminosity-corrected and uncorrected pupil size. We repeated this procedure, eliminating one participant at a time, and finally calculated the average results for our sample.\n\nAll the code relative to the models presented in this paper was written in Python.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iii. Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Results Of The Luminosity Effect Prediction Model (Lepm)",
      "text": "The model illustrated in section \"A. Development of Luminosity Effect Prediction Model\" effectively predicts pupil size based on the RGB intensity values of the images on the screen in both dark and bright environments. The calibration procedure described above ensures that the method is flexible and adapted to inter-subjective differences, as well as the settings and type of screen used. As explained in section II-A, we initially used two different approaches, named \"graybased\" and the \"color-based\", and then we used a method consisting of combining both approaches, named the \"combined\" approach. We assessed each approach using leave-one-imageout cross-validation, i.e., training the model on 27 images, eliminating one image at a time, and predicting the pupil size measured when the eliminated image was shown.\n\nThe results obtained from the 18 participants in a dark laboratory, i.e., the mean Pearson correlation coefficient with the mean and max p-value, the mean R2, the mean NRMSE 7 , and the mean percentage error is shown in Table  II .\n\nTable  III , on the other hand, shows the results obtained by aggregating the data across 18 participants.\n\nThe results for the \"combined\" method were better than when using only the \"gray-based\" method or only the \"colorbased\" method, as shown in tables II and III, demonstrating that the two methods contribute synergistically to the evaluation of pupil size. Since the best results were achieved with the \"combined\" method, all the subsequent results were obtained using this approach.\n\nAs explained in the paragraph Testing the Luminosity Effect Prediction Model (LEPM) in a well-lit laboratory and with non-monochrome images, we tested the LEPM and calibration procedure on non-monochrome, non-primary color images 7 Normalized Root Mean Squared Error (NRMSE) is a scale-independent error metric that allows comparison between datasets or models with different scales. We computed it by dividing the Root Mean Squared Error (RMSE) by a normalization factor, consisting of the range (max-min) of the observed data.  and in both a dark and a lit environment. The test was conducted with 10 participants using a video composed of 27 monochrome images and 46 non-monochrome emotionally neutral images with different colors and brightness. Figures  6  and 8  show the results for participant IFL3 in a dark and a well-lit laboratory, respectively. The average results across participants, obtained in the dark and well-lit laboratories, are shown in Table  IV .\n\nFigures  7  and 9  and Table  V , instead, show the results obtained by aggregating the data from all the participants. The average error between the two conditions is about 0.2%, and the difference in NRMSE is about 0.02, highlighting the model's effectiveness across dark and well-lit settings. Additionally, the model performs consistently across various   monitor brightness and contrast settings and under different ambient lighting levels. Therefore, there was no need to modify the model to account for changes in environmental luminosity, given that our calibration procedure takes this into account effectively. However, we repeated the calibration every",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Results Of The Arousal Detection Model (Adm)",
      "text": "To test the ADM, we predicted the arousal of a participant watching video clips based on the measured pupil size and compared it with the ground truth. For each participant and each video clip, we plotted the value of the measured pupil size PS measured throughout the video clip. This is shown in Figure  10  (green line) for a particular participant, and a video with increasing high emotional intensity (Figure  10(a) , and one with low intensity (Figure  10(b) ). In the same figure, we show the average RGB value frame by frame, representing the luminous intensity of the video (red line). We then calculated the pupil size component due to luminosity PS luminosity using the LEPM (see Figure  10 , blue line). Finally, we calculated the pupil size component due to arousal PS arousal by subtracting PS luminosity from PS measured , according to Equation 11 (see the black line in Figure  10 ).\n\nOur analysis revealed a clear distinction in the arousal responses between high-arousal and low-arousal video clips, as shown in the example illustrated in Figure  10 . In fact, in the left panel (high arousal), the pupil size component due to arousal PS arousal (black line) assumes higher values than in the right panel (low arousal).\n\nWe then calculated the average of the PS arousal (black line in Figure  10 ) in the salient intervals, as explained in the paragraph Model Testing of section II-B. For example, for the videos represented in Figure  10 , the salient intervals were [0s, 20s] for video clip (a) and [0s, 10s] for video clip (b). Finally, we plotted the pupil size corrected for luminosity PS arousal versus the self-reported arousal Arousal self-reported for all video clips and each participant, which is shown in Figure  11  for participant XPi3pA, where the red circles correspond to each video clip (see figure  11  for more information). For that participant, we obtained a correlation of 0.71 (p = 1.684e-06); see the red line. For the measured pupil size, non-corrected for luminosity PS measured , the correlation with the ground truth arousal was much worse, e.g., see for participant XPi3pA in Figure  11 , where the blue dots correspond to each video clip. We obtained a correlation of 0.01 (p = 0.971) for that participant (see blue line in the figure  11 ).\n\nTable  VI  shows all participants' average results. Correcting for luminosity dramatically increased correlation compared to non-corrected pupil size. The most surprising result, however, is that pupil size had no predictive power for arousal without correcting for luminosity since its correlation with self-reported arousal was not significantly different from zero (mean p = 0.2283). To further test the model performance, we computed the predicted arousal Arousal self-reported utilizing the equations 12 and 13 and using a leave-one-participant out cross-validation as explained in the paragraph ADM Testing. We plotted the predicted arousal against the ground truth one Arousal self-reported both with and without correction for luminosity, as shown in Figure  12  for the participant XPI3pA.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Table Vi: Relationship Between Self-Reported Arousal And Pupil Size With And Without Correction For Luminosity",
      "text": "The average results across all the participants are shown in the left panel in Table  VII . Again, correcting for luminosity produces a dramatic increase in correlation compared to uncorrected pupil size. While the left panel shows the results obtained by calculating self-reported arousal using individual scaling, the right panel shows the results obtained by calculating self-reported arousal using factor analysis. As mentioned in the paragraph Development of Arousal Detection  Model, INDSCAL works much better than factor analysis in this case.\n\nFinally, we noticed that, in the case of pupil size corrected for luminosity, the average values of the coefficients of Equation 12 were a = 0.3463 ± 0.0551, b = -0.0126 ± 0.00038 and the fit was quite good (R2 = 0.567 ± 0.073). The variance of the coefficients and R2 is since leaving one participant out at a time changes the slope and the intercept of the regression line. However, this variation is minuscule (the standard deviation is much smaller than the mean), indicating that our sample was sufficiently large. In the case of pupil size not corrected for luminosity, the average values of the coefficients were a = 0.36723 ± 0.06341, b = 3.8296 ± 0.0120, and the fit was inferior (R2 = 0.023 ± 0.007).\n\nC. Testing models developed by other researchers with our data a) Testing Nakayama's Hyperbolic Model with our data: Nakayama and colleagues  [47]  found a hyperbolic relation-\n\nwhere x is the luminosity. The function f should predict the variation in pupil size with luminosity to within a constant multiplicative factor. The authors, therefore, hypothesized that if they use emotionally neutral images, dividing the measured pupil size by the function f should yield a constant factor that they called the \"compensated pupil size\". If, on the other hand, images with emotional content are used, a variation is found concerning the constant value, and the variation is due to the arousal. We have tested this model on our 73 emotionally neutral test images and obtained a coefficient of variation of 0.09 ± 0.025. Our exponential model works better, obtaining a coefficient of variation of 0.03 ± 0.006.\n\nWe then used Nakayama's Hyperbolic Model with our emotional video clips. We used the calibration procedure to calculate the only coefficient of the model, consisting of the ratio between the measured pupil size and the standardized pupil size. The comparison with our method is presented in the table VIII. Nakayama's method worked less well than ours. For example, there is a worsening of the NRMSE of 51.8%.\n\nb) Testing Linear Models (e.g., Raiturkar's and Asano's Model) with our data: Both Raiturkar et al.  [25]  and Asano et al.  [50] ,  [51]  have developed dynamic linear models that take into account the temporal response of pupil dilation. We did not develop dynamic models because we took the average pupil size over relatively long intervals (at least 5 seconds). However, we wanted to test linear models with our data and obtained very poor results, as shown in the table VIII.\n\nc) Testing our model without self-reported arousal: As mentioned, where possible, we applied to our data the methods developed by other researchers to exploit the fact that we had recorded the self-reported arousal for each participant and each video, something that, to the best of our knowledge, no other research on the effect of luminosity on pupil size measurement has done. We investigated how important it is to include self-reported arousal. To do this, we used the approach other researchers have used: asking 10 independent judges (new participants) to give an arousal value to each video (e.g., Raiturkar et al.  [25] ). For each clip, we took the average value. We then used that arousal value in our method, instead of the self-reported one, to observe how much the average arousal of the independent judges could be predicted by pupil size corrected for the luminosity of our 47 participants. The comparison between the results obtained with our model using as arousal values those self-reported by the 47 participants, i.e., what we described so far, and the results obtained with our model using as arousal values those given by the 10 independent judges, is shown in table VIII. The model trained on the arousal reported by the 10 independent judges provides clearly worse results than the one trained on the arousal reported by the 47 participants. For example, there is a worsening in NRMSE of 188%. This is because when training a model, one must use a ground truth recorded from the same subjects from which the predictors are recorded, in this case, respectively, the self-reported arousal and pupil size of the 47 participants in the study.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Iv. Discussion",
      "text": "In this study, we developed a novel mathematical model to remove the effects of stimulus and ambient luminosity and isolate emotional arousal information from measures of pupil size. This model serves as a simple yet effective and robust tracking system that can be used as a standalone physiological indicator for emotion arousal detection or can be integrated into more complex machine learning models. Our approach was designed to overcome longstanding methodological obstacles in the field, most notably the confounding effects of dynamic changes in luminosity on pupil dilation, which have historically obscured accurate emotion detection.\n\nPrior studies in emotion detection have addressed the impact of luminosity on pupillary responses using methods such as Linear Regression  [25] ,  [48] , controlled visual stimuli  [60] , isoluminant conditions  [47] , and baseline correction  [44] . For instance, LR models the relationship between pupil size and luminosity, enabling the subtraction of luminosity effects from pupil measurements  [48] . Controlled visual stimuli and isoluminant conditions provide stable or uniform luminosity across experimental stimuli to minimize variability. Baseline correction uses pupil size measurements from a gray screen stimulus, subtracting them from emotion stimulus pupil sizes to remove the influence of luminosity and account for individual differences  [44] . Other more advanced methods further incorporate localized gaze luminosity alongside average luminosity to improve accuracy  [25] ,  [28] ,  [44] ,  [47] ,  [60] ,  [61] . However, those methods have considerable limitations. For example, Nakayama et al.  [47]  developed a method to predict pupil size as a function of computer screen luminosity in a darkroom. Here, we tested that method and obtained inferior results when applying their method to our stimulus set. The average coefficient of variation of the ratio between measured and standardized pupil size went from 0.03 for our method to 0.09 for Nakayama's method. We hypothesize that the advantage in our model resulted from the calibration process required to adapt to different subjects and monitors. Furthermore, our exponential model, although more complex and computationally expensive, predicts pupil size as a function of luminosity more accurately. Our model better predicts self-reported arousal starting from pupil size compared to the hyperolic model, going from an average R2 of 0.10 ± 0.087 to an average R2 of 0.436 ± 0.125 (see also Table  VIII ).\n\nOther linear models, for example, Raiturkar et al.  [25]  and Asano et al.  [50] ,  [51]  have given even more inferior results with applied to our dataset (see table  VIII ). Nevertheless, the models of Raiturkar and Asano are particularly interesting because, in addition to developing a calibration procedure, as we did, they have taken into account the temporal response of pupil dilation, something we have not done. Our method relies on average pupil size over relatively long time intervals of at least 5 seconds, making the dynamic aspects that occur over hundreds of milliseconds irrelevant. In fact, we used videos of at least 8 seconds, like most video clips with emotional content. However, it is possible to have real cases in which it is important to measure emotional arousal with stimuli of much shorter duration. Therefore, in future research, we propose to extend our model to account for dynamic changes. We are working to combine our approach with the dynamic approach of Raiturkar et al.  [25]  and to further make it automatic rather than requiring the manual adjustment of the time delay for each participant and each video, which introduces potential bias and lacks standardized methodology. An intriguing approach to developing a dynamic model involves employing a neural network incorporating temporal parameters, as demonstrated in  [50] ,  [51] .\n\nIn this study, we applied existing methodologies from the literature wherever possible to maximize the value of our dataset, which comprises a relatively large sample of over 40 participants, a collection of over 30 emotional video clips, and self-reported arousal ratings for each participant and each video. To our knowledge, no previous research on pupil size measurement has incorporated self-reported arousal ratings with this level of detail. Some authors have hypothesized and described in their articles the emotional content of the videos shown to the participants without, however, having feedback from the participants themselves(  [48] ,  [47] ,  [50] ,  [51] ). Raiturkar and colleagues  [25]  used a less subjective approach based on the evaluations of three independent judges and used these as ground truths, generating \"excitement scores\" for the emotionally involved scenes. However, we contend that this method may not fully capture individuals' emotional perception variability. We hypothesize that not just physiological responses but also conscious emotional perception and the link between the two vary significantly from individual to individual and are influenced by subjective experiences. To demonstrate this hypothesis, we deliberately disregarded the emotions reported by our 47 participants in the study and instead used the emotions reported by 10 independent judges as ground truth. As reported in paragraph III-C0c, we trained our model on the pupil size corrected for luminosity in the 47 participants, using the average arousal reported by the 10 independent judges as ground truth. The predictive capacity of our model worsened considerably because the pupil size recorded from the 47 participants could not be significantly linked to the excitement reported by other people (the 10 independent judges). In our case, this is evident because reported arousal levels for a specific video clip varied considerably across participants. Therefore, we believe that it is essential to consider individual differences and cognitive biases in studies of emotional perception. In a future study, we aim to show that the pupils' responses to emotional stimuli can also vary according to health conditions  [35] , personality traits  [44] , and emotional sensitivity  [62] .\n\nOur model effectively predicts emotional arousal with high accuracy, surpassing the results of uncorrected pupil size metrics. Its ability to adjust to different luminosity conditions and accommodate individual variations represents a notable enhancement compared to current models. Our novel approach incorporates several innovative features. First, by recognizing that pupillary responses vary across individuals  [63] , we ensured that our model accounts for these individual differences, enhancing its generalizability. Second, we addressed the fluctuating nature of ambient luminosity in real-world scenarios, a critical factor often overlooked in controlled laboratory studies. Third, our methodology considers the varying luminosity levels of different visual stimuli, including the inherent variability in monitor brightness and contrast settings. Taken together, we have addressed previous methodological obstacles through a calibration video technique that normalizes these multiple sources of luminosity variation, significantly improving the accuracy of emotion detection.\n\nOur two-stage model accounts for pupil changes due to luminosity and isolates the emotional component from the pupil size measure, offering greater precision and broader application than existing methods. Based on the groundwork laid by previous research, our approach provides a more comprehensive solution for the real-world complexities of experiencing emotions. In the present study, we aimed to demonstrate that the pupil size corrected for luminosity generated by our Python software correlates well with self-reported arousal without needing further processing. This makes our software easy to use, even without advanced knowledge of mathematics, machine learning, or neurobiology. Therefore, we deliberately confined the greater complexity to the first stage of our model, the one responsible for calculating the portion of pupil size due to luminosity. The second stage is very simple and consists of a linear regression that relates arousal to pupil size. Still better results may be obtained by hypothesizing a more complex relationship than a linear one and by applying more complex machine learning and deep learning techniques to the luminosity-corrected pupil size time series. In future studies, we will make our model dynamic, extend it to extreme lighting conditions, such as when wearing eye-tracker glasses outside a laboratory on a sunny day, and adapt it to experiments without eye-trackers and with regular webcams in online experiments.\n\nDue to its ease of implementation, our model has practical applications, particularly in mental health monitoring and consumer research. It requires only existing human insights software, such as iMotions, and basic eye-tracking hardware  [59] . No advanced skills are required to detect emotional arousal from pupil size, even without using more complex biosignals to analyze, such as GSR, EEG, and ECG. In the mental health field, where a shortage of trained counselors persists  [64] , our model offers an accessible AI-driven solution for emotion detection, benefitting under-served populations. In consumer research, pupillometry has been used to assess emotional responses to advertisements but has not seen much popularity among marketers because of its low reliability  [65] . By eliminating luminosity effects, our model enables marketers to gauge the emotional arousal elicited by their advertisements much more accurately. Extensive trials in real-world settings will be essential to improve generalizability, and models should be tested across diverse populations and stimuli.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "V. Conclusion",
      "text": "In conclusion, our research represents a significant step in emotion arousal detection using physiological indicators. By developing a model that effectively isolates emotional responses from pupillary reactions to luminosity, we have laid the groundwork for more accurate and versatile emotion detection systems. As we continue to refine and expand this technology, the potential for enhancing our understanding of human emotions and their applications in various domains remains exciting and profound.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: displays the average pupil size (aggregated",
      "page": 4
    },
    {
      "caption": "Figure 1: shows that the chosen model",
      "page": 5
    },
    {
      "caption": "Figure 1: Pupil size as a function of luminosity for red, green,",
      "page": 5
    },
    {
      "caption": "Figure 1: [49]. These were then re-calibrated for each",
      "page": 5
    },
    {
      "caption": "Figure 2: ). Given the constraint in Equation 6, it is expected that, after",
      "page": 6
    },
    {
      "caption": "Figure 2: Testing the combined approach.",
      "page": 6
    },
    {
      "caption": "Figure 3: Experiment flow of LEPM validation.",
      "page": 6
    },
    {
      "caption": "Figure 3: ). Each image was displayed for 4",
      "page": 6
    },
    {
      "caption": "Figure 4: Experimental setup.",
      "page": 7
    },
    {
      "caption": "Figure 4: ). At the beginning of the experiment,",
      "page": 7
    },
    {
      "caption": "Figure 5: ). INDSCAL creates",
      "page": 8
    },
    {
      "caption": "Figure 5: Aggregate ground truth responses across all partici-",
      "page": 8
    },
    {
      "caption": "Figure 6: Relationship between measured and predicted pupil size",
      "page": 10
    },
    {
      "caption": "Figure 7: Measured and predicted pupil size in a dark laboratory",
      "page": 10
    },
    {
      "caption": "Figure 8: Measured and Predicted pupil size in a well-lit lab-",
      "page": 11
    },
    {
      "caption": "Figure 9: Measured and Predicted pupil size in a well-lit labora-",
      "page": 11
    },
    {
      "caption": "Figure 10: (green line) for a particular participant, and a video",
      "page": 11
    },
    {
      "caption": "Figure 10: (b)). In the same figure, we",
      "page": 11
    },
    {
      "caption": "Figure 10: , blue line). Finally, we calculated the",
      "page": 11
    },
    {
      "caption": "Figure 10: In fact, in",
      "page": 11
    },
    {
      "caption": "Figure 10: ) in the salient intervals, as explained in the",
      "page": 11
    },
    {
      "caption": "Figure 10: , the salient intervals were",
      "page": 11
    },
    {
      "caption": "Figure 11: for participant XPi3pA, where the red circles correspond to",
      "page": 11
    },
    {
      "caption": "Figure 11: for more information). For that",
      "page": 11
    },
    {
      "caption": "Figure 11: , where the blue dots correspond to each video",
      "page": 11
    },
    {
      "caption": "Figure 12: for the participant XPI3pA.",
      "page": 11
    },
    {
      "caption": "Figure 10: Plot of participant XPI3pA’s response for selected high-arousal (a) and low-arousal (b) videos, showing measured",
      "page": 12
    },
    {
      "caption": "Figure 11: Pupil size comparison with and without luminosity",
      "page": 12
    },
    {
      "caption": "Figure 12: Predicted arousal versus self-reported arousal with and",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": "High Arousal Positive Valence (HP)"
        },
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": ""
        },
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": ""
        },
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": ""
        },
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": "LP_HP_1\nLP_6"
        },
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": ""
        },
        {
          "Low Arousal Negative Valence (LN)\nHigh Arousal Negative Valence (HN)\nLow Arousal Positive Valence (LP)": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Color-Based",
          "Correlation": "<\n0.82\n(p\n10−7)",
          "R2\nScore": "0.67",
          "NRMSE": "0.10",
          "Average Er-\nror": "8.62 ± 0.79\n%"
        },
        {
          "Method": "Gray-Based",
          "Correlation": "<\n0.85\n(p\n10−7)",
          "R2\nScore": "0.73",
          "NRMSE": "0.09",
          "Average Er-\nror": "7.89 ± 0.80\n%"
        },
        {
          "Method": "Combination\nof\ncolor\nand\ngray-based\napproach",
          "Correlation": "<\n0.94\n(p\n10−7)",
          "R2\nScore": "0.88",
          "NRMSE": "0.06",
          "Average Er-\nror": "4.62 ± 0.62\n%"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion theory and research: Highlights, unanswered questions, and emerging issues",
      "authors": [
        "C Izard"
      ],
      "year": "2009",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "2",
      "title": "Emotion regulation: Current status and future prospects",
      "authors": [
        "J Gross"
      ],
      "year": "2015",
      "venue": "Psychological inquiry"
    },
    {
      "citation_id": "3",
      "title": "The influences of emotion on learning and memory",
      "authors": [
        "C Tyng",
        "H Amin",
        "M Saad",
        "A Malik"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "4",
      "title": "The impact of emotion on perception, attention, memory, and decision-making",
      "authors": [
        "T Brosch",
        "K Scherer",
        "D Grandjean",
        "D Sander"
      ],
      "year": "1920",
      "venue": "Swiss medical weekly"
    },
    {
      "citation_id": "5",
      "title": "Predictable chaos: A review of the effects of emotions on attention, memory and decision making",
      "authors": [
        "V Leblanc",
        "M Mcconnell",
        "S Monteiro"
      ],
      "year": "2015",
      "venue": "Advances in Health Sciences Education"
    },
    {
      "citation_id": "6",
      "title": "Emotion and decision making",
      "authors": [
        "J Lerner",
        "Y Li",
        "P Valdesolo",
        "K Kassam"
      ],
      "year": "2015",
      "venue": "Annual review of psychology"
    },
    {
      "citation_id": "7",
      "title": "How emotions regulate social life: The emotions as social information (easi) model",
      "authors": [
        "G Van Kleef"
      ],
      "year": "2009",
      "venue": "How emotions regulate social life: The emotions as social information (easi) model"
    },
    {
      "citation_id": "8",
      "title": "The role of emotional intelligence in effective business management",
      "authors": [
        "A Raza"
      ],
      "year": "2024",
      "venue": "Journal of Management & Social Science"
    },
    {
      "citation_id": "9",
      "title": "The role of emotional intelligence in leadership effectiveness",
      "authors": [
        "P Kalraa"
      ],
      "year": "2024",
      "venue": "Scholar's Digest: Journal of Commerce & Management"
    },
    {
      "citation_id": "10",
      "title": "Integrating artificial intelligence to assess emotions in learning environments: A systematic literature review",
      "authors": [
        "A Vistorte",
        "A Deroncele-Acosta",
        "J Ayala",
        "A Barrasa",
        "C López-Granero",
        "M Martí-González"
      ],
      "year": "2024",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "11",
      "title": "Revolutionizing education and therapy for students with autism spectrum disorder: A scoping review of ai-driven tools, technologies, and ethical implications",
      "authors": [
        "F Habibi",
        "S Sedaghatshoar",
        "T Attar",
        "M Shokoohi",
        "A Kiani",
        "A Malek"
      ],
      "year": "2025",
      "venue": "Revolutionizing education and therapy for students with autism spectrum disorder: A scoping review of ai-driven tools, technologies, and ethical implications"
    },
    {
      "citation_id": "12",
      "title": "Facial emotion recognition in children with externalising behaviours: A systematic review",
      "authors": [
        "S Cooper",
        "C Hobson",
        "S Van Goozen"
      ],
      "year": "2020",
      "venue": "Clinical child psychology and psychiatry"
    },
    {
      "citation_id": "13",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Physiological changes associated with emotions",
      "authors": [
        "D Galvano"
      ],
      "venue": "Physiological changes associated with emotions"
    },
    {
      "citation_id": "15",
      "title": "The effects of measuring emotion: Physiological reactions to emotional situations depend on whether someone is asking",
      "authors": [
        "K Kassam",
        "W Mendes"
      ],
      "year": "2013",
      "venue": "PloS one"
    },
    {
      "citation_id": "16",
      "title": "Development of emotional intelligence: Towards a multi-level investment model",
      "authors": [
        "M Zeidner",
        "G Matthews",
        "R Roberts",
        "C Mac-Cann"
      ],
      "year": "2003",
      "venue": "Human development"
    },
    {
      "citation_id": "17",
      "title": "Review on psychological stress detection using biosignals",
      "authors": [
        "G Giannakakis",
        "D Grigoriadis",
        "K Giannakaki",
        "O Simantiraki",
        "A Roniotis",
        "M Tsiknakis"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "18",
      "title": "Measuring emotions in education using wearable devices: A systematic review",
      "authors": [
        "S Ba",
        "X Hu"
      ],
      "year": "2023",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "19",
      "title": "Expanding the frontiers of visual analytics and visualization",
      "authors": [
        "C Peter",
        "B Urban"
      ],
      "year": "2012",
      "venue": "Expanding the frontiers of visual analytics and visualization"
    },
    {
      "citation_id": "20",
      "title": "Emotional states detection approaches based on physiological signals for healthcare applications: A review",
      "authors": [
        "D Vallejo",
        "A Saddik"
      ],
      "year": "2020",
      "venue": "Smart Cities"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition for healthcare surveillance systems using neural networks: A survey",
      "authors": [
        "M Dhuheir",
        "A Albaseer",
        "E Baccour",
        "A Erbad",
        "M Abdallah",
        "M Hamdi"
      ],
      "year": "2021",
      "venue": "2021 International Wireless Communications and Mobile Computing (IWCMC)"
    },
    {
      "citation_id": "22",
      "title": "Facial emotion recognition through artificial intelligence",
      "authors": [
        "J Ballesteros",
        "G Ramírez",
        "F Moreira",
        "A Solano",
        "C Pelaez"
      ],
      "year": "2024",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition of the singing voice: Toward a real-time analysis tool for singers",
      "authors": [
        "D Szelogowski"
      ],
      "year": "2021",
      "venue": "Emotion recognition of the singing voice: Toward a real-time analysis tool for singers",
      "arxiv": "arXiv:2105.00173"
    },
    {
      "citation_id": "24",
      "title": "Emotion recognition using physiological signals",
      "authors": [
        "L Li",
        "J.-H Chen"
      ],
      "year": "2006",
      "venue": "International conference on artificial reality and telexistence"
    },
    {
      "citation_id": "25",
      "title": "Decoupling light reflex from pupillary dilation to measure emotional arousal in videos",
      "authors": [
        "P Raiturkar",
        "A Kleinsmith",
        "A Keil",
        "A Banerjee",
        "E Jain"
      ],
      "year": "2016",
      "venue": "Proceedings of the ACM Symposium on Applied Perception"
    },
    {
      "citation_id": "26",
      "title": "The effects of emotional arousal on pupil size depend on luminance",
      "authors": [
        "J Pan",
        "X Sun",
        "E Park"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "27",
      "title": "Role of sympathetic and parasympathetic systems in reflex dilatation of the pupil: Pupillographic studies",
      "authors": [
        "O Lowenstein",
        "I Loewenfeld"
      ],
      "year": "1950",
      "venue": "Archives of Neurology & Psychiatry"
    },
    {
      "citation_id": "28",
      "title": "Pupil size variation as an indication of affective processing",
      "authors": [
        "T Partala",
        "V Surakka"
      ],
      "year": "2003",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "29",
      "title": "Ocular autonomic nervous system: An update from anatomy to physiological functions",
      "authors": [
        "F Wu",
        "Y Zhao",
        "H Zhang"
      ],
      "year": "2022",
      "venue": "Vision"
    },
    {
      "citation_id": "30",
      "title": "Autonomic nervous system disorders",
      "authors": [
        "R Henderson",
        "J Spies"
      ],
      "year": "2020",
      "venue": "Hankey's Clinical Neurology"
    },
    {
      "citation_id": "31",
      "title": "Pupillometry",
      "authors": [
        "S Sirois",
        "J Brisson"
      ],
      "year": "2014",
      "venue": "Wiley Interdisciplinary Reviews: Cognitive Science"
    },
    {
      "citation_id": "32",
      "title": "Fatigue and arousal modulations revealed by saccade and pupil dynamics",
      "authors": [
        "J.-T Chen",
        "Y.-C Kuo",
        "T.-Y Hsu",
        "C.-A Wang"
      ],
      "year": "2022",
      "venue": "International journal of environmental research and public health"
    },
    {
      "citation_id": "33",
      "title": "Emotion detection based on pupil variation",
      "authors": [
        "C.-L Lee",
        "W Pei",
        "Y.-C Lin",
        "A Granmo",
        "K.-H Liu"
      ],
      "year": "2023",
      "venue": "Healthcare"
    },
    {
      "citation_id": "34",
      "title": "Pupillometry and autonomic nervous system responses to cognitive load and false feedback: An unsupervised machine learning approach",
      "authors": [
        "E Alshanskaia",
        "G Portnova",
        "K Liaukovich",
        "O Martynova"
      ],
      "year": "2024",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "35",
      "title": "Neurobehavioral meaning of pupil size",
      "authors": [
        "N Grujic",
        "R Polania",
        "D Burdakov"
      ],
      "year": "2024",
      "venue": "Neuron"
    },
    {
      "citation_id": "36",
      "title": "Background luminance effects on pupil size associated with emotion and saccade preparation",
      "authors": [
        "Y.-G Cherng",
        "T Baird",
        "J.-T Chen",
        "C.-A Wang"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "37",
      "title": "Pupil size estimation based on spatially weighted corneal flux density",
      "authors": [
        "Y Zhang",
        "S Li",
        "J Wang"
      ],
      "year": "2019",
      "venue": "IEEE Photonics Journal"
    },
    {
      "citation_id": "38",
      "title": "Evidence that pupil size and reactivity are determined more by your parents than by your environment",
      "authors": [
        "A Ansari",
        "J Vehof",
        "C Hammond",
        "F Bremner",
        "K Williams"
      ],
      "year": "2021",
      "venue": "Frontiers in Neurology"
    },
    {
      "citation_id": "39",
      "title": "Clinical Methods: The History, Physical, and Laboratory Examinations",
      "authors": [
        "R Spector"
      ],
      "year": "1990",
      "venue": "Clinical Methods: The History, Physical, and Laboratory Examinations"
    },
    {
      "citation_id": "40",
      "title": "Influences of luminance and accommodation stimuli on pupil size and pupil center location",
      "authors": [
        "A Mathur",
        "J Gehrmann",
        "D Atchison"
      ],
      "year": "2014",
      "venue": "Influences of luminance and accommodation stimuli on pupil size and pupil center location"
    },
    {
      "citation_id": "41",
      "title": "Eye movements, convergence distance and pupil-size when reading from smartphone, computer, print and tablet",
      "authors": [
        "A Miranda",
        "E Nunes-Pereira",
        "K Baskaran",
        "A Macedo"
      ],
      "year": "2018",
      "venue": "Scandinavian Journal of Optometry and Visual Science"
    },
    {
      "citation_id": "42",
      "title": "Pupil size reflects the focus of feature-based attention",
      "authors": [
        "P Binda",
        "M Pereverzeva",
        "S Murray"
      ],
      "year": "2014",
      "venue": "Journal of neurophysiology"
    },
    {
      "citation_id": "43",
      "title": "The effects of age, refractive status, and luminance on pupil size",
      "authors": [
        "M Guillon",
        "K Dumbleton",
        "P Theodoratos",
        "M Gobbe",
        "C Wooley",
        "K Moody"
      ],
      "year": "2016",
      "venue": "Optometry and vision science"
    },
    {
      "citation_id": "44",
      "title": "The pupil as a measure of emotional arousal and autonomic activation",
      "authors": [
        "M Bradley",
        "L Miccoli",
        "M Escrig",
        "P Lang"
      ],
      "year": "2008",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "45",
      "title": "Pupil adaptation corresponds to quantitative measures of autism traits in children",
      "authors": [
        "A Dicriscio",
        "V Troiani"
      ],
      "year": "2017",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "46",
      "title": "The pupillary light response as a physiological index of aphantasia, sensory and phenomenological imagery strength",
      "authors": [
        "L Kay",
        "R Keogh",
        "T Andrillon",
        "J Pearson"
      ],
      "year": "2022",
      "venue": "Elife"
    },
    {
      "citation_id": "47",
      "title": "Controlling the effects of brightness on the measurement of pupil size as a means of evaluating mental activity",
      "authors": [
        "M Nakayama",
        "I Yasuike",
        "Y Shimizu"
      ],
      "year": "2021",
      "venue": "Pupil Reactions in Response to Human Mental Activity"
    },
    {
      "citation_id": "48",
      "title": "Eye-tracking analysis for emotion recognition",
      "authors": [
        "P Tarnowski",
        "M Kołodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "49",
      "title": "Towards an accurate measure of emotional pupil dilation responses: A model for removing the effect of luminosity",
      "authors": [
        "Z Pansara",
        "G Navyte",
        "H Gillmeister",
        "C Cinel",
        "V Feo"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Metrology for eXtended Reality"
    },
    {
      "citation_id": "50",
      "title": "A neuralnetwork-based eye pupil reaction model for use with television programs",
      "authors": [
        "S Asano",
        "M Nakayama",
        "Y Shimizu"
      ],
      "year": "2021",
      "venue": "A neuralnetwork-based eye pupil reaction model for use with television programs"
    },
    {
      "citation_id": "51",
      "title": "Pupil reaction model using a neural network for brightness change",
      "authors": [
        "S Asano",
        "I Yasuike",
        "M Nakayama",
        "Y Shimizu"
      ],
      "year": "2021",
      "venue": "Pupil Reactions in Response to Human Mental Activity"
    },
    {
      "citation_id": "52",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "53",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "54",
      "title": "Identifying core affect in individuals from fmri responses to dynamic naturalistic audiovisual stimuli",
      "authors": [
        "J Kim",
        "J Wang",
        "D Wedell",
        "S Shinkareva"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "55",
      "title": "Multimodal emotion recognition: A comprehensive review, trends, and challenges",
      "authors": [
        "M Ramaswamy",
        "S Palaniswamy"
      ],
      "year": "2024",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "56",
      "title": "Alexithymia and emotional processing: A longitudinal mixed methods research",
      "authors": [
        "A Da Silva",
        "A Vasco",
        "J Watson"
      ],
      "year": "2018",
      "venue": "Research in Psychotherapy: Psychopathology, Process, and Outcome"
    },
    {
      "citation_id": "57",
      "title": "Emotion processing in depression and anxiety disorders in older adults: Systematic review",
      "authors": [
        "V Gray",
        "K Douglas",
        "R Porter"
      ],
      "venue": "BJPsych Open"
    },
    {
      "citation_id": "58",
      "title": "Emotion processing deficits: A liability spectrum providing insight into comorbidity of mental disorders",
      "authors": [
        "M Kret",
        "A Ploeger"
      ],
      "year": "2015",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "59",
      "title": "How pupil responses reveal brain activity",
      "year": "2025",
      "venue": "How pupil responses reveal brain activity"
    },
    {
      "citation_id": "60",
      "title": "Spontaneous but not explicit processing of positive sentences impaired in asperger's syndrome: Pupillometric evidence",
      "authors": [
        "L Kuchinke",
        "D Schneider",
        "S Kotz",
        "A Jacobs"
      ],
      "year": "2011",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "61",
      "title": "Eye tracking: A comprehensive guide to methods and measures",
      "authors": [
        "K Holmqvist"
      ],
      "year": "2011",
      "venue": "Eye tracking: A comprehensive guide to methods and measures"
    },
    {
      "citation_id": "62",
      "title": "Pupillary response abnormalities in depressive disorders",
      "authors": [
        "S Laurenzo",
        "R Kardon",
        "J Ledolter"
      ],
      "year": "2016",
      "venue": "Psychiatry research"
    },
    {
      "citation_id": "63",
      "title": "Photorealistic models for pupil light reflex and iridal pattern deformation",
      "authors": [
        "V Pamplona",
        "M Oliveira",
        "G Baranoski"
      ],
      "year": "2009",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "64",
      "title": "An ensemble-based machine learning model for emotion and mental health detection",
      "authors": [
        "A Jonnalagadda",
        "M Rajvir",
        "S Singh",
        "S Chandramouliswaran",
        "J George",
        "F Kamalov"
      ],
      "year": "2023",
      "venue": "Journal of Information & Knowledge Management"
    },
    {
      "citation_id": "65",
      "title": "Validity, reliability, and applicability of psychophysiological techniques in marketing research",
      "authors": [
        "Y Wang",
        "M Minor"
      ],
      "year": "2008",
      "venue": "Psychology & Marketing",
      "doi": "10.1002/mar.20206"
    }
  ]
}