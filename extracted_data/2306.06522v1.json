{
  "paper_id": "2306.06522v1",
  "title": "Ts-Moco: Time-Series Momentum Contrast For Self-Supervised Physiological Representation Learning",
  "published": "2023-06-10T21:17:42Z",
  "authors": [
    "Philipp Hallgarten",
    "David Bethge",
    "Ozan Özdenizci",
    "Tobias Grosse-Puppendahl",
    "Enkelejda Kasneci"
  ],
  "keywords": [
    "self-supervised learning",
    "physiological signal processing",
    "EEG",
    "emotion recognition",
    "human activity recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Limited availability of labeled physiological data often prohibits the use of powerful supervised deep learning models in the biomedical machine intelligence domain. We approach this problem and propose a novel encoding framework that relies on self-supervised learning with momentum contrast to learn representations from multivariate time-series of various physiological domains without needing labels. Our model uses a transformer architecture that can be easily adapted to classification problems by optimizing a linear output classification layer. We experimentally evaluate our framework using two publicly available physiological datasets from different domains, i.e., human activity recognition from embedded inertial sensory and emotion recognition from electroencephalography. We show that our self-supervised learning approach can indeed learn discriminative features which can be exploited in downstream classification tasks. Our work enables the development of domainagnostic intelligent systems that can effectively analyze multivariate time-series data from physiological domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A major challenge in developing high-performing machine learning algorithms in the biomedical domain is the limited availability of labeled data, whereas state-of-the-art deep learning models require more labeled data to generalize better. This poses a significant problem if such models shall be leveraged to automatize tasks in this domain. Accordingly, in this paper, we focus on the following: \"Is it possible to learn physiological domain agnostic deep learning architectures?\" and \"Can we train such an architecture in a self-supervised manner without the need for labeled data?\".\n\nOne promising avenue in this research domain focuses on deep representation learning  [1] [2] [3] [4] . These models learn to extract generalizable features, omitting the need for domainspecific hand-designed features. We particularly focus on selfsupervised learning (SSL) frameworks, a machine learning paradigm where the model is trained without labeled data,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Eeg Inertial Sensory",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ts-Moco Emotion Recognition",
      "text": "Human Activity Recognition i.e., the supervision signal necessary for training is generated on-the-fly from the data itself. We investigate the architectural design space in depth and develop a novel SSL framework to learn generalized features across multiple physiological signals. Our proposed deep SSL framework, annotated as TS-MoCo, optimizes a general feature encoder agnostic to the physiological domain. We introduce a two-fold contrastive loss function for optimization based on time-series momentum contrast  [5] . The feature encoder of TS-MoCo predicts physiological labels using only one dense layer that must be fitted on the labeled data on top of the learned embedding space. In contrast to prior research that focuses on single, specialized domains, we evaluate our framework in two physiological data domains, i.e., emotion classification, and human activity recognition. Overall, we address the following questions:\n\n• How can we learn a generalizable feature encoder architecture and learning algorithm for diverse domains of physiological signal recordings?\n\n• What is the technical design space for such a deep signal processing framework?\n\n• How are the domains affecting the encoding architecture? arXiv:2306.06522v1 [cs.LG] 10 Jun 2023\n\nOur framework thereby focuses on learning a signal domain agnostic feature encoder. We propose and explore selfsupervised learning of general feature encoders as an alternative approach for medical deep learning researchers that enables use in downstream tasks with few labeled data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Deep Learning For Physiological Signal Processing",
      "text": "In recent years various works explored deep learning based signal processing pipelines for physiological time-series data recordings. In  [6] , different autoencoder architectures for encoding physiological signals recorded through smartwatches are compared.  [7]  implements a transfer learning approach to train a deep learning model for human activity recognition from radar databases. Another set of works on physiological signal processing focuses on electroencephalography (EEG) signals. Cura et al.  [8]  use a deep convolutional neural network, to detect epilepsy from patients EEG recordings, and Geoffroy et al.  [9]  detect drowsiness from EEG and electrocardigrams (ECG) with a deep learning architecture.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Self-Supervised Learning For Physiological Domains",
      "text": "Self-supervised models have been previously explored for various physiological domains since physiological data monitoring is becoming ubiquitous, resulting in large amounts of unlabeled data. In  [10] , the authors present an SSL framework trained with a contrastive loss that learns representations for human activity recordings of inertial sensory data from multiple devices. The authors of  [11]  demonstrate that one can successfully improve emotion classification from electroencephalography (EEG) signals by training a generative adversarial network to synthesize data using SSL. Recently  [1]  proposed a vanilla SSL framework, TS-TCC, trained with a combination of temporal and contextual contrastive loss that achieved significant results across several domains.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Self-Supervised Learning With Momentum Contrast",
      "text": "In SSL with momentum contrast, two models of the same architecture are used: a student encoder and a teacher (or momentum) encoder. The student encoder is trained via loss backpropagation, while the teacher model parameters are set as an exponentially moving average of the student model parameters. Seminal work in  [5]  proposes MoCo, a framework that is trained with momentum contrast. The momentum encoder is hereby used to build a representation dictionary on-the-fly, which is then matched to the representation output by the student encoder network in a contrastive loss. MoCo achieves superior performance than supervised baselines on multiple vision tasks. In the BYOL framework by  [12] , both the momentum encoder and the student encoder predict representations where different input augmentations are applied. BYOL achieves comparable results to state-of-the-art baselines in vision tasks while neglecting the need for negative samples. Finally,  [13]  proposes DINO as an SSL framework trained with momentum contrast without negative samples using vision transformers  [14]  as a backbone and evaluated on vision tasks.\n\nMomentum contrast reduces computational demand in comparison to conventional contrastive methods by neglecting negative samples, and exhibits a promising potential. However, to date, this SSL mechanism is not explored in depth for analyzing biomedical data. In our work, we tackle this research gap by proposing a framework based on momentum contrast and evaluating it on different physiological domains.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Training Schema",
      "text": "We denote the signal with t discrete timesteps by x = [x t ] T t=0 , and signals can be given with labels into C classes. The training procedure we used is twofold. Firstly, we use a self-supervised pre-training paradigm to learn a generalizable feature encoder. Secondly, we optimize a single linear classifier using the representations by this encoder to solve the downstream classification problem. During self-supervised pre-training, the supervision signal is created directly from the signal itself, omitting the need for laborious annotations. We use a combination of two supervision signals.\n\n1) Reconstructing the Future: The first supervision signal is generated by splitting of the final K time-steps from the signal i.e. by splitting x into two parts\n\nThe former is encoded into a representation vector c by a feature encoder described through f ENC with parameters θ, and the latter is used as target for a reconstruction head, that predicts the subsequent timesteps t ∈ [T -K, T ] of its input from a context vector. To compare the predicted subsequent time-steps xfuture with the ground truth x future we use a mean squared error loss:\n\n(1)\n\n2) Momentum Contrast: The second supervision signal is generated through momentum contrast  [5] . Two identical architectures, teacher and student models, are used during training. The teacher model is provided with x past as input and predicts an output, in our case, a representation vector:\n\n(\n\nFor the student model, the signal is first augmented with an augmentation function before computing the output:\n\nTraining aims to guide the student towards computing the same representation as the teacher from an augmented version of the input. We compare these two output vectors using a cosine similarity loss metric:\n\n3) Parameter Update: After each iteration, student model parameters are updated by loss backpropagation\n\nwhere the loss weight λ is set as a hyperparameter. The teacher model parameters θ <T > are updated in order to state exponentially moving average (EMA) of the student model parameters θ <S> , as utilized in  [5] :\n\nwith τ being the momentum weight hyperparameter. 4) Linear Evaluation: Following self-supervised pretraining, we employ a linear evaluation scheme to evaluate the learned feature encoder. We first use the pre-trained frozen encoder to compute representations of non-augmented input signals x. Then a single linear layer that predicts class logits from these representations is utilized according to\n\nwhere a conventional cross-entropy loss is used.\n\nB. TS-MoCo Framework Architecture 1) Augmentation Function: Whether or not training the proposed framework is successful heavily depends on the used augmentation function as it defines the encoding and reconstruction task. If the augmentation is too weak, the tasks become trivial and can be solved by the models without learning the underlying concepts of the data. On the other hand, if the augmentation is too strong, training the model may fail due to limited capabilities of the architecture. Further, we explicitly refrain from domain-specific augmentation functions to allow a fast an easy adaption of our framework to various domains. By employing a window-wise temporal masking, we fulfill aforementioned constraints to the augmentation function. Hereby, a certain percentage p of the signal is overwritten with a masking token i.e. 0. Especially for high-frequency signals, masking singular timesteps scattered along the timeaxis would state an easy task, therefore we assure that all masked timesteps span a continuous window.\n\n2) Feature Encoder: The used feature encoder consists of a tokenizer, the addition of positional embeddings, and a transformer encoder. As a first step, the signal used as input to the encoder is mapped into an embedding space by a single linear layer named tokenizer\n\nNext, like in  [15] , a classification token (< CLS >) is prepended to the signal. The value of this token is initialized randomly and learned during training.\n\n[h\n\nIn order to allow the feature encoder to exploit positional information of the signal values, we add positional embeddings to the tokenized signals, i.e. we add a unique vector to the signal values of each timestep. We create the positional embedding matrix P from sinusoidal according to previous work  [16] . We also introduce a hyperparameter α ∈ {0, 1}, allowing us to enable/disable positional embeddings:\n\nFinally, the latent signal is input to a transformer encoder  [16]  of depth d, outputting a contextual embedding vector for each input token [h\n\n]. We use the output embedding for the classification token as context encoding c.\n\n3) Reconstruction Head: We use Gated Recurrent Units (GRU) for the reconstruction head. The context vector output by the student feature encoder is thereby used as initial hidden state. Further, we apply teacher-forcing for predicting the future timesteps, i.e. we pass the ground truth values for a timestep besides a hidden state to the GRU cell for predicting the values of the next timestep. Our overall architecture is illustrated in Figure  2 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Experimental Study",
      "text": "A. Datasets 1) SEED: The SJTU Emotion EEG Dataset (SEED)  [17] ,  [18]  is a dataset of EEG recordings from 15 subjects during viewing of emotion-eliciting videos. Signals were recorded from 62 channels at 1000 Hz and later downsampled to 200 Hz. Following recent work  [4] ,  [19] ,  [20] , we applied a 4 -40 Hz bandpass filter and segmented the signals into 2 s non-overlapping windows. EEG recording signals are labeled to be of either negative, neutral, or positive emotions.\n\n2) UCIHAR: This dataset  [21]  for human activity recognition comprises recordings from 30 subjects during six activities of daily living: walking, walking upstairs, walking downstairs, sitting, standing, laying, measured from waist-mounted smartphone accelerometer and gyroscope. Class imbalance is handled via stochastic undersampling  [19] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Baseline Comparison Models",
      "text": "1) Random Classifier: We use this as a baseline that is expected to make predictions based on the relative frequency of each class in the training data. We employ a stratified strategy, i.e. predicted class label is sampled from a multinomial distribution with empirical priors.\n\n2) Supervised Trained Models: To demonstrate an upper bound of performance, we also compare our framework to a fully-supervised pipeline. The architecture of this model is similar to that of the self-supervised trained model during evaluation i.e., it consists of a transformer-based feature encoder followed by a single dense layer for classification.\n\nIt is important to note that the supervised, trained model task is much easier to accomplish. However, self-supervised trained models can offer additional advantages of being suitable in scenarios where labeled data is limited and expensive. Furthermore, the self-supervised model learns task-agnostic embeddings, which allows the feature encoder to be reused for other tasks arbitrarily. We highlight these factors for consideration while interpreting our results.\n\n3) Self-Supervised Trained Models: We compare TS-MoCo against another self-supervised learning framework for physiological data, TS-TCC as introduced in Section II-B. In contrast to TS-MoCo, TS-TCC uses a conventional contrastive loss instead of momentum contrast.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Parameter Optimization",
      "text": "We pre-trained our models for 100 epochs and then performed 100 (UCIHAR) / 150 (SEED) epochs of linear evaluation with early stopping after 20 epochs based on the validation loss. Baseline models were similarly trained for 100 epochs. We used a 60-20-20 split for the train, validation, and test.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Comparison With Baseline Models",
      "text": "Table I presents a comparison of the self-supervised TS-MoCo framework with a supervised trained baseline model with same encoding architecture (Supervised), the selfsupervised TS-TCC framework, and a random classifier based on the strategy introduced in Section IV-B1. Note that the supervised model demonstrates an upper bound of performance to the problems. Our results clearly show that TS-MoCo achieves significantly above chance-level accuracies on both datasets. Classification accuracies of the self-supervised models are comparable on the SEED dataset, however, the simplified training pipeline of TS-MoCo appears to introduce a classification accuracy trade-off on the UCIHAR dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Ablation Studies 1) Model Components:",
      "text": "To evaluate the influence of the hyperparameters κ, λ, K, p M , and α, we report the results of an ablation study in Table  II . We observe different influences for different domains. For the SEED dataset, varying the hyperparameters barely results in different classification performance, as the best configuration achieves only +0.04 better accuracy than the worst configuration.\n\nFor UCIHAR, we observe results being highly dependent on the choice of the hyperparameters and can result in low accuracies e.g., if the value of λ is increased.\n\n2) Randomly Initialized Encoder: To evaluate the feasibility of the pre-training phase of TS-MoCo, we compare it against a model with same encoding architecture, but the encoder of this setup is randomly initialized and freezed i.e., only a classifier is fitted to the outputs of a randomly initialized encoder. By comparing the classification results on the UCI-HAR dataset, we observe that the classification accuracy of the random encoder falls short of that achieved through TS-MoCo by 0.06 (0.52 vs 0.46), i.e. pre-training of TS-MoCo does help to slightly improve classification accuracy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Discussion",
      "text": "We present TS-MoCo 1 , the first transformer encoder-based self-supervised learning framework with momentum contrast for physiological signal recording domain datasets. We performed an experimental pilot study using a linear evaluation scheme to demonstrate the representational capacity of our self-supervised encoding architecture.\n\nWe observed that the performance of TS-MoCo does not reach to supervised trained baseline models. However importantly, representations learned by TS-MoCo offer the additional benefit of being task agnostic and do not require labeled data to be trained. Further, we note that simplifying the training schema with momentum contrast can limit classification accuracies in certain domains compared to conventional contrastive learning. We also observed that a strong influence of used hyperparameters can occur depending on the data domain.\n\nAlthough the supervised baselines performed better than TS-MoCo, our model introduces a valuable encoding mechanism for physiological signal domains where no labeled data is available at all when supervised learning is not possible. Especially in the medical domain, labeling data is often cumbersome and dissemination of such information often leads to concerns regarding data privacy  [22] .\n\nIn future work, we aim to analyze the amount of labeled data necessary to train a classification layer based on the TS-MoCo encoded representations. We expect this amount to be significantly lower than needed for the supervised trained models. This would favor the use of our pipeline for domains where only very little labeled data is available.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed TS-MoCo framework enables optimizing a physiological",
      "page": 1
    },
    {
      "caption": "Figure 2: Architecture of the TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Emotion"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Recognition"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "EEG"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Human Activity"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "TS-MoCo"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Recognition"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Inertial"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Sensory"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "Fig. 1.\nProposed TS-MoCo framework enables optimizing a physiological"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "signal-agnostic encoder architecture for e.g., emotion recognition from EEG"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "or human activity recognition from inertial sensory."
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "i.e.,\nthe supervision signal necessary for\ntraining is generated"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "on-the-fly from the data itself. We investigate the architectural"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "design space\nin depth and develop a novel SSL framework"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "to\nlearn\ngeneralized\nfeatures\nacross multiple\nphysiological"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "signals. Our proposed deep SSL framework, annotated as TS-"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "MoCo, optimizes\na general\nfeature\nencoder\nagnostic\nto the"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "physiological\ndomain. We\nintroduce\na\ntwo-fold\ncontrastive"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "loss function for optimization based on time-series momentum"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "contrast [5]. The feature encoder of TS-MoCo predicts physi-"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "ological\nlabels using only one dense layer\nthat must be fitted"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "on the labeled data on top of the learned embedding space. In"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "contrast\nto prior\nresearch that\nfocuses on single,\nspecialized"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "domains, we\nevaluate\nour\nframework\nin\ntwo\nphysiological"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "data domains,\ni.e., emotion classification, and human activity"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "recognition. Overall, we address the following questions:"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "• How can we learn a generalizable feature encoder ar-"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "chitecture and learning algorithm for diverse domains of"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "physiological signal recordings?"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "• What\nis the technical design space for such a deep signal"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": ""
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "processing framework?"
        },
        {
          "5TU Graz - SAL Dependable Embedded Systems Lab, Silicon Austria Labs, Graz, Austria": "• How are the domains affecting the encoding architecture?"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "main agnostic feature encoder. We propose and explore self-",
          "Momentum contrast reduces computational demand in com-": "parison\nto\nconventional\ncontrastive methods\nby\nneglecting"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "supervised learning of general\nfeature\nencoders\nas\nan alter-",
          "Momentum contrast reduces computational demand in com-": "negative samples, and exhibits a promising potential. However,"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "native\napproach\nfor medical\ndeep\nlearning\nresearchers\nthat",
          "Momentum contrast reduces computational demand in com-": "to\ndate,\nthis SSL mechanism is\nnot\nexplored\nin\ndepth\nfor"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "enables use in downstream tasks with few labeled data.",
          "Momentum contrast reduces computational demand in com-": "analyzing biomedical data. In our work, we tackle this research"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "gap by proposing a framework based on momentum contrast"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "II. RELATED WORK",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "and evaluating it on different physiological domains."
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "A. Deep Learning for Physiological Signal Processing",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "III. METHODOLOGY"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "In recent years various works explored deep learning based",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "A. Training Schema"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "signal processing pipelines for physiological\ntime-series data",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "recordings.\nIn [6], different autoencoder architectures for en-",
          "Momentum contrast reduces computational demand in com-": "t\nWe\ndenote\nthe\nsignal with\ndiscrete\ntimesteps\nby x ="
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "coding physiological\nsignals\nrecorded through smartwatches",
          "Momentum contrast reduces computational demand in com-": "into C classes.\n[xt]T\nt=0, and signals can be given with labels"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "are compared.\n[7]\nimplements a transfer\nlearning approach to",
          "Momentum contrast reduces computational demand in com-": "The\ntraining procedure we used is\ntwofold. Firstly, we use"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "train a deep learning model\nfor human activity recognition",
          "Momentum contrast reduces computational demand in com-": "a self-supervised pre-training paradigm to learn a generaliz-"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "from radar databases. Another set of works on physiological",
          "Momentum contrast reduces computational demand in com-": "able\nfeature\nencoder. Secondly, we optimize\na\nsingle\nlinear"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "signal processing focuses on electroencephalography (EEG)",
          "Momentum contrast reduces computational demand in com-": "classifier using the\nrepresentations by this\nencoder\nto solve"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "signals. Cura\net\nal.\n[8]\nuse\na\ndeep\nconvolutional\nneural",
          "Momentum contrast reduces computational demand in com-": "the downstream classification problem. During self-supervised"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "network,\nto\ndetect\nepilepsy\nfrom patients EEG recordings,",
          "Momentum contrast reduces computational demand in com-": "pre-training,\nthe supervision signal\nis created directly from the"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "and Geoffroy\net\nal.\n[9]\ndetect\ndrowsiness\nfrom EEG and",
          "Momentum contrast reduces computational demand in com-": "signal\nitself, omitting the need for\nlaborious annotations. We"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "electrocardigrams (ECG) with a deep learning architecture.",
          "Momentum contrast reduces computational demand in com-": "use a combination of\ntwo supervision signals."
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "1) Reconstructing the Future: The first supervision signal"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "B. Self-Supervised Learning for Physiological Domains",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "is generated by splitting of\nthe final K time-steps\nfrom the"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "Self-supervised models have been previously explored for",
          "Momentum contrast reduces computational demand in com-": "signal\ni.e. by splitting x into two parts xpast = [xt]T −K−1"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "various physiological domains since physiological data mon-",
          "Momentum contrast reduces computational demand in com-": "former\nis\nencoded\ninto\na\nand\n= [xt]T\nxfuture"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "t=T −K. The"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "itoring is becoming ubiquitous,\nresulting in large amounts of",
          "Momentum contrast reduces computational demand in com-": "representation vector c by a feature encoder described through"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "unlabeled data. In [10],\nthe authors present an SSL framework",
          "Momentum contrast reduces computational demand in com-": "is used as\ntarget\nfor\nfENC with parameters θ, and the latter"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "trained with\na\ncontrastive\nloss\nthat\nlearns\nrepresentations",
          "Momentum contrast reduces computational demand in com-": "a reconstruction head,\nthat predicts\nthe subsequent\ntimesteps"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "for human activity recordings of\ninertial\nsensory data\nfrom",
          "Momentum contrast reduces computational demand in com-": "t ∈ [T − K, T ] of its input from a context vector. To compare"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "multiple devices. The\nauthors of\n[11] demonstrate\nthat one",
          "Momentum contrast reduces computational demand in com-": "the\npredicted\nsubsequent\ntime-steps\nthe\nground\nxfuture with"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "can\nsuccessfully\nimprove\nemotion\nclassification\nfrom elec-",
          "Momentum contrast reduces computational demand in com-": "loss:\ntruth xfuture we use a mean squared error"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "troencephalography\n(EEG)\nsignals\nby\ntraining\na\ngenerative",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "(1)\nLRec = ∥xfuture − ˆxfuture∥2\n2."
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "adversarial network to synthesize data using SSL. Recently",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "[1] proposed a vanilla SSL framework, TS-TCC,\ntrained with",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "2) Momentum Contrast:\nThe\nsecond\nsupervision\nsignal"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "a combination of temporal and contextual contrastive loss that",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "is generated through momentum contrast\n[5]. Two identical"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "achieved significant\nresults across several domains.",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "architectures,\nteacher\nand\nstudent models,\nare\nused\nduring"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "training. The\nteacher model\nas\ninput\nis provided with xpast"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "C. Self-Supervised Learning with Momentum Contrast",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "and predicts an output,\nin our case, a representation vector:"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "In SSL with momentum contrast,\ntwo models of\nthe same",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "architecture\nare\nused:\na\nstudent\nencoder\nand\na\nteacher\n(or",
          "Momentum contrast reduces computational demand in com-": ": θ<T >).\n(2)\nc<T > = fENC(xpast"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "momentum) encoder. The student encoder\nis\ntrained via loss",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "For\nthe student model,\nthe signal\nis first augmented with an"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "backpropagation, while the teacher model parameters are set",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "augmentation function before computing the output:"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "as\nan\nexponentially moving\naverage\nof\nthe\nstudent model",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "parameters. Seminal work in [5] proposes MoCo, a framework",
          "Momentum contrast reduces computational demand in com-": "(3)\nc<S> = fENC(A(xpast) : θ<S>)."
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "that\nis\ntrained with momentum contrast.\nThe momentum",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "Training aims to guide the student towards computing the same"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "encoder\nis hereby used to build a\nrepresentation dictionary",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "representation as the teacher from an augmented version of the"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "on-the-fly, which is then matched to the representation output",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "input. We\ncompare\nthese\ntwo output vectors using a\ncosine"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "by the student encoder network in a contrastive loss. MoCo",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "similarity loss metric:"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "achieves\nsuperior performance\nthan supervised baselines on",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "multiple vision tasks.\nIn the BYOL framework by [12], both",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "(4)\nLMC = 1 − cos(c<S>, c<T >)."
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "the momentum encoder and the student encoder predict\nrep-",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "resentations where different\ninput augmentations are applied.",
          "Momentum contrast reduces computational demand in com-": "3) Parameter Update: After each iteration, student model"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "BYOL achieves comparable results to state-of-the-art baselines",
          "Momentum contrast reduces computational demand in com-": "parameters are updated by loss backpropagation"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "in vision tasks while neglecting the need for negative samples.",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "",
          "Momentum contrast reduces computational demand in com-": "(5)\nLSS = LRec + λLMC,"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "Finally, [13] proposes DINO as an SSL framework trained with",
          "Momentum contrast reduces computational demand in com-": ""
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "momentum contrast without\nnegative\nsamples\nusing\nvision",
          "Momentum contrast reduces computational demand in com-": "where\nthe\nloss weight λ is\nset\nas\na\nhyperparameter. The"
        },
        {
          "Our\nframework thereby focuses on learning a\nsignal do-": "transformers [14] as a backbone and evaluated on vision tasks.",
          "Momentum contrast reduces computational demand in com-": "teacher model parameters θ<T > are updated in order\nto state"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "2) Feature Encoder:\nan exponentially moving average (EMA) of the student model\nThe\nused\nfeature\nencoder\nconsists"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "parameters θ<S>, as utilized in [5]:\nof a tokenizer,\nthe addition of positional embeddings, and a"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "transformer encoder. As a first step,\nthe signal used as input"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "θ<T > = τ θ<T > + (1 − τ )θ<S>,\n(6)"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "to the encoder is mapped into an embedding space by a single"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "linear\nlayer named tokenizer"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "with τ being the momentum weight hyperparameter."
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "4) Linear\nEvaluation:\nFollowing\nself-supervised\npre-\n[h(1)\n]T −K−1"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": ".\n(8)\n= θtokenizer ∗ [xi]T −K−1\nt=0\ni"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "training, we employ a linear evaluation scheme to evaluate the"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "Next,\nlike\nin\n[15],\na\nclassification\ntoken\n(< CLS >)\nis\nlearned feature\nencoder. We first use\nthe pre-trained frozen"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "prepended to the signal. The value of\nthis token is initialized\nencoder\nto compute representations of non-augmented input"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "randomly and learned during training.\nsignals x. Then a single linear\nlayer\nthat predicts class logits"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "from these representations is utilized according to"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "[h(2)\n] = [\n< CLS >,\n[h1\n]\n(9)\ni ]T −K−1"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "i"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": ": θ<C>),\n(7)\nyi = log qi = fCLA(c<S>"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "In\norder\nto\nallow the\nfeature\nencoder\nto\nexploit\npositional"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "information of the signal values, we add positional embeddings"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "where a conventional cross-entropy loss is used."
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "to\nthe\ntokenized\nsignals,\ni.e. we\nadd\na\nunique\nvector\nto"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "the signal values of each timestep. We create the positional"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "B. TS-MoCo Framework Architecture"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "embedding matrix P from sinusoidal\naccording to previous"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "1) Augmentation Function: Whether\nor\nnot\ntraining\nthe"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "work [16]. We also introduce a hyperparameter α ∈ {0, 1},"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "proposed\nframework\nis\nsuccessful\nheavily\ndepends\non\nthe"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "allowing us to enable/disable positional embeddings:"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "used\naugmentation\nfunction\nas\nit\ndefines\nthe\nencoding\nand"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "reconstruction task. If the augmentation is too weak,\nthe tasks\n[h(3)\n] = [h(2)"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "(10)\n] + αPi.\ni\ni"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "become\ntrivial\nand\ncan\nbe\nsolved\nby\nthe models without"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "Finally,\nthe latent signal\nis input\nto a transformer encoder [16]\nlearning the underlying concepts of\nthe data. On the other"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "of depth d, outputting a contextual embedding vector for each\nhand, if the augmentation is too strong, training the model may"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "input\ntoken [h(d+3)\n]. We use\nthe output\nembedding for\nthe\nfail due to limited capabilities of the architecture. Further, we\ni"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "classification token as context encoding c.\nexplicitly refrain from domain-specific augmentation functions"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "3) Reconstruction Head: We\nuse Gated Recurrent Units\nto allow a fast an easy adaption of our\nframework to various"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "(GRU)\nfor\nthe reconstruction head. The context vector output\ndomains. By employing a window-wise temporal masking, we"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "by the student feature encoder is thereby used as initial hidden\nfulfill\naforementioned constraints\nto the\naugmentation func-"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "state.\nFurther, we\napply\nteacher-forcing\nfor\npredicting\nthe\ntion. Hereby, a certain percentage p of the signal is overwritten"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "future\ntimesteps,\ni.e. we pass\nthe ground truth values\nfor\na\nwith a masking token i.e. 0. Especially for high-frequency"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "timestep besides a hidden state to the GRU cell for predicting\nsignals, masking singular\ntimesteps scattered along the time-"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "the values of\nthe next\ntimestep. Our overall\narchitecture\nis\naxis would state\nan easy task,\ntherefore we\nassure\nthat\nall"
        },
        {
          "Fig. 2. Architecture of\nthe TS-MoCo framework consisting of a student and teacher context encoder, and a GRU-based reconstruction head.": "illustrated in Figure 2.\nmasked timesteps span a continuous window."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "IV. EXPERIMENTAL STUDY"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "A. Datasets"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "1)\nSEED: The SJTU Emotion EEG Dataset\n(SEED)\n[17],"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "[18]\nis a dataset of EEG recordings from 15 subjects during"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "viewing\nof\nemotion-eliciting\nvideos. Signals were\nrecorded"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "1000 Hz\nfrom 62\nchannels\nat\nand\nlater\ndownsampled\nto"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "200 Hz. Following recent work [4],\n[19],\n[20], we applied a"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "4 −40 Hz bandpass filter and segmented the signals\ninto 2 s"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "non-overlapping windows. EEG recording signals are labeled"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "to be of either negative, neutral, or positive emotions."
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "2) UCIHAR: This dataset [21] for human activity recogni-"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "tion comprises recordings from 30 subjects during six activi-"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "ties of daily living: walking, walking upstairs, walking down-"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "stairs, sitting, standing,\nlaying, measured from waist-mounted"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "smartphone accelerometer and gyroscope. Class imbalance is"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "handled via stochastic undersampling [19]."
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "B. Baseline Comparison Models"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "1) Random Classifier: We use\nthis\nas\na baseline\nthat\nis"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "expected to make predictions based on the relative frequency"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": ""
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "of each class in the training data. We employ a stratified strat-"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "egy,\ni.e. predicted class label\nis sampled from a multinomial"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "distribution with empirical priors."
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "2)\nSupervised Trained Models:\nTo demonstrate\nan upper"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "bound of performance, we\nalso compare our\nframework to"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "a fully-supervised pipeline. The architecture of\nthis model\nis"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "similar to that of the self-supervised trained model during eval-"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "uation i.e.,\nit consists of a transformer-based feature encoder"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "followed by a single dense layer\nfor classification."
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "It\nis\nimportant\nto note\nthat\nthe\nsupervised,\ntrained model"
        },
        {
          "0.43\n0.38\n—\n—\n—\n—\n1": "task is much easier\nto accomplish. However,\nself-supervised"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SEED": "UCIHAR",
          "Emotion Classification": "Activity Recognition",
          "EEG": "",
          "3 / 62": "6 / 9",
          "0.79\n0.42\n0.43\n0.33": "0.89\n0.90\n0.52\n0.16"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "trained models can offer additional advantages of being suit-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "able in scenarios where labeled data is limited and expensive."
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "Furthermore,\nthe\nself-supervised model\nlearns\ntask-agnostic"
        },
        {
          "SEED": "",
          "Emotion Classification": "VALUE AS IN THE FIRST ROW.",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "embeddings, which allows\nthe\nfeature\nencoder\nto be\nreused"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "Classification Accuracies",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "for\nother\ntasks\narbitrarily. We\nhighlight\nthese\nfactors\nfor"
        },
        {
          "SEED": "κ",
          "Emotion Classification": "pM",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "consideration while interpreting our\nresults."
        },
        {
          "SEED": "0.9",
          "Emotion Classification": "0.5",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "3)\nSelf-Supervised Trained Models: We compare TS-MoCo"
        },
        {
          "SEED": "0.99",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "against another self-supervised learning framework for physio-"
        },
        {
          "SEED": "0.7",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "—",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "logical data, TS-TCC as introduced in Section II-B. In contrast"
        },
        {
          "SEED": "—",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "to TS-MoCo, TS-TCC uses\na\nconventional\ncontrastive\nloss"
        },
        {
          "SEED": "—",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "instead of momentum contrast."
        },
        {
          "SEED": "—",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "—",
          "Emotion Classification": "0.75",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "—",
          "Emotion Classification": "0.25",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "C. Parameter Optimization"
        },
        {
          "SEED": "—",
          "Emotion Classification": "—",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "We pre-trained our models\nfor 100 epochs\nand then per-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "formed 100 (UCIHAR) / 150 (SEED) epochs of linear evalua-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "tion with early stopping after 20 epochs based on the validation"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "loss. Baseline models were similarly trained for 100 epochs."
        },
        {
          "SEED": "A. Datasets",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "We used a 60-20-20 split\nfor\nthe train, validation, and test."
        },
        {
          "SEED": "1)",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "[17],",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "[18]",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "V. RESULTS"
        },
        {
          "SEED": "viewing\nof",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "from 62",
          "Emotion Classification": "",
          "EEG": "later",
          "3 / 62": "to",
          "0.79\n0.42\n0.43\n0.33": "A. Comparison with Baseline Models"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "[19],",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "Table\nI presents\na\ncomparison of\nthe\nself-supervised TS-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "MoCo framework with a\nsupervised trained baseline model"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "with\nsame\nencoding\narchitecture\n(Supervised),\nthe\nself-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "supervised TS-TCC framework, and a random classifier based"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "on the\nstrategy introduced in Section IV-B1. Note\nthat\nthe"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "supervised model\ndemonstrates\nan\nupper\nbound\nof\nperfor-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "mance\nto\nthe\nproblems. Our\nresults\nclearly\nshow that TS-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "MoCo achieves significantly above chance-level accuracies on"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "both datasets. Classification accuracies of\nthe self-supervised"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "models\nare\ncomparable on the SEED dataset, however,\nthe"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "simplified training pipeline of TS-MoCo appears to introduce"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "a classification accuracy trade-off on the UCIHAR dataset."
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "as",
          "3 / 62": "is",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "B. Ablation Studies"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": ""
        },
        {
          "SEED": "egy,",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "1) Model Components:\nTo\nevaluate\nthe\ninfluence\nof\nthe"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "the results\nhyperparameters κ, λ, K, pM , and α, we report"
        },
        {
          "SEED": "2)",
          "Emotion Classification": "",
          "EEG": "To demonstrate",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "of an ablation study in Table II. We observe different\ninflu-"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "ences\nfor different domains. For\nthe SEED dataset, varying"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "is",
          "0.79\n0.42\n0.43\n0.33": "the hyperparameters barely results\nin different\nclassification"
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "performance,\nas\nthe best\nconfiguration achieves only +0.04"
        },
        {
          "SEED": "uation i.e.,",
          "Emotion Classification": "",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "better accuracy than the worst configuration."
        },
        {
          "SEED": "",
          "Emotion Classification": "",
          "EEG": "for classification.",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "For UCIHAR, we observe\nresults being highly dependent"
        },
        {
          "SEED": "It\nis",
          "Emotion Classification": "",
          "EEG": "supervised,",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "on the choice of\nthe hyperparameters and can result\nin low"
        },
        {
          "SEED": "",
          "Emotion Classification": "to accomplish. However,",
          "EEG": "",
          "3 / 62": "",
          "0.79\n0.42\n0.43\n0.33": "accuracies e.g.,\nif\nthe value of λ is increased."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "¨": "[3] O.\nOzdenizci and D. Erdo˘gmus¸, “Stochastic mutual information gradient"
        },
        {
          "¨": "estimation for dimensionality reduction networks,” Information Sciences,"
        },
        {
          "¨": ""
        },
        {
          "¨": "vol. 570, pp. 298–305, 2021."
        },
        {
          "¨": ""
        },
        {
          "¨": "[4] D. Bethge, P. Hallgarten, T. Grosse-Puppendahl, M. Kari, L. L. Chuang,"
        },
        {
          "¨": "¨"
        },
        {
          "¨": "O.\nOzdenizci,\nand A. Schmidt,\n“EEG2Vec: Learning\naffective EEG"
        },
        {
          "¨": "IEEE International\nrepresentations\nvia\nvariational\nautoencoders,”\nin"
        },
        {
          "¨": ""
        },
        {
          "¨": "Conference on Systems, Man, and Cybernetics, 2022, pp. 3150–3157."
        },
        {
          "¨": ""
        },
        {
          "¨": "[5] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast"
        },
        {
          "¨": "the\nfor unsupervised visual\nrepresentation learning,” in Proceedings of"
        },
        {
          "¨": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "¨": ""
        },
        {
          "¨": "2020, pp. 9729–9738."
        },
        {
          "¨": ""
        },
        {
          "¨": "[6] M. Panagiotou, A. Zlatintsi, P. Filntisis, A. Roumeliotis, N. Efthymiou,"
        },
        {
          "¨": "and P. Maragos, “A comparative study of autoencoder architectures for"
        },
        {
          "¨": "mental health analysis using wearable sensors data,” in 30th European"
        },
        {
          "¨": "Signal Processing Conference, 2022, pp. 1258–1262."
        },
        {
          "¨": ""
        },
        {
          "¨": "[7]\nJ. Fix,\nI. Hinostroza, C. Ren, G. Manfredi, and T. Letertre, “Transfer"
        },
        {
          "¨": "learning for human activity classification in multiple radar\nsetups,” in"
        },
        {
          "¨": "30th European Signal Processing Conference, 2022, pp. 1576–1580."
        },
        {
          "¨": ""
        },
        {
          "¨": "[8] O. K. Cura, M. A. Ozdemir, and A. Akan, “Epileptic EEG classification"
        },
        {
          "¨": ""
        },
        {
          "¨": "using\nsynchrosqueezing\ntransform with machine\nand\ndeep\nlearning"
        },
        {
          "¨": "techniques,” in 28th European Signal Processing Conference, 2021, pp."
        },
        {
          "¨": "1210–1214."
        },
        {
          "¨": ""
        },
        {
          "¨": "[9] G. Geoffroy, L. Chaari,\nJ.-Y. Tourneret,\nand H. Wendt,\n“Drowsiness"
        },
        {
          "¨": ""
        },
        {
          "¨": "29th\ndetection\nusing\njoint EEG-ECG data with\ndeep\nlearning,”\nin"
        },
        {
          "¨": "European Signal Processing Conference, 2021, pp. 955–959."
        },
        {
          "¨": "[10] Y.\nJain, C.\nI. Tang, C. Min, F. Kawsar,\nand A. Mathur,\n“ColloSSL:"
        },
        {
          "¨": ""
        },
        {
          "¨": "Collaborative self-supervised learning for human activity recognition,”"
        },
        {
          "¨": ""
        },
        {
          "¨": "Proceedings of\nthe ACM on Interactive, Mobile, Wearable and Ubiqui-"
        },
        {
          "¨": "tous Technologies, vol. 6, no. 1, pp. 1–28, 2022."
        },
        {
          "¨": "[11]\nZ. Zhang, S.-h. Zhong, and Y. Liu, “GANSER: A self-supervised data"
        },
        {
          "¨": ""
        },
        {
          "¨": "IEEE\naugmentation\nframework\nfor EEG-based\nemotion\nrecognition,”"
        },
        {
          "¨": ""
        },
        {
          "¨": "Transactions on Affective Computing, 2022."
        },
        {
          "¨": "[12]\nJ.-B. Grill, F. Strub, F. Altch´e, C. Tallec, P. Richemond, E. Buchatskaya,"
        },
        {
          "¨": "et\nC. Doersch, B. Avila\nPires,\nZ. Guo, M. Gheshlaghi Azar\nal.,"
        },
        {
          "¨": ""
        },
        {
          "¨": "“Bootstrap your own latent-a new approach to self-supervised learn-"
        },
        {
          "¨": ""
        },
        {
          "¨": "ing,” Advances in Neural\nInformation Processing Systems, vol. 33, pp."
        },
        {
          "¨": "21 271–21 284, 2020."
        },
        {
          "¨": "[13] M. Caron, H. Touvron, I. Misra, H. J´egou, J. Mairal, P. Bojanowski, and"
        },
        {
          "¨": ""
        },
        {
          "¨": "A. Joulin, “Emerging properties in self-supervised vision transformers,”"
        },
        {
          "¨": ""
        },
        {
          "¨": "in Proceedings of"
        },
        {
          "¨": "Vision, 2021, pp. 9650–9660."
        },
        {
          "¨": "[14] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers"
        },
        {
          "¨": ""
        },
        {
          "¨": "for image recognition at scale,” in International Conference on Learning"
        },
        {
          "¨": ""
        },
        {
          "¨": "Representations, 2021."
        },
        {
          "¨": "[15]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training"
        },
        {
          "¨": "of deep bidirectional\ntransformers\nfor\nlanguage understanding,” arXiv"
        },
        {
          "¨": ""
        },
        {
          "¨": "preprint arXiv:1810.04805, 2018."
        },
        {
          "¨": ""
        },
        {
          "¨": "[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,"
        },
        {
          "¨": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances"
        },
        {
          "¨": "Neural\nInformation Processing Systems, vol. 30, 2017."
        },
        {
          "¨": ""
        },
        {
          "¨": "[17] W.-L. Zheng\nand B.-L. Lu,\n“Investigating\ncritical\nfrequency\nbands"
        },
        {
          "¨": ""
        },
        {
          "¨": "and\nchannels\nfor EEG-based\nemotion\nrecognition with\ndeep\nneural"
        },
        {
          "¨": "IEEE Transactions\nnetworks,”\non Autonomous Mental Development,"
        },
        {
          "¨": "2015."
        },
        {
          "¨": "[18] R.-N. Duan,\nJ.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for"
        },
        {
          "¨": ""
        },
        {
          "¨": "6th\nInternational\nIEEE/EMBS\nEEG-based\nemotion\nclassification,”\nin"
        },
        {
          "¨": "Conference on Neural Engineering.\nIEEE, 2013, pp. 81–84."
        },
        {
          "¨": "[19] D. Bethge, P. Hallgarten, T. Grosse-Puppendahl, M. Kari, R. Mikut,"
        },
        {
          "¨": ""
        },
        {
          "¨": "¨\nA. Schmidt, and O.\nOzdenizci, “Domain-invariant\nrepresentation learn-"
        },
        {
          "¨": "ing from EEG with private encoders,” in IEEE International Conference"
        },
        {
          "¨": "on Acoustics, Speech and Signal Processing, 2022, pp. 1236–1240."
        },
        {
          "¨": "¨\n[20] D. Bethge, P. Hallgarten, O.\nOzdenizci, R. Mikut, A. Schmidt,\nand"
        },
        {
          "¨": "T. Grosse-Puppendahl,\n“Exploiting multiple EEG data\ndomains with"
        },
        {
          "¨": ""
        },
        {
          "¨": "International Conference of\nthe\nadversarial\nlearning,”\nin 44th Annual"
        },
        {
          "¨": "IEEE Engineering in Medicine & Biology Society, 2022, pp. 3154–3158."
        },
        {
          "¨": "[21] D. Anguita, A. Ghio, L. Oneto, X. Parra Perez, and J. L. Reyes Ortiz,"
        },
        {
          "¨": "“A public domain dataset\nfor human activity recognition using smart-"
        },
        {
          "¨": "the 21th International European Symposium\nphones,” in Proceedings of"
        },
        {
          "¨": "on Artificial Neural Networks, Computational Intelligence and Machine"
        },
        {
          "¨": "Learning, 2013, pp. 437–442."
        },
        {
          "¨": "[22] M. Benˇcevi´c, M. Habijan,\nI. Gali´c, and A. Pizurica, “Self-supervised"
        },
        {
          "¨": "learning as\na means\nto reduce\nthe need for\nlabeled data\nin medical"
        },
        {
          "¨": "image analysis,” in 30th European Signal Processing Conference, 2022,"
        },
        {
          "¨": "pp. 1328–1332."
        },
        {
          "¨": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Time-series representation learning via temporal and contextual contrasting",
      "authors": [
        "E Eldele",
        "M Ragab",
        "Z Chen",
        "M Wu",
        "C Kwoh",
        "X Li",
        "C Guan"
      ],
      "year": "2021",
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Universal physiological representation learning with soft-disentangled rateless autoencoders",
      "authors": [
        "M Han",
        "O Özdenizci",
        "T Koike-Akino",
        "Y Wang",
        "D Erdogmus"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "3",
      "title": "Stochastic mutual information gradient estimation for dimensionality reduction networks",
      "authors": [
        "O Özdenizci",
        "D Erdogmus"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "4",
      "title": "EEG2Vec: Learning affective EEG representations via variational autoencoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "L Chuang",
        "O Özdenizci",
        "A Schmidt"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "5",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "A comparative study of autoencoder architectures for mental health analysis using wearable sensors data",
      "authors": [
        "M Panagiotou",
        "A Zlatintsi",
        "P Filntisis",
        "A Roumeliotis",
        "N Efthymiou",
        "P Maragos"
      ],
      "year": "2022",
      "venue": "30th European Signal Processing Conference"
    },
    {
      "citation_id": "7",
      "title": "Transfer learning for human activity classification in multiple radar setups",
      "authors": [
        "J Fix",
        "I Hinostroza",
        "C Ren",
        "G Manfredi",
        "T Letertre"
      ],
      "year": "2022",
      "venue": "30th European Signal Processing Conference"
    },
    {
      "citation_id": "8",
      "title": "Epileptic EEG classification using synchrosqueezing transform with machine and deep learning techniques",
      "authors": [
        "O Cura",
        "M Ozdemir",
        "A Akan"
      ],
      "year": "2021",
      "venue": "28th European Signal Processing Conference"
    },
    {
      "citation_id": "9",
      "title": "Drowsiness detection using joint EEG-ECG data with deep learning",
      "authors": [
        "G Geoffroy",
        "L Chaari",
        "J.-Y Tourneret",
        "H Wendt"
      ],
      "year": "2021",
      "venue": "29th European Signal Processing Conference"
    },
    {
      "citation_id": "10",
      "title": "ColloSSL: Collaborative self-supervised learning for human activity recognition",
      "authors": [
        "Y Jain",
        "C Tang",
        "C Min",
        "F Kawsar",
        "A Mathur"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "11",
      "title": "GANSER: A self-supervised data augmentation framework for EEG-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S.-H Zhong",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning",
      "authors": [
        "J.-B Grill",
        "F Strub",
        "F Altché",
        "C Tallec",
        "P Richemond",
        "E Buchatskaya",
        "C Doersch",
        "B Avila Pires",
        "Z Guo",
        "M Azar"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Emerging properties in self-supervised vision transformers",
      "authors": [
        "M Caron",
        "H Touvron",
        "I Misra",
        "H Jégou",
        "J Mairal",
        "P Bojanowski",
        "A Joulin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "15",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "16",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "18",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "Neural Engineering. IEEE"
    },
    {
      "citation_id": "19",
      "title": "Domain-invariant representation learning from EEG with private encoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "R Mikut",
        "A Schmidt",
        "O Özdenizci"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Exploiting multiple EEG data domains with adversarial learning",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "O Özdenizci",
        "R Mikut",
        "A Schmidt",
        "T Grosse-Puppendahl"
      ],
      "year": "2022",
      "venue": "44th Annual International Conference of the IEEE Engineering in Medicine"
    },
    {
      "citation_id": "21",
      "title": "A public domain dataset for human activity recognition using smartphones",
      "authors": [
        "D Anguita",
        "A Ghio",
        "L Oneto",
        "X Perez",
        "J Ortiz"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21th International European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning"
    },
    {
      "citation_id": "22",
      "title": "Self-supervised learning as a means to reduce the need for labeled data in medical image analysis",
      "authors": [
        "M Benčević",
        "M Habijan",
        "I Galić",
        "A Pizurica"
      ],
      "year": "2022",
      "venue": "30th European Signal Processing Conference"
    }
  ]
}