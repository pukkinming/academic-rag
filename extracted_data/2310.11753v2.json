{
  "paper_id": "2310.11753v2",
  "title": "Bias In Emotion Recognition With Chatgpt",
  "published": "2023-10-18T07:28:12Z",
  "authors": [
    "Naoki Wake",
    "Atsushi Kanehira",
    "Kazuhiro Sasabuchi",
    "Jun Takamatsu",
    "Katsushi Ikeuchi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This technical report explores the ability of Chat-GPT in recognizing emotions from text, which can be the basis of various applications like interactive chatbots, data annotation, and mental health analysis. While prior research has shown ChatGPT's basic ability in sentiment analysis, its performance in more nuanced emotion recognition is not yet explored. Here, we conducted experiments to evaluate its performance of emotion recognition across different datasets and emotion labels. Our findings indicate a reasonable level of reproducibility in its performance, with noticeable improvement through fine-tuning. However, the performance varies with different emotion labels and datasets, highlighting an inherent instability and possible bias. The choice of dataset and emotion labels significantly impacts ChatGPT's emotion recognition performance. This paper sheds light on the importance of dataset and label selection, and the potential of fine-tuning in enhancing ChatGPT's emotion recognition capabilities, providing a groundwork for better integration of emotion analysis in applications using ChatGPT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The ability to recognize emotions from text is a fundamental aspect of various applications ranging from interactive chatbots  [1] ,  [2] , annotated data generation  [3] , text analysis  [4] , and mental health analysis  [5] -  [7] . As chatbots become increasingly sophisticated, understanding the emotional nuance within user inputs can significantly enhance the conversational experience by enabling more empathetic and contextually appropriate responses. Similarly, in the realm of mental health, analyzing textual data for emotional content can provide crucial insights into an individual's mental health conditions.\n\nIn recent years, the advancements in Large Language Model (LLM), notably models like ChatGPT, have shown promise in various Natural Language Processing (NLP) tasks, including emotion recognition. Early studies have demonstrated the capability of ChatGPT in basic sentiment analysis tasks where the primary goal is to distinguish between positive and negative sentiments  [7] -  [10] . Beyond this binary classification a few reports delved into a nuanced understanding of emotions like joy, sadness, anger, and surprise and showed that ChatGPT shows reasonable performance in such detailed emotion analysis  [5] ,  [11]  in a zero-shot and few-shot prompting conditions.\n\nHowever, one of the caveats associated with LLMs is their tendency to exhibit biases in the generated outputs. These biases often have roots in the non-uniformity of their training data and the labeling methodologies employed  [12] . Consequently, it is likely that potential biases manifest in the nuances of emotional recognition. While understanding these specific tendencies in emotion recognition could offer insights for the responsible deployment of ChatGPT in realworld applications, detailed studies on this particular aspect remain limited.\n\n1 Applied Robotics Research, Microsoft, Redmond, WA 98052, USA naoki.wake@microsoft.com Using ChatGPT as a representative LLM, this study delves into its emotion recognition capabilities across various datasets and emotion labels. In experiments that solely relied on prompting (i.e., a zero-shot condition), ChatGPT exhibited commendable emotion recognition performance, consistent with previous research findings. However, we observed disparities in performance across different emotion labels, and intriguingly, the same label sometimes yielded varied results across datasets. While fine-tuning generally enhanced the model's recognition abilities, inconsistencies among labels persisted. A notable observation was the sensitivity of the model to slight label name changes (e.g., from \"happiness\" to \"happy\"). Despite these fluctuations, ChatGPT's responses remained largely consistent, and the labeling context had minimal influence on its overall performance. Our findings not only elucidate ChatGPT's strengths and limitations in emotion recognition but also set the stage for future investigations focused on optimizing performance and minimizing biases in ChatGPT-driven emotion recognition tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Chatgpt And Prompts",
      "text": "ChatGPT is an LLM developed by OpenAI  [13] . One of its distinct features is its capability to generate text in a conversational manner. This interactive nature allows users to leverage the model for a wide range of NLP tasks using conversational texts, which is called a prompt. The methodology of crafting prompts to obtain desired outputs from the off-the-shelf model without additional dataset (i.e, zero-shot setting) is referred to as prompt engineering  [10] . Additionally, there is a methodology that employs a dataset of expected input-output pairs to fine-tune the model's behavior. In this research, we employed the gpt-3.5-turbo, a fixed version of the GPT model that is publicly available for fine-tuning. This version represents the latest iteration of ChatGPT that publicly supports fine-tuning to date. We assessed its efficacy in both prompt engineering and fine-tuning conditions.\n\nWe evaluate ChatGPT's ability to discern emotions in complex scenarios based on the emotion recognition in conversation (ERC) task  [14] . In ERC, the goal is to recognize the emotion of each utterance within a conversation from a predefined set of emotion categories. In this paper, we assume an ongoing conversation scenario where future conversation information is unobserved. We estimate the emotion label for the current utterance based solely on the history of past conversations. That is, given a conversation history C = {u 1 , u 2 , . . . , u t } consisting of a sequence of t utterances, the task is to recognize the emotion label to u t , referred to as y t . Here, y t ∈ E, where E is the set of predefined emotion labels.\n\nFor the zero-shot setting, we designed a prompt based on a previous study  [5] : , where [Dialogue History] denotes the history of dialogue containing a list of speaker names and utterances, [Query] denotes a target utterance with the speaker name, and [Label List] denotes emotion labels for a dataset. When using a dataset without speaker IDs, the name was denoted as 'unknown.'",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Dataset And Evaluation Metrics",
      "text": "We evaluates the performance of emotion recognition by ChatGPT across five datasets: IEMOCAP  [15] , MELD  [16] , EmoryNLP  [17] , DailyDialog  [18] . For the IEMOCAP multimodal dataset, we focus solely on the text modality. The detailed information of those dataset is summarized in Table  I .\n\nIn our experiments, we adhered to the established metrics in line with the prevailing approach in the literature. Specifically, for the IEMOCAP, MELD, and EmoryNLP datasets, we employed the Weighted-F1 metric. For the DailyDialog dataset, considering the disproportionately high occurrence of neutral utterances, we utilized the micro-F1 metric, excluding the neutral category. It is noteworthy that our analysis was not limited to overall performance but also we also examined the recognition performance for each individual emotion label, to explore the specific tendencies in emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Fine-Tuning",
      "text": "When fine-tuning ChatGPT, we used the prompts from the zero-shot experiments along with their corresponding correct labels. To test with a more generalized model, we combined four distinct datasets previously mentioned for fine-tuning, and tested the model across the four datasets. We utilized data from the validation sets of each dataset: 1365 samples from IEMOCAP, 1109 from MELD, 1344 from EmoryNLP, and 961 from DailyDialog. The distribution of the labels are shown in Fig.  1  III. RESULTS",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Main Results",
      "text": "The overall performance in emotion recognition significantly exceeded chance levels, aligning with the findings from previous studies (TableII,III,IV,V). However, there was noticeable variance when examining performance based on individual emotion labels. For example, in the zero-shot experiments on the EmoryNLP dataset, the recognition performance (F1 score) for 'joyful' was 51%, while it was below 5% for 'peaceful' and 'powerful.' Fine-tuning generally observed to enhance performance (Fig.  2 ). Interestingly, despite the EmoryNLP dataset using text based on the same TV series as MELD, the impact of finetuning wasn't as pronounced.\n\nDespite the variability in the labels of the datasets used for fine-tuning, a strong correlation was observed between the performance in zero-shot and fine-tuning (correlation coefficient of 0.81). This result suggests that while fine-tuning improves performance, it still retains inherent tendencies.\n\nTo explore the potential semantic relationship between recognition performance and label tendencies, we classified the labels into three overarching categories: positive, neutral, and negative (Fig.  3 ). However, our analysis did not reveal any discernible correlation between recognition performance and the semantic nature of the labels.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Sensitivity To Label Text",
      "text": "From these results, we observed that recognition performance varied significantly across datasets, even for similar labels. For instance, in the DailyDialog dataset, the recognition performance for 'happiness' was around 94%, while in the IEMOCAP dataset, the recognition performance for 'happy' was only about 22% (see the fine-tuning columns in TableII and V). Notably, this tendency is observed in the zero-shot condition, as well. While these differences may be influenced by the unique characteristics of the sentences contained within each dataset, we hypothesized that there might be sensitivity to the label text. To test this, we changed the label name in the IEMOCAP dataset from 'happy' to 'happiness' and applied the same model to evaluate its performance.\n\nAs a result, the recognition performance for 'happiness' improved to 43% (TableVI). Additionally, the recognition performance for other labels did not suffer significantly, indicating that changing the label improved the recognition performance for the 'happy' label without compromising the performance of other labels. This result suggests a sensitivity of the language model to the specific text of the labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Reproducibility Of The Performance",
      "text": "As an additional experiment, we investigated the reproducibility of ChatGPT's results using Cohen's Kappa score  [19] . Under the zero-shot condition, when applying the Chat-GPT model twice to the MELD test data, the Kappa score was 0.65. This suggests that we can expect reasonable reproducibility when applying the model multiple times.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Dataset Selection",
      "text": "In the experiment, the model was fine-tuned on a blend of all datasets. This dataset mixture can prevent over-fitting to a specific dataset, but it also has the potential to significantly affect the model performance. To gain insights on the impact of dataset mixture on the main result, we compared the results of fine-tuning on a single dataset against those trained on the mixed dataset, using the MELD dataset as an example. The model trained on a single dataset performed better for most labels, which is reasonable. However, the difference was at most about 7 percent, which is marginal compared to the difference across labels (49 percent at most), and there was no significant difference in the trend of performance differences between labels. These results suggest that the model's recognition bias is less likely to be affected by the scope of the dataset.  IV. DISCUSSION In this study, we evaluated ChatGPT's ability to recognize emotions associated with text. Within the context of the ERC task, ChatGPT demonstrated a commendable performance, surpassing chance levels and aligning with prior research findings. While fine-tuning generally improved recognition, we observed remaining variability in performance across different emotion labels and datasets.\n\nThe variability in recognition performance can be attributed to biases in the training data and the labeling methodologies used  [12] . In prior research that attempted to generate datasets with emotion labels using ChatGPT, Koptyra et al.  [3]  reported variability in the number of generated labels. This observation aligns with our result and both suggest that the inherent bias in ChatGPT may influence emotion analysis. In the context of emotion recognition, biases towards certain label classes have been reported in pioneering language models  [20] . Consequently, it is of utmost importance for researchers to understand and account for the intrinsic biases of language models when applying them to emotion analysis.\n\nLLMs are praised for their advanced linguistic skills, making them an appealing solution for various applications, including mental health ( [5]-  [7] ,  [21] ,  [22] ; see  [23]  for the survey). Given the potential biases reflected in emotion recognition, it would be prudent for researchers and practitioners to exercise caution when applying applying these models in sensitive domains like mental health. Over-reliance on these models without thorough evaluation might inadvertently propagate biases or yield misleading results.\n\nEven within datasets such as MELD and EmoryNLP, which are based on the same television series, there was a notable variance in performance, with particularly low performances for \"powerful\" and \"peaceful.\" We hypothesize that one reason these emotion labels are not adequately associated with the text might be due to biases within the knowledge base of ChatGPT. Figure  4  presents examples of ChatGPT's responses when inquired about commonly used emotion labels. The responses from ChatGPT encompass widely accepted emotional definitions, such as Ekman's basic emotions  [24]  and Plutchik's wheel of emotions  [25] . While the input-output mechanisms of LLM are black-box in nature, the fact that the model produces such knowledge suggests that for ChatGPT, associating these emotions with the input text might be relatively easier than associating less commonly used labels. These experimental results, alongside the sensitivity of the labels to the text, highlight the importance of label selection.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Limitation Of This Study",
      "text": "While our study sheds light on several aspects of emotion recognition using ChatGPT, there are certain limitations that should be acknowledged: 1) Inconsistencies in labeling criteria: Different datasets might have similar labels but with varied labeling criteria. This could account for the observed discrepancies in our results, a sentiment discussed in prior research  [11] . 2) Potential for Further Optimization: It is noteworthy that our experimental results might not fully reflect the optimal performance of ChatGPT for the corresponding tasks. As our results underscore the model's sensitivity to specific label texts, an adjustment of prompt could enhance its performance. We believe that through more refined prompt engineering and careful selection of context examples, the performance of ChatGPT can be further enhanced. 3) Model Specificity: Our study focused exclusively on the evaluation of ChatGPT, a representative large language model. Whether similar results would be observed when using other models remains undetermined.\n\nIn future research, a broader study that encompasses various linguistic and cultural contexts, as well as a wider range of emotion labels and models, could offer a more holistic perspective on LLMs' strengths and shortcomings in emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "Our exploration of ChatGPT's capabilities in emotion recognition has illuminated both its potential and pitfalls. While the model demonstrates impressive proficiency, especially when fine-tuned, it is susceptible to inconsistencies across various emotion labels and datasets. These variations might be influenced by underlying biases, highlighting the importance of meticulous dataset and label selection. As the allure of deploying models like ChatGPT in critical domains grows, it is paramount to approach their integration with caution and diligence, ensuring their application remains reliable and unbiased in real-world contexts.\n\n(User) In the context of emotion recognition research, what are the commonly used emotion labels? (ChatGPT) In emotion recognition research, there are several commonly used emotion labels that researchers use to categorize and study human emotional states. These labels are typically based on various emotion theories and models. Some of the most common emotion labels include:\n\n1) Basic Emotions: These are often considered the core, universally recognized emotions proposed by Paul Ekman. They include:\n\n• Surprise 2) Emotion Intensity: Researchers may also measure the intensity or strength of emotions, ranging from very weak to very strong. 3) Valence-Arousal Model: This model categorizes emotions based on their valence (positive, negative, or neutral) and arousal (intensity or activation). Common labels include:\n\n• Positive emotions (e.g., joy, love)\n\n• Negative emotions (e.g., anger, sadness)\n\n• Neutral emotions (e.g., boredom, indifference) 4) Plutchik's Wheel of Emotions: Robert Plutchik's model suggests that there are eight primary emotions, which can be combined to form more complex emotional states. These primary emotions include:\n\n• Surprise • Anticipation ... It's important to note that the choice of emotion labels can vary depending on the specific goals of the research and the theoretical framework being used. Additionally, advances in emotion recognition technology and machine learning have led to more fine-grained and automated approaches for detecting and categorizing emotions, which may use a combination of these labels or even entirely different methods.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The distribution of labels used for fine-tuning ChatGPT. For the",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison of emotion recognition performance between zero-shot",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison of emotion recognition performance between zero-shot",
      "page": 3
    },
    {
      "caption": "Figure 4: presents examples of ChatGPT’s responses when",
      "page": 5
    },
    {
      "caption": "Figure 4: An example of the output of ChatGPT regarding commonly used",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Artificial emotion expression for a robot by dynamic color change",
      "authors": [
        "K Terada",
        "A Yamauchi",
        "A Ito"
      ],
      "year": "2012",
      "venue": "2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "2",
      "title": "Multimodal expression of artificial emotion in social robots using color, motion and sound",
      "authors": [
        "D Löffler",
        "N Schmidt",
        "R Tscharn"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "3",
      "title": "Clarin-emo: Training emotion recognition models using human annotation and chatgpt",
      "authors": [
        "B Koptyra",
        "A Ngo",
        "Ł Radliński",
        "J Kocoń"
      ],
      "year": "2023",
      "venue": "International Conference on Computational Science"
    },
    {
      "citation_id": "4",
      "title": "Clusterllm: Large language models as a guide for text clustering",
      "authors": [
        "Y Zhang",
        "Z Wang",
        "J Shang"
      ],
      "year": "2023",
      "venue": "Clusterllm: Large language models as a guide for text clustering",
      "arxiv": "arXiv:2305.14871"
    },
    {
      "citation_id": "5",
      "title": "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis",
      "authors": [
        "K Yang",
        "S Ji",
        "T Zhang",
        "Q Xie",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "On the evaluations of chatgpt and emotion-enhanced prompting for mental health analysis",
      "arxiv": "arXiv:2304.03347"
    },
    {
      "citation_id": "6",
      "title": "Evaluation of chatgpt for nlp-based mental health applications",
      "authors": [
        "B Lamichhane"
      ],
      "year": "2023",
      "venue": "Evaluation of chatgpt for nlp-based mental health applications",
      "arxiv": "arXiv:2303.15727"
    },
    {
      "citation_id": "7",
      "title": "Will affective computing emerge from foundation models and general ai? a first evaluation on chatgpt",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Will affective computing emerge from foundation models and general ai? a first evaluation on chatgpt",
      "arxiv": "arXiv:2303.03186"
    },
    {
      "citation_id": "8",
      "title": "Is chatgpt a general-purpose natural language processing task solver?",
      "authors": [
        "C Qin",
        "A Zhang",
        "Z Zhang",
        "J Chen",
        "M Yasunaga",
        "D Yang"
      ],
      "year": "2023",
      "venue": "Is chatgpt a general-purpose natural language processing task solver?",
      "arxiv": "arXiv:2302.06476"
    },
    {
      "citation_id": "9",
      "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
      "authors": [
        "Q Zhong",
        "L Ding",
        "J Liu",
        "B Du",
        "D Tao"
      ],
      "year": "2023",
      "venue": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
      "arxiv": "arXiv:2302.10198"
    },
    {
      "citation_id": "10",
      "title": "Chatgpt: Jack of all trades, master of none",
      "authors": [
        "J Kocoń",
        "I Cichecki",
        "O Kaszyca",
        "M Kochanek",
        "D Szydło",
        "J Baran",
        "J Bielaniewicz",
        "M Gruza",
        "A Janz",
        "K Kanclerz"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "Is chatgpt equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is chatgpt equipped with emotional dialogue capabilities?",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "12",
      "title": "Decoding chatgpt: a taxonomy of existing research, current challenges, and possible future directions",
      "authors": [
        "S Sohail",
        "F Farhat",
        "Y Himeur",
        "M Nadeem",
        "D Madsen",
        "Y Singh",
        "S Atalla",
        "W Mansoor"
      ],
      "year": "2023",
      "venue": "Decoding chatgpt: a taxonomy of existing research, current challenges, and possible future directions"
    },
    {
      "citation_id": "13",
      "title": "Chatgpt",
      "authors": [
        "Openai"
      ],
      "venue": "Chatgpt"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "16",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "17",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "18",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "19",
      "title": "A coefficient of agreement for nominal scales",
      "authors": [
        "J Cohen"
      ],
      "year": "1960",
      "venue": "Educational and psychological measurement"
    },
    {
      "citation_id": "20",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Chatgpt outperforms humans in emotional awareness evaluations",
      "authors": [
        "Z Elyoseph",
        "D Hadar-Shoval",
        "K Asraf",
        "M Lvovsky"
      ],
      "year": "2023",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "22",
      "title": "Testing creativity of chatgpt in psychology: Interview with chatgpt",
      "authors": [
        "K Uludag"
      ],
      "year": "2023",
      "venue": "Available at SSRN"
    },
    {
      "citation_id": "23",
      "title": "A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics",
      "authors": [
        "K He",
        "R Mao",
        "Q Lin",
        "Y Ruan",
        "X Lan",
        "M Feng",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics",
      "arxiv": "arXiv:2310.05694"
    },
    {
      "citation_id": "24",
      "title": "Are there basic emotions?",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions?"
    },
    {
      "citation_id": "25",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    }
  ]
}