{
  "paper_id": "2201.02710v1",
  "title": "A New Amharic Speech Emotion Dataset And Classification Benchmark",
  "published": "2022-01-07T23:50:34Z",
  "authors": [
    "Ephrem A. Retta",
    "Eiad Almekhlafi",
    "Richard Sutcliffe",
    "Mustafa Mhamed",
    "Haider Ali",
    "Jun Feng"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Amharic dataset",
    "Classifiers",
    "Feature extraction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper we present the Amharic Speech Emotion Dataset (ASED), which covers four dialects (Gojjam, Wollo, Shewa and Gonder) and five different emotions (neutral, fearful, happy, sad and  angry). We believe it is the first Speech Emotion Recognition (SER) dataset for the Amharic language. 65 volunteer participants, all native speakers of Amharic, recorded 2,474 sound samples, two to four seconds in length. Eight judges (two for each dialect) assigned emotions to the samples with high agreement level (Fleiss kappa = 0.8). The resulting dataset is freely available for download. Next, we developed a four-layer variant of the well-known VGG model which we call VGGb. Three experiments were then carried out using VGGb for SER, using ASED. First, we investigated whether Mel-spectrogram features or Mel-frequency Cepstral coefficient (MFCC) features work best for Amharic. This was done by training two VGGb SER models on ASED, one using Mel-spectrograms and the other using MFCC. Four forms of training were tried, standard cross-validation, and three variants based on sentences, dialects and speaker groups. Thus, a sentence used for training would not be used for testing, and the same for a dialect and speaker group. The conclusion was that MFCC features are superior under all four training schemes. MFCC was therefore adopted for Experiment 2, where VGGb and three other existing models were compared on ASED: RESNet50, Alex-Net and LSTM. VGGb was found to have very good accuracy (90.73%) as well as the fastest training time. In Experiment 3, the performance of VGGb was compared when trained on two existing SER datasets, RAVDESS (English) and EMO-DB (German) as well as on ASED (Amharic). Results are comparable across these languages, with ASED being the highest. This suggests that VGGb can be",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays a significant role in everyday human interactions  [12] . It is essential to rational decision making and helps us match and understand others' feelings by conveying our own feelings and giving feedback to others. Emotion conveys considerable information about the mental state of an individual. This has opened up a new research field called automatic emotion recognition. Various modalities have been explored in prior studies to recognize emotional states such as facial expressions, speech, physiological signals, etc.  [12] . Several inherent advantages make speech signals a good source for affective computing. For example, compared to many other biological signals (e.g., electrocardiograms), speech signals can usually be acquired more readily and economically. This is why many researchers are interested in Speech Emotion Recognition (SER).\n\nSER is an important research area that has been active for more than two decades  [28] . The results of SER can already be seen in many application fields, including entertainment, computer games, audio monitoring, online learning, clinical research, polygraph tests, and call centers  [18, 12, 3] . Even though SER has many benefits, it is still a difficult task to perform with high accuracy  [17] . One key problem is choosing the right features; an incorrect choice can lead to moderate performance  [10] . Audio features are usually divided into two domains; time-domain features and frequency-domain features. The time-domain functions are elementary to extract and allow easy analysis of audio signals. In the case of small audio datasets, the frequency domain features will show deeper patterns, which may help distinguish the signal's basic emotion. Frequency-domain features include spectrograms, Mel Frequency Cepstral Coefficients (MFCCs), spectral centroid, spectral roll-off, spectral entropy, and Chroma coefficients  [35] . In this paper, a comprehensive analysis of each feature was performed during the exploratory data analysis. However, for the purpose of this work, we limited ourselves to two principal features, Mel-spectrograms and MFCC.\n\nIn recent years, numerous speech datasets have been created to support deep learning for SER. The languages of these datasets include English, Chinese, Spanish, French, German and many others  [13] . However, no SER dataset has been created for Amharic yet. Amharic is the second-largest Semitic language in the world after Arabic and the national language of Ethiopia  [23] . In terms of the number of speakers and the significance of its politics, history, and culture, it is one of the 55 most important languages in the world  [21] . However, despite this, Amharic and its dialects have very few language resources compared to other languages such as English. It is for this reason that we have created a new SER dataset for Amharic.\n\nUnlike English, Amharic is a syllabic language; each character represents one syllable  [4] . The language uses a script derived from the Ge'ez alphabet  [4] . It has 33 main characters, and each consonant-vowel combination has seven forms. Compared with English, Amharic has some unique phonemes. Gemination in Amharic is one of the most distinctive features of the speech's cadence, and it has great semantic and syntactic functional weight. In contrast to English, where the rhythm is mainly characterized by stress (loudness), the rhythm of Amharic is mainly characterized by longer and shorter syllables depending on the germination of consonants and certain characteristics of the phrase  [1] . Amharic gemination is either lexical or morphological. In the speech synthesis field, it is vital to introduce emotional features to give greater expression to a machine, so that it speaks more like a human. Additionally, expressions and intonation for emotions and mental states differ markedly in their nature and significance from language to language.\n\nIn summary, therefore, the two main challenges for Amharic are the limited availability of datasets and the language's complex morphological characteristics  [24] . This paper addresses both points by introducing a new Amharic dataset suitable for training and testing SER systems, and by presenting experiments which show how frequency-domain features can be effectively used for Amharic SER.\n\nIn addition, we have developed an SER model called VGGb, based on the well-known VGG SER architecture, and used it to carry out three experiments. The first experiment was to choose an appropriate method from the Mel-spectrogram and Mel-frequency Cepstral Coefficient (MFCC) technologies for extracting features from recordings in our ASED dataset, using VGGb. Four train-test schemes were used. The first was standard cross-validation, withholding 10% of training utterances for testing. The second trained on just five of the seven sentences in ASED, testing on the remaining two. The third trained only with utterances spoken in three of the four dialects in ASED, testing on utterances in the remaining dialect. The fourth trained using utterances by participants in two of the three speaker groups in ASED, testing on utterances in the remaining speaker group. Under all four schemes, MFCC was found to be the most effective in terms of accuracy and training time.\n\nThe second experiment was to compare the classification performance of VGGb and three other popular models using MFCC features. VGGb achieved a high accuracy (90.73%) as well as having the shortest training time. In the third experiment, we evaluated VGGb on the English RAVDESS and German EMO-DB datasets. The results for English and German were comparable to those achieved for Amharic on the ASED dataset.\n\nThe contributions of this paper are as follows:\n\n• We create for the very first time an SER dataset for Amharic, called ASED. There are 65 speakers and 2,474 recordings, 522 neutral, 510 fearful, 486 happy, 470 sad, and 486 angry.\n\n• All four Amharic dialects (Gojjam, Wollo, Shewa and Gonder) are included in the dataset.\n\n• Eight judges evaluate the recordings, and agreement between them is high (Fleiss kappa = 0.8). So the data is of high quality.\n\n• We analyse ASED with respect to nine other well-known SER datasets and show that it compares very well in terms of the number of participants, the amount of data produced, the method of quality control, the number of judges, and their agreement level.\n\n• We develop a high-performing variant of the VGG SER model which has just four CNN layers. We call this VGGb.\n\n• Using VGGb and ASED data, we compare Mel-spectrogram and Mel-frequency Cepstral Coefficient (MFCC) features and show experimentally in a SER task that MFCC leads to higher accuracy. We show that the superiority of MFCC is independent of training sentence, Amharic dialect and speaker group.\n\n• We apply VGGb and three other architectural models to the SER task, working with ASED data, and show that VGGb is very effective, and by far the fastest.\n\n• Using VGGb and working with MFCC, we train models to recognise five emotions in Amharic, English and German, using the ASED, RAVDESS, and EMO-DB datasets respectively. VGGb shows excellent performance and proves very effective for SER on our Amharic data.\n\nThe rest of this paper is organised as follows: Section 2 presents the ASED dataset, describing the rationale behind its design and the method by which it was created. A detailed comparison with nine other datasets is also included (four for English, one each for Chinese, German, Greek, Gujarati, Hindi). Sim Lab A",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Design Of Ased",
      "text": "Dialects: Amharic is considered one of the most challenging languages to be utilized in speech emotion recognition systems because of its huge lexical variety and complicated morphology  [1] . There are four main types of Amharic dialect, namely Gojjam (Gojjamegna), Wollo (Wollogna), Shewa (Shewagna), and Gonder (Gonderegna)  [21] . We wished the proposed dataset to contain examples of all four dialects.\n\nEmotions: RAVDESS contains eight emotions (see above) while Emo-DB has seven. It was decided to adopt five emotions which are common to the other datasets. So, relative to RAVDESS, calm, surprise and disgust are omitted, while relative to Emo-DB, disgust and boredom are omitted. The use of a common subset allows direct SER comparisons to be made across languages.\n\nTest sentences: For each of the five emotions in ASED, five sentences expressing that emotion were composed in Amharic. For example, for Happy we have 'Abebe's wedding is very nice' (Table  3 ), a sentence which expresses a happy concept. Similarly, 'My cousin died in a car accident' is a Sad sentence. Furthermore the dataset contains two common sentences which express no strong emotion, e.g. 'My sister is coming by plane'. The total number of sentences in the dataset is thus 27, 5 × 5 emotion-specific sentences plus two common sentences.\n\nTable  3 : Examples of sentences expressing each of the five emotions.\n\nApproach to generating emotion: Three methods can be used to collect the recordings in emotion recognition datasets: Simulated, Induced, and Natural  [13] . For simulated emotions, participants are asked to read a sentence while expressing a stated emotion. So if the sentence was 'My sister is coming by plane', one participant could be asked to read it in an angry way, while another read it in a sad way. In the Induced approach, the participant is made to feel the required emotion before reading the sentence. In the Natural approach, a recording must be made when a speaker happens to be feeling a particular emotion. The Simulated approach is the most practical to implement, and nearly 60% of speech datasets are collected using this method  [13] . For this reason, the Simulated approach was also adopted for ASED data collection.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Creation Of Ased",
      "text": "Judges:\n\nAll judges were Ethiopian postgraduate students of Computer Science or Management at universities in Xi'an, China. All were native speakers of one Amharic dialect. Two judges were from Xidian University (西安电子科技大学) and spoke Shewa and Gonder dialects respectively; two were from Chang'an University (长安大学) (Wollo, Gojjam); one was from Xi'an Shiyou University (西安石油大学), (Shewa); two were from Xi'an Jiaotong University (西安交通 大学), (Gonder, Gojjam); finally, one was from Northwest University (西北大学), (Wollo). Judges were responsible for the quality control of the dataset (see below).\n\nParticipants: There were three classes of participant, undergraduate students, postgraduate students and business people. The undergraduate and postgraduate students came from Ethiopia to China in order to study. The business people came to China for professional reasons concerning their work. In order to take part, participants had to be native speakers of one of the four Amharic dialects, and they had to be capable of speaking in different emotions, according to the opinions of the judges, following some initial tests. During the selection process, the judges assigned each participant to one of three groups, depending on their expertise in the task: Professional, Semi-professional and Amateur (Table  5 ). Participants who were clearly experienced at acting and were excellent at expressing the different emotions, in the opinion of the judges, were assigned to the Professional group. Those who were judged very good at expressing the emotions were assigned to the Semi-professional group. Finally, participants who were not experienced at acting but were nevertheless judged good enough to participate in the task were assigned to the Amateur group. In total there were 65 participants (25 female, 40 male), aged from 20 to 40 years. Professional  (20) , Semi-professional  (26) , Amateur  (19)  Recording: In order to record the speech, we used six Huawei nova4 mobile phones, on which an Android-based speech recording software app  [1]  had been installed. Mobile phones were used because professional audio equipment was not available to us. The software was set up to capture the speech utterances utilizing a 16 kHz sampling rate at 16 bits, resulting in a mono .Wav file. The recording software displayed the text for them one sentence at a time and indicated the required emotion. They then recorded the sentence onto the phone. When they finished speaking, the recorded audio file was saved. They then recorded the same sentence with the same emotion a second time. The recording was done at the School of Information Science and Technology, Northwest University, in a quiet room in order to obtain speech signals with minimum noise. The distance between the speaker's mouth and the microphone of the mobile phone was 25 cm. We used the Audacity audio editing software [2] to reduce the background noise of the speech signal. Audacity is very reputable and is also open source.\n\nData creation: We adopted a semi-supervised recording approach: The participants were given a short description of the purpose of the study and the recording system, and were then allowed to choose a convenient time to conduct the recording. Each participant was first asked to record all 25 emotion-specific sentences. They were provided with the sentence and the required emotion, for example, 'Turn away and let me not see you again' to be spoken in an Angry way. Each recording was made twice. Next on the list could be the sentence 'I think he was a thief who knocked on the door', to be spoken in a Fearful way, and so on. For these emotion-specific sentences, the required emotion was always the same for a given sentence, for all participants. After the 25 sentences, the participant was then asked to record the two common sentences, for example 'my sister is coming on a plane'. Each common sentence was recorded twice for each of the five emotions, Neutral, Fearful, Happy, Sad and Angry. So they would first record the sentence in a Neutral way (twice), then record the same sentence in a Fearful way (twice) and so on. Participants always spoke in their own Amharic dialect. A complete set therefore comprised 25 × 2 = 50 recordings of emotion-specific sentences, and 2 × 5 × 2 = 20 recordings of common sentences, 70 recordings in all.\n\nJudgments: Every recording was independently reviewed by all eight judges. Concerning emotion-specific sentences, each judge decided whether the recording expressed the emotion adequately or not, and made a binary decision, Accept or Reject. For the common sentences, the judge did not know the intended emotion of the recording. They decided which emotion the recording expressed. If it was unclear, their decision was Reject, otherwise their decision was one of the five emotions.\n\nFor emotion-specific sentences, a recording was only accepted for inclusion in the ASED dataset if five or more judges returned the decision Accept. Similarly, a common sentence was only accepted if five or more judges assigned the same emotion to it.\n\nBecause there were 65 partipants each of whom made 70 recordings, we would expect ASED to contain 4,550 recordings. However, because of the above selection process, many of these were rejected, resulting in 2,474 recordings in the dataset (Table  4 ).\n\nInter-annotator agreement: Since we had eight judges, the Fleiss kappa  [26]  coefficient was used to calculate the pairing agreement between participants.\n\nThe factor 1 -p e gives the degree of agreement that is attainable above chance, and p 0 -p e gives the degree of agreement actually achieved above chance: k = 1, if all the raters are in complete agreement. Evaluation of the inter-rater agreement for our dataset in terms of Fleiss kappa is 0.8. This value shows a high agreement among our eight raters. The final dataset consists of 2,474 recordings, each between two and four seconds in length, 522 neutral, 510 fearful, 486 happy, 470 sad, and 486 angry. Recorded phrases were stored in five different folders, one for each emotion. Table  4  shows the final breakdown of utterances across emotions and dialects. The ASED dataset is evenly distributed across all emotion classes, as shown in Fig.  1 . ASED was split into training and testing sets randomly. The training set contains 90% of the whole dataset. The test set contains the rest of the data. The ASED dataset is freely available for research purposes  3  .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison",
      "text": "Before presenting our experiments, we briefly compare ASED to nine other datasets (Table  2 ) in terms of parameters and creation methods. There are four for English, and one each for Chinese, German, Greek, Gujarati, and Hindi.\n\nAESDD  [36]  is for Greek, uses five emotions and contains 19 emotionally neutral utterances derived from theatrical plays plus one improvised utterance. Each of these 20 is spoken in all five emotions by each actor. There are five professional actors and 500 utterances in total. No quality control or annotation process is mentioned. Recording is done at 44.1 kHz and 16 bits. Recordings were made at the sound studio of the Laboratory of Electronic Media.\n\nCHEAVD  [19]  is for Mandarin Chinese and uses six basic emotions plus an additional 20 emotions. There are 2,600 segments selected from 34 films, two tv series, two tv shows, one speech and one talk show. Each segment involves one speaker and there are 238 different speakers in all. Thus there are no recruited participants in this dataset, and all emotions are naturally occurring. Annotation of emotions is carried out by four judges. Pairwise kappa coefficients range between 0.41 and 0.58. However, there are 26 emotions rather than the usual six, so this accounts for the low agreement level. Recordings are all from pre-existing films etc and are at 44.1 kHz and 16 bits.\n\nEGSC  [34]  is for Gujarati, uses six emotions and is based on 24 individual words. Each recording is for just one word, spoken with one of the emotions. There are nine speakers who are experts in drama, and about 1,296 utterances in total.\n\nRecording is done at 44.1 kHz using mobile phones. A quiet room was used.\n\nEMO-DB  [5]  (as already discussed) is for German, uses five emotions and contains ten everyday sentences, five made of one phrase, five made of two phrases. There are ten speakers, nine qualified in acting and about 800 raw utterances in total. All recordings were judged by twenty listeners, who found that about 300 of the 800 utterances had a recognition rate greater than 80%. 535 utterances were finally selected for the database. Recording is done at 16 kHz and 16 bits, and was carried out in the Anechoic chamber of the Technical Acoustics Department at the Technical University Berlin.\n\nIEMOCAP  [6]  is for English, uses five emotions and contains three scripts selected from plays, plus improvised emotional dialogues. Seven speakers are professional actors, three are students. Recordings were judged by six evaluators using a majority voting system. There are 10,039 dialogue turns in the dataset (5,255 scripted turns and 4,784 spontaneous turns). Recording is done at 48 kHz and 16 bits, and took place in the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC).\n\nITKGP-SEHSC  [14]  is for Hindi, uses eight emotions and contains 15 sentences. There are ten professional artists who each record every emotion on every sentence, all in one session. The total number of utterances in the database is 12,000.\n\nRecording was done at 16 kHz and 16 bits, working in a quiet room. Recordings were judged by 25 postgraduate and research students of IIT Kharagpur.\n\nRAVDESS  [20]  (as already discussed) is for English, uses eight emotions and contains just two sentences. The 24 speakers are professional actors. Interestingly, emotion in this dataset is 'self-induced'  [32] , rather than Acted. Moreover, there are two levels of each emotion. There are 4,320 utterances. Project investigators first selected the best two clips for each speaker and each emotion. The selected recordings were then judged by 247 naive judges. The average Fleiss Kappa inter-rater score for speech was 0.57. Recording was at 48 kHz and 16 bits, and it was carried out in a professional recording studio at Ryerson University.\n\nSAVEE  [11]  is for English, uses six emotions and contains three common sentences plus two emotion-specific sentences and ten generic sentences (different for each emotion and phonetically-balanced). There are four speakers (postgraduates and researchers, not professional actors) and there are 480 utterances in total. Recordings were judged by ten evaluators (all students) and average classification accuracy was 66.5%. Recording was at 44.1 kHz and 16 bits, and took place in the 3D vision laboratory at University of Surrey.\n\nTESS  [25]  [7] is for English, uses seven emotions and contains 200 individual words. These are spoken in a carrier sentence 'Say the word X'. There are two female speakers who are professional actors, and 2,800 utterances. Fifty-six judges identified the emotion in each recording with an average accuracy of 82%  [7] . 'Pleasant' was the least well recognised, and 'Anger' the most. Recording was at 24.4 kHz and 16 bits, and took place in a sound attenuating booth at University of Toronto.\n\nA number of interesting comparisons can be made between ASED and the datasets mentioned above. Firstly, concerning participants, we can see that mostly professional actors are used for the recordings. The only exceptions are SAVEE which uses members of the campus community, and CHEAVD which has no participants as such, but uses pre-occurring speech in tv shows etc. For ASED we have a mixture of postgraduates, undergraduates and business persons making the recordings. It is an interesting assumption among all the papers that actors are the best choice. However, the way in which emotions are portrayed in drama is surely entirely different from everyday life. Actors exaggerate and distort the facets of the emotion they portray. Usually they also do this in a way which has come to be expected within the particular entertainment genre, but which is not real. So we would argue that alternatives should be explored if the ultimate goal is to train SER systems for practical tasks such as detecting an upset customer in a call center. The number of participants used for the datasets varies dramatically from just two (TESS) up to 231 (CHEAVD). Excluding CHEAVD, the average is 9.2, so the use of 65 speakers in ASED is well above average, covers all the main dialects in Ethiopia and can thus be considered a representative sample.\n\nFor emotions, mostly the standard five are chosen as the basis and ASED conforms to this norm. Expression of the emotions is generally of the Acted form, i.e. simply asking the participant to speak in, say, an angry way, and leaving it to them how to do it. However, RAVDESS uses self-inducement  [31]  whereby the actor conjurs up the emotion in themselves over a period of some minutes before speaking. IEMOCAP uses scripts selected from plays, where the emotion is developed in the dialogue over several turns. They also try improvised dialogues based around an idea, e.g. for 'sad' the idea is that a close friend has recently died and one person is conforting another. This can allow the emotion to develop more naturally. At the other extreme, TESS expresses the emotion in a single word, with no preparation. For ASED, we use the simulated approach (e.g. please speak in a sad way) but we rely on our quality control mechanism to ensure that the results are satisfactory.\n\nFor the choice of sentences which participants must speak, this varies greatly. EGSC uses single words, while TESS uses 'Say the word X' where X is a single word. RAVDESS has just two neutral sentences, spoken with different emotions. EMO-DB has ten, ITKGP-SEHSC 15, AESDD 20 and SAVEE 120; by contrast, IEMOCAP is using drama scripts plus dialogues, so this is a more naturalistic approach. At the extreme, CHEAVD has 2,600 tv show scripts, so this is an extrapolation of the IEMOCAP idea. For ASED, we are using 25 emotion-specific sentences plus two neutral sentences which are common to all emotions, so this is closest to EMO-DB and AESDD.\n\nConsidering the amount of data produced, ASED (2,474 utterances) compares well with the other datasets; the minimum is SAVEE (480) the maximum ITKGP-SEHSC (12,000 dialogue turns) and the average is 3680. So, for a first Amharic dataset, ASED provides enough data to perform basic training tasks, as we show later. Finally, we consider the judges and agreement level. The number of judges used for CHEAVD (4), IEMOCAP (6) and SAVEEE  (10)  are comparable to ASED  (8) . EMO-DB  (20) , ITKGP-SEHSC  (25) , TESS (56) and RAVDESS (247) use greater numbers. Considering the sampling frequency, both EMO-DB and ITKGP-SEHSC use 16 kHz which is the same as ASED. The majority of datasets are recorded in a quiet laboratory environment, the approach adopted for ASED as well.\n\nThe eight ASED judges showed a high level of inter-rater agreement (Fleiss kappa 0.8 -see Section 2.3 above). RAVDESS report a kappa of 0.57 over their 247 judges, while CHEAVD report kappa 0.41-0.58 over four judges and 26 emotions. Obviously, these figures are not directly comparable, for example the CHEAVD task is much harder because there are many more emotions, however, the ASED agreement level seems very good.\n\nIn conclusion, the proposed ASED SER dataset for Amharic appears to compare well with the others we mention here.\n\nIn the following sections, we present some initial experiments where the ASED data is used for Amharic SER.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Feature Extraction For Ser",
      "text": "The speech signal contains a large number of parameters reflecting emotional characteristics. One of the key issues within SER research is the choice of features which should be used. After reviewing many works on emotion recognition, it is clear that Mel-spectrograms and Mel-Frequency Cepstral Coefficients (MFCC) are broadly utilized in audio classification and speech emotion recognition  [10] . We briefly review these methods below.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mel-Spectrograms",
      "text": "The spectrogram is the relationship between the time and frequency of the audio signal. Different emotions can show different patterns in the energy spectrum. The Mel-spectrogram represents the audio signal in Mel-scale. The logarithmic form of the Mel-spectrogram can be better understood because humans perceive sound on a logarithmic scale. The human ear is observed to act as a sub-band filter bank. These filters overlap and are unevenly spaced on the frequency axis. In audio processing, the signal is considered stationary within 10 to 30 ms, and therefore a window with a shorter duration is selected  [33] . Sample Mel-spectrogram plots for each of the five emotions are shown in Fig.  2 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Mel-Frequency Cepstral Coefficients (Mfcc)",
      "text": "Mel-Frequency Cepstral Coefficient (MFCC) is a coefficient that expresses the short-term power spectrum of a sound. It uses a series of steps to imitate the human cochlea, thereby converting audio signals. The Mel scale is significant because it approximates the human perception of sound instead of being a linear scale  [30] . Sample MFCC plots for each of the five emotions are shown in Fig.  3 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Network Architectures And Settings 4.1 Deep Learning Architectures",
      "text": "As discussed earlier, most of the previous studies employ CNN-based models for SER  [18] . Among such models, the notable ones include Alex-Net  [15, 27] , VGG  [31, 22] , and ResNet50  [9, 8] , as well as LSTM  [16] . VGG is one of the earliest CNN models used for signal processing. It is well known that the early CNN layers capture the general features of sounds such as wavelength, amplitude, etc., and later layers capture more specific features such as the spectrum and the cepstral coefficients of waves. This makes a VGG-style model suitable for the SER task. After some experimentation, we found that a model based on VGG but using four layers gave the best performance. We call this proposed model VGGb and used it for our experiments. Table  6  shows the settings for VGGb. Alex-Net, ResNet50 and LSTM were also used for comparison.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "The standard code for the Alex-Net, VGG, ResNet50 and LSTM models was used for the experiments. The VGG code was adapted to create VGGb (Table  6 ). For the other models, the original network configuration and parameters were used.\n\nWe used the Keras deep learning library, version 2.0, with Tensorflow 1.6.0 backend  [16]  to build the emotion recognition models. The models were trained using a 2.30 GHz (CPU) Intel(R) Xeon(R) CPU. The Adam optimization algorithm was used to train our model, with categorical cross-entropy as the loss function; training stopped after 100 epochs, and the batch size was set to 16.\n\n5 Experiment 1: Choice of Features for Amharic SER",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Outline",
      "text": "As we have mentioned, Mel-spectrograms and MFCC are two forms of feature which are widely used within SER systems for other languages. We therefore wished to determine which of these was most suitable for Amharic SER. The experiment was divided into four parts. First, a direct comparison of Mel-Spectrograms and MFCC was carried out, by substituting each one in turn into the VGGb architecture and determining the resulting SER performance. Second, the dataset sentences used for training and testing were varied in order to show that this did not affect results. Third, the dialects used for training and testing were varied. Fourth, the groups of speakers used for training and testing were varied. The effect of steps 2-4 was to validate the performance of the two feature types by performing a kind of cross-validation based on sentences, dialects and speaker groups. We now describe each experiment in turn.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiment 1.1: Initial Comparison",
      "text": "We adopted the VGGb model for our experiments. The main settings are shown in Table  6 . Training and testing were performed with ASED data. We extracted both Mel-spectrogram and MFCC features, using the librosa v0.7.2 library  [29] . Mel-spectrogram was extracted with 128 bands, and MFCC with 40 bands, according to the standard settings of the tool.\n\nFirst, the model was trained and evaluated using just Mel features. Second, it was trained and evaluated using just MFCC features. Data was split 90% train and 10% test. The model was trained five times with a different random train/test split, and the average result was reported.\n\nThe results are shown in Table  8 . MFCC has proven to be effective at extracting the important features  [20, 27, 22] , as has been demonstrated in previous experiments with VGG CNN-based SER models. Our results with VGGb confirmed this trend. The classification accuracy was 90.73% for MFCC as compared with 81.05% for Mel-spectrograms.\n\nThe confusion matrix for each feature is shown in Fig.  4 . As shown in Fig.  4 (a), the Mel-spectrogram VGGb model incorrectly classifies 13% of sad cases as fearful. In consequence, prediction for the sad class is reduced to 73.0%. The VGGb model over Mel-spectrogram, Fig.  4(a) , shows less prediction gains in predicting the fearful as neutral (16.0%) when compared to MFCC (73.0% to 90.0%). This outcome appears to be conceivable because the MFCC can benefit from the difference between the word distributions of fearful and neutral expressions. It is striking that the VGGb model using Mel-spectrogram incorrectly predicts the happy class as fearful and neutral 7.0% of the time, sad as fearful 13.0% of the time, and sad as happy 9.0% of the time, even though these emotional states are opposite to each other.\n\nCompared to the Mel-spectrogram, the MFCC-based model in Figure  4 (b) shows significant gains when predicting the sad class (93% vs. 73% for Mel) and the fearful class (90% vs. 73% for Mel). Here, MFCC incorrectly predicts instances of the sad class as happy only 2.0% of the time, compared to 9% for Mel-spectrogram. The frequency of incorrect cases fearful-to-neutral and sad-to-fearful relative to the Mel-spectrogram decreased from 16.0% to 0.0%, and from 13.0% to 5.0% respectively. The values on the diagonal axis indicate that the accuracy of the correctly predicted class has increased.\n\nThe Confusion matrices show that certain emotions are often mistaken for other emotions. Likewise, a few emotions are easier to recognize. This could be because the test data is based on participants pretending to speak with designated emotions, and people found it difficult to imitate certain emotions. It can be seen that neutral, fearful, sad, and angry are the emotions that are less difficult to recognize in Amharic speech as opposed to happy which is the most difficult. Experiment 1.1 suggested that MFCC features were superior to Mel-spectrogram, but it did not indicate why this was the case, or what experimental factors could influence the outcome. Therefore, three further variations of the experiment were performed to investigate this further.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Experiment 1.2: Independence Of Sentences",
      "text": "Recall that the dataset consists of five sentences, one for each emotion, and two common sentences, which are spoken in every emotion. In this experiment sentences were either used for training or for testing according to the scheme in Table  8 . This was to determine how dependent the recognition of emotion is on the sentence used to express it. For each of the schemes shown in the table, VGGb was trained once using Mel-spectrograms and once using MFCC. The average result for each feature type is shown at the bottom. As can be seen in Table  8 , the average performance for MFCC (80.37%) is higher than that for Mel-spectrogram (72.12%). The standard deviations for these values are 1.60 and 3.42 respectively. In each training scenario, MFCC is better than Mel-spectrogram; moreover, the standard deviations are low, indicating that there is only a small difference in performance between different scenarios, i.e. choices of sentences for training and testing. ASED consists of recordings in four dialects, Gojjam, Wollo, Shewa and Gonder. In this experiment, the model is trained with utterances spoken in three dialects, and then tested with utterances in the fourth dialect, according to the scheme in Table  9 . Similar to Experiment 1.2, this was to determine how dependent the emotion found is on the dialect used to express it. In Table  9 , the average performance for MFCC (74.51%) is higher than that for Mel-spectrogram (67.29%). The standard deviations for these values are 1.93 and 1.99 respectively, suggesting that dialect is only making small changes to the performance figures and is not affecting the overall superiority of MFCC under all training scenarios. Recall that the four models are: Alex-Net, VGGb, ResNet50 and LSTM. The network configuration for VGGb was the same as in the previous Experiment (Table  6 ). For the other models, the standard configuration and settings were used. Once again, the librosa v0.7.2 library  [29]  was used to extract MFCC features to input to the models. Each model was trained five times using a 90%/10% test/train split and the average results were computed.  Results are presented in Table  11 . Although ResNet50 had the highest accuracy (91.13%), VGGb was just 0.4% behind (90.73%). Moreover, VGGb was much faster than ResNet50 (00:31:18 vs. 08:44:15), showing that it is much more efficient and hence more suitable for applying to large datasets.\n\nThe computational simplicity of VGGb in terms of training time and overall accuracy is again shown in Fig.  5  which represents the val-accuracy curve for the implemented models, AlexNet, ResNet50, LSTM and VGGb. The models are trained for 100 epochs. The curves show that after the 20th epoch, the val-accuracy starts stabilizing. The curve for VGGb looks like a better fit, while that for ResNet50 shows more noisy movements than the other models.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Experiment 3: Comparison Of English, German And Amharic Ser",
      "text": "The aim of Experiment 3 was to compare the performance of VGGb on three datasets, RAVDESS (English), EMO-DB (German) and ASED (Amharic). Naturally there are interesting SER databases for many different languages (Table  2 ). However, in previous work, RAVDESS and EMO-DB have been extensively used for SER research, and are frequently quoted as baselines. This is why we chose to use these datasets, rather than others, for the comparison. Table  1  (Section 2.1) provides summary information for the three datasets, which we also analysed in detail in Section 2.4.\n\nThe VGGb model was trained using each of the three datasets. Network configuration and settings were there same as for previous experiments (Table  6 ). Input features were MFCC, extracted by librosa v0.7.2. In each case, training was carried out five times with a 90%/10% test train split and the average results were computed.  Table  12  shows the results. Performance on ASED is the best (90.73%) but all accuracy figures are within a range of 4.2%. From this we can conclude that SER can be successfully performed on Amharic speech, and that the accuracy which can be attained is similar to that for English and German. The training time is longer (00:31:18) relative to EMO-DB (00:06:02) and RAVDESS (00:12:33) but this is because ASED is more than double the size of the other datasets. Fig.  6  shows the model's performance across the three datasets and across the five emotions. We can see that the fearful and sad classes are more clearly distinguished in ASED than in the other datasets. Conversely, the angry class is not as clearly distinguished as in the others.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we first collected what we believe to be the very first SER dataset for the Amharic language, working with five emotions, neutral, fearful, happy, sad and angry. We then conducted three experiments, based on a four-layer version of VGG which we call VGGb. Experiment 1 was to determine whether Mel-spectrogram features or MFCC features were most suitable for SER in Amharic and to establish how robust this difference was relative to sentence choice, dialect and speaker group. Experiment 1.1 first compared performance on the whole dataset with standard cross-validation. Experiments 1.2-1.4 then tested the results using a form of cross-validation relative to sentences, dialects and speaker groups. The results showed that MFCC features were superior to Mel-spectrogram for Amharic SER and that this result was robust relative to the variations made.\n\nIn the second experiment, working with MFCC features and the ASED data, we compared the performance of four different models when applied to the SER task, Alex-Net, ResNet50, LSTM, and VGGb. While ResNet50 was the best (91.13%), VGGb was very close (90.73%) and was considerably faster (training time for ResNet50 08:44:15, for VGGb 00:31:18).\n\nThe third experiment compared the performance of VGGb on three datasets in different languages, RAVDESS (English), EMO-DB (German) and ASED (Amharic). The accuracies in each case were similar, suggesting that VGGb is equally applicable to the three languages.\n\nFuture work on ASED will include enlarging the scale of the database, using further elicitation techniques and adopting a dimensional emotion annotation strategy. We also plan to build a better SER model for Amharic and to study the extent to which emotion recognition is dependent on language.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ASED dataset distribution across all emotion classes.",
      "page": 5
    },
    {
      "caption": "Figure 1: ASED was split into training and testing sets randomly. The training set contains",
      "page": 7
    },
    {
      "caption": "Figure 2: (a) Neutral",
      "page": 9
    },
    {
      "caption": "Figure 2: Examples of Mel-spectrogram features for each emotion in ASED.",
      "page": 9
    },
    {
      "caption": "Figure 3: (a) Neutral",
      "page": 9
    },
    {
      "caption": "Figure 3: Examples of MFCC features for each emotion in ASED.",
      "page": 9
    },
    {
      "caption": "Figure 4: As shown in Fig. 4(a), the Mel-spectrogram VGGb model",
      "page": 10
    },
    {
      "caption": "Figure 4: (a), shows less prediction gains in predicting the fearful as neutral (16.0%)",
      "page": 10
    },
    {
      "caption": "Figure 4: (b) shows signiﬁcant gains when predicting",
      "page": 10
    },
    {
      "caption": "Figure 4: VGGb confusion matrices using Mel-Spectogram and MFCC features on text-dependent ASED.",
      "page": 11
    },
    {
      "caption": "Figure 5: Val-accuracy training curves for Alex-Net, ResNet50, LSTM and VGGb models on ASED.",
      "page": 13
    },
    {
      "caption": "Figure 6: Performance of VGGb on the RAVDESS, Emo-DB and ASED databases.",
      "page": 14
    },
    {
      "caption": "Figure 6: shows the model’s performance across the three datasets and across the ﬁve emotions. We can see that",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the main data. The Ryerson Audio-Visual Database of Emotional Speech and Song",
      "page": 3
    },
    {
      "caption": "Table 1: Comparison between RAVDESS, Emo-DB and ASED datasets.",
      "page": 3
    },
    {
      "caption": "Table 2: Some datasets for speech emotion recognition (Sp+Utt = Speakers and Utterances, Sampl = Sampling",
      "page": 4
    },
    {
      "caption": "Table 3: ), a sentence which expresses a",
      "page": 4
    },
    {
      "caption": "Table 3: Examples of sentences expressing each of the ﬁve emotions.",
      "page": 5
    },
    {
      "caption": "Table 4: Number of utterances per speaker in the ASED database.",
      "page": 5
    },
    {
      "caption": "Table 5: ). Participants who were clearly experienced at acting and were excellent at expressing the different",
      "page": 6
    },
    {
      "caption": "Table 5: Classiﬁcation of 65 participants in ASED dataset.",
      "page": 6
    },
    {
      "caption": "Table 4: shows the ﬁnal breakdown of utterances across emotions and dialects. The ASED dataset is evenly distributed across all",
      "page": 7
    },
    {
      "caption": "Table 2: ) in terms of parameters",
      "page": 7
    },
    {
      "caption": "Table 6: VGGb architecture used in experiments.",
      "page": 9
    },
    {
      "caption": "Table 6: shows the settings for VGGb. Alex-Net, ResNet50 and",
      "page": 10
    },
    {
      "caption": "Table 6: ). For the other models, the original network conﬁguration and parameters were",
      "page": 10
    },
    {
      "caption": "Table 6: Training and testing were",
      "page": 10
    },
    {
      "caption": "Table 8: MFCC has proven to be effective at extracting the important features [20, 27, 22], as",
      "page": 10
    },
    {
      "caption": "Table 7: Accuracy of VGGb on ASED using MFCC and Mel-Spectrogram Features.",
      "page": 11
    },
    {
      "caption": "Table 8: This was to determine how dependent the recognition of emotion is on the sentence used to express it. For each of",
      "page": 11
    },
    {
      "caption": "Table 8: , the average performance for MFCC",
      "page": 11
    },
    {
      "caption": "Table 8: Accuracy of VGGb on ASED using MFCC and Mel-Spectrogram Features: Sentence Independence.",
      "page": 11
    },
    {
      "caption": "Table 9: Similar to Experiment 1.2, this was to determine how dependent the emotion found is on the dialect",
      "page": 11
    },
    {
      "caption": "Table 9: , the average performance for MFCC (74.51%) is higher than that for Mel-spectrogram",
      "page": 11
    },
    {
      "caption": "Table 9: Accuracy of VGGb on ASED using MFCC and Mel-Spectrogram Features: Dialect Independence.",
      "page": 12
    },
    {
      "caption": "Table 10: The results in that table once again show that",
      "page": 12
    },
    {
      "caption": "Table 10: Accuracy of VGGb on ASED using MFCC and Mel-Spectrogram Features: Speaker Group Independence.",
      "page": 12
    },
    {
      "caption": "Table 6: ). For the other models, the standard conﬁguration and settings were used.",
      "page": 12
    },
    {
      "caption": "Table 11: Comparison of VGGb with other CNN and RNN models, all applied to the ASED dataset.",
      "page": 12
    },
    {
      "caption": "Table 11: Although ResNet50 had the highest accuracy (91.13%), VGGb was just 0.4% behind",
      "page": 13
    },
    {
      "caption": "Table 6: ). Input features were MFCC, extracted by librosa v0.7.2. In each case, training was",
      "page": 13
    },
    {
      "caption": "Table 12: Comparison of VGGb applied to different datasets.",
      "page": 13
    },
    {
      "caption": "Table 12: shows the results. Performance on ASED is the best (90.73%) but all accuracy ﬁgures are within a range of",
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Grapheme-to-phoneme conversion for amharic text-to-speech system",
      "authors": [
        "T Anberbir",
        "M Gasser",
        "T Takara",
        "K Yoon"
      ],
      "year": "2011",
      "venue": "Proceedings of conference on human language technology for development"
    },
    {
      "citation_id": "2",
      "title": "Effects of emotion on physiological signals",
      "authors": [
        "S Basu",
        "A Bag",
        "M Aftabuddin",
        "M Mahadevappa",
        "J Mukherjee",
        "R Guha"
      ],
      "year": "2016",
      "venue": "2016 IEEE Annual India Conference (INDICON)"
    },
    {
      "citation_id": "3",
      "title": "Amharic ocr: An end-to-end learning",
      "authors": [
        "B Belay",
        "T Habtegebrial",
        "M Meshesha",
        "M Liwicki",
        "G Belay",
        "D Stricker"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "4",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Recognition of emotional speech for younger and older talkers: behavioural findings from the toronto emotional speech set",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2011",
      "venue": "Canadian Acoustics"
    },
    {
      "citation_id": "7",
      "title": "Deep transfer learning: A new deep learning glitch classification method for advanced ligo",
      "authors": [
        "D George",
        "H Shen",
        "E Huerta"
      ],
      "year": "2017",
      "venue": "Deep transfer learning: A new deep learning glitch classification method for advanced ligo",
      "arxiv": "arXiv:1706.07446"
    },
    {
      "citation_id": "8",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "11",
      "title": "Automatic speech emotion recognition using machine learning",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "M Mbarki",
        "K Raoof",
        "M Mahjoub",
        "C Cleder"
      ],
      "year": "2019",
      "venue": "Social Media and Machine Learning"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Iitkgp-sehsc: Hindi speech corpus for emotion analysis",
      "authors": [
        "S Koolagudi",
        "R Reddy",
        "J Yadav",
        "K Rao"
      ],
      "year": "2011",
      "venue": "2011 International conference on devices and communications (ICDeCom)"
    },
    {
      "citation_id": "14",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using mfcc features and lstm network",
      "authors": [
        "H Kumbhar",
        "S Bhandari"
      ],
      "year": "2019",
      "venue": "2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA)"
    },
    {
      "citation_id": "16",
      "title": "Clstm: Deep feature-based speech emotion recognition using the hierarchical convlstm network",
      "authors": [
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Mathematics"
    },
    {
      "citation_id": "17",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "Cheavd: a chinese natural emotional audio-visual database",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao",
        "W Bao",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "19",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "20",
      "title": "Text independent amharic language dialect recognition using neuro-fuzzy gaussian membership function",
      "authors": [
        "A Mengistu",
        "M Bedane"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Studies in Computers, Science and Engineering"
    },
    {
      "citation_id": "21",
      "title": "Pruning convolutional neural networks for resource efficient inference",
      "authors": [
        "P Molchanov",
        "S Tyree",
        "T Karras",
        "T Aila",
        "J Kautz"
      ],
      "year": "2016",
      "venue": "Pruning convolutional neural networks for resource efficient inference",
      "arxiv": "arXiv:1611.06440"
    },
    {
      "citation_id": "22",
      "title": "Social network hate speech detection for amharic language",
      "authors": [
        "Z Mossie",
        "J.-H Wang"
      ],
      "year": "2018",
      "venue": "Computer Science & Information Technology"
    },
    {
      "citation_id": "23",
      "title": "Learning morphological rules for amharic verbs using inductive logic programming. Language Technology for Normalisation of Less-Resourced Languages",
      "authors": [
        "W Mulugeta",
        "M Gasser"
      ],
      "year": "2012",
      "venue": "Learning morphological rules for amharic verbs using inductive logic programming. Language Technology for Normalisation of Less-Resourced Languages"
    },
    {
      "citation_id": "24",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (TESS)"
    },
    {
      "citation_id": "25",
      "title": "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa. Online submission",
      "authors": [
        "J Randolph"
      ],
      "year": "2005",
      "venue": "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss' fixed-marginal multirater kappa. Online submission"
    },
    {
      "citation_id": "26",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "28",
      "title": "Bengali spoken digit classification: A deep learning approach using convolutional neural network",
      "authors": [
        "R Sharmin",
        "S Rahut",
        "M Huq"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition and classification in speech using artificial neural networks",
      "authors": [
        "A Shaw",
        "R Vardhan",
        "S Saxena"
      ],
      "year": "2016",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "30",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "31",
      "title": "An actor prepares (new york",
      "authors": [
        "C Stanislavski"
      ],
      "year": "1936",
      "venue": "Theatre Arts"
    },
    {
      "citation_id": "32",
      "title": "Novel phase encoded mel filterbank energies for environmental sound classification",
      "authors": [
        "R Tak",
        "D Agrawal",
        "H Patil"
      ],
      "year": "2017",
      "venue": "International Conference on Pattern Recognition and Machine Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Creation of speech corpus for emotion analysis in gujarati language and its evaluation by various speech parameters",
      "authors": [
        "V Tank",
        "S Hadia"
      ],
      "year": "2020",
      "venue": "International Journal of Electrical and Computer Engineering"
    },
    {
      "citation_id": "34",
      "title": "Speech Emotion Recognition Using Convolutional Neural Networks",
      "authors": [
        "G Tomas"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Using Convolutional Neural Networks"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    }
  ]
}