{
  "paper_id": "2309.07925v1",
  "title": "Hierarchical Audio-Visual Information Fusion With Multi-Label Joint Decoding For Mer 2023",
  "published": "2023-09-11T03:19:10Z",
  "authors": [
    "Haotian Wang",
    "Yuxuan Xi",
    "Hang Chen",
    "Jun Du",
    "Yan Song",
    "Qing Wang",
    "Hengshun Zhou",
    "Chenxi Wang",
    "Jiefeng Ma",
    "Pengfei Hu",
    "Ya Jiang",
    "Shi Cheng",
    "Jie Zhang",
    "Yuzhe Weng"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computing methodologies ‚Üí Neural networks",
    "Artificial intelligence",
    "Multi-task learning",
    "Computer vision MER2023, deep feature fusion, joint decoding, multi-task learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a novel framework for recognizing both discrete and dimensional emotions. In our framework, deep features extracted from foundation models are used as robust acoustic and visual representations of raw video. Three different structures based on attention-guided feature gathering (AFG) are designed for deep feature fusion. Then, we introduce a joint decoding structure for emotion classification and valence regression in the decoding stage. A multi-task loss based on uncertainty is also designed to optimize the whole process. Finally, by combining three different structures on the posterior probability level, we obtain the final predictions of discrete and dimensional emotions. When tested on the dataset of multimodal emotion recognition challenge (MER  2023), the proposed framework yields consistent improvements in both emotion classification and valence regression. Our final system achieves state-of-the-art performance and ranks third on the leaderboard on MER-MULTI sub-challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MER) plays a crucial role in natural human-machine interaction  [25, 30] , intelligent education tutoring  [23, 24] , and mental health diagnoses  [16, 32] , etc. In our daily life, when we engage in dialogue and communication, we usually convey our emotions through both verbal and non-verbal content, such as facial expressions and body language  [26] . Previous studies focused on emotion recognition in text  [12, 34] , facial expression  [26, 36]  and audio  [17, 27] . However, it has been observed that research on single-modality approaches has reached a certain bottleneck, thereby leading to increased attention toward the use of multimodal approaches  [3, 9, 14, 18] .\n\nRegarding human communication scenarios, emotions are mainly expressed through speech and facial expressions, each providing complementary information. As a result, researchers are dedicated to fusing audio and video modal features  [10, 22, 37, 38] . For example, Han et al.  [10]  proposed a hierarchical approach that maximized the Mutual Information (MI) among unimodal inputs. Hazarika et al.  [22]  projected each modality to modality-invariant and modalityspecific spaces to learn effective representations. Recently, inspired by the success of pre-trained deep features like wav2vec2.0  [1]  and HUBURT  [13]  in other speech-related tasks, some researchers investigate their superiority over hand-engineered features in speech emotion recognition and discovered that these deep features capture more robust acoustic representations  [4, 15, 31, 33] .\n\nAccording to the theory of psychological research, there are two main emotional calculation models: discrete theory and dimensional theory. Discrete theory  [7]  describes emotional states as discrete labels such as \"sad\" and \"happy\". On the other hand, the theory of dimensionality  [8]  suggests that emotional states exist as points in a continuous space. This allows for simulating subtle, complex, and sustained emotional behaviors. Previous studies have found that there exists a strong correlation between these two models  [5, 31] .  In this paper, we propose an efficient multimodal emotion recognition system to recognize both discrete emotion (emotion) and dimensional emotion (valence). Firstly, we extract deep features through different layers of various pre-trained models as robust acoustic and visual representations of raw video segments. Then we fuse these features through three proposed feature fusion structures based on AFG  [20] . Afterwards, we have designed a joint decoding module that considers both discrete and dimensional theories based on the correlation between emotion and valence to generate decisions for these two dimensions. A multi-task loss function based on uncertainty  [6]  is also designed to optimize the whole encoding and decoding process. Finally, we generate final predictions of both discrete and dimensional emotions by decision-level fusion. We evaluate the proposed framework on the dataset of the multimodal emotion recognition challenge (MER 2023)  [19] . Experiments show that our framework yields consistent improvements on both emotion classification and valence regression and ranks third on the leaderboard on MER-MULTI sub-challenge.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "In this section, we will discuss our proposed multimodal emotion recognition system in two subsections. The feature extraction and fusion strategies will be illustrated in the first subsection. The proposed joint decoding module of discrete and dimensional emotions and the designed multi-task loss function will be introduced in the second subsection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Features Encoding And Attention-Guided Fusion",
      "text": "In our proposed architecture, we first extract deep features from pre-trained models as robust acoustic and visual representations of raw video segments. The details are illustrated in Figure  1 . Previous research has indicated that different layers in pre-trained speech model HUBURT  [13]  capture audio hidden states with distinctive characteristics  [35] . The hidden states captured by layers closer to the front exhibit increased sensitivity to the acoustic features of the original audio, such as tone or frequency. In contrast, the hidden states captured by layers towards the back demonstrate a heightened sensitivity to the semantic information embedded within the audio. These different features with distinct acoustic information from the same audio segment can be complementary. Hence, for the acoustic modality, our framework incorporates different HUBURT layers to extract low-level, mid-level, and high-level audio features, forming a unified representation of the original audio. For visual modality, we first crop and align faces of raw video in each frame using the OpenFace  [2]  toolkit. Then, we utilize various pre-trained models (such as MANet  [21]  and ResNet-50  [11] ) to extract framelevel features and apply average encoding to compress them into video-level embeddings. Then, these different acoustic and visual representations will be fused together with deep feature fusion frameworks based on attention-guided feature gathering (AFG)  [20] . In addition, features of text modality, however, are proved to be underperforming comparing with acoustic and visual features in this task [19] so we do not take features from text encoders into consideration in our framework. The architecture of AFG module is shown in Figure  2 , whose principle is as follows:\n\nwhere ùíâ  Three different feature fusion frameworks have been designed based on AFG  [20] , as depicted in Figure  1 . In the first framework, we use AFG to fuse all features from acoustic and visual modalities in parallel. In the second framework, we fuse each acoustic representation with visual features to generate different audio-visual representations. These representations are then fused together, as they exhibit strong complementarity. While in the last framework, intra-modal fusion is firstly performed on the acoustic hidden states extracted by different layers of HUBURT  [13]  to create a unified acoustic representation, then inter-modal fusion is conducted based on the unified acoustic representation and visual representations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Joint Decoding Of Discrete And Dimensional Emotions",
      "text": "In our studies, we found there exists a relatively stable distribution between discrete and dimensional emotions  [5, 31] . In other words, the discrete emotions can determine the dimensional emotions according to the following formula, where ùëí ùëñ represents discrete emotions while ùë£ ùëñ represents dimensional emotions. ùëò represents categories of emotions.\n\nTherefore, our research incorporates a branch for discrete emotions judgment of dimensional emotions into the network structure. This branch establishes a mapping from the discrete space to the dimensional space, as depicted in Figure  3 . The prediction of discrete and dimensional emotions by our multi-task framework is as follows:\n\nwhere ƒ•ùëñ ‚àà R ùê∑ is the fused feature, √™ùëñ ‚àà R ùê∂ and vùëñ ‚àà R 1 are the estimated emotion and valence possibilities, respectively. ùëæ ùíÜ ‚àà R ùê∑ √óùê∂ , ùíÉ ùíÜ ‚àà R ùê∑ √óùê∂ , ùëæ ùíóùíó ‚àà R 2√ó1 and ùíÉ ùíóùíó ‚àà R 2√ó1 are trainable parameters. ·πΩùëñ ‚àà R and ·πΩùëñ ùëí ‚àà R are the estimated valence possibilities according to the fused state ƒ•ùëñ and emotion hidden state √™ùëñ with trainable parameters ùëæ ùíó ‚àà R ùê∑ √ó1 , ùíÉ ùíó ‚àà R ùê∑ √ó1 , ùëæ ùíÜùíó ‚àà R ùê∂ √ó1 and ùíÉ ùíÜùíó ‚àà R ùê∂ √ó1 , as follows: ·πΩùëñ = ƒ•ùëñ ùëæ ùíó + ùíÉ ùíó (7)\n\nDuring training, we use the cross-entropy (CE) loss as the classification loss, denoted as L ùëí , for emotion prediction. The mean squared error (MSE) loss is adopted for valence prediction, denoted as L ùë£ . Moreover, we introduce dynamic weights to combine the two losses for better performance in the multi-task learning process.\n\nInspired by the Uncertainty loss  [6] , we introduce uncertainty loss weighting to L ùëí and L ùë£ , whose principle is as follows:\n\nwhere ùõø 1 and ùõø 2 are trainable uncertainty weights. We improved the regular loss term to log(1 + ùõø 1 ) and log(1 + ùõø 2 ) to avoid effects caused by enormous negative weights.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "We have conducted several experiments to evaluate the effectiveness of the proposed multimodal framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Metric",
      "text": "In this research, we conduct experiments on MER 2023 dataset  [19] .\n\nThe dataset consists of 3373 labeled single-speaker video segments used as the training dataset. There are 411 and 412 unlabeled video segments for the test set in tracks 1 and 2, respectively. Same with the baseline [19], the combined metric of emotion classification and valence regression is chosen to evaluate the overall performance of discrete and dimensional emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementations",
      "text": "We have employed several data augmentation techniques specifically designed for emotional data on the training dataset for MER-NOISE sub-challenge. For the audio modality, we add speakerindependent noise with 7 different signal-to-noise ratios (from 5dB to 11dB with a step of 1dB) from the ùë†ùëùùëíùëíùëê‚Ñé subset of MU-SAN  [28]  to simulate audio segments of various qualities. For the visual modality, we perform various image transformations to augment the data, including variations in brightness (e.g., solarize), changes in articulation (e.g., blur), alterations in position (e.g., rotate), and modifications in image content (e.g., cutout). We also conduct decision-level fusion on predictions of multimodal emotion recognition systems with three fusion strategies. The fused predictions are as follows:\n\nwhere √™ùëñ and vùëñ represents the emotion and valence prediction vectors from three different multimodal emotion recognition systems. ùëò 1 , ùëò 2 , ùëò 3 (ùëò 1 + ùëò 2 + ùëò 3 = 1) represents the weighted factors of three systems respectively. The final decisions of emotion and valence are based on the posterior probability outputs of separate systems with different feature fusion encoders.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance Comparison Of Unimodal Systems",
      "text": "We compared the performance of different unimodal emotion recognition systems. Different unimodal systems utilizing different layers of the same pre-trained model as deep feature encoders have also been considered. Figure  4  presents the performance diversity of different layers of audio and visual systems on Train&Val. HL(ùëñ) indicates using the output of ùëñ-th layer of HUBURT-large  [13]  model as acoustic feature. RF(ùëó) and MR(ùëó) indicate using the output of ùëó-th block of ResNet-FER2013  [11]  or MANet-RAFDB  [21]  as encoder output. Relative output position indicates the relative positions that the output layer in the whole pre-trained model (HUBURT-large  [13] , ResNet  [11]  or MANet  [21] ).\n\nAmong various acoustic features, HL  (18) , HL(19), HL(20) outperform others, confirming that the mid-level features from HUBERTlarge  [13]  model is more suitable for emotion recognition. For visual modality, high-level features generated by the last block of ResNet  [11]  and MANet  [21]  outperform others. In general, acoustic features performs better than visual features in this task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance Comparison Of Multimodal Systems",
      "text": "In this section, we perform multimodal fusion based on 3 wellperforming acoustic features HL  (18) , HL(19), HL(20) and 2 visual features RF(pooling) and MR(block2). We conduct both intra-modal and inter-modal fusion. Three fusion strategies are conducted separately in inter-modal fusion. Then, the baseline[19] and the proposed joint decoding manner are utilized separately to obtain predictions for emotion and valence. The results are shown in Table  1 .\n\nThe results indicate that features from different layers of the same model can be complementary, as the fusion metric improves by 1-2 percent points comparing with single-layer feature. Furthermore, the results demonstrate that incorporating features from different modalities significantly improves performance. The final multimodal system achieves a score of 0.6846 tested on MER-MULTI, which is a 16.6 percent improvement to the unimodal system. Interestingly, we observed a significant gain on MSE loss for dimensional emotion regression when utilizing JDEV. This phenomenon suggests that the relatively reliable results of emotion classification can assist in improving the accuracy of dimensional valence regression by joint decoding.\n\nAmong the three proposed fusion strategies, the parallel fusion strategy 1 achieves the highest metric of 0.6779 on MER-MULTI. Additionally, when combining all three strategies on posterior probability level, we obtain an additional score gain of 0.67 percent points when tested on MER-MULTI sub-challenge.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a hierarchical audio-visual information fusion framework for recognizing both discrete and dimensional emotions. Three different feature fusion encoders are designed for deep feature fusion. In the decoding stage, we introduce a joint decoding structure for emotion classification and valence regression. In addition, a multi-task loss is also designed as optimizer for the whole process. Finally, by combining three different structures on posterior probability level, we obtain the final predictions of both emotion and valence. When tested on the dataset of MER 2023, our final system ranks third on the MER-MULTI sub-challenge.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Deep acoustic and visual features extraction and",
      "page": 2
    },
    {
      "caption": "Figure 2: Attention-guided feature gathering (AFG) module.",
      "page": 2
    },
    {
      "caption": "Figure 1: . Previous",
      "page": 2
    },
    {
      "caption": "Figure 3: Joint decoding module of discrete and dimensional",
      "page": 3
    },
    {
      "caption": "Figure 1: Three different feature fusion frameworks have been designed",
      "page": 3
    },
    {
      "caption": "Figure 1: In the first framework,",
      "page": 3
    },
    {
      "caption": "Figure 3: The prediction of discrete and",
      "page": 3
    },
    {
      "caption": "Figure 4: Performance comparison of unimodal systems",
      "page": 4
    },
    {
      "caption": "Figure 4: presents the performance diversity of different layers",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "H\nu\nb\ne\nr\nt\n-\nl\na\nr\ng\ne\nH\nL\n(\n1\n8\n)\nH\nL\n(\n2\n0\n)\n \nR\ne\ns\nN\ne\nt\n-\nF\nE\nR\n2\n0\n2\n3\nH\nL\n(\n1\n6\n)\nH\nL\n(\n1\n4\n)\n \nM\nA\nN\ne\nt\n-\nR\nA\nF\nD\nB\nH\nL\n(\n1\n9\n)": "H\nL\n(\n1\n2\n)\nH\nL\n(\n2\n4\n)\nH\nL\n(\n1\n0\n)\nR\nF\n(\n)\np\no\no\nl\ni\nn\ng\nH\nL\n(\n8\n)\nR\nF\n(\nb\nl\no\nc\nk\n5\n)\nH\nL\n(\n6\n)\nM\nR\n(\nb\nl\no\nc\nk\n2\n)\nR\nF\n(\nb\nl\no\nc\nk\n4\n)\nM\nR\n(\nb\nl\no\nc\nk\n1\n)\nR\nF\n(\nb\nl\no\nc\nk\n3\n)\nR\nF\n(\nb\nl\no\nc\nk\n2\n)\nM\nR\n(\nb\nl\no\nc\nk\n0\n)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Framework for Self-Supervised Learning of Speech Representations",
      "arxiv": "arXiv:2006.11477[cs.CL]"
    },
    {
      "citation_id": "2",
      "title": "OpenFace: An open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision",
      "doi": "10.1109/WACV.2016.7477553"
    },
    {
      "citation_id": "3",
      "title": "Multimodal Multi-Task Learning for Dimensional and Continuous Emotion Recognition",
      "authors": [
        "Shizhe Chen",
        "Qin Jin",
        "Jinming Zhao",
        "Shuai Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge",
      "doi": "10.1145/3133944.3133949"
    },
    {
      "citation_id": "4",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Xiangzhan Zeng",
        "Furu Yu",
        "Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing",
      "doi": "10.1109/JSTSP.2022.3188113"
    },
    {
      "citation_id": "5",
      "title": "Exploiting Cooccurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier",
      "authors": [
        "Huang-Cheng Chou",
        "Chi-Chun Lee",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-11041"
    },
    {
      "citation_id": "6",
      "title": "Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics",
      "authors": [
        "Roberto Cipolla",
        "Yarin Gal",
        "Alex Kendall"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00781"
    },
    {
      "citation_id": "7",
      "title": "Emotion Detection: A Technology Review",
      "authors": [
        "Jose Maria",
        "Garcia- Garcia",
        "M Victor",
        "Maria Penichet",
        "Lozano"
      ],
      "year": "2017",
      "venue": "Proceedings of the XVIII International Conference on Human Computer Interaction",
      "doi": "10.1145/3123818.3123852"
    },
    {
      "citation_id": "8",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "Hatice Gunes",
        "Bjoern Schuller"
      ],
      "year": "2013",
      "venue": "Image & Vision Computing"
    },
    {
      "citation_id": "9",
      "title": "Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Alexander Gelbukh",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction",
      "doi": "10.1145/3462244.3479919"
    },
    {
      "citation_id": "10",
      "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "arxiv": "arXiv:2109.00412[cs.CL]"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion Detection from Text via Ensemble Classification Using Word Embeddings",
      "authors": [
        "Jonathan Herzig",
        "Michal Shmueli-Scheuer",
        "David Konopnicki"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval",
      "doi": "10.1145/3121050.3121093"
    },
    {
      "citation_id": "13",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2021.3122291"
    },
    {
      "citation_id": "14",
      "title": "Multimodal Sentiment Analysis To Explore the Structure of Emotions",
      "authors": [
        "Anthony Hu",
        "Seth Flaxman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",
      "doi": "10.1145/3219819.3219853"
    },
    {
      "citation_id": "15",
      "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Emotion Cognizance Improves Health Fake News Identification",
      "authors": [
        "K Anoop",
        "P Deepak",
        "V L Lajish"
      ],
      "year": "2020",
      "venue": "Proceedings of the 24th Symposium on International Database Engineering & Applications",
      "doi": "10.1145/3410566.3410595"
    },
    {
      "citation_id": "17",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "Edward Ruhul Amin Khalil",
        "Mohammad Jones",
        "Tariqullah Inayatullah Babar",
        "Mohammad Jan",
        "Thamer Haseeb Zafar",
        "Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2936124"
    },
    {
      "citation_id": "18",
      "title": "CTNet: Conversational Transformer Network for Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao ; Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jinming Zhao",
        "Ye Liu",
        "Bin Liu",
        "Jiangyan Yi",
        "Meng Wang",
        "Erik Cambria",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Modality Robustness, and Semi-Supervised Learning",
      "doi": "10.1109/TASLP.2021.3049898",
      "arxiv": "arXiv:2304.08981[cs.CL]"
    },
    {
      "citation_id": "19",
      "title": "Conversational Emotion Analysis via Attention Mechanisms",
      "authors": [
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Jian Huang"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2019-1577"
    },
    {
      "citation_id": "20",
      "title": "Luc Van Gool, and Radu Timofte. 2021. Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution",
      "authors": [
        "Jingyun Liang",
        "Guolei Sun",
        "Kai Zhang"
      ],
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV48922.2021.00406"
    },
    {
      "citation_id": "21",
      "title": "ScaleVLAD: Improving Multimodal Sentiment Analysis via Multi-Scale Fusion of Locally Descriptors",
      "authors": [
        "Huaishao Luo",
        "Lei Ji",
        "Yanyong Huang",
        "Bin Wang",
        "Shenggong Ji",
        "Tianrui Li"
      ],
      "year": "2021",
      "venue": "ScaleVLAD: Improving Multimodal Sentiment Analysis via Multi-Scale Fusion of Locally Descriptors",
      "arxiv": "arXiv:2112.01368[cs.CL]"
    },
    {
      "citation_id": "22",
      "title": "A Survey of Some Interdisciplinary Methods and Tools to Measure Learners' Emotions in Intelligent Tutoring Systems",
      "authors": [
        "Mona Hafez"
      ],
      "year": "2019",
      "venue": "2019 6th International Conference on Advanced Control Circuits and Systems (ACCS) and 2019 5th International Conference on New Paradigms in Electronics & information Technology (PEIT)",
      "doi": "10.1109/ACCS-PEIT48329.2019.9062885"
    },
    {
      "citation_id": "23",
      "title": "A review of emotion regulation in intelligent tutoring systems",
      "authors": [
        "Mehdi Malekzadeh",
        "Mumtaz Begum Mustafa",
        "Adel Lahsasna"
      ],
      "year": "2015",
      "venue": "Journal of Educational Technology & Society"
    },
    {
      "citation_id": "24",
      "title": "Audio-Visual Emotion Recognition in Video Clips",
      "authors": [
        "Fatemeh Noroozi",
        "Marina Marjanovic",
        "Angelina Njegus",
        "Sergio Escalera",
        "Gholamreza Anbarjafari"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2713783"
    },
    {
      "citation_id": "25",
      "title": "Facial Sentiment Analysis Using AI Techniques: State-of-the-Art, Taxonomies, and Challenges",
      "authors": [
        "Keyur Patel",
        "Dev Mehta",
        "Chinmay Mistry",
        "Rajesh Gupta",
        "Sudeep Tanwar",
        "Neeraj Kumar",
        "Mamoun Alazab"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2993803"
    },
    {
      "citation_id": "26",
      "title": "Spectral and Cepstral Audio Noise Reduction Techniques in Speech Emotion Recognition",
      "authors": [
        "Jouni Pohjalainen",
        "Fabien Fabien Ringeval",
        "Zixing Zhang",
        "Bj√∂rn Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM International Conference on Multimedia",
      "doi": "10.1145/2964284.2967306"
    },
    {
      "citation_id": "27",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "28",
      "title": "Multi-Modal Continuous Dimensional Emotion Recognition Using Recurrent Neural Network and Self-Attention Mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-Life Media Challenge and Workshop",
      "doi": "10.1145/3423327.3423672"
    },
    {
      "citation_id": "29",
      "title": "Processing affected speech within human machine interaction",
      "authors": [
        "Bogdan Vlasenko",
        "Andreas Wendemuth"
      ],
      "year": "2009",
      "venue": "10th Annual Conference of the International Speech Communication Association. ISCA"
    },
    {
      "citation_id": "30",
      "title": "Emotional Reaction Analysis Based on Multi-Label Graph Convolutional Networks and Dynamic Facial Expression Recognition Transformer",
      "authors": [
        "Kexin Wang",
        "Zheng Lian",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao",
        "Yin Fan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge",
      "doi": "10.1145/3551876.3554810"
    },
    {
      "citation_id": "31",
      "title": "Mobile Emotion Healthcare System Applying Sentiment analysis",
      "authors": [
        "Shu-Lin Wang",
        "I-En Chiang Honours",
        "Alex Kuo",
        "Jing-Ya Lin"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Big Data (Big Data)",
      "doi": "10.1109/BigData55660.2022.10021053"
    },
    {
      "citation_id": "32",
      "title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2022",
      "venue": "Speaker Verification and Spoken Language Understanding",
      "arxiv": "arXiv:2111.02735[cs.CL]"
    },
    {
      "citation_id": "33",
      "title": "Current State of Text Sentiment Analysis from Opinion to Emotion Mining",
      "authors": [
        "Ali Yadollahi",
        "Ameneh Gholipour Shahraki",
        "Osmar Zaiane"
      ],
      "year": "2017",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3057270"
    },
    {
      "citation_id": "34",
      "title": "Improving Automatic Speech Recognition Performance for Low-Resource Languages With Self-Supervised Models",
      "authors": [
        "Jing Zhao",
        "Wei-Qiang Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing",
      "doi": "10.1109/JSTSP.2022.3184480"
    },
    {
      "citation_id": "35",
      "title": "Former-DFER: Dynamic Facial Expression Recognition Transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia (MM '21)",
      "doi": "10.1145/3474085.3475292"
    },
    {
      "citation_id": "36",
      "title": "Information Fusion in Attention Networks Using Adaptive and Multi-Level Factorized Bilinear Pooling for Audio-Visual Emotion Recognition",
      "authors": [
        "Hengshun Zhou",
        "Jun Du",
        "Yuanyuan Zhang",
        "Qing Wang",
        "Qing-Feng Liu",
        "Chin-Hui Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc",
      "doi": "10.1109/TASLP.2021.3096037"
    },
    {
      "citation_id": "37",
      "title": "Exploring Emotion Features and Fusion Strategies for",
      "authors": [
        "Hengshun Zhou",
        "Debin Meng",
        "Yuanyuan Zhang",
        "Xiaojiang Peng",
        "Jun Du",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "Exploring Emotion Features and Fusion Strategies for"
    }
  ]
}