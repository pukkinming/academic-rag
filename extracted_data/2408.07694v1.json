{
  "paper_id": "2408.07694v1",
  "title": "End-To-End Semantic-Centric Video-Based Multimodal Affective Computing",
  "published": "2024-08-14T17:50:27Z",
  "authors": [
    "Ronghao Lin",
    "Ying Zeng",
    "Sijie Mai",
    "Haifeng Hu"
  ],
  "keywords": [
    "Multimodal representation learning",
    "Semanticcentric feature interaction and label generation",
    "Intra-and intersample contrastive learning",
    "Video-based affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities. For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention. However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth. Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks. To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos. We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information. Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multitask pseudo label generation, and intra-/inter-sample contrastive learning. Finally, SemanticMAC effectively learn specific-and shared-semantic representations in the guidance of semanticcentric labels. Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "M ULTIMODAL Affective Computing (MAC) aims at predicting the sentiment polarity, emotion class, or behavioral intention by comprehensively integrating information from different modalities of speakers such as textual (utterance), acoustic (human voice) and visual (facial expression, head movement, body gesture) modality in a humancentric video  [1] ,  [2] . With the surge of human-spoken content on social media platforms, research on multimodal affective computing has become crucial in the community of multimodal learning  [3] . Considering various application purposes, multimodal affective computing is divided into diverse specific tasks, including multimodal sentiment analysis  [4] -  [6] , multimodal emotion recognition  [7] -  [9] , multimodal humor and sarcasm detection  [10] ,  [11] .\n\nThis work was supported by the National Natural Science Foundation of China (62076262, 61673402, 61273270, 60802069). The authors are with the School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou 510006, China. (E-mail: {linrh7,zengy268,maisj}@mail2.sysu.edu.cn, huhaif@mail.sysu.edu.cn). Affective computing are originated from conventional Natural Language Processing (NLP) tasks referring to understanding the affection contained in human-spoken utterances and conversations  [1] ,  [12] ,  [13] . The performance of affectionrelated algorithms highly relies on semantic information  [14]  and are mostly improved by exploring the abundant semantic context embedded in language models. Nevertheless, immoderate reliance on language may easily overfit on subjective affective components, resulting in biased prediction  [15] ,  [16] . Thus, auxiliary features from other modalities, such as audio and image, are introduced to enhance affective understanding with multimodal learning  [3] . In previous MAC methods, unlike textual features learned by language models, acoustic and visual features are mostly extracted by manual preprocessing toolkit such as CMU-MultimodalSDK 1    [5] ,  [17] -  [20] , due to the information sparsity and inherent noise in audio and image. However, conducting multimodal learning with manual features may raise issues as shown in Figure  1 .\n\nOn the one hand, the vague description of pre-processing causes the extraction of manual features hard to reproduce  [6] , introducing inevitable gap between training and inference stages for multimodal learning. Besides, the manual feature extractors such as COVAREP  [21]  and Facet  [22]  are untrainable, which brings difficulty in developing end-to-end multimodal learning pipeline and affects the generalization of the pretrained models in various downstream scenarios.\n\nOn the other hand, due to the demand of semantic context for MAC task, the manual features such as facial landmarks for visual features and Mel-frequency cepstral coefficients for acoustic features, are not efficiently suitable for affectionrelated tasks. Lack of semantic information, such low-level features lead to poor embedding performance comparing with textual modality  [6] ,  [23] ,  [24]  and bring semantic imbalance issue in multimodal learning. Since the scale of language models is increasing rapidly, the number of trainable parameters for other modalities are much smaller than the ones for textual modality for MAC models, which further exacerbates the semantic imbalance for various modalities.\n\nTo better understand the issue of semantic imbalance for different modalities, we visualize the contribution of the unimodal features for the fusion multimodal representations in Figure  2 . Inspired by but diverse from  [25] ,  [26] , we compute the Precision-Recall (PR) curve for the feature distributions between the unimodal and multimodal representations, taking the representations from state-of-the-art multimodal sentiment analysis models  [17] ,  [23] ,  [27] -  [29]  as examples.\n\nAs shown in Figure  2 (a), we can observe that the manual acoustic and visual features contribute similarly when utilizing low-level textual features such as Glove  [30]  which computes word vector based on global word co-occurrence counts statistics. However, when we substitute the textual features with BERT  [31]  which embeds high-level semantic context by pretrained language model in Figure  2 (b)-2(f), the contributions of manual acoustic and visual features drop significantly to the multimodal representations compared with the one of textual features, no matter in which models. Although the existing fusion strategies  [5]  may adjust the contributions of different unimodal representations adaptively, they fail in balancing the contributions of different modalities, mostly due to the inherent discrepancy of semantic abundance from various unimodal representations.\n\nMoreover, we remove features from each modality input in traversal manner as MissModal  [29]  to construct unimodal, bimodal and trimodal representations, and then compute the PR-curve among the distributions of these representations as shown in Figure  2 (f). The bimodal representations with textual features contribute more than the representations with acoustic and visual features solely or both, which further indicates that the introduction of textual features can effectively increase the semantic information to the fusion multimodal representations.\n\nFrom the visualization in Figure  2 , we can conclude that existing low-level manual acoustic and visual features are no longer appropriate for high-level textual features embedded by context-based language model. The difference of semantic abundance from various modalities causes the issue of semantic imbalance and affects the multimodal fusion process, leading to an urgent need of new solutions for unimodal feature extraction of acoustic and visual modalities.\n\nIn addition, different modalities may bring diverse affective intensities or classes for MAC task  [15] ,  [19] ,  [32] , meaning that the affection semantics of various modalities may not remain consistent in the same video. Previous methods categorize unimodal features into modality-specific and -shared features to deal with such semantic inconsistency circumstance  [23] ,  [24] ,  [33] ,  [34] . However, they utilize final multimodal ground truth labels to jointly supervise representations learning, confusing the training of modality-specific features with different affection as shown in Figure  1 . We summarize this issue as semantic mismatch raised by inconsistent semantics among unimodal features and the corresponding multimodal ground truth labels. Moreover, as interpreted in Du et al. (f) MissModal  [29]  with BERT Fig.  2 . The PR curve of the fusion multimodal representations and the unimodal representations, including text, audio and vision modalities by stateof-the-art models training with Glove  [30]  and BERT  [31]  features on CMU-MOSEI dataset. Note that such PR curve is initially proposed as an evaluation metric for genrative models by Sajjadi et al.  [25]  to formulate the relative probability densities of the distributions of real and generated data.  [35] , multimodal joint training easily suffer from modality laziness, which makes the model neglect the learning of modality-specific features regardless of the paired features. Therefore, relying solely on annotated multimodal labels as the supervision is insufficient for multimodal learning  [27] .\n\nParticularly for MAC task, it is crucial to explore the unimodal semantics contained in various modalities and enhance nuanced comprehension of fine-grained affection in multimodal learning, ensuring more precise prediction without bias  [36] .\n\nThe case study in  manner. Firstly, we utilize powerful pre-trained Transformer models  [37]  instead of manual features to extract unimodal features of different modalities from the raw videos. Such pre-processing operation ensures the end-to-end training and inference of the multimodal learning model. In order to reduce the modality heterogeneity and generalize to various scenarios for MAC task, we unify the embedding form for diverse input modalities according to the temporal sequences. Inspired by the thought of positional embedding  [38] , we utilize learnable frame embedding to denote videos with different frame lengths, which enhances the performance when dealing with varying length human-spoken videos. To further collect taffective information, we design a module named affective perceiver to process the features into fixed number of learnable tokens in the latent space, meanwhile filtering the noisy content contained in the generic acoustic and visual features. Then, we conduct Semantic-centric Gated Feature Interaction (SGFI) inside and among the unimodal features from various modalities by bridge attention and gated mechanism to extract specific-and shared-semantic representations. The former representations explore the intra-modal dynamics for affection-specific knowledge and the latter ones integrates the cross-modal commonality by reducing the modality gap, both of which enhance model's ability of affective perception and multimodal reasoning. Next, targeting at training representations with various affective content, we present Semanticcentric Label Generation (SCLG) to calculate specific-and shared-semantic labels for each sample from multimodal ground truth in a momentum-updated policy. The generated pseudo labels are served as weak supervision signals to guide the learning of specific-and shared-semantic representations in a multi-task training paradigm, alleviating the semantic mismatch issue of multimodal joint training.\n\nBesides, diverse with conducting contrastive learning among paired modalities  [39] -  [41] , we perform Semantic-centric Contrastive Learning (SCCL) for various modalities from the perspectives of intra-and inter-sample. The introduction of semantics in contrastive learning significantly improves the convergence of representations for both unimodal and multimodal sub-tasks. Specifically, the intra-sample one measure the similarity of features from different modalities in each sample, encouraging cross-modal interaction with the guidance of specific and shared semantics. While the inter-sample one is presented under the guidance of the sentiment intensity or emotion classes of multimodal representations, enabling affection-related cooperation in the multimodal fusion. Lastly, we calculate the multi-task losses supervised by the groundtruth and generated paseudo labels and the contrastive learning losses as the final optimization objective.\n\nThe main contributions of our paper can be summarized as:\n\n• A unified and novel end-to-end framework for MAC: Focusing on the affective semantic of for textual, acoustic and visual modalities from the human-spoken video, we propose a novel framework named SemanticMAC to unify the learning process of multimodal representations and predict human's affective intensity in an end-toend manner for multimodal affective computing (MAC) task. Rather than manual toolkit in previous methods, we utilize the pre-trained Transformer model and design the affective perceiver to extract unimodal features, which enable flexible pre-processing with various length video and address the issue of semantic imbalance. • Semantic-centric representation learning approach:\n\nAccording to the specific semantics inside each modality and the shared semantics across diverse modalities, we present Semantic-centric Gated Feature Interaction (SGFI) to capture intra-and cross-modal dynamics. Meanwhile, we introduce Semantic-centric Label Generation (SCLG) to generate weak supervision for specificand shared-semantic representations respectively, which eases the semantic mismatch in the label space. We also conduct Semantic-centric Contrastive Learning (SCCL) to promote the interaction among modalities and across samples guided by semantics and affection information. • Achieving state-of-the-art performance: Extensive experiments demonstrates the effectiveness of our approach on 7 public datasets for 4 MAC downstream tasks universally, including multimodal sentiment analysis, multimodal emotion recognition, multimodal humor and sarcasm detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Multimodal Affective Computing",
      "text": "As a sub-field of multimodal learning, the key question of Multimodal Affective Computing (MAC) is summarized as how to extract semantic-rich unimodal features and effectively fuse the affective related information from each modality to learn multimodal representations  [2] ,  [42] ,  [43] . Therefore, developing pipeline to conduct MAC contains two main aspects: unimodal feature extraction and multimodal fusion  [1] ,  [3] .\n\nCompared with the traditional low-level hand-craft features  [21] ,  [44] ,  [45] , unimodal features extracted by deep learning based models have achieved impressive performance for diverse modalities when applied in different fields. Particularly, unimodal pre-trained models consist of Transformer  [37] , such as BERT  [31]  and GPT  [46]  for text in natural language processing, ViT  [47]  for image in computer vision, and HuBERT  [48]  for audio in speech processing, are capable of learning efficient unimodal representations and generalizing to various downstream tasks in the pre-train and fine-tuning paradigm. Additionally, multimodal fusion focuses on jointly integrating information from diverse modalities to perform affective prediction  [2] . Gkoumas et al.  [5]  and Geetha et al.  [9]  have provided comprehensive surveys on the current multimodal fusion techniques for MAC, which have attained remarkable results while still suffered from the huge modality gap and the issues of semantic imbalance and mismatch.\n\nThe MAC task consists of multiple affective prediction downstream tasks, including 1) multimodal sentiment analysis  [4] -  [6]  to compute a continuous score as the sentiment polarity of utterance in a regression method; 2) multimodal emotion recognition  [7] -  [9]  to classify the emotion class of the utterance in monologue or conversation; 3) multimodal humor and sarcasm detection  [10] ,  [11]  to identify whether the utterance contains the humorous or satirical intent. Previous methods address single task according to distinctive forms of input data and objective functions. Differently in this paper, we present one unifying framework to effectively adopt these downstream tasks, providing a unique insight for future research.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Attention Mechanism",
      "text": "Current multi-head attention mechanism is mostly based on Transformer  [37] , named self-attention, which presents normalized scaled dot-product among the input query, key and value from the same input sequence. Multiple variants of attention mechanism have been proposed to adapt in distinct scenarios, such as linear attention  [49]  to reduce the inference computation from quadratic complexity into linear one, crossattention  [50]  to process different input, and mutli-query attention  [51]  to decrease the model parameters and the key/value cache and so on.\n\nMultimodal learning with attention mechanism has been exploited extensively in previous researches  [52] . Whisper  [53]  trains a robust speech recognition model by cross-attention with a large-scale web text-audio data as a weakly supervised datasets in a multi-task training approach. Flamingo  [54]  presents perceiver resampler to convert varying-size large feature maps to fixed visual tokens and interact these tokens with textual data by masked attention layers in the visionlanguage pre-training. However, most of them leverage the attention mechanism to construct cross-modal connection instantly, ignoring the different interaction manners and various supervision of the fine-grained features in the semantic space.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Contrastive Learning",
      "text": "Contrastive learning focuses on dividing the samples into positive and negative pairs sets and adjusting the similarity of the corresponding representations  [55] . The most popular form of contrastive loss function is InfoNCE, which is utilized to encode underlying shared latents by maximizing the lower bound of the mutual information  [56] . As a pretext task, contrastive learning is initially adopted at unimodal models in an unsupervised manner  [57] -  [59] , and then extended to supervised methods and multimodal models due to its great effectiveness and generality. Supcon  [60]  leverages label information to conduct contrastive learning in a fully-supervised setting. Recent works such as CLIP  [39] , ALIGN  [61]  and wav2vec2.0  [62]  and so on have claimed the better crossmodal alignment performance with contrastive objectives. Particularly, ImageBind  [63]  extend contrastive learning into the joint embedding space across six modalities. Nevertheless, most of them lack exploration into multimodal fusion and reveal significant modality gap  [64] , which is imperatively needed to be addressed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The proposed SemanticMAC is presented in detail in this section. We first define the input and output of MAC task and clarify the corresponding notations. Then we introduce the end-to-end architecture of the proposed framework. Next, we put forward the extraction process of unimodal features for different modalities. Following the semantic-centric thought, the module of gated feature interaction and the strategy of label generation are described additionally. Finally, we formulate the total optimization objective with the semantic-centric contrastive learning loss and the individual task prediction losses.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Definition",
      "text": "Multimodal Affective Computing (MAC) concentrates on learning efficient representations to conduct various regression or classification tasks for affective analysis from the multimodal signals contained in a human-spoken video. To unify diverse downstream MAC tasks, we formulate the multimodal input of the raw videos as I u ∈ R ℓu×cu , where ℓ u denotes the temporal length of utterance sequence and c u denotes various contents of unimodal signal at the sampled timestep of the video. Particularly, since each video clip contains at least an utterance spoken by one human with facial expression, head movement and body gesture, u ∈ {t, a, v} represents the textual, acoustic and visual modalities respectively  [3] . According to the semantics contained in various modalities, the proposed SemanticMAC processes each modality of the raw multimodal data to unimodal representations F u and then integrates the affective information into multimodal representations F M by",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "L Nce",
      "text": "CrossEntropy / L2 Loss pull push push push cross-modal interaction in the semantic space. Lastly, the task predictor utilize the final mutlimodal representations to output ŷM , which serves as the sentiment scores in regression task or as the emotion classes in recognition and detection task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Architecture Overview",
      "text": "The architecture of the proposed SemanticMAC processing raw video in an end-to-end manner is depicted as Figure  3 . Aiming at avoiding the issue of semantic imbalance from root, we firstly take pre-trained Transformer models instead of manual toolkits to pre-process unimodal data into embeddings with consistent form for various modalities. The unimodal embeddings X u are multiple tokens in R fu×du where fu denotes the numbers of tokens and d u denotes the representation dimension of modality u ∈ {t, a, v}. Then, for acoustic and visual modalities, we design the affective perceiver to integrate affective information contained in the generic unimodal embeddings and transfer the knowledge into multiple learnable tokens F a and F v with fixed length, which enable the flexible handling of videos with different lengths at the same time. Besides, the acoustic and visual features are summed with learnable frame embedding E f r to attend by relative temporal order of various frames of the video. While for textual modality, the pre-trained language model is utilized to learn the affective textual representation F t . Note that we freeze the pre-processed encoders for acoustic and visual modalities and the tokenizer for textual modality during the training stage while update the parameters of affective perceivers and language model as fine-tuning paradigm. Next, to make the model distinguish different modalities in latter modules, the unimodal representations F u are attached with learnable modality embeddings E md according to the corre-sponding type of modality. In addition, we conduct semanticcentric feature interaction among various unimodal representations to learn semantic-specific representations (F t sp , F a sp , F v sp ) and semantic-shared representations (F t sh , F a sh , F v sh ) for each modality, which further address semantic imbalance issue induced by overfitting on the dominant modality. Specifically, we design the interaction mechanism as gated multihead intra-and cross-attention to efficiently capture intramodal dynamics and explore cross-modal commonality. To focus on the learning of query modality at one time, we measure the attention score with multi-query setting in each interaction. Additionally, to reduce the impact of the modality gap introduced by modality heterogeneity, we utilize a set of bridge tokens to interact the information between query and key modalities with massively diverse distributions. Lastly, we concatenate the semantic-specific representations (F t sp , F a sp , F v sp ) and semantic-shared representations F sh and then project them into multimodal representations F M , which are fed to the task predictor to output the final affective prediction ŷM .\n\nTargeting at the issue of semantic mismatch raised by various contents of each modality, we tend to utilize different semantic-centric labels as the supervision for different features in a multi-task training manner, which competently guides the learning of unimodal and multimodal representations in the semantic space. According to semantic attributes, we divide the training of representations into five sub-tasks denoted as * ∈ {M, S, T, A, V }, including the sub-tasks of multimodal representations (M ), semantic-shared representations (S) and semantic-specific representations for each modality (T, A, V ).\n\nMost datasets only manually annotate the multimodal groundtruth labels y gt for multimodal representations F M in sub-task\n\n,  [36] . Due to this, we consider to generate pseudo labels p * according to the fine-grained level of semantics for other representations (F t sp , F a sp , F v sp , F sh ) as a weaklysupervised strategy compared with the ground-truth annotations. By calculating the similarity ranking matrix for each type of representation in the feature space, the pseudo labels are then generated by scaling and shifting the corresponding ground-truth labels of k-nearest neighborhood samples. Besides, we stabilize the generation process of the pseudo labels in a momentum-based updating policy as the training epoch increases. Supervised by the ground truth labels and pseudo labels, the multi-task predictors take the unimodal and multimodal representations as the input and output the affective prediction ŷ * for each sub-task. Moreover, we perform semanticcentric contrastive learning in the level of intra-sample and inter-sample at the unit hypersphere  [65]  to further enhance the convergence of multimodal representations learning. The former one pulls closer the semantic-shared representations of all modalities inside the same video sample and pushes away the semantic-specific representations for each modality, which encourage the decoupling of semantic information for unimodal features. While the latter one constructs positive and negative pairs based on the ground-truth affective category for the multimodal representations among different samples. More technical details are introduced in the following subsections.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Unimodal Feature Extraction",
      "text": "To unify the pre-processing of various modalities, we adopt Transformer-based models to extract unimodal features. As shown in Figure  3 , the text data I t are firstly processed to tokens X t by tokenizer according to the specific language models  [31] ,  [66] ,  [67]  in particular downstream task. Note that our framework is suitable for various language model, which is latter validated in the experiments. Then, we utilize the pre-trained language model to learn the textual representations F t , which is formulated as:\n\nThe pre-trained language model are set in a fine-tuning paradigm where the parameters θ t are updated during the training stage.\n\nWhile for the audio data I a and video data I v , where the general upper limit of the micro-expression duration is observed as 1/2 seconds  [68] , we uniformly sample the audio and vision stream at 2 frames per second, efficiently reducing the input data volume and the model inference time. Next, the sampled audio and video frames are directly fed into the frozen pre-trained encoders of ImageBind  [63]  to jointly learn acoustic embedding X a and visual embedding X v , which stack all [CLS] tokens from each sampled frame of the stream, formulated as:\n\nwhere θ u denotes the encoders parameters and the [CLS] token are taken as the global embedding to aggregate the contents of each frame  [39] ,  [63] ,  [69] . Note that we leverage the power of ImageBind for its excellent performance in aligning different multimodal data in the joint embedding space  [70] .\n\nAlthough ImageBind has been proved effectively in multimodal alignment, the extracted unimodal embeddings are coarse-grained and generic in the embedding space, containing massive task-unrelated noise and affective-irrelevant information. Besides, directly utilize [CLS] embeddings as the unimodal representations cause the model lack of temporal interaction for each modality. Thus, we design an extra module named Affective Perceiver to further learn fine-grained unimodal features and explore affective dynamics by interacting the [CLS] embeddings across frames as shown in Figure  4 . Affective Perceiver For acoustic and visual modalities, given video stream with fu frames, the unimodal embeddings extracted by the corresponding unimodal encoders of ImageBind are represented as X f u , where u ∈ {a, v} and f ∈ [1, fu ]. Firstly, as the positional embedding in language model  [31] , we sum the unimodal embeddings X u with learnable frame embeddings E f r ∈ R d to increase relative temporal order to the module when conducting cross-frame attention, represented as:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "Then, we innovate unimodal learnable tokens L u ∈ R n×du with fixed length n for individual modality aiming at collecting affective features in the generic unimodal embeddings. Due to the excellent performance of attention mechanism  [37] ,\n\nwe design a multi-layer Transformer-based module named as Affective Perceiver by adopting multi-head attention (MHA) and feed forward network (FFN) in each layer. For u ∈ {a, v}, the Affective Perceiver gradually encourages the affective information of the unimodal embeddings X u to flow to the learnable tokens L u . By constructing query, key and value as\n\nthe computations of each Affective Perceiver layer are formulated as follows:\n\nwhere layer normalization (LN) and residual connections are employed around each of the sub-layers. Note that L u is initialized randomly and the output of the last layer is taken as the unimodal representations F u ∈ R n×du . The effectiveness of such fixed number of leanrbale tokens in content abstraction for various modalities have been proved in recent researches  [54] ,  [71] ,  [72] . Moreover, the unimodal learnable tokens L u in the Affective Perceiver can not only integrate the most useful information for downstream tasks while removing irrelevant noise, but empower the model with the ability to align acoustic and visual modalities with different language model in the feature space. With the introduction of Affective Perceiver, the proposed framework is capable of processing video with various frame length and extracting affective unimodal features competently, which further address the issue of semantic imbalance as shown in Figure  1 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Semantic-Centric Feature Interaction",
      "text": "After the extraction of unimodal features, the essential question of multimodal learning has become how to interact various type of information from different modalities and conduct multimodal fusion with huge modality gap. In the perspective of semantic, we decouple the feature space into semantic-specific and semantic-shared features, where the former features focus on modal-specific semantic information according to the contents of diverse modalities while the latter ones integrates the invariant commonalities among all modalities. Such feature disentanglement strategy is intuitive and works successfully with theoretical interpretability  [23] ,  [34] ,  [73] . Diverse from previous researches, we propose Semanticcentric Gated Feature Interaction (SGFI) to learn semanticspecific and -shared representations by the designed bridge attention and gated mechanism, which effectively transfer intra-and cross-modal knowledge through bridge tokens and filter the irrelevant features by weighted activation layer.\n\nAs shown in Figure  5 , inspired by VilT  [38] , each unimodal representation F u is firstly summed with a learnable modality embedding E md ∈ R d , which indicates the modality type for the module to distinguish corresponding representation in latter interaction, which is formulated as:\n\nSimilar as self-attention  [37] , cross-attention has been proved competent in aligning different input data as query and key/value, respectively  [74] -  [76] . However, due to the huge modality gap and delicate modality relationship, the interaction Fkv= Fu ～ ～\n\n.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Modality Embedding",
      "text": "Semantic-centric Gated Feature Interaction p u ( ) / h u ( )\n\none-head proj.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Adaptiveavgpool1D Scale Down",
      "text": "Bridge Tokens\n\nCross-Attention Cross-Attention in multimodal fusion for multimodal affective computing is far more complicated than simply multimodal alignment  [5] . Therefore, we improve the cross-attention mechanism in three ways for SGFI module, named Gated Bridge Attention (GBA), to adapt at the complex multimodal fusion:\n\n1) Multi-query Attention: We adopt multi-query attention  [51] ,  [77]  to primarily excavate various semantics inside query vectors, which accelerate the convergence of mutlimodal learning and lower the memory-bandwidth requirements concurrently. Specifically, we utilize multihead projection W x h for query vectors while maintain a single head of key/value vectors which share the same weights in the linear projection W y for each head of query vectors, formulated as:\n\nwhere head denotes the number of heads, d head = d c /head denotes the dimension of each head and d c is set as the common dimension for each representation. 2) Bridge Token: Aiming at bridging the modality gap among various modalities in the semantic space and conducting efficient feature interaction, we introduce Bridge Tokens with fixed m tokens (m < n) as bottleneck to restrict the intra-and cross-attention flow, inspired by the thought of information bottleneck  [78] -  [80] . The Bridge Tokens B are obtained by aggregating features in adaptive average pooling based on semantics from query vectors:\n\nThen, scaling down by √ d c , the attention matrix is computed as:\n\n3) Gated ReLU: To filter the redundancy according to the semantic of individual representations, we adopt the gated mechanism between each attention and feed forward sublayer by Rectified Linear Unit (ReLU)  [81] , which has been proved suitable for Transformer models due to its activation sparsity and inference efficiency  [82] ,  [83] . Thus, for u ∈ {t, a, v}, the computation in GBA is finally formulated as:\n\nThe SGFI module is conducted by stacking multiple GBA attention layers and outputs semantic-centric representations according to the input modality, which are denoted as p u (•) for semantic-specific feature interaction and h u (•) for semanticshared feature interaction.\n\nOn the one hand, to capture intra-modal dynamics and filter affective-unrelated noise, we take unimodal representations from the same modality to construct the input query and key/value for the SGFI module, which are denoted as Q = K = V = F u . Then, the semantic-specific representations F u sp can be computed as:\n\nOn the other hand, to effectively fuse knowledge among different modalities and incorporate the affective commonalities, given the input query as Q = F u from arbitrary modality, we set the key/value as\n\nwhich is the concatenation of the other unimodal representations. Thus, the semantic-shared representations F sh are formulated as:\n\nFinally, to summarize the semantic-specific and -shared information from various modalities, the multimodal representations F M are formulated as:",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Semantic-Centric Label Generation",
      "text": "For the learning of various semantic-specific representations F u sp and semantic-shared representations F sh , the supervision should be produced according to the semantics information. However, due to the absence of unimodal labels in most datasets, most of previous work  [23] ,  [34]  directly utilize the multimodal ground truth labels to supervise the learning process of features with various semantic, which are essentially contrary with the thought of disentangled representation learning. Besides, the affection expressed through single modality can be quite diverse, which is concluded as the semantic mismatch issue as shown in Figure  1 . Aiming at addressing this issue, we present Semantic-centric Label Generation (SCLG) to construct pseudo label space based on semantics as the weak supervision strategy to improve the learning of semantic-centric representations.\n\nSpecifically, we deem the learning processes of representations F * with various semantics as distinct sub-tasks * ∈ {S, T, A, V }, which denote the sub-task of semantic-shared representations F sh and semantic-specific representations F u sp for textual, audio and visual modalities. Each subtask should be trained under the guidance of the corresponding semnaticcentric pseudo labels. Inspired by Yu et.al  [27] , the semanticcentric labels p * are assumed to share the distribution space with multimodal ground truth labels y gt . Thus, we utilize the common semantics contained in the representations across various samples and their ground truth labels to generate the pseudo specific-and shared-semantic labels as shown in Figure  3 .\n\nGiven a query of representations B = {F i * } B i=1 , we conduct k-Nearest Neighbor (k-NN) algorithm to find the K most nearest samples {F k * } K k=1 (K < B) for each representation F i * by comparing the similarity in the feature space and then output the euclidean distance matrix D * between each sample and the nearest samples, denoted as:\n\n) where the dimension of the representations d c is utilized as a scaling factor to mitigate the adverse effect of excessive distance. The distance matrix D * represents the similarity of various representations, indicating the relationship among different samples at the level of specific or shared semantics.\n\nFor each sub-task, to transfer the knowledge of multimodal ground truth labels y gt to the semantic-centric pseudo labels p * , we design a scaling map to control the transferring magnitude related to semantics abundance and a shifting map to decide the direction and value of the label movement ∆. Therefore, we intuitively construct pseudo labels by considering the distance matrix D * in the Gaussian potential kernel form function  [84]  as the scaling map, and the difference between multimodal labels y k gt and p * of the corresponding nearest samples as the shifting map, which is formulated as:\n\nwhere ∆ i * denotes the varying value for pseudo label p i * of sample i in sub-task * . Moreover, the pseudo labels are initialized as the corresponding multimodal ground truth labels and updated in a momentum manner by combining the computed movement and history values with the increasing of training epochs z, represented as:\n\nwhere the momentum-based updating policy is intended to relieve the fluctuations caused by noisy samples and the updating process of pseudo labels is started after the rth epoch for more stable label generation and better convergence.\n\nFor each subtask, the specific-semantic labels are generated to reveal the intra-modal connections among different samples while the shared-semantic labels are expected to show the inter-modal commonality. Comparing with the multimodal ground truth labels, the generated pseudo labels are used to guide the learning of various semantic-centric representations in a weakly-supervised manner. Note that the semantic-centric pseudo labels are allowed to be zero for individual samples when there are few unimodal features or rare paired features related to the downstream prediction  [14] ,  [35] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Semantic-Centric Contrastive Learning",
      "text": "To promote the disentanglement of semantics and encourage the feature interaction of unimodal and multimodal sub-tasks, we conduct Semantic-centric Contrastive Learning (SCCL) among various modalities inside and across different samples. Previous works  [39] ,  [85]  utilizes cross-modal contrastive learning directly on unimodal representations suffering from the huge modality gap  [64] . Diversely, SCCL is presented in the perspective of intra-and inter-sample at the semantic space, where the modality gap has been mitigated by SGFI.\n\nAs suggested by Wang et al.  [65] , we firstly employ L2-normalization on all representations for both intra-and inter-sample contrastive learning to restrict the contrastive learning space on the unit hypersphere, as shown in Figure  3 . We implement the intra-sample contrastive learning among semantic-specific representations F u sp and semantic-shared representations F sh for u ∈ {t, a, v}, which efficiently decouple the semantic-specific features and the consistent information contained in each modality u. Given {F u sp , F u sh } from each sample i, the goals are pushing away F u sp and pulling closer F u sh from different modalities, and pushing apart F u sp and F u sh according to the semantics inside the corresponding representations. Thus, we construct positive pairs as {F u sh ; F u ′ sh }, and the negative pairs as {F u sp ; F u ′ sp } and {F u sp ; F u ′ sh }, and adopt dot-product similarity between the query and key in the pairs, formulated as:\n\nwhere τ serves as a temperature hyper-parameter for altering the strength of penalties on hard samples due to the modality gap  [86] . Then we take InfoNCE  [56]  form function to compute the intra-sample contrastive learning loss L intraCL , which is formulated as:\n\nSimultaneously, the inter-sample contrastive learning is adopted for the multimodal representations F M among diverse samples under the supervision of multimodal ground truth labels to further excavate the affective information inspired by SupCon  [60] . Given a mini-batch of B = {F i M } B i=1 , we divide the representations into positive and negative sets according to the labels annotated as sentiment scores or emotion classes. For sentiment analysis task, we categorize the representations based on sentiment classes with a label threshold which decide the class each sentiment scores belongs to. While for emotion recognition and detection classes, we treat the representations with the same class as the positive pairs while the other representations as the negative pairs. Note that such setting is suitable for multi-label emotion recognition dataset  [4] , where we treat the representations with non-empty intersection set of emotion annotations as the positive pairs. To make the representations from various classes more discriminative with the guidance of multimodal labels, denoting positive pairs sets as P ∈ {F j M }, the inter-sample contrastive learning loss L interCL is computed as:\n\nwhere υ is another temperature hyper-parameter to regulate the probability distribution over diverse instance samples  [87] .\n\nCombining the intra-and inter-sample contrastive learning losses, the final semantic-centric contrastive learning loss L CL is computed as:\n\nwhere α and β are hyper-parameters to adjust the contribution of each loss in the semantic-centric contrastive learning.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "G. Optimization Objective",
      "text": "Regarding the learning of multimodal and semantic-centric representations as multi-task training paradigm where subtask * ∈ {M, S, T, A, V }, we utilize various multi-layer perceptron (MLPs) as the mutli-task predictors to output the corresponding affective predictions, formulated as:\n\nwhere r = 1 denotes the sentiment scores as for sentiment analysis, r = class denotes the number of emotion classes for recognition and r = 2 denotes the binary classification for detection task.\n\nAlong with the guidance of multimodal ground truth labels y gt and the supervision of the generated semantic-centic pseudo labels p * for each sub-task, the task prediction loss is formulated as:\n\nwhere y denotes the ground truth y gt for multimodal subtask and pseudo labels p * for the sub-tasks of semantic-centric representations. For multimodal sentiment analysis task, we utilize L2 Loss for the regression of sentiment scores; for multimodal emotion recognition and multimodal humor and sarcasm detection tasks, we adopt CrossEntropy Loss for the classification of emotion or binary classes. Lastly, the total optimization objective is formulated as:\n\nIV. EXPERIMENTS\n\nA. Tasks and Datasets 1) Multimodal sentiment analysis: CMU-MOSI  [89]  contains 2,199 monologue utterances clipped from 93 opinion videos spoken by 89 YouTube movie reviewers, which is annotated with a continuous sentiment score from -3 (strongly negative) to +3 (strongly positive).\n\nCMU-MOSEI  [4]  expands the size of dataset into 20k video clips segmented from 3,228 videos with 250 diverse topics collected by 1,000 distinct YouTube speakers, each of which is annotated for the sentiment on a [-3, +3] Likert scale.\n\nCH-SIMS  [36]  collects 2,281 segments from 60 videos in different movies, TV serials, and variety shows with spontaneous expressions, various head poses, occlusions, and illuminations performed by 474 distinct speakers in Chinese. While CH-SIMS v2  [90]  doubles the scale of dataset by introducing more supervised and unsupervised instances with the same annotation method, where we only utilize the supervised ones in our experiments for fair comparision.\n\n2) Multimodal emotion recognition: CMU-MOSEI  [4]  annotates the utterance of each video into multiple emotional labels from {happy, sad, angry, surprise, disgust, f ear} as the settings of Ekman emotion classes  [91] .\n\nIEMOCAP  [7]  provides 12 hours videos with two-way dialogues performed by 10 actors annotated into 6 classes {happy, sad, neutral, angry, excited, f rustrated}.\n\nMELD  [8]  consists of about 13K utterances from 1,433 multi-party conversations from the TV-series Friends, categories the emotion classes into 7 universal classes {neutral, surprise, f ear, sadness, joy, disgust, angry}\n\n3) Multimodal humor and sarcasm detection: UR-FUNNY  [10]  comprises nearly 10K TED talk videos across 417 topics given by 1,741 different speakers, providing target punchline and the preceding context with even number of humor and non-humor instances.\n\nMUStARD  [11]  incorporates 690 videos containing target utterance along with associated historical dialogue, which are collected from famous TV shows including Friends, The Big Bang Theory, The Golden Girls and Sarcasmaholics, manually annotated with balanced numbers for the sarcasm property.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "We use public metrics of regression, recognition and detection task to evaluate the performance of the proposed Seman-ticMAC framework and conduct fair comparison with baselines: For regression, seven-class/five-class/three-class classification accuracy (Acc7/Acc5/Acc3) indicating the correct sentiment label predictions in the label range; binary classification accuracy (Acc2) and F1-score are calculated with settings of positive and negative; mean absolute error (MAE) computing the average absolute difference between the final prediction and ground truth labels; Pearson correlation (Corr) measuring the degree of prediction skew. For recognition and detection, weighted accuracy (w-Acc) and F1-score (w-F1) along with weighted precision (w-Precision) and recall (w-Recall) score are computed according to the relative frequency of individual class; besides, standard accuracy (s-Acc), negative-weighted accuracy (n-Acc) and binary F1-score (b-F1) are reported according to dataset properties  [19] ,  [92] ,  [93] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Implementation Details",
      "text": "All experiments are conducted on a single A100 GPU with CUDA 11.8. For each dataset, we convert the raw video into LMDB database for higher access speed in the end-to-end training and inference stage as Lei et al.  [101] . Note that for fair comparison with baselines, we remain the same language model with the state-of-the-art models for each MAC task. Following Gkoumas et al.  [5] , we present fifty-times random grid search to find the best hyper-parameters and we report the average results of 5 runs as the final performance. The splits of dataset and the settings of hyper-parameters are shown in Table  II . We adopt AdamW  [102]  as the optimizer and utilize a warmup strategy for all learning rates at the first epoch. For regression task, we utilize the minimum loss of validation set in the training stage as the reference to get the best parameters, while for recognition and detection tasks, we utilize w-F1 score of validation set as the one to determine the best model due to the confidence calibration issue  [103] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Baselines",
      "text": "We report the results of baseline models by reproducing the corresponding open-source codes without extra mention. The baseline models are broadly categorized into: (1) Early and late fusion: EF-LSTM, LF-LSTM, LF-Transformer; (2) Tensor-based fusion models: TFN  [15] , LMF  [96] ; (3) Explicitly intra-and inter-modal dynamics manipulation models: MFN  [97] , MFM  [98] , C-MFN  [10] , EmoEmbs  [19] ; (4) Attention-based fusion models: MulT  [17] , MISA  [23] , MAG-BERT/MAG-XLNet  [99] , TBJE  [104] , FE2E/MESM  [92] , ME2ET  [105] , I-Attention  [106] , BBFN  [107] , MuLoT  [108] ; (5) Knowledge guidance models: Self-MM  [27] , MTMD  [24] ; (6) Contrastive learning based models: MMIM  [28] , MMCL  [100] ;  (7)  Graph neural network based models: Graph-MFN  [4] , DialogueGCN  [109] , MMGCN  [110] , COGMEN  [111] , CORECT  [112] ;  (8)  Context aware models: bc-LSTM  [113] , CMN  [114] , ICON  [115] , DialogueRNN  [116] , DialogueCRN  [117] , Multilogue-Net  [118] ; (9) Data augmentation models: AV-MC  [90] , MissModal  [29] .   [92]  AND  [105] .     [92]  AND  [105] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "E. Experiment Results",
      "text": "For multimodal sentiment analysis task, as shown in Table  III , SemanticMAC reaches the state-of-the-art performance when utilizing BERT as the same language model as the baselines. Besides, comparing the improvement over the best baseline models on CMU-MOSI and CMU-MOSEI datasets, higher performance gains can be obtained by training Seman-ticMAC with a larger scale of dataset. Moreover, the proposed architecture can be further generalized to other language model such as XLNet without modifications, which achieves more superior performance on all metrics. Additionally, as shown in Table  IV , since CH-SIMS (v2) dataset is collected in Chinese environment, the results demonstrate that the visual and audio features extracted by ImageBind and learned by the proposed Affective Perceiver can be effectively utilized to boost the performance of language model in multilingual settings. We reckon that this is mainly attributed to the fact that the affective information contained in vision and audio data is mostly independent with specific language.\n\nFor multimodal emotion recognition task, as shown in Table  V  -VIII, we compare SemanticMAC with the baselines in both conversation and utterance settings, where the former means feeding all context inside the dialogue into the models while the latter denotes utilizing the single target utterance as input. Note that on both specific emotion class and average accuracy, SemanticMAC mostly outperforms the baseline models no matter training on CMU-MOSEI, IEMOCAP or MELD datasets without any usage of speaker information. Meanwhile, SemanticMAC surpasses recent graph-based models  [109] , [111],  [112]  and context-aware models  [116] -  [118]  in needless of graph neural networks or delicate context-aware module which have been proved effective in constructing the complicated emotion relationships of utterances in conversation. The reason is that as the weak supervision in the training of unimodal representations, semantic-centric label succeeds in integrating emotion-related semantics and tackling the semantic mismatch among representations with various emotion classes.\n\nFor multimodal humor and sarcasm detection task, as shown in Table  IX , compared with previous models relying on manual extracted features provided by the original datasets, SemanticMAC detects the intention of the target utterance more accurately for humor and sarcasm with BERT on UR-FUNNY and MUStARD datasets. When training with RoBERTa, SemanticMAC reaches higher performance and defeats the baseline with larger language model  [119] . The  advanced results indicate the efficiency of SemanticMAC in capturing the contradictory correlation among the punchline and context to predict the humorous and sarcastic anchors.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "F. Ablation Study",
      "text": "To further reveal the contributions of different modules inside the proposed architecture, we perform ablation study for SemanticMAC on CMU-MOSEI dataset as shown in Table  X . Firstly, when capturing intra-and inter-modal dynamics in Affective Perceriver and SGFI, the learnable frame embeddings E f r and the modality embeddings E md are productive in assigning temporal information for both audio and vision modalities and revealing the type of modality in multimodal fusion. Moreover, multi-query attention and Bridge Tokens B are effective in decreasing the modality gap and exploiting the common semantics when conducting cross-modal attention in feature interaction. The gated ReLU succeeds in filtering unrelated noise, leading to performance decrease on Acc7 lacking of the gated mechanism.\n\nAdditionally, SemanticMAC adopts semantic-centric labels p * in SCLG to guide the learning of multiple sub-tasks. Therefore, when replacing pseudo labels p * with the ground truth labels y gt in the learning of semantic-specific and semanticshared representations F u sp and F sh , the model suffer from the issue of semantic mismatch, leading to huge performance  resentations according to semantics and multimodal ground truth, so that optimize without contrastive learning largely hurt the final performance.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "V. Further Analysis A. Effect In Tackling Semantic Imbalance",
      "text": "Aiming at addressing the issue of semantic imablance shown in Figure  2 , we visualize the Precision-Recall curve of SemanticMAC with BERT on CMU-MOSEI to reveal the semantic abundance of various unimodal representations as shown in the Figure  6(a) . By replacing the manual acoustic and visual features with ImageBind in SemanticMAC, the contribution of unimodal features from different modalities are well balanced in the final multimodal representations. Besides, Affective Perceiver competently integrates the affective information into learnable unimodal tokens, increasing the semantic abundance of acoustic and visual representations. As the endto-end training processes, the issue of semantic imbalance can be practically tackled in the stage of multimodal fusion, which further demonstrates the effectiveness of the proposed architecture.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Visualization In The Embedding Space",
      "text": "To better reveal the distribution of the semantic-specific features, we utilize T-SNE  [121]  to visualize representations trained on CMU-MOSEI in the embedding space. As shown in Figure  6 (a), for u ∈ {t, a, v}, the semantic-specific representations F u sp are discriminative according to the semantic-centric labels, regardless of the observable modality gap. Meanwhile, the representations with consistent sentiment from various modalities are well classified into the opposite ends of the embedding space, indicating the productivity of semanticcentric cross-modal feature interaction.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C. Distribution Of Semantic-Centric Labels",
      "text": "We present case study to concretize the issue of semantic mismatch as shown in Table  I . For sentiment analysis (#1-#3), the intensity of generated pseudo labels p * for various modalities are quite different, even contradictory due to the exclusive content contained in semantic-specific representations. The similar trend can be observed when classifying emotion class for unimodal and multimodal representations in the examples (#4-#6) of multimodal emotion recognition. Besides, the semantic-centric labels are discrepant from the value or class of ground truth labels y gt , implicitly validating that the pseudo labels are productive in capturing the semantics information contained in diverse representations.\n\nTo further verify the effectiveness of semantic-centric label in tackling semantic mismatch, we visualize the generation process of semantic-centric labels during training on sub-tasks * ∈ {S, T, A, V } in Figure  6 . Notes that the multimodal labels of the 1 st epoch denotes the ground truth labels. As the training stage proceeds, the distributions of semantic-centric labels for diverse semantic-specific and -shared representations vary in distinctive ways, demonstrating the pseudo labels can be generated according to different semantic-centric representations. For multimodal sentiment analysis in Figure  6 (b), the pseudo labels polarize with more discriminative sentiment tendency, where more samples are assigned with positive or negative intensity. For multimodal emotion recognition in Figure  6 (c), the frequency of emotion classes in semanticcentric labels are rearranged by the affective semantics in various modalities. Besides, for the emotion classes such as happy and sad which can be expressed explicitly in language, the pseudo labels are more frequently arranged for textual modality. While for the emotion more likely to be revealed in expressive face or tune such as surprise and fear, the pseudo labels are more presented in audio and vision modalities.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "D. Unimodal Feature Performance Comparison",
      "text": "Aiming at validating the effectiveness of acoustic and visual features learned by ImageBind and Affective Perceiver, we conduct experiments on X u and F u in the architecture by linear probing  [122]  for the sole audio and vision modality u ∈ {a, v} on CMU-MOSEI. Note that the prediction of unimodal features is attained through conducting average pooling and linear projection trained on the learned and manual features with other parameters frozen. As shown in Table  XI , compared with manually extracted features, both of unimodal embeddings X u and representations F u achieve superior performance on both regression and classification metrics. The performance of X u reveals the multimodal alignment and generalization power of ImageBind. Additionally, Affective Perceiver productively filters the noise of X u and integrates the affective information in F u , leading to higher performance.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "E. Influence Of Fixed And Varying Video Length",
      "text": "To demonstrate the effectiveness of SemanticMAC in processing videos with various length, we conduct experiments in the settings of both fixed and various frames (in both audio and vision streams) on CMU-MOSEI, which has a wide range of video lengths from 0.7 s to 108.9 s  [4] . As shown in XII, the model trained on videos with various frames outperform the ones trained on videos with fixed frames. Besides, either too few or too much frames are not beneficial for the information extraction of Affective Perceiver or the feature interaction among different modalities, remaining consistent trend with sparse to dense uniform sampling  [101] . This indicates the importance of balancing the information redundancy and semantic abundance for the performance of affective computing model. Therefore, the ability of handling videos with various length results in higher robustness and applicability when adopting SemanticMAC in diverse downstream scenarios.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we proposed a novel end-to-end multimodal affective computing framework, SemanticMAC, to effectively learn semantic-specific and -shared representations with the supervision of the generated semantic-centric labels. Extensive experiments on 7 public video-based datasets in 4 downstream MAC tasks demonstrate the effectiveness of the proposed approach. The visualization and ablation study consistently reveals that SemanticMAC productively tackles the challenges of semantic imbalance and semantic mismatch for various modalities.\n\nIn the future, we will utilize recent emerging large language models to promote higher performance of the proposed method, since SemnaticMAC has been verified universally across different language models. Moreover, we tend to extend the end-to-end pipeline for multimodal affective computing in more downstream applications of human-AI interaction.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The two main challenges in conducting multimodal affective comput-",
      "page": 1
    },
    {
      "caption": "Figure 1: On the one hand, the vague description of pre-processing",
      "page": 1
    },
    {
      "caption": "Figure 2: Inspired by but diverse from [25], [26], we compute",
      "page": 2
    },
    {
      "caption": "Figure 2: (a), we can observe that the manual",
      "page": 2
    },
    {
      "caption": "Figure 2: (b)-2(f), the contributions",
      "page": 2
    },
    {
      "caption": "Figure 2: (f). The bimodal representations with textual",
      "page": 2
    },
    {
      "caption": "Figure 2: , we can conclude that",
      "page": 2
    },
    {
      "caption": "Figure 1: We summarize this",
      "page": 2
    },
    {
      "caption": "Figure 2: The PR curve of the fusion multimodal representations and the",
      "page": 2
    },
    {
      "caption": "Figure 3: The overall architecture of the proposed SemanticMAC. Note that the frame embeddings and modality embeddings are updated during the stage of",
      "page": 5
    },
    {
      "caption": "Figure 3: Aiming at avoiding the issue of semantic imbalance from",
      "page": 5
    },
    {
      "caption": "Figure 3: , the text data It are firstly processed to",
      "page": 6
    },
    {
      "caption": "Figure 4: The designed Affective Perceiver to learn affective unimodal features",
      "page": 6
    },
    {
      "caption": "Figure 1: D. Semantic-centric Feature Interaction",
      "page": 7
    },
    {
      "caption": "Figure 5: , inspired by VilT [38], each unimodal",
      "page": 7
    },
    {
      "caption": "Figure 5: The proposed Semantic-centric Gated Feature Interaction module.",
      "page": 7
    },
    {
      "caption": "Figure 3: Given a query of representations B = {F i",
      "page": 8
    },
    {
      "caption": "Figure 3: We implement the intra-sample contrastive learning among",
      "page": 9
    },
    {
      "caption": "Figure 6: The distribution of semantic-centric features and labels for SemanticMAC on CMU-MOSEI dataset. (a) The upper left figure is the PR curve",
      "page": 14
    },
    {
      "caption": "Figure 2: , we visualize the Precision-Recall curve",
      "page": 14
    },
    {
      "caption": "Figure 6: (a). By replacing the manual acoustic",
      "page": 14
    },
    {
      "caption": "Figure 6: (a), for u ∈{t, a, v}, the semantic-specific representa-",
      "page": 14
    },
    {
      "caption": "Figure 6: Notes that the multimodal",
      "page": 14
    },
    {
      "caption": "Figure 6: (c), the frequency of emotion classes in semantic-",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "Abstract—In the pathway\ntoward Artificial General\nIntelli-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "semantic mismatch\nsemantic imbalance"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "gence\n(AGI),\nunderstanding\nhuman’s\naffection\nis\nessential\nto",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "hard to reproduce"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "enhance machine’s cognition abilities. For achieving more sensual",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Loud voice,"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "untrainable\nEmphasized tune"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "human-AI interaction, Multimodal Affective Computing (MAC)",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Neutral\n.\u0001.\u0001."
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Audio"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Confusing?\nFA / V"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "in\nhuman-spoken\nvideos\nhas\nattracted\nincreasing\nattention.",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "lack of semantics\nMultimodal"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "However,\nprevious methods\nare mainly\ndevoted\nto\ndesigning",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Laughing face, \nBias\nGround\nFM\nManual Toolkit"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Waving hands"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "multimodal\nfusion\nalgorithms,\nsuffering\nfrom two\nissues:\nse-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Truth\nPositive"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "FT"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Vision"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "FA / V"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "mantic\nimbalance\ncaused by diverse pre-processing\noperations",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Confusing?"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "FT"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "and semantic mismatch raised by inconsistent affection content",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "“That was sick!”"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "contained in different modalities comparing with the multimodal",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Negative"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Language Model\nFeature Distribution Space\nText"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "ground truth. Besides,\nthe usage of manual\nfeatures extractors",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "make they fail\nin building end-to-end pipeline for multiple MAC",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Fig. 1. The two main challenges in conducting multimodal affective comput-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "downstream tasks. To address above\nchallenges, we propose a",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "ing from the perspective of semantic."
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "novel\nend-to-end framework named SemanticMAC to compute",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "multimodal semantic-centric affection for human-spoken videos.",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Affective computing are originated from conventional Natu-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "We firstly employ pre-trained Transformer model\nin multimodal",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "ral Language Processing (NLP) tasks referring to understand-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "data pre-processing\nand design Affective Perceiver module\nto",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "ing the\naffection contained in human-spoken utterances\nand"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "capture unimodal\naffective\ninformation. Moreover, we present",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "a semantic-centric approach to unify multimodal representation",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "conversations\n[1],\n[12],\n[13]. The performance of\naffection-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "learning in three ways, including gated feature interaction, multi-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "related algorithms highly relies on semantic information [14]"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "task pseudo label generation, and intra-/inter-sample contrastive",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "and are mostly improved by exploring the abundant semantic"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "learning. Finally, SemanticMAC effectively\nlearn specific-\nand",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "context embedded in language models. Nevertheless,\nimmod-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "shared-semantic\nrepresentations\nin\nthe\nguidance\nof\nsemantic-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "erate\nreliance on language may easily overfit on subjective"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "centric\nlabels. Extensive\nexperimental\nresults demonstrate\nthat",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "our approach surpass\nthe\nstate-of-the-art methods on 7 public",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "affective components, resulting in biased prediction [15], [16]."
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "datasets in four MAC downstream tasks.",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "Thus, auxiliary features from other modalities, such as audio"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "and image, are introduced to enhance affective understanding"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "Index Terms—Multimodal representation learning, Semantic-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "centric feature interaction and label generation, Intra- and inter-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "with multimodal\nlearning\n[3].\nIn\nprevious MAC methods,"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "sample contrastive learning, Video-based affective computing.",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "unlike textual\nfeatures\nlearned by language models, acoustic"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "and\nvisual\nfeatures\nare mostly\nextracted\nby manual\npre-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "processing toolkit such as CMU-MultimodalSDK1\n[5],\n[17]–"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "I.\nINTRODUCTION",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "[20], due\nto the\ninformation sparsity and inherent noise\nin"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "(MAC)\naims\nat",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "audio and image. However,\nconducting multimodal\nlearning"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "emotion\nclass,\nor\nM ULTIMODAL Affective Computing",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "with manual\nfeatures may raise issues as shown in Figure 1."
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "behavioral\nintention by comprehensively integrating informa-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "On the one hand,\nthe vague description of pre-processing"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "tion\nfrom different modalities\nof\nspeakers\nsuch\nas\ntextual",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "causes\nthe\nextraction of manual\nfeatures hard to reproduce"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "(utterance), acoustic (human voice) and visual\n(facial expres-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "[6],\nintroducing inevitable gap between training and inference"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "sion, head movement, body gesture) modality in a human-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "stages for multimodal learning. Besides, the manual feature ex-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "centric video [1], [2]. With the surge of human-spoken content",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "tractors such as COVAREP [21] and Facet [22] are untrainable,"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "on social media platforms,\nresearch on multimodal affective",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "which brings difficulty in developing end-to-end multimodal"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "computing has become\ncrucial\nin the\ncommunity of multi-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "learning\npipeline\nand\naffects\nthe\ngeneralization\nof\nthe\npre-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "modal\nlearning [3]. Considering various application purposes,",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "trained models in various downstream scenarios."
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "multimodal affective computing is divided into diverse specific",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "On the other hand, due to the demand of semantic context"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "tasks,\nincluding multimodal\nsentiment analysis\n[4]–[6], mul-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "for MAC task,\nthe manual\nfeatures\nsuch as\nfacial\nlandmarks"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "timodal emotion recognition [7]–[9], multimodal humor and",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "for visual features and Mel-frequency cepstral coefficients for"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "sarcasm detection [10],\n[11].",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "acoustic\nfeatures,\nare\nnot\nefficiently\nsuitable\nfor\naffection-"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "related tasks. Lack of\nsemantic\ninformation,\nsuch low-level"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "This work was\nsupported by the National Natural Science Foundation of",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "features lead to poor embedding performance comparing with"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "China (62076262, 61673402, 61273270, 60802069).",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": "textual modality [6],\n[23],\n[24] and bring semantic imbalance"
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "The\nauthors\nare with\nthe\nSchool\nof\nElectronics\nand\nInformation\nTech-",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        },
        {
          "Ronghao Lin ˙ID , Ying Zeng, Sijie Mai": "nology,\nSun\nYat-sen\nUniversity,\nGuangzhou\n510006,\nChina.\n(E-mail:",
          ", Member,\nIEEE\nID , and Haifeng Hu ˙ID": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.0\n0.0": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0"
        },
        {
          "0.0\n0.0": "Recall\nRecall"
        },
        {
          "0.0\n0.0": ""
        },
        {
          "0.0\n0.0": "(e) MMIM [28] with BERT\n(f) MissModal\n[29] with BERT"
        },
        {
          "0.0\n0.0": ""
        },
        {
          "0.0\n0.0": "Fig.\n2.\nThe PR curve\nof\nthe\nfusion multimodal\nrepresentations\nand\nthe"
        },
        {
          "0.0\n0.0": "unimodal representations, including text, audio and vision modalities by state-"
        },
        {
          "0.0\n0.0": ""
        },
        {
          "0.0\n0.0": "of-the-art models training with Glove [30] and BERT [31] features on CMU-"
        },
        {
          "0.0\n0.0": ""
        },
        {
          "0.0\n0.0": "MOSEI dataset. Note that such PR curve is initially proposed as an evaluation"
        },
        {
          "0.0\n0.0": "metric for genrative models by Sajjadi et al.\n[25]\nto formulate the relative"
        },
        {
          "0.0\n0.0": "probability densities of\nthe distributions of\nreal and generated data."
        },
        {
          "0.0\n0.0": ""
        },
        {
          "0.0\n0.0": ""
        },
        {
          "0.0\n0.0": "[35], multimodal\njoint\ntraining\neasily\nsuffer\nfrom modality"
        },
        {
          "0.0\n0.0": "laziness, which makes\nthe model\nneglect\nthe\nlearning\nof"
        },
        {
          "0.0\n0.0": "modality-specific\nfeatures\nregardless\nof\nthe\npaired\nfeatures."
        },
        {
          "0.0\n0.0": "Therefore,\nrelying solely on annotated multimodal\nlabels\nas"
        },
        {
          "0.0\n0.0": "the\nsupervision is\ninsufficient\nfor multimodal\nlearning [27]."
        },
        {
          "0.0\n0.0": "Particularly for MAC task, it is crucial to explore the unimodal"
        },
        {
          "0.0\n0.0": "semantics\ncontained in various modalities\nand enhance nu-"
        },
        {
          "0.0\n0.0": "anced comprehension of fine-grained affection in multimodal"
        },
        {
          "0.0\n0.0": "learning, ensuring more precise prediction without bias [36]."
        },
        {
          "0.0\n0.0": "The case study in Table I\nfurther\nreveals\nthat\nthe MAC task"
        },
        {
          "0.0\n0.0": "require individual supervision signals to capture the affective"
        },
        {
          "0.0\n0.0": "semantic information for various modalities."
        },
        {
          "0.0\n0.0": "Aiming at addressing the challenges of semantic imbalance"
        },
        {
          "0.0\n0.0": "and mismatch, we propose a novel Semantic-centric Multi-"
        },
        {
          "0.0\n0.0": "modal Affective Computing framework, named Semantic-"
        },
        {
          "0.0\n0.0": "MAC,\nto\nlearn multimodal\nrepresentations\nin\nthe\nsemantic"
        },
        {
          "0.0\n0.0": "space\nfor various video-based MAC tasks\nin an end-to-end"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "issue\nin multimodal\nlearning. Since\nthe\nscale\nof\nlanguage",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "1.0\n1.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models is increasing rapidly,\nthe number of\ntrainable param-",
          "2": "fusion_text\nfusion_text"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_audio\nfusion_audio"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eters for other modalities are much smaller\nthan the ones for",
          "2": "fusion_vision\nfusion_vision\n0.8\n0.8"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "textual modality for MAC models, which further exacerbates",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.6\n0.6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the semantic imbalance for various modalities.",
          "2": "Precision\nPrecision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "semantic\nimbalance\nTo better understand the\nissue of\nfor",
          "2": "0.4\n0.4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "different modalities, we visualize the contribution of\nthe uni-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.2\n0.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modal\nfeatures\nfor\nthe\nfusion multimodal\nrepresentations\nin",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Figure 2. Inspired by but diverse from [25], [26], we compute",
          "2": "0.0\n0.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the Precision-Recall\n(PR)\ncurve\nfor\nthe\nfeature distributions",
          "2": "Recall\nRecall"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "between the unimodal and multimodal\nrepresentations,\ntaking",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "(a) MulT [17] with Glove\n(b) MulT [17] with BERT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the representations from state-of-the-art multimodal sentiment",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "analysis models [17],\n[23],\n[27]–[29] as examples.",
          "2": "1.0\n1.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_text"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "As\nshown in Figure 2(a), we can observe that\nthe manual",
          "2": "fusion_audio"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_vision\n0.8\n0.8"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "acoustic and visual features contribute similarly when utilizing",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "low-level\ntextual features such as Glove [30] which computes",
          "2": "0.6\n0.6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_text"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "word vector based on global word co-occurrence counts statis-",
          "2": "Precision\nPrecision\nfusion_audio"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_vision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tics. However, when we\nsubstitute\nthe\ntextual\nfeatures with",
          "2": "0.4\n0.4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "BERT [31] which embeds high-level semantic context by pre-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.2\n0.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "trained language model\nin Figure 2(b)-2(f),\nthe contributions",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of manual acoustic and visual features drop significantly to the",
          "2": "0.0\n0.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal\nrepresentations compared with the one of\ntextual",
          "2": "Recall\nRecall"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features, no matter\nin which models. Although the\nexisting",
          "2": "(c) MISA [23] with BERT\n(d) Self-MM [27] with BERT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fusion strategies [5] may adjust\nthe contributions of different",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "1.0\n1.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "unimodal\nrepresentations\nadaptively,\nthey\nfail\nin\nbalancing",
          "2": "fusion_text"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_audio"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the\ncontributions of different modalities, mostly due\nto the",
          "2": "fusion_vision\n0.8\n0.8"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_ta"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "inherent\ndiscrepancy\nof\nsemantic\nabundance\nfrom various",
          "2": "fusion_tv"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "fusion_av"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.6\n0.6"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "unimodal\nrepresentations.",
          "2": "fusion_text"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Precision\nPrecision\nfusion_audio"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Moreover, we remove features from each modality input\nin",
          "2": "fusion_vision"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.4\n0.4"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "traversal manner\nas MissModal\n[29]\nto construct unimodal,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "bimodal and trimodal\nrepresentations, and then compute the",
          "2": "0.2\n0.2"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "PR-curve among the distributions of\nthese representations as",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "0.0\n0.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "shown in Figure 2(f). The bimodal representations with textual",
          "2": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Recall\nRecall"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features contribute more than the representations with acoustic",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "(e) MMIM [28] with BERT\n(f) MissModal\n[29] with BERT"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and visual features solely or both, which further indicates that",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the introduction of textual features can effectively increase the",
          "2": "Fig.\n2.\nThe PR curve\nof\nthe\nfusion multimodal\nrepresentations\nand\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "unimodal representations, including text, audio and vision modalities by state-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "semantic information to the fusion multimodal representations.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "of-the-art models training with Glove [30] and BERT [31] features on CMU-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "From the visualization in Figure 2, we can conclude that",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "MOSEI dataset. Note that such PR curve is initially proposed as an evaluation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "existing low-level manual acoustic and visual\nfeatures are no",
          "2": "metric for genrative models by Sajjadi et al.\n[25]\nto formulate the relative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "probability densities of\nthe distributions of\nreal and generated data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "longer\nappropriate\nfor high-level\ntextual\nfeatures\nembedded",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by context-based language model. The difference of semantic",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "abundance\nfrom various modalities\ncauses\nthe\nissue\nof\nse-",
          "2": "[35], multimodal\njoint\ntraining\neasily\nsuffer\nfrom modality"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mantic imbalance and affects the multimodal\nfusion process,",
          "2": "laziness, which makes\nthe model\nneglect\nthe\nlearning\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "leading to an urgent need of new solutions for unimodal feature",
          "2": "modality-specific\nfeatures\nregardless\nof\nthe\npaired\nfeatures."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "extraction of acoustic and visual modalities.",
          "2": "Therefore,\nrelying solely on annotated multimodal\nlabels\nas"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In addition, different modalities may bring diverse affective",
          "2": "the\nsupervision is\ninsufficient\nfor multimodal\nlearning [27]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "intensities or classes for MAC task [15],\n[19],\n[32], meaning",
          "2": "Particularly for MAC task, it is crucial to explore the unimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "that\nthe\naffection\nsemantics\nof\nvarious modalities may\nnot",
          "2": "semantics\ncontained in various modalities\nand enhance nu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "remain consistent\nin the same video. Previous methods cat-",
          "2": "anced comprehension of fine-grained affection in multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "egorize unimodal\nfeatures\ninto modality-specific and -shared",
          "2": "learning, ensuring more precise prediction without bias [36]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features to deal with such semantic inconsistency circumstance",
          "2": "The case study in Table I\nfurther\nreveals\nthat\nthe MAC task"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23],\n[24],\n[33],\n[34]. However,\nthey utilize final multimodal",
          "2": "require individual supervision signals to capture the affective"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ground truth labels to jointly supervise representations learn-",
          "2": "semantic information for various modalities."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing, confusing the training of modality-specific features with",
          "2": "Aiming at addressing the challenges of semantic imbalance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "different affection as\nshown in Figure 1. We summarize this",
          "2": "and mismatch, we propose a novel Semantic-centric Multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "issue as\nsemantic mismatch raised by inconsistent\nsemantics",
          "2": "modal Affective Computing framework, named Semantic-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "among unimodal\nfeatures and the corresponding multimodal",
          "2": "MAC,\nto\nlearn multimodal\nrepresentations\nin\nthe\nsemantic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nal.\nground\ntruth\nlabels. Moreover,\nas\ninterpreted\nin Du",
          "2": "space\nfor various video-based MAC tasks\nin an end-to-end"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TABLE I",
          "3": "cross-modal commonality by reducing the modality gap, both"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "CASE STUDY OF SEMANTICMAC TO TACKLE SEMANTIC MISMATCH.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "of which enhance model’s ability of affective perception and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "multimodal\nreasoning. Next,\ntargeting\nat\ntraining\nrepresen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Pseudo\nGround\nSemanticMAC",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "#\nModality Description",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Label p∗\nTruth ygt\nPrediction ˆy∗",
          "3": "tations with various affective content, we present Semantic-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“It’s berry berry berry red and",
          "3": "centric Label Generation (SCLG)\nto calculate\nspecific-\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "-1.006\n-0.395",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "it’s way too not good for me”",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "shared-semantic\nlabels\nfor\neach\nsample\nfrom multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "ground truth in a momentum-updated policy. The generated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1\n1.016\n0.875\n0.000\n-0.110",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "...",
          "3": "pseudo labels are served as weak supervision signals to guide"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Soaring and emphatic tone\n0.902\n1.397",
          "3": "the learning of\nspecific- and shared-semantic representations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Hannibal Lecter\nis one twisted character",
          "3": "in\na multi-task\ntraining\nparadigm,\nalleviating\nthe\nsemantic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "-0.330\n-0.373",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and this movie’s all about him”",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "mismatch issue of multimodal\njoint\ntraining."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Besides, diverse with conducting contrastive learning among"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2\n0.870\n0.977\n0.936\n1.333",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "paired modalities\n[39]–[41], we\nperform Semantic-centric"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "...",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Excited and fast\ntone\n-0.143\n-0.988",
          "3": "Contrastive Learning (SCCL)\nfor various modalities from the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Rent\nthis one”\n0.146\n0.003",
          "3": "perspectives of\nintra-\nand inter-sample. The\nintroduction of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "semantics\nin contrastive\nlearning significantly improves\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "-0.667\n-0.414\n-1.274\n-1.073",
          "3": "convergence of\nrepresentations\nfor both unimodal and multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "...",
          "3": "modal\nsub-tasks. Specifically,\nthe\nintra-sample one measure"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Depressed and lowering tone\n-1.080\n-0.802",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "the\nsimilarity\nof\nfeatures\nfrom different modalities\nin\neach"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“You really don’t want\nto even\nAnger\nAnger",
          "3": "sample, encouraging cross-modal interaction with the guidance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mess with this movie”\nDisgust",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "of specific and shared semantics. While the inter-sample one"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4\nDisgust\nDisgust",
          "3": "is\npresented\nunder\nthe\nguidance\nof\nthe\nsentiment\nintensity"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Happy\nHappy",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "or\nemotion\nclasses\nof multimodal\nrepresentations,\nenabling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "...",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Disgust\nNegative and emphatic tone\nDisgust",
          "3": "affection-related cooperation in the multimodal fusion. Lastly,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "we calculate the multi-task losses\nsupervised by the ground-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“You can choose to work with a",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sad\nNone",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "transaction broker or a buyer’s agent”",
          "3": "truth and generated paseudo labels and the contrastive learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sad\nSad\nSad\nSad",
          "3": "losses as the final optimization objective."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "5",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fear\nFear\nFear\nAnger",
          "3": "The main contributions of our paper can be summarized as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "...",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "• A unified and novel end-to-end framework for MAC:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fear\nFear\nUncertained and rhetorical\ntone",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Focusing on the affective semantic of for textual, acoustic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“But\nI\nreally didn’t\nlike the apocalyptic\nSad",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sad\nending,\nits just\nleft me disappointed”\nAnger",
          "3": "and visual modalities from the human-spoken video, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Surprise",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Surprise",
          "3": "propose\na\nnovel\nframework\nnamed\nSemanticMAC to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sad\nSad\nSad\n6",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sad\nAnger\nDisgust\nDisgust",
          "3": "unify the learning process of multimodal\nrepresentations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Disgust",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Disgust",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "...",
          "3": "and\npredict\nhuman’s\naffective\nintensity\nin\nan\nend-to-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transition and definite tone\nSurprise\nSurprise",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "end manner\nfor multimodal affective computing (MAC)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "task. Rather than manual\ntoolkit\nin previous methods, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "manner. Firstly, we utilize powerful pre-trained Transformer",
          "3": "utilize the pre-trained Transformer model and design the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models\n[37]\ninstead of manual\nfeatures\nto extract unimodal",
          "3": "affective perceiver\nto extract unimodal\nfeatures, which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features\nof\ndifferent modalities\nfrom the\nraw videos. Such",
          "3": "enable flexible pre-processing with various length video"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pre-processing operation ensures\nthe end-to-end training and",
          "3": "and address the issue of semantic imbalance."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "inference of the multimodal learning model. In order to reduce",
          "3": "•\nSemantic-centric\nrepresentation\nlearning\napproach:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the modality heterogeneity and generalize to various scenarios",
          "3": "According to the specific semantics\ninside each modal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for MAC task, we\nunify\nthe\nembedding\nform for\ndiverse",
          "3": "ity and the\nshared semantics\nacross diverse modalities,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "input modalities according to the temporal sequences. Inspired",
          "3": "we\npresent Semantic-centric Gated Feature\nInteraction"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by\nthe\nthought\nof\npositional\nembedding\n[38], we\nutilize",
          "3": "(SGFI)\nto\ncapture\nintra-\nand\ncross-modal\ndynamics."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learnable\nframe\nembedding\nto\ndenote\nvideos with\ndifferent",
          "3": "Meanwhile, we introduce Semantic-centric Label Gener-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "frame lengths, which enhances the performance when dealing",
          "3": "ation (SCLG)\nto generate weak supervision for specific-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with varying length human-spoken videos. To further collect",
          "3": "and shared-semantic\nrepresentations\nrespectively, which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "taffective\ninformation, we design a module named affective",
          "3": "eases the semantic mismatch in the label space. We also"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "perceiver to process the features into fixed number of learnable",
          "3": "conduct Semantic-centric Contrastive Learning (SCCL)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tokens\nin\nthe\nlatent\nspace, meanwhile\nfiltering\nthe\nnoisy",
          "3": "to promote the interaction among modalities and across"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "content contained in the generic acoustic and visual\nfeatures.",
          "3": "samples guided by semantics and affection information."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Then, we conduct Semantic-centric Gated Feature Interac-",
          "3": "• Achieving state-of-the-art performance: Extensive ex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion\n(SGFI)\ninside\nand\namong\nthe\nunimodal\nfeatures\nfrom",
          "3": "periments demonstrates the effectiveness of our approach"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "various modalities by bridge attention and gated mechanism",
          "3": "on 7 public datasets\nfor 4 MAC downstream tasks uni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to extract\nspecific- and shared-semantic representations. The",
          "3": "versally,\nincluding multimodal sentiment analysis, multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former\nrepresentations explore the intra-modal dynamics\nfor",
          "3": "modal emotion recognition, multimodal humor and sar-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "affection-specific knowledge and the latter ones integrates the",
          "3": "casm detection."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "II. RELATED WORK",
          "4": "with textual data by masked attention layers\nin the vision-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "language pre-training. However, most of\nthem leverage\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Multimodal Affective Computing",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "attention mechanism to construct cross-modal connection in-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "As a sub-field of multimodal\nlearning,\nthe key question of",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "stantly,\nignoring the different\ninteraction manners and various"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Multimodal Affective Computing (MAC)\nis\nsummarized as",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "supervision of the fine-grained features in the semantic space."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "how to extract semantic-rich unimodal features and effectively",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fuse the affective related information from each modality to",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learn multimodal representations [2], [42], [43]. Therefore, de-",
          "4": "C. Contrastive Learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "veloping pipeline to conduct MAC contains two main aspects:",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "Contrastive learning focuses on dividing the samples\ninto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "unimodal\nfeature extraction and multimodal\nfusion [1],\n[3].",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "positive\nand negative pairs\nsets\nand adjusting the\nsimilarity"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Compared with the traditional\nlow-level hand-craft features",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "of\nthe corresponding representations\n[55]. The most popular"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21],\n[44],\n[45], unimodal\nfeatures\nextracted by deep learn-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "form of contrastive loss function is InfoNCE, which is utilized"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing based models have achieved impressive performance for",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "to encode underlying shared latents by maximizing the lower"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "diverse modalities when\napplied\nin\ndifferent fields. Partic-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "bound\nof\nthe mutual\ninformation\n[56]. As\na\npretext\ntask,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ularly,\nunimodal\npre-trained models\nconsist\nof Transformer",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "contrastive\nlearning is\ninitially adopted at unimodal models"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[37],\nsuch as BERT [31]\nand GPT [46]\nfor\ntext\nin natural",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "in an unsupervised manner\n[57]–[59],\nand then extended to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "language processing, ViT [47]\nfor\nimage in computer vision,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "supervised methods and multimodal models due to its great"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and HuBERT [48] for audio in speech processing, are capable",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "effectiveness and generality. Supcon [60]\nleverages\nlabel\nin-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of learning efficient unimodal representations and generalizing",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "formation to conduct contrastive learning in a fully-supervised"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to various downstream tasks\nin the pre-train and fine-tuning",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "setting. Recent works\nsuch as CLIP [39], ALIGN [61]\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "paradigm. Additionally, multimodal\nfusion focuses on jointly",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "wav2vec2.0\n[62]\nand\nso\non\nhave\nclaimed\nthe\nbetter\ncross-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "integrating\ninformation\nfrom diverse modalities\nto\nperform",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "modal alignment performance with contrastive objectives. Par-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nal.\net\naffective\nprediction\n[2]. Gkoumas\n[5]\nand Geetha",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "ticularly,\nImageBind [63] extend contrastive learning into the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "al.\n[9] have provided comprehensive surveys on the current",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "joint\nembedding\nspace\nacross\nsix modalities. Nevertheless,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal\nfusion techniques for MAC, which have attained",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "most\nof\nthem lack\nexploration\ninto multimodal\nfusion\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "remarkable results while still suffered from the huge modality",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "reveal\nsignificant modality\ngap\n[64], which\nis\nimperatively"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gap and the issues of semantic imbalance and mismatch.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "needed to be addressed."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The MAC task\nconsists\nof multiple\naffective\nprediction",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "downstream tasks,\nincluding 1) multimodal sentiment analysis",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "III. METHODOLOGY"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[4]–[6] to compute a continuous score as the sentiment polarity",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "The proposed SemanticMAC is presented in detail\nin this"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of utterance in a regression method; 2) multimodal emotion",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "section. We first define\nthe\ninput\nand output of MAC task"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition [7]–[9]\nto classify the emotion class of\nthe utter-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "and clarify the corresponding notations. Then we introduce the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ance in monologue or conversation; 3) multimodal humor and",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "end-to-end architecture of\nthe proposed framework. Next, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sarcasm detection [10],\n[11]\nto identify whether\nthe utterance",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "put\nforward the\nextraction process of unimodal\nfeatures\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "contains\nthe humorous or\nsatirical\nintent. Previous methods",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "different modalities. Following the semantic-centric thought,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "address single task according to distinctive forms of input data",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "the module of gated feature interaction and the strategy of label"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and objective functions. Differently in this paper, we present",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "generation are described additionally. Finally, we\nformulate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "one unifying framework to effectively adopt these downstream",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "the total optimization objective with the semantic-centric con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tasks, providing a unique insight\nfor\nfuture research.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "trastive learning loss and the individual\ntask prediction losses."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Attention Mechanism",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "A. Problem Definition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Current multi-head\nattention mechanism is mostly\nbased",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on Transformer\n[37],\nnamed\nself-attention, which\npresents",
          "4": "Multimodal Affective Computing (MAC)\nconcentrates on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "normalized\nscaled\ndot-product\namong\nthe\ninput\nquery,\nkey",
          "4": "learning efficient representations to conduct various regression"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and value from the same input sequence. Multiple variants of",
          "4": "or\nclassification tasks\nfor\naffective\nanalysis\nfrom the multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention mechanism have been proposed to adapt\nin distinct",
          "4": "modal\nsignals contained in a human-spoken video. To unify"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "scenarios, such as linear attention [49] to reduce the inference",
          "4": "diverse downstream MAC tasks, we formulate the multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "computation from quadratic complexity into linear one, cross-",
          "4": "input of the raw videos as Iu ∈ Rℓu×cu , where ℓu denotes the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention [50] to process different input, and mutli-query atten-",
          "4": "temporal\nlength of utterance sequence and cu denotes various"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion [51]\nto decrease the model parameters and the key/value",
          "4": "contents of unimodal\nsignal\nat\nthe\nsampled timestep of\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cache and so on.",
          "4": "video. Particularly, since each video clip contains at\nleast an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Multimodal\nlearning with\nattention mechanism has\nbeen",
          "4": "utterance spoken by one human with facial expression, head"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "exploited extensively in previous researches [52]. Whisper [53]",
          "4": "movement and body gesture, u ∈ {t, a, v} represents the tex-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "trains\na\nrobust\nspeech recognition model by cross-attention",
          "4": "tual, acoustic and visual modalities respectively [3]. According"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with a large-scale web text-audio data as a weakly supervised",
          "4": "to the semantics contained in various modalities,\nthe proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "datasets\nin\na multi-task\ntraining\napproach.\nFlamingo\n[54]",
          "4": "SemanticMAC processes each modality of the raw multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "presents\nperceiver\nresampler\nto\nconvert\nvarying-size\nlarge",
          "4": "data to unimodal\nand then integrates\nthe\nrepresentations Fu"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature maps to fixed visual\ntokens and interact\nthese tokens",
          "4": "affective information into multimodal\nrepresentations FM by"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "training while then fixed and generalized into the downstream inference.",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "cross-modal\ninteraction in the semantic space. Lastly,\nthe task",
          "the frame embeddings and modality embeddings are updated during the stage of": "sponding type of modality. In addition, we conduct semantic-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "predictor utilize the final mutlimodal representations to output",
          "the frame embeddings and modality embeddings are updated during the stage of": "centric feature interaction among various unimodal representa-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "the sentiment\nscores\nin regression task\nyM , which serves as",
          "the frame embeddings and modality embeddings are updated during the stage of": "tions to learn semantic-specific representations (F t\nsp)\nsp, F v\nsp, F a"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "and semantic-shared representations (F t"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "or as the emotion classes in recognition and detection task.",
          "the frame embeddings and modality embeddings are updated during the stage of": "sh, F a\nsh, F v\nsh) for each"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "modality, which\nfurther\naddress\nsemantic\nimbalance\nissue"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "induced\nby\noverfitting\non\nthe\ndominant modality.\nSpecifi-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "B. Architecture Overview",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "cally, we design the\ninteraction mechanism as gated multi-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "The architecture of the proposed SemanticMAC processing",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "head\nintra-\nand\ncross-attention\nto\nefficiently\ncapture\nintra-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "raw video in an end-to-end manner\nis depicted as Figure 3.",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "modal\ndynamics\nand\nexplore\ncross-modal\ncommonality. To"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "Aiming\nat\navoiding\nthe\nissue\nof\nsemantic\nimbalance\nfrom",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "focus\non\nthe\nlearning\nof\nquery modality\nat\none\ntime, we"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "root, we firstly take pre-trained Transformer models\ninstead",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "measure the attention score with multi-query setting in each"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "of manual\ntoolkits\nto\npre-process\nunimodal\ndata\ninto\nem-",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "interaction. Additionally,\nto reduce the impact of\nthe modal-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "beddings with\nconsistent\nform for\nvarious modalities. The",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "ity\ngap\nintroduced\nby modality\nheterogeneity, we\nutilize\na"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "unimodal\nare multiple\ntokens\nin R ˜fu×du\nembeddings Xu",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "set\nof\nbridge\ntokens\nto\ninteract\nthe\ninformation\nbetween"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "where\n˜fu denotes the numbers of\ntokens and du denotes the",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "query and key modalities with massively diverse distributions."
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "representation\ndimension\nof modality u ∈ {t, a, v}. Then,",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "Lastly, we\nconcatenate\nthe\nsemantic-specific\nrepresentations"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "for\nacoustic\nand\nvisual modalities, we\ndesign\nthe\naffective",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "and\nsp, F a\nsp, F v\nsp) and semantic-shared representations Fsh"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "perceiver\nto integrate\naffective\ninformation contained in the",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "then project\nthem into multimodal representations FM , which"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "generic unimodal embeddings and transfer the knowledge into",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "are\nfed\nto\nthe\ntask\npredictor\nto\noutput\nthe\nfinal\naffective"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "multiple learnable tokens Fa and Fv with fixed length, which",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "",
          "the frame embeddings and modality embeddings are updated during the stage of": "prediction ˆyM ."
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "enable the flexible handling of videos with different\nlengths",
          "the frame embeddings and modality embeddings are updated during the stage of": ""
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "at\nthe\nsame\ntime. Besides,\nthe\nacoustic\nand visual\nfeatures",
          "the frame embeddings and modality embeddings are updated during the stage of": "Targeting\nat\nthe\nissue\nof\nsemantic mismatch\nraised\nby"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "to attend\nare summed with learnable frame embedding Ef r",
          "the frame embeddings and modality embeddings are updated during the stage of": "various contents of each modality, we tend to utilize different"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "by\nrelative\ntemporal\norder\nof\nvarious\nframes\nof\nthe\nvideo.",
          "the frame embeddings and modality embeddings are updated during the stage of": "semantic-centric labels as the supervision for different features"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "While for\ntextual modality,\nthe pre-trained language model\nis",
          "the frame embeddings and modality embeddings are updated during the stage of": "in a multi-task training manner, which competently guides the"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "utilized to learn the affective textual\nrepresentation Ft. Note",
          "the frame embeddings and modality embeddings are updated during the stage of": "learning of unimodal\nand multimodal\nrepresentations\nin the"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "that we\nfreeze\nthe pre-processed encoders\nfor\nacoustic\nand",
          "the frame embeddings and modality embeddings are updated during the stage of": "semantic space. According to semantic attributes, we divide"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "visual modalities and the tokenizer for textual modality during",
          "the frame embeddings and modality embeddings are updated during the stage of": "the training of\nrepresentations\ninto five sub-tasks denoted as"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "the\ntraining\nstage while\nupdate\nthe\nparameters\nof\naffective",
          "the frame embeddings and modality embeddings are updated during the stage of": "∗ ∈ {M, S, T, A, V },\nincluding the sub-tasks of multimodal"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "perceivers and language model as fine-tuning paradigm. Next,",
          "the frame embeddings and modality embeddings are updated during the stage of": "representations (M ), semantic-shared representations (S) and"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "to make\nthe model distinguish different modalities\nin latter",
          "the frame embeddings and modality embeddings are updated during the stage of": "semantic-specific representations for each modality (T, A, V )."
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "modules,\nthe unimodal\nare attached with\nrepresentations Fu",
          "the frame embeddings and modality embeddings are updated during the stage of": "Most datasets only manually annotate the multimodal ground-"
        },
        {
          "Fig. 3.\nThe overall architecture of\nthe proposed SemanticMAC. Note that": "according to the corre-\nlearnable modality embeddings Emd",
          "the frame embeddings and modality embeddings are updated during the stage of": "truth labels ygt for multimodal representations FM in sub-task"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "M [5],\n[36]. Due\nto\nthis, we\nconsider\nto\ngenerate\npseudo",
          "6": "where θu denotes the encoders parameters and the [CLS] token"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "labels\naccording\nto\nthe\nfine-grained\nlevel\nof\nsemantics\np∗",
          "6": "are taken as the global embedding to aggregate the contents of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(F t\nfor\nother\nrepresentations\nas\na weakly-\nsp, F a\nsp, F v\nsp, Fsh)",
          "6": "each frame [39], [63], [69]. Note that we leverage the power of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "supervised strategy compared with the ground-truth annota-",
          "6": "ImageBind for\nits excellent performance in aligning different"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tions. By calculating the\nsimilarity ranking matrix for\neach",
          "6": "multimodal data in the joint embedding space [70]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "type of\nrepresentation in the feature space,\nthe pseudo labels",
          "6": "Although ImageBind has been proved effectively in mul-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "are then generated by scaling and shifting the corresponding",
          "6": "timodal\nalignment,\nthe\nextracted\nunimodal\nembeddings\nare"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ground-truth labels of k-nearest neighborhood samples. Be-",
          "6": "coarse-grained\nand\ngeneric\nin\nthe\nembedding\nspace,\ncon-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sides, we stabilize the generation process of the pseudo labels",
          "6": "taining massive\ntask-unrelated noise\nand affective-irrelevant"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in a momentum-based updating policy as\nthe training epoch",
          "6": "information. Besides, directly utilize [CLS] embeddings as the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "increases. Supervised by the ground truth labels and pseudo",
          "6": "unimodal\nrepresentations\ncause\nthe model\nlack of\ntemporal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "labels,\nthe multi-task predictors take the unimodal and multi-",
          "6": "interaction for each modality. Thus, we design an extra module"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modal representations as the input and output the affective pre-",
          "6": "named Affective Perceiver\nto further\nlearn fine-grained uni-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "diction ˆy∗ for each sub-task. Moreover, we perform semantic-",
          "6": "modal\nfeatures and explore affective dynamics by interacting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "centric\ncontrastive\nlearning in the\nlevel of\nintra-sample\nand",
          "6": "the [CLS] embeddings across frames as shown in Figure 4."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "inter-sample at\nthe unit hypersphere [65]\nto further enhance",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the convergence of multimodal\nrepresentations\nlearning. The",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Fu\n× n\nAffective Perceiver"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former one pulls\ncloser\nthe\nsemantic-shared representations",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nall modalities\ninside\nthe\nsame video sample\nand pushes",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "{a, v}\n× # layers\nu ∈"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "away the semantic-specific representations for each modality,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which encourage the decoupling of semantic information for",
          "6": "Feed Forward"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "unimodal features. While the latter one constructs positive and",
          "6": "affective-related"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "information flow"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "negative pairs based on the ground-truth affective category for",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the multimodal representations among different samples. More",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "technical details are introduced in the following subsections.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Multi-head Attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Q = Lu\nK = V = Concat[Xu , Lu]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "C. Unimodal Feature Extraction",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To unify the pre-processing of various modalities, we adopt",
          "6": "Xu\nLu"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "～"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transformer-based models\nto extract unimodal\nfeatures. As",
          "6": "frame"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "shown in Figure 3,\nthe\ntext data\nare firstly processed to\nIt",
          "6": "embedding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "f = i"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by\ntokenizer\naccording\nto\nthe\nspecific\nlanguage\ntokens Xt",
          "6": "f = 1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Learnable Tokens\n× n"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "~"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models\n[31],\n[66],\n[67]\nin particular downstream task. Note",
          "6": "…\n× f"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "f = 0"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "that our\nframework is\nsuitable\nfor various\nlanguage model,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Random Initialization"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which is latter validated in the experiments. Then, we utilize",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the pre-trained language model\nto learn the textual\nrepresen-",
          "6": "LN + Select [CLS] Token + Pooling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tations Ft, which is formulated as:",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Unimodal\nUnimodal\nUnimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Encoder\nEncoder\nEncoder"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "we design a multi-layer Transformer-based module named as",
          "7": "."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Semantic-centric Gated Feature Interaction pu(  )  / hu(  )"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Affective Perceiver by adopting multi-head attention (MHA)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "u\nu"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "{t, a, v}\nor\nu ∈\nFsp\nFsh"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and feed forward network (FFN) in each layer. For u ∈ {a, v},",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "mean pooling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the Affective\nPerceiver\ngradually\nencourages\nthe\naffective",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "× # layers\nGated Bridge Attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information of\nto flow to the\nthe unimodal embeddings Xu",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learnable tokens Lu. By constructing query, key and value as",
          "7": "ReLU Gating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the computations of\nQ = Lu, K = V = Concat[Xu, Lu],",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Feed Forward"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "each Affective Perceiver\nlayer are formulated as follows:",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lu = M HA(LN (Q, K, V )) + Lu",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(4)",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lu = F F N (LN (Lu)) + Lu",
          "7": "ReLU Gating"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where layer normalization (LN) and residual connections are",
          "7": "Cross-Attention\nCross-Attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "employed\naround\neach\nof\nthe\nsub-layers. Note\nis\nthat Lu",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Bridge Tokens"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "initialized randomly and the output of the last layer is taken as",
          "7": "scale down"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "scale down"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the unimodal\nrepresentations Fu ∈ Rn×du . The effectiveness",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of such fixed number of leanrbale tokens in content abstraction",
          "7": "AdaptiveAvgPool1d"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for various modalities have been proved in recent\nresearches",
          "7": "one-head"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "proj.\nmulti-head"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[54], [71], [72]. Moreover, the unimodal learnable tokens Lu in",
          "7": "proj."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Q\nK = V"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the Affective Perceiver can not only integrate the most useful",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information for downstream tasks while removing irrelevant",
          "7": "modality\nmodality\n～\n～\n～\nembedding\nembedding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "modality"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "～\nembedding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "noise, but empower the model with the ability to align acoustic",
          "7": "or"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and visual modalities with different\nlanguage model\nin the",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Fq = Fu\nFkv= Fu"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature\nspace. With the\nintroduction of Affective Perceiver,",
          "7": "Fkv = Concat[Fu  , Fu   ]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Semantic-shared\nSemantic-specific"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the proposed framework is capable of processing video with",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "various frame length and extracting affective unimodal features",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Fig. 5.\nThe proposed Semantic-centric Gated Feature Interaction module."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "competently, which\nfurther\naddress\nthe\nissue\nof\nsemantic",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "imbalance as shown in Figure 1.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "in multimodal\nfusion for multimodal\naffective\ncomputing is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. Semantic-centric Feature Interaction",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "far more complicated than simply multimodal alignment\n[5]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "After\nthe\nextraction\nof\nunimodal\nfeatures,\nthe\nessential",
          "7": "Therefore, we improve the cross-attention mechanism in three"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "question of multimodal\nlearning has become how to interact",
          "7": "ways for SGFI module, named Gated Bridge Attention (GBA),"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "various\ntype\nof\ninformation\nfrom different modalities\nand",
          "7": "to adapt at\nthe complex multimodal\nfusion:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conduct multimodal\nfusion with huge modality gap.\nIn the",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "1) Multi-query Attention: We adopt multi-query attention"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "perspective of\nsemantic, we decouple the feature space into",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "[51],\n[77]\nto\nprimarily\nexcavate\nvarious\nsemantics\nin-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "semantic-specific and semantic-shared features, where the for-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "side query vectors, which accelerate the convergence of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mer\nfeatures\nfocus\non modal-specific\nsemantic\ninformation",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "mutlimodal\nlearning and lower\nthe memory-bandwidth"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "according to the contents of diverse modalities while the latter",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "requirements concurrently. Specifically, we utilize multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ones integrates the invariant commonalities among all modal-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "for query vectors while maintain a\nhead projection W x"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ities. Such feature disentanglement\nstrategy is\nintuitive\nand",
          "7": "h"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "single head of key/value vectors which share the same"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "works successfully with theoretical\ninterpretability [23],\n[34],",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "weights\nin the\nlinear projection W y\nfor\neach head of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[73]. Diverse from previous researches, we propose Semantic-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "query vectors,\nformulated as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "centric Gated Feature\nInteraction (SGFI)\nto learn semantic-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "specific\nand -shared representations by the designed bridge",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention\nand\ngated mechanism, which\neffectively\ntransfer",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Qh = QW x\nh ∈ Rn×dhead , h ∈ [1, head]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "intra- and cross-modal knowledge through bridge tokens and",
          "7": "(6)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Kh = Vh = KW y = V W y ∈ Rn×dhead"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "filter\nthe irrelevant\nfeatures by weighted activation layer.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "As shown in Figure 5,\ninspired by VilT [38], each unimodal",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representation Fu is firstly summed with a learnable modality",
          "7": "head\n="
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "where\ndenotes\nthe\nnumber\nof\nheads,\ndhead"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "indicates\nthe modality\ntype\nembedding Emd ∈ Rd, which",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "is\ndc/head denotes\nthe dimension of each head and dc"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for\nthe module to distinguish corresponding representation in",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "set as the common dimension for each representation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "latter\ninteraction, which is formulated as:",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "2) Bridge Token: Aiming\nat\nbridging\nthe modality\ngap"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "among various modalities in the semantic space and con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "u ∈ {t, a, v}\n(5)\nFu = Fu + Emd,",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "ducting efficient feature interaction, we introduce Bridge"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Similar\nas\nself-attention\n[37],\ncross-attention\nhas\nbeen",
          "7": "Tokens with fixed m tokens\n(m < n) as bottleneck to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "proved competent\nin aligning different\ninput data as query and",
          "7": "restrict\nthe intra- and cross-attention flow,\ninspired by the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "key/value,\nrespectively [74]–[76]. However, due to the huge",
          "7": "thought of\ninformation bottleneck [78]–[80]. The Bridge"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modality gap and delicate modality relationship, the interaction",
          "7": "Tokens B are obtained by aggregating features in adaptive"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "average pooling based on semantics from query vectors:",
          "8": "essentially contrary with the\nthought of disentangled repre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "sentation learning. Besides,\nthe\naffection expressed through"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Q′ = Concat[Q1 . . . Qhead] ∈ Rn×dc",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "single modality can be quite diverse, which is concluded as"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(7)\nK ′ = V ′ = Repeat(Kh) ∈ Rn×dc",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "the semantic mismatch issue as\nshown in Figure 1. Aiming"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B = AdaptiveAvgP ool(Q′) ∈ Rm×dc",
          "8": "at\naddressing this\nissue, we present Semantic-centric Label"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "√",
          "8": "Generation (SCLG)\nto construct pseudo label space based on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Then,\nscaling\ndown\nby\nthe\nattention matrix\nis\ndc,",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "semantics\nas\nthe weak\nsupervision\nstrategy\nto\nimprove\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "computed as:",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "learning of semantic-centric representations."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(Q′BT )(BK ′)",
          "8": "Specifically, we deem the learning processes of\nrepresen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "√\nBridgeAttn(Q, K, V ) = Sof tmax(\nV ′)",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dc",
          "8": "semantics\nas distinct\nsub-tasks ∗ ∈\ntations F∗ with various"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(8)",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "{S, T, A, V }, which denote\nthe\nsub-task of\nsemantic-shared"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3) Gated ReLU: To filter\nthe redundancy according to the",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "representations Fsh and semantic-specific representations F u\nsp"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "semantic of individual representations, we adopt the gated",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "for\ntextual, audio and visual modalities. Each subtask should"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mechanism between each attention and feed forward sub-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "be trained under\nthe guidance of\nthe corresponding semnatic-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "layer by Rectified Linear Unit\n(ReLU)\n[81], which has",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "centric pseudo labels. Inspired by Yu et.al [27],\nthe semantic-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "been proved suitable for Transformer models due to its",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "are assumed to share the distribution space\ncentric labels p∗"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "activation\nsparsity\nand\ninference\nefficiency\n[82],\n[83].",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "with multimodal\nground\ntruth\nlabels\nutilize\nygt. Thus, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Thus, for u ∈ {t, a, v}, the computation in GBA is finally",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "the common semantics contained in the representations across"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formulated as:",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "various samples and their ground truth labels to generate the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F u = ReLU (BridgeAttn(Q, K, V )) + F u",
          "8": "pseudo specific- and shared-semantic labels as shown in Figure"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(9)",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "3."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F u = ReLU (F F N (F u)) + F u",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "Given a query of representations B = {F i\n∗}B\ni=1, we conduct"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The SGFI module is conducted by stacking multiple GBA",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "k-Nearest Neighbor\n(k-NN)\nalgorithm to find\nthe K most"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention layers\nand outputs\nsemantic-centric\nrepresentations",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "nearest\nsamples {F k\nfor\neach representation\n∗ }K"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "k=1(K < B)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "according to the input modality, which are denoted as pu(·) for",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "F i\n∗ by comparing the similarity in the feature space and then"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for\nsemantic-\nsemantic-specific feature interaction and hu(·)",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "output\nthe euclidean distance matrix D∗ between each sample"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "shared feature interaction.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "and the nearest samples, denoted as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "On the one hand,\nto capture intra-modal dynamics and filter",
          "8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "movement and history values with the increasing of\ntraining",
          "9": "where τ\nserves as a temperature hyper-parameter\nfor altering"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "epochs z,\nrepresented as:",
          "9": "the strength of penalties on hard samples due to the modality"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "gap\n[86].\nThen we\ntake\nInfoNCE\n[56]\nform function\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pi\nr ≥ 1\n∗|0 = pi\n∗|1 = ... = pi\n∗|r = yi\ngt,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "compute the intra-sample contrastive learning loss LintraCL,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "z − 1",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pi",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 z\npi\npi\n∗|z =\n∗|z−1 +\n∗|z",
          "9": "which is formulated as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "z",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(15)\nz − 1",
          "9": "S+"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pi\n(pi",
          "9": "(17)\nLintraCL = −E(Fsp,Fsh)∼B log"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 z\n=\n∗|z−1 +\n∗|z−1 + ∆i\n∗|z)",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "z",
          "9": "S+ + S−"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "= pi",
          "9": "Simultaneously,\nthe\ninter-sample\ncontrastive\nlearning\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1 z\n∆i\nz > r\n∗|z−1 +\n∗|z,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "adopted for the multimodal representations FM among diverse"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "where\nthe momentum-based updating policy\nis\nintended to",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "samples\nunder\nthe\nsupervision\nof multimodal\nground\ntruth"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relieve\nthe\nfluctuations\ncaused\nby\nnoisy\nsamples\nand\nthe",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "labels to further excavate the affective information inspired by"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "updating process of pseudo labels is started after the rth epoch",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "SupCon [60]. Given a mini-batch of B = {F i\ni=1, we divide\nM }B"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for more stable label generation and better convergence.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "the representations into positive and negative sets according to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For each subtask,\nthe specific-semantic labels are generated",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "the labels annotated as\nsentiment\nscores or emotion classes."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to reveal\nthe intra-modal connections among different samples",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "For sentiment analysis task, we categorize the representations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "while\nthe\nshared-semantic\nlabels\nare\nexpected\nto\nshow the",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "based on sentiment classes with a label threshold which decide"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "inter-modal\ncommonality. Comparing with\nthe multimodal",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "the class each sentiment scores belongs to. While for emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ground truth labels,\nthe generated pseudo labels are used to",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "recognition and detection classes, we treat\nthe representations"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "guide the learning of various semantic-centric representations",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "with\nthe\nsame\nclass\nas\nthe\npositive\npairs while\nthe\nother"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in a weakly-supervised manner. Note that\nthe semantic-centric",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "representations as the negative pairs. Note that such setting is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pseudo labels are allowed to be zero for\nindividual\nsamples",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "suitable for multi-label emotion recognition dataset [4], where"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "when there are few unimodal\nfeatures or\nrare paired features",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "we treat\nthe representations with non-empty intersection set"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "related to the downstream prediction [14],\n[35].",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "of\nemotion\nannotations\nas\nthe\npositive\npairs. To make\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "representations from various classes more discriminative with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F\n.\nSemantic-centric Contrastive Learning",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "the\nguidance\nof multimodal\nlabels,\ndenoting\npositive\npairs"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To promote the disentanglement of semantics and encourage",
          "9": "sets as P ∈ {F j"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "the inter-sample contrastive learning loss\nM },"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the feature interaction of unimodal and multimodal sub-tasks,",
          "9": "LinterCL is computed as:"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "INCLUDING DATA SPLITTING AND HYPER-PARAMETERS SETTINGS. NOTE THAT FOR LEARNING"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "CMU-MOSEI"
        },
        {
          "TABLE II": "16,322"
        },
        {
          "TABLE II": "1,871"
        },
        {
          "TABLE II": "4,659"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "10"
        },
        {
          "TABLE II": "64"
        },
        {
          "TABLE II": "5e-6/1e-4"
        },
        {
          "TABLE II": "1e-2"
        },
        {
          "TABLE II": "Constant"
        },
        {
          "TABLE II": "0.3"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "MAE↓"
        },
        {
          "TABLE III": "0.952"
        },
        {
          "TABLE III": "0.978"
        },
        {
          "TABLE III": "0.925"
        },
        {
          "TABLE III": "0.931"
        },
        {
          "TABLE III": "0.951"
        },
        {
          "TABLE III": "0.939"
        },
        {
          "TABLE III": "0.948"
        },
        {
          "TABLE III": "0.918"
        },
        {
          "TABLE III": "0.752"
        },
        {
          "TABLE III": "0.730"
        },
        {
          "TABLE III": "0.731"
        },
        {
          "TABLE III": "0.738"
        },
        {
          "TABLE III": "0.705"
        },
        {
          "TABLE III": "0.705"
        },
        {
          "TABLE III": "0.698"
        },
        {
          "TABLE III": "0.685"
        },
        {
          "TABLE III": "0.632"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "MAE↓"
        },
        {
          "TABLE IV": "0.590"
        },
        {
          "TABLE IV": "0.444"
        },
        {
          "TABLE IV": "0.429"
        },
        {
          "TABLE IV": "0.443"
        },
        {
          "TABLE IV": "0.447"
        },
        {
          "TABLE IV": "0.438"
        },
        {
          "TABLE IV": "0.440"
        },
        {
          "TABLE IV": "0.442"
        },
        {
          "TABLE IV": "0.435"
        },
        {
          "TABLE IV": "0.416"
        },
        {
          "TABLE IV": "0.431"
        },
        {
          "TABLE IV": "0.372"
        },
        {
          "TABLE IV": "0.366"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C.\nImplementation Details": "",
          "D. Baselines": "We\nreport\nthe\nresults\nof\nbaseline models\nby\nreproduc-"
        },
        {
          "C.\nImplementation Details": "",
          "D. Baselines": "ing the corresponding open-source codes without extra men-"
        },
        {
          "C.\nImplementation Details": "All experiments are conducted on a single A100 GPU with",
          "D. Baselines": "tion.\nThe\nbaseline models\nare\nbroadly\ncategorized\ninto:"
        },
        {
          "C.\nImplementation Details": "CUDA 11.8. For each dataset, we convert\nthe raw video into",
          "D. Baselines": "LF-\n(1)\nEarly\nand\nlate\nfusion:\nEF-LSTM,\nLF-LSTM,"
        },
        {
          "C.\nImplementation Details": "LMDB database\nfor higher\naccess\nspeed in the\nend-to-end",
          "D. Baselines": "Transformer;\n(2) Tensor-based\nfusion models: TFN [15],"
        },
        {
          "C.\nImplementation Details": "training and inference stage as Lei et al.\n[101]. Note that\nfor",
          "D. Baselines": "LMF [96];\n(3) Explicitly\nintra-\nand\ninter-modal\ndynamics"
        },
        {
          "C.\nImplementation Details": "fair comparison with baselines, we remain the same language",
          "D. Baselines": "manipulation models: MFN [97], MFM [98], C-MFN [10],"
        },
        {
          "C.\nImplementation Details": "model with the\nstate-of-the-art models\nfor\neach MAC task.",
          "D. Baselines": "EmoEmbs\n[19];\n(4) Attention-based\nfusion models: MulT"
        },
        {
          "C.\nImplementation Details": "Following Gkoumas et al.\n[5], we present fifty-times random",
          "D. Baselines": "[17], MISA [23], MAG-BERT/MAG-XLNet\n[99], TBJE"
        },
        {
          "C.\nImplementation Details": "grid search to find the best hyper-parameters and we report the",
          "D. Baselines": "[104], FE2E/MESM [92], ME2ET [105], I-Attention [106],"
        },
        {
          "C.\nImplementation Details": "average results of 5 runs as the final performance. The splits",
          "D. Baselines": "BBFN [107], MuLoT [108];\n(5) Knowledge guidance mod-"
        },
        {
          "C.\nImplementation Details": "of dataset and the settings of hyper-parameters are shown in",
          "D. Baselines": "els: Self-MM [27], MTMD [24];\n(6) Contrastive\nlearning"
        },
        {
          "C.\nImplementation Details": "Table II. We adopt AdamW [102] as the optimizer and utilize",
          "D. Baselines": "based models: MMIM [28], MMCL [100];\n(7) Graph neu-"
        },
        {
          "C.\nImplementation Details": "a warmup strategy for all\nlearning rates at\nthe first epoch. For",
          "D. Baselines": "ral network based models: Graph-MFN [4], DialogueGCN"
        },
        {
          "C.\nImplementation Details": "regression task, we utilize the minimum loss of validation set",
          "D. Baselines": "[109], MMGCN [110], COGMEN [111], CORECT [112];"
        },
        {
          "C.\nImplementation Details": "in the training stage as the reference to get the best parameters,",
          "D. Baselines": "(8) Context\naware models:\nbc-LSTM [113], CMN [114],"
        },
        {
          "C.\nImplementation Details": "while\nfor\nrecognition\nand\ndetection\ntasks, we\nutilize w-F1",
          "D. Baselines": "ICON [115], DialogueRNN [116], DialogueCRN [117],"
        },
        {
          "C.\nImplementation Details": "score of validation set as the one to determine the best model",
          "D. Baselines": "Multilogue-Net\n[118];\n(9) Data\naugmentation models: AV-"
        },
        {
          "C.\nImplementation Details": "due to the confidence calibration issue [103].",
          "D. Baselines": "MC [90], MissModal\n[29]."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": ""
        },
        {
          "TABLE V": "b-F1"
        },
        {
          "TABLE V": "47.1"
        },
        {
          "TABLE V": "47.7"
        },
        {
          "TABLE V": "49.4"
        },
        {
          "TABLE V": "47.5"
        },
        {
          "TABLE V": "49.6"
        },
        {
          "TABLE V": "49.3"
        },
        {
          "TABLE V": "51.1"
        },
        {
          "TABLE V": "52.6"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "b-F1"
        },
        {
          "TABLE VI": "47.0"
        },
        {
          "TABLE VI": "49.7"
        },
        {
          "TABLE VI": "48.7"
        },
        {
          "TABLE VI": "53.7"
        },
        {
          "TABLE VI": "58.4"
        },
        {
          "TABLE VI": "52.0"
        },
        {
          "TABLE VI": "58.7"
        },
        {
          "TABLE VI": "61.1"
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": ""
        },
        {
          "TABLE VI": "the"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "ON CMU-MOSEI FOR MULTIMODAL EMOTION RECOGNITION TASK IN"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "THE CONVERSATION SCENARIO. † INDICATES RESULTS FROM [112]."
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "Happy"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "67.8"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "65.9"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "70.9"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "71.4"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": "71.9"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "RECOGNITION TASK IN THE CONVERSATION SCENARIO. THE WEIGHTED F1 (W-F1) OF EACH EMOTION CLASS IS REPORTED FOR FINE-GRAINED"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "COMPARISON. THE BASELINE MODELS ARE REPRODUCED WITH THE CORRESPONDING OPEN-SOURCE CODES."
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": ""
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "Happy"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "32.4"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "29.7"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "31.6"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "34.8"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "41.9"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "41.1"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "60.1"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "53.5"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "56.6"
        },
        {
          "PERFORMANCE COMPARISON BETWEEN SEMANTICMAC AND BASELINES ON IEMOCAP AND MELD DATASET FOR MULTIMODAL EMOTION": "53.4"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IX": ""
        },
        {
          "TABLE IX": ""
        },
        {
          "TABLE IX": ""
        },
        {
          "TABLE IX": ""
        },
        {
          "TABLE IX": "w-Acc ↑"
        },
        {
          "TABLE IX": "65.23"
        },
        {
          "TABLE IX": "-"
        },
        {
          "TABLE IX": "-"
        },
        {
          "TABLE IX": "64.71"
        },
        {
          "TABLE IX": "70.61"
        },
        {
          "TABLE IX": "71.68"
        },
        {
          "TABLE IX": "72.43"
        },
        {
          "TABLE IX": "73.97"
        },
        {
          "TABLE IX": "75.10"
        },
        {
          "TABLE IX": "74.48"
        },
        {
          "TABLE IX": "75.60"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "ABLATION STUDY OF SEMANTICMAC WITH BERT AS THE LANGUAGE"
        },
        {
          "advanced results": "capturing the contradictory correlation among the punchline",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "MODEL ON CMU-MOSEI DATASET."
        },
        {
          "advanced results": "and context\nto predict",
          "indicate the efficiency of SemanticMAC in": "the humorous and sarcastic anchors.",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "Acc7↑"
        },
        {
          "advanced results": "F\n. Ablation Study",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "54.5"
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "To\nfurther\nreveal\nthe",
          "indicate the efficiency of SemanticMAC in": "contributions\nof",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.9"
        },
        {
          "advanced results": "inside",
          "indicate the efficiency of SemanticMAC in": "the proposed architecture, we perform ablation study",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.8"
        },
        {
          "advanced results": "for SemanticMAC on CMU-MOSEI dataset as shown in Table",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.6"
        },
        {
          "advanced results": "X. Firstly, when capturing intra- and inter-modal dynamics in",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "(2) Semantic-centric Gated Feature Interaction (SGFI)"
        },
        {
          "advanced results": "Affective Perceriver",
          "indicate the efficiency of SemanticMAC in": "and SGFI,\nthe\nlearnable",
          "TABLE X": "54.0"
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.7"
        },
        {
          "advanced results": "dings Ef r and the modality embeddings Emd are productive",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "54.0"
        },
        {
          "advanced results": "in assigning temporal",
          "indicate the efficiency of SemanticMAC in": "information for both audio and vision",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.5"
        },
        {
          "advanced results": "modalities and revealing the type of modality in multimodal",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "(3) Semantic-centric Label Generation (SCLG)"
        },
        {
          "advanced results": "fusion. Moreover, multi-query attention and Bridge Tokens B",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.5"
        },
        {
          "advanced results": "are\neffective",
          "indicate the efficiency of SemanticMAC in": "in decreasing the modality gap and exploiting",
          "TABLE X": "53.3"
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.0"
        },
        {
          "advanced results": "the common semantics when conducting cross-modal attention",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "in feature interaction. The gated ReLU succeeds",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "(4) Semantic-centric Contrastive Learning (SCCL)"
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.1"
        },
        {
          "advanced results": "unrelated\nnoise,\nleading",
          "indicate the efficiency of SemanticMAC in": "to\nperformance",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.6"
        },
        {
          "advanced results": "lacking of\nthe gated mechanism.",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": ""
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "",
          "TABLE X": "53.0"
        },
        {
          "advanced results": "",
          "indicate the efficiency of SemanticMAC in": "Additionally, SemanticMAC adopts semantic-centric labels",
          "TABLE X": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "#1\n1.0",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.12",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Density",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.8",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.6\n0.00",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Precision",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "#4",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.4",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.12",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Density",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fusion_text\n0.2",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fusion_audio",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fusion_vision",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.00\n0.0",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "#7\nRecall",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.12",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ft+\nFt-",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1.0",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fa+\nFa-",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Density\nFv+\nFv-",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.8",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.00",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.6",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "#10",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.12",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.4",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Density",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.2",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.00",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.0\n3\n2",
          "14": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "0.0\n0.2\n0.4\n0.6\n0.8\n1.0",
          "14": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "in the\nfeature\nspace where +/− denotes\nrepresentations Ft, Fa, Fv",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "the positive/negative\nsentiment\nintensity with pseudo labels. The\nright ones\nare\nthe"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "variations of semantic-centric labels distribution on (b) multimodal sentiment analysis task and (c) multimodal emotion recognition task with #1,4,7,10 epoch."
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "resentations\naccording to semantics\nand multimodal ground",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "C. Distribution of Semantic-centric Labels"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "truth, so that optimize without contrastive learning largely hurt",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": ""
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "the final performance.",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "We present case study to concretize the issue of\nsemantic"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "mismatch as\nshown in Table\nI. For\nsentiment\nanalysis\n(#1-"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "V. FURTHER ANALYSIS",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": ""
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "#3),\nthe\nfor various\nintensity of generated pseudo labels p∗"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "A. Effect\nin tackling semantic imbalance",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "modalities are quite different, even contradictory due to the"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "Aiming\nat\naddressing\nthe\nissue\nof\nsemantic\nimablance",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "exclusive\ncontent\ncontained in semantic-specific\nrepresenta-"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "shown in Figure 2, we visualize\nthe Precision-Recall\ncurve",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "tions. The\nsimilar\ntrend\ncan\nbe\nobserved when\nclassifying"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "of SemanticMAC with BERT on CMU-MOSEI\nto reveal\nthe",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "emotion class\nfor unimodal\nand multimodal\nrepresentations"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "semantic\nabundance of various unimodal\nrepresentations\nas",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "in the examples\n(#4-#6) of multimodal emotion recognition."
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "shown in the Figure 6(a). By replacing the manual acoustic",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "Besides,\nthe\nsemantic-centric\nlabels\nare discrepant\nfrom the"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "and\nvisual\nfeatures with\nImageBind\nin SemanticMAC,\nthe",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "implicitly validating\nvalue or class of ground truth labels ygt,"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "contribution of unimodal features from different modalities are",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "that the pseudo labels are productive in capturing the semantics"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "well balanced in the final multimodal representations. Besides,",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "information contained in diverse representations."
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "Affective Perceiver competently integrates the affective infor-",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "To further verify the effectiveness of semantic-centric label"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "mation into learnable unimodal tokens, increasing the semantic",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "in tackling semantic mismatch, we visualize\nthe generation"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "abundance of acoustic and visual representations. As the end-",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "process of semantic-centric labels during training on sub-tasks"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "to-end\ntraining\nprocesses,\nthe\nissue\nof\nsemantic\nimbalance",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "∗ ∈ {S, T, A, V }\nin Figure\n6. Notes\nthat\nthe multimodal"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "can be practically tackled in the stage of multimodal\nfusion,",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "labels of the 1st epoch denotes the ground truth labels. As the"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "which further demonstrates the effectiveness of\nthe proposed",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "training stage proceeds,\nthe distributions of\nsemantic-centric"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "architecture.",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "labels for diverse semantic-specific and -shared representations"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "vary in distinctive ways, demonstrating the pseudo labels can"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "B. Visualization in the Embedding Space",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": ""
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "be\ngenerated\naccording\nto\ndifferent\nsemantic-centric\nrepre-"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "To\nbetter\nreveal\nthe\ndistribution\nof\nthe\nsemantic-specific",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "sentations. For multimodal sentiment analysis in Figure 6(b),"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "features, we utilize T-SNE [121]\nto visualize representations",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "the pseudo labels polarize with more discriminative sentiment"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "trained on CMU-MOSEI in the embedding space. As shown in",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "tendency, where more\nsamples\nare\nassigned with\npositive"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "Figure 6(a), for u ∈ {t, a, v}, the semantic-specific representa-",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "or negative intensity. For multimodal emotion recognition in"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "tions F u\nsp are discriminative according to the semantic-centric",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "Figure 6(c),\nthe\nfrequency of\nemotion classes\nin semantic-"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "labels, regardless of the observable modality gap. Meanwhile,",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "centric\nlabels\nare\nrearranged\nby\nthe\naffective\nsemantics\nin"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "the\nrepresentations with\nconsistent\nsentiment\nfrom various",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "various modalities. Besides,\nfor\nthe emotion classes\nsuch as"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "modalities\nare well\nclassified into the opposite\nends of\nthe",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "happy and sad which can be expressed explicitly in language,"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "embedding\nspace,\nindicating\nthe\nproductivity\nof\nsemantic-",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "the\npseudo\nlabels\nare more\nfrequently\narranged\nfor\ntextual"
        },
        {
          "of\nthe fusion multimodal\nrepresentations and the unimodal\nrepresentations": "centric cross-modal\nfeature interaction.",
          "training with BERT. The lower\nleft one is\nthe distribution of\nsemantic-specific": "modality. While for the emotion more likely to be revealed in"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "expressive face or\ntune such as surprise and fear,\nthe pseudo",
          "15": "VI. CONCLUSION"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "labels are more presented in audio and vision modalities.",
          "15": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "15": "In this paper, we proposed a novel end-to-end multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "15": "affective computing framework, SemanticMAC,\nto effectively"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. Unimodal Feature Performance Comparison",
          "15": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "15": "learn semantic-specific\nand -shared representations with the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Aiming at validating the effectiveness of acoustic and visual",
          "15": "supervision of the generated semantic-centric labels. Extensive"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features\nlearned by ImageBind and Affective Perceiver, we",
          "15": "experiments on 7 public video-based datasets in 4 downstream"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conduct\nin the\narchitecture by\nexperiments on Xu\nand Fu",
          "15": "MAC tasks\ndemonstrate\nthe\neffectiveness\nof\nthe\nproposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "linear probing [122]\nfor\nthe sole audio and vision modality",
          "15": "approach. The\nvisualization\nand\nablation\nstudy\nconsistently"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "u ∈ {a, v}\non CMU-MOSEI. Note\nthat\nthe\nprediction\nof",
          "15": "reveals that SemanticMAC productively tackles the challenges"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "unimodal features is attained through conducting average pool-",
          "15": "of\nsemantic\nimbalance\nand\nsemantic mismatch\nfor\nvarious"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing and linear projection trained on the learned and manual",
          "15": "modalities."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features with other parameters frozen. As shown in Table XI,",
          "15": "In\nthe\nfuture, we will\nutilize\nrecent\nemerging\nlarge\nlan-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "compared with manually extracted features, both of unimodal",
          "15": "guage models to promote higher performance of the proposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "embeddings Xu and representations Fu achieve superior per-",
          "15": "method,\nsince SemnaticMAC has\nbeen\nverified\nuniversally"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formance on both regression and classification metrics. The",
          "15": "across different language models. Moreover, we tend to extend"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance\nreveals\nthe multimodal\nalignment\nand\nof Xu",
          "15": "the end-to-end pipeline for multimodal affective computing in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "generalization\npower\nof\nImageBind. Additionally, Affective",
          "15": "more downstream applications of human-AI\ninteraction."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Perceiver productively filters\nthe noise of Xu and integrates",
          "15": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the affective information in Fu, leading to higher performance.",
          "15": "REFERENCES"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "15": "[1]\nS. Poria, D. Hazarika, N. Majumder, and R. Mihalcea, “Beneath the"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "ones trained on videos with fixed frames. Besides, either",
          "frames outperform the": "too"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "few or too much frames are not beneficial for the information",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "extraction\nof Affective Perceiver",
          "frames outperform the": "or\nthe\nfeature\ninteraction"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "among different modalities,",
          "frames outperform the": "remaining consistent\ntrend with"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "sparse",
          "frames outperform the": "to dense uniform sampling [101]. This\nindicates\nthe"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "importance of balancing the information redundancy and se-",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "mantic abundance for the performance of affective computing",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "model. Therefore,",
          "frames outperform the": "the ability of handling videos with various"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "length\nresults\nin",
          "frames outperform the": "higher\nrobustness\nand\napplicability when"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "adopting SemanticMAC in diverse downstream scenarios.",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": "TABLE XII"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": "PERFORMANCE COMPARISON BETWEEN THE VIDEO DATA WITH THE"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": "SETTINGS OF FIXED AND VARIOUS FRAMES FOR SEMANTICMAC."
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "Frame Setting",
          "frames outperform the": "Acc7↑\nAcc2↑\nF1↑\nMAE↓\nCorr↑"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "Fixed\n5",
          "frames outperform the": "52.9\n86.0\n85.9\n0.533\n0.773"
        },
        {
          "model\ntrained on videos with various": "(frames/video)\n100",
          "frames outperform the": "52.6\n85.8\n85.7\n0.536\n0.775"
        },
        {
          "model\ntrained on videos with various": "",
          "frames outperform the": ""
        },
        {
          "model\ntrained on videos with various": "Various frames",
          "frames outperform the": "54.5\n87.3\n87.2\n0.518\n0.792"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Perceiver productively filters\nthe noise of Xu and integrates",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "the affective information in Fu, leading to higher performance.",
          "more downstream applications of human-AI": "",
          "interaction.": "REFERENCES"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "[1]",
          "interaction.": "S. Poria, D. Hazarika, N. Majumder, and R. Mihalcea, “Beneath the"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "tip of\nthe iceberg: Current challenges and new directions in sentiment"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "TABLE XI",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "analysis research,” IEEE Trans. Affect. Comput., 2020."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "PERFORMANCE COMPARISON THROUGH LINEAR PROBING OF ACOUSTIC",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "[2]",
          "interaction.": "T. Baltru ˚A¡aitis, C. Ahuja, and L.-P. Morency, “Multimodal machine"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "AND VISUAL FEATURES LEARNED BY SEMANTICMAC AND EXTRACTED",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "learning: A survey and taxonomy,” IEEE Trans. Pattern Anal. Mach."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "BY COMMONLY-USED MANUAL TOOLKIT CMU-MULTIMODALSDK.",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Intell., vol. 41, no. 2, pp. 423–443, 2019."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "[3]",
          "interaction.": "S. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of affective"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Unimodal Feature\nAcc3↑\nAcc2↑\nF1↑\nMAE↓\nCorr↑",
          "more downstream applications of human-AI": "",
          "interaction.": "computing: From unimodal analysis to multimodal fusion,” Inf. Fusion,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "vol. 37, pp. 98–125, 2017."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Manual\n42.3\n64.7\n60.8\n0.824\n0.196",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "[4] A. Zadeh,",
          "interaction.": "P.\nP. Liang,\nS.\nPoria, E. Cambria,\nand L.-P. Morency,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Audio\n46.1\n71.4\n69.8\n0.798\n0.331\nXa",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "“Multimodal\nlanguage analysis in the wild: CMU-MOSEI dataset and"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "48.5\n72.3\n71.3\n0.774\n0.433\nFa",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "interpretable\ndynamic\nfusion\ngraph,”\nin Proc. Annu. Meet. Assoc."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Manual\n43.5\n64.4\n60.5\n0.818\n0.204",
          "more downstream applications of human-AI": "",
          "interaction.": "Comput. Linguist., Jul. 2018, pp. 2236–2246."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Vision\n44.9\n73.9\n72.9\n0.790\n0.351\nXv",
          "more downstream applications of human-AI": "",
          "interaction.": "[5] D. Gkoumas, Q. Li, C. Lioma, Y. Yu, and D. Song, “What makes the"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "50.3\n74.5\n73.6\n0.770\n0.450\nFv",
          "more downstream applications of human-AI": "",
          "interaction.": "difference? an empirical comparison of fusion strategies for multimodal"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "language analysis,” Inf. Fusion, vol. 66, pp. 184–197, 2021."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "[6] H. Mao, Z. Yuan, H. Xu, W. Yu, Y. Liu,\nand K. Gao,\n“M-SENA:"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "An integrated platform for multimodal\nsentiment\nanalysis,”\nin Proc."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "E.\nInfluence of Fixed and Varying Video Length",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Annu. Meet. Assoc. Comput. Linguist.\nInt. Jt. Conf. Nat., May 2022,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "To demonstrate the effectiveness of SemanticMAC in pro-",
          "more downstream applications of human-AI": "",
          "interaction.": "pp. 204–213."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "[7] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower Provost,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "cessing videos with various length, we conduct experiments in",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "S. Kim, J. Chang, S. Lee, and S. Narayanan, “Iemocap: Interactive emo-"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "the settings of both fixed and various frames (in both audio and",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "tional dyadic motion capture database,” Lang. Resour. Eval., vol. 42,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "vision streams) on CMU-MOSEI, which has a wide range of",
          "more downstream applications of human-AI": "",
          "interaction.": "pp. 335–359, 12 2008."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "[8]",
          "interaction.": "S.\nPoria, D. Hazarika, N. Majumder, G. Naik,\nE. Cambria,\nand"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "video lengths from 0.7 s to 108.9 s [4]. As shown in XII,\nthe",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "R. Mihalcea, “MELD: A multimodal multi-party dataset\nfor emotion"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "model\ntrained on videos with various\nframes outperform the",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "recognition in conversations,”\nin Proc. Annu. Meet. Assoc. Comput."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "ones trained on videos with fixed frames. Besides, either\ntoo",
          "more downstream applications of human-AI": "",
          "interaction.": "Linguist., Jul. 2019, pp. 527–536."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "[9] G. A.V., M. T., P. D., and U. E., “Multimodal emotion recognition with"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "few or too much frames are not beneficial for the information",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "deep learning: Advancements, challenges, and future directions,” Inf."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "extraction\nof Affective Perceiver\nor\nthe\nfeature\ninteraction",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Fusion, vol. 105, p. 102218, 2024."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "among different modalities,\nremaining consistent\ntrend with",
          "more downstream applications of human-AI": "",
          "interaction.": "[10] M. K. Hasan, W. Rahman, A. Bagher Zadeh, J. Zhong, M. I. Tanveer,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "L.-P. Morency, and M. E. Hoque, “UR-FUNNY: A multimodal\nlan-"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "sparse\nto dense uniform sampling [101]. This\nindicates\nthe",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "guage dataset for understanding humor,” in Prof. Conf. Empir. Methods"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "importance of balancing the information redundancy and se-",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Nat. Lang. Process.\nInt. Jt. Conf., Nov. 2019, pp. 2046–2056."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "mantic abundance for the performance of affective computing",
          "more downstream applications of human-AI": "[11]",
          "interaction.": "S. Castro, D. Hazarika, V. P´erez-Rosas, R. Zimmermann, R. Mihalcea,"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Obviously\nand S. Poria, “Towards multimodal sarcasm detection (an"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "model. Therefore,\nthe ability of handling videos with various",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "perfect paper),”\nin Proc. Annu. Meet. Assoc. Comput. Linguist.,\nJul."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "length\nresults\nin\nhigher\nrobustness\nand\napplicability when",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "2019, pp. 4619–4629."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "adopting SemanticMAC in diverse downstream scenarios.",
          "more downstream applications of human-AI": "[12] M.",
          "interaction.": "Soleymani, D. Garcia, B.\nJou, B.\nSchuller,\nS.-F. Chang,\nand"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Image Vis.\nM. Pantic,\n“A survey of multimodal\nsentiment\nanalysis,”"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "TABLE XII",
          "more downstream applications of human-AI": "",
          "interaction.": "Comput., vol. 65, pp. 3–14, 2017."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "PERFORMANCE COMPARISON BETWEEN THE VIDEO DATA WITH THE",
          "more downstream applications of human-AI": "[13]",
          "interaction.": "Z. Lian, L. Chen, L. Sun, B. Liu, and J. Tao, “Gcnet: Graph completion"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "SETTINGS OF FIXED AND VARIOUS FRAMES FOR SEMANTICMAC.",
          "more downstream applications of human-AI": "",
          "interaction.": "IEEE\nnetwork for\nincomplete multimodal\nlearning in conversation,”"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Trans. Pattern Anal. Mach. Intell., vol. 45, no. 7, pp. 8419–8432, 2023."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "[14]",
          "interaction.": "P. P. Liang, A. Zadeh,\nand L.-P. Morency,\n“Foundations & trends"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Frame Setting\nAcc7↑\nAcc2↑\nF1↑\nMAE↓\nCorr↑",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "in multimodal machine\nlearning:\nPrinciples,\nchallenges,\nand\nopen"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "Fixed\n5\n52.9\n86.0\n85.9\n0.533\n0.773",
          "more downstream applications of human-AI": "",
          "interaction.": "questions,” ACM Comput. Surv., apr 2024."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "(frames/video)\n100\n52.6\n85.8\n85.7\n0.536\n0.775",
          "more downstream applications of human-AI": "",
          "interaction.": "[15] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P. Morency, “Tensor"
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "fusion\nnetwork\nfor multimodal\nsentiment\nanalysis,”\nin Proc. Conf."
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "54.5\n87.3\n87.2\n0.518\n0.792\nVarious frames",
          "more downstream applications of human-AI": "",
          "interaction.": ""
        },
        {
          "generalization\npower\nof\nImageBind. Additionally, Affective": "",
          "more downstream applications of human-AI": "",
          "interaction.": "Empir. Methods Nat. Lang. Process., Sep. 2017, pp. 1103–1114."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[16] Y. Wang, Y. Shen, Z. Liu, P. P. Liang, A. Zadeh, and L.-P. Morency,",
          "16": "[38] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language transformer"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Words\ncan shift: Dynamically adjusting word representations using",
          "16": "Int. Conf. Mach.\nwithout convolution or\nregion supervision,” in Proc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nonverbal behaviors,” in Proc. AAAI Conf. Artif. Intell., vol. 33, no. 01,",
          "16": "Learn., vol. 139, 18–24 Jul 2021, pp. 5583–5594."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019, pp. 7216–7223.",
          "16": "[39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[17] Y.-H. H. Tsai, S. Bai, P. P. Liang,\nJ. Z. Kolter, L.-P. Morency,\nand",
          "16": "G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "R. Salakhutdinov, “Multimodal\ntransformer\nfor unaligned multimodal",
          "16": "“Learning transferable visual models\nfrom natural\nlanguage supervi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "language sequences,” in Proc. Annu. Meet. Assoc. Comput. Linguist.,",
          "16": "sion,” in Proc.\nInt. Conf. Mach. Learn., vol. 139, 18–24 Jul 2021, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Jul. 2019, pp. 6558–6569.",
          "16": "8748–8763."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[18] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, and L.-P. Morency,",
          "16": "[40]\nZ. Li, B. Xu, C. Zhu, and T. Zhao,\n“CLMLF:a contrastive\nlearning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Multi-attention recurrent network for human communication compre-",
          "16": "and multi-layer fusion method for multimodal sentiment detection,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hension,” in Proc. AAAI Conf. Artif.\nIntell., vol. 32, no. 1, Apr. 2018.",
          "16": "Find. Assoc. Comput. Linguist., Jul. 2022, pp. 2282–2294."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[41] G. Hu, T.-E. Lin, Y. Zhao, G. Lu, Y. Wu, and Y. Li, “UniMSE: Towards"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] W. Dai, Z. Liu, T. Yu, and P. Fung, “Modality-transferable emotion",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "unified multimodal\nsentiment\nanalysis\nand\nemotion\nrecognition,”\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "embeddings\nfor\nlow-resource multimodal\nemotion\nrecognition,”\nin",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "Proc. Conf. Empir. Methods Nat. Lang. Process., Dec. 2022, pp. 7837–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. Conf. Asia-Pacific Assoc. Comput. Linguist.\nInt. Jt. Conf. Nat.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "7851."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lang. Process., Dec. 2020, pp. 269–280.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[42] M. A. Manzoor, S. Albarri, Z. Xian, Z. Meng, P. Nakov, and S. Liang,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20] M. K. Hasan, S. Lee, W. Rahman, A. Zadeh, R. Mihalcea, L.-P.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "“Multimodality representation learning: A survey on evolution, pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Morency,\nand E. Hoque,\n“Humor\nknowledge\nenriched\ntransformer",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "training and its applications,” ACM Trans. Multimedia Comput. Com-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for understanding multimodal humor,” Proc. AAAI Conf. Artif.\nIntell.,",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "mun. Appl., vol. 20, no. 3, oct 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 35, no. 14, pp. 12 972–12 980, May 2021.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[43] R. Lin and H. Hu, “Adapt and explore: Multimodal mixup for\nrepre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer, “Covarep",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "sentation learning,” Inf. Fusion, vol. 105, p. 102216, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "- a collaborative voice analysis repository for speech technologies,” in",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[44]\nT. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Int. Conf. Acoust. Speech Signal Process., 2014, pp. 960–964.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "of word representations in vector space,” arxiv:1301.3781, 2013."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22]\niMotions 2017, “Facial expression analysis,” [Online], https://imotions.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[45]\nT. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “Openface"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "com/.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "2.0: Facial behavior analysis toolkit,” in IEEE Int. Conf. Autom. Face"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23] D. Hazarika, R. Zimmermann, and S. Poria, “Misa: Modality-invariant",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "Gesture Recognit., 2018, pp. 59–66."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and-specific\nrepresentations\nfor multimodal\nsentiment\nanalysis,”\nin",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[46] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improving"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. ACM Int. Conf. Multimed., 2020, pp. 1122–1131.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "language understanding by generative pre-training,” 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] R. Lin and H. Hu, “Multi-task momentum distillation for multimodal",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[47] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentiment analysis,” IEEE Trans. Affect. Comput., pp. 1–18, 2023.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25] M. S. M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly,",
          "16": "J. Uszkoreit,\nand N. Houlsby,\n“An\nimage\nis worth\n16x16 words:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Assessing generative models via precision and recall,” in Proc. Adv.",
          "16": "Transformers for image recognition at scale,” in Prof. Int. Conf. Learn."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Neural\nInf. Process. Syst., vol. 31, 2018.",
          "16": "Represent., 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[26]\nT. Kynk¨a¨anniemi,\nT. Karras,\nS.\nLaine,\nJ.\nLehtinen,\nand\nT. Aila,",
          "16": "[48] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Improved precision and recall metric for assessing generative models,”",
          "16": "A. Mohamed, “Hubert: Self-supervised speech representation learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Proc. Adv. Neural\nInf. Process. Syst., vol. 32, 2019.",
          "16": "IEEE/ACM Trans. Audio,\nby masked\nprediction\nof\nhidden\nunits,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[27] W. Yu, H. Xu, Z. Yuan,\nand\nJ. Wu,\n“Learning modality-specific",
          "16": "Speech, Language Process., vol. 29, pp. 3451–3460, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representations with self-supervised multi-task learning for multimodal",
          "16": "[49] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, “Transformers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentiment analysis,” in Proc. AAAI Conf. Artif.\nIntell., vol. 35, no. 12,",
          "16": "are RNNs: Fast autoregressive transformers with linear attention,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2021, pp. 10 790–10 797.",
          "16": "Proc.\nInt. Conf. Mach. Learn., vol. 119, 13–18 Jul 2020, pp. 5156–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[28] W. Han, H. Chen, and S. Poria, “Improving multimodal fusion with hi-",
          "16": "5165."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "erarchical mutual\ninformation maximization for multimodal sentiment",
          "16": "[50] H. Tan and M. Bansal, “LXMERT: Learning cross-modality encoder"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "analysis,”\nin Proc. Conf. Empir. Methods Nat. Lang. Process., Nov.",
          "16": "representations from transformers,” in Proc. Conf. Empir. Methods Nat."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2021, pp. 9180–9192.",
          "16": "Lang. Process.\nInt. Jt. Conf., Nov. 2019, pp. 5100–5111."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[51] N. Shazeer,\n“Fast\ntransformer\ndecoding: One write-head\nis\nall\nyou"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[29] R. Lin\nand H. Hu,\n“MissModal:\nIncreasing Robustness\nto Missing",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "need,” arxiv:1911.02150, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Modality in Multimodal Sentiment Analysis,” Trans. Assoc. Comput.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[52]\nP. Xu, X. Zhu, and D. A. Clifton, “Multimodal\nlearning with trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Linguist., vol. 11, pp. 1686–1702, 12 2023.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "formers: A survey,” IEEE Trans. Pattern Anal. Mach.\nIntell., vol. 45,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[30]\nJ. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "no. 10, pp. 12 113–12 132, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for word representation,”\nin Proc. Conf. Empir. Methods Nat. Lang.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[53] A. Radford,\nJ. W. Kim, T. Xu, G. Brockman, C. Mcleavey,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Process., 2014, pp. 1532–1543.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "I. Sutskever, “Robust speech recognition via large-scale weak supervi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[31]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“BERT: Pre-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "sion,” in Proc.\nInt. Conf. Mach. Learn., vol. 202, 23–29 Jul 2023, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "training of deep bidirectional transformers for language understanding,”",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "28 492–28 518."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Am. Chapter\nAssoc. Comput.\nLinguist.: Hum.\nLang.\nin Conf. N.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[54]\nJ.-B. Alayrac, J. Donahue, P. Luc et al., “Flamingo: a visual\nlanguage"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Technol., Jun. 2019, pp. 4171–4186.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "model\nfor\nfew-shot\nlearning,” in Proc. Adv. Neural\nInf. Process. Syst.,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[32] R. Lin and H. Hu, “Dynamically shifting multimodal\nrepresentations",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "via hybrid-modal attention for multimodal\nsentiment analysis,” IEEE",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[55] R. Hadsell,\nS. Chopra,\nand Y. LeCun,\n“Dimensionality\nreduction"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Trans. Multimedia, vol. 26, pp. 2740–2755, 2023.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "by learning an invariant mapping,” in IEEE Computer Society Conf."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[33] M. S. Akhtar, D. Chauhan, D. Ghosal, S. Poria, A. Ekbal, and P. Bhat-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "Comput. Vis. Pattern Recognit., vol. 2, 2006, pp. 1735–1742."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tacharyya,\n“Multi-task learning for multi-modal\nemotion recognition",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[56] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\nsentiment\nanalysis,”\nin Conf. N. Am. Chapter Assoc. Comput.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "contrastive predictive coding,” arXiv:1807.03748, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Linguist.: Hum. Lang. Technol., Jun. 2019, pp. 370–379.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[57] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[34] Y. Wu, Z. Lin, Y. Zhao, B. Qin, and L.-N. Zhu, “A text-centered shared-",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "for unsupervised visual\nrepresentation learning,” in IEEE/CVF Conf."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "private framework via cross-modal prediction for multimodal sentiment",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "Comput. Vis. Pattern Recognit., 2020, pp. 9726–9735."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "analysis,”\nin Find. Assoc. Comput. Linguist., Aug. 2021, pp. 4730–",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "[58]\nT. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple frame-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4738.",
          "16": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "16": "Int.\nwork for contrastive learning of visual\nrepresentations,” in Proc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[35] C. Du, J. Teng, T. Li, Y. Liu, T. Yuan, Y. Wang, Y. Yuan, and H. Zhao,",
          "16": "Conf. Mach. Learn., vol. 119, 13–18 Jul 2020, pp. 1597–1607."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“On uni-modal\nfeature learning in supervised multi-modal\nlearning,”",
          "16": "[59]\nT. Gao, X. Yao, and D. Chen, “SimCSE: Simple contrastive learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Proc. Int. Conf. Mach. Learn., vol. 202, 23–29 Jul 2023, pp. 8632–",
          "16": "of\nsentence embeddings,” in Proc. Conf. Empir. Methods Nat. Lang."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "8656.",
          "16": "Process., Nov. 2021, pp. 6894–6910."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[36] W. Yu, H. Xu, F. Meng, Y. Zhu, Y. Ma, J. Wu, J. Zou, and K. Yang,",
          "16": "[60]\nP. Khosla,\nP.\nTeterwak,\nC. Wang, A.\nSarna, Y.\nTian,\nP.\nIsola,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“CH-SIMS: A Chinese multimodal sentiment analysis dataset with fine-",
          "16": "Inf.\nA. Maschinot, C. Liu,\nand D. Krishnan,\nin Proc. Adv. Neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "grained annotation of modality,” in Proc. Annu. Meet. Assoc. Comput.",
          "16": "Process. Syst., pp. 18 661–18 673."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Linguist., Jul. 2020, pp. 3718–3727.",
          "16": "[61] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[37] A. Vaswani, N. Shazeer, N. Parmar,\nJ. Uszkoreit, L.\nJones, A. N.",
          "16": "Sung, Z. Li,\nand T. Duerig,\n“Scaling up visual\nand vision-language"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in",
          "16": "representation learning with noisy text supervision,” in Proc. Int. Conf."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. Adv. Neural\nInf. Process. Syst., vol. 30, 2017.",
          "16": "Mach. Learn., vol. 139, 18–24 Jul 2021, pp. 4904–4916."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[62] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:",
          "17": "back: Exploiting activation sparsity in large language models,” in Prof."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A framework for\nself-supervised learning of\nspeech representations,”",
          "17": "Int. Conf. Learn. Represent., 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. Adv. Neural Inf. Process. Syst., vol. 33, pp. 12 449–12 460, 2020.",
          "17": "[84] H. Cohn and A. Kumar, “Universally optimal distribution of points on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[63] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin,",
          "17": "spheres,” J. Am. Math. Soc., vol. 20, no. 1, pp. 99–148, 2007."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and I. Misra, “Imagebind: One embedding space to bind them all,” in",
          "17": "[85] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2023, pp.",
          "17": "L. Zettlemoyer,\nand C. Feichtenhofer,\n“VideoCLIP: Contrastive pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "15 180–15 190.",
          "17": "training for zero-shot video-text understanding,” in Proc. Conf. Empir."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[64] W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Zou, “Mind the gap:",
          "17": "Methods Nat. Lang. Process., Nov. 2021, pp. 6787–6800."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Understanding the modality gap in multi-modal contrastive represen-",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[86]\nF. Wang and H. Liu, “Understanding the behaviour of contrastive loss,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tation learning,” in Proc. Adv. Neural\nInf. Process. Syst., 2022.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[65]\nT. Wang and P. Isola, “Understanding contrastive representation learn-",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pp. 2495–2504."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing through alignment and uniformity on the hypersphere,” in Int. Conf.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[87] G. Hinton, O. Vinyals,\nJ. Dean et al., “Distilling the knowledge in a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Mach. Learn., 2020, pp. 9929–9939.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "neural network,” arxiv:1503.02531, vol. 2, no. 7, 2015."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[66]\nZ. Yang, Z. Dai, Y. Yang,\nJ. Carbonell, R. R. Salakhutdinov,\nand",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[88] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Q. V. Le, “Xlnet: Generalized autoregressive pretraining for\nlanguage",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "understanding,” Proc. Adv. Neural\nInf. Process. Syst., vol. 32, 2019.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pretraining approach,” arXiv:1907.11692, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[67] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[89] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Mosi: multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using Siamese BERT-networks,” in Proc. Conf. Empir. Methods Nat.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "corpus of sentiment intensity and subjectivity analysis in online opinion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lang. Process. and the 9th Int.\nJt. Conf. Nat. Lang. Process., Nov.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "videos,” arxiv:1606.06259, 2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019, pp. 3982–3992.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[90] Y. Liu, Z. Yuan, H. Mao, Z. Liang, W. Yang, Y. Qiu, T. Cheng, X. Li,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[68] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen, and X. Fu,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "H. Xu, and K. Gao, “Make acoustic and visual cues matter: Ch-sims"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Casme ii: An improved spontaneous micro-expression database and",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "v2.0 dataset and av-mixup consistent module,” in Proc. ACM Int. Conf."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the baseline evaluation,” PLOS ONE, vol. 9, no. 1, pp. 1–8, 01 2014.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Proc. Ser., 2022, pp. 247–258."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[69] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[91]\nP. Ekman, W. V. Freisen, and S. Ancoli, “Facial\nsigns of emotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "experience.” J. Pers. Soc. Psychol., vol. 39, no. 6, p. 1125, 1980."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "J. Uszkoreit,\nand N. Houlsby,\n“An\nimage\nis worth\n16x16 words:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transformers for image recognition at scale,” in Prof. Int. Conf. Learn.",
          "17": "[92] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, “Multimodal end-to-end"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Represent., 2021.",
          "17": "sparse model for emotion recognition,” in Proc. Conf. N. Am. Chapter"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[70] Y. Su, T. Lan, H. Li,\nJ. Xu, Y. Wang, and D. Cai, “PandaGPT: One",
          "17": "Assoc. Comput. Linguist.: Hum. Lang. Technol., Jun. 2021, pp. 5305–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "model to instruction-follow them all,” in Proc. Workshop Taming Large",
          "17": "5316."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lang. Model.: Controllability Era Interact. Assist.!, Sep. 2023, pp. 11–",
          "17": "[93] Y. Wu, P. Peng, Z. Zhang, Y. Zhao, and B. Qin, “An end-to-end trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "23.",
          "17": "former with progressive tri-modal attention for multi-modal emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[71] A.\nJaegle,\nF. Gimeno, A. Brock, O. Vinyals, A. Zisserman,\nand",
          "17": "recognition,” in Pattern Recognit. Comput. Vis., 2023, pp. 396–408."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "J. Carreira, “Perceiver: General perception with iterative attention,” in",
          "17": "[94]\nJ. Williams, S. Kleinegesse, R. Comanescu, and O. Radu, “Recognizing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc.\nInt. Conf. Mach. Learn., vol. 139, 18–24 Jul 2021, pp. 4651–",
          "17": "emotions\nin video using multimodal DNN feature\nfusion,”\nin Proc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4664.",
          "17": "Annu. Meet. Assoc. Comput Linguist., Jul. 2018, pp. 11–19."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[72]\nJ. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: bootstrapping language-",
          "17": "[95]\nJ. Williams, R. Comanescu, O. Radu, and L. Tian, “DNN multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "image\npre-training with\nfrozen\nimage\nencoders\nand\nlarge\nlanguage",
          "17": "fusion techniques for predicting video sentiment,” in Proc. Annu. Meet."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models,” in Proc.\nInt. Conf. Mach. Learn., 2023.",
          "17": "Assoc. Comput Linguist., Jul. 2018, pp. 64–72."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[73]\nP. P. Liang, Y. Lyu, G. Chhablani, N.\nJain, Z. Deng, X. Wang, L.-",
          "17": "[96]\nZ.\nLiu,\nY\n.\nShen,\nV\n.\nB.\nLakshminarasimhan,\nP.\nP.\nLiang,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "P. Morency,\nand R.\nSalakhutdinov,\n“Multiviz: Towards\nvisualizing",
          "17": "A. Bagher Zadeh, and L.-P. Morency, “Efficient\nlow-rank multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Int. Conf.\nLearn.\nand\nunderstanding multimodal models,”\nin Prof.",
          "17": "fusion with modality-specific\nfactors,”\nin Proc. Annu. Meet. Assoc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Represent., 2023.",
          "17": "Comput. Linguist., Jul. 2018, pp. 2247–2256."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[74]\nJ. Li, R. Selvaraju, A. Gotmare, S.\nJoty, C. Xiong,\nand S. C. H.",
          "17": "[97] A. Zadeh, P. P. Liang, N. Mazumder, S. Poria, E. Cambria, and L.-P."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Hoi, “Align before fuse: Vision and language representation learning",
          "17": "Morency, “Memory fusion network for multi-view sequential learning,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with momentum distillation,” in Proc. Adv. Neural\nInf. Process. Syst.,",
          "17": "in Proc. AAAI Conf. Artif.\nIntell., vol. 32, no. 1, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 34, 2021, pp. 9694–9705.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[98] Y.-H. H. Tsai, P. P. Liang, A. Zadeh, L.-P. Morency, and R. Salakhut-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[75] R. Rombach, A. Blattmann, D. Lorenz,\nP. Esser,\nand B. Ommer,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Int.\ndinov, “Learning factorized multimodal\nrepresentations,” in Prof."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“High-resolution\nimage\nsynthesis with\nlatent\ndiffusion models,”\nin",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Conf. Learn. Represent., 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2022, pp.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[99] W. Rahman, M. K. Hasan, S. Lee, A. Bagher Zadeh, C. Mao, L.-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "10 684–10 695.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "P. Morency,\nand E. Hoque,\n“Integrating multimodal\ninformation\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[76] A.\nJaegle,\nS. Borgeaud,\nJ.-B. Alayrac, C. Doersch,\nC.\nIonescu,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "large pretrained transformers,” in Proc. Annu. Meet. Assoc. Comput."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. J. Henaff,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Linguist., Jul. 2020, pp. 2359–2369."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "M. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira, “Perceiver",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[100] R. Lin and H. Hu,\n“Multimodal\ncontrastive\nlearning via uni-modal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IO: A general architecture for\nstructured inputs & outputs,” in Prof.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "coding and cross-modal prediction for multimodal sentiment analysis,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Int. Conf. Learn. Represent., 2022.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Find. Assoc. Comput. Linguist., Dec. 2022, pp. 511–523."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[77]\nJ. Ainslie,\nJ. Lee-Thorp, M.\nde\nJong, Y. Zemlyanskiy, F. Lebron,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[101]\nJ. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and S. Sanghai, “GQA: Training generalized multi-query transformer",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "is more: Clipbert for video-and-language learning via sparse sampling,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models from multi-head checkpoints,” in Proc. Conf. Empir. Methods",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Nat. Lang. Process., Dec. 2023, pp. 4895–4901.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pp. 7331–7341."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[78] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, “Deep variational",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[102]\nI. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information bottleneck,” in Prof.\nInt. Conf. Learn. Represent., 2017.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Prof.\nInt. Conf. Learn. Represent., 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[79] A. Nagrani, S. Yang, A. Arnab, A.\nJansen, C. Schmid, and C. Sun,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[103] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Attention bottlenecks\nfor multimodal\nfusion,”\nin Proc. Adv. Neural",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "modern neural networks,” in Proc.\nInt. Conf. Mach. Learn., vol. 70,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Inf. Process. Syst., 2021.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "06–11 Aug 2017, pp. 1321–1330."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[80] D. Han, T. Ye, Y. Han, Z. Xia, S. Song, and G. Huang, “Agent attention:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "On the integration of softmax and linear attention,” arxiv:2312.08874,",
          "17": "[104]\nJ.-B.\nDelbrouck,\nN.\nTits, M.\nBrousmiche,\nand\nS.\nDupont,\n“A"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2023.",
          "17": "transformer-based joint-encoding for\nemotion recognition and senti-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[81] K. Fukushima, “Visual\nfeature extraction by a multilayered network",
          "17": "ment\nanalysis,”\nin Proc. Annu. Meet. Assoc. Comput Linguist.,\nJul."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of analog threshold elements,” IEEE Trans. Syst. Sci. Cybern., vol. 5,",
          "17": "2020, pp. 1–7."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 4, pp. 322–333, 1969.",
          "17": "[105] Y. Wu, P. Peng, Z. Zhang, Y. Zhao, and B. Qin, “An end-to-end trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[82]\nZ. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi, K. Ye,",
          "17": "former with progressive tri-modal attention for multi-modal emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F. Chern, F. Yu, R. Guo, and S. Kumar, “The lazy neuron phenomenon:",
          "17": "recognition,” in Pattern Recognit. Comput. Vis., 2023, pp. 396–408."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "On emergence of activation sparsity in transformers,” in Prof. Int. Conf.",
          "17": "[106] D. S. Chauhan, D. S R, A. Ekbal, and P. Bhattacharyya, “Sentiment"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Learn. Represent., 2023.",
          "17": "and emotion help sarcasm? a multi-task learning framework for multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[83]\nS.\nI. Mirzadeh, K. Alizadeh-Vahid,\nS. Mehta, C. C.\ndel Mundo,",
          "17": "modal sarcasm, sentiment and emotion analysis,” in Proc. Annu. Meet."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "O. Tuzel, G. Samei, M. Rastegari, and M. Farajtabar, “ReLU strikes",
          "17": "Assoc. Comput. Linguist., Jul. 2020, pp. 4351–4360."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[62] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:",
          "17": "back: Exploiting activation sparsity in large language models,” in Prof."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A framework for\nself-supervised learning of\nspeech representations,”",
          "17": "Int. Conf. Learn. Represent., 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. Adv. Neural Inf. Process. Syst., vol. 33, pp. 12 449–12 460, 2020.",
          "17": "[84] H. Cohn and A. Kumar, “Universally optimal distribution of points on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[63] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin,",
          "17": "spheres,” J. Am. Math. Soc., vol. 20, no. 1, pp. 99–148, 2007."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and I. Misra, “Imagebind: One embedding space to bind them all,” in",
          "17": "[85] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2023, pp.",
          "17": "L. Zettlemoyer,\nand C. Feichtenhofer,\n“VideoCLIP: Contrastive pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "15 180–15 190.",
          "17": "training for zero-shot video-text understanding,” in Proc. Conf. Empir."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[64] W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Zou, “Mind the gap:",
          "17": "Methods Nat. Lang. Process., Nov. 2021, pp. 6787–6800."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Understanding the modality gap in multi-modal contrastive represen-",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[86]\nF. Wang and H. Liu, “Understanding the behaviour of contrastive loss,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tation learning,” in Proc. Adv. Neural\nInf. Process. Syst., 2022.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[65]\nT. Wang and P. Isola, “Understanding contrastive representation learn-",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pp. 2495–2504."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing through alignment and uniformity on the hypersphere,” in Int. Conf.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[87] G. Hinton, O. Vinyals,\nJ. Dean et al., “Distilling the knowledge in a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Mach. Learn., 2020, pp. 9929–9939.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "neural network,” arxiv:1503.02531, vol. 2, no. 7, 2015."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[66]\nZ. Yang, Z. Dai, Y. Yang,\nJ. Carbonell, R. R. Salakhutdinov,\nand",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[88] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Q. V. Le, “Xlnet: Generalized autoregressive pretraining for\nlanguage",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "understanding,” Proc. Adv. Neural\nInf. Process. Syst., vol. 32, 2019.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pretraining approach,” arXiv:1907.11692, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[67] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[89] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Mosi: multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using Siamese BERT-networks,” in Proc. Conf. Empir. Methods Nat.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "corpus of sentiment intensity and subjectivity analysis in online opinion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lang. Process. and the 9th Int.\nJt. Conf. Nat. Lang. Process., Nov.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "videos,” arxiv:1606.06259, 2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2019, pp. 3982–3992.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[90] Y. Liu, Z. Yuan, H. Mao, Z. Liang, W. Yang, Y. Qiu, T. Cheng, X. Li,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[68] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen, and X. Fu,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "H. Xu, and K. Gao, “Make acoustic and visual cues matter: Ch-sims"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Casme ii: An improved spontaneous micro-expression database and",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "v2.0 dataset and av-mixup consistent module,” in Proc. ACM Int. Conf."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the baseline evaluation,” PLOS ONE, vol. 9, no. 1, pp. 1–8, 01 2014.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Proc. Ser., 2022, pp. 247–258."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[69] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[91]\nP. Ekman, W. V. Freisen, and S. Ancoli, “Facial\nsigns of emotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "experience.” J. Pers. Soc. Psychol., vol. 39, no. 6, p. 1125, 1980."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "J. Uszkoreit,\nand N. Houlsby,\n“An\nimage\nis worth\n16x16 words:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transformers for image recognition at scale,” in Prof. Int. Conf. Learn.",
          "17": "[92] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, “Multimodal end-to-end"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Represent., 2021.",
          "17": "sparse model for emotion recognition,” in Proc. Conf. N. Am. Chapter"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[70] Y. Su, T. Lan, H. Li,\nJ. Xu, Y. Wang, and D. Cai, “PandaGPT: One",
          "17": "Assoc. Comput. Linguist.: Hum. Lang. Technol., Jun. 2021, pp. 5305–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "model to instruction-follow them all,” in Proc. Workshop Taming Large",
          "17": "5316."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Lang. Model.: Controllability Era Interact. Assist.!, Sep. 2023, pp. 11–",
          "17": "[93] Y. Wu, P. Peng, Z. Zhang, Y. Zhao, and B. Qin, “An end-to-end trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "23.",
          "17": "former with progressive tri-modal attention for multi-modal emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[71] A.\nJaegle,\nF. Gimeno, A. Brock, O. Vinyals, A. Zisserman,\nand",
          "17": "recognition,” in Pattern Recognit. Comput. Vis., 2023, pp. 396–408."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "J. Carreira, “Perceiver: General perception with iterative attention,” in",
          "17": "[94]\nJ. Williams, S. Kleinegesse, R. Comanescu, and O. Radu, “Recognizing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc.\nInt. Conf. Mach. Learn., vol. 139, 18–24 Jul 2021, pp. 4651–",
          "17": "emotions\nin video using multimodal DNN feature\nfusion,”\nin Proc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4664.",
          "17": "Annu. Meet. Assoc. Comput Linguist., Jul. 2018, pp. 11–19."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[72]\nJ. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: bootstrapping language-",
          "17": "[95]\nJ. Williams, R. Comanescu, O. Radu, and L. Tian, “DNN multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "image\npre-training with\nfrozen\nimage\nencoders\nand\nlarge\nlanguage",
          "17": "fusion techniques for predicting video sentiment,” in Proc. Annu. Meet."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models,” in Proc.\nInt. Conf. Mach. Learn., 2023.",
          "17": "Assoc. Comput Linguist., Jul. 2018, pp. 64–72."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[73]\nP. P. Liang, Y. Lyu, G. Chhablani, N.\nJain, Z. Deng, X. Wang, L.-",
          "17": "[96]\nZ.\nLiu,\nY\n.\nShen,\nV\n.\nB.\nLakshminarasimhan,\nP.\nP.\nLiang,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "P. Morency,\nand R.\nSalakhutdinov,\n“Multiviz: Towards\nvisualizing",
          "17": "A. Bagher Zadeh, and L.-P. Morency, “Efficient\nlow-rank multimodal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Int. Conf.\nLearn.\nand\nunderstanding multimodal models,”\nin Prof.",
          "17": "fusion with modality-specific\nfactors,”\nin Proc. Annu. Meet. Assoc."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Represent., 2023.",
          "17": "Comput. Linguist., Jul. 2018, pp. 2247–2256."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[74]\nJ. Li, R. Selvaraju, A. Gotmare, S.\nJoty, C. Xiong,\nand S. C. H.",
          "17": "[97] A. Zadeh, P. P. Liang, N. Mazumder, S. Poria, E. Cambria, and L.-P."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Hoi, “Align before fuse: Vision and language representation learning",
          "17": "Morency, “Memory fusion network for multi-view sequential learning,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with momentum distillation,” in Proc. Adv. Neural\nInf. Process. Syst.,",
          "17": "in Proc. AAAI Conf. Artif.\nIntell., vol. 32, no. 1, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 34, 2021, pp. 9694–9705.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[98] Y.-H. H. Tsai, P. P. Liang, A. Zadeh, L.-P. Morency, and R. Salakhut-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[75] R. Rombach, A. Blattmann, D. Lorenz,\nP. Esser,\nand B. Ommer,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Int.\ndinov, “Learning factorized multimodal\nrepresentations,” in Prof."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“High-resolution\nimage\nsynthesis with\nlatent\ndiffusion models,”\nin",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Conf. Learn. Represent., 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2022, pp.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[99] W. Rahman, M. K. Hasan, S. Lee, A. Bagher Zadeh, C. Mao, L.-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "10 684–10 695.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "P. Morency,\nand E. Hoque,\n“Integrating multimodal\ninformation\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[76] A.\nJaegle,\nS. Borgeaud,\nJ.-B. Alayrac, C. Doersch,\nC.\nIonescu,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "large pretrained transformers,” in Proc. Annu. Meet. Assoc. Comput."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. J. Henaff,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Linguist., Jul. 2020, pp. 2359–2369."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "M. Botvinick, A. Zisserman, O. Vinyals, and J. Carreira, “Perceiver",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[100] R. Lin and H. Hu,\n“Multimodal\ncontrastive\nlearning via uni-modal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IO: A general architecture for\nstructured inputs & outputs,” in Prof.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "coding and cross-modal prediction for multimodal sentiment analysis,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Int. Conf. Learn. Represent., 2022.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Find. Assoc. Comput. Linguist., Dec. 2022, pp. 511–523."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[77]\nJ. Ainslie,\nJ. Lee-Thorp, M.\nde\nJong, Y. Zemlyanskiy, F. Lebron,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[101]\nJ. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and S. Sanghai, “GQA: Training generalized multi-query transformer",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "is more: Clipbert for video-and-language learning via sparse sampling,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models from multi-head checkpoints,” in Proc. Conf. Empir. Methods",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., June 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Nat. Lang. Process., Dec. 2023, pp. 4895–4901.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pp. 7331–7341."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[78] A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, “Deep variational",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[102]\nI. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information bottleneck,” in Prof.\nInt. Conf. Learn. Represent., 2017.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "in Prof.\nInt. Conf. Learn. Represent., 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[79] A. Nagrani, S. Yang, A. Arnab, A.\nJansen, C. Schmid, and C. Sun,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[103] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Attention bottlenecks\nfor multimodal\nfusion,”\nin Proc. Adv. Neural",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "modern neural networks,” in Proc.\nInt. Conf. Mach. Learn., vol. 70,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Inf. Process. Syst., 2021.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "06–11 Aug 2017, pp. 1321–1330."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[80] D. Han, T. Ye, Y. Han, Z. Xia, S. Song, and G. Huang, “Agent attention:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "On the integration of softmax and linear attention,” arxiv:2312.08874,",
          "17": "[104]\nJ.-B.\nDelbrouck,\nN.\nTits, M.\nBrousmiche,\nand\nS.\nDupont,\n“A"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2023.",
          "17": "transformer-based joint-encoding for\nemotion recognition and senti-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[81] K. Fukushima, “Visual\nfeature extraction by a multilayered network",
          "17": "ment\nanalysis,”\nin Proc. Annu. Meet. Assoc. Comput Linguist.,\nJul."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of analog threshold elements,” IEEE Trans. Syst. Sci. Cybern., vol. 5,",
          "17": "2020, pp. 1–7."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 4, pp. 322–333, 1969.",
          "17": "[105] Y. Wu, P. Peng, Z. Zhang, Y. Zhao, and B. Qin, “An end-to-end trans-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[82]\nZ. Li, C. You, S. Bhojanapalli, D. Li, A. S. Rawat, S. J. Reddi, K. Ye,",
          "17": "former with progressive tri-modal attention for multi-modal emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F. Chern, F. Yu, R. Guo, and S. Kumar, “The lazy neuron phenomenon:",
          "17": "recognition,” in Pattern Recognit. Comput. Vis., 2023, pp. 396–408."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "On emergence of activation sparsity in transformers,” in Prof. Int. Conf.",
          "17": "[106] D. S. Chauhan, D. S R, A. Ekbal, and P. Bhattacharyya, “Sentiment"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Learn. Represent., 2023.",
          "17": "and emotion help sarcasm? a multi-task learning framework for multi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[83]\nS.\nI. Mirzadeh, K. Alizadeh-Vahid,\nS. Mehta, C. C.\ndel Mundo,",
          "17": "modal sarcasm, sentiment and emotion analysis,” in Proc. Annu. Meet."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "O. Tuzel, G. Samei, M. Rastegari, and M. Farajtabar, “ReLU strikes",
          "17": "Assoc. Comput. Linguist., Jul. 2020, pp. 4351–4360."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[107] W. Han, H. Chen, A. Gelbukh, A. Zadeh, L.-p. Morency, and S. Poria,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Bi-bimodal modality\nfusion\nfor\ncorrelation-controlled multimodal",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentiment analysis,” in Proc.\nInt. Conf. Proc. Ser., 2021, pp. 6–15.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[108]\nS. Pramanick, A. Roy, and V. M. Patel, “Multimodal\nlearning using",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "optimal transport for sarcasm and humor detection,” in Proc. IEEE/CVF",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Winter Conf. Appl. Comput. Vis., January 2022, pp. 3930–3940.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[109] D. Ghosal, N. Majumder,\nS.\nPoria, N. Chhaya,\nand A. Gelbukh,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“DialogueGCN: A graph\nconvolutional\nneural\nnetwork\nfor\nemotion",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition in conversation,” in Prof. Conf. Empir. Methods Nat. Lang.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Process.\nInt. Jt. Conf., Nov. 2019, pp. 154–164.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[110]\nJ. Hu, Y. Liu, J. Zhao, and Q. Jin, “MMGCN: Multimodal\nfusion via",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "deep graph convolution network for emotion recognition in conversa-",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion,” in Proc. Annu. Meet. Assoc. Comput. Linguist. Int. Jt. Conf. Nat.,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Aug. 2021, pp. 5666–5675.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[111] A.\nJoshi, A. Bhat, A.\nJain, A. Singh,\nand A. Modi,\n“COGMEN:",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "COntextualized GNN based multimodal emotion recognitioN,” in Proc.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol.,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Jul. 2022, pp. 4148–4164.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[112] C. V. T. Nguyen, T. Mai, S. The, D. Kieu, and D.-T. Le, “Conversation",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "understanding using relational\ntemporal graph neural networks with",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "auxiliary cross-modality interaction,”\nin Proc. Conf. Empir. Methods",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Nat. Lang. Process., Dec. 2023, pp. 15 154–15 167.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[113]\nS. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, and L.-",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "P. Morency, “Context-dependent\nsentiment analysis\nin user-generated",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "videos,” in Proc. Annu. Meet. Assoc. Comput. Linguist., Jul. 2017, pp.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "873–883.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[114] D. Hazarika, S. Poria, A. Zadeh, E. Cambria, L.-P. Morency,\nand",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "R. Zimmermann, “Conversational memory network for emotion recog-",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nition in dyadic dialogue videos,” in Proc. Conf. N. Am. Chapter Assoc.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Comput. Linguist.: Hum. Lang. Technol., Jun. 2018, pp. 2122–2132.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[115] D. Hazarika, S. Poria, R. Mihalcea, E. Cambria, and R. Zimmermann,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“ICON:\nInteractive\nconversational memory\nnetwork\nfor multimodal",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion detection,” in Proc. Conf. Empir. Methods Nat. Lang. Process.,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Oct.-Nov. 2018, pp. 2594–2604.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[116] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh, and",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "E. Cambria, “Dialoguernn: an attentive rnn for emotion detection in",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conversations,” in Proc. AAAI Conf. Artif.\nIntell., 2019.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[117] D. Hu, L. Wei,\nand X. Huai,\n“DialogueCRN: Contextual\nreasoning",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "networks\nfor\nemotion recognition in conversations,”\nin Proc. Annu.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Meet. Assoc. Comput. Linguist.\nInt.\nJt. Conf. Nat., Aug.\n2021,\npp.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "7042–7052.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[118] A. Shenoy and A. Sardana, “Multilogue-net: A context-aware RNN for",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multi-modal emotion detection and sentiment analysis in conversation,”",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Proc. Annu. Meet. Assoc. Comput Linguist., Jul. 2020, pp. 19–28.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[119]\nL. Hyun, K.\nSung-Bin,\nS. Han, Y. Yu,\nand T.-H. Oh,\n“SMILE:",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Multimodal dataset\nfor understanding laughter\nin video with language",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models,” in Find. Assoc. Comput. Linguist., Jun. 2024, pp. 1149–1167.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[120] C. Cortes\nand V. Vapnik,\n“Support-vector networks,” Mach. Learn.,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 20, no. 3, pp. 273–297, sep 1995.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[121]\nL. Van der Maaten and G. Hinton, “Visualizing data using t-sne,” J.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Mach. Learn. Res., vol. 9, no. 86, pp. 2579–2605, 2008.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[122] K. He, X. Chen, S. Xie, Y. Li, P. Doll´ar, and R. Girshick, “Masked",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE/CVF Conf.\nautoencoders are scalable vision learners,” in Proc.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Comput. Vis. Pattern Recognit., June 2022, pp. 16 000–16 009.",
          "18": ""
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "R Mihalcea"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltru Å¡aitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "3",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "4",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "5",
      "title": "What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis",
      "authors": [
        "D Gkoumas",
        "Q Li",
        "C Lioma",
        "Y Yu",
        "D Song"
      ],
      "year": "2021",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "6",
      "title": "M-SENA: An integrated platform for multimodal sentiment analysis",
      "authors": [
        "H Mao",
        "Z Yuan",
        "H Xu",
        "W Yu",
        "Y Liu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist. Int. Jt. Conf. Nat"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Provost",
        "J Kim",
        "S Chang",
        "S Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval"
    },
    {
      "citation_id": "8",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "9",
      "title": "Multimodal emotion recognition with deep learning: Advancements, challenges, and future directions",
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "10",
      "title": "UR-FUNNY: A multimodal language dataset for understanding humor",
      "authors": [
        "M Hasan",
        "W Rahman",
        "A Bagher",
        "J Zadeh",
        "M Zhong",
        "L.-P Tanveer",
        "M Morency",
        "Hoque"
      ],
      "year": "2019",
      "venue": "Methods Nat. Lang. Process. Int. Jt. Conf"
    },
    {
      "citation_id": "11",
      "title": "Towards multimodal sarcasm detection (an Obviously perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "12",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "13",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "14",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2024",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "15",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "16",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "17",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "18",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "19",
      "title": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "authors": [
        "W Dai",
        "Z Liu",
        "T Yu",
        "P Fung"
      ],
      "year": "2020",
      "venue": "Proc. Conf. Asia-Pacific Assoc"
    },
    {
      "citation_id": "20",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "M Hasan",
        "S Lee",
        "W Rahman",
        "A Zadeh",
        "R Mihalcea",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "21",
      "title": "Covarep -a collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "22",
      "title": "Facial expression analysis",
      "year": "2017",
      "venue": "Facial expression analysis"
    },
    {
      "citation_id": "23",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proc. ACM Int. Conf. Multimed"
    },
    {
      "citation_id": "24",
      "title": "Multi-task momentum distillation for multimodal sentiment analysis",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "25",
      "title": "Assessing generative models via precision and recall",
      "authors": [
        "M Sajjadi",
        "O Bachem",
        "M Lucic",
        "O Bousquet",
        "S Gelly"
      ],
      "year": "2018",
      "venue": "Proc. Adv. Neural Inf. Process. Syst."
    },
    {
      "citation_id": "26",
      "title": "Improved precision and recall metric for assessing generative models",
      "authors": [
        "T Kynkäänniemi",
        "T Karras",
        "S Laine",
        "J Lehtinen",
        "T Aila"
      ],
      "year": "2019",
      "venue": "Proc. Adv. Neural Inf. Process. Syst."
    },
    {
      "citation_id": "27",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "28",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "29",
      "title": "MissModal: Increasing Robustness to Missing Modality in Multimodal Sentiment Analysis",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "venue": "Trans. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "30",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. Conf. Empir. Methods Nat. Lang. Process"
    },
    {
      "citation_id": "31",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Conf. N. Am. Chapter Assoc"
    },
    {
      "citation_id": "32",
      "title": "Dynamically shifting multimodal representations via hybrid-modal attention for multimodal sentiment analysis",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "D Ghosal",
        "S Poria",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol"
    },
    {
      "citation_id": "34",
      "title": "A text-centered sharedprivate framework via cross-modal prediction for multimodal sentiment analysis",
      "authors": [
        "Y Wu",
        "Z Lin",
        "Y Zhao",
        "B Qin",
        "L.-N Zhu"
      ],
      "year": "2021",
      "venue": "Find. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "35",
      "title": "On uni-modal feature learning in supervised multi-modal learning",
      "authors": [
        "C Du",
        "J Teng",
        "T Li",
        "Y Liu",
        "T Yuan",
        "Y Wang",
        "Y Yuan",
        "H Zhao"
      ],
      "year": "2023",
      "venue": "Proc. Int. Conf. Mach. Learn."
    },
    {
      "citation_id": "36",
      "title": "CH-SIMS: A Chinese multimodal sentiment analysis dataset with finegrained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "37",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. Adv. Neural Inf. Process. Syst."
    },
    {
      "citation_id": "38",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": [
        "W Kim",
        "B Son",
        "I Kim"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "39",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Mach. Learn."
    },
    {
      "citation_id": "40",
      "title": "CLMLF:a contrastive learning and multi-layer fusion method for multimodal sentiment detection",
      "authors": [
        "Z Li",
        "B Xu",
        "C Zhu",
        "T Zhao"
      ],
      "year": "2022",
      "venue": "Find. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "41",
      "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "42",
      "title": "Multimodality representation learning: A survey on evolution, pretraining and its applications",
      "authors": [
        "M Manzoor",
        "S Albarri",
        "Z Xian",
        "Z Meng",
        "P Nakov",
        "S Liang"
      ],
      "year": "2023",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl"
    },
    {
      "citation_id": "43",
      "title": "Adapt and explore: Multimodal mixup for representation learning",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "44",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space"
    },
    {
      "citation_id": "45",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE Int. Conf. Autom. Face Gesture Recognit"
    },
    {
      "citation_id": "46",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "47",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Prof. Int. Conf. Learn. Represent."
    },
    {
      "citation_id": "48",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech, Language Process"
    },
    {
      "citation_id": "49",
      "title": "Transformers are RNNs: Fast autoregressive transformers with linear attention",
      "authors": [
        "A Katharopoulos",
        "A Vyas",
        "N Pappas",
        "F Fleuret"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "50",
      "title": "LXMERT: Learning cross-modality encoder representations from transformers",
      "authors": [
        "H Tan",
        "M Bansal"
      ],
      "year": "2019",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "51",
      "title": "Fast transformer decoding: One write-head is all you need",
      "authors": [
        "N Shazeer"
      ],
      "year": "2019",
      "venue": "Fast transformer decoding: One write-head is all you need"
    },
    {
      "citation_id": "52",
      "title": "Multimodal learning with transformers: A survey",
      "authors": [
        "P Xu",
        "X Zhu",
        "D Clifton"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "53",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "54",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac",
        "J Donahue",
        "P Luc"
      ],
      "year": "2022",
      "venue": "Proc. Adv. Neural Inf. Process. Syst."
    },
    {
      "citation_id": "55",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "R Hadsell",
        "S Chopra",
        "Y Lecun"
      ],
      "year": "2006",
      "venue": "IEEE Computer Society Conf. Comput. Vis. Pattern Recognit."
    },
    {
      "citation_id": "56",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "57",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "58",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "59",
      "title": "SimCSE: Simple contrastive learning of sentence embeddings",
      "authors": [
        "T Gao",
        "X Yao",
        "D Chen"
      ],
      "year": "2021",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "60",
      "title": "Proc. Adv. Neural Inf. Process. Syst.",
      "authors": [
        "P Khosla",
        "P Teterwak",
        "C Wang",
        "A Sarna",
        "Y Tian",
        "P Isola",
        "A Maschinot",
        "C Liu",
        "D Krishnan"
      ],
      "venue": "Proc. Adv. Neural Inf. Process. Syst."
    },
    {
      "citation_id": "61",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "authors": [
        "C Jia",
        "Y Yang",
        "Y Xia",
        "Y.-T Chen",
        "Z Parekh",
        "H Pham",
        "Q Le",
        "Y.-H Sung",
        "Z Li",
        "T Duerig"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Mach. Learn."
    },
    {
      "citation_id": "62",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "63",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "R Girdhar",
        "A El-Nouby",
        "Z Liu",
        "M Singh",
        "K Alwala",
        "A Joulin",
        "I Misra"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "64",
      "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
      "authors": [
        "W Liang",
        "Y Zhang",
        "Y Kwon",
        "S Yeung",
        "J Zou"
      ],
      "year": "2022",
      "venue": "Proc"
    },
    {
      "citation_id": "65",
      "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
      "authors": [
        "T Wang",
        "P Isola"
      ],
      "year": "2020",
      "venue": "Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "66",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "67",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Proc. Conf. Empir. Methods Nat. Lang. Process. and the 9th Int. Jt. Conf. Nat. Lang. Process"
    },
    {
      "citation_id": "68",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "69",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Prof. Int. Conf. Learn. Represent."
    },
    {
      "citation_id": "70",
      "title": "PandaGPT: One model to instruction-follow them all",
      "authors": [
        "Y Su",
        "T Lan",
        "H Li",
        "J Xu",
        "Y Wang",
        "D Cai"
      ],
      "year": "2023",
      "venue": "Proc. Workshop Taming Large Lang. Model.: Controllability Era Interact. Assist.!"
    },
    {
      "citation_id": "71",
      "title": "Perceiver: General perception with iterative attention",
      "authors": [
        "A Jaegle",
        "F Gimeno",
        "A Brock",
        "O Vinyals",
        "A Zisserman",
        "J Carreira"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "72",
      "title": "Blip-2: bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "73",
      "title": "Multiviz: Towards visualizing and understanding multimodal models",
      "authors": [
        "P Liang",
        "Y Lyu",
        "G Chhablani",
        "N Jain",
        "Z Deng",
        "X Wang",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2023",
      "venue": "Prof. Int. Conf. Learn. Represent."
    },
    {
      "citation_id": "74",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "authors": [
        "J Li",
        "R Selvaraju",
        "A Gotmare",
        "S Joty",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "Proc"
    },
    {
      "citation_id": "75",
      "title": "High-resolution image synthesis with latent diffusion models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser",
        "B Ommer"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "76",
      "title": "Perceiver IO: A general architecture for structured inputs & outputs",
      "authors": [
        "A Jaegle",
        "S Borgeaud",
        "J.-B Alayrac",
        "C Doersch",
        "C Ionescu",
        "D Ding",
        "S Koppula",
        "D Zoran",
        "A Brock",
        "E Shelhamer",
        "O Henaff",
        "M Botvinick",
        "A Zisserman",
        "O Vinyals",
        "J Carreira"
      ],
      "year": "2022",
      "venue": "Prof. Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "77",
      "title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints",
      "authors": [
        "J Ainslie",
        "J Lee-Thorp",
        "M Jong",
        "Y Zemlyanskiy",
        "F Lebron",
        "S Sanghai"
      ],
      "year": "2023",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "78",
      "title": "Deep variational information bottleneck",
      "authors": [
        "A Alemi",
        "I Fischer",
        "J Dillon",
        "K Murphy"
      ],
      "year": "2017",
      "venue": "Prof. Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "79",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "A Nagrani",
        "S Yang",
        "A Arnab",
        "A Jansen",
        "C Schmid",
        "C Sun"
      ],
      "year": "2021",
      "venue": "Proc"
    },
    {
      "citation_id": "80",
      "title": "Agent attention: On the integration of softmax and linear attention",
      "authors": [
        "D Han",
        "T Ye",
        "Y Han",
        "Z Xia",
        "S Song",
        "G Huang"
      ],
      "year": "2023",
      "venue": "Agent attention: On the integration of softmax and linear attention"
    },
    {
      "citation_id": "81",
      "title": "Visual feature extraction by a multilayered network of analog threshold elements",
      "authors": [
        "K Fukushima"
      ],
      "year": "1969",
      "venue": "IEEE Trans. Syst. Sci. Cybern"
    },
    {
      "citation_id": "82",
      "title": "The lazy neuron phenomenon: On emergence of activation sparsity in transformers",
      "authors": [
        "Z Li",
        "C You",
        "S Bhojanapalli",
        "D Li",
        "A Rawat",
        "S Reddi",
        "K Ye",
        "F Chern",
        "F Yu",
        "R Guo",
        "S Kumar"
      ],
      "year": "2023",
      "venue": "Prof. Int. Conf. Learn. Represent."
    },
    {
      "citation_id": "83",
      "title": "ReLU strikes back: Exploiting activation sparsity in large language models",
      "authors": [
        "S Mirzadeh",
        "K Alizadeh-Vahid",
        "S Mehta",
        "C Del Mundo",
        "O Tuzel",
        "G Samei",
        "M Rastegari",
        "M Farajtabar"
      ],
      "year": "2024",
      "venue": "Prof. Int. Conf. Learn. Represent."
    },
    {
      "citation_id": "84",
      "title": "Universally optimal distribution of points on spheres",
      "authors": [
        "H Cohn",
        "A Kumar"
      ],
      "year": "2007",
      "venue": "J. Am. Math. Soc"
    },
    {
      "citation_id": "85",
      "title": "VideoCLIP: Contrastive pretraining for zero-shot video-text understanding",
      "authors": [
        "H Xu",
        "G Ghosh",
        "P.-Y Huang",
        "D Okhonko",
        "A Aghajanyan",
        "F Metze",
        "L Zettlemoyer",
        "C Feichtenhofer"
      ],
      "year": "2021",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "86",
      "title": "Understanding the behaviour of contrastive loss",
      "authors": [
        "F Wang",
        "H Liu"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "87",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network"
    },
    {
      "citation_id": "88",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "89",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos"
    },
    {
      "citation_id": "90",
      "title": "Make acoustic and visual cues matter: Ch-sims v2.0 dataset and av-mixup consistent module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proc. ACM Int. Conf. Proc. Ser"
    },
    {
      "citation_id": "91",
      "title": "Facial signs of emotional experience",
      "authors": [
        "P Ekman",
        "W Freisen",
        "S Ancoli"
      ],
      "year": "1980",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "92",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Proc. Conf. N. Am. Chapter Assoc"
    },
    {
      "citation_id": "93",
      "title": "An end-to-end transformer with progressive tri-modal attention for multi-modal emotion recognition",
      "authors": [
        "Y Wu",
        "P Peng",
        "Z Zhang",
        "Y Zhao",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Pattern Recognit. Comput. Vis"
    },
    {
      "citation_id": "94",
      "title": "Recognizing emotions in video using multimodal DNN feature fusion",
      "authors": [
        "J Williams",
        "S Kleinegesse",
        "R Comanescu",
        "O Radu"
      ],
      "year": "2018",
      "venue": "Proc. Annu. Meet. Assoc. Comput Linguist"
    },
    {
      "citation_id": "95",
      "title": "DNN multimodal fusion techniques for predicting video sentiment",
      "authors": [
        "J Williams",
        "R Comanescu",
        "O Radu",
        "L Tian"
      ],
      "year": "2018",
      "venue": "Proc. Annu. Meet. Assoc. Comput Linguist"
    },
    {
      "citation_id": "96",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Bagher",
        "L.-P Zadeh",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "97",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "98",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Prof. Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "99",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Bagher",
        "C Zadeh",
        "L.-P Mao",
        "E Morency",
        "Hoque"
      ],
      "year": "2020",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "100",
      "title": "Multimodal contrastive learning via uni-modal coding and cross-modal prediction for multimodal sentiment analysis",
      "authors": [
        "R Lin",
        "H Hu"
      ],
      "year": "2022",
      "venue": "Find. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "101",
      "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
      "authors": [
        "J Lei",
        "L Li",
        "L Zhou",
        "Z Gan",
        "T Berg",
        "M Bansal",
        "J Liu"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "102",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Prof. Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "103",
      "title": "On calibration of modern neural networks",
      "authors": [
        "C Guo",
        "G Pleiss",
        "Y Sun",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "104",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "J.-B Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Proc. Annu. Meet. Assoc. Comput Linguist"
    },
    {
      "citation_id": "105",
      "title": "An end-to-end transformer with progressive tri-modal attention for multi-modal emotion recognition",
      "authors": [
        "Y Wu",
        "P Peng",
        "Z Zhang",
        "Y Zhao",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Pattern Recognit. Comput. Vis"
    },
    {
      "citation_id": "106",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multimodal sarcasm, sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "D S R",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "107",
      "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "A Gelbukh",
        "A Zadeh",
        "L -P. Morency",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proc. Int. Conf. Proc. Ser"
    },
    {
      "citation_id": "108",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "S Pramanick",
        "A Roy",
        "V Patel"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis"
    },
    {
      "citation_id": "109",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Methods Nat. Lang. Process. Int. Jt. Conf"
    },
    {
      "citation_id": "110",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proc"
    },
    {
      "citation_id": "111",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proc. Conf. N. Am. Chapter Assoc"
    },
    {
      "citation_id": "112",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "C Nguyen",
        "T Mai",
        "S The",
        "D Kieu",
        "D.-T Le"
      ],
      "year": "2023",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "113",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "114",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proc. Conf. N. Am. Chapter Assoc"
    },
    {
      "citation_id": "115",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proc. Conf. Empir"
    },
    {
      "citation_id": "116",
      "title": "Dialoguernn: an attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "117",
      "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proc. Annu. Meet. Assoc. Comput. Linguist. Int. Jt. Conf. Nat"
    },
    {
      "citation_id": "118",
      "title": "Multilogue-net: A context-aware RNN for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "A Shenoy",
        "A Sardana"
      ],
      "year": "2020",
      "venue": "Proc. Annu. Meet. Assoc. Comput Linguist"
    },
    {
      "citation_id": "119",
      "title": "SMILE: Multimodal dataset for understanding laughter in video with language models",
      "authors": [
        "L Hyun",
        "K Sung-Bin",
        "S Han",
        "Y Yu",
        "T.-H Oh"
      ],
      "year": "2024",
      "venue": "Find. Assoc. Comput. Linguist"
    },
    {
      "citation_id": "120",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Mach. Learn"
    },
    {
      "citation_id": "121",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "122",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    }
  ]
}