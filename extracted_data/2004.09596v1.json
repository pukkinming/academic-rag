{
  "paper_id": "2004.09596v1",
  "title": "On-The-Fly Detection Of User Engagement Decrease In Spontaneous Human-Robot Interaction Using Recurrent And Deep Neural Networks",
  "published": "2020-04-20T19:41:55Z",
  "authors": [
    "Atef Ben Youssef",
    "Giovanna Varni",
    "Slim Essid",
    "Chloé Clavel"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper we consider the detection of a decrease of engagement by users spontaneously interacting with a socially assistive robot in a public space. We first describe the UE-HRI dataset that collects spontaneous Human-Robot Interactions following the guidelines provided by the Affective Computing research community to collect data \"in-the-wild\". We then analyze the users' behaviors, focusing on proxemics, gaze, head motion, facial expressions and speech during interactions with the robot. Finally, we investigate the use of deep leaning techniques (Recurrent and Deep Neural Networks) to detect user engagement decrease in realtime. The results of this work highlight, in particular, the relevance of taking into account the temporal dynamics of a user's behavior. Allowing 1 to 2 seconds as buffer delay improves the performance of taking a decision on user engagement.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Socially assistive robots (SAR) should be able to communicate and cooperate with humans in order to provide assistance, coaching, companionship, support for convalescence, rehabilitation, learning, or therapeutic aid, etc. (e.g.  [25, 50] ). SAR deployed in public spaces have considerable potential for providing the humans with whom they engage, with a multitude of services: welcoming them, giving them recommendations or interacting in a personalized way  [1, 26, 13, 12] . These types of robot employ short-term adaptation in order to keep the user's attention and achieve their goal of assisting them through social interaction. They are equipped with sensors combined with software modules to track humans and inform the interaction process. These modules can for instance track faces, recognize speech, and synthesize speech synchronized with animation. Extracting basic information such as facial expressions, gaze, and head motions allows the robots to better understand the person. Processing this information serves more sophisticated modules that analyze emotions, mood, affective state, and user's engagement in order to give appropriate responses.\n\nThis study focuses on real-time detection of user's engagement decrease during a social interaction with a robot in a public space. In public space settings, it is not easy for the robot to achieve its goal in spontaneous social interaction, where participants are free to treat the robot as they like and leave the interaction when they wish  [8] . Recognizing user's engagement state represents a key issue in socially assistive robotics.\n\nFor this study, we recorded a multimodal dataset of spontaneous interactions with the humanoid robot Pepper 1  [9] . In keeping with the current emerging trend in Affective Computing, this dataset consists of data collected in-the-wild  [47] . It comprises 278 interactions where the users were free to participate in the interaction if they wished to and free to leave it when they wanted to, and where they were left to behave without unconstraints. Multimodal information describing the user's behavior (i.e. distance to the robot, gaze and head motion as well as facial expressions and speech features) was thus synchronously recorded. We analyze the dataset, focusing on the non-verbal behavior displayed by the users. We then make use of data-driven methods for detecting engagement decrease. Such methods rely on a ground-truth obtained by manual annotation of the engagement. Perceived engagement can be a subjective observation. For this reason, each interaction was annotated independently by two annotators: a researcher who knew the purpose of the work and an uninformed one who did not.\n\nThis paper is organized as follows. Section 2 presents the related work on user engagement in Human-Robot interaction (HRI). Section 3 describes the dataset of spontaneous HRI. Section 4 focuses on the analysis of user engagement decrease. Section 5 describes our approach to detect the decrease of user engagement. A discussion is presented in Section 6. Finally, conclusions are drawn in Section 7.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Socially Assistive Robots In Public Space",
      "text": "SAR are robots providing assistance to human users through social interaction  [50] . These robots are designed e assist users by creating effective interactions  [25] . SAR deployed in real world settings need to secure and maintain the users engagement. Pitsch et al.  [43]  analyzed interactions between a robot deployed as a guide in a museum, and visitors. They found that the first five seconds of the interaction had a relevant impact on the user's engagement during the interaction (e.g. leaving/staying, responsiveness, exchanging rituals). Gehle et al.  [28]  likewise analyzed the interaction opening strategies of a robot playing the role of a museum's guide, in its interaction with visitors. Hayashi et al.  [31]  proposed to use robots in train stations to assist passengers. Their goal was to identify the best way to provide users with travel information. They compared the use of one vs two robots. The findings of this study showed that the most effective way of attracting people's interest was by presenting information using two humanoid robots rather than one. They reported also that the interactivity was useful in giving the feeling of talking with robots. Another interesting scenario is the use of SAR to provide shopping information to customers  [26, 35] . The MuMMER project aims to develop a socially intelligent humanoid robot that is able to operate in a public shopping mall  [26] . In  [35] , SAR were designed to naturally interact with customers and to provide shopping information. In public spaces, SAR could inspire the design of hotel-assistive robots  [25] .\n\nIn such application contexts, robots are expected to respond appropriately to the users' behavior and engage them in stimulating experiences  [20, 23] . In particular, they should be able to monitor a users state of engagement in order to be able to react to possible signs of disengagement in such a way as to maintain their interest. In real world settings, one of the challenges is to deal with the dynamic and flexible nature of human behavior in order to secure and maintain users' engagement in their interaction with SAR.\n\nTackling these challenges, our research aims to detect user engagement states in real-time in order to assist humans for the purpose of providing such public services. The proposed detection model integrates data on the temporal dynamics of engagement behavior, with the multimodal data collected in-the-wild.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Engagement And Disengagement In Hri",
      "text": "The engagement was defined in human-computer interaction by Sidner  [49]  as \"the process by which individuals in an interaction start, maintain and end their perceived connection to one another\"; and by Poggi  [44]  as \"the value that a participant in an interaction attributes to the goal of being together with the other participant(s) and of continuing the interaction\". This concept of engagement has been explored from different perspectives with regard to humans interacting with social robots or virtual agents  [21] . More specifically, a focus has been put on user engagement prediction  [27, 16] , the analysis of the emergence of engagement  [52, 43] , the identification of the addressees to interact with  [38] , and the study of the relationship between personality and engagement  [17, 33]  and engagement perception  [30] . Similarly, disengagement has been tackled in many studies by analyzing interaction problems, the time of their occurrence and their causes  [4, 51] , the dynamics of affective states  [22, 15]  and the prediction of disengagement  [14, 37] . The most crucial causes of interaction problems are found to be the limitations of the systems used to detect social signals and of the interaction models. For example, it was reported that the most frequent causes were the engagement model, face tracker, turn-taking model, or speech recognition issues, misunderstanding, lack of adaptation, repetition and long pauses, over-fragmentation, over-clarity, overcoordination, over-directedness, insufficient or exaggerated state-of-mind updates and repair requests  [14, 4, 40] .\n\nHumans behave differently during social norm violations and technical errors in HRI  [51] . It was shown that the automatic detection of these errors based on human behavior works to some extent. The performance of error detection is better when the robot knows the human with whom it is interacting . Detecting social norm violation is harder than detecting technical failures. We conclude from the work of Trung et al.  [51]  that detecting disengagement in social interaction with a robot is difficult.\n\nThe most common features used in these studies to asses engagement and disengagement were, among others, gaze  [5, 33, 46, 49, 37] , head motion  [5, 37] , face  [14, 37, 39] , posture  [5, 37] , speech  [33, 37] , and distance  [52] . Other, more subtle, features were also included: semantics, attention, emotions and affects  [14, 37, 22, 15] . In a previous study, we show that the use of combined multimodal features effectively improves the performance of a user engagement breakdown system  [8] . Combining features from two or more modalities allows one to achieve better results in engagement detection/prediction, compared to the use of features from only one modality. Kendon  [36]  analyzed gaze and speech. He found that the speakers looks at each other during fluent speech and at the end of sentences, but look away during hesitations or unfluent speech. This type of social signal is probably relevant information to evaluate the engagement level during the interaction. Prosody, articulation, voice-quality related features, linguistic analysis as well as facial expressions and gaze were used to detect interest in  [48] .\n\nTo model user engagement in HRI, researchers have considered a subset of systems going from rule-based to machine-learning-based. Machine learning approaches have been compared to rule-based approaches in  [27] . It has been shown that the rule-based classifiers have a competitive performance compared to the set of supervised classifiers trained on a small labeled corpus. The authors found that Conditional Random Fields (CRF), which give an accuracy of 61.5% and F1-score of 0.61, is a much more stable classifier than others. Machine learning approaches are the most commonly used for automatically predicting engagement in HRI. By comparing logistic regression and boosted decision tree models in  [14] , the logistic regression model was selected for managing disengagement decisions. In  [11] , Bohus et al. used a frame-by-frame binary classification scheme using a maximum entropy model to predict engagement intentions. Leave-one-out cross-validation us-ing Support Vector Machines (SVMs) was used in  [16, 48, 37] . SVMs with a polynomial kernel were successfully used to recognize the interest in  [48] . The problem to address the engagement of only one user or more than one in interaction was studied by Leite et al.  [37] . They found that the disengagement model trained in the single-user condition might not be appropriate for the group condition, but the group condition model generalizes better to the single-user condition. A mixed model combining both conditions is a good compromise, but it does not achieve the performance levels of the models trained for a specific type of interaction. Their best models give an accuracy of 63% and AUC of 0.61 for the single-user condition and an accuracy of 73% and AUC of 0.62 for the group condition. This finding has encouraged us to work with mixed conditions. Liu et al. applied the Echo State Networks (ESNs) architecture, a variant of Recurrent Neural Networks, to a real-world dataset and showed that these networks are able to predict engagement breakdown behavior using 30 seconds of facial expression features  [39] .\n\nOur positioning in relation to these previous studies is as follows. First, our collected dataset targets the diverse social signals that are involved in user engagement, considering a wide range of heterogeneous sensors: microphone array, cameras, sonars, lasers, along with user tracked variables (i.e. face features, head angles, eye gaze and position toward the robot). To the best of our knowledge, none of the existing datasets provide such a thorough coverage of signals amenable to exploitation for user engagement analysis. This is also the first significant dataset offering a large amount of data collected by the robot \"Pepper\". Pepper offers a large combination of features compared to the other robots used in the literature (NAO, iCub, MyKeepon, and so on). Second, the use of such a large and realworld dataset allows us to investigate deep learning approaches such as recurrent neural networks for the multimodal detection of user engagement decrease. This \"into-the wild\" dataset is here used to model the temporal user behavior in order to make decision in realtime about engagement decrease. It follows the work of : i)  [39]  that uses such neural networks with facial expression alone on a reduced set of our dataset that has already been made public; ii)  [8]  that shows the superiority of multimodality for a related but close task which is the prediction of engagement breakdown using task-designed logistic regression. This could lead to the development of lifelike humanoid SAR that could better understand the behavior of the humans they are interacting with, and therefore respond more appropriately in order to increase their engagement.\n\n3 Spontaneous Human-Humanoid Interactions",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Design",
      "text": "The experiments were conducted in a public space at Telecom ParisTech over 17 months. The recordings consisted of interactions between humans and the robot Pepper (see Fig.  1 ). The collected data constitute the UE-HRI dataset 2 described in  [9] . It includes all data streams available on Pepper, packaged in the opensource Robot Operating System (ROS) framework 3 . Each stream is translated into a message (called ROS topic) and packaged together into a ROSbag file. In order to keep all the streams synchronized, they were indexed using the robot timestamps. The recorded data is split into ROSbag files of 100 Mb in order to quickly move them from the robot to a storage server over Ethernet. ROSbag files are then merged together into one ROSbag file in order to get one file per interaction. Fig.  2  shows the experimental setup of the interaction.\n\nPepper automatically starts the interaction when it detects movement, and focuses on the participant in front of it, who is in the interaction zone (i.e. a distance of less than 1.5-meters from the robot, indicated by means of black tape stuck on the floor in Fig.  1 ).\n\nFirst, the robot asks the user to sign the agreement form displayed on its embedded tablet, authorizing researchers to use her collected data for further analysis. After validation of the agreement, the robot enters the welcome phase by introducing itself through very lively animations and providing the user with the following instructions: \"speak loud and be alone in the 1 st engagement zone\". It then enters the dialog phase. This includes a set of open-ended questions where the robot asks the participant to introduce herself and to talk about her favorite restaurants and films. The next phase is the cucumber phase, when the robot presents its vision technology to the user in a humoristic way by showing that, from its viewpoint, the difference between a cucumber and a human is the face. Finally, the robot enters the survey phase, during which the user is asked to assess her satisfaction with the interaction with Pepper, by answering 15 questions on a 5-level Likert scale (from disagree \"1\" to agree \"5\")  [29, 17]  (see Appendix A).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Participants",
      "text": "The recordings involved 278 users (182 males, 96 females), whose average age was 25 (±9.5) years. This was estimated using an ad hoc software module embedded in Pepper  [2] . The users were students, teachers, researchers, visitors and other staff of Telecom Paris-Tech. A poster on the wall warned users that they were being recorded during the interaction with the robot. The contact information of the main researcher was also made available on the poster. This was done to allow the users to contact the researcher, should they have concerns about the exploitation of their data, and to be able to ask to have it deleted if they so wished. No instructions were given to the user except those provided by the robot in the welcome phase. Users were free to participate in the interaction and free to leave when they wished. The interaction was unsupervised, so the number of users simultaneously involved in it was not controlled. Even though the robot warned that only one user was to be in the first engagement zone at a time, the collected data included 209 interactions featuring a single user, and 69 multiparty interactions (32 started as multiparty and ended as single-user). Note that only 46 users stayed until the end of the scenario and the remaining 72, 84, 70 and 6 users left the interaction at the welcome, dialogue, cucumber and survey phase, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Social Signals On The Robot Pepper",
      "text": "Pepper can record a large variety of data streams ranging from raw signals (audio, video, sonar and laser) to face tracking and estimation of gaze direction, head motion and facial expression. In this work, features were extracted by using the available trackers of NAOqi-SDK as they are integrated in the robot.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interaction Zone",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Storage Server",
      "text": "Transfer over Ethernet Fig.  2  Technical setup. Distance: The distance between the user and the robot was computed using measured raw signals (i.e. sonar) and tracked variables as described below. More specifically, the front sonar 4  (i.e. ultrasonic sensor) was used. The NAOqi People Perception 5  module was also used to extract the distance of the participant's face from the robot camera as well as her 3D head position in relation to the robots torso reference. The space in front of the robot was divided into three configurable zones using the ALEngagementZones module. The default configuration was used here. The first engagement zone is the area about 1.5m away from the robot. In this work, this was used as the interaction zone. The second zone is the area between 1.5m and 2.5m away. The third zone is the area more than 2.5m away from the robot. The participant's position was classified to be in one of these three spaces (or 0 if unknown) using the 3D coordinates of the user's head in the robot frame  6  .\n\nGaze: Pepper's ALGazeAnalysis module gives information about the user face orientation in order to detect whether the user is looking at the robot or not. OpenFace 2.0  [7]  was used to compute gaze direction in relation to the plane of the face  [54] .\n\nHead and Face: OpenFace 2.0  [7]  was also used to compute the head pose of the user along the three axes (yaw, pitch, roll). Moreover, it was also used to recognize the occurrence and intensity of each facial Action Unit (AU)  [6] .\n\nSpeech: The audio signal was recorded at a sampling frequency of 48KHz using 4 microphones that are available inside the head of the robot. The audio signal contains the speech of both the participant and the robot as well as noise in the environment. In order to simplify the analysis of the audio, we selected the first channel (i.e. first microphone) to extract speech features  7  . Speech features included: the fundamental frequency (F0) (extracted via an autocorrelation and cepstrum based method), log-energy, loudness contours, voicing probability and the first 12 MFCCs excluding the 0 th MFCC. All these features were computed from the audio signal over 50-ms windows at a frame rate of 100 Hz with openSMILE  [24] . Features indicating if the robot is speaking or not, as well as the robot's and the user's speech duration, were computed from the dialog (Text-To-Speech and Automatic Speech Recognition) ROSbag topics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Annotation Of Engagement",
      "text": "We developed a script that extracts synchronized front and bottom images 8  and audio from the corresponding ROSbag topics and merges them into a video using ffmpeg  9  . Two annotators with different scientific backgrounds annotated the dataset: a researcher who knew the purpose of the work and an uninformed one who did not. The ELAN annotation tool  [53]  was used to annotate the videos. On all recordings, the annotator indicates the start and the end of the interaction as well as the number of participants (i.e. mono-user or multiusers). In order to characterize engagement, annotators were asked to annotate the interaction video segment by segment based on verbal and non-verbal behaviors expressed by the user that exhibits an engagement decrease, with the following label \"Sign of Engagement Decrease (SED)\". A sign of engagement decrease (SED) reflects any cue exhibited by the user showing any form of disinterest in the robot. It could occur any time during the interaction. This cue may correspond to verbal or nonverbal behaviors of the participant. SED could represent an early sign of future engagement breakdown, that is, a sign that leaving the interaction will occur in the near future and before the end of the scenario. Fig.  3  shows a flow-chart that summarizes the annotation process described above. A video tutorial was created to explain the annotation process and how to annotate the interaction using ELAN. The annotator defines the start and the end segment as well as the corresponding label, observed cues and negative affect of that segment. For each defined segment, the annotator assigns the corresponding observed cues of that decrease, in order of importance. This part could be subsegmented. For example, if the participant says:\"I'm bored\", with a corresponding facial expression, the annotator indicates in the \"Cues 1\" track: \"speech linguistic\" and in \"Cues 2\" track: \"face\". The annotator decides which one is more visible in the segment to appear in \"Cues 1\". If these two cues are successive in time, both should appear in \"Cues 1\" with a sub-segmentation of the start and end of each one. The annotator also assigns the corresponding negative affect of that segment (if relevant) of that decrease. Negative affects (frustration, boredom, nervousness, disappointment, anger, submission) are based on verbal and nonverbal behavior while interacting with Pepper. Annotators are free to add more information concerning this segment. We recommend that they add information about the causes in the \"Causes\" track.  The overall Cohen kappa agreement score on annotated recordings for SED annotation is κ = 0.73 (substantial agreement) (see Fig.  9  in Appendix B). If we automatically correct the annotations by merging together the \"engaged\" segment located between 2 SED segments and inferior to 1 second in duration, to get 1 large SED segment instead of 2 separated by 1 or 2 frames of \"engaged\", the Kappa increases slighty to κ = 0.74 (see Fig.  9 ).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "User Engagement Decrease Analysis",
      "text": "According to both annotators, the average duration of the interaction is 7 (±5) minutes. During interactions, users displayed SED in around 6 segments lasting in average 6 (±9) seconds. Note that for participants who left the interaction by the second phase, the intervals between SED were shorter compared to those who stayed till the end of the interaction. Note also that the last segment where SED were shown is generally longer. In average, its duration is around 9 (±15) seconds. In 90% of the interaction duration, the users are engaged. For the reaming 10%, the users exhibit SED.  Fig.  4  displays the number of occurrences of the behavior exhibited by the users when their engagement decreases, as perceived by the annotators. Fig.  4a  confirms that the non-verbal behaviors play a special role to point the engagement level. Head motion, gesture (i.e. posture, hand waving, and so on) and eye gaze are the most recurrent features to identify a decrease of engagement in our dataset. Fig.  4b  shows that annotators disagreed on selecting the appropriate affects related to the SED segments. This showed that the annotation of affects was more subjective here than the annotations of the SED category and their cues.\n\nDue to the wide variety of possible factors that can cause engagement decrease in spontaneous interactions, it is difficult to determine the exact cause for each SED segment. However, we asked the annotators to try to mention any information related to the cause of that decrease. Table  2  presents the main causes of the engagement decrease detected by the two annotators, with their percentage of occurrence. We individuated two principal sources that lead to the decrease of engagement: the first is due to a social norm violation (e.g. another person interrupts the interaction while the robot is talking; user time constraint; user is using her phone); the second cause is due to robot's technical issues (e.g. robot makes long pauses or misunderstands the user).\n\nWe compared users' behaviors when they were engaged with the robot vs. when they showed signs of engagement decrease based on the annotations. Figure  5  presents the results of the comparison for the different configurations: when both annotators perceived the user as being engaged (denoted by \"Engagement agreed\"), when both annotators agreed about the user engagement decrease (denoted by \"SED agreed\") and when both annotators disagreed about the engagement state (denoted by \"SED: Ax \" when a decrease of engagement is perceived by one annotator x and not by the other one). Figure  5a  shows the average distance be-tween the user and the robot. The users were closer to the robot when they were fully engaged than when their engagement decreased. Regarding gaze, when users were engaged they looked more at the robot than when their engagement decreased (Figure  5e  \"1\" when the user looks at the robot, \"0\" otherwise). This could be confirmed with vertical gaze direction around pitch axis (i.e. angle x) in Figure  5b . Head motion (i.e. shaking, tilting and nodding) were displayed in Figure  5c . Users move their head more when their engagement decreases. Concerning action units (AU)  [45]  (see Figure  5d ), we found that users have the appearance of being happier (where happiness involves AU06 and AU12) when they are engaged, compared to when their engagement decreases. Similarly, for sadness, which is the combination of AU01, AU04, AU15, anger (the combination of AU04, AU05, AU07, AU23) and disgust (the combination of AU09, AU15, AU16), it appears that users express these negative emotions when their engagement decreases, compared to when they are engaged. Figure  5f  shows that the users are more engaged when the robot is speaking. This could be confirmed with Figure 5e (i.e. \"1\" when the robot is speaking, \"0\" when the robot is listening).\n\nIn the next section, for training and testing of engagement decrease detection, we consider only the segments where both annotators agreed on the engagement category.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Detection Of User Engagement Decrease",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "User Engagement Modeling",
      "text": "We modeled the task of user engagement decrease detection as a binary classification, where the goal is to predict, in real-time, whether the user is engaged or not with the robot, based on the user's behavior analysis.\n\nOur SED detection approach is illustrated in Figure  6 . We define observation window as a window of [t -τ, t], that is, a window that ends at time t and takes into account the last τ seconds of user behavior. We use [x t-τ , . . . , x t-1 , x t ] as a feature vector computed over the frames of the observation window as input for the classifier. As for the output, each observation window is labeled as either engaged or not.\n\nAt running time t, we build a model that classifies the observed behavior over [t -τ, t] as user engaged or user not engaged. Let X = [x 1 , x 2 , . . . , x T ] denote the sequence of multimodal user-behavior feature vectors and Y η = [y η 1 , y η 2 , ..., y η T ] denote the corresponding sequence of (binary valued) output labels, where η is the duration of the buffer for holding more observations and\n\nwhere C(.) is the classifier decision function and y η t = 1, SED perceived at time t -η y η t = 0, otherwise",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Deep Networks",
      "text": "In this study, a sequential modeling approach is proposed to detect SED using deep learning techniques  [10] .\n\nRemembering information for long periods of time is the default behavior of Long-Short Term Memory (LSTM)  [32] . LSTM uses a memory unit that can remember information/context from the beginning of the input sequence (i.e. t-τ ). Gated Recurrent Unit (GRU) networks  [18]  are similar to the LSTM, but use a simplified structure. Both LSTM and GRU can be used for modeling temporal sequences. However, GRU involves less computation units than LSTM, since they do not have an output gate. Therefore LSTM are usually preferred if trained on very large datasets (big data).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "The data streams of different sampling frequencies were indexed using the robot's timestamps. To obtain synchronized feature vectors, temporal integration  [34]  (a.k.a temporal pooling), is performed over all feature streams using common integration windows. The integrated features are obtained by applying an integration function f over sliding (possibly overlapping) integration windows of length L seconds. The functions f used in this study are statistics, namely the mean and variance. Also, we fix the integration window length L to 500 ms. No overlapping was used. It was shown that combining multiple features gives the highest performance in disengagement prediction (c.f. Section 2.2). Therefore, the synchronized texture-window level feature vectors of Distance, Gaze, Head, Face and Speech Streams shown in Table  1  were concatenated together to describe users' behavior and were employed as the input features for the SED detection model. Further details are given in our previous work  [8]  Our dataset contains missing values. For example, we have missing values on the face features (i.e. head motion, gaze, AU) when some occlusion occur. This happens for instance when the robot's head is moving, causing the user's face to go out of the cameras' field of view. We chose to replace the missing values by means of the corresponding feature from the training data. We then normalized the data by subtracting the mean value and dividing by the standard deviation of each feature, using the training data.\n\nObservation window of τ seconds  The whole dataset using both single user and multiparty interactions was used, since this was reported to be a good compromise in  [37] . We used 3-fold crossvalidation to train and test a set of SED classifiers. The split of train and test sets was done at the interactionlevel. Hence, the users of the test set (i.e. all observations of the user) were not seen during the training phase, which resulted in a user-independent detection model.\n\nWe used scikit-learn's  [42]  implementation of logistic regression as a baseline and Keras's  [19]  implementation for DNN, GRU and LSTM. We leave the further optimization of the classifiers' hyper-parameters for future work and focus here on the validation of the usefulness of the recurrent network architectures considered.\n\nFollowing preliminary experiments, we used 2 layers with 32 units followed by 2 units, ReLU activation, dropout with probability of 0.1 and the RMSprop algorithm as optimizer to train the deep networks (see Fig.  7 ). We used 10% of the training data as a validation set. We trained each model with 100 epochs, using an early stopping callback to stop the training once the validation accuracy started to decrease. In general, the models converge after a maximum of 35 epochs.\n\nFor logistic regression, we used 2 regularization and the inverse of the regularization strength C set to 1. To deal with the imbalanced data distribution, the weights for each class were computed and used for training the models.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Criteria",
      "text": "Traditionally, the accuracy rate and F1-score have been the most commonly used evaluation criteria. However, they are not well suited to our study because the dataset is unbalanced. We have around 90% of the data labeled as engaged and only 10% of SED. In case of imbalanced data, the accuracy reflects only the underlying class distribution, not the prediction performance of the minority class. In order to compute meaningful accuracy and F1-score, the test set should represent the true distribution of both classes. Therefore, the test set is resampled to be the average over all the samples of the minority class and the n-differing samples of the majority class selected from the available samples. We also computed the area under the receiver operating characteristic curve (AUC) in order to determine which of the models used predicts the classes best. The AUC corresponds to the probability of correctly identifying the SED class  [3] . The closer the AUC comes to 1, the more accurate it is.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "The performance of the different sets of classifiers was compared (see Fig.  8 ). We found that deep learning techniques are better than conventional machine learning techniques (i.e. Logistic Regression). With the chosen hyper-parameter values the best results were obtained with LSTM for all tested buffer durations η. This is because they better model the temporal dynamics through connections between hidden units in the same layer.\n\nWhen we use a buffer delay in the range of  [1, 3]  seconds, the performance of all the classifiers increases. This could be explained by the fact that using more information about the users behavior plays an important role in inferring the state of their engagement. A buffer longer than 3 seconds does not give a better performance. In addition, a buffer of 3 seconds is already large for real-time detection  [41] .\n\nTo better understand how performance is affected by the size τ of the observation window of the user behavior, we varied it from 0 to 6 seconds. Table  3  shows this variation for each buffer η. For real-time operation using η = 0, the best results were found using short observation windows of τ = 1 seconds for detecting SED. Increasing the buffer duration up to 3 seconds improves the performance of the SED detector. The best performance was found using an observation window of τ = 5 seconds for a buffer η of 3 seconds and at approximately the same performance for a buffer of 2 seconds. We note that taking a buffer duration to make a decision approximately in the middle of the observation window is the best strategy to detect SED, and the optimal size of the observation window is inferior to the average duration of SED segments (i.e. 6 seconds). Table  4  shows that logistic regression presents 30% more false alerts than LSTM and 7% fewer undetected engagement decreases.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "In order to develop lifelike humanoid robots that understand better the behavior of the humans with whom they interact and can respond appropriately to increase user engagement, we investigated the use of deep networks to successfully detect SED. We achieved good performance: 78% of accuracy, 0.78 of F1-score and 0.87 of AUC. Note that in other related studies, the performances of engagement detection systems, using different datasets, were 62% accuracy and 0.61 F1-score in  [27]  and 73% accuracy and 0.62 AUC in  [37] . Thus we are using a bigger data-set with different annotation schema. But, we achieve promising results that could be improved and integrated in the robot architecture to detect SED with real-time capability.\n\nThe classifiers provide not only the class of user engagement, but also the estimated confidence that could be used as additional information, representing the system's uncertainty, in real-world human-robot applications.\n\nIn preliminary experiments using less data (e.g. 195 interactions), the best performances for GRU/LSTM using a buffer of η = 1 second and an observation widow of τ = 2 seconds were 76%/75%, 0.75/0.75 and 0.84/0.84 for accuracy, F1-score and AUC, respectively. Thus, the GRU gives a slightly better performance.\n\nWe evaluate the impact of two different extractors: OpenFace  [7] , and Pepper's OKAO TM Vision software  [2]  tracker of gaze direction, head motion and facial expression/AU. Table  5  compares the performance of these extractors on the task of detecting SED using LSTM with an observation window of τ = 5 seconds and a buffer of η = 2 seconds. We found that Open-Face performs better than Pepper's tracker. Note that when features are missing (e.g. when the robot's head is moving and user's facial features cannot be determined), we focus on the other modality (i.e. distance, speech) to detect SED.\n\nIn spontaneous HRI, finding the exact moment of SED is a hard decision. It depends on the head motion, the looking away, the spoken word, getting away from the robot, etc. The annotated start and end of this segment is flexible and could vary by ±n frames (see Fig.  9b  in Appendix B). It would be interesting to take into account this flexibility both in the training and in the testing phases instead of using it only when the annotators agree and ignoring the parts where they disagree.\n\nFuture work should also investigate whether the SED detection model generalizes well to other interaction settings (i.e. other scenarios, multiparty). Tracker Accuracy F1-score AUC OpenFace  [7]  78.56 0.784 0.869 Pepper OKAO software  [2]  76.33 0.762 0.849",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "We analyzed users' behavior in two engagement states where they exhibited engaged behavior or, alternatively, signs of engagement decrease. We found significant differences in their behavior that allowed us to develop a real-time detector of engagement decrease during a spontaneous interaction with a humanoid robot.\n\nWe then studied the use of deep learning techniques with multimodal data for real-time detection of user engagement decrease. Our engagement classification results show that the real-time detector taking into account the past user behavior without any buffer performs well. Using the temporal dynamics of user behavior improves the results as well. The optimal size of the observation window of user behavior is found to be smaller than the average duration of SED segments (i.e. 6 seconds). Moreover, by using a delay of 1 or 2 seconds, we improved the performance of the detector. Depending on the application context, these delays could be reasonably suitable to improve the experience quality of interacting with the robot in-the-wild.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The collected data constitute the",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the experimental setup of the interaction.",
      "page": 4
    },
    {
      "caption": "Figure 1: Participant in the ﬁrst engagement zone (less than",
      "page": 4
    },
    {
      "caption": "Figure 2: Technical setup.",
      "page": 5
    },
    {
      "caption": "Figure 3: Flow-chart of the diﬀerent annotation levels. The ’*’",
      "page": 6
    },
    {
      "caption": "Figure 3: shows a ﬂow-chart that summarizes the an-",
      "page": 6
    },
    {
      "caption": "Figure 4: Cues and aﬀects distribution of signs of engagement",
      "page": 6
    },
    {
      "caption": "Figure 9: in Appendix B). If we",
      "page": 6
    },
    {
      "caption": "Figure 5: Selected features of users’ behavior when the two annotators (A1 and A2) agreed on their engagement as well as when",
      "page": 7
    },
    {
      "caption": "Figure 4: displays the number of occurrences of the be-",
      "page": 7
    },
    {
      "caption": "Figure 4: b shows that annotators",
      "page": 7
    },
    {
      "caption": "Figure 5: a shows the average distance be-",
      "page": 7
    },
    {
      "caption": "Figure 5: e “1” when the user",
      "page": 8
    },
    {
      "caption": "Figure 5: b. Head motion (i.e. shaking,",
      "page": 8
    },
    {
      "caption": "Figure 6: Illustration of the detection approach. Input: observation window of user behavior is shown in green. Output: buﬀer",
      "page": 9
    },
    {
      "caption": "Figure 7: Many-to-One deep architecture with 2 layers.",
      "page": 9
    },
    {
      "caption": "Figure 7: ). We used 10% of the training data as a valida-",
      "page": 9
    },
    {
      "caption": "Figure 8: ). We found that deep learning",
      "page": 9
    },
    {
      "caption": "Figure 8: Performance of a set of classiﬁers using an observation",
      "page": 10
    },
    {
      "caption": "Figure 9: b in Appendix B). It would be interesting",
      "page": 10
    },
    {
      "caption": "Figure 9: Example of annotation. BD: Engagement BreakDown",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 3: LSTMperformancefordifferentobservation windowsτ (sec)foreachbufferη (sec)with τ ≥η.",
      "data": [
        {
          "Accuracy": "0\n1\n2\n3\n4\n5",
          "F1-score": "0\n1\n2\n3\n4\n5",
          "AUC": "0\n1\n2\n3\n4\n5"
        },
        {
          "Accuracy": "73.42\n-\n-\n-\n-\n-\n74.03\n76.97\n-\n-\n-\n-\n73.70\n76.73\n77.26\n-\n-\n-\n73.74\n77.25\n78.06\n78.32\n-\n-\n72.11\n75.60\n77.43\n77.97\n77.02\n-\n72.62\n76.88\n78.56\n78.83\n77.65\n76.07\n71.52\n76.28\n74.95\n78.23\n77.67\n75.75",
          "F1-score": "0.732\n-\n-\n-\n-\n-\n0.738\n0.768\n-\n-\n-\n-\n0.734\n0.765\n0.771\n-\n-\n-\n0.734\n0.770\n0.779\n0.782\n-\n-\n0.716\n0.752\n0.772\n0.777\n0.767\n-\n0.721\n0.766\n0.784\n0.787\n0.774\n0.739\n0.708\n0.759\n0.744\n0.780\n0.774\n0.732",
          "AUC": "0.820\n-\n-\n-\n-\n-\n0.827\n0.851\n-\n-\n-\n-\n0.823\n0.850\n0.849\n-\n-\n-\n0.826\n0.860\n0.863\n0.862\n-\n-\n0.815\n0.844\n0.860\n0.861\n0.850\n-\n0.817\n0.859\n0.869\n0.865\n0.859\n0.863\n0.818\n0.847\n0.847\n0.865\n0.861\n0.856"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Robots in public spaces: towards multi-party, shortterm, dynamic human-robot interaction",
      "year": "2013",
      "venue": "International Conference on Social Robotics (ICSR 2013)"
    },
    {
      "citation_id": "2",
      "title": "Human Vision Components (HVC-P2) B5T-007001 Command Specifications. Tech. rep., OMRON Corporation Electronic and Mechanical Components Company",
      "year": "2016",
      "venue": "Human Vision Components (HVC-P2) B5T-007001 Command Specifications. Tech. rep., OMRON Corporation Electronic and Mechanical Components Company"
    },
    {
      "citation_id": "3",
      "title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
      "authors": [
        "Andrew Bradley"
      ],
      "year": "1997",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "What Went Wrong and Why? Diagnosing Situated Interaction Failures in the Wild",
      "authors": [
        "S Andrist",
        "D Bohus",
        "E Kamar",
        "E Horvitz"
      ],
      "year": "2017",
      "venue": "th International Conference on Social Robotics (ICSR)"
    },
    {
      "citation_id": "5",
      "title": "Automated prediction of extraversion during human-robot interaction",
      "authors": [
        "S Anzalone",
        "G Varni",
        "E Zibetti",
        "S Ivaldi",
        "M Chetouani"
      ],
      "year": "2015",
      "venue": "AIRO@AI*IA"
    },
    {
      "citation_id": "6",
      "title": "Crossdataset learning and person-specific normalisation for automatic Action Unit detection",
      "authors": [
        "T Baltrusaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "7",
      "title": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "8",
      "title": "Early detection of user engagement breakdown in spontaneous humanhumanoid interaction",
      "authors": [
        "A Ben-Youssef",
        "C Clavel",
        "S Essid"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2898399"
    },
    {
      "citation_id": "9",
      "title": "UE-HRI: A New Dataset for the Study of User Engagement in Spontaneous Human-robot Interactions",
      "authors": [
        "A Ben-Youssef",
        "C Clavel",
        "S Essid",
        "M Bilac",
        "M Chamoux",
        "A Lim"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Learning Deep Architectures for AI",
      "authors": [
        "Y Bengio",
        "Yoshua"
      ],
      "year": "2009",
      "venue": "Foundations and Trends R in Machine Learning"
    },
    {
      "citation_id": "11",
      "title": "Learning to predict engagement with a spoken dialog system in open-world settings",
      "authors": [
        "D Bohus",
        "E Horvitz"
      ],
      "year": "2009",
      "venue": "Proceedings of the SIGDIAL 2009 Conference on The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue -SIGDIAL '09"
    },
    {
      "citation_id": "12",
      "title": "Models for Multiparty Engagement in Open-world Dialog",
      "authors": [
        "D Bohus",
        "E Horvitz"
      ],
      "year": "2009",
      "venue": "Proceedings of the SIG-DIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIG-DIAL '09"
    },
    {
      "citation_id": "13",
      "title": "Open-World Dialog: Challenges, Directions, and a Prototype",
      "authors": [
        "D Bohus",
        "E Horvitz"
      ],
      "year": "2009",
      "venue": "Proceedings of the IJCAI'2009 Workshop on Knowledge and Reasoning in Practical Dialogue Systems"
    },
    {
      "citation_id": "14",
      "title": "Managing Human-Robot Engagement with Forecasts and... um... Hesitations",
      "authors": [
        "D Bohus",
        "E Horvitz"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction -ICMI '14"
    },
    {
      "citation_id": "15",
      "title": "The Affective Experience of Novice Computer Programmers",
      "authors": [
        "N Bosch",
        "S D'mello"
      ],
      "year": "2015",
      "venue": "International Journal of Artificial Intelligence in Education"
    },
    {
      "citation_id": "16",
      "title": "Detecting Engagement in HRI: An Exploration of Social and Task-Based Context",
      "authors": [
        "G Castellano",
        "I Leite",
        "A Pereira",
        "C Martinho",
        "A Paiva",
        "P Mcowan"
      ],
      "year": "2012",
      "venue": "2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing"
    },
    {
      "citation_id": "17",
      "title": "Multimodal Human-Human-Robot Interactions (MHHRI) Dataset for Studying Personality and Engagement",
      "authors": [
        "O Celiktutan",
        "E Skordos",
        "H Gunes"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "authors": [
        "K Cho",
        "B Van Merrienboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
    },
    {
      "citation_id": "19",
      "title": "keras",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "keras"
    },
    {
      "citation_id": "20",
      "title": "Fostering User Engagement in Face-to-Face Human-Agent Interactions: A Survey",
      "authors": [
        "C Clavel",
        "A Cafaro",
        "S Campano",
        "C Pelachaud"
      ],
      "year": "2016",
      "venue": "Fostering User Engagement in Face-to-Face Human-Agent Interactions: A Survey"
    },
    {
      "citation_id": "21",
      "title": "Engagement Perception and Generation for Social Robots and Virtual Agents",
      "authors": [
        "L Corrigan",
        "C Peters",
        "D Küster",
        "G Castellano"
      ],
      "year": "2016",
      "venue": "Engagement Perception and Generation for Social Robots and Virtual Agents"
    },
    {
      "citation_id": "22",
      "title": "Dynamics of affective states during complex learning",
      "authors": [
        "S D'mello",
        "A Graesser"
      ],
      "year": "2012",
      "venue": "Learning and Instruction"
    },
    {
      "citation_id": "23",
      "title": "Anticipation and initiative in human-humanoid interaction",
      "authors": [
        "P Dominey",
        "G Metta",
        "F Nori",
        "L Natale"
      ],
      "year": "2008",
      "venue": "Humanoids 2008 -8th IEEE-RAS International Conference on Humanoid Robots"
    },
    {
      "citation_id": "24",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the international conference on Multimedia -MM '10"
    },
    {
      "citation_id": "25",
      "title": "Defining Socially Assistive Robotics",
      "authors": [
        "D Feil-Seifer",
        "M Mataric"
      ],
      "year": "2005",
      "venue": "th International Conference on Rehabilitation Robotics"
    },
    {
      "citation_id": "26",
      "title": "The MuM-MER Project: Engaging Human-Robot Interaction in Real-World Public Spaces",
      "authors": [
        "M Foster",
        "R Alami",
        "O Gestranius",
        "O Lemon",
        "M Niemelä",
        "J Odobez",
        "A Pandey"
      ],
      "year": "2016",
      "venue": "The MuM-MER Project: Engaging Human-Robot Interaction in Real-World Public Spaces"
    },
    {
      "citation_id": "27",
      "title": "Automatically Classifying User Engagement for Dynamic Multi-party HumanRobot Interaction",
      "authors": [
        "M Foster",
        "A Gaschler",
        "M Giuliani"
      ],
      "year": "2017",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "28",
      "title": "How to Open an Interaction Between Robot and Museum Visitor?: Strategies to Establish a Focused Encounter in HRI",
      "authors": [
        "R Gehle",
        "K Pitsch",
        "T Dankert",
        "S Wrede"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction -HRI '17"
    },
    {
      "citation_id": "29",
      "title": "User Engagement and Preferences in Information-Giving Chat with Virtual Agents",
      "authors": [
        "N Glas",
        "C Pelachaud"
      ],
      "year": "2015",
      "venue": "User Engagement and Preferences in Information-Giving Chat with Virtual Agents"
    },
    {
      "citation_id": "30",
      "title": "Perception of own and robot engagement in humanrobot interactions and their dependence on robotics knowledge",
      "authors": [
        "J Hall",
        "T Tritton",
        "A Rowe",
        "A Pipe",
        "C Melhuish",
        "U Leonards"
      ],
      "year": "2014",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "31",
      "title": "Humanoid robots as a passive-social medium",
      "authors": [
        "K Hayashi",
        "D Sakamoto",
        "T Kanda",
        "M Shiomi",
        "S Koizumi",
        "H Ishiguro",
        "T Ogasawara",
        "N Hagita"
      ],
      "year": "2007",
      "venue": "Proceeding of the ACM/IEEE international conference on Human-robot interaction -HRI '07"
    },
    {
      "citation_id": "32",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "33",
      "title": "Towards Engagement Models that Consider Individual Factors in HRI: On the Relation of Extroversion and Negative Attitude Towards Robots to Gaze and Speech During a HumanRobot Assembly Task",
      "authors": [
        "S Ivaldi",
        "S Lefort",
        "J Peters",
        "M Chetouani",
        "J Provasi",
        "E Zibetti"
      ],
      "year": "2017",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "34",
      "title": "Temporal integration for audio classification with application to musical instrument classification",
      "authors": [
        "C Joder",
        "S Essid",
        "G Richard"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "An affective guide robot in a shopping mall",
      "authors": [
        "T Kanda",
        "M Shiomi",
        "Z Miyashita",
        "H Ishiguro",
        "N Hagita"
      ],
      "year": "2009",
      "venue": "Proceedings of the 4th ACM/IEEE international conference on Human robot interaction -HRI '09"
    },
    {
      "citation_id": "36",
      "title": "Some functions of gaze-direction in social interaction",
      "authors": [
        "A Kendon"
      ],
      "year": "1967",
      "venue": "Acta Psychologica"
    },
    {
      "citation_id": "37",
      "title": "Comparing Models of Disengagement in Individual and Group Interactions",
      "authors": [
        "I Leite",
        "M Mccoy",
        "D Ullman",
        "N Salomons",
        "B Scassellati"
      ],
      "year": "2015",
      "venue": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction -HRI '15"
    },
    {
      "citation_id": "38",
      "title": "Attention-based Addressee Selection for Service and Social Robots to Interact with Multiple Persons",
      "authors": [
        "L Li",
        "Q Xu",
        "Y Tan"
      ],
      "year": "2012",
      "venue": "Proceedings of the Workshop at SIGGRAPH Asia, WASA '12"
    },
    {
      "citation_id": "39",
      "title": "Predicting engagement breakdown in hri using thin-slices of facial expressions",
      "authors": [
        "T Liu",
        "A Kappas"
      ],
      "year": "2018",
      "venue": "Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "The Error is the Clue: Breakdown in Human-Machine Interaction",
      "authors": [
        "B Martinovski",
        "D Traum"
      ],
      "year": "2003",
      "venue": "Proceedings of the ISCA Workshop on Error Handling in Spoken Dialogue Systems"
    },
    {
      "citation_id": "41",
      "title": "fall joint computer conference, part I on -AFIPS '68 (Fall, part I)",
      "authors": [
        "R Miller"
      ],
      "year": "1968",
      "venue": "Proceedings of the December 9-11"
    },
    {
      "citation_id": "42",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "É Duchesnay"
      ],
      "year": "2011",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "43",
      "title": "The first five seconds: Contingent stepwise entry into an interaction as a means to secure sustained engagement in HRI",
      "authors": [
        "K Pitsch",
        "H Kuzuoka",
        "Y Suzuki",
        "L Sussenbach",
        "P Luff",
        "C Heath"
      ],
      "year": "2009",
      "venue": "RO-MAN 2009 -The 18th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "44",
      "title": "Mind, Hands, Face and Body: A Goal and Belief View of Multimodal Communication",
      "authors": [
        "I Poggi"
      ],
      "year": "2007",
      "venue": "Mind, Hands, Face and Body: A Goal and Belief View of Multimodal Communication"
    },
    {
      "citation_id": "45",
      "title": "Scalable Daily Human Behavioral Pattern Mining from Multivariate Temporal Data",
      "authors": [
        "R Rawassizadeh",
        "E Momeni",
        "C Dobbins",
        "J Gharibshah",
        "M Pazzani"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "46",
      "title": "Recognizing engagement in human-robot interaction",
      "authors": [
        "C Rich",
        "B Ponsler",
        "A Holroyd",
        "C Sidner"
      ],
      "year": "2010",
      "venue": "2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "47",
      "title": "Multimodal sentiment analysis in the wild: Ethical considerations on data collection, annotation, and exploitation",
      "authors": [
        "B Schuller",
        "J Ganascia",
        "L Devillers"
      ],
      "year": "2016",
      "venue": "Actes du Workshop on Ethics In Corpus Collection, Annotation & Application"
    },
    {
      "citation_id": "48",
      "title": "Audiovisual recognition of spontaneous interest within conversations",
      "authors": [
        "B Schuller",
        "R Müeller",
        "B Höernler",
        "A Höethker",
        "H Konosu",
        "G Rigoll"
      ],
      "year": "2007",
      "venue": "Proceedings of the ninth international conference on Multimodal interfaces -ICMI '07"
    },
    {
      "citation_id": "49",
      "title": "Explorations in engagement for humans and robots",
      "authors": [
        "C Sidner",
        "C Lee",
        "C Kidd",
        "N Lesh",
        "C Rich"
      ],
      "year": "2005",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "50",
      "title": "Socially Assistive Robots: The Link between Personality, Empathy, Physiological Signals, and Task Performance",
      "authors": [
        "A Tapus",
        "M Mataric"
      ],
      "year": "2008",
      "venue": "Socially Assistive Robots: The Link between Personality, Empathy, Physiological Signals, and Task Performance"
    },
    {
      "citation_id": "51",
      "title": "Head and shoulders: automatic error detection in human-robot interaction",
      "authors": [
        "P Giuliani",
        "M Miksch",
        "M Stollnberger",
        "G Stadler",
        "S Mirnig",
        "N Tscheligi"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction -ICMI 2017"
    },
    {
      "citation_id": "52",
      "title": "Starting engagement detection towards a companion robot using multimodal features",
      "authors": [
        "D Vaufreydaz",
        "W Johal",
        "C Combe"
      ],
      "year": "2016",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "53",
      "title": "ELAN: a Professional Framework for Multimodality Research",
      "authors": [
        "P Wittenburg",
        "H Brugman",
        "A Russel",
        "A Klassmann",
        "H Sloetjes"
      ],
      "year": "2006",
      "venue": "LREC 2006"
    },
    {
      "citation_id": "54",
      "title": "Rendering of Eyes for Eye-Shape Registration and Gaze Estimation",
      "authors": [
        "E Wood",
        "T Baltruaitis",
        "X Zhang",
        "Y Sugano",
        "P Robinson",
        "A Bulling"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)"
    }
  ]
}