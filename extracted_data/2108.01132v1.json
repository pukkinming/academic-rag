{
  "paper_id": "2108.01132v1",
  "title": "The Role Of Phonetic Units In Speech Emotion Recognition",
  "published": "2021-08-02T19:19:47Z",
  "authors": [
    "Jiahong Yuan",
    "Xingyu Cai",
    "Renjie Zheng",
    "Liang Huang",
    "Kenneth Church"
  ],
  "keywords": [
    "speech emotion recognition",
    "Wav2vec 2.0",
    "finetuning",
    "broad phonetic classes"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose a method for emotion recognition through emotiondependent speech recognition using Wav2vec 2.0. Our method achieved a significant improvement over most previously reported results on IEMOCAP, a benchmark emotion dataset. Different types of phonetic units are employed and compared in terms of accuracy and robustness of emotion recognition within and across datasets and languages. Models of phonemes, broad phonetic classes, and syllables all significantly outperform the utterance model, demonstrating that phonetic units are helpful and should be incorporated in speech emotion recognition. The best performance is from using broad phonetic classes. Further research is needed to investigate the optimal set of broad phonetic classes for the task of emotion recognition. Finally, we found that Wav2vec 2.0 can be fine-tuned to recognize coarsergrained or larger phonetic units than phonemes, such as broad phonetic classes and syllables.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is essentially a sequence-toone classification problem whereas speech recognition is a sequence-to-sequence problem. In this paper we attempt to bridge these two problems. More specifically, we propose a method for emotion recognition through emotion-dependent speech recognition.\n\nSpeech emotion recognition has witnessed a stable advancement over the last two decades  [1, 2, 3, 4, 5] . Much of the earlier effort was concentrated on feature engineering. The emobase feature set in the widely used OpenSMILE toolkit  [6] , for example, consists of 998 acoustic features for emotion recognition, including prosodic, spectral, as well as voice quality features. In recent years, more effort has been devoted to improving deep learning model architectures. Many studies in the literature are based on IEMOCAP  [7] , a benchmark emotion dataset. At ICASSP 2020, half a dozen papers reported a cross-validation accuracy of 70% or better on this dataset  [8, 9, 10, 11, 12] , based on acoustic data only.  [8]  achieved the highest accuracy of 73% from employing a dual-sequence LSTM model.\n\nA confounding factor for emotion recognition is the acoustic variability of different phonetic units such as phonemes. Figure  1  compares the spectra of two vowels (from the same speaker) in the same emotion. Clearly, their spectra are different in terms of the location of spectral peaks, which determines vowel quality. There are two approaches to overcome this problem. The predominant one is to make emotion features and models independent and more robust to phonetic variability. The other approach is to take consideration of phonetic variability by developing phonetically-aware features and models. This study adopts the second approach.\n\nA relatively small number of studies of speech emotion recognition have investigated the effect of phonetic variability.\n\n[13] incorporated articulatory information in emotion recognition. They observed that for the vowel /AE/ anger forces a larger opening of the jaw as opposed to sadness, while for the vowel /IY/ anger makes lips more protruded towards the outside.  [14]  computed statistics of Mel-Frequency Cepstral Coefficients over three phonetic classes (stressed vowels, unstressed vowels, and consonants) respectively for emotion recognition. They found that spectral features computed from consonant regions of the utterance contain more information about emotion than either stressed or unstressed vowel features.  [15]  investigated whether acoustic emotion recognition strongly depends on phonetic content. They demonstrated that phoneme-specific emotion models can lead to higher accuracies.  [16]  used a selfattention based emotion classification model to understand the phonetic bases of emotions by discovering the most \"attended\" phonemes for each emotion. They found that the distribution of these attended-phonemes varies significantly between natural versus acted emotions.\n\nTo make use of the effect of phonetic variability for emotion recognition, we train emotion-dependent speech recognition models. For example, the vowel /AA/ in \"happy\" and \"sad\" are different phonemes and have different acoustic models. With emotion-dependent speech recognition models, emotion recognition can be done as a by-product of speech recognition. Similar efforts have been made in the literature.  [17]  trained two sets of HMM/GMM acoustic models of phonemes for high-arousal and low-arousal/neutral emotions respectively, and determined the emotion of each phoneme in an utterance (generated from speech recognition) by applying and comparing the two models of the phoneme. Compared to  [17] , our approach is simpler. We combine speech recognition and emotion classification into one step of recognizing emotion-dependent phonetic units such as phonemes.\n\nEmotion-dependent models may require more training data than emotion-independent models because the number of units/models is increased by multiple times (n times where n is the number of emotions). However, with the recent advancement of pretrained acoustic models, the amount of data needed for training speech recognition models is greatly reduced. For example, Wav2vec 2.0  [18]  outperforms the previous state of the art on the Librispeech  [19]  test set in terms of word error rate while using 100 times less labeled data. By fine-tuning Wav2vec 2.0, we can train well-performing emotion-dependent models even with a small amount of data.\n\nWe can also reduce the size of phonetic units in emotiondependent models by using coarser-grained (e.g., broad phonetic classes) or larger (e.g., syllables) units than phonemes. Besides to increase the training data for each unit, we explore this idea in the study from three perspectives: 1. Can pretrained acoustic representations be fine-tuned to recognize coarser or larger phonetic units than phonemes? 2. What are the best pho- netic units for emotion recognition? 3. Given that phonemes are language-specific while broad phonetic classes and syllables are language-general, are broad-phonetic-class or syllable models more robust than phoneme models for cross-lingual emotion recognition?\n\nIn the following sections we will first introduce the method of fine-tuning Wav2vec 2.0 for emotion recognition, followed by experiments on the English IEMOCAP dataset and three other datasets, in German, Arabic, and Mandarin Chinese respectively. Conclusions and discussion are made in the last section.\n\n2. Fine-tuning Wav2vec2.0 for emotion recognition The procedure of fine-tuning Wav2vec 2.0 for emotion recognition is illustrated and described in Figure  2 . The core idea is to use emotion-dependent units as labeled targets. A randomly initialized linear projection is added on top of the contextual representations of Wav2vec 2.0 to map the representations into emotion-dependent units (i.e., classes), and the entire model is optimized by minimizing the CTC loss  [20]  through fine-tuning.\n\nThe fine-tuned model can be used to transcribe speech into emotion-dependent units. For the purpose of speech emotion recognition, the recognized emotion-dependent units of an utterance are mapped to an emotion category by majority vote. For example, if most of the recognized units are of \"happy\", then the utterance will be classified as \"happy\".",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Phonetic Units",
      "text": "We tried four types of phonetic units for recognition: phonemes, broad phonetic classes, syllables, and the entire utterance. They are summarized in Figure  4  with examples.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments On Iemocap",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "IEMOCAP is a benchmark emotion dataset in English. It consists of 12 hours of speech from 10 professional actors. Following the literature, we extracted 5531 utterances of four emotion types from the dataset: 1708 neutral, 1636 happy (also including excited), 1103 angry, and 1084 sad.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fine-Tuning",
      "text": "The wav2vec 2.0 large model pre-trained on 960 hours of Librispeech audio, libri960 big.pt, was used for fine-tuning. The model was fine-tuned with 15k updates in all our experiments. For the first 10k updates only the output classifier is trained, after which the Transformer is also updated. The max tokens was set to 1m (which is equivalent to 62.5-second audio with sampling rate of 16 kHz), the learning rate was 5e-5.\n\nFigure  4  shows a typical training loss curve observed in fine-tuning.\n\nTo be consistent with the literature, we conducted 10-fold cross validation on the dataset. In each fold, the utterances of one speaker were used for testing, and the other nine speakers for fine-tuning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Classification Of Emotion",
      "text": "The performance of emotion recognition has been evaluated in terms of both weighted and unweighted accuracy. Weighted accuracy (WA) is the overall accuracy on the entire dataset while unweighted accuracy (UA) is the average of accuracies (recalls) of the emotion categories. The accuracies of models using different phonetic units are reported in Table  1 .\n\nFrom Table  1  we can see that all models perform significantly better than most previously reported results, except the utterance model. In the utterance model, the entire utterance is a unit therefore there are only four emotion dependent units (one for each of the four emotions) for recognition. This is essentially a classification on the utterance without considering its phonetic content. The results demonstrate that phonetic units are helpful and should be incorporated in speech emotion recognition.\n\nThe best performance is from broad phonetic classes, and phonemes and syllables perform equally well. Table  2  is the confusion matrix of the emotion recognition results from fine- tuning broad phonetic class models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Recognition Of Phonetic Units",
      "text": "Although Wav2vec 2.0 has achieved great success on speech recognition in terms of word error rate and phoneme error rate, it remains to be investigated whether the model can be finetuned to recognize other phonetic unites such as broad phonetic classes and syllables.\n\nWe evaluated the recognition results of phonemes, broad phonetic classes, and syllables using the NIST scoring toolkit SCTK  [21] , both including and excluding emotions. The error rates are listed in Table  3 . If we don't count the emotion type of each recognized phonetic unit, the recognition error rates for phonemes, broad phonetic classes, and syllables are 14.9%, 11.5%, and 16.3% respectively. The results show that Wav2vec 2.0 can be fine-tuned to recognize not only phonemes, but also coarser-grained or larger phonetic units such as broad phonetic classes and syllables, without a language model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments On Datasets In Other Languages",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "We re-trained models of phonemes, broad phonetic classes, and syllables on the entire dataset of IEMOCAP, with the same hyperparameters as used in the experiments above. We then tested these models on three other emotion datasets, EmoDB  [22] , KSUEmotions  [23] , and Mandarin Affective Speech  [24] . EmoDB is a German dataset of emotional speech, containing utterances performed in seven target emotions (including neutral) by ten actors. KSUEmotions contains emotional Modern Standard Arabic (MSA) speech from 23 subjects, in six emotions. Mandarin Affective Speech contains recordings in five emotions by 68 Mandarin speakers. The datasets all contain neutral, happy, angry and sad emotions. The utterances in these emotions were extracted from these datasets for our experiments. The total number of utterances in each emotion and each dataset is listed in Table  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Table  5  lists the recognition results on the three datasets. The Wav2vec 2.0 pre-trained models were trained on audio with sampling rate of 16 kHz. Recordings in the IEMOCAP, EmoDB, and KSUEmotions are also sampled at 16 kHz. The Mandarin Affective Speech dataset, however, uses 8 kHz sampling rate. We upsampled the audio to 16 kHz for testing this dataset. Although the sampling rate is the same, the audio upsampled from 8 kHz to 16 kHz contains no components in frequencies between 4 and 8 kHz, which is dramatically different from the training data. This may explain why the IEMOCAP models performed poorly on the Mandarin dataset. We also directly tested on 8k Hz audio with 16 kHz models. However, the results were even worse.\n\nWe conducted a 7-fold cross validation experiment on the Mandarin Affective Speech dataset, by partitioning the dataset into training and test sets. We used initials and finals as the phonetic units in the experiment. The overall (weighted) accuracy was 75.2%. This result shows that the poor performance of IEMOCAP models on this dataset is not due to the nature of the dataset. The mismatch of sampling rate is probably the main factor responsible for the poor performance, together with other possible factors such as linguistic and cultural differences.\n\nThe models performed reasonably well on the German and Arabic datasets, suggesting that our method has the potential for cross-lingual emotion recognition. The models of broad phonetic classes and syllables seem to work better than the phoneme models. This may be because broad phonetic classes and syllables are language-general whereas phonemes are language-specific. It may also be because there are more training data for broad phonetic classes and syllables than phonemes, therefore the models of broad phonetic classes and",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Discussion",
      "text": "We propose a method for emotion recognition through finetuning wav2vec 2.0 for recognition of emotion-dependent phonetic units, including phonemes, broad phonetic classes, syllables, as well as the entire utterance. The models of phonemes, broad phonetic classes, and syllables all significantly outperform the utterance model, demonstrating that phonetic units are helpful and should be incorporated in speech emotion recognition.\n\nThe best performance is from broad phonetic classes. The advantages of using broad phonetic classes for other tasks such as speaking rate estimation  [25]  and speech enhancement  [26]  have been reported in the literature. Using broad phonetic classes may force the model to disregard differences of phonemes within a broad phonetic class, but also encourage the model to pay attention to the phonetic variability among broad phonetic classes in emotion recognition. It will be interesting to further investigate the optimal set of broad phonetic classes for the task of emotion recognition.\n\nModels of broad phonetic classes and syllables outperform phonemes in the setting of cross-lingual emotion recognition. This may be because broad phonetic classes and syllables are language-general whereas phonemes are language-specific, or because more training data are available for broad phonetic classes and syllables than phonemes. Further research is needed to test these hypotheses.\n\nOur study also shows that Wav2vec 2.0 can be fine-tuned to recognize not only phonemes, but also coarser-grained or larger phonetic units such as broad phonetic classes and syllables, without a language model.\n\nThe proposed method of fine-tuning Wav2vec 2.0 for emotion recognition achieved a significant improvement over most previously reported results on IEMOCAP. In the future we will investigate how to make fine-tuned models more robust across datasets where there are mismatches in recording conditions as well as sampling rate.",
      "page_start": 1,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: compares the spectra of two vowels (from the same",
      "page": 1
    },
    {
      "caption": "Figure 1: Spectra of happy /AE/ and /IY/.",
      "page": 2
    },
    {
      "caption": "Figure 2: . The core",
      "page": 2
    },
    {
      "caption": "Figure 4: with examples.",
      "page": 2
    },
    {
      "caption": "Figure 2: Fine-tuning wav2vec 2.0 for emotion recognition.",
      "page": 2
    },
    {
      "caption": "Figure 4: shows a typical training loss curve observed in",
      "page": 2
    },
    {
      "caption": "Figure 3: Phonetic units for speech emotion recognition.",
      "page": 3
    },
    {
      "caption": "Figure 4: A typical training loss curve observed in ﬁne-tuning.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Emotion recognition accuracy from using different",
      "data": [
        {
          "Phonetic units": "Phonemes\n(N=194,473)",
          "with emotion": "31.9%\n(24.2%,5.0%,2.8%)",
          "without emotion": "14.9%\n(6.6%,5.2%,3.0%)"
        },
        {
          "Phonetic units": "Broad phon. cl.\n(N=194,473)",
          "with emotion": "28.2%\n(20.3%,5.2%,2.8%)%",
          "without emotion": "11.5%\n(3.0%,5.4%,3.0%)"
        },
        {
          "Phonetic units": "Syllables\n(N=78,943)",
          "with emotion": "32.4%\n(21.4%,8.8%,2.2%)",
          "without emotion": "16.3%\n(4.9%,9.0%,2.4%)"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech: A review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "5",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C.-N Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "opensmile -the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "MM'10 -Proceedings of the ACM Multimedia 2010 International Conference"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "venue": "Speech emotion recognition with dual-sequence lstm architecture"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang",
        "L Guo",
        "J Dang"
      ],
      "venue": "Speech emotion recognition with local-global aware deep representation learning"
    },
    {
      "citation_id": "11",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C.-C Chiu",
        "J Fan"
      ],
      "venue": "Speech sentiment analysis via pre-trained features from end-to-end asr models"
    },
    {
      "citation_id": "12",
      "title": "Xvectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "venue": "Xvectors meet emotions: A study on dependencies between emotion and speaker recognition"
    },
    {
      "citation_id": "13",
      "title": "A dialogical emotion decoder for speech motion recognition in spoken dialog",
      "authors": [
        "S Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "venue": "A dialogical emotion decoder for speech motion recognition in spoken dialog"
    },
    {
      "citation_id": "14",
      "title": "Articulation constrained learning with application to speech emotion recognition",
      "authors": [
        "M Shah",
        "M Tu",
        "V Berisha",
        "C Chakrabarti",
        "A Spanias"
      ],
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "15",
      "title": "Class-level spectral features for emotion recognition",
      "authors": [
        "D Bitouk",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2010",
      "venue": "Speech communication"
    },
    {
      "citation_id": "16",
      "title": "Combining speech recognition and acoustic word emotion models for robust text-independent emotion recognition",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "D Arsic",
        "G Rigoll",
        "A Wendemuth"
      ],
      "year": "2008",
      "venue": "2008 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "17",
      "title": "The phonetic bases of vocal expressed emotion: natural versus acted",
      "authors": [
        "H Dhamyal",
        "S Memon",
        "B Raj",
        "R Singh"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "18",
      "title": "Determining the smallest emotional unit for level of arousal classification",
      "authors": [
        "B Vlasenko",
        "A Wendemuth"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "19",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "20",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "22",
      "title": "The nist scoring toolkit",
      "venue": "The nist scoring toolkit"
    },
    {
      "citation_id": "23",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "24",
      "title": "",
      "authors": [
        "A Meftah",
        "Y Alotaibi",
        "S.-A Selouani",
        "Ksuemotions"
      ],
      "venue": ""
    },
    {
      "citation_id": "25",
      "title": "Mandarin affective speech",
      "authors": [
        "Y Yang",
        "Z Wu",
        "T Wu",
        "D Li"
      ],
      "venue": "Mandarin affective speech"
    },
    {
      "citation_id": "26",
      "title": "Robust speaking rate estimation using broad phonetic class recognition",
      "authors": [
        "J Yuan",
        "M Liberman"
      ],
      "venue": "ICASSP 2010, 04 2010"
    },
    {
      "citation_id": "27",
      "title": "Incorporating broad phonetic information for speech enhancement",
      "authors": [
        "Y.-J Lu",
        "C.-F Liao",
        "X Lu",
        "J.-W Hung",
        "Y Tsao"
      ],
      "venue": "Incorporating broad phonetic information for speech enhancement"
    }
  ]
}