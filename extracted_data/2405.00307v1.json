{
  "paper_id": "2405.00307v1",
  "title": "Active Learning With Task Adaptation Pre-Training For Speech Emotion Recognition",
  "published": "2024-05-01T04:05:29Z",
  "authors": [
    "Dongyuan Li",
    "Ying Zhang",
    "Yusong Wang",
    "Funakoshi Kataro",
    "Manabu Okumura"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has garnered increasing attention due to its wide range of applications in various fields, including human-machine interaction, virtual assistants, and mental health assistance. However, existing SER methods often overlook the information gap between the pre-training speech recognition task and the downstream SER task, resulting in sub-optimal performance. Moreover, current methods require much time for fine-tuning on each specific speech dataset, such as IEMOCAP, which limits their effectiveness in real-world scenarios with large-scale noisy data. To address these issues, we propose an active learning (AL)-based fine-tuning framework for SER, called AFTER, that leverages task adaptation pre-training (TAPT) and AL methods to enhance performance and efficiency. Specifically, we first use TAPT to minimize the information gap between the pretraining speech recognition task and the downstream speech emotion recognition task. Then, AL methods are employed to iteratively select a subset of the most informative and diverse samples for fine-tuning, thereby reducing time consumption. Experiments demonstrate that our proposed method AFTER, using only 20% of samples, improves accuracy by 8.45% and reduces time consumption by 79%. The additional extension of AFTER and ablation studies further confirm its effectiveness and applicability to various real-world scenarios. Our source code is available on Github for reproducibility. (https://github.com/Clearloveyuan/AFTER). 1 This paper is an extended version of our preliminary paper [2] presented in the IEEE AUTOMATIC SPEECH RECOGNITION and UNDERSTANDING (IEEE ASRU 2023). Please refer to the detailed differences outlined in Section 7 2024-th Journal of Natural Language Processing.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The language of tones is the oldest and most universal of all our means of communication  [1] . Speech emotion recognition (SER) aims to identify emotional states conveyed in vocal expressions, making it an essential topic in tone and language analysis. 1 It has garnered significant attraction in both the industrial and academic communities, including speech-to-text translation  [3, 4] , dialogue system  [5] [6] [7] , medical surveillance systems  [8] , psychological treatments  [9, 10] , and intelligent virtual voice assistants  [11, 12] .\n\nWith the development of deep learning techniques in natural language processing  [13] [14] [15]  and computer vision  [16] [17] [18] , many SER methods have been proposed. These SER methods are broadly classified into classic machine learning-based methods and deep learning-based methods  [19] . The former methods  [20, 21]  typically consist of three main components: feature extraction, feature selection, and emotion recognition. However, selecting and designing features for specific corpora is time-consuming  [22] , and they consistently exhibit poor generalization on unseen datasets  [23] . Deep learning-based methods can address these issues by automatically extracting more abstract features to improve generalization  [24] [25] [26] . They benefit from various neural network architectures such as convolutional neural networks (CNNs)  [27]  and transformers  [28] . With the development of pre-trained language models  [29]  and the availability of large-scale datasets, various pre-trained automatic speech recognition (ASR) models have been proposed. ASR models, in this draft, refer to those models that use machine learning or artificial intelligence technology to process human speech into readable text such as wav2vec 2.0  [30] , HuBERT  [31]  and Data2vec  [32] . These ASR models use speech's acoustic and linguistic properties to provide more robust and context-aware representations for speech signals. Xia et al.  [33]  proved that fine-tuning SER datasets on wav2vec 2.0  [34]  obtain state-of-the-art (SOTA) performance on IEMOCAP  [35] . This finding has inspired researchers to explore new fine-tuning strategies on ASR models, becoming a new paradigm for SER. For example,  [36]  proposed a self-distillation SER model to fine-tune wav2vec 2.0 obtaining SOTA performance on the DEMoS dataset  [37] . And  [38]  fine-tuned wav2vec 2.0 by jointly optimizing the SER and ASR tasks achieving SOTA performance in Portuguese datasets.\n\nAlthough the aforementioned methods achieve considerable success, several issues still need to be addressed.  (1)  Current methods seldom consider the information gap between the pre-trained ASR and downstream SER tasks. For example, wav2vec 2.0  [30]  adopts a masked learning objective to predict missing frames from the remaining context, while the downstream SER  [27, 39]  task aims to minimize cross-entropy loss between predicted and referenced emotion labels for speech signals. Suchin et al.  [40]  proved that the information gap would decrease the performance of downstream tasks. To address it, Pseudo-TAPT  [41]  first uses K-means to obtain pseudo-labels of speech signals and uses supervised TAPT  [40]  for continual pre-training. However, K-means is sensitive to the initial value, making Pseudo-TAPT unstable and computationally expensive. (2) Current methods only fine-tune and validate the performance on a specific speech dataset. For example,  [33]  train their models solely on the IEMOCAP, leading to over-fitting and poor generalization for unseen datasets. Real-world scenarios contain much heterogeneous and noisy data, which hinders the application of these SER methods. Heterogeneous means that real-world scenarios should contain different voice background, languages, devices for recording speech, and speech types (spontaneous speech and acted speech). Please note that \"noisy data\" does not refer to acoustically noisy data (unclear speech or unrecognized audio). We define \"noisy data\" as the specific noise in the speech emotion recognition task, including outliers and redundant samples. Specifically, outliers encompass various ambiguous emotions due to the complexity of speech, which can lead to inaccurate emotional annotations and degrade the performance of the model. Redundant samples being trained repeatedly does not improve the model's accuracy. Instead, they lead to an uneven distribution of data, making it more challenging to identify emotions with a limited amount of data. (3) Pre-trained ASR models often contain millions of parameters, for example, wav2vec 2.0 contains 317 million parameters, which is time-consumption for real-world and large-scale datasets.\n\nTo address the aforementioned issues, we propose an active learning-based fine-tuning framework for SER, referred to as AFTER, which can be easily applied to noisy and heterogeneous realworld scenarios. Specifically, we first propose an unsupervised task adaptation pre-training (TAPT) method  [40]  to reduce the information gap between the pre-trained and downstream SER tasks, enabling the pre-trained model to understand the semantic information of the SER task. Then, we create two large-scale heterogeneous and noisy datasets to simulate real-world scenes. Furthermore, we propose AL strategies with clustering-based initialization to iteratively select a smaller, more informative, and diverse subset of samples for fine-tuning. This approach can efficiently eliminate noise and outliers, improve generalization, and reduce time consumption.\n\nOur main contributions can be summarized as follows:\n\n• To the best of knowledge, we are the first to propose a general task adaptation pre-training and active learning-based fine-tuning framework for the speech emotion recognition task to address the information gap, noisy sensitive, and low efficiency issues.\n\n• We created three additional large-scale speech emotion recognition datasets to simulate different complex real-world scenarios by merging existing high-quality speech emotion datasets. These datasets represent noisy and heterogeneous real-world situations. And we released our created datasets on Github link to share with other researchers.\n\n• Extensive experiments demonstrate the effectiveness and efficiency of our proposed methods AFTER, and we perform well on IEMOCAP, Merged Dataset, and Merged-2 Dataset with four emotional categories, as well as SAVEE and Merged-3 Dataset with seven emotional categories. Additional extensions of AFTER and demonstrate the effectiveness and applicability.\n\nThe remainder of this paper is organized as follows. In Section 2, we provide a literature review of the most related work, including speech emotion recognition, active learning, and task-adaptation pre-training. In Section 3, we carefully introduce our methodology: AFTER, in detail. In Section 4, we describe the experimental corpora and setup in detail. In Section 5, we present our experimental results and analyses. We present the limitations of this study in Section 6 and the differences with the previous conference version in Section 7. Finally, we give the conclude and discuss future work of this study in Section 8.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition (Ser)",
      "text": "The SER task is one of the key components in human-machine interaction and human communication systems  [42] . With the development of deep learning  [43] [44] [45] , several attempts have been made to automatically learn emotion representations from audio signals using neural networks  [46] [47] [48] . However, commonly used SER datasets, such as MSP-Podcast  [49] , IEMOCAP  [35]  and CMU-MOSEI  [50] , are relatively small compared to automatic speech recognition datasets. This limitation restricts the ability for pre-trained ASR models to improve the accuracy of emotion recognition. Self-supervised pre-trained models, such as Transformers, provide a solution by first learning from a large-scale speech corpus without explicit labeling  [39, 51] . The knowledge learned from pre-training datasets can be transferred to downstream tasks by either using the model as a feature extractor  [52]  or directly fine-tuning the whole model  [53] . Please note that a model trained by a self-supervised learning algorithm is called a self-supervised learning (SSL) model in speech research. While initially introduced for natural language processing (NLP), several SSL-based pre-trained ASR models have been developed for speech processing, including wav2vec 2.0  [30] , HuBERT  [54] , and CLAP  [55] . Taking wav2vec 2.0  [30]  as an example, which serves as the base model in this draft, it comprises a multi-layer convolutional neural network (CNNs) designed to predict future frames based on past frames, achieving through the minimization of a contrastive loss. Additionally, wav2vec 2.0 utilizes a transformer-based architecture, employing a masked learning objective to predict missing frames within the given context. These pre-trained models consistently demonstrate state-of-the-art performance across various SER datasets. For instance,  [56]  observe that wav2vec 2.0 features surpass traditional spectral-based features in SER applications.  [41]  showcase the advantages of task-adaptive pre-training in the wav2Vec 2.0 model, leading to a significant improvement in overall model performance. Furthermore,  [33]  conduct a comparative analysis of features extracted with different temporal spans, concluding that features with longer temporal context, such as those of wav2vec 2.0, exhibit superior performance in SER.  [57]  demonstrate that the features derived from a linear combination of layers outperform single-layer representations in wav2vec 2.0 for SER applications.\n\nWhile these studies demonstrate the usefulness of pre-trained models as feature extractors, little research has been done on how to efficiently fine-tune them for SER. Different from the abovementioned works, we focus on proposing a general fine-tuning framework to apply effectively and efficiently to any type of pre-trained ASR models for SER tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Active Learning",
      "text": "Active learning is an extensively research challenge in the field of machine learning, encompassing a variety of scenarios and query strategies  [58, 59] . In recent years, there has been a resurgence of interest in active learning within the NLP community  [59] . Recent studies have employed active learning with BERT models for specific tasks such as intent classification  [60] , sentence matching  [61] , parts-of-speech tagging  [62] , and named entity recognition  [63] .  [64]  advocate for continued pretraining on unlabeled data in the context of active learning.  [65]  adapt active learning for multi-task scenarios involving transformer models.  [66]  conduct an extensive empirical study of existing active learning strategies on binary classification tasks.  [67]  adapt the BADGE  [68]  framework for active learning with BERT. While BADGE computes gradient embeddings from a neural network's output layer and subsequently clusters the gradient space.\n\nTo the best of our knowledge, our work is the first active learning-based fine-tuning framework in the speech domain. Instead of focusing on proposing complex active learning query strategies, we concentrate on evaluating the effectiveness of active learning for SER. Through experimentation, in Section 5.2, we validate its effectiveness and aspire to propose more efficient methods to advance this task in the future.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task Adaptation Pre-Training",
      "text": "Task-adaptive pre-training (TAPT) is a significant area of research, as introduced by  [40] . Essentially, TAPT involves customizing a language model (LM) for a specific task, leading to improved model performance.  [40]  explore the benefits of tailoring a pre-trained model like RoBERTa to a specific task domain. They investigate four distinct domains, covering biomedical and computer science publications, news, and reviews, and spanning eight classification tasks. Their exploration extended to assessing the transferability of adapted language models across different tasks and domains.\n\nAdditionally, they conduct a study to evaluate the importance of pre-training on human-curated data.\n\n[69] discuss various strategies for adapting BERT and DistilBERT to historical domains and tasks in computational humanities. The outcomes support the idea of continuous pre-training in machine learning tasks to enhance performance stability. A combined approach of domain adaptation and task adaptation shows positive effects. Task adaptation alone is versatile and applicable in various setups, unlike domain adaptation, which requires a substantial amount of in-domain data.\n\nSeveral approaches have been explore to make TAPT more efficient, especially with methods involving word embeddings. For example,  [70]  propose TAPTER, enhancing pre-trained language model embeddings for domain adaptation. It outperforms standard methods when in-domain data is limited.  [71]  advocate re-training from a general model for low-resource scenarios, yielding comparable performance with slight trade-offs.  [72]  adapt tokenizers to transfer pre-trained models to new domains, achieving over 97% performance benefits but introducing a 6% increase in model parameters.\n\nIn this study, we introduce a straightforward approach for continuous training of a pre-trained model with a task-related loss function on downstream tasks. Our experimental results, detailed in section 5.3, demonstrate the effectiveness of this method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "The overall framework of AFTER is depicted in Figure  1 , comprising three main components: a task adaptation pre-trained module, an active learning-based fine-tuning module, and an emotion classification module. First, we will formally define the task of SER, and subsequently introduce each component of AFTER in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Notations And Task Formulation",
      "text": "Given a speech dataset D = {(x i , y i )} N i=1 where x i represents the i-th speech signal and y i represents its corresponding emotion label, we aim to fine-tune a pre-trained automatic speech recognition model M, such as wav2vec 2.0  [41] , on the labeled speech datasets D train to obtain accurately predicted emotion labels for all speech signals.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Task Adaptation Pre-Training (Tapt)",
      "text": "As shown in Figure  1  (a), we introduce the TAPT component in detail. To better leverage pre-trained prior knowledge for the benefit of downstream tasks and minimize the information gap between pre-trained tasks and downstream tasks,  [40]  continued training the pre-trained model RoBERTa  [73]   Then, we adopt an AL method to select unlabeled samples for iterative annotation. These labeled samples are used to fine-tune the wav2vec 2.0 model for SER.\n\non downstream datasets via the same loss of the pre-training task (reconstructing the masked tokens of input sentences, similar to BERT  [29] ), resulting in significant improvements in downstream text classification tasks. Inspired by their work, we added an additional step into AFTER by continuing training the pre-trained automatic speech recognition model, using wav2vec 2.0 as an example in this study, on downstream training datasets for the speech emotion recognition task. By conducting this process, we can bridge the information gap between the pre-trained ASR task and the target SER task, as confirmed by our experiments in section 5.3.\n\nAs depicted in Figure  1  (a), the wav2vec 2.0 model M(W 0 ), with pre-trained weights W 0 , consists of three sub-modules: the feature encoder module, the transformer module, and the quantization module. Specifically, we utilize a CNN-based encoder to encode the i-th input unlabeled speech signals into low-dimensional vectors, denoted as x i . Subsequently, we randomly mask 15% of the features (following BERT  [29] ) of the speech vectors. We then decode them using two decoders to obtain quantized and context representations. The quantization decoder can transform continuous speech vectors x i into discrete codewords from phonemes codebooks 2  , resulting in z q i . Meanwhile the wav2vec 2.0 decoder (transformer layers) employs self-attention to decode continuous speech vectors x i into context-aware representations z c i . Then, we design contrastive loss  [30]  (cl) to minimize the differences between quantized and context representations as follows:\n\nwhere the temperature hyperparameter κ is set to 0.1, and sim(a, b) = a T b/∥a∥ 2 ∥b∥ 2 , where T represents the transposition of a vector. Eq. (  1 ) can help obtain better quantized and context representations because two decoders can provide highly heterogeneous contexts for each speech signal  [74] .\n\nTo minimize the information gap between the pre-trained model and downstream SER task, following BERT  [29] , we first randomly mask 15% of the tokens of each speech signal. We then apply reconstruction loss on the corrupted downstream SER dataset to generate tokens for reconstructing the original data, which can be formulated as follows:\n\nwhere N m is the number of masked tokens, s true i and s predicted i are the ground-truth and predicted token probability of the i-th masked token, respectively. Finally, we combine contrastive loss and reconstruction loss for the TAPT process as follows:\n\nPlease note that although pseudo-TAPT  [41]  also adopts TAPT, we employ different loss functions. We believe that our method is simpler and more suitable for upstream ASR tasks. Specifically, they invest significant time using K-means to extract frame-level emotional pseudo-labels and continually pre-train their model in a supervised manner by predicting their frame-level emotion pseudo-labels. However, K-means is sensitive to the initial value and outliers  [75] , making Pseudo-TAPT unstable and computationally expensive.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Active Learning (Al) Based Fine-Tuning",
      "text": "When we finish the TAPT process, we obtain the model M TAPT (W ′ 0 ) with W ′ 0 as the weight initialization for the AL process (cf. Line 1 of Algorithm 1). A typical AL setup starts by treating D as a pool of unlabeled data D pool and performs τ iterations of sample selection. Specifically, in the i-th iteration, k samples are selected using a given acquisition function ac():\n\nHere, k is a variable parameter. We determine it based on the number of iterations τ and the predefined total number of selected samples N s , i.e., k = ROUND(N s /τ ), where ROUND() rounds the number down. For example, we adopt Entropy  [76]  as the ac() function to measure the uncertainty of the samples and select the most uncertain k samples. These selected samples are then labeled and added to the i-th training dataset D i train , with which a model is fine-tuned for SER. One primary goal of AFTER is to explore whether AL strategies can reduce the number of annotation samples, as labeling large-scale datasets is the most laborious part of SER. Instead of focusing on proposing new active learning query strategies, we adopt five of the most well-known and influential AL strategies for evaluation, including Entropy  [76] , Least Confidence  [77] , Margin Confidence  [77] , ALPs  [78] , and BatchBald  [79] . These methods use different criteria to help select the most uncertain and informative samples from D pool , and we introduce them briefly in this paper.\n\n① Entropy measures the uncertainty of x i as\n\nwhere c is the number of emotional classes and P (ŷ j |x i ) represents the predicted probability of x i for the j-th emotion.\n\n② Least Confident measures the most incontinent samples as\n\nwhere c is the number of emotional classes and P (y j |x i ) represents the predicted probability of x i for the j-th emotion.\n\n③ Margin Confidence is the process of selecting the sample with the smallest difference between the maximum and second largest probability predicted by the model, which can be formulated as Margin Confidence(x i ) = (P (ŷ\n\nwhere P (y 1 |x i ) represents the largest predicted probability of x i and P (y 2 |x i ) represents the second largest predicted probability of x i .\n\n④ ALPs iteratively selects the sample closest to the cluster center as the most differentiated and informative sample each batch. This can be formulated as follows:\n\nwhere centers is the clustering centers (we follow their paper using K-means to obtain clustering centers).\n\n⑤ BatchBald jointly score samples by estimating the mutual information (I) between a set of multiple data points and the model parameters:\n\nAfter applying the above query strategies for the samples, we select the k most uncertain or diverse samples for annotation and add them to the training dataset D train . Traditional AL methods often use random initialization; however, we found that these AL methods are sensitive to the initialization process, leading to the selection of redundant samples or outliers in each AL iteration with poor initialization. Therefore, instead of directly using AL methods, we propose a clustering-based initialization for all AL methods (K-means in this study), resulting in better performance (details about K-means are given in section 4.4). Please note that, as illustrated in Algorithm 1, clusteringbased initialization is applied only in the initialization process, and subsequent iterations of the AL loop do not require a K-means process.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Initialization For Active Learning",
      "text": "We observe that AL methods are particularly sensitive to initialization, as the initial set of samples can substantially impact the selection order of subsequent samples in each iteration of AL. However, most AL methods randomly select 1% of samples for initialization  [64] . In contrast, we propose a novel clustering-based (K-means) initialization method to improve the performance of SER. Specifically, we first extract sample representations of the training data from the wav2vec 2.0 CNN-based encoder. Then, we employ K-means on the training data and select 1% of samples closest to the cluster centers as our initialized samples. It is important to note that we use the elbow method  [80]  to automatically determine the number of clusters for K-means, and we use the Euclidean distance to measure the distance between sample representations.",
      "page_start": 7,
      "page_end": 10
    },
    {
      "section_name": "Emotion Recognition Classifier",
      "text": "As shown in Figure . 1 (b), we incorporate a task-specific classification layer with additional parameters W c for emotion recognition on top of wav2vec 2.0. We fine-tune the classification model\n\n) in each AL iteration using all labeled samples in D train (cf. Lines 6-10 of Algorithm 1). We formulate the cross-entropy loss for the emotion recognition classifier as follows:\n\nwhere c is the number of emotion classes, k is the number of selected samples at t-th iteration, ŷj i is the i-th predicted label, and y j i is the i-th ground-truth of the j-th class.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Experiment Settings",
      "text": "In this section, we first introduce all datasets used in this study in Section 4.1. Following this, we present the selected baselines in Section 4.2 and provide implementation details in Section 4.3.\n\nFinally, we delve into the detailed active learning strategies used in the following experiments in Section 4.4.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets",
      "text": "IEMOCAP: We first evaluated the performance of all baseline models using the widely used benchmark dataset, IEMOCAP  [35] . IEMOCAP is a multimodal database commonly employed to evaluate SER performance. It comprises five conversation sessions, each featuring a female and a male actor engaging in improvised and scripted scenarios. The dataset includes 10,039 speech utterances, all sampled at 16kHz with a 16 bits resolution. To ensure a fair comparison, we merged the \"excited\" class into the \"happy\" class, resulting in four considered emotions: neutral, happy, angry, and sad. Following the approach of  [41] , we adopted a 5-fold cross-validation method, where each IEMOCAP session served as the test set. Additionally, we randomly selected 10% of the data from the remaining four sessions for our validation dataset, with the rest allocated to our training dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Savee:",
      "text": "To explore the performance of AFTER with broader range of emotions, we incorporated an additional datasets, the Surrey Audio-Visual Expressed Emotion (SAVEE) dataset  [81] . SAVEE contains four male speakers: DC, JE, JK, and KL. Each speaker reads the same set of 120 sentences, labeled with one of seven emotion categories: angry, disgust, sad, fear, happy, surprise, and neutral. Utilizing all emotion categories, the dataset comprises 480 utterances, totaling 30 minutes of speech.\n\nFor fair comparisons with SOTA approaches in experiments, following the previous works  [82] [83] [84] [85] [86] , we mainly conducted 10-fold cross-validation. In each fold, we allocated 90% of the data for training and 10% for testing to evaluate the model's fitting ability.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Merged Dataset:",
      "text": "Many existing methods are inadequate for real-world applications and are susceptible to noise due to their heavy reliance on fine-tuning models using specific small-scale datasets. For example, pseudo-TAPT  [41]  is fine-tuned by the training dataset of IEMOCAP and performs well on IEMOCAP. However, pseudo-TAPT performs poorly when tested on other datasets. To provide a potential solution to address this issue, we conducted additional experiments by training on two larger noisy and heterogeneous datasets. We achieved this by merging various datasets from different sources to simulate the noisy environments encountered in real-world scenarios. It is important to note that any emotion recognition datasets containing the corresponding emotional categories can be incorporated into the Merged datasets. In this draft, we selected five widely-used datasets with different languages, recording equipment, and number of actors. We first introduce each dataset of the Merged dataset as follows:\n\n• EmoDB  [87]  database is a freely available German emotional database, created by the Technical University, Berlin, Germany. It features ten professional speakers, including five males and five females, who participated in the data recording process. The database contains a total of 535 utterances and comprises seven emotions: anger, boredom, anxiety, happiness, sadness, disgust, and neutral. The data was recorded at a 48-kHz sampling rate and then down-sampled to 16-kHz.\n\n• ShEMO  [88]  database includes 3,000 semi-natural utterances, totaling three hours and 25 minutes of speech data extracted from online radio plays. ShEMO encompasses speech samples of 87 native-Persian speakers, covering six basic emotions: anger, fear, happiness, sadness, surprise, and neutral states.\n\n• RAVDESS  [89]  database contains 7,356 files and features 24 professional actors (12 female, 12 male). Each actor vocalizes two lexical-matched statements in a neutral North American accent. The speech includes calm, happy, sad, angry, fearful, surprise, and disgust. Each expression is produced at two levels of emotional intensity (normal, strong), alongside an additional neutral expression. All conditions are available in three modality formats: Audio-only, Audio-Video, and Video-only.\n\n• EMov-DB  [90]  database includes recordings from four speakers, including two males and two females. The emotional styles covered include neutral, sleepiness, anger, disgust, and amused. Each audio file is recorded in 16bits .wav format.\n\n• CREMA-D  [91]  database is an emotional multimodal actor dataset consisting of As shown in Table  1 , we manually controlled the number of instances for each of the four labels in the Merged dataset to maintain label balance. Different from IEMOCAP, EmoDB is a German emotional database, ShEMO is a Persian emotional database, and both RAVDESS and CREMA-D contain more actors (24 actors and 91 actors, respectively). We constructed the Merged dataset by merging the training data of the mentioned following datasets with the training data of IEMOCAP. To explore whether the Merged dataset could improve performance on a single dataset, such as IEMOCAP, we employed a 5-fold cross-validation approach. This involved leaving each IEMOCAP session out as the test set and randomly selecting 10% of the dataset from the remaining Merged dataset as our validation dataset, while the remainder was allocated for training purposes. It is important to note that we only use the training data for both the TAPT and AL-based fine-tuning processes to prevent data leakage during evaluation. Furthermore, the training procedures are conducted from scratch separately for the IEMOCAP, SAVEE, Merged, Merged-2, and Merged-3 dataset.\n\nMerged-2 dataset: Merged dataset contains almost acted speech datasets. To better simulate \"real-world\" scenarios, we incorporated two additional spontaneous datasets to Merged dataset to construct Merged-2 dataset: AFEW5.0 and BAUM-1s. The two added datasets are as follows:\n\n• AFEW5.0  [92]  is a spontaneous audiovisual emotional video dataset developed for emotion recognition in the wild (EmotiW) challenge in 2015. It contains seven emotional categories: anger, disgust, fear, joy, neutral, sadness, and surprise. These emotions were annotated by 3 annotators. The dataset is divided into three parts: train set (723 samples), validation (val) set (383 samples), and test set (539 samples). In this work, we used the train and val sets to validate the performance of our method since the Test set is only available to participants in competitions. • BAUM-1s  [93]  is a spontaneous audiovisual affective face database of affective and mental states developed in 2016, featuring 31 Turkish individuals. The video samples were collected in real scenarios, where emotions were elicited by watching films in an unscripted and unguided way. The target emotions include seven basic ones: joy, anger, sadness, disgust, fear, neutral, and surprise, as well as boredom and contempt. Several mental states, such as unsure (confused, and undecided), thinking, concentrating, and bothered, are also included.\n\nWe only used four emotion categories from AFEW5.0 and BAUM-1s, including anger, neutral, sad, and happy, to construct the Merged-2 dataset. And we used the test data of IEMOCAP as test data for the Merged-2 dataset. Training and evaluation processes are similar to those of the Merged dataset.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Merged-3 Dataset:",
      "text": "To demonstrate the performance of AFTER on spontaneous datasets, we used the test set of BAUM-1s, which comprises seven emotion categories, for evaluation. The detailed implantation for evaluation follows the approach outlined in  [94] . Additionally, we constructed the Merged-3 dataset by merging two acted speech datasets (EmoDB, RAVDESS) with two spontaneous datasets (AFEW5.0, BAUM-1s).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baselines",
      "text": "We selected different SOTA baselines for different datasets. For the IEMOCAP dataset, Merged dataset, and Merged-3 dataset, we selected the best-performing methods: LSSED  [95] , GLAM  [96] , RH-emo  [97] , Light  [27] , Pseudo-TAPT  [41] , and w2v2-L-robust  [98] . For the SAVEE dataset, we selected the recently best-performing approaches: DCNN  [85] , TSP+INCA  [82] , CPAC  [84] , GM-TCN  [83] , and TIM-Net  [86] . For the Merged-3 dataset, we select the baselines as MFCC+PLP+SVM  [93] , CNN+SVM  [99] , CNN+DTPM+SVM  [100] , CNN+SVM  [101]  and CNN+LSTM  [94] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Implementation Details",
      "text": "All experiments used the same learning rate of 10 -4 with the Adam optimizer. Our implementation of wav2vec 2.0 (wav2vec2-base) is based on the Hugging Face framework  3  . The audio window length was set to 20 ms. We fine-tuned the model in a few-shot manner, involving longer fine-tuning, more evaluation steps during training, and early stopping with 20 epochs based on validation loss.\n\nTo ensure fair comparison with previous studies, we employ either off-the-shelf software packages or utilize the provided code by respective authors. Each model underwent ten executions, and the average performance across these runs is considered the final result. The hyper-parameters are chosen as default if provided, or tuned otherwise. Following the work  [102] , we evaluated the models using weighted accuracy (WA) and unweighted accuracy (UA)  [103]  in speaker-independent settings. Please note that we did not require the data to be labeled by actual annotators. Instead, we used the ground-truth labels available in the training dataset. Specifically, we masked the labels and only receive them when the AL methods determined that the samples should be labeled. This approach is a common technique used by AL researchers to validate their methods  [76] . However, it is worth mentioning that in real-world scenarios, human annotators would be responsible for labelling the data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Active Learning Strategies Selection For After",
      "text": "As shown in Figure  1  (c), AFTER incorporates an AL strategy for sample selection. To identify the most suitable AL method for AFTER, we combined it with multiple well-known AL methods and evaluated their performance. Furthermore, we find that AL methods are sensitive to initialization, with most AL methods randomly selecting 1% samples for initialization  [64] . Unlike them, we proposed a novel clustering-based (K-means) initialization method to improve the performance of SER. Specifically, we first extract sample representations of training data from the wav2vec 2.0 CNN-based encoder. Then, we employed K-means on the training data and selected 1% of samples closest to the cluster centers as our initialized samples. Please note that we use the elbow method  [80]  to determine the number of clusters for K-means automatically, and we use the Euclidean distance to measure the distance between sample representations.\n\nFigure  2  shows that clustering-based initialization outperformed the random initialization for all AL methods. The initial set of samples significantly influenced the selection order of samples in each iteration of AL, and an effective initialization enhanced the performance and stability of AL methods. Figure  2  illustrates that Entropy+Clustering emerged as the most effective AL strategy for AFTER on the Merge dataset. Although we only displayed the diagram for UA due to space constraints, the diagram for WA exhibited similar trends. Therefore, Entropy+Clustering was selected as the primary AL method for AFTER. We recommend using Entropy+Clustering, the simplest yet most efficient strategy for real-world applications.\n\nWe analyzed the relationship between the ratio of labeled samples, performance, and time consumption of AFTER. Results in by 79% compared to fine-tuning on 100% samples. Thus, we selected 20% labeled samples as a trade-off between performance and time consumption for subsequent experiments.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experimental Results And Discussion",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Comparison With Other Initialized Strategies",
      "text": "As illustrated in Section 3.4, we propose a simply but efficient K-means initialization method applicable to various SER tasks. While representative-based methods like BMAL  [104]  and density-based methods like DACS  [105]  are also available, our focus remains on demonstrating the effectiveness of our proposed initialization method. To assess the effectiveness of our approach, we compare it with BMAL, DACS, and random sampling as baseline initialization methods. Specifically, the selected initialization baselines are introduced as follows.\n\nDACS  [105]  is a density-aware Core-set approach used to estimate sample densities and selectively choose diverse points from sparse regions. For each input x i , the density score for each sample is calculated as follows:\n\nwhere N (x i , k) represents the k-nearest neighbors of x i  [105] . We use default parameters from their original paper, selecting the top 1% of samples with the highest scores as initialized samples for downstream tasks.\n\nBMAL  [104]  considers the distance between a sample and its surrounding labeled samples to enrich the diversity of the labeled dataset. Diversity is measured by the KL-divergence of the class probabilities distribution of similar neighboring instances, formulated as:\n\nwhere ŷj is the predicted label for the j-th sample. We use the default parameters from their original paper, selecting the top 1% of samples with the highest scores as initialized samples for downstream tasks.  As depicted in Figure  3 , we observe that K-means exhibits comparable performance to DACS and outperforms BMAL and Random Sampling. DACS operates as a density-aware core-set approach. When selecting 1% diverse samples for initialization, both K-means and DACS tend to choose the same sample set nearest to each clustering center. In contrast, BMAL and Random Sampling struggle to select the most representative samples, leading to decreased classification performance. Instead of employing DACS with its numerous hyperparameters requiring adjustment, we opted for the simple yet efficient K-means as our initialization method. Furthermore, our future work will explore the utilization of more representative-based methods for initialization. Based on Tables  3  and 4 , we have four findings to share as follows:\n\n• (1) Larger-scale pre-training models yield better performance: Traditional CNN-based backbones were insensitive to the pre-training process, as evidenced by GLAM (without pre-training) outperforming LSSED and RH-emo (pre-trained with ResNet). Conversely, larger-scale wav2vec 2.0-based pre-training methods, such as pseudo-TAPT and w2v2-Lrobust, significantly outperformed CNN-based models, benefiting from a broader range of hyperparameters and larger pre-training datasets. Even when employing the same backbone as pseudo-TAPT, our method AFTER outperformed pseudo-TAPT and w2v2-L-robust on all three datasets, demonstrating the effectiveness and applicability of active learning for SER.\n\n• (2) Larger-scale pre-training models exhibit denoising capabilities to a certain extent: wav2vec 2.0-based methods significantly outperformed CNN-based methods on the Merged dataset and Merged-2 dataset, while showing comparable performance on the IEMOCAP dataset. LSSED  [95]  and RH-emo  [97]  achieved favorable results with IEMOCAP but showed poor performance with the Merged dataset and Merged-2 dataset, possibly due to their limited denoising and domain transfer capabilities. In contrast, GLAM  [96]  and Light  [27]  employ multi-scale feature representations and deep convolution blocks to capture high-level global data features, advantageous for filtering out noisy low-level features and enhancing performance across all datasets. Pseudo-TAPT  [41] , w2v2-L-robust, and AFTER adopt larger-scale pre-training models, which understand relevant features for the downstream SER task and help denoise irrelevant or noisy features to improve robustness against real-world datasets.\n\n• (3) Active Learning can help achieve better performance on real-world datasets: AFTER achieved superior classification accuracy when utilizing the Merged dataset and Merged-2 dataset compared to solely relying on the IEMOCAP. However, baselines achieved their optimal performance solely with the IEMOCAP, as they are susceptible to the influence of outliers and redundant data. Pseudo-TAPT  [41]  enhances model robustness by using K-means to capture higher-level frame emotion labels as pseudo labels for supervised TAPT.\n\nAlthough baselines can mitigate dataset noise to a certain extent, they exhibit high time complexity during fine-tuning with large-scale datasets and fail to effectively bridge the gap between pre-training and the downstream SER task. In contrast, AFTER uses unsupervised TAPT to mitigate the information gap between the source domain (ASR) and the target (SER) domain. Additionally, AFTER selects a subset of the most informative and diverse samples for iterative fine-tuning, offering three advantages: Firstly, it reduces labor consumption for manually labeling large-scale SER samples; Secondly, by utilizing a smaller labeled dataset, AFTER significantly reduces the overall time consumption (Figure  5 ), making it practical and feasible for real-world applications; Finally, the iterative fine-tuning process employed by AFTER improves performance and stability by eliminating noise and outliers present in the selected samples, leading to enhanced overall model performance in SER tasks.\n\n• (4) Baselines performed better on the Merged-2 dataset than on the Merged-1 dataset: AFTER achieved superior classification performance on the Merged-2 dataset compared to the Merged dataset. However, the combination of acted and spontaneous speech datasets posed greater challenges for other baselines due to their sensitivity to heterogeneous and noisy samples. Merging multiple datasets enabled AFTER to extract a wider variety of samples from a larger pool of data. This increased diversity of samples contains more information, resulting in improved classification performance.\n\nTable  5 : Overall performance comparison on the SAVEE dataset with seven emotion categories. AFTER adopted Entropy+Clustering and selected 20% samples for fine-tuning. We obtained the baselines' performance directly from TIM-Net. As depicted in Table  5 , AFTER demonstrated superior classification performance on the SAVEE dataset with seven emotion categories, underscoring its capacity to recognize a wider spectrum of emotions. Specifically, AFTER improved UA and WA by 0.19% and 0.31%, respectively, using only 20% of samples. By iteratively extracting the most informative and uncertain samples, AFTER fine-tunes the SSL model wav2vec 2.0, effectively removing irrelevant samples and outliers, thereby improving classification performance.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Methods",
      "text": "As shown in Table  6 , we also assess the performance of AFTER on BAUM-1s and Merged-3 Dataset (containing spontaneous datasets) with seven emotion categories (Evaluation on BAUM-1s  [94] ). We observed that AFTER can achieve SOTA performance on both BAUM-1s and Merged-3 Dataset. AFTER obtained better performance on the Merged-3 dataset by selecting more diverse samples from a large pool of datasets. Baselines lacked the ability to remove noisy data from the merged dataset, decreasing their performance on real-world scenarios.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Study For After",
      "text": "We performed an additional ablation study to assess the efficacy of AFTER, as shown in Table  7 . Specifically, we conducted fine-tuning (FT) and TAPT+FT on random sample selection and AL-based (Entropy) sample selection with varying ratios of labeled samples, ranging from 10% to 100%. To ensure a fair comparison between them, we adopted the same K-means initialization for them and other hyperparameters, such as learning rate and random seeds.\n\nFrom Table  7 , we have four interesting observations: (1) Fine-tuning with active learning significantly improved performance compared to random sampling (FT+Entropy vs. FT+Random), regardless of the number of labeled samples. This result demonstrates that the AL-based fine-tuning strategy efficiently eliminates noise and outliers and selects the most informative and diverse samples for finetuning; (2) TAPT+FT outperformed FT on both random sampling and Entropy sampling, indicating that TAPT can effectively minimize the domain difference and significantly enhance the performance of the downstream SER task; (3) With the same number of labeled samples, AFTER obtained",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Visualization Of After",
      "text": "As depicted in Figure  4 , we present qualitative comparisons of AFTER with random sampling.\n\nOur observations indicate that AFTER tends to select samples that are representative and uncertain. Specifically, AFTER selects samples near each clustering center, benefiting from the K-means initialization, which are the most representative samples of the entire datasets. Using entropy as a criterion, AFTER selects the most uncertain samples for labeling, which almost lie on the clustering boundaries. Based on previous experimental results, we discovered that only these selected representative and highly uncertain samples could achieve comparable or even superior performance compared to training with the entire datasets. Additionally, the samples selected for training via random sampling are shown in Figure  4  (A) and (C). We found that random sampling tends to select outliers and redundant samples (some points overlap due to repeated selection).",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Time Consumption Comparison",
      "text": "Figure  5  (A) demonstrates that FT+AL with 20% labeled samples significantly reduced the time consumption of FT (fine-tuning on all labeled samples). Compared to TAPT+FT, TAPT+FT+AL significantly decreased the time consumption with the main cost incurred by TAPT. Additionally, the relationship between time consumption and the ratio of labeled samples is shown in Figure  5  (B). AL-based fine-tuning exhibited a linear increase in time consumption with sample size from 1%∼20% (exponential growth from 30%∼100% in Table  2 ), indicating the efficiency of AFTER and its potential to be applied in large-scale unlabeled real-world scenarios.\n\nFigure  6  illustrates the relationship between running time and unweighted accuracy of classification on both the IEMOCAP and Merged dataset. We observe that FT+AL outperformed FT on both datasets, demonstrating the effectiveness of active learning. Additionally, we also observe that TAPT+FT+AL underperformed FT and FT+AL within the time intervals of 0 to 250 minutes on IEMOCAP and 0 to 300 minutes on the Merged dataset, respectively. This is because TAPT requires time to adaptively pre-train the model using downstream unlabeled training datasets. Following the TAPT process, TAPT+FT+AL (AFTER) significantly outperformed the other two baselines on both datasets, thus demonstrating the effectiveness of our proposed method.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Adapting After With Different Pre-Trained Asr Models",
      "text": "Many SSL models have been proposed recently  6  , such as LABERT  [106]  and DPHuBERT  [107] .\n\nTo demonstrate the versatility of our proposed method, AFTER, across various SSL models, we replaced wav2vec 2.0 with another widely used SSL model, HuBERT  [108, 109] , in conducting our experiments.  We first briefly introduce the HuBERT. HuBERT, also known as Hidden-Unit BERT, is a variant of BERT (Bidirectional Encoder Representations from Transformers) designed specifically for speech processing tasks. It leverages the powerful pre-training strategies of BERT while adapting its architecture to handle speech data efficiently. Its core techniques are summarized as follows:\n\n• Architecture Adaptation: HuBERT adapts the BERT architecture to process speech data. It modifies the input layer to accommodate raw audio waveforms and adjusts subsequent layers to handle sequential data inherent in speech signals. HuBERT incorporates transformer layers to capture contextual information from the extracted features. These layers enable the model to understand the temporal dependencies and nuances present in speech sequences.\n\n• Feature Extraction: Similar to traditional speech processing pipelines, HuBERT first extracts high-level features from raw audio using techniques like Mel-frequency cepstral coefficients or filter banks. These features capture important acoustic characteristics of the speech signal.\n\n• Training Strategy: HuBERT is pre-trained on large-scale unlabeled speech datasets using self-supervised learning techniques. During pre-training, the model learns to predict masked portions of the input sequence or to reconstruct corrupted segments, leveraging the bidirectional context provided by the transformer architecture.\n\nThe TAPT process for HuBERT-based AFTER follows a similar approach to that of word2vec 2.0based AFTER. We first pre-train HuBERT using downstream speech emotion training datasets without using emotion labels, in an SSL manner. We randomly mask 20% of tokens and reconstruct them using emotion recognition training datasets. Then, similar to theword2vec 2.0-based AFTER method, we use K-means to select 1% of samples for initialization. Finally, we iteratively query samples to train the HuBERT-based AFTER model.\n\nExperimental results are presented in Table  8 . We have two observations: (1) HuBERT-based finetuning with random sampling surpassed wav2vec 2.0-based fine-tuning with random sampling on the Merged datasets by nearly 2%, showcasing HuBERT's effectiveness. This improvement can be attributed to HuBERT's tailored architecture for processing speech data, which may better capture subtle emotional cues. Additionally, HuBERT's pre-training objectives are more aligned with the demands of emotion recognition tasks. (2) wav2vec 2.0-based AFTER outperformed HuBERT-based AFTER. This is because wav2vec 2.0-based AFTER pre-trains the model with contrastive loss and reconstructing loss, which better assists the SSL model in understanding the context of downstream datasets. These observations highlight the advantages and differences between HuBERT-based fine-tuning and wav2vec 2.0-based fine-tuning, as well as the impact of pre-training strategies on downstream performance.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Extension Of After To Multiple Annotators",
      "text": "In this study, following previous works  [58, 59] , we assumed that each sample is annotated with its ground truth labels. Thus, we first masked the labels on all training datasets and considered them as unlabeled data. Then, we unmasked the labels of some samples for training if these samples were selected by active learning. However, in the real world, annotators with different knowledge, ages, genders, intuitions, backgrounds, and cultures  [110, 111]  may annotate the same sample differently.\n\nFollowing previous studies, such as learning from the soft label  [112] [113] [114]  and learning from the hard label of individual annotators  [115] [116] [117] , we extended our proposed method, AFTER, to address the aforementioned real-world situation by suggesting the following potential solutions:\n\n• (1) Individual-level Entropy (indi): We can measure the reliability of each annotator by calculating the individual-level entropy for each annotator. Given the prediction label for sample x as z a = [z a 1 , • • • , z a n ] by annotator a, the entropy can be calculated by\n\nwhere p a i (x) = softmax(z a i (x)). We select the (instance, annotator) pair with the highest entropy using:\n\nwhere U denotes the unlabeled set and A denotes the annotator pool.\n\n• (2) Group-level Entropy (group): Instead of focusing solely on individual uncertainty, we can query the data by considering the group-level uncertainty. To represent the uncertainty of the group on a sample, we calculate the entropy baseline based on the aggregation of each annotator's specific output. Therefore, we normalize and sum the logits of each annotator at the group level:\n\n|A| a=1 z a norm , and calculate the group-level entropy as follows:\n\nwhere p i (x) = softmax(z i (x)) and |A| represent the number of annotators. We then query the data with the highest group-level uncertainty.\n\n• (3) Vote Variance (vote): Another method to measure the uncertainty among a group is by computing the variance of the votes. Given the prediction y a of annotator a, we calculate the vote variance as follows:\n\nwhere µ = We need to apply our methods to more complicated scenes, such as social networks with clustering technique  [119] [120] [121]  and clinical patient emotion detection  [122, 123] . (4) Finally, AFTER is designed specifically for multimodal emotion recognition tasks, which are not as straightforward and generalizable as language models  [73, 29]  or image processing techniques  [124] .",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Comparison With The Previous Conference Version",
      "text": "In this section, we primarily delineate three key distinctions between this version and the conference version of our research.  (1)  We introduced novel methodologies by extending AFTER to accommodate more complex real-world scenarios. Specifically, we first explored various initialization methods for AFTER in Section 5.1. Subsequently, we investigated the impact of integrating AFTER with different pre-trained ASR models in Section 5.6. Finally, we extended AFTER to encompass more complex scenarios involving multiple annotators in Section 5.7 and soft-labels in Section 5.8. (2) We redefined the motivation behind this study in Section 1, as well as redefined the meaning of \"noisy\", \"heterogeneous\", and \"real-world\". To better simulate \"real-world\" scenarios, we created a new dataset named \"Merged-2 Dataset\", comprising both acted and spontaneous datasets. Additionally, we created another novel dataset, \"Merged-3 Dataset\", incorporating two acted speech datasets (EmoDB, RAVDESS), and two spontaneous datasets (AFEW5.0, BAUM-1s) across seven emotional categories. (3) We conducted additional experiments and provided in-depth analysis. Firstly, we conducted experiments to analyze the effects of initialization methods in Section 5.1 and the influence of various pre-trained ASR models in Section 5.6. Subsequently, we conducted experiments on the newly added datasets \"Merged-2 Dataset\" and \"Merged-3 Dataset\", accompanied by detailed analysis in Section 5.2. Finally, we added additional ablation studies in Section 5.3 and compared time consumption in Section 5.5. Additionally, we explored the applicability of our methods to multiple annotators and soft-labels in Section 5.8.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we investigated unsupervised TAPT and the AL-based fine-tuning strategy to improve the performance of SER. To extend SER applications to real-world scenarios, we created three largescale noisy and heterogeneous datasets, and we used TAPT to bridge the information gap between pre-trained and the target SER task. Experimental findings demonstrate that AFTER significantly improved performance and reduced time consumption. In our future work, we plan to create largerscale speech emotion recognition datasets for testing in the speech domain. Furthermore, we aim to explore and design more effective and efficient active learning strategies tailored to the SER task, aiming to minimize time consumption. Finally, we would like to propose a more general framework that extends beyond SER, focusing on a wider range of speech or language-related tasks.",
      "page_start": 20,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , comprising three main components: a",
      "page": 4
    },
    {
      "caption": "Figure 1: (a), we introduce the TAPT component in detail. To better leverage pre-trained",
      "page": 4
    },
    {
      "caption": "Figure 1: Model overview. First, we pre-train an off-the-shelf wav2vec 2.0 in the TAPT manner.",
      "page": 5
    },
    {
      "caption": "Figure 1: (a), the wav2vec 2.0 model M(W0), with pre-trained weights W0, consists of",
      "page": 5
    },
    {
      "caption": "Figure 1: (c), AFTER incorporates an AL strategy for sample selection. To identify the",
      "page": 10
    },
    {
      "caption": "Figure 2: shows that clustering-based initialization outperformed the random initialization for all AL",
      "page": 10
    },
    {
      "caption": "Figure 2: illustrates that Entropy+Clustering emerged as the most effective AL strategy for AFTER",
      "page": 10
    },
    {
      "caption": "Figure 2: Ratio of labeled samples vs. Unweighted Accuracy.",
      "page": 11
    },
    {
      "caption": "Figure 3: Comparison of various initialization methods for AL, with Entropy employed as the active",
      "page": 12
    },
    {
      "caption": "Figure 3: , we observe that K-means exhibits comparable performance to DACS and",
      "page": 12
    },
    {
      "caption": "Figure 5: ), making it practical",
      "page": 13
    },
    {
      "caption": "Figure 4: , we present qualitative comparisons of AFTER with random sampling.",
      "page": 15
    },
    {
      "caption": "Figure 4: (A) and (C). We found that random sampling tends to select",
      "page": 15
    },
    {
      "caption": "Figure 5: (A) demonstrates that FT+AL with 20% labeled samples significantly reduced the time",
      "page": 15
    },
    {
      "caption": "Figure 5: (B). AL-based fine-tuning exhibited a linear increase in time consumption with sample size from",
      "page": 15
    },
    {
      "caption": "Figure 6: illustrates the relationship between running time and unweighted accuracy of classification",
      "page": 15
    },
    {
      "caption": "Figure 4: t-SNE visualization of AFTER and randomly sampled methods. The selected samples are",
      "page": 16
    },
    {
      "caption": "Figure 5: (A) Time Consumption Comparison and (B) Relationship between ratio of labeled samples",
      "page": 16
    },
    {
      "caption": "Figure 6: A plot illustrating the efficiency of AFTER. The x-axis represents the running time, while",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Task adaptation pre-trained (b) Fine-tuning on Wav2vec 2.0 (c) Active Learning Loop\nContrastive Loss Angry Happy Sad Neutral serocs\nto predict the masked tokens ✔ ✔\n+ ytniatrecnU ✖ ✖ ✖\nPredict ✖ ✖\nQuantized representations Context representations Active Learning\nSamples\nClassification Layer Annotation Samples\nQuantization Wav2vec 2.0\nSample Select Label\nModule Decoder 𝑥! ✔ Happy\n𝑥\" ✖ No\nWav2vec 2.0 Transformer Decoder Masked 𝑥# ✖ No 𝑥$ ✖ No 𝑥% ✔ Neutral elcarO\n𝑥& ✖ No Fine-tuning\n𝑥’ ✖ No\nWav2vec 2.0 CNN-Based Encoder Wav2vec 2.0 CNN-Based Encoder K-means initialization\nFine-tuning\nnput: Large-scale unlabeled speech signals Input: Labeled speech signals Select 1% samples closest to the cluster center": "Contrastive Loss\nto predict the masked tokens\n+\nQuantized representations Context representations\nQuantization Wav2vec 2.0\nModule Decoder\nMasked\nWav2vec 2.0 CNN-Based Encoder\nnput: Large-scale unlabeled speech signals",
          "Column_2": "serocs\n✔\n✔\nytniatrecnU ✖ ✖ ✖\n✖ ✖\nSamples\nAnnotation Samples\nSample Select Label\n𝑥! ✔ Happy\n𝑥\" ✖ No\n𝑥# ✖ No elcarO\n𝑥$ ✖ No\n𝑥% ✔ Neutral\n𝑥& ✖ No\n𝑥’ ✖ No"
        },
        {
          "(a) Task adaptation pre-trained (b) Fine-tuning on Wav2vec 2.0 (c) Active Learning Loop\nContrastive Loss Angry Happy Sad Neutral serocs\nto predict the masked tokens ✔ ✔\n+ ytniatrecnU ✖ ✖ ✖\nPredict ✖ ✖\nQuantized representations Context representations Active Learning\nSamples\nClassification Layer Annotation Samples\nQuantization Wav2vec 2.0\nSample Select Label\nModule Decoder 𝑥! ✔ Happy\n𝑥\" ✖ No\nWav2vec 2.0 Transformer Decoder Masked 𝑥# ✖ No 𝑥$ ✖ No 𝑥% ✔ Neutral elcarO\n𝑥& ✖ No Fine-tuning\n𝑥’ ✖ No\nWav2vec 2.0 CNN-Based Encoder Wav2vec 2.0 CNN-Based Encoder K-means initialization\nFine-tuning\nnput: Large-scale unlabeled speech signals Input: Labeled speech signals Select 1% samples closest to the cluster center": "",
          "Column_2": "K-means initialization\nSelect 1% samples closest to the cluster center"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Overall performance comparison on 4 emotion categories. AFTER adopted En-",
      "data": [
        {
          "Random+Entropy\nDACS+Entropy\nBMAL+Entropy\nK-means+Entropy": "",
          "Column_2": "Random+Entropy\nDACS+Entropy\nBMAL+Entropy\nK-means+Entropy"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 3: Overall performance comparison on 4 emotion categories. AFTER adopted En-",
      "data": [
        {
          "Random+Entropy\nDACS+Entropy\nBMAL+Entropy\nK-means+Entropy": "",
          "Column_2": "Random+Entropy\nDACS+Entropy\nBMAL+Entropy\nK-means+Entropy"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FT+\nTAP": "TAP",
          "AL (Merged\nT+AL (IEM": "T+AL (Mer",
          "Data)\nOCAP)": "ged Data)",
          "Column_4": ""
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The voice and the emotions",
      "authors": [
        "Smiley Blanton"
      ],
      "year": "1915",
      "venue": "Quarterly Journal of Speech"
    },
    {
      "citation_id": "2",
      "title": "After: Active learning based fine-tuning framework for speech emotion recognition",
      "authors": [
        "Dongyuan Li",
        "Yusong Wang",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Proc. of ASRU"
    },
    {
      "citation_id": "3",
      "title": "A change of heart: Improving speech emotion recognition through speech-to-text modality conversion",
      "authors": [
        "Zeinab Sadat Taghavi",
        "Ali Satvaty",
        "Hossein Sameti"
      ],
      "year": "2023",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition based on self-attention weight correction for acoustic and text features",
      "authors": [
        "Jennifer Santoso",
        "Takeshi Yamada",
        "Kenkichi Ishizuka",
        "Taiichi Hashimoto",
        "Shoji Makino"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Real-time speech emotion and sentiment recognition for interactive dialogue systems",
      "authors": [
        "Dario Bertero",
        "Farhad Bin Siddique",
        "Chien-Sheng Wu",
        "Yan Wan",
        "Ricky Ho Yin Chan",
        "Pascale Fung"
      ],
      "year": "2016",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "6",
      "title": "Inter-modality and intra-sample alignment for multi-modal emotion recognition",
      "authors": [
        "Yusong Wang",
        "Dongyuan Li",
        "Jialun Shen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Joyful: Joint modality fusion and graph contrastive learning for multimoda emotion recognition",
      "authors": [
        "Dongyuan Li",
        "Yusong Wang",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Feartype emotion recognition for audio-based vasilescusystems",
      "authors": [
        "C Clavel",
        "I Vasilescu"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using supervised deep recurrent system for mental health monitoring",
      "authors": [
        "Nelly Elsayed",
        "Zag Elsayed"
      ],
      "year": "2022",
      "venue": "Speech emotion recognition using supervised deep recurrent system for mental health monitoring"
    },
    {
      "citation_id": "10",
      "title": "A-TIP: attribute-aware text infilling via pre-trained language model",
      "authors": [
        "Dongyuan Li",
        "Jingyi You",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2022",
      "venue": "Proc. of COLING"
    },
    {
      "citation_id": "11",
      "title": "Human-machine interaction personalization: A review on gender and emotion recognition through speech analysis",
      "authors": [
        "M La",
        "P Mura",
        "Lamberti"
      ],
      "year": "2020",
      "venue": "IEEE IoT"
    },
    {
      "citation_id": "12",
      "title": "Emp: emotion-guided multi-modal fusion and contrastive learning for personality traits recognition",
      "authors": [
        "Yusong Wang",
        "Dongyuan Li",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "13",
      "title": "A language model-based generative classifier for sentence-level discourse parsing",
      "authors": [
        "Ying Zhang",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Bidirectional transformer reranker for grammatical error correction",
      "authors": [
        "Ying Zhang",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "year": "2024",
      "venue": "Journal of Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "The new agronomists: Language models are experts in crop management",
      "authors": [
        "Jing Wu",
        "Zhixin Lai",
        "Suiyao Chen",
        "Ran Tao",
        "Pan Zhao",
        "Naira Hovakimyan"
      ],
      "year": "2024",
      "venue": "The new agronomists: Language models are experts in crop management",
      "arxiv": "arXiv:2403.19839"
    },
    {
      "citation_id": "16",
      "title": "Insect pest classification with state space model",
      "authors": [
        "Qianning Wang",
        "Chenglin Wang",
        "Zhixin Lai",
        "Yucheng Zhou",
        "Insectmamba"
      ],
      "year": "2024",
      "venue": "Insect pest classification with state space model",
      "arxiv": "arXiv:2404.03611"
    },
    {
      "citation_id": "17",
      "title": "Ecnet: Effective controllable text-to-image diffusion models",
      "authors": [
        "Sicheng Li",
        "Keqiang Sun",
        "Zhixin Lai",
        "Xiaoshi Wu",
        "Feng Qiu",
        "Haoran Xie",
        "Kazunori Miyata",
        "Hongsheng Li"
      ],
      "year": "2024",
      "venue": "Ecnet: Effective controllable text-to-image diffusion models",
      "arxiv": "arXiv:2403.18417"
    },
    {
      "citation_id": "18",
      "title": "A comparative study on enhancing prediction in social network advertisement through data augmentation",
      "authors": [
        "Qikai Yang",
        "Panfeng Li",
        "Xinyu Shen",
        "Zhicheng Ding",
        "Wenjing Zhou",
        "Yi Nian",
        "Xinhe Xu"
      ],
      "year": "2024",
      "venue": "A comparative study on enhancing prediction in social network advertisement through data augmentation",
      "arxiv": "arXiv:2404.13812"
    },
    {
      "citation_id": "19",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "D Bj.Abbaschian",
        "A Sierra-Sosa",
        "Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "20",
      "title": "Linear discriminant differential evolution for feature selection in emotional speech recognition",
      "authors": [
        "Soumaya Gharsellaoui",
        "Sid-Ahmed Selouani",
        "Mohammed Sidi"
      ],
      "year": "2019",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised low-rank representations for speech emotion recognition",
      "authors": [
        "Georgios Paraskevopoulos",
        "Efthymios Tzinis"
      ],
      "year": "2019",
      "venue": "Nikolaos Ellinas, Theodoros Giannakopoulos, and Alexandros Potamianos"
    },
    {
      "citation_id": "22",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Moataz",
        "Mohamed Ayadi",
        "Fakhri Kamel",
        "Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "23",
      "title": "Improved speech emotion recognition using transfer learning and spectrogram augmentation",
      "authors": [
        "Sarala Padi",
        "Seyed Omid Sadjadi"
      ],
      "year": "2021",
      "venue": "Proc. of ICMI"
    },
    {
      "citation_id": "24",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Nat"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Da",
        "Silva Morais",
        "Ron Hoory"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "26",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing"
      ],
      "year": "2023",
      "venue": "Vesper: A compact and effective pretrained model for speech emotion recognition"
    },
    {
      "citation_id": "27",
      "title": "A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Sentiment-aware automatic speech recognition pre-training for enhanced speech emotion recognition",
      "authors": [
        "Ayoub Ghriss",
        "Bo Yang"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "29",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Chang Devlin"
      ],
      "year": "2019",
      "venue": "Proc. of naacL-HLT"
    },
    {
      "citation_id": "30",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou"
      ],
      "year": "2020",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "31",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "Wang Babu"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "32",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Hsu"
      ],
      "year": "2022",
      "venue": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "arxiv": "arXiv:2202.03555"
    },
    {
      "citation_id": "33",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Yangyang Xia",
        "Li-Wei Chen"
      ],
      "year": "2021",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "34",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski"
      ],
      "year": "2019",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "35",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "M Bulut",
        "C Busso"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "36",
      "title": "Fast yet effective speech emotion recognition with self-distillation",
      "authors": [
        "Thanh Zhao Ren",
        "Tam Nguyen"
      ],
      "year": "2022",
      "venue": "Fast yet effective speech emotion recognition with self-distillation"
    },
    {
      "citation_id": "37",
      "title": "Demos: an italian emotional speech corpus",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Giovanni Costantini"
      ],
      "year": "2020",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "38",
      "title": "Domain specific wav2vec 2.0 fine-tuning for the se&r 2022 challenge",
      "authors": [
        "Gdr Oliveira"
      ],
      "year": "2022",
      "venue": "Domain specific wav2vec 2.0 fine-tuning for the se&r 2022 challenge"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition via generation using an attention-based variational recurrent neural network",
      "authors": [
        "Murchana Baruah",
        "Bonny Banerjee"
      ],
      "year": "2022",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "40",
      "title": "Don't stop pretraining: Adapt language models to domains and tasks",
      "authors": [
        "G Suchin",
        "M Ana"
      ],
      "year": "2020",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "41",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "L Chen",
        "A Rudnicky"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "42",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Junaid Qadir",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "43",
      "title": "Language models are free boosters for biomedical imaging tasks",
      "authors": [
        "Zhixin Lai",
        "Jing Wu",
        "Suiyao Chen",
        "Yucheng Zhou",
        "Anna Hovakimyan",
        "Naira Hovakimyan"
      ],
      "year": "2024",
      "venue": "Language models are free boosters for biomedical imaging tasks",
      "arxiv": "arXiv:2403.17343"
    },
    {
      "citation_id": "44",
      "title": "Adaptive ensembles of fine-tuned transformers for llm-generated text detection",
      "authors": [
        "Zhixin Lai",
        "Xuesheng Zhang",
        "Suiyao Chen"
      ],
      "year": "2024",
      "venue": "Adaptive ensembles of fine-tuned transformers for llm-generated text detection",
      "arxiv": "arXiv:2403.13335"
    },
    {
      "citation_id": "45",
      "title": "Generic mechanism for reducing repetitions in encoder-decoder models",
      "authors": [
        "Ying Zhang",
        "Hidetaka Kamigaito",
        "Tatsuya Aoki",
        "Hiroya Takamura",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Journal of Natural Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Knowledge transfer for on-device speech emotion recognition with neural structured learning",
      "authors": [
        "Yi Chang",
        "Zhao Ren",
        "Thanh Nguyen",
        "Kun Qian",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "47",
      "title": "DST: deformable speech transformer for emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "48",
      "title": "EMIX: A data augmentation method for speech emotion recognition",
      "authors": [
        "An Dang",
        "Toan Vu",
        "Le Dinh Nguyen",
        "Jia-Ching Wang"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "49",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "Sundararajan Srinivasan",
        "Zhaocheng Huang",
        "Katrin Kirchhoff"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "50",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "51",
      "title": "Self-supervised representation fusion for speech and wearable based emotion recognition",
      "authors": [
        "Sachith Vipula Dissanayake",
        "Hussel Seneviratne",
        "Elliott Suriyaarachchi",
        "Suranga Wen",
        "Nanayakkara"
      ],
      "year": "2022",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "52",
      "title": "Utility-preserving privacy-enabled speech embeddings for emotion detection",
      "authors": [
        "Chandrashekhar Lavania",
        "Sanjiv Das",
        "Xin Huang",
        "Kyu Jeong Han"
      ],
      "year": "2023",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "53",
      "title": "Pre-finetuning for few-shot emotional speech recognition",
      "authors": [
        "Maximillian Chen",
        "Zhou Yu"
      ],
      "year": "2023",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "54",
      "title": "Arabic speech emotion recognition employing wav2vec2.0 and hubert based on BAVED dataset",
      "authors": [
        "Omar Mohamed",
        "Salah Aly"
      ],
      "year": "2021",
      "venue": "Arabic speech emotion recognition employing wav2vec2.0 and hubert based on BAVED dataset"
    },
    {
      "citation_id": "55",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keywordto-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "56",
      "title": "Recognizing more emotions with less data using self-supervised transfer learning",
      "authors": [
        "Jonathan Boigne",
        "Biman Liyanage",
        "Ted Östrem"
      ],
      "year": "2020",
      "venue": "Recognizing more emotions with less data using self-supervised transfer learning"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "INTERSPEECH 2021, 22nd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "58",
      "title": "Literature survey of active learning in multimedia annotation and retrieval",
      "authors": [
        "Yan Xu",
        "Fuming Sun",
        "Xue Zhang"
      ],
      "year": "2013",
      "venue": "Proc. of ICIMCS"
    },
    {
      "citation_id": "59",
      "title": "A survey of active learning for natural language processing",
      "authors": [
        "Zhisong Zhang",
        "Emma Strubell",
        "Eduard Hovy"
      ],
      "year": "2022",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "60",
      "title": "An ensemble deep active learning method for intent classification",
      "authors": [
        "Leihan Zhang",
        "Le Zhang"
      ],
      "year": "2020",
      "venue": "Proc. of CSAI"
    },
    {
      "citation_id": "61",
      "title": "Pre-trained language model based active learning for sentence matching",
      "authors": [
        "Guirong Bai",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao",
        "Zaiqing Nie"
      ],
      "year": "2020",
      "venue": "Proc. of COLING"
    },
    {
      "citation_id": "62",
      "title": "Reducing confusion in active learning for part-of-speech tagging",
      "authors": [
        "Aditi Chaudhary",
        "Zaid Sheikh"
      ],
      "year": "2021",
      "venue": "Trans. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "63",
      "title": "LTP: A new active learning strategy for crf-based named entity recognition",
      "authors": [
        "Mingyi Liu",
        "Zhiying Tu",
        "Tong Zhang",
        "Tonghua Su",
        "Xiaofei Xu",
        "Zhongjie Wang"
      ],
      "year": "2022",
      "venue": "Neural Process. Lett"
    },
    {
      "citation_id": "64",
      "title": "Active learning by acquiring contrastive examples",
      "authors": [
        "Katerina Margatina",
        "Giorgos Vernikos"
      ],
      "year": "2021",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "65",
      "title": "Multi-task active learning for pre-trained transformer-based models",
      "authors": [
        "Guy Rotman",
        "Roi Reichart"
      ],
      "year": "2022",
      "venue": "Trans. Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "66",
      "title": "Active Learning for BERT: An Empirical Study",
      "authors": [
        "Liat Ein-Dor",
        "Alon Halfon",
        "Ariel Gera",
        "Eyal Shnarch",
        "Lena Dankin",
        "Leshem Choshen",
        "Marina Danilevsky",
        "Ranit Aharonov",
        "Yoav Katz",
        "Noam Slonim"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "67",
      "title": "Cold-start active learning through self-supervised language modeling",
      "authors": [
        "Michelle Yuan",
        "Hsuan-Tien Lin",
        "Jordan Boyd-Graber"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "68",
      "title": "Deep batch active learning by diverse, uncertain gradient lower bounds",
      "authors": [
        "Jordan Ash",
        "Chicheng Zhang",
        "Akshay Krishnamurthy",
        "John Langford",
        "Alekh Agarwal"
      ],
      "year": "2020",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "69",
      "title": "Domain and task adaptive pretraining for language models",
      "authors": [
        "Leonard Konle",
        "Fotis Jannidis"
      ],
      "year": "2020",
      "venue": "Proc. of CHR"
    },
    {
      "citation_id": "70",
      "title": "Task-adaptive pre-training of language models with word embedding regularization",
      "authors": [
        "Kosuke Nishida",
        "Kyosuke Nishida",
        "Sen Yoshida"
      ],
      "year": "2021",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "71",
      "title": "Domain adaptation of word embeddings through the exploitation of in-domain corpora and knowledge bases. Theses",
      "authors": [
        "Hicham El"
      ],
      "year": "2021",
      "venue": "Domain adaptation of word embeddings through the exploitation of in-domain corpora and knowledge bases. Theses"
    },
    {
      "citation_id": "72",
      "title": "Efficient domain adaptation of language models via adaptive tokenization",
      "authors": [
        "Vin Sachidananda",
        "Jason Kessler",
        "Yi'an Lai"
      ],
      "year": "2021",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "73",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "74",
      "title": "Graph contrastive learning with augmentations",
      "authors": [
        "Yuning You",
        "Tianlong Chen",
        "Yongduo Sui",
        "Ting Chen",
        "Zhangyang Wang",
        "Yang Shen"
      ],
      "year": "2020",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "75",
      "title": "A robust k-means clustering algorithm based on observation point mechanism",
      "authors": [
        "Xiaoliang Zhang",
        "Yulin He"
      ],
      "year": "2020",
      "venue": "Complex"
    },
    {
      "citation_id": "76",
      "title": "Toward optimal active learning through monte carlo estimation of error reduction",
      "authors": [
        "R Nicholas",
        "M Andrew"
      ],
      "year": "2001",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "77",
      "title": "Active learning with confidence",
      "authors": [
        "M Dredze",
        "K Crammer"
      ],
      "year": "2008",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "78",
      "title": "Cold-start active learning through self-supervised language modeling",
      "authors": [
        "Jb",
        "M Graber",
        "H Yuan",
        "Lin"
      ],
      "year": "2020",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "79",
      "title": "Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning",
      "authors": [
        "A Kirsch",
        "Jv Amersfoort",
        "Y Gal"
      ],
      "year": "2019",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "80",
      "title": "An optimized approach for prostate image segmentation using k-means clustering algorithm with elbow method",
      "authors": [
        "Rachid Sammouda",
        "Ali El-Zaart"
      ],
      "year": "2021",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "81",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "82",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "Turker Tuncer",
        "Sengul Dogan",
        "U Rajendra"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "83",
      "title": "Gm-tcnet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "Xin-Cheng Jia-Xin Ye",
        "Xuan-Ze Wen",
        "Yong Wang",
        "Yan Xu",
        "Chang-Li Luo",
        "Li-Yan Wu",
        "Kun-Hong Chen",
        "Liu"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "84",
      "title": "Ctl-mtnet: A novel capsnet and transfer learning-based mixed task net for the single-corpus and cross-corpus speech emotion recognition",
      "authors": [
        "Xin-Cheng Wen",
        "Jia-Xin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuan-Ze Wang",
        "Chang-Li Wu",
        "Kun-Hong Liu"
      ],
      "year": "2022",
      "venue": "Ctl-mtnet: A novel capsnet and transfer learning-based mixed task net for the single-corpus and cross-corpus speech emotion recognition",
      "arxiv": "arXiv:2207.10644"
    },
    {
      "citation_id": "85",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "Misbah Farooq",
        "Fawad Hussain",
        "Naveed Khan Baloch",
        "Fawad Riasat Raja",
        "Heejung Yu",
        "Yousaf Bin"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "86",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Yujie Wei",
        "Yong Xu",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "87",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt1",
        "A Paeschke"
      ],
      "year": "2005",
      "venue": "Proc. of INTERSPEECH"
    },
    {
      "citation_id": "88",
      "title": "Shemo: a large-scale validated database for persian speech emotion detection. Language Resources and Evaluation",
      "authors": [
        "M Karami",
        "O Nezami",
        "P Lou"
      ],
      "year": "2019",
      "venue": "Shemo: a large-scale validated database for persian speech emotion detection. Language Resources and Evaluation"
    },
    {
      "citation_id": "89",
      "title": "The ryerson audio-visual database of emotional speech and song",
      "authors": [
        "Sr",
        "F Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "90",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adaeze",
        "T Noé"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "91",
      "title": "CREMA-D: crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "92",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "O Abhinav Dhall",
        "Roland Ramana Murthy",
        "Jyoti Goecke",
        "Tom Joshi",
        "Gedeon"
      ],
      "year": "2015",
      "venue": "Proc. of ICMI"
    },
    {
      "citation_id": "93",
      "title": "Baum-1: A spontaneous audio-visual face database of affective and mental states",
      "authors": [
        "Sara Zhalehpour",
        "Onur Onder",
        "Zahid Akhtar",
        "Cigdem Erdem"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "94",
      "title": "Spontaneous speech emotion recognition using multiscale deep convolutional LSTM",
      "authors": [
        "Shiqing Zhang",
        "Xiaoming Zhao",
        "Qi Tian"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "95",
      "title": "Lssed: A large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "X Xu",
        "W Fan"
      ],
      "year": "2021",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "96",
      "title": "Speech emotion recognition with global-aware fusion on multi-scale feature representation",
      "authors": [
        "W Zhu",
        "X Li"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "97",
      "title": "Learning speech emotion representations in the quaternion domain",
      "authors": [
        "E Guizzo",
        "T Weyde"
      ],
      "year": "2022",
      "venue": "Learning speech emotion representations in the quaternion domain"
    },
    {
      "citation_id": "98",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "99",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao",
        "Qi Tian"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "100",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "101",
      "title": "Audio-visual emotion fusion (avef): A deep efficient weighted approach",
      "authors": [
        "Yaxiong Ma",
        "Yixue Hao",
        "Min Chen",
        "Jincai Chen",
        "Ping Lu",
        "Andrej Košir"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "102",
      "title": "Multiple acoustic features speech emotion recognition using cross-attention transformer",
      "authors": [
        "Yurun He",
        "Nobuaki Minematsu",
        "Daisuke Saito"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "103",
      "title": "Decision level combination of multiple modalities for recognition of emotional expression",
      "authors": [
        "A Metallinou",
        "S Lee"
      ],
      "year": "2010",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "104",
      "title": "Active batch selection via convex relaxations with guaranteed solution bounds",
      "authors": [
        "Shayok Chakraborty",
        "Qian Vineeth Nallure Balasubramanian",
        "Sethuraman Sun",
        "Jieping Panchanathan",
        "Ye"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "105",
      "title": "In defense of core-set: A density-aware core-set selection for active learning",
      "authors": [
        "Yeachan Kim",
        "Bonggun Shin"
      ],
      "year": "2022",
      "venue": "Proc. of SIGKDD"
    },
    {
      "citation_id": "106",
      "title": "LABERT: A Combination of Local Aggregation and Self-Supervised Speech Representation Learning for Detecting Informative Hidden Units in Low-Resource ASR Systems",
      "authors": [
        "Kavan Fatehi",
        "Ayse Kucukyilmaz"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "107",
      "title": "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models",
      "authors": [
        "Yifan Peng",
        "Yui Sudo",
        "Shakeel Muhammad",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "108",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "109",
      "title": "Laughter synthesis using pseudo phonetic tokens with a large-scale in-the-wild laughter corpus",
      "authors": [
        "Shinnosuke Detai Xin",
        "Ai Takamichi",
        "Hiroshi Morimatsu",
        "Saruwatari"
      ],
      "year": "2023",
      "venue": "Laughter synthesis using pseudo phonetic tokens with a large-scale in-the-wild laughter corpus"
    },
    {
      "citation_id": "110",
      "title": "Anveshan: A framework for analysis of multiple annotators' labeling behavior",
      "authors": [
        "Vikas Bhardwaj",
        "Rebecca Passonneau",
        "Ansaf Salleb-Aouissi",
        "Nancy Ide"
      ],
      "year": "2010",
      "venue": "Proc. of LAW"
    },
    {
      "citation_id": "111",
      "title": "Comparison of emotion perception among different cultures",
      "authors": [
        "Jianwu Dang",
        "Aijun Li",
        "Donna Erickson",
        "Atsuo Suemitsu",
        "Masato Akagi",
        "Kyoko Sakuraba",
        "Nobuaki Minematsu",
        "Keikichi Hirose"
      ],
      "year": "2010",
      "venue": "Acoustical science and technology"
    },
    {
      "citation_id": "112",
      "title": "Human uncertainty makes classification more robust",
      "authors": [
        "Joshua Peterson",
        "Ruairidh Battleday",
        "Thomas Griffiths",
        "Olga Russakovsky"
      ],
      "year": "2019",
      "venue": "Proc. of ICCV"
    },
    {
      "citation_id": "113",
      "title": "A case for soft loss functions",
      "authors": [
        "Alexandra Uma",
        "Tommaso Fornaciari",
        "Dirk Hovy",
        "Silviu Paun",
        "Barbara Plank",
        "Massimo Poesio"
      ],
      "year": "2020",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "114",
      "title": "Beyond black & white: Leveraging annotator disagreement via soft-label multi-task learning",
      "authors": [
        "Tommaso Fornaciari",
        "Alexandra Uma",
        "Silviu Paun",
        "Barbara Plank",
        "Dirk Hovy",
        "Massimo Poesio"
      ],
      "year": "2021",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "115",
      "title": "Modelling annotator bias with multi-task Gaussian processes: An application to machine translation quality estimation",
      "authors": [
        "Trevor Cohn",
        "Lucia Specia"
      ],
      "year": "2013",
      "venue": "Hinrich Schuetze, Pascale Fung, and Massimo Poesio"
    },
    {
      "citation_id": "116",
      "title": "Deep learning from crowds",
      "authors": [
        "Filipe Rodrigues",
        "Francisco Pereira"
      ],
      "year": "2018",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "117",
      "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
      "authors": [
        "Aida Mostafazadeh Davani",
        "Mark Díaz",
        "Vinodkumar Prabhakaran"
      ],
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "118",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "M Haytham",
        "Margaret Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2016",
      "venue": "Proc. of IJCNN"
    },
    {
      "citation_id": "119",
      "title": "Identification of dynamic community in temporal network via joint learning graph representation and nonnegative matrix factorization",
      "authors": [
        "Dongyuan Li",
        "Qiang Lin",
        "Xiaoke Ma"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "120",
      "title": "Temporal and topological augmentation-based cross-view contrastive learning model for temporal link prediction",
      "authors": [
        "Dongyuan Li",
        "Shiyin Tan",
        "Yusong Wang",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "121",
      "title": "Joint learning of feature extraction and clustering for large-scale temporal networks",
      "authors": [
        "Dongyuan Li",
        "Xiaoke Ma",
        "Maoguo Gong"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "122",
      "title": "Nonnegative matrix factorization for dynamic modules in cancer attribute temporal networks",
      "authors": [
        "Dongyuan Li",
        "Xiaoke Ma"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "123",
      "title": "Dynamic module detection in temporal attributed networks of cancers",
      "authors": [
        "Dongyuan Li",
        "Shuyao Zhang",
        "Xiaoke Ma"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
    },
    {
      "citation_id": "124",
      "title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio"
      ],
      "year": "1995",
      "venue": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks"
    }
  ]
}