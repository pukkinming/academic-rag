{
  "paper_id": "2110.05635v2",
  "title": "Real-Time Eeg-Based Emotion Recognition Using Discrete Wavelet Transforms On Full And Reduced Channel Signals",
  "published": "2021-10-11T22:28:43Z",
  "authors": [
    "Josef Bajada",
    "Francesco Borg Bonello"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Real-time EEG-based Emotion Recognition (EEG-ER) with consumer-grade EEG devices involves classification of emotions using a reduced number of channels. These devices typically provide only four or five channels, unlike the high number of channels (32 or more) typically used in most current state-of-the-art research. In this work we propose to use Discrete Wavelet Transforms (DWT) to extract time-frequency domain features, and we use time-windows of a few seconds to perform EEG-ER classification. This technique can be used in real-time, as opposed to post-hoc on the full session data. We also apply baseline removal preprocessing, developed in prior research, to our proposed DWT Entropy and Energy features, which improves classification accuracy significantly. We consider two different classifier architectures, a 3D Convolutional Neural Network (3D CNN) and a Support Vector Machine (SVM). We evaluate both models on subject-independent and subject dependent setups to classify the Valence and Arousal dimensions of an individual's emotional state. We test them on both the full 32-channel data provided by the DEAP dataset, and also a reduced 5-channel extract of the same dataset. The SVM model performs best on all the presented scenarios, achieving an accuracy of 95.32% on Valence and 95.68% on Arousal for the full 32-channel subject-dependent case, beating prior real-time EEG-ER subject-dependent benchmarks. On the subject-independent case an accuracy of 80.70% on Valence and 81.41% on Arousal was also obtained. Reducing the input data to 5 channels only degrades the accuracy by an average of 3.54% across all scenarios, making this model appropriate for use with more accessible low-end EEG devices.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "EEG-based Emotion Recognition (EEG-ER) involves the inference of an individual's emotional state from Electroencephalogram (EEG) signals. There are numerous applications for EEG-ER, including emotion-adaptive Human Computer Interaction (HCI)  [7]  and entertainment systems, that typically fall under the category of Affective Computing  [30] . EEG-ER also has useful medical applications, such as diagnosing mood disorders  [19]  and tools that could assist in psychiatric therapy.\n\nEEG devices are non-invasive and read brain activity in real-time  [28] . They come in various shapes and sizes, ranging from expensive medical-grade 32 or 62 channel caps, to headbands with 5 channels or less targeting the consumer market. One of these more affordable devices is the Emotiv Insight, shown in Figure  1 , which has 5 channels together with 2 reference channels. This brand is quite popular with researchers looking into Brain Computer Interface (BCI) technology  [25, 12, 37] , and the cheaper models with limited channels are easier to wear and more practical for day-to-day use in BCI applications.\n\nMost of the current research in EEG-ER makes use of the full 32 or 62 channels  [40] . These are often based on either the Database for Emotion Analysis using Physiological signals (DEAP)  [20]  dataset  (32 channels)  or the SJTU Emotion EEG Dataset (SEED)  [11, 46]  (62 channels)  [2, 9, 15, 17, 24, 35, 38, 40, 43] . Other less popularly used datasets are also available, such as the DREAMER  [18]  dataset (14 channels) and the MAHNOB-HCI  [36]    (32 channels) .\n\nIn this research, we propose techniques that improve on the current approaches for real-time EEG-ER found in literature. The proposed models are based on preprocessing the EEG signals using Discrete Wavelet Transforms, which are often used to perform time-frequency analysis of nonstationary signals, such as EEG. As part of our preprocessing, we also remove the baseline reference signal (provided as part of the DEAP  [20]  dataset), to normalise the signal, yielding a higher classification accuracy. We consider both the subject independent case, where a single model is used across all individuals, and also the subject dependent case, where a separate model is trained for each individual. Our best 32channel model achieves a high accuracy that exceeds the current state-of-the-art subject-dependent models that also use the DEAP  [20]  dataset. We also propose an adaptation of our model to make it work with 5 channels, corresponding to those used by low-end devices such as the Emotiv Insight, while still retaining a high accuracy.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Contributions",
      "text": "The contributions of this work are as follows:\n\n1. A novel approach to preprocess 32-channel EEG signals for emotion recognition using Discrete Wavelet Transforms combined with baseline removal. 2. A real-time emotion classification model that yields an accuracy that exceeds 95% for subject-dependent and 83% for subject-independent models. 3. An adaptation of the proposed model from 32 channels to 5 channels that retains a high subject-dependent accuracy of 92%, together with 81% for the subjectindependent case.\n\nThis study proposes a set of generic high accuracy classification models for emotion recognition using both 32channel and 5-channel setups, for both subject-independent and subject dependent scenarios, that can be used in real-time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Background",
      "text": "Emotion recognition has been attempted using a variety of different inputs such as speech, facial gestures and other physiological measures  [1] . Using EEG signals for emotion recognition has been seeing an increase in popularity and research interest due to it being less susceptible to manipulation by the individual  [40] . In this section we define how we are going to approach real-time emotion classification, together with a review of the state-of-the-art EEG-ER models found in existent literature to date.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Classification",
      "text": "One of the most popular models for emotion classification is the Russell's Circumplex Model of Affect  [33] . It maps different emotions on a 2-dimensional space, with Valence on the x-axis, and Arousal on the y-axis. A High Valence (HV) corresponds to a pleasant emotion, while a Low Valence (LV) corresponds to an unpleasant one. Similarly, a High Arousal (HA) corresponds to an active emotion (a high level of excitement) while a Low Arousal (LA) corresponds to an inactive one. Each quadrant is associated with four categories of emotions being Angry (HALV), Happy (HAHV), Sad (LALV), and Relaxed (LAHV). Twelve individual emotions, such as Tense, Calm and Fatigued, are then mapped across these four quadrants, as shown in Figure  2 .\n\nThe DEAP dataset  [20]  is often used in EEG-ER benchmarks because it provides ratings for both Valence and Arousal, which correspond to the Russell Model  [33] . It consists of EEG signals recorded from thirty-two participants aged between 19 and 37. Each participant was asked to watch forty 60-second long music videos during which the EEG signals were recorded. Prior to each of the 60-second trials, a 3-second pre-trial baseline was also recorded to measure the   [33]  rest state of the subject prior to evoking an emotion through the music videos. After watching each video the participant filled out a Self-Assessment Manikin (SAM)  [6]  where they rated Valence and Arousal on a continuous scale from 1-9.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Real-Time Eeg-Er",
      "text": "To perform EEG-ER in real-time, EEG signals are typically segmented into samples corresponding to time windows of a few seconds duration  [32] . Various characteristics of the signals can then extracted from these samples, such as Spectral Power, Kurtosis coefficients, Band Power, and Hjorth parameters  [4] . When these features were used with Support Vector Machines (SVM) using a Radial Basis Function (RBF) kernel a mean accuracy of 73.1% on both the Valence and Arousal dimensions was achieved  [4] , across all participants from the DEAP dataset. However, these features only carry information from the time-domain. Since EEG signals are non-stationary, techniques that extract timefrequency domain features are more adequate  [31] .\n\nA Discrete Wavelet Transform (DWT)  [10]  is a transform that processes a signal in the time-frequency domain. It breaks down the signal into a number of components, each corresponding to a time series of coefficients that describe how the signal changes over time within a frequency band. When different wavelets were evaluated, the features extracted using db4 wavelet achieved the best results compared to other feature sets computed using other wavelets  [14] . When the db4 wavelet was used to extract features across 4-second time windows, an accuracy of 86.8% on Valence and 84.1% on Arousal was achieved  [27] .\n\nA 5-level DWT-db4 was also used to compute the wavelet entropy and energy features across 2-second time frames  [3] . These features were used with SVM-RBF, outperforming Quadratic Discriminant Analysis (QDA) and k-Nearest Neighbours (kNN) for subject-independent EEG-ER. The performance of SVM-RBF was also compared to that of Artificial Neural Networks (ANNs), where SVM-RBF also had a superior accuracy on the DEAP dataset  [5] .\n\nA 4-level DWT-db4, computing wavelet entropy and energy across theta, alpha, beta and gamma sub bands, was also used on an SVM-RBF  [35] , achieving a cross-validated accuracy of 83.4% on Valence and 81.4% on Accuracy for the subject-independent case, rising to 85.3% and 83.0% respectively for the subject-dependent case.\n\nAlthough traditional machine learning models such as SVMs are able to classify Valence and Arousal with high accuracy, such models do not take into account spatial information  [23]  such as the topological relationship between the different channels and their position on the skull. An approach inspired from computer vision transforms EEG samples into grid-like frames, that include channel and scale information on each axis  [23] . A Convolution Neural Network (CNN) is combined with a Recurrent Neural Network (RNN) forming a Convolutional Recurrent Neural Network (CRNN). This achieved a mean subject-dependent accuracy of 72.1% on Valence and 74.1% on Arousal.\n\nA 3D CNN model was also proposed, which makes use of a 3-dimensional frame consisting of a stack of grids  [34, 43] . In the case of Salama et al.  [34] , with each frame consists of the readings from each channel for the duration of the set timewindow. This is preprocessed using a high pass filter and band stop filter to reduce noise, which are then fed into a CNN, achieving an accuracy of 87.44% for Valence and 88.49% for Arousal. In the case of Yang et al.  [43] , each value in the 3D frame corresponds to the Differential Entropy (DE) for each channel, positioned according to the channel's sensor location on the scalp. Furthermore, the initial 3-second pre-trial baseline included in the DEAP dataset, which corresponds to the subject's rest state, was used to normalise the readings. A significant increase in subject-dependent accuracy on both Valence and Arousal was observed when this baseline removal preprocessing was applied, with Valence accuracy increasing from 68.6% to 89.5% and Arousal increasing from 69.6% to 90.2%. This baseline preprocessing technique was also used with an Attention-based CRNN (ACRNN) model  [38] , achieving a mean subject-dependent accuracy of 92.7% for Valence and 93.1%.\n\nAll of the aforementioned models make use of the full 32 channel data provided by the DEAP dataset  [20] . For these models to be used in real life, high-end devices are needed, which are not practical for consumer applications. A 5-channel approach was used with channels, 3, 5, 2, , and 2, chosen for their position on the prefrontal region  [16] . A sample entropy technique was used to extract features, which were then used to train an SVM. This model achieved an accuracy of 80.43% on Valence and 71.16% on Arousal.\n\nIn this research we will make use of DWT to extract time-frequency domain features, and use them with the stateof-the-art Machine Learning models, SVM and 3DCNN, to determine which perform best in both the full 32-channel and also in the reduced 5-channel scenarios. The chosen for the latter are 3, 7, , 4 and 8, highlighted in Figure  3 , corresponding to those used by the Emotiv Insight EEG device. Both subject-independent and subject-dependent flavours will be compared. We will also make use of the DEAP dataset  [20] , and analyse the difference in accuracy when using baseline removal preprocessing  [43, 38] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "We model the Emotion Classification problem as two separate binary classifiers, one for Valence (Low vs High) and one for Arousal (Low vs High). The output of both classifiers can then combined to obtain the emotion category of one of the four quadrants of Russell's Circumplex Model of Affect  [33]  shown in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Preprocessing",
      "text": "Since the DEAP  [20]  dataset provides ratings between 1 and 9 for both Valence and Arousal, ratings were divided into the two classes, High and Low, with a threshold of 5, consistent with previous work  [43, 38] .\n\nThe DEAP  [20]  dataset is available in both raw and preprocessed forms. The raw signals are at a frequency of 512Hz. The preprocessed signals were downsampled to 128Hz as well as applying a 4.0-45.0Hz bandwidth filter. The EOG components, representing the electrical activity of the eyeball and eyelid motions, were also removed. The data from each channel was then processed using common average referencing, where the average of all electrical activity is subtracted from each signal in the recording. For this reason, the preprocessed version could only be used for the 32channel study. For the 5-channel case, the raw data was imported into MNE 1  , retaining the data from 3, 7, , 4 and 8, and preprocessed in the same way. Each EEG recording is 63 seconds in length. We therefore need to segment the data into shorter time frames in order to simulate real-time classification. In our study we consider time-windows of length ∈ {1, 3}, which allow us to also make use of baseline removal preprocessing on each time window. We use a tumbling window approach, where each frame is non-overlapping, as shown in Figure  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "We extract features from the time-frequency domain using DWT, which is often used to extract features from nonstationary signals  [42]  such as EEG  [14, 21, 39, 29] . For each channel within each EEG sample we apply 4-level DWT using the db4 wavelet, deriving the coefficients for theta; alpha; beta; and gamma frequency bands. These coefficients then allow the calculation of entropy and energy as shown in Equations (  1 ) and (  2\n\nwhere is the signal in question and is the coefficient value at timestep  [27, 35, 44] . These calculations are carried out using PyWavelets 2  [22]  and EBAPy 3  [8] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Removal",
      "text": "Each EEG recording in the DEAP  [20]  dataset begins with a 3-second pre-trial baseline period, during which the individual was not shown any stimulus, thus measuring the emotional rest state. While this was previously done for DE  [43, 38] , we now adapt the same technique for DWT-based wavelet entropy.\n\nWhen using baseline removal preprocessing, the features are modified to reflect the difference between the rest state and active state. After the features are extracted for both baseline and active period, the difference between both is calculated across the full 60-second sample. We consider two time-window sizes, = 1 second  [43]  and = 3 seconds. The baseline feature is calculated from the mean across all baseline segments, , as shown in Equation (3):\n\nwhere , is the computed feature for frequency band , channel and segment . , is the evoked feature for 2 https://github.com/PyWavelets/pywt 3 https://github.com/DustinCarrion/EBAPy the same band, channel and segment, and , is the baseline feature at rest for the same band and channel, and baseline segment .\n\ncorresponds to the total baseline segments. Since the DEAP  [20]  dataset has a 3-second baseline, when the time-window is of size = 1 second, the number of baseline segments, , is 3. On the other hand, when the time-window is of size = 3 seconds, the base line segment is the whole reference baseline included in the dataset, and thus = 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification",
      "text": "We consider two types of classifiers, a 3D CNN, similar to the one described by  [43] , and an SVM with RBF kernel similar to the one used in  [27, 35] .\n\nThe 3D CNN approach was chosen since it preserves the topological relationship of the readings as positioned on the subject's scalp. We use the DWT Entropy, as described in Equation ??, and also evaluate a stacked model where both DWT Entropy and Differential Entropy  [43]  are combined together.\n\nThe SVM was chosen due to its simple architecture, making it a very good candidate for real-time use, together with its high classification accuracy as reported in recent work  [27, 35] . In our case, the input features for the SVM are the DWT Entropy, as described in Equation ?? and DWT Energy, as described in Equation ??.\n\nFor both types of classifiers, we measure the difference in accuracy when applying the baseline removal technique on all models, for both subject independent and subject dependent scenarios.\n\nThe models were trained using a CUDA-compatible GPU, NVIDIA RTX 2060 SUPER 8GB. All samples were shuffled and a stratified n-fold cross validation using the Scikit-learn 4  library to evaluate the performance of the proposed model in both subject-dependent and subject-independent cases.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "3Dcnn With Dwt Entropy",
      "text": "We adapt the 3DCNN model  [43]  to use the DWT entropy instead of DE. It considers a stack of 9 × 9 grids containing channel feature values located according to their corresponding topological position on the scalp.\n\nFor the reduced channel version, we reduce the size of the grid to a 5×5 matrix, in order to eliminate the unwanted channels while retaining the same topology, as shown in Figure  5 . This process reduces the sparsity of the matrix, while retaining the same inter-channel topological relationship.\n\nFour of these grids are stacked on top of each other forming a 3D EEG cube, with each grid representing a specific frequency band: theta ( ); alpha ( ); beta ( ); and gamma ( ), as shown in Figure  6 . The formation of the 3D cube is similar to a colour image in a computer vision context; where the DWT entropy value for each of the frequency bands ( , , , ) is used instead of the colour intensity of the 3 colour channels (Red, Green, Blue).\n\nThe 3D CNN architecture is identical to the original model  [43] , shown in Figure  7 . This CNN structure is contin- uous since it does not use pooling layers between the convolutional layers. This is because the input sample is far smaller than the typical input used in computer vision, and therefore a pooling operation is not necessary. The kernel size is set to 4 with a stride of 1. The first convolutional layer consists of 64 feature maps; doubling the amount in the next two layers with 128 in the second and 256 in the third. The fourth layer fuses the feature maps back into 64 9 × 9 or 5 × 5 feature maps, which are then passed to a fully connected layer of 1024 neurons that map them to a feature vector ∈ 1024 . This is finally passed to a softmax layer to obtain the High or Low prediction  [43] .\n\nThe TensorFlow 5  framework was used to implement and train the 3DCNN models. The Adam optimizer was used to minimize the cross-entropy loss, with the initial learning rate set to 10 -4 . In order to avoid overfitting and improve the overall generalization capability of the system, the dropout probability was set to 0.5, and L2 regularization was also added with a penalty strength of 0.5. We applied a stratified 10-fold cross-validation to both the subject-independent and subject-dependent cases, to minimise overfitting bias from the accuracy results.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "3Dcnn With Stacked Dwt Entropy And De",
      "text": "In the previous section, we made use of only the DWT entropy for the input features that form the 3D EEG cubes. Since DE also performed considerably well  [43] , we also experiment with combining both DWT entropy and DE into an 8-layered EEG cube, with the layers 1 to 4 consisting of the DWT entropy values and layers 5 to 8 corresponding to DE. The architecture of the model was kept identical, as described in Section 3.3.1. The same Tensorflow configuration was used, together with a stratified 10-fold cross-validation for both subject independent and dependent cases.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Svm With Rbf Kernel.",
      "text": "Two SVM classifiers with RBF kernel were implemented, one for Valence and one for Arousal. Each SVM took an input feature vector of size Ω × × , where = 4 corresponds to the number of frequency bands ( , , , ), = 2 represents the number of extracted features per band (wavelet energy, , and entropy, ), and Ω ∈ {5, 32} corresponds to the amount of channels. The sizes of the feature vectors for the full 32-channel and reduced 5-channel cases are 256 and 40 respectively.\n\nThe models were implemented using ThunderSVM 6  to utilize the GPU. Grid Search with Cross Validation was used to find the best hyperparameter values for and for both subject-independent and subject-dependent cases. Table  2  shows the search space of possible parameters considered.\n\nFor the subject-independent case, a sample of one third the size of the full dataset was taken, while retaining the same class distribution of the entire set. From this sample, we found the optimal hyperparameters based on a stratified 6-fold cross validation. These were computed for each model and are presented in Table  1 .\n\nSince it is impractical to optimise the hyperparameters for every individual, we set out to find one generic hyperparameter value for that performs well across all subjects. We first ran a Grid Search on each individual subject and  then computed the mean across all individuals, rounding it to the nearest value in the search space of values in Table  2 , resulting in = 200. The parameter was left to the default setting used by the ThunderSVM implementation, which scales it with respect to the number of features and variance of the input data, = 1∕( × ), thus adapting to each subject's data automatically. This configuration was then used to train the subject-dependent models for each participant using a stratified 8-fold cross validation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Chained Svm Classifiers",
      "text": "Since we are using two separate classifiers for the Valence and Arousal, the models we have discussed so far assume that both classifiers are independent of each other, even though they are using the same input data. However, since the data is the same, these two dimensions are only conditionally independent on the same emotional state represented by the data (",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "⟂ ⟂ |",
      "text": "). This aspect was also observed in previous work  [16] , where Arousal was kept constant when classifying Valence, and vice versa. Discovering either Valence or Arousal can thus be used as an extra input to feed to the other classifier, helping it to get a more accurate prediction of the emotion given the same input data.\n\nThe first model, | (VALARO), predicts the Arousal using the SVM described in Section 3.3.3, and then feeds its output to the second SVM to predict the Valence given the known Arousal. The input feature vector of the second SVM is of size (Ω × × ) + 1, where the last feature is a one-hot encoding of the Arousal (LA = 0, HA = 1).\n\nThe second model, | (AROVAL), first predicts the Valence, and then feeds the prediction to the second SVM to predict the Arousal given the known Valence. The input feature vector of the second SVM is also of size (Ω × × ) + 1, where the last feature is a one-hot encoding of the Valence (LV = 0, HV = 1).\n\nThe two models cannot be used together, but only one of them can be used in conjunction with its SVM counterpart that predicts the first dimension, Arousal or Valence from just the input data.\n\nThe hyperparameters for the subject independent models are listed in Table  1 , while the subject dependent models use the same = 200 and scaling configuration.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "In this section we present results achieved for the proposed models using the DEAP  [20]  dataset. We first analyse the data itself for correlation between each of the chosen 5 channels and the rest of the channels, to determine whether these channels are likely to carry enough information to perform accurate EEG-ER classification.\n\nThe 5 proposed models, 3DCNN with DWT Entropy, 3DCNN with Stacked DWT Entropy and DE, SVM with RBF kernel using DWT Entropy and Energy, and the two Chained SVM models, VALARO and AROVAL are then evaluated. We test each model on two time-window sizes, 1 and 3 seconds, with and without baseline, for both subjectindependent and subject-dependent cases on the full 32 channels and also the reduced 5-channel setup.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Channel Correlation",
      "text": "Prior to evaluating the aforementioned models, the data from all the channels was analysed to determine whether the hypothesis that the chosen 5 channels are enough to perform EEG-ER has sound foundations. The Pearson correlation coefficient was computed between each of the chosen five channels and the other 31 channels, on the four frequency subbands, to analyse whether there is a clear correlation between topologically adjacent channels as opposed to channels that are positioned further away.\n\nFigure  8  illustrates the correlation between channels, with the respective reference channel marked with a \"+\". One can observe that for each of the chosen channels, there is a high correlation with its adjacent channels. This indicates that when using 32 channels, a lot of redundant information is being processed, which might not be needed to achieve high EEG-ER accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Subject Independent",
      "text": "Table  3  shows the accuracy results for the evaluated subject-independent models. In all scenarios the SVM had a superior performance to the 3DCNN, which had a bad performance overall irrespective of whether the baseline removal technique was used or not. For the full 32-channel setup, a time-window of 1 second was sufficient to achieve the best performance for both Valence and Arousal, while for the reduced 5-channel setup, a time-window of 3 seconds achieved a slightly superior accuracy, albeit in both cases the difference was very small. The best accuracy for the full 32-channel was achieved using the baseline removal technique, achieving 83.79% for Valence and 84.31% for Arousal. Similarly, the reduced 5-channel also had its best results using the baseline removal technique, with 80.7% accuracy for Valence and 81.41% accuracy for Arousal. Interestingly, the chained SVM models VALARO and AROVAL consistently improved this accuracy even more, achieving 84.0% for Valence and 85.0% for Arousal on the 32-channel setup, and 82.75% for Valence and 83.44% for Arousal. However, one must keep in mind that these models need to be paired with their SVM counterparts to work, i.e. SVM (VALARO) needs to be paired with the ARO SVM, while SVM (AROVAL) needs to be paired with the VAL SVM. The loss of accuracy   between the full 32-channel and reduced 5-channel setups was also consistently low, confirming that EEG-ER can still be achieved with reliable accuracy with only a few channels.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Subject Dependent",
      "text": "Table  4  shows the results for the mean subject-dependent accuracies across all subjects, together with the standard deviation. Once again, there is a significant difference in performance between the models that use baseline removal preprocessing and those that do not. The 3DCNN using Differential Entropy  [43]  had the best accuracy when baseline removal was not used for the full 32-channel case, at 68.52% for Valence and 69.55% for Arousal using a 1 second timewindow. This was followed closely by the SVM model with 68.10% for Valence and 69.05% for Arousal using a 3-second time window. However, when using the baseline removal preprocessing, the accuracy of the SVM model went up to 95.32% for Valence and 95.68% for Arousal, surpassing the current state-of-the-art 3DCNN DE model by more than 6%.\n\nOn the reduced 5-channel setup, the best accuracy without Similarly to the subject-independent case, the chained SVM models pushed up the accuracy higher, surpassing every model on both the full 32-channel and reduced 5-channel scenarios, with or without baseline removal. However, one must keep in mind that only one of the SVM (VALARO) or SVM (AROVAL) models can be used concurrently, since they have to be paired with their corresponding ARO SVM or VAL SVM respectively.\n\nThe drop in accuracy between the 32 channels and 5 channels was less than 4% for the best performing models using baseline removal preprocessing, while when this was not used, the drop in accuracy was of 7% on Valence and 5% on Arousal for the best performing model (3DCNN DE  [43] ). This was brought down to just around 1% when using the chained SVM models.\n\nAn interesting observation is that the 3DCNN with stacked DWT Entropy and DE for the 5-channel case when using the baseline removal technique and using 3 second time windows had a higher accuracy than the 3DCNN with DE  [43]  and the 3DCNN with DWT Entropy and DE for the full 32-channels with 1 second time windows. While both use a very similar 3DCNN architecture with the same number of layers and configuration, the addition of DWT Entropy with a larger time window yielded a better performance. Figure  9  shows the accuracy for each of the 32 subjects in the dataset, for the 32-channel 3DCNN DE model ( = 1)  [43] , together with other 3DCNN 32-channel and 5-channel variants. The 5-channel DE+DWT model ( = 3) maintains the highest and most consistent accuracy across all subjects.\n\nAs shown in Figure  10  the difference in classification accuracy between the full 32-channels and the reduced 5channel setup is not significant, sometimes even having an inverse effect of increasing accuracy due to less model dimensionality and complexity. What makes a significant difference is whether the baseline removal technique is used, bringing the accuracy of both Valence and Arousal above 80% for all subjects in the dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "The objective of this research was to find the best models for real-time EEG-ER that perform well in both full 32channel and reduced 5-channel modes. Figure  11  shows the best results for both subject-independent and subjectdependent cases. These were all achieved using the proposed SVM with RBF kernel model that makes use of the DWT Entropy and Energy features with baseline removal preprocessing. The mean drop in accuracy between 32-channels and 5-channels is only 3.53%, showing that having a few strategically located EEG sensors is enough to perform reliable EEG-ER. Furthermore, the model performs well in both subjectindependent and subject-dependent cases. The chained SVM architecture also offers the opportunity to increase the accuracy of one of the dimensions, Valence or Arousal, a few percentage points higher, by using the output of one classifier as an extra input to the other classifier. The choice of time window size does not seem to have a significant effect on the SVM classification accuracy, although the 3-second time window performed marginally better in most cases.\n\nApart from achieving a high accuracy for reduced 5channel scenarios, the results on the full 32-channels of the DEAP  [20]  dataset beat the current state-of-the-art accuracy achieved by subject-dependent models. Our proposed SVM has the highest accuracy, 95.3% for Valence and 95.7% for Arousal, while the best accuracy that was achieved at the time of writing was of 92.7% for Valence and 93.1% for Arousal using an ACRNN model  [38] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions & Future Work",
      "text": "This work proposes a real-time solution for EEG-ER using a reduced number of channels, thus opening up the possibilities to make use of less costly consumer-grade EEG devices. A time-windowing approach is used to classify a person's emotion from a few seconds of data, thus providing the opportunity to create BCI systems that interact with users in response to their current emotional state.\n\nThe DEAP  [20]  dataset was used to benchmark the classification models and compare them with those described in prior work. We propose to use DWT to extract time-frequency domain features for both subject-independent models, where one generic model works across all individuals, and subjectdependent models, where the data from each participant is used to create a personalised model for each individual. We also adapt the baseline removal preprocessing technique  [43, 38] , that computes the difference between the individual's active state and rest state, for the DWT features. This preprocessing technique delivers a significant increase in accuracy across both subject-independent and subjectdependent models.\n\nThe 3D CNN model that makes use of DE  [43]  was adapted to work with DWT Entropy and also a combination of both DE and DWT Entropy features. However, an SVM with RBF kernel, making use of DWT Entropy and Energy features proved to perform best on both full 32-channel and reduced 5-channel scenarios, beating the known state-of-theart benchmarks. Furthermore, the mean difference in accuracy between 32 channels and 5 channels for the SVM model is only 3.54%, showing that performing accurate EEG-ER with devices that only have a few channels is feasible.\n\nWe also propose a chained SVM model, where the Arousal and Valence classifiers are chained together, with the output of the Arousal SVM feeding into the Valence SVM, or vice-versa. This approach takes advantage of the conditional independence of both Arousal and Valence on the same      [20]  with and without using baseline removal preprocessing  [43, 38] , for both full 32-channel and reduced 5-channel scenarios.\n\nemotional state, where knowing one of the dimensions can influence the interpretation of the data to predict the other dimension. These models achieve a slightly higher accuracy than the single SVM model. Future work includes validating these models on other datasets, such as SEED  [11, 46] , DREAMER  [18] , and MAHNOB-HCI  [36] . This can then be followed by realworld experiments using the actual consumer-grade devices, such as the Emotiv Insight, in order to assess the performance and generality of the models. Other dimensions of emotion, such as Dominance, can also be explored. The effect of further data preprocessing the data, such as using Independent Component Analysis (ICA) or Principal Component Analysis can also be investigated, which could lead to lower dimensionality (especially in the 32 channel case) and better convergence during training of the classifiers. Furthermore, other classification techniques can also be investigated, such as the ones proposed very recently to detect epileptic seizures  [41, 45] , which make use of Recurrence Plots  [13, 26] .\n\nAchieving a high EEG-ER accuracy with affordable consumer-grade EEG devices opens up a lot of opportunities for applications. BCI-based emotion-adaptive entertainment systems could recommend music or movies according to the user's mood. Video games could also adapt the environment or intensity of the game according to the player's real-time emotion state. Real-time EEG-ER can also be useful to diagnose mental or mood disorders, or even during psychiatric therapy sessions, providing the practitioner with additional real-time information about the feelings of the patient. The machine learning models presented in this work can be further developed and integrated into BCI systems to make these applications a reality.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which has 5 channels together",
      "page": 1
    },
    {
      "caption": "Figure 1: The 5-channel Emotiv Insight EEG Device",
      "page": 1
    },
    {
      "caption": "Figure 2: The DEAP dataset [20] is often used in EEG-ER bench-",
      "page": 2
    },
    {
      "caption": "Figure 2: Russell’s Circumplex Model of Aﬀect. [33]",
      "page": 2
    },
    {
      "caption": "Figure 3: , corresponding to those used by the Emotiv Insight EEG",
      "page": 3
    },
    {
      "caption": "Figure 3: The selected sensors for the reduced 5-channel",
      "page": 3
    },
    {
      "caption": "Figure 2: 3.1. Data Preprocessing",
      "page": 3
    },
    {
      "caption": "Figure 4: 1https://mne.tools/stable/index.html.",
      "page": 3
    },
    {
      "caption": "Figure 4: Time-windows of length 1 second.",
      "page": 4
    },
    {
      "caption": "Figure 5: This process reduces the sparsity of the matrix, while",
      "page": 4
    },
    {
      "caption": "Figure 6: The formation of the 3D",
      "page": 4
    },
    {
      "caption": "Figure 7: This CNN structure is contin-",
      "page": 4
    },
    {
      "caption": "Figure 5: (1) - the positions of 32 electrodes, with the selected 5 channels highlighted in green; (2) - the matrix representation of",
      "page": 5
    },
    {
      "caption": "Figure 6: Four 2D grids, one for each sub-band, are combined",
      "page": 5
    },
    {
      "caption": "Figure 7: Continuous Convolutional Neural Network with 3D DWT entropy input. 퐶∈{5, 9} represents the number of rows and",
      "page": 6
    },
    {
      "caption": "Figure 8: Pearson correlation based on wavelet entropy features between each of the selected 5 channels (indicated with a “+\"",
      "page": 7
    },
    {
      "caption": "Figure 8: illustrates the correlation between channels, with",
      "page": 7
    },
    {
      "caption": "Figure 9: shows the accuracy for each of the 32 subjects in the dataset,",
      "page": 9
    },
    {
      "caption": "Figure 10: the diﬀerence in classiﬁcation",
      "page": 9
    },
    {
      "caption": "Figure 9: Comparison of subject-dependent 3DCNN accuracies on each of the 32 subjects in the DEAP dataset [20] using baseline",
      "page": 10
    },
    {
      "caption": "Figure 10: Comparison of the subject-dependent SVM accuracy (휏= 3) on each of the 32 subjects in the DEAP dataset [20] with",
      "page": 10
    },
    {
      "caption": "Figure 11: Comparison of the best accuracy for Valence and",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Optimal SVM RBF hyperparameters for subject-independent models found via grid search.",
      "data": [
        {
          "Model": "Size\nBaseline",
          "Hyperparameters": "VAL\nARO\nVALARO\nAROVAL"
        },
        {
          "Model": "1\nNo",
          "Hyperparameters": "C :1, g :1\nC :1, g :1\nC :1, g :1\nC :1, g :1"
        },
        {
          "Model": "1\nYes",
          "Hyperparameters": "C :50, g :1\nC :50, g :1\nC :50, g :1\nC :300, g :1"
        },
        {
          "Model": "3\nNo",
          "Hyperparameters": "C :50, g :1\nC :300, g :0.001\nC :1, g :1\nC :1, g :1"
        },
        {
          "Model": "3\nYes",
          "Hyperparameters": "C :50, g :1\nC :200, g :1\nC :50, g :1\nC :50, g :1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Optimal SVM RBF hyperparameters for subject-independent models found via grid search.",
      "data": [
        {
          "Model": "Size\nBaseline",
          "Hyperparameters": "VAL\nARO\nVALARO\nAROVAL"
        },
        {
          "Model": "1\nNo",
          "Hyperparameters": "C :1, g :1\nC :1, g :1\nC :1, g :1\nC :1, g :1"
        },
        {
          "Model": "1\nYes",
          "Hyperparameters": "C :50, g :1\nC :50, g :1\nC :50, g :1\nC :50, g :1"
        },
        {
          "Model": "3\nNo",
          "Hyperparameters": "C :1, g :1\nC :1, g :1\nC :1, g :1\nC :1, g :1"
        },
        {
          "Model": "3\nYes",
          "Hyperparameters": "C :50, g :1\nC :50, g :1\nC :50, g :1\nC :50, g :1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Mean subject-independent accuracy results (%) together with standard deviation, for",
      "data": [
        {
          "VAL": "",
          "1": "3",
          "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (VALARO)": "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (VALARO)",
          "55.31 (±5.53)\n55.31 (±5.53)\n55.31 (±5.52)\n55.31 (±5.52)\n55.31 (±4.95)\n55.31 (±4.95)\n64.36 (±3.1)\n83.79 (±2.2)\n68.0 (±5.3)\n84.0 (±1.8)": "55.31 (±3.53)\n55.31 (±3.53)\n55.31 (±3.53)\n55.31 (±3.53)\n55.31 (±4.82)\n55.31 (±4.82)\n63.10 (±6.8)\n82.45 (±4.2)\n67.0 (±7.12)\n83.0 (±2.71)",
          "55.31 (±4.97)\n55.31 (±4.96)\n55.31 (±4.15)\n55.31 (±4.16)\n55.31 (±4.32)\n55.31 (±4.32)\n63.36 (±3.8)\n79.58 (±1.4)\n64.34 (±3.6)\n81.68 (±2.6)": "55.31 (±5.01)\n55.31 (±3.87)\n55.31 (±4.95)\n55.31 (±4.92)\n55.31 (±4.82)\n55.31 (±4.83)\n64.86 (±6.4)\n80.70 (±6.8)\n67.24 (±5.01)\n82.75 (±9.43)",
          "0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n-1.55\n-5.02\n-5.00\n-2.76": "0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n2.79\n-2.12\n0.35\n0.3"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Mean subject-independent accuracy results (%) together with standard deviation, for",
      "data": [
        {
          "ARO": "",
          "1": "3",
          "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (AROVAL)": "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (AROVAL)",
          "57.58 (±4.18)\n57.58 (±4.18)\n57.58 (±4.18)\n57.58 (±4.18)\n57.58 (±4.34)\n57.58 (±3.16)\n66.90 (±2.4)\n84.31 (±2.5)\n70.0 (±1.81)\n85.0 (±2.41)": "57.58 (±3.85)\n57.58 (±3.85)\n57.58 (±3.85)\n57.58 (±3.84)\n57.58 (±3.67)\n57.58 (±3.67)\n65.98 (±7.21)\n83.86 (±3.83)\n69.0 (±5.7)\n84.0 (±4.5)",
          "57.58 (±4.34)\n57.58 (±4.34)\n57.58 (±3.47)\n60.24 (±4.34)\n57.58 (±3.78)\n57.58 (±4.33)\n64.34 (±3.1)\n80.54 (±3.9)\n66.86 (±3.0)\n82.57 (±2.9)": "56.68 (±4.01)\n55.49 (±2.80)\n58.68 (±3.47)\n57.58 (±4.07)\n57.58 (±3.67)\n57.58 (±3.67)\n65.96 (±8.31)\n81.41 (±9.10)\n68.42 (±6.2)\n83.44 (±5.9)",
          "0.00\n0.00\n0.00\n4.62\n0.00\n0.00\n-2.49\n-3.96\n-4.49\n-2.85": "-1.56\n-3.63\n1.91\n0.00\n-1.64\n0.00\n-0.03\n-2.92\n-0.84\n-0.67"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Mean subject-independent accuracy results (%) together with standard deviation, for",
      "data": [
        {
          "VAL": "",
          "1": "3",
          "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (VALARO)": "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (VALARO)",
          "68.52 (±6.03)\n89.45 (±4.42)\n62.80 (±7.14)\n82.83 (±9.02)\n66.63 (±7.32)\n88.75 (±6.93)\n67.24 (±5.68)\n94.41 (±3.02)\n71.97 (±5.61)\n95.26 (±2.6)": "65.19 (±6.28)\n82.31 (±5.02)\n63.67 (±6.80)\n81.63 (±9.66)\n67.35 (±6.64)\n86.51 (±7.86)\n68.10 (±5.55)\n95.32 (±2.99)\n72.37 (±5.71)\n95.93 (±2.59)",
          "63.66 (±6.95)\n73.20 (±3.80)\n65.96 (±6.98)\n82.45 (±4.18)\n64.95 (±7.02)\n84.17 (±7.41)\n67.45 (±6.64)\n90.59 (±4.31)\n72.54 (±6.86)\n92.45 (±3.92)": "62.16 (±6.43)\n72.48 (±4.06)\n68.01 (±6.54)\n86.18 (±3.87)\n67.91 (±6.38)\n91.38 (±2.72)\n66.99 (±6.83)\n92.2 (±4.41)\n72.15 (±7.47)\n93.81 (±3.77)",
          "-7.09\n-18.17\n5.03\n-0.46\n-2.52\n-5.16\n0.31\n-4.05\n0.79\n-2.94": "-4.65\n-11.94\n6.82\n5.57\n0.83\n5.63\n-1.63\n-3.27\n-0.3\n-2.21"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Mean subject-independent accuracy results (%) together with standard deviation, for",
      "data": [
        {
          "ARO": "",
          "1": "3",
          "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (AROVAL)": "3DCNN (DE) [43]\n3DCNN (DWT)\n3DCNN (Stacked)\nSVM\nSVM (AROVAL)",
          "69.55 (±7.46)\n90.24 (±4.08)\n64.74 (±9.10)\n84.01 (±9.29)\n67.56 (±7.91)\n89.74 (±7.45)\n68.92 (±6.86)\n94.86 (±2.96)\n73.37 (±5.95)\n95.58 (±2.58)": "66.59 (±7.66)\n83.9 (±4.43)\n64.82 (±8.95)\n83.1 (±8.99)\n67.75 (±7.44)\n87.69 (±6.67)\n69.05 (±7.23)\n95.68 (±2.87)\n73.22 (±6.41)\n96.26 (±2.31)",
          "66.26 (±9.16)\n76.52 (±4.83)\n67.99 (±8.98)\n83.58 (±4.37)\n66.22 (±9.02)\n85.62 (±6.77)\n67.90 (±6.88)\n90.97 (±3.60)\n72.58 (±6.27)\n92.91 (±3.19)": "65.11 (±9.04)\n75.29 (±5.48)\n68.78 (±8.49)\n86.88 (±4.27)\n69.51 (±8.08)\n91.81 (±2.89)\n67.50 (±6.83)\n92.13 (±3.53)\n72.12 (±6.76)\n94.19 (±2.92)",
          "-4.73\n-15.20\n5.02\n-0.51\n-1.98\n-4.59\n-1.48\n-4.10\n-1.08\n-2.79": "-2.22\n-10.26\n6.11\n4.55\n2.60\n4.70\n-2.24\n-3.71\n-1.5\n-2.15"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Eeg-based automatic emotion recognition: Feature extraction, selection and classification methods",
      "authors": [
        "P Ackermann",
        "C Kohlschein",
        "J Bitsch",
        "K Wehrle",
        "S Jeschke"
      ],
      "year": "2016",
      "venue": "IEEE 18th international conference on e-health networking, applications and services"
    },
    {
      "citation_id": "2",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "Emotions recognition using EEG signals: A survey",
      "doi": "10.1109/TAFFC.2017.2714671"
    },
    {
      "citation_id": "3",
      "title": "EEG-based emotion recognition approach for e-healthcare applications",
      "authors": [
        "M Ali",
        "A Mosa",
        "F Al Machot",
        "K Kyamakya"
      ],
      "year": "2016",
      "venue": "International Conference on Ubiquitous and Future Networks",
      "doi": "10.1109/ICUFN.2016.7536936"
    },
    {
      "citation_id": "4",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications",
      "doi": "10.1016/j.eswa.2015.10.049"
    },
    {
      "citation_id": "5",
      "title": "Emotion Recognition with Machine Learning using EEG Signals",
      "authors": [
        "O Bazgir",
        "Z Mohammadi",
        "S Habibi"
      ],
      "year": "2018",
      "venue": "2018 25th National and 3rd International Iranian Conference on Biomedical Engineering (ICBME)"
    },
    {
      "citation_id": "6",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "7",
      "title": "Emotion in human-computer interaction, in: The human-computer interaction handbook",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2007",
      "venue": "Emotion in human-computer interaction, in: The human-computer interaction handbook"
    },
    {
      "citation_id": "8",
      "title": "EBAPy: A Python framework for analyzing the factors that have an influence in the performance of EEG-based applications",
      "authors": [
        "D Carrión-Ojeda",
        "P Martínez-Arias",
        "R Fonseca-Delgado",
        "I Pineda"
      ],
      "year": "2021",
      "venue": "Software Impacts",
      "doi": "10.1016/j.simpa.2021.100062"
    },
    {
      "citation_id": "9",
      "title": "Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "Ten lectures on wavelets",
      "authors": [
        "I Daubechies"
      ],
      "year": "1992",
      "venue": "Ten lectures on wavelets"
    },
    {
      "citation_id": "11",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R Duan",
        "J Zhu",
        "B Lu"
      ],
      "year": "2013",
      "venue": "th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "12",
      "title": "Performance of the emotiv epoc headset for p300-based applications",
      "authors": [
        "M Duvinage",
        "T Castermans",
        "M Petieau",
        "T Hoellinger",
        "G Cheron",
        "T Dutoit"
      ],
      "year": "2013",
      "venue": "Biomedical engineering online"
    },
    {
      "citation_id": "13",
      "title": "Recurrence plots of dynamical systems",
      "authors": [
        "J Eckmann",
        "S Kamphorst",
        "D Ruelle"
      ],
      "year": "1995",
      "venue": "World Scientific Series on Nonlinear Science Series A"
    },
    {
      "citation_id": "14",
      "title": "Evaluation of Features in Detection of Dislike Responses to Audio-Visual Stimuli from EEG Signals",
      "authors": [
        "F Feradov",
        "I Mporas",
        "T Ganchev"
      ],
      "year": "2020",
      "venue": "Computers",
      "doi": "10.3390/computers9020033"
    },
    {
      "citation_id": "15",
      "title": "Cross-subject eeg-based emotion recognition through neural networks with stratified normalization",
      "authors": [
        "J Fernandez",
        "N Guttenberg",
        "O Witkowski",
        "A Pasquali"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition based on the sample entropy of eeg",
      "authors": [
        "X Jie",
        "R Cao",
        "L Li"
      ],
      "year": "2014",
      "venue": "Bio-medical materials and engineering"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based emotion recognition using deep learning network with principal component based covariate shift adaptation",
      "authors": [
        "S Jirayucharoensak",
        "S Pan-Ngum",
        "P Israsena"
      ],
      "year": "2014",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "18",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "19",
      "title": "Emotion dysregulation in mood disorders: a review of current challenges",
      "authors": [
        "M Keshky"
      ],
      "year": "2018",
      "venue": "J Psychol Clin Psychiatry"
    },
    {
      "citation_id": "20",
      "title": "DEAP: A database for Emotion Analysis using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Wavelet transform for classification of EEG signal using SVM and ANN",
      "authors": [
        "N Kumar",
        "K Alam",
        "A Siddiqi"
      ],
      "year": "2017",
      "venue": "Biomedical and Pharmacology Journal",
      "doi": "10.13005/bpj/1328"
    },
    {
      "citation_id": "22",
      "title": "Pywavelets: A python package for wavelet analysis",
      "authors": [
        "G Lee",
        "R Gommers",
        "F Waselewski",
        "K Wohlfahrt",
        "A O'leary"
      ],
      "year": "2019",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition from multi-channel EEG data through Convolutional Recurrent Neural Network",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "G Yu",
        "Y Hou",
        "B Hu"
      ],
      "year": "2016",
      "venue": "Proceedings -2016 IEEE International Conference on Bioinformatics and Biomedicine",
      "doi": "10.1109/BIBM.2016.7822545"
    },
    {
      "citation_id": "24",
      "title": "Latent factor decoding of multi-channel eeg for emotion recognition through autoencoder-like neural networks",
      "authors": [
        "X Li",
        "Z Zhao",
        "D Song",
        "Y Zhang",
        "J Pan",
        "L Wu",
        "J Huo",
        "C Niu",
        "D Wang"
      ],
      "year": "2020",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "25",
      "title": "Measuring brain signals using emotiv devices",
      "authors": [
        "Martin Strmiska",
        "Zuzana Koudelková",
        "Martina Žabčíková"
      ],
      "year": "2018",
      "venue": "WSEAS Transactions on Systems and Control"
    },
    {
      "citation_id": "26",
      "title": "Recurrence plots for the analysis of complex systems",
      "authors": [
        "N Marwan",
        "M Romano",
        "M Thiel",
        "J Kurths"
      ],
      "year": "2007",
      "venue": "Physics reports"
    },
    {
      "citation_id": "27",
      "title": "Wavelet-based emotion recognition system using EEG signal",
      "authors": [
        "Z Mohammadi",
        "J Frounchi",
        "M Amiri"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-015-2149-8"
    },
    {
      "citation_id": "28",
      "title": "Studies of Emotion: A Theoretical and Empirical Review of Psychophysiological Studies of Emotion",
      "authors": [
        "C Niemic"
      ],
      "year": "2004",
      "venue": "Studies of Emotion: A Theoretical and Empirical Review of Psychophysiological Studies of Emotion"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition based on eeg features in movie clips with channel selection",
      "authors": [
        "M Özerdem",
        "H Polat"
      ],
      "year": "2017",
      "venue": "Brain informatics"
    },
    {
      "citation_id": "30",
      "title": "Affective computing for hci",
      "authors": [
        "R Picard"
      ],
      "year": "1999",
      "venue": "HCI"
    },
    {
      "citation_id": "31",
      "title": "Wavelet entropy: A new tool for analysis of short duration brain electrical signals",
      "authors": [
        "O Rosso",
        "S Blanco",
        "J Yordanova",
        "V Kolev",
        "A Figliola",
        "M Schürmann",
        "E Ba Ar"
      ],
      "year": "2001",
      "venue": "Journal of Neuroscience Methods",
      "doi": "10.1016/S0165-0270(00)00356-3"
    },
    {
      "citation_id": "32",
      "title": "Robust EEG emotion classification using segment level decision fusion",
      "authors": [
        "V Rozgic",
        "S Vitaladevuni",
        "R Prasad"
      ],
      "year": "2013",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP.2013.6637858"
    },
    {
      "citation_id": "33",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "34",
      "title": "EEG-based emotion recognition using 3D convolutional neural networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Wahby Shalaby"
      ],
      "year": "2018",
      "venue": "International Journal of Advanced Computer Science and Applications",
      "doi": "10.14569/ijacsa.2018.090843"
    },
    {
      "citation_id": "35",
      "title": "EEG Classification by factoring in Sensor Configuration. arXiv e-prints",
      "authors": [
        "L Shibly Mokatren",
        "R Ansari",
        "A Enis Cetin",
        "A Leow",
        "H Klumpp",
        "O Ajilore",
        "Yarman",
        "F Vural"
      ],
      "year": "2019",
      "venue": "EEG Classification by factoring in Sensor Configuration. arXiv e-prints"
    },
    {
      "citation_id": "36",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "37",
      "title": "Evaluation of consumer eeg device emotiv epoc",
      "authors": [
        "K Stytsenko",
        "E Jablonskis",
        "C Prahm"
      ],
      "year": "2011",
      "venue": "MEi: CogSci Conference"
    },
    {
      "citation_id": "38",
      "title": "EEG-based Emotion Recognition via Channel-wise Attention and Self Attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3025777"
    },
    {
      "citation_id": "39",
      "title": "Familiarity effects in eeg-based emotion recognition",
      "authors": [
        "N Thammasan",
        "K Moriyama",
        "K Fukui",
        "M Numao"
      ],
      "year": "2017",
      "venue": "Brain informatics"
    },
    {
      "citation_id": "40",
      "title": "EEG-based BCI emotion recognition: A survey",
      "authors": [
        "P Torres",
        "E Torres",
        "E Hernández-Álvarez",
        "M Yoo"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "41",
      "title": "Eeg signal analysis for seizure detection using recurrence plots and tchebichef moments",
      "authors": [
        "K Tziridis",
        "T Kalampokas",
        "G Papakostas"
      ],
      "year": "2021",
      "venue": "2021 IEEE 11th Annual Computing and Communication Workshop and Conference"
    },
    {
      "citation_id": "42",
      "title": "Discrete wavelet transfom for nonstationary signal processing",
      "authors": [
        "Y Wang",
        "W Wu",
        "Q Zhu",
        "G Shen"
      ],
      "year": "2011",
      "venue": "Discrete Wavelet Transforms -Theory and Applications"
    },
    {
      "citation_id": "43",
      "title": "Continuous Convolutional Neural Network with 3D Input for EEG-based Emotion Recognition",
      "authors": [
        "Y Yang",
        "Q Wu",
        "Y Fu",
        "X Chen"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "44",
      "title": "Emotion Recognition of EEG Signals Based on the Ensemble Learning Method",
      "authors": [
        "C Yu",
        "C Rui",
        "G Jifeng"
      ],
      "year": "2021",
      "venue": "Emotion Recognition of EEG Signals Based on the Ensemble Learning Method"
    },
    {
      "citation_id": "45",
      "title": "Grp-dnet: A gray recurrence plot-based densely connected convolutional network for classification of epileptiform eeg",
      "authors": [
        "M Zeng",
        "X Zhang",
        "C Zhao",
        "X Lu",
        "Q Meng"
      ],
      "year": "2021",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "46",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W Zheng",
        "B Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development",
      "doi": "10.1109/TAMD.2015.2431497"
    }
  ]
}