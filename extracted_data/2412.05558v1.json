{
  "paper_id": "2412.05558v1",
  "title": "Wavfusion: Towards Wav2Vec 2.0 Multimodal Speech Emotion Recognition",
  "published": "2024-12-07T06:43:39Z",
  "authors": [
    "Feng Li",
    "Jiusong Luo",
    "Wanjun Xia"
  ],
  "keywords": [
    "Speech emotion recognition",
    "multimodal",
    "wav2vec 2.0",
    "A-GRU",
    "A-GRU-LVC"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) remains a challenging yet crucial task due to the inherent complexity and diversity of human emotions. To address this problem, researchers attempt to fuse information from other modalities via multimodal learning. However, existing multimodal fusion techniques often overlook the intricacies of cross-modal interactions, resulting in suboptimal feature representations. In this paper, we propose WavFusion, a multimodal speech emotion recognition framework that addresses critical research problems in effective multimodal fusion, heterogeneity among modalities, and discriminative representation learning. By leveraging a gated cross-modal attention mechanism and multimodal homogeneous feature discrepancy learning, WavFusion demonstrates improved performance over existing state-of-the-art methods on benchmark datasets. Our work highlights the importance of capturing nuanced cross-modal interactions and learning discriminative representations for accurate multimodal SER. Experimental results on two benchmark datasets (IEMOCAP and MELD) demonstrate that WavFusion succeeds over the state-of-the-art strategies on emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, speech emotion recognition (SER) is a fascinating field that utilizes technology to analyze and identify different emotions present in human speech  [1] . This technology has various applications, including in customer service and market research  [2] , learning and education  [3] , mental health  [4] , and social media analytics  [5] . In real-life scenarios, humans express emotions not only through speech but also through alternative modalities, such as text and visuals  [6, 7] . Previous studies on SER typically rely on speech information. However, different modalities provide complementary information for emotion recognition, and emotion recognition of the single modality is not inadequate to meet real-world demands. To address this problem, researchers utilize multimodal information to identify emotional states  [8] . In the domain of Multimodal Emotion Recognition (MER), the information of diverse modalities is complementary, providing additional cues to mitigate semantic and emotional ambiguities.\n\nIn addition to multimodality, another challenge of SER is achieving better interaction during the fusion of different modalities. Firstly, multimodal data often exhibit asynchrony  [9] . For instance, visual signals typically precede audio signals by approximately 120 ms in emotional expressions  [10] . This asynchronicity poses a challenge to feature fusion and model design, necessitating methods to address temporal alignment and matching issues. To address this issue, Tsai et al.  [11]  have proposed specific asynchronous models and cross-modal attention mechanisms. Zheng et al.  [12]  solved heterogeneity among different encoder output features by employing unsupervised training of a multi-channel weight-sharing autoencoder. This approach minimizes the differences among features extracted from different modalities. Additionally, the interactions are simulated by supervised training of cascaded multi-head attention mechanisms. However, most methods with cross-modal attention mechanisms ignore redundant information during the fusion process, thus restricting the performance of MER. Additionally, samples with the same emotion in multimodal data may exhibit differences across modalities, referred to as homogeneous feature differences. For instance, some features in speech and text may exhibit formal similarity but convey different emotional states  [13] . Hazarika et al.  [14]  projected each modality into two different subspaces capturing modality-invariant and modality-specific features. However, they only considered the differences between the different emotion of same modalities and ignored the differences between different modalities with the same emotion. DialogueTRM explores intra-and inter-modal emotional behaviors in conversations, using Transformers to model the context  [15] . MMGCN proposes a multimodal fusion approach via a deep graph convolution network, modeling the interactions between different modalities using a graph  [16] . MM-DFN introduces a dynamic fusion network that leverages intra-and inter-modal information at different levels of representation  [17] . M2FNet proposes a multimodal fusion network that learns and fuses complementary information from audio, visual, and textual modalities  [18] . Therefore, in this paper, we propose a novel arhitecture called WavFusion for emotion recognition. Unlike DialogueTRM, WavFusion specifically focuses on incorporating wav2vec2.0  [19]  with a gated cross-modal attention mechanism to dynamically fuse multimodal features. Additionally, WavFusion introduces multimodal homogeneous feature discrepancy learning to distinguish between same-emotion but different-modality representations. WavFusion does not rely on graph-based modeling but instead uses a transformer architecture with a modified cross-modal attention mechanism. WavFusion also emphasizes capturing both global and local visual information through the A-GRU-LVC module. While MM-DFN focuses on dynamic fusion strategies, WavFusion emphasizes the use of wav2vec2.0 pre-trained representations and a gated cross-modal attention mechanism to mitigate redundant information during fusion. Additionally, WavFusion incorporates multimodal homogeneous feature discrepancy learning to distinguish between representations of the same emotion across different modalities.\n\nThe main contributions of this paper can be summarized as follows:\n\n-We propose a multimodal speech emotion recognition model (WavFusion) that leverages the power of wav2vec 2.0 and incorporates textual and visual modalities to enhance the performance of audio-based emotion recognition. -We integrate the designed gated cross-modal attention mechanism into the wav2vec 2.0 model to mitigate redundant information during the fusion process. Meanwhile, we employ multimodal homogeneous feature discrepancy learning to enhance the discriminative capability of the model. -Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed method. Our WavFusion succeeds over existing stateof-the-art methods.\n\n2 Proposed Method",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Statement",
      "text": "Given a multimodal signal S j = S a j , S t j , S v j , we can represent the unimodal raw sequence extracted from the video fragment j as S m j , m ∈ {a, t, v}. Here, the modalities are denoted by {a, t, v}, which refer to audio, text, and visual modalities.\n\nIn WavFusion, we aim to predict the emotion category for each utterance. It focuses on categorizing the emotion conveyed in each utterance, assigning it to a specific emotion class or category, y j ∈ R c . c is the number of emotion categories. Figure  1  illustrates the overall structure of WavFusion, including an auxiliary modal encoder, a primary modal encoder, and multimodal homogeneous feature discrepancy learning. The orange color represents positive emotions and the green color represents negative emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Auxiliary Modality Encoder",
      "text": "Video Representation For the visual modality, we use the EfficientNet pretrained model as a feature extractor to obtain visual features e v j . This model is a self-supervised framework for visual representation learning. In this paper, we attempt to extend EfficientNet to emotion recognition. e v j can be formulated as:\n\nwhere Φ visual denotes the function of EfficientNet model. On the other hand, we consider the context and situation conveyed by the global information in the visual modality, along with the specific details of actions and expressions from local information. The visual feature is fed into the proposed A-GRU-LVC module, which aims to extract both global and local features.\n\n...\n\nWow, it's great to see you. where F SA and F GRU denote the learning functions of GRU and self-attentive mechanism, respectively. Simultaneously, to preserve local corner point regions and extract local information, a learnable visual center (LVC) is implemented on the visual features  [20] . This LVC aggregates features from local areas, ensuring that important local information is retained. In contrast to the approach, we utilize one-dimensional convolution instead of two-dimensional convolution.\n\nwhere F LV C denotes the learning functions of the LVC block. Finally, the output of the A-GRU-LVC block is obtained by connecting the output of the self-attention module X v1 j and the output of the LVC block X v2 j along the last dimension.\n\nContextualized Word Representation To capture rich contextual information from textual data, we utilize the RoBERTa-base model, which belongs to the transformer family, as a contextual encoder. The architecture of RoBERTa consists of multiple Transformer layers, including a stack of encoders. Each encoder layer contains a multi-head self-attention mechanism and a feed-forward neural network. RoBERTa is designed to capture contextualized representations of words in a sentence, allowing it to understand the meaning and relationships between different words. e t j can be formulated as:\n\nwhere Φ text denotes the function of the RoBERTa pre-train model.\n\nTo further consider context-sensitive dependence for text features, we feed it into the GRU and the self-attention mechanism to obtain global features of the text information. Due to the strong temporal continuity present in textual information, we opted not to employ the LVC mechanism to capture local feature.\n\nwhere F SA and F GRU denote the learming functions of GRU and self-attentive mechanism, respectively.\n\nMajor Modality Encoder In WavFusion, we encode low-level audio features through the shallow transformer layer, followed by combining text and visual features through the deep transformer layer to form a comprehensive multimodal representation. We define the original transformer layer as a shallow transformer layer and the modified transformer layer as a deep transformer. The incorporation of text and vision into wav2vec 2.0 detects relevant information within the extensive pre-trained audio knowledge, thereby enhancing emotional information within the multimodal fusion representation. The low-level acoustic features X a j extracted by the shallow transformer block are calculated as follows:\n\nwhere F ST is the learning function of shallow transformer layers.\n\nFinally, the augmented features X F 1 j and X F 2 j are processed through the following gated filtering mechanism. The ratio of each channel can be dynamically defined by a learnable parameter that filters out misinformation generated during cross-modal interactions.\n\nMultimodal Homogeneous Feature Discrepancy Learning Multimodal homogeneous feature discrepancy learning has made significant progress in multimodal emotion recognition. It can optimize the modal representation ability and extract richer and more accurate emotional information by learning the relationships and differences between homogeneous features. First, we feed unfused audio features X a j , text features X t j and visual features X v j into a shared encoder to obtain homogeneous features. It minimizes the feature gap from different modalities and contributes to multimodal alignment.\n\nwhere SD is the shared encoder learning function that consists of a simple linear layer.\n\nIn this study, we perform multimodal homogeneous feature discrepancy learning to enhance the interactions between the same emotions but different modalities, and amplify the differences between the same modalities but different emotions. We define this loss function as margin loss.\n\nwhere\n\nis the modality of the sample i, and the c[i] is the label of sample i. cos denotes the cosine similarity between two feature vectors. By applying a distance margin α, we ensure that the distance between positive samples is smaller than the distance between negative samples. Here, positive samples refer to the same emotion but different emotions, and negative samples refer to the same modality but different emotions.\n\nSimilarly, cross-entropy serves as a commonly employed loss function for optimizing model parameters and enhancing classification accuracy during training.\n\nwhere y j is the true label of the sample, ŷj is the prediction of the sample, and N D is the number of samples in the dataset D.\n\nwhere λ is the balance factor.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate our proposed method on two prevalent benchmark datasets for ERC, including IEMOCAP  [21]  and MELD  [22] , respectively. The IEMOCAP dataset consists of 12 hours of improvised and scripted audio-visual data from 10 UC theatre actors (five males and five females). The dataset is divided into five binary sessions, and each conversation is annotated with emotional information in four modalities: video, audio, transcription, and motion capture of facial movements. We evaluate our model using audio, transcribed, and video data. The dataset contains a total of 7380 data samples. E.g., happy, neutral, angry, excited, sad, and frustrated. For evaluation, we employ a five-fold cross-validation approach.\n\nThe first four sessions are utilized as the training set and the validation set, and the last session is utilized as the testing set.\n\nThe MELD dataset is derived from over 1,400 dialogues and 13,000 utterances extracted from the TV series Friends. Each utterance in the dataset is annotated with one of seven emotion labels: neutral, surprise, fear, sadness, joy, disgust, and anger. The dataset includes multimodal scenes, making it suitable for studying multimodal emotion recognition tasks. For our experiments, we utilize the predefined training/validation splits provided with the MELD dataset. This ensures consistency with existing approaches and allows for a fair comparison with other models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Setting",
      "text": "For text and visual modalities, we freeze the parameters in the RoBERTa and EfficientNet pre-trained models and treat them as a feature extractor. The last dimension of the text and visual features is 768 and 64. For speech modalities, we unfreeze the parameters of the deep transformer layer in the wav2vec 2.0 pre-train model. These parameters are updated during model training, while the parameters of the other layers are freezing. The last dimension of the speech features are 768 and 64.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparative Analysis",
      "text": "In Tables  1  and 2 , we show the performance of different approaches on the IEMOCAP and MELD datasets. The evaluation metrics are Accuracy (ACC) and Weighted F1 score (WF1). On the IEMOCAP dataset, our method outperformed the state-of-the-art by 0.84% in ACC and 0.74% in WF1. Similarly, on the MELD dataset, our method surpassed the state-of-the-art by 0.43% in ACC and 0.44% in WF1. The reasons are probably twofold. Firstly, we argue that this is because most of these models do not explicitly consider redundant information in the cross-modal fusion process, but our proposed method considers these through a gated cross-modal attention mechanism. Secondly, most of them only take into account the distances of different emotion samples of the same modality, but not the distances of the same emotion samples of different modalities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "To verify the effectiveness of WavFusion model, we conduct ablation studies on the IEMOCAP dataset. First, we reveal the importance of each modality in this section. Specifically, when utilizing a single modality, we omitted the gated cross-modal attention and multimodal homogeneous feature discrepancy learning. The results in Table  3  illustrate that the highest accuracy and weighted average F1 scores are attained when incorporating all three modalities. Due to the complexity of emotion recognition, recognizing emotions using a single modality is challenging to meet the demands of reality. We can achieve better recognition performance by integrating multimodal information. Additionally, we introduce LVC blocks to capture local information related to visual features. To assess the significance of LVC blocks, we conducted an experiment where we omitted the LVC blocks from the model, thus failing to capture local information about visual features. From Table  4 , we observe that the model with the LVC block outperforms the model without the LVC block. The inclusion of LVC blocks improves ACC by 0.63% and the WF1 by 0.76%. The experiment demonstrates that the LVC blocks are beneficial for capturing relevant contextual details and spatial dependencies. We also investigate the impact of multimodal homogeneous feature discrepancy learning in our framework. In this work, we assigned weights to the balance factor λ for margin loss and observed its effects across various weight values. The corresponding results are presented in Table  5 . The results indicate that the optimal performance on the IEMOCAP dataset is achieved. The model shows a significant improvement by 2.64% and WF1 by 2.94% compared to the absence of margin loss (λ = 0). This demonstrates the effectiveness of multimodal homogeneous feature difference learning in enhancing the model's capacity to discern emotions across diverse modalities. However, we also observe that the performance deteriorates when the balance factor is excessively large (λ = 10). This suggests that an excessive emphasis on margin loss might have a detrimental effect on the original classification task.\n\nWe also observe the effect of gated cross-modal attention mechanism in the proposed framework. In our experiments, we define the original transformer layer the shallow transformer and the modified transformer layer as deep transformer, and observe their effect on the different numbers. In the first line, we omit the proposed gated cross-modal attention mechanism and solely conduct a basic concatenation of the three modal features at the last dimension. The corresponding results are shown in Table  6  where it is observed that 9 shallow transformer layers, 3 deep transformer layers yield the optimal performance for the IEMOCAP dataset. Moreover, from the first and second lines, we can discern the significance of the gated cross-modal attention mechanism for fusion.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel SER approach, which is designed a gated cross-modal attention alternative to self-attention in the wav2vec 2.0 pre-trained model to dynamically fuse features from different modalities. Additionally, we introduce a novel LVC block to efficiently capture the local information of visual features. The model can more effectively utilize the spatial characteristics of visual data, resulting in more comprehensive representations. Finally, we design the concept of multimodal homogeneous feature discrepancy learning, which helps the model to effectively learn and distinguish representations of the same modalities but different emotions. The effectiveness of the proposed model is demonstrated on the IEMOCAP and MELD datasets. The results show promising performance compared to state-of-the-art methods. In the future, we plan to utilize the leveraging large amounts of unlabeled audio and video data available to recognize the different emotion.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the overall structure of WavFusion, including an auxiliary",
      "page": 3
    },
    {
      "caption": "Figure 1: The overview of WavFusion.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "Speech Emotion Recognition"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "Feng Li1,2((cid:66)), Jiusong Luo1, and Wanjun Xia1"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "1 Department of Computer Science and Technology, Anhui University of Finance and"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "Economics, Anhui, China"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "2 School of Information Science and Technology, University of Science and"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "Technology of China, Anhui, China"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "lifeng@aufe.edu.cn"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "Abstract. Speech emotion recognition (SER) remains a challenging yet"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "crucial task due to the inherent complexity and diversity of human emo-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "tions. To address this problem, researchers attempt to fuse information"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "from other modalities via multimodal\nlearning. However, existing multi-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "modal\nfusion techniques often overlook the intricacies of cross-modal\nin-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "teractions, resulting in suboptimal feature representations. In this paper,"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "we propose WavFusion, a multimodal speech emotion recognition frame-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "work that addresses critical\nresearch problems\nin effective multimodal"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "fusion, heterogeneity among modalities, and discriminative representa-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "tion learning. By leveraging a gated cross-modal attention mechanism"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "and multimodal homogeneous feature discrepancy learning, WavFusion"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "demonstrates improved performance over existing state-of-the-art meth-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "ods on benchmark datasets. Our work highlights the importance of cap-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "turing nuanced cross-modal interactions and learning discriminative rep-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "resentations for accurate multimodal SER. Experimental results on two"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "benchmark datasets (IEMOCAP and MELD) demonstrate that WavFu-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal": "sion succeeds over the state-of-the-art strategies on emotion recognition."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nF. Li et al.": "to identify emotional states [8]. In the domain of Multimodal Emotion Recogni-"
        },
        {
          "2\nF. Li et al.": "tion (MER), the information of diverse modalities is complementary, providing"
        },
        {
          "2\nF. Li et al.": "additional cues to mitigate semantic and emotional ambiguities."
        },
        {
          "2\nF. Li et al.": "In addition to multimodality, another challenge of SER is achieving better in-"
        },
        {
          "2\nF. Li et al.": "teraction during the fusion of different modalities. Firstly, multimodal data often"
        },
        {
          "2\nF. Li et al.": "exhibit asynchrony [9]. For instance, visual signals typically precede audio signals"
        },
        {
          "2\nF. Li et al.": "by approximately 120 ms in emotional expressions [10]. This asynchronicity poses"
        },
        {
          "2\nF. Li et al.": "a challenge to feature fusion and model design, necessitating methods to address"
        },
        {
          "2\nF. Li et al.": "temporal alignment and matching issues. To address this issue, Tsai et al.\n[11]"
        },
        {
          "2\nF. Li et al.": "have proposed specific asynchronous models and cross-modal attention mech-"
        },
        {
          "2\nF. Li et al.": "anisms. Zheng et al.\n[12]\nsolved heterogeneity among different encoder output"
        },
        {
          "2\nF. Li et al.": "features by employing unsupervised training of a multi-channel weight-sharing"
        },
        {
          "2\nF. Li et al.": "autoencoder. This approach minimizes the differences among features extracted"
        },
        {
          "2\nF. Li et al.": "from different modalities. Additionally,\nthe\ninteractions are\nsimulated by su-"
        },
        {
          "2\nF. Li et al.": "pervised training of cascaded multi-head attention mechanisms. However, most"
        },
        {
          "2\nF. Li et al.": "methods with cross-modal attention mechanisms ignore redundant information"
        },
        {
          "2\nF. Li et al.": "during the fusion process, thus restricting the performance of MER. Addition-"
        },
        {
          "2\nF. Li et al.": "ally, samples with the same emotion in multimodal data may exhibit differences"
        },
        {
          "2\nF. Li et al.": "across modalities, referred to as homogeneous feature differences. For instance,"
        },
        {
          "2\nF. Li et al.": "some features in speech and text may exhibit formal similarity but convey dif-"
        },
        {
          "2\nF. Li et al.": "ferent emotional states [13]. Hazarika et al.[14] projected each modality into two"
        },
        {
          "2\nF. Li et al.": "different subspaces capturing modality-invariant and modality-specific features."
        },
        {
          "2\nF. Li et al.": "However, they only considered the differences between the different emotion of"
        },
        {
          "2\nF. Li et al.": "same modalities and ignored the differences between different modalities with"
        },
        {
          "2\nF. Li et al.": "the same emotion. DialogueTRM explores intra- and inter-modal emotional be-"
        },
        {
          "2\nF. Li et al.": "haviors in conversations, using Transformers to model the context [15]. MMGCN"
        },
        {
          "2\nF. Li et al.": "proposes a multimodal\nfusion approach via a deep graph convolution network,"
        },
        {
          "2\nF. Li et al.": "modeling the interactions between different modalities using a graph [16]. MM-"
        },
        {
          "2\nF. Li et al.": "DFN introduces a dynamic fusion network that leverages intra- and inter-modal"
        },
        {
          "2\nF. Li et al.": "information at different levels of representation [17]. M2FNet proposes a multi-"
        },
        {
          "2\nF. Li et al.": "modal\nfusion network that\nlearns and fuses\ncomplementary information from"
        },
        {
          "2\nF. Li et al.": "audio, visual, and textual modalities [18]."
        },
        {
          "2\nF. Li et al.": "Therefore,\nin this paper, we propose a novel arhitecture called WavFusion"
        },
        {
          "2\nF. Li et al.": "for emotion recognition. Unlike DialogueTRM, WavFusion specifically focuses"
        },
        {
          "2\nF. Li et al.": "on incorporating wav2vec2.0 [19] with a gated cross-modal attention mechanism"
        },
        {
          "2\nF. Li et al.": "to dynamically fuse multimodal\nfeatures. Additionally, WavFusion introduces"
        },
        {
          "2\nF. Li et al.": "multimodal homogeneous\nfeature discrepancy learning to distinguish between"
        },
        {
          "2\nF. Li et al.": "same-emotion but different-modality representations. WavFusion does not rely"
        },
        {
          "2\nF. Li et al.": "on graph-based modeling but\ninstead uses a transformer architecture with a"
        },
        {
          "2\nF. Li et al.": "modified cross-modal attention mechanism. WavFusion also emphasizes captur-"
        },
        {
          "2\nF. Li et al.": "ing both global and local visual\ninformation through the A-GRU-LVC mod-"
        },
        {
          "2\nF. Li et al.": "ule. While MM-DFN focuses on dynamic fusion strategies, WavFusion empha-"
        },
        {
          "2\nF. Li et al.": "sizes the use of wav2vec2.0 pre-trained representations and a gated cross-modal"
        },
        {
          "2\nF. Li et al.": "attention mechanism to mitigate\nredundant\ninformation during fusion. Addi-"
        },
        {
          "2\nF. Li et al.": "tionally, WavFusion incorporates multimodal homogeneous feature discrepancy"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "learning to distinguish between representations of the same emotion across dif-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "ferent modalities."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "The main contributions of this paper can be summarized as follows:"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "– We propose a multimodal\nspeech emotion recognition model\n(WavFusion)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "that leverages the power of wav2vec 2.0 and incorporates textual and visual"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "modalities to enhance the performance of audio-based emotion recognition."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "– We integrate the designed gated cross-modal attention mechanism into the"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "wav2vec 2.0 model to mitigate redundant information during the fusion pro-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "cess. Meanwhile, we employ multimodal homogeneous\nfeature discrepancy"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "learning to enhance the discriminative capability of the model."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "– Experimental results on two benchmark datasets demonstrate the effective-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "ness of the proposed method. Our WavFusion succeeds over existing state-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n3": "of-the-art methods."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. The overview of WavFusion.": "where FSA and FGRU denote the learning functions of GRU and self-attentive"
        },
        {
          "Fig. 1. The overview of WavFusion.": "mechanism, respectively."
        },
        {
          "Fig. 1. The overview of WavFusion.": "Simultaneously,\nto preserve local corner point\nregions and extract\nlocal\nin-"
        },
        {
          "Fig. 1. The overview of WavFusion.": "formation, a learnable visual center (LVC) is implemented on the visual features"
        },
        {
          "Fig. 1. The overview of WavFusion.": "[20]. This LVC aggregates features from local areas, ensuring that important local"
        },
        {
          "Fig. 1. The overview of WavFusion.": "information is retained. In contrast to the approach, we utilize one-dimensional"
        },
        {
          "Fig. 1. The overview of WavFusion.": "convolution instead of two-dimensional convolution."
        },
        {
          "Fig. 1. The overview of WavFusion.": "X v2\n(cid:0)ev\n(3)\n= FLV C\nj\nj"
        },
        {
          "Fig. 1. The overview of WavFusion.": "where FLV C denotes the learning functions of the LVC block."
        },
        {
          "Fig. 1. The overview of WavFusion.": "Finally, the output of the A-GRU-LVC block is obtained by connecting the"
        },
        {
          "Fig. 1. The overview of WavFusion.": "output of the self-attention module X v1\nand the output of the LVC block X v2"
        },
        {
          "Fig. 1. The overview of WavFusion.": "j\nj"
        },
        {
          "Fig. 1. The overview of WavFusion.": "along the last dimension."
        },
        {
          "Fig. 1. The overview of WavFusion.": "X v\n⊕ X v2\n(4)\nj = X v1\nj"
        },
        {
          "Fig. 1. The overview of WavFusion.": "Contextualized Word Representation To capture rich contextual\ninforma-"
        },
        {
          "Fig. 1. The overview of WavFusion.": "tion from textual data, we utilize the RoBERTa-base model, which belongs to"
        },
        {
          "Fig. 1. The overview of WavFusion.": "the transformer family, as a contextual encoder. The architecture of RoBERTa"
        },
        {
          "Fig. 1. The overview of WavFusion.": "consists of multiple Transformer layers,\nincluding a stack of encoders. Each en-"
        },
        {
          "Fig. 1. The overview of WavFusion.": "coder layer contains a multi-head self-attention mechanism and a feed-forward"
        },
        {
          "Fig. 1. The overview of WavFusion.": "neural network. RoBERTa is designed to capture contextualized representations"
        },
        {
          "Fig. 1. The overview of WavFusion.": "of words in a sentence, allowing it to understand the meaning and relationships"
        },
        {
          "Fig. 1. The overview of WavFusion.": "between different words. et"
        },
        {
          "Fig. 1. The overview of WavFusion.": "j can be formulated as:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "where Φtext denotes the function of the RoBERTa pre-train model."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "To further consider context-sensitive dependence for text features, we feed it"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "into the GRU and the self-attention mechanism to obtain global\nfeatures of the"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "text\ninformation. Due to the strong temporal continuity present\nin textual\nin-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "formation, we opted not to employ the LVC mechanism to capture local feature."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "(cid:1)(cid:1)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "(6)\nX t\n(cid:0)ei\n(cid:0)FGRU\nj = FSA\nj"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "where FSA and FGRU denote the learming functions of GRU and self-attentive"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "mechanism, respectively."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "Major Modality Encoder\nIn WavFusion, we encode low-level audio features"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "through the shallow transformer layer, followed by combining text and visual fea-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "tures through the deep transformer layer to form a comprehensive multimodal"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "representation. We define the original transformer layer as a shallow transformer"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "layer and the modified transformer layer as a deep transformer. The incorpora-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "tion of text and vision into wav2vec 2.0 detects relevant information within the"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "extensive pre-trained audio knowledge, thereby enhancing emotional information"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "within the multimodal fusion representation. The low-level acoustic features X a"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "j"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "extracted by the shallow transformer block are calculated as follows:"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "X a\n(cid:0)Sa\n(7)\nj = FST\nj"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "is the learning function of shallow transformer layers.\nwhere FST"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "X F 1\n(cid:0)X a\n(8)\n= CMA−T\nj\nj , X t"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "X F 2\n(cid:0)X a\n(9)\n= CMA−V\nj\nj , X v"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "Finally,\nthe augmented features X F 1\nand X F 2\nare processed through the"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "j\nj"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "following gated filtering mechanism. The ratio of each channel can be dynami-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "cally defined by a learnable parameter that filters out misinformation generated"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "during cross-modal\ninteractions."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "(10)\n⊕ X F 1\nP∗ = sigmoid (cid:0)F C (cid:0)X F 1\nj"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "X F\n(11)\n+ (1 − P∗) ⊙ X F 2\nj = P∗ ⊙ X F 1"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "Multimodal Homogeneous Feature Discrepancy Learning Multimodal"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "homogeneous feature discrepancy learning has made significant progress in mul-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "timodal emotion recognition.\nIt can optimize the modal\nrepresentation ability"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "and extract\nricher and more accurate\nemotional\ninformation by learning the"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "relationships and differences between homogeneous features. First, we feed un-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n5": "into a shared\nj , text features X t\nj and visual\nj"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "encoder to obtain homogeneous features. It minimizes the feature gap from dif-",
          "F. Li et al.": ""
        },
        {
          "6": "ferent modalities and contributes to multimodal alignment.",
          "F. Li et al.": ""
        },
        {
          "6": "",
          "F. Li et al.": ""
        },
        {
          "6": "where SD is the shared encoder learning function that consists of a simple linear",
          "F. Li et al.": ""
        },
        {
          "6": "layer.",
          "F. Li et al.": ""
        },
        {
          "6": "",
          "F. Li et al.": "In this study, we perform multimodal homogeneous feature discrepancy learn-"
        },
        {
          "6": "ing to enhance the interactions between the same emotions but different modali-",
          "F. Li et al.": ""
        },
        {
          "6": "ties, and amplify the differences between the same modalities but different emo-",
          "F. Li et al.": ""
        },
        {
          "6": "tions. We define this loss function as margin loss.",
          "F. Li et al.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: The results of different methods on the IEMOCAP database.",
      "data": [
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "theatre actors (five males and five females). The dataset is divided into five binary"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "sessions, and each conversation is annotated with emotional\ninformation in four"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "modalities: video, audio, transcription, and motion capture of facial movements."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "We evaluate our model using audio,\ntranscribed, and video data. The dataset"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "contains a total of 7380 data samples. E.g., happy, neutral, angry, excited, sad,"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "and frustrated. For evaluation, we employ a five-fold cross-validation approach."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "The first four sessions are utilized as the training set and the validation set, and"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "the last session is utilized as the testing set."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "The MELD dataset\nis derived from over 1,400 dialogues and 13,000 utter-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "ances extracted from the TV series Friends. Each utterance in the dataset\nis"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "annotated with one of seven emotion labels: neutral, surprise,\nfear, sadness,\njoy,"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "disgust, and anger. The dataset includes multimodal scenes, making it suitable"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "for studying multimodal emotion recognition tasks. For our experiments, we uti-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "lize the predefined training/validation splits provided with the MELD dataset."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "This ensures consistency with existing approaches and allows for a fair compar-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "ison with other models."
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "3.2\nSetting"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "For text and visual modalities, we freeze the parameters in the RoBERTa and"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "EfficientNet pre-trained models and treat them as a feature extractor. The last"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "dimension of the text and visual\nfeatures is 768 and 64. For speech modalities,"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "we unfreeze the parameters of\nthe deep transformer\nlayer\nin the wav2vec 2.0"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "pre-train model. These parameters are updated during model\ntraining, while"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "the parameters of the other layers are freezing. The last dimension of the speech"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n7": "features are 768 and 64."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: The results of different methods on the IEMOCAP database.",
      "data": [
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "ACC(%)"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "68.92"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "-"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "-"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "-"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "68.2"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "68.21"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "69.69"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "69.48"
        },
        {
          "Table 1. The results of different methods on the IEMOCAP database.": "70.53"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: The results of different methods on the MELD database.",
      "data": [
        {
          "F. Li et al.": ""
        },
        {
          "F. Li et al.": "Method"
        },
        {
          "F. Li et al.": "DialogueTRM [15]"
        },
        {
          "F. Li et al.": "MMGCN [16]"
        },
        {
          "F. Li et al.": "MM-DFN [17]"
        },
        {
          "F. Li et al.": "UniMSE [27]"
        },
        {
          "F. Li et al.": "HAAN-ERC [26]"
        },
        {
          "F. Li et al.": "Ours"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: The results of different methods on the MELD database.",
      "data": [
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "Ours\n66.93\n66.1\n2024"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "3.3\nComparative Analysis"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "In Tables 1 and 2, we\nshow the performance of different approaches on the"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "IEMOCAP and MELD datasets. The evaluation metrics are Accuracy (ACC)"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "and Weighted F1 score (WF1). On the IEMOCAP dataset, our method out-"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "performed the state-of-the-art by 0.84% in ACC and 0.74% in WF1. Similarly,"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "on the MELD dataset, our method surpassed the state-of-the-art by 0.43% in"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "ACC and 0.44% in WF1. The reasons are probably twofold. Firstly, we argue"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "that this is because most of these models do not explicitly consider redundant"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "information in the cross-modal fusion process, but our proposed method consid-"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "ers these through a gated cross-modal attention mechanism. Secondly, most of"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "them only take into account\nthe distances of different emotion samples of\nthe"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "same modality, but not the distances of the same emotion samples of different"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "modalities."
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "3.4\nAblation Studies"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "To verify the\neffectiveness of WavFusion model, we\nconduct ablation studies"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "on the\nIEMOCAP dataset. First, we\nreveal\nthe\nimportance of\neach modality"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "in this\nsection. Specifically, when utilizing a single modality, we omitted the"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "gated cross-modal attention and multimodal homogeneous feature discrepancy"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "learning. The results in Table 3 illustrate that the highest accuracy and weighted"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "average F1 scores are attained when incorporating all\nthree modalities. Due"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "to the complexity of emotion recognition,\nrecognizing emotions using a single"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "modality is challenging to meet the demands of reality. We can achieve better"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "recognition performance by integrating multimodal\ninformation."
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "Additionally, we introduce LVC blocks to capture local\ninformation related"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "to visual\nfeatures. To assess\nthe significance of LVC blocks, we conducted an"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "experiment where we omitted the LVC blocks\nfrom the model,\nthus\nfailing to"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "capture local\ninformation about visual\nfeatures. From Table 4, we observe that"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "the model with the LVC block outperforms the model without the LVC block."
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "The inclusion of LVC blocks improves ACC by 0.63% and the WF1 by 0.76%."
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "The experiment demonstrates that the LVC blocks are beneficial\nfor capturing"
        },
        {
          "HAAN-ERC [26]\n66.5\n65.66\n2023": "relevant contextual details and spatial dependencies."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Experiment results on the diffferent modalities.",
      "data": [
        {
          "Table 3. Experiment results on the diffferent modalities.": "ACC(%)"
        },
        {
          "Table 3. Experiment results on the diffferent modalities.": "66.06"
        },
        {
          "Table 3. Experiment results on the diffferent modalities.": "58.74"
        },
        {
          "Table 3. Experiment results on the diffferent modalities.": "29.88"
        },
        {
          "Table 3. Experiment results on the diffferent modalities.": "67.75"
        },
        {
          "Table 3. Experiment results on the diffferent modalities.": "66.33"
        },
        {
          "Table 3. Experiment results on the diffferent modalities.": "70.53"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Experiment results on the diffferent modalities.",
      "data": [
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "ancy learning in our framework. In this work, we assigned weights to the balance"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "factor λ for margin loss and observed its effects across various weight values. The"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "corresponding results are presented in Table 5. The results indicate that the op-"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "timal performance on the IEMOCAP dataset\nis achieved. The model\nshows a"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "significant improvement by 2.64% and WF1 by 2.94% compared to the absence"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "of margin loss (λ = 0). This demonstrates the effectiveness of multimodal homo-"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "geneous feature difference learning in enhancing the model’s capacity to discern"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "emotions across diverse modalities. However, we also observe that\nthe perfor-"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "mance deteriorates when the balance factor is excessively large (λ = 10). This"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "suggests\nthat an excessive emphasis on margin loss might have a detrimental"
        },
        {
          "We also investigate the impact of multimodal homogeneous feature discrep-": "effect on the original classification task."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: Experiment results on the diffferent λ.",
      "data": [
        {
          "Table 5. Experiment results on the diffferent λ.": "ACC(%)"
        },
        {
          "Table 5. Experiment results on the diffferent λ.": "67.89"
        },
        {
          "Table 5. Experiment results on the diffferent λ.": "68.63"
        },
        {
          "Table 5. Experiment results on the diffferent λ.": "69.11"
        },
        {
          "Table 5. Experiment results on the diffferent λ.": "70.53"
        },
        {
          "Table 5. Experiment results on the diffferent λ.": "64.43"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Experiment results on the diffferent λ.",
      "data": [
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": "Shallow transformer"
        },
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": "12"
        },
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": "11"
        },
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": "10"
        },
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": ""
        },
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": "9"
        },
        {
          "Table 6. Experiment results on the transformed layers in wav2vec 2.0.": "8"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Experiment results on the diffferent λ.",
      "data": [
        {
          "4\nConclusion": "In this paper, we propose a novel SER approach, which is designed a gated"
        },
        {
          "4\nConclusion": "cross-modal attention alternative to self-attention in the wav2vec 2.0 pre-trained"
        },
        {
          "4\nConclusion": "model\nto dynamically fuse features\nfrom different modalities. Additionally, we"
        },
        {
          "4\nConclusion": "introduce a novel LVC block to efficiently capture the local\ninformation of vi-"
        },
        {
          "4\nConclusion": "sual\nfeatures. The model can more effectively utilize the spatial characteristics"
        },
        {
          "4\nConclusion": "of visual data, resulting in more comprehensive representations. Finally, we de-"
        },
        {
          "4\nConclusion": "sign the concept of multimodal homogeneous feature discrepancy learning, which"
        },
        {
          "4\nConclusion": "helps the model to effectively learn and distinguish representations of the same"
        },
        {
          "4\nConclusion": "modalities but different\nemotions. The\neffectiveness of\nthe proposed model\nis"
        },
        {
          "4\nConclusion": "demonstrated on the IEMOCAP and MELD datasets. The results show promis-"
        },
        {
          "4\nConclusion": "ing performance compared to state-of-the-art methods. In the future, we plan to"
        },
        {
          "4\nConclusion": "utilize the leveraging large amounts of unlabeled audio and video data available"
        },
        {
          "4\nConclusion": "to recognize the different emotion."
        },
        {
          "4\nConclusion": "Acknowledgments. This work was supported in part by the Natural Science Foun-"
        },
        {
          "4\nConclusion": "dation of\nthe Higher Education Institutions\nof Anhui Province under Grant Nos."
        },
        {
          "4\nConclusion": "2024AH050018 and KJ2021A0486, Excellent Research and Innovation Team of Uni-"
        },
        {
          "4\nConclusion": "versities at Anhui Province under Grant Nos. 2024AH010001 and 2023AH010008, and"
        },
        {
          "4\nConclusion": "Science Research Fund of Anhui University of Finance and Economics under Grant"
        },
        {
          "4\nConclusion": "No. ACKYB23016."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "References"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "1. Ayadi, M.E., Kamel, M.S., and Karray., F.: Survey on speech emotion recognition:"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Features, classification schemes, and databases. Pattern recognition, 44(3):572–587"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "(2011)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "2. Li, X., Lin, R.:\nSpeech emotion recognition for power customer service.\nIn 2021"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "7th International Conference on Computer and Communications\n(ICCC), pages"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "514–518 (2021)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "3. Li, W., Zhang, Y., Fu, Y.: Speech emotion recognition in e-learning system based"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "on affective computing.\nIn Third international conference on natural computation"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "(ICNC 2007), volume 5, pages 809–813 (2007)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "4. Elsayed, E., ElSayed, Z, Asadizanjani, N., et al.: Speech emotion recognition using"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "supervised deep recurrent system for mental health monitoring.\nIn 2022 IEEE 8th"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "World Forum on Internet of Things (WF-IoT), pages 1–6 (2022)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "5. Ahire, V., Borse, S.: Emotion detection from social media using machine learning"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "techniques: a survey.\nIn Applied Information Processing Systems: Proceedings of"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "ICCET 2021, pages 83–92. Springer (2022)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "6. Calefato, F., Lanubile, F., Novielli, N.: Emotxt: a toolkit for emotion recognition"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "from text.\nIn 2017 seventh international conference on Affective Computing and"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Intelligent Interaction Workshops and Demos (ACIIW), pages 79–80 (2017)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "7. You, Q., Luo, J., Jin, H., et al.:\nBuilding a large scale dataset\nfor\nimage emo-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "tion recognition: The fine print and the benchmark.\nIn Proceedings of the AAAI"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "conference on artificial\nintelligence, volume 30 (2016)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "8. Abdullah, S.S, Ameen, S.A., Sadeeq, A.: and Subhi Zeebaree. Multimodal emotion"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "recognition using deep learning. Journal of Applied Science and Technology Trends,"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "2(02):52–58 (2021)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "9. Wu, W., Zhang, C, Woodland, p.: Emotion recognition by fusing time synchronous"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "and time asynchronous representations.\nIn ICASSP 2021-2021 IEEE International"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6269–"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "6273 (2021)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "10. Grant, K.W., Greenberg, S.: Speech intelligibility derived from asynchronous pro-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "cessing of auditory-visual\ninformation.\nIn AVSP 2001-International Conference on"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Auditory-Visual Speech Processing (2001)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "11. Tsai, Y.H.H., Bai, S.J., Liang, P.P., et al.: Multimodal transformer for unaligned"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "multimodal\nlanguage sequences.\nIn Proceedings of the conference. Association for"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Computational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "(2019)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "12. Zheng, J., Zhang, S., Wang, Z., et al.: Multi-channel weight-sharing autoencoder"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "based on cascade multi-head attention for multimodal emotion recognition.\nIEEE"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Transactions on Multimedia (2022)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "13. Chen, B., Cao, Q., Hou, M., et al.: Multimodal emotion recognition with tem-"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "poral and semantic consistency.\nIEEE/ACM Transactions on Audio, Speech, and"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "Language Processing, 29:3592–3603 (2021)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "14. Hazarika, D., Zimmermann, R., Poria, S.: Misa: Modality-invariant and-specific"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "representations\nfor multimodal\nsentiment analysis.\nIn Proceedings of\nthe 28th"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "ACM international conference on multimedia, pages 1122–1131 (2020)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "15. Mao, Y., Sun, Q., Liu, G., et al.: Dialoguetrm: Exploring the intra-and inter-modal"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "emotional behaviors in the conversation. arXiv preprint arXiv:2010.07637 (2020)"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "16. Hu, L., Liu, Y., Zhao,\nJ.,\net\nal.:\nMmgcn: Multimodal\nfusion via deep graph"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "convolution\nnetwork\nfor\nemotion\nrecognition\nin\nconversation.\narXiv\npreprint"
        },
        {
          "WavFusion: Towards wav2vec 2.0 Multimodal Speech Emotion Recognition\n11": "arXiv:2107.06779 (2021)"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12\nF. Li et al.": "17. Hu, D., Hou, X., Wei, L., et al.: Mm-dfn: Multimodal dynamic fusion network for"
        },
        {
          "12\nF. Li et al.": "emotion recognition in conversations.\nIn ICASSP 2022-2022 IEEE International"
        },
        {
          "12\nF. Li et al.": "Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7037–"
        },
        {
          "12\nF. Li et al.": "7041 (2022)"
        },
        {
          "12\nF. Li et al.": "18. Chudasama, V., Kar, P., Gudmalwar, A., et al.: M2fnet: Multi-modal\nfusion net-"
        },
        {
          "12\nF. Li et al.": "work for emotion recognition in conversation.\nIn Proceedings of\nthe IEEE/CVF"
        },
        {
          "12\nF. Li et al.": "Conference on Computer Vision and Pattern Recognition, pages 4652–4661 (2022)"
        },
        {
          "12\nF. Li et al.": "19. Baevski, A., Zhou, Y., Mohamed, A., et al.: wav2vec 2.0: A framework for\nself-"
        },
        {
          "12\nF. Li et al.": "supervised learning of\nspeech representations.\nAdvances\nin neural\ninformation"
        },
        {
          "12\nF. Li et al.": "processing systems, 33:12449–12460 (2020)"
        },
        {
          "12\nF. Li et al.": "20. Quan, Y, Zhang, D, Zhang, L.,\net al.:.\nCentralized feature pyramid for object"
        },
        {
          "12\nF. Li et al.": "detection.\nIEEE Transactions on Image Processing 2023."
        },
        {
          "12\nF. Li et al.": "21. Busso, C., Bulut, M., Le, C., et al.:\nIemocap: Interactive emotional dyadic motion"
        },
        {
          "12\nF. Li et al.": "capture database. Language resources and evaluation, 42:335–359 (2008)"
        },
        {
          "12\nF. Li et al.": "22. Poria, S., Hazarika D., Majumder, N.,\net al.: Meld: A multimodal multi-party"
        },
        {
          "12\nF. Li et al.": "dataset for emotion recognition in conversations. arXiv preprint arXiv:1810.02508"
        },
        {
          "12\nF. Li et al.": "(2018)"
        },
        {
          "12\nF. Li et al.": "23. Li, J., Ji, D., Li, F., et al.:\nHitrans: A transformer-based context-and speaker-"
        },
        {
          "12\nF. Li et al.": "sensitive model\nfor emotion detection in conversations.\nIn Proceedings of the 28th"
        },
        {
          "12\nF. Li et al.": "International Conference on Computational Linguistics, pages 4190–4200 (2020)"
        },
        {
          "12\nF. Li et al.": "24. Shen, W., Chen, J., Quan, X., et al.: Dialogxl: All-in-one xlnet\nfor multi-party"
        },
        {
          "12\nF. Li et al.": "conversation emotion recognition.\nIn Proceedings of\nthe AAAI Conference on"
        },
        {
          "12\nF. Li et al.": "Artificial Intelligence, volume 35, pages 13789–13797 (2021)"
        },
        {
          "12\nF. Li et al.": "25.\nJoshi, A., Bhat, A., Jain, A., et al.: Cogmen: Contextualized gnn based multimodal"
        },
        {
          "12\nF. Li et al.": "emotion recognition. arXiv preprint arXiv:2205.02455 (2022)"
        },
        {
          "12\nF. Li et al.": "26. Zhang, T., Tan, Z., Wu, X.: Haan-erc: hierarchical adaptive attention network for"
        },
        {
          "12\nF. Li et al.": "multimodal emotion recognition in conversation. Neural Computing and Applica-"
        },
        {
          "12\nF. Li et al.": "tions, pages 1–14 (2023)"
        },
        {
          "12\nF. Li et al.": "27. Hu, G., Lin, T., Zhao, Y., et al.: Unimse: Towards unified multimodal sentiment"
        },
        {
          "12\nF. Li et al.": "analysis and emotion recognition. arXiv preprint arXiv:2211.11256 (2022)"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition for power customer service",
      "authors": [
        "X Li",
        "R Lin"
      ],
      "year": "2021",
      "venue": "2021 7th International Conference on Computer and Communications (ICCC)"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition in e-learning system based on affective computing",
      "authors": [
        "W Li",
        "Y Zhang",
        "Y Fu"
      ],
      "year": "2007",
      "venue": "Third international conference on natural computation (ICNC 2007)"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using supervised deep recurrent system for mental health monitoring",
      "authors": [
        "E Elsayed",
        "Z Elsayed",
        "N Asadizanjani"
      ],
      "year": "2022",
      "venue": "IEEE 8th World Forum on Internet of Things (WF-IoT)"
    },
    {
      "citation_id": "5",
      "title": "Emotion detection from social media using machine learning techniques: a survey",
      "authors": [
        "V Ahire",
        "S Borse"
      ],
      "year": "2022",
      "venue": "Applied Information Processing Systems: Proceedings of ICCET 2021"
    },
    {
      "citation_id": "6",
      "title": "Emotxt: a toolkit for emotion recognition from text",
      "authors": [
        "F Calefato",
        "F Lanubile",
        "N Novielli"
      ],
      "year": "2017",
      "venue": "2017 seventh international conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "7",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "8",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "S Abdullah",
        "S Ameen",
        "A Sadeeq",
        "Subhi Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "Woodland"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Speech intelligibility derived from asynchronous processing of auditory-visual information",
      "authors": [
        "K Grant",
        "S Greenberg"
      ],
      "year": "2001",
      "venue": "AVSP 2001-International Conference on Auditory-Visual Speech Processing"
    },
    {
      "citation_id": "11",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "12",
      "title": "Multi-channel weight-sharing autoencoder based on cascade multi-head attention for multimodal emotion recognition",
      "authors": [
        "J Zheng",
        "S Zhang",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Multimodal emotion recognition with temporal and semantic consistency",
      "authors": [
        "B Chen",
        "Q Cao",
        "M Hou"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "15",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Y Mao",
        "Q Sun",
        "G Liu"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "arxiv": "arXiv:2010.07637"
    },
    {
      "citation_id": "16",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "L Hu",
        "Y Liu",
        "J Zhao"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "17",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Centralized feature pyramid for object detection",
      "authors": [
        "Y Quan",
        "D Zhang",
        "L Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Le"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "23",
      "title": "Hitrans: A transformer-based context-and speakersensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain"
      ],
      "year": "2022",
      "venue": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "arxiv": "arXiv:2205.02455"
    },
    {
      "citation_id": "26",
      "title": "Haan-erc: hierarchical adaptive attention network for multimodal emotion recognition in conversation",
      "authors": [
        "T Zhang",
        "Z Tan",
        "X Wu"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "27",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T Lin",
        "Y Zhao"
      ],
      "year": "2022",
      "venue": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    }
  ]
}