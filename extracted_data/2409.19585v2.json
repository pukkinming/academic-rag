{
  "paper_id": "2409.19585v2",
  "title": "Two-Stage Framework For Robust Speech Emotion Recognition Using Target Speaker Extraction In Human Speech Noise Conditions",
  "published": "2024-09-29T07:04:50Z",
  "authors": [
    "Jinyi Mi",
    "Xiaohan Shi",
    "Ding Ma",
    "Jiajun He",
    "Takuya Fujimura",
    "Tomoki Toda"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Developing a robust speech emotion recognition (SER) system in noisy conditions faces challenges posed by different noise properties. Most previous studies have not considered the impact of human speech noise, thus limiting the application scope of SER. In this paper, we propose a novel twostage framework for the problem by cascading target speaker extraction (TSE) method and SER. We first train a TSE model to extract the speech of target speaker from a mixture. Then, in the second stage, we utilize the extracted speech for SER training. Additionally, we explore a joint training of TSE and SER models in the second stage. Our developed system achieves a 14.33% improvement in unweighted accuracy (UA) compared to a baseline without using TSE method, demonstrating the effectiveness of our framework in mitigating the impact of human speech noise. Moreover, we conduct experiments considering speaker gender, showing that our framework performs particularly well in different-gender mixture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech is a significant part of human communication. Besides linguistic information, it contains unique paralinguistic information such as gender, emotion, and age, which is essential to the normal communication. In certain instances, misunderstanding paralinguistic features would distort the correct information conveyed by speech, leading to an ineffective communication. Therefore, it is necessary to develop human-like communication machines that can comprehend paralinguistic data.\n\nSpeech emotion recognition (SER), as a branch of affective computing, has garnered growing attention over the past two decades because of its contribution to human-computer interactions  [1] ,  [2] . Generally, the mechanism of SER involves extracting and classifying effective emotional features from audio signals so that various emotions of a speaker can be captured, thanks to which SER has been applied in healthcare  [3] ,  [4] , driver safety  [5] ,  [6] , call center  [7] ,  [8] , and online education  [9] ,  [10] . At present, research on SER systems on scenarios devoid of background noises, often referred to as clean scenarios, has shown good performance  [11] -  [13] . However, in real-world environments, the performance of SER significantly degrades, mainly due to the presence of various noises from different sources. These unknown noises severely affect the performance of SER systems, which poses major challenges for the widespread application of SER systems.\n\nSeveral studies have focused on SER tasks in the environment affected by specific noise sources, including communication systems  [14] ,  [15] , transportation  [15] ,  [16] , and industrial activities  [15] . In  [14] , Huang et al. studied SER from speech signals with additive white Gaussian noise, they proposed two speech enhancement methods based on spectral subtraction and masking properties, respectively. In  [16] , Chenchah et al. used power-normalized cepstral coefficients as acoustic features for improving the robustness of SER systems in noisy environment from cars and trains. In  [15] , Liu et al. proposed a multi-level knowledge distillation framework, which significantly reduced the affects of noises from channel, car, and factory.\n\nHowever, the aforementioned studies mainly concentrate on addressing the noise sources associated with non-human activities, leaving a gap in addressing prevalent sources of noise in human-centric environments. While Shi et al.  [17]  did adopt ASR representations to filter out a specific category of noise related to human activities, which is typically from human physical actions like knocking on doors, a more common category of noise stemming from human speech itself remains underexplored. This type of noise called human speech noise, which is common in activities involving human interactions such as social gatherings, often becomes entangled with target speech data, forming a complex acoustic environment. Therefore, human speech noise becomes more unpredictable and more challenging to address.\n\nOn the other hand, humans have an extraordinary ability to selectively concentrate on a single speaker among a complex acoustic environment, commonly called cocktail party effect  [18] . To replicate this specialized listening ability in machines, target speaker extraction (TSE) technique has been developed. This technique exploits an auxiliary information of the target speaker and extracts speech of that speaker from the mixture. Švec et al.  [19]  explored the potential of TSE for extracting target emotional speech. In light of this, we propose a novel two-stage framework by cascading TSE method and SER to mitigate the impact of human speech noise. In the experiments, we utilize the TSE-SER framework to ShiftCNN  [12]  that is a state-of-the-art SER model, and compare its performance to verify the effectiveness of our framework. Furthermore, we investigate different factors on the performance of SER, includ-arXiv:2409.19585v2 [cs.SD] 17 Dec 2024\n\ning training methods and speaker genders. Our contributions to this work are as follows:\n\n• We apply the TSE method on SER tasks to address the practical scenario in which target speech is interfered by human speech noises. To our knowledge, this study is the pioneering effort in exploring the integration of TSE with SER.\n\n• We propose a two-stage framework using different training methods. According to comparative experiments against the baselines without using TSE technique, our framework significantly improves the accuracy of SER in human speech noise conditions. • We investigate the impact of human speech noise on our framework, especially on same-gender mixture and different-gender mixture. Results indicate that our framework performs better in different-gender mixture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Framework Overview",
      "text": "We assume that an observed time-domain mixture signal in human speech noise conditions is defined as\n\nwhere I is the number of speakers in the mixture, s i for i = 0, ..., I -1 is the speech signal of the ith speaker. In particular, i = 0 indicates the speech signal of the target speaker. Directly using the mixture signal y corrupted by overlapping speakers for SER task, would cause the poor SER performance, as the SER model cannot selectively focus on a single speaker. Therefore, the goal of the proposed framework is to extract the target speaker signal s 0 from the mixture signal y for SER training. As illustrated in Figure  1 , the framework contains two stages: First, the TSE model is trained using a large-scale mixed-speech corpus. This ensures a well-trained TSE model that can extract high-quality speech of the target speaker given the input speech and the enrollment utterance of that speaker. Note that we use a neutral speech utterance for enrollment, which is a more convenient and realistic setting than using corresponding emotional speech. Then, we apply the TSE model as the form of data augmentation to extract the exclusive speech information of the target speaker from a mixed emotional speech corpus. This denoised corpus is used for both training and testing in SER tasks. We denote this training method as TSE-SER-base. In addition, we further propose another training method called TSE-SER-ft shown in Figure  2 . We start from the same TSE pretraining as the first stage. In the second stage, we introduce mixed emotional speech as input, simultaneously fine-tuning the pretrained TSE model and training the SER model. This joint training process not only refines the TSE system by adjusting its parameters but also benefits the SER training.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Target Speaker Extraction Model",
      "text": "TSE refers to the task of reconstructing the speech signal of the target speaker from the mixture given auxiliary information of that speaker. This process can be formulated as\n\nwhere ŝ0 is the estimated speech of the target speaker, a 0 is the enrollment utterance of the target speaker, g represents the transformation carried out by the TSE system. In this work, we adopt time-domain SpeakerBeam (TD-SpeakerBeam)  [20] ,  [21]  as the TSE model. TD-SpeakerBeam consists of an auxiliary network and a speech extraction network, represented by h and f , respectively. The auxiliary network h accepts the enrollment utterance a 0 and computes an embedding vector, denoted by E 0 , to represent the acoustic characteristics of the target speaker, i.e.,\n\nThen, the speech extraction network f accepts the mixture signal y and the embedding vector E 0 of the target speaker as inputs to predict the speech signal of the target speaker, i.e.,\n\nwhere f comprises an encoder E, a mask estimator B, and a decoder D. This process is formulated as\n\nwhere ⊙ denotes element-wise multiplication. The mixture signal y is fed into the encoder E that is represented by a 1D convolution layer. Then, the mask estimator B maps the output Y of the encoder E to a mask M 0 for the target speaker, utilizing multiple convolution blocks. In particular, a multiplicative adaptation layer, accepting the embedding vector E 0 of the target speaker as auxiliary information, is inserted between the first and second blocks to drive the network towards extracting the target speaker. Finally, the mask M 0 and the encoder features Y are fed into a 1D deconvolution layer-based decoder D, to output the time-domain signal of the target speaker.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Loss Functions",
      "text": "In the first stage, TSE-SER-base and TSE-SER-ft use scaleinvariant source-to-noise ratio (SiSNR)  [22]  as the loss for TSE training. In the second stage, TSE-SER-base uses cross entropy (CE) loss for SER training, whereas TSE-SER-ft jointly trains the pretrained TSE model and SER model, considering both SiSNR and CE losses. The second stage losses of TSE-SERbase (L base ) and TSE-SER-ft (L f t ) are represented as",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experimental Evaluations",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Datasets",
      "text": "In this work, we designed two kinds of datasets for comparable experiments: 1) the clean emotional dataset, and 2) the emotional dataset mixed with human speech noise. For the latter, two different human speech datasets were used as noise. All the mentioned datasets were sampled at 16 kHz. IEMOCAP: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus  [23] , consisting of approximately 12 hours of recordings, includes five dyadic sessions, each with one English male speaker and one English female speaker. For our experiments, we used IEMOCAP as the clean emotional dataset, where we considered only four emotional categories of happy, angry, sad, and neutral. Note that \"excited\" was merged with \"happy\" to ensure category balance  [13] ,  [24] -  [27] . LibriSpeech: The LibriSpeech corpus  [28]  contains about 1000 hours of read English speech. For our experiments, 105 hours of this corpus were chosen as a source of human speech noise. ESD: The Emotional Speech Database (ESD) corpus  [29]  comprises about 29 hours of recordings from 10 English speakers and 10 Chinese speakers. For our experiments, we considered only the English part as another source of human speech noise.\n\nIn order to more accurately evaluate the performance of the proposed framework, we adopted leave-one-session-out 5-fold cross-validation to test all the models. Note that we used the following terms to represent different datasets designed: Clean means a clean set, Noisy means a dataset mixed with human speech noise, and Denoised indicates a dataset denoised from Noisy by the pretrained TSE model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Experimental Procedure",
      "text": "The first experiment investigated the impact of human speech noise on SER (see Section III-D1). The noisy dataset was generated by randomly selecting utterances from different The second and third experiments explored the effect of TSE method on SER and the proposed training methods on our framework (see Sections III-D2 and III-D3). We used the same dataset as the first experiment for the second stage of TSE-SER-base and TSE-SER-ft. We adopted LibriMix  [30]  where 100 hours of LibriSpeech were additionally used to generate mixtures for TSE pretraining. Furthermore, for the target speaker in the mixture, we randomly chose one neutral utterance of this speaker that does not belong to the mixture as the enrollment utterance.\n\nThe fourth experiment explored the impact of gender states of mixtures on SER (see Section III-D4). We used the same clean speech from the first experiment and used ESD as noise. We generated two types of mixtures: same-gender mixture and different-gender mixture, where the first type was stipulated that two speakers have the same genders, while the latter type required two speakers of opposite genders.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Implementation And Metrics",
      "text": "To build TSE model, we followed an open-source Speaker-Beam implementation  1  . For SER model, we used ShiftCNN  [12] , which has shown advanced performance in clean environments, adopting the same hyperparameters as  [12] . All implemented models in experiments are shown in Table  I .\n\nFor evaluation metrics of SER, we used the unweighted accuracy (UA) and weighted accuracy (WA). UA was the mean of the accuracies for each individual class while WA represented the ratio of correctly predicted samples to the total number of samples. In addition, we used scale-invariant signal-to-distortion ratio (SI-SDR) and scale-invariant signalto-distortion ratio improvement (SI-SDRi) to evaluate the performance of TSE model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Experimental Results",
      "text": "To conduct a comparative study of all the experiments, aside from the proposed TSE-SER-base and TSE-SER-ft, we built two typical SER baselines using ShiftCNN, which were directly trained on Clean and Noisy, referred to as clean SER model and noisy SER model, respectively.  1) The impact of human speech noise on SER: To clarify the impact of human speech noise on SER, we compare the clean SER model and the noisy SER model. As shown in Table  II , the clean SER model obtains an accuracy of over 70% for both UA and WA in the clean test set. But their performance drops significantly on the noisy test data, with a maximum decrease of up to 23.09% and 25.62% in terms of WA and UA. These results demonstrate that the clean SER model is fragile against human speech noise. Furthermore, when the SER model uses noisy data for training, the relatively better performance can be observed. Nonetheless, the results are still significantly worse than those of the clean SER model on the clean test set, suggesting deficient robustness. We argue that human speech noise severely hinders the SER model from establishing an effective mapping to the target emotional speech with the direct training.\n\n2) The effect of TSE method on SER: To verify the effectiveness of the proposed framework on SER, we compare the performance of the SER model using the TSE method with those of the SER baselines. As presented in Table  III , we first observe that the results of the clean SER model on denoised test set are not ideal, demonstrating that the clean model is unable to adapt to the denoised speech with distorted properties. We hence design the corresponding system trained on denoised data using TSE-SER-base, referred to as SB SC in Table  III . Our proposed system SB SC performs significantly better than the other systems. Especially, the UA reaches 63.42%, which is a 9.48% increase compared to the noisy SER model in noisy conditions. Meanwhile, for the unbalanced training data from IEMOCAP corpus, the WA result is also competitive, closely resembling the UA results. The overall results demonstrate that the proposed framework with TSE-SER-base adapts well to human speech noise, showcasing effectiveness and robustness.  3) The effect of training methods on TSE-SER framework: We compare the performance of TSE-SER-base and TSE-SER-ft. As indicated in Table  IV , TSE-SER-ft significantly outperforms TSE-SER-base. Moreover, the UA of TSE-SERft reaches a 14.33% improvement compared with the noisy SER model presented in Table  III . The SI-SDR of the noisy speech before being processed by the TSE model is 0.09 dB. We calculate the SI-SDRi for the corresponding TSE models of TSE-SER-base and TSE-SER-ft to be 7.68 dB and 12.90 dB, respectively, verifying that TSE-SER-ft can improve TSE performance. Therefore, the TSE fine-tuning, applied to the noisy dataset containing the task-specific emotional data, enables the TSE model to extract purer emotional-related acoustic features to benefit the SER training.\n\n4) The impact of gender states of mixtures on SER: Table  V  shows the results of TSE-SER-base, the SER baselines on same-and different-gender mixtures. First, the clean SER models unsurprisingly gain the lowest performance. Moreover, the noisy SER model shows a non-obvious trend on the noisy test set across both gender states, whereas the clean SER model in same-gender mixture clearly outperforms that in different-gender mixture, demonstrating better adaptability of SER models to same-gender mixture. In addition, besides the results of TSE-SER-base being all better than those of other systems for both same-and different-gender mixtures, we can see a significant performance gap of TSE-SER-base for dealing with same-and different-gender mixtures, which is opposite to the finding for the clean SER model in noisy conditions. Since our framework uses the TSE model, we conjecture that same- We further conduct an ablation study for TSE-SER-base using two SB SC systems from Table V on test sets denoised from the same-and different-gender mixtures. We have an interesting finding in Table VI that using training data denoised from different-gender mixture can enhance our model performance on both test sets denoised from same-and differentgender mixtures. We argue that the pretrained TSE model can extract higher-quality denoised data from different-gender mixture, thus finalizing a higher-performance SER model.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this work, we presented a two-stage framework to mitigate the impact of human speech noise on SER. Based on the framework, we designed two training methods, TSE-SERbase and TSE-SER-ft. The effectiveness and robustness of both methods have been verified. Moreover, we investigated the impact of human speech noise on SER, especially on sameand different-gender mixtures. In the future, we plan to explore how our proposed framework performs on human speech noise with different attributes, such as emotion classes and languages. Another possible direction involves multi-interference environments, such as noisy and reverberant environments.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the framework contains two",
      "page": 2
    },
    {
      "caption": "Figure 2: We start from the same TSE pretraining as the",
      "page": 2
    },
    {
      "caption": "Figure 1: Two-stage framework with TSE-SER-base.",
      "page": 2
    },
    {
      "caption": "Figure 2: Two-stage framework with TSE-SER-ft.",
      "page": 2
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "Dimensional emotion prediction based on interactive context in conversation",
      "authors": [
        "X Shi",
        "S Li",
        "J Dang"
      ],
      "year": "2020",
      "venue": "Dimensional emotion prediction based on interactive context in conversation"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition using speech and neural structured learning to facilitate edge intelligence",
      "authors": [
        "M Uddin",
        "E Nilsson"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Cloud-assisted speech and face recognition framework for health monitoring",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2015",
      "venue": "Mobile Networks and Applications"
    },
    {
      "citation_id": "5",
      "title": "On the necessity and feasibility of detecting a driver's emotional state while driving",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "H Harris"
      ],
      "year": "2007",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "6",
      "title": "Speech based emotion classification framework for driver assistance system",
      "authors": [
        "A Tawari",
        "M Trivedi"
      ],
      "year": "2010",
      "venue": "2010 IEEE Intelligent Vehicles Symposium"
    },
    {
      "citation_id": "7",
      "title": "Emotion in speech: Recognition and application to call centers",
      "authors": [
        "V Petrushin"
      ],
      "year": "1999",
      "venue": "Proceedings of artificial neural networks in engineering"
    },
    {
      "citation_id": "8",
      "title": "Ensemble methods for spoken emotion recognition in call-centres",
      "authors": [
        "D Morrison",
        "R Wang",
        "L Silva"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition in e-learning system based on affective computing",
      "authors": [
        "W Li",
        "Y Zhang",
        "Y Fu"
      ],
      "year": "2007",
      "venue": "Third international conference on natural computation (ICNC 2007)"
    },
    {
      "citation_id": "10",
      "title": "Emotional recognition from the speech signal for a virtual education agent",
      "authors": [
        "A Tickle",
        "S Raghu",
        "M Elshaw"
      ],
      "year": "2013",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "11",
      "title": "A discriminative feature representation method based on cascaded attention network with adversarial strategy for speech emotion recognition",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Mingling or misalignment? temporal shift for speech emotion recognition with pre-trained representations",
      "authors": [
        "S Shen",
        "F Liu",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition under white noise",
      "authors": [
        "C Huang",
        "C Guoming",
        "Y Hua",
        "B Yongqiang",
        "Z Li"
      ],
      "year": "2013",
      "venue": "Archives of Acoustics"
    },
    {
      "citation_id": "15",
      "title": "Multi-level knowledge distillation for speech emotion recognition in noisy conditions",
      "authors": [
        "Y Liu",
        "H Sun",
        "G Chen"
      ],
      "year": "2023",
      "venue": "Multi-level knowledge distillation for speech emotion recognition in noisy conditions",
      "arxiv": "arXiv:2312.13556"
    },
    {
      "citation_id": "16",
      "title": "A bio-inspired emotion recognition system under real-life conditions",
      "authors": [
        "F Chenchah",
        "Z Lachiri"
      ],
      "year": "2017",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "17",
      "title": "On the effectiveness of asr representations in real-world noisy speech emotion recognition",
      "authors": [
        "X Shi",
        "J He",
        "X Li",
        "T Toda"
      ],
      "year": "2023",
      "venue": "On the effectiveness of asr representations in real-world noisy speech emotion recognition",
      "arxiv": "arXiv:2311.07093"
    },
    {
      "citation_id": "18",
      "title": "Listening: An introduction to the perception of auditory events",
      "authors": [
        "S Handel"
      ],
      "year": "1993",
      "venue": "Listening: An introduction to the perception of auditory events"
    },
    {
      "citation_id": "19",
      "title": "Analysis of impact of emotions on target speech extraction and speech separation",
      "authors": [
        "J Švec",
        "K Žmolíková",
        "M Kocour"
      ],
      "year": "2022",
      "venue": "2022 International Workshop on Acoustic Signal Enhancement (IWAENC)"
    },
    {
      "citation_id": "20",
      "title": "Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures",
      "authors": [
        "K Žmolíková",
        "M Delcroix",
        "K Kinoshita"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Improving speaker discrimination of target speech extraction with time-domain speakerbeam",
      "authors": [
        "M Delcroix",
        "T Ochiai",
        "K Zmolikova"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Sdr-half-baked or well done?",
      "authors": [
        "J Le Roux",
        "S Wisdom",
        "H Erdogan",
        "J Hershey"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
      "authors": [
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Multimodal fusion of music theory-inspired and self-supervised representations for improved emotion recognition",
      "authors": [
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2024",
      "venue": "Multimodal fusion of music theory-inspired and self-supervised representations for improved emotion recognition"
    },
    {
      "citation_id": "28",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "29",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "30",
      "title": "Librimix: An open-source dataset for generalizable speech separation",
      "authors": [
        "J Cosentino",
        "M Pariente",
        "S Cornell",
        "A Deleforge",
        "E Vincent"
      ],
      "year": "2020",
      "venue": "Librimix: An open-source dataset for generalizable speech separation",
      "arxiv": "arXiv:2005.11262"
    }
  ]
}