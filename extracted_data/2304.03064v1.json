{
  "paper_id": "2304.03064v1",
  "title": "An Experimental Study In Real-Time Facial Emotion Recognition On New 3Rl Dataset Research Article",
  "published": "2023-04-06T13:29:11Z",
  "authors": [
    "Rahmeh Abou Zafra",
    "Lana Ahmad Abdullah",
    "Rouaa Alaraj",
    "Rasha Albezreh",
    "Tarek Barhoum",
    "Khloud Al Jallad"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Dataset",
    "Deep Learning",
    "Computer Vision"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although real-time facial emotion recognition is a hot topic research domain in the field of human-computer interaction, state-ofthe-art available datasets still suffer from various problems, such as some unrelated photos such as document photos, unbalanced numbers of photos in each class, and misleading images that can negatively affect correct classification. The 3RL dataset was created, which contains approximately 24K images and will be publicly available, to overcome previously available dataset problems. The 3RL dataset is labelled with five basic emotions: happiness, fear, sadness, disgust, and anger. Moreover, we compared the 3RL dataset with other famous state-of-the-art datasets (FER dataset, CK+ dataset), and we applied the most commonly used algorithms in previous works, SVM and CNN. The results show a noticeable improvement in generalization on the 3RL dataset. Experiments have shown an accuracy of up to 91.4% on 3RL dataset using CNN where results on FER2013, CK+ are, respectively (approximately from 60% to 85%).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotion recognition is a technology that analyses facial expressions from both static images and videos to reveal information on one's emotional state. The shape of the eyebrows, lips, nose, and chin plays an important role in determining facial expressions. This paper proposed a new dataset to overcome these problems. Moreover, two models are applied to compare our dataset with previously available datasets.\n\nThe first model is SVM using landmark and HOG (histogram of oriented gradients) feature descriptors extracted from images. The second model is CNN using images and landmarks as features. The paper is organized as follows: section 1 is an introduction, and section 2 is related works. Section 3 is about previously available datasets. Experiments are shown in section 4 and then setup, and finally section 6 contains conclusion and future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "SVM is one of the most commonly used machine learning models in FER (Facial Expression Recognition), as in (Alshamsi and Këpuska)  1  . Alshamsi et al. used SVM for classification, COG and landmarks for feature extraction. In (Patwardhan) 2 , (Youssef, Aly and Ibrahim)  3  , and (Zhang , Cui and Liu )  4  , geometric, kinematic and extracted features from daily behavioral patterns were used in feature extraction, whereas the SVM algorithm was used to recognize emotions and constructed a facial emotion recognition dataset that contains 84 samples. SVM and k-NN (k Nearest Neighbors) are used to classify emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Svms Have Proven Their Qualification Of Multiclass Classification.",
      "text": "As in (Alshamsi and Këpuska) 1 , (Joseph and P. Geetha)  5  , (Lucey, Cohn and Kanade)  6  , (Duan and Keerthi)  7  , (Chandran and Dr. Naveen S)  8  , and (GLAUNER) 9 to classify emotions. SVMs are usually implemented by combining several two-class SVMs. The hyperplane SVMs used in n-dimensional space distinguish points to correct classes by their labels. The optimal hyperplane is the one that is able to form the largest distance between dataset points in this case images. SVMs avoid the \"curse of dimensionality\" that arises in high-dimensional spaces by tuning the regularization parameter c in linear problems or by kernel trick along with tuning the kernel parameters in nonlinear problems.\n\nThe second approach is the adoption of deep learning algorithms, such as CNN, which is the most commonly used deep learning",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Issn: 2836-8495",
      "text": "model used for emotion recognition.\n\nHafiz et al in (Ahamed, Alam and Islam)  10  proposed CNN and HOG as feature extraction in (Ahamed, Alam and Islam) 10 , However, in (Li and Deng)  11  , Li & Deng used handcrafted HOG features and deep learned features and linear SVM for classification. In (GLAUNER)  9  , Patrick also used CNN, and handcrafted features were used for feature extraction. to determine the appropriate feeling for the input image and by reading many articles and studies, finding that the best algorithms used and the most accurate in machine learning to describe the feeling is the algorithm of the SVM and that the best deep learning algorithm in this field is the CNN algorithm, so these two algorithms are used mainly in this study and made some structural improvements and adjustments for both networks to get the best results, and then we did a simple comparison to determine the differences between using deep learning algorithms and machine learning to determine the feeling from the human face.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Network",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datsets",
      "text": "The 3RL dataset consists of 24394 images collected and edited from three famous datasets.\n\nAs Table  1  shows, the results on CK+ (Extended Cohn-Kanade Dataset) are better than the results on other datasets in terms of accuracy, but it is the smallest dataset between them. \"FER-2013\", \"CK+\" and a dataset for facial expression recognition that contains more images to work on, mixing them will lead to a sufficient number of images. However, many misleading images were found in the mentioned datasets, so these images were manually checked one by one, and some deleting, replicating and replacing processes were performed while checking them. A large number of misleading images were noticed that may immediately and negatively affect the output of the applied network. This filtering process has a noticeable improvement on model performance, and balancing between classes was applied to obtain fair results. The resulting dataset (3RL) will be available at https://github.com/ Rahma-AZ/3RL-images-dataset-for-Emotion-Recognition-The three datasets used to create the 3RL dataset are 1-FER-2013 (Facial Expression Recogntion,(FERc), 2013) 16 2-CK+48 (Cohn-Kanade: (CK+), 2010)  17  3-A dataset for facial expression recognition (Facial expressions)  18  .\n\nTables 2, 3, and 6 show statistics about previously available datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Facial Expression Recognition Dataset",
      "text": "This dataset contains 13718 images collected without classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Creation Steps 1)",
      "text": "To obtain better results, including the largest number of facial situations, merging datasets may be a good practice. After merging the CK+ 48 dataset, which contains 981 images, with the Fer2013 dataset, which contains 35887 images, the final merged dataset was 36858 images. The dataset was edited manually by removing all misclassified images and some images that contain hands that cover the face completely. The edited merged dataset is shown in table  4 .\n\n2) Next, for dataset (CK+ & FER-2013), after deleting all replicated images or images that contain confusion and splitting data, the dataset was as in table  5 .\n\n3) A third dataset was added (dataset for facial expression) with the previous datasets (CK+ & FER (2013). This dataset contains 13718 images with (350*350) size, which is available at (Facial expressions)  18  , and the dataset was merged and concise from the last seven emotions to five classes (merging between anger & disgust, sad & fear) and almost divided to (80% train and 20% test) with balancing between classes (each class contains approximately the same number of images) in each training class 2000 images and in each test class 400 as in Table  6 . 4) Finally, the dataset was maximized by adding some images and replicating others that may contain strong features, which benefit the model to classify better, with a low number of images that include this feature so that each training class had 4000 images, and each test class had 800 images to obtain the 3RL dataset, as shown in table  7 .\n\nThe whole process explained earlier in all three datasets was concise in the conceptual diagram in Figure  1 . A random collection of photos is shown from the final 3RL dataset in Figure  2 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Test",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Setup",
      "text": "All experiments were performed on core i7 from the seventh generation, 2.8 GHz processor and 16 Giga RAM memory.\n\nOpenCV was used to permit using the camera, and TensorFlow and Keras were used. Most of the experiments were performed with 50 epochs because a repetitive process was noticed when increasing the number of epochs without obvious differences in accuracy.\n\nIn the last experiment in The above Table  9  demonstrates the results when preprocessing the features to avoid the previous overfitting problem. The extracted features of the 3RL dataset were preprocessed with standard scalars and principal component analysis, which performed feature reduction once with 136 components and obtained 92% accuracy and 37% validation accuracy. Second, 208 components obtained 92% accuracy and 37% validation accuracy. Still not satisfied so, extra attempts were made in Table  10",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "Train Accuracy Test Accuracy FER-2013 99% 29% CK+48 33% 32% 3RL dataset 99% 58% Table  11 : dataset ML comparative Table 11 shows the dataset's accuracy with the best achieved parameters for RBF (Radial Basis Function), c, and gamma based on Table  9  and Table  10 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Deep Learning",
      "text": "First, a similar approach as in (Correa, Jonker, Ozo, & Stolk, 2016)  19  , (Chandran and Dr. Naveen S)  8  , (Li and Deng)  11  , (Zavarez, Berriel, & Oliveira-Santos, 2017)  20  was followed to extract features by CNN, and mainly as (Correa, Jonker, Ozo, & Stolk, 2016) 19 approach was followed and started experiments on FER2013 dataset (Table  2 ), four layers of convolutional neural network, the filters number in each layer was 32, 64, 128 & 128 for the fourth layer, 64 batch size and 150 epochs with changing the values of network parameters in order to obtain the best results as shown in the The last result is considered acceptable, but the prediction results\n\nwere not satisfactory in all previous experiments. Therefore, a change was made to the merged dataset in Table  4 , keeping the learning rate (3e -4 ) and the layers as it was and changing the dropout layers to 0.35, 0.5 and 0.5 for the last layer. Then, the results as shown in Table  13  were obtained:   4",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "As shown in the previous Table  13 , an improvement in the prediction was noticed in the last two experiments when heading to batch size 32, so in the next experiments, the batch size of 32 will be kept.\n\nAnother trial was performed using the dataset in Table  5 , keeping the other parameters the same as in the last experiment. A 98.9% training accuracy was achieved, and a 92.7% validation accuracy was achieved. The prediction was good but still not enough. Therefore, the model was trained with the three datasets, adding a dataset for facial expression recognition. The last dataset was cleaned and sorted into five main classes (emotions), as shown in Table  6 , and the dropout layers were edited to 0.35, 0.5 & 0.5 for the last one. The results are shown in Table  14 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Batch Lr Activate Loss Fn Epoch Accuracy",
      "text": "Test Accuracy 32 1e-4 ReLU categorical 50 82% 66%",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Table 14: Deep Learning Results Table 6 Dataset",
      "text": "For this result in Table  14 , the prediction was considered not bad, but reaching a higher accuracy was better, so we concluded that the number of images in the dataset may not be sufficient for deep learn-ing; therefore, the dataset was maximized, as shown in Table  7 .\n\nIn some results with the 3RL dataset, the batch size was 32, the learning rate was 1e-4 and the dropout layers were as in the previous try. In Table  11  the predict was bad so the activation function was changed to ReLU (Rectified Linear Unit), the loss function to binary and the last layer of drop out to 0.25 keeping other parameters as it was, an improvement was noticed in predict, so more experiments were done to obtain a better predict.\n\nThen, tying to change the number of convolutional layer's filters to be 32, 64, 128 and 512 for the last one, the learning rate was e-4 and the batch size was 64, Then, some experiments were done as follow in  As noted in Table  16 , if the tangent function was used as an activation function with binary cross entropy as a loss function, the result would be more satisfying than using tangent with categorical cross entropy. The best dropout layers were achieved in the first experiment in the previous table, so those dropout layers were retained. Finally, the desired result was obtained, achieving the best accuracy at 99.9% and 91.4% as validation accuracy, 0.4 for validation loss and 0.004 for loss. The prediction was very good. The network was able to detect all five emotions correctly. Figure  5  shows the curves of accuracy and loss in the last experiment in Table  16 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Figure 5: Accuracy And Loss Curves",
      "text": "In Table  17 , a short comparison was performed to show the benefit of the 3RL dataset in the prediction phase, where models were tested on FER-2013, CK+48, and 3RL while keeping the same parameters as in the last result in    Therefore, during experiments, screenshots were taken from real time applying the best parameters as in the last try in Table  16  on the three datasets (FRE-2013, CK+48, 3RL dataset).\n\nA complete disappearance of many emotions was noticed with some errors in prediction, and feeling fear dominated the rest of the feelings when using the FER-2013 dataset, as shown in Figure  6 , with the first row of results in Table  17 .\n\nWhen using CK+48 as a dataset, the second row of Table  17  shows the accuracy details. In Figure  7 , it is noticeable that most of the emotions are classified as contempt with the absence of a complete classification of other feelings except disgust, which is classified correctly, as the use of this dataset showed a significant decline in the classification compared to the FER-2013 dataset, which gave better results and a noticeable improvement in prediction than CK+48.\n\nIn the 3RL dataset, the last row in Table  17 , as screenshots of Figure 8, no absence of any feeling from the classification was noted, but very slight and minor errors were noted, such as confusing the feelings of disgust with anger when using the eyebrow-contract feature to express both feelings, which makes this dataset outperform its previous competitors.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion & Future Work",
      "text": "Emotion detection is the key to human interaction and understanding. Emotion detection is a challenging task. CNN is one of the best solutions for classifying facial emotions using large datasets such as 3RL. In this study, a new dataset named 3RL was created combining three datasets, CK+48 and Fer2013, and a dataset for facial expression recognition plus merging classes of emotion into 5 main classes. The 3RL dataset will be available at 1. The experiments showed high accuracies of 99%, whereas generalization was better when implementing CNN as the DL method.\n\nThe final CNN architecture consists of four 2-D convolutional layers, three dropout layers (32, 64, 128, 512), three max-pooling layers, and two fully connected layers. The input to the network is a preprocessed gray image of 48x48 for the face. The number of layers was selected to maintain a high level of accuracy while still being fast enough for real-time purposes. In addition, max pooling and dropout are utilized more effectively to minimize overfitting.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A random collection of photos is shown from the final 3RL dataset",
      "page": 3
    },
    {
      "caption": "Figure 1: 3RL dataset conceptual diagram",
      "page": 4
    },
    {
      "caption": "Figure 2: 3RL dataset samples",
      "page": 5
    },
    {
      "caption": "Figure 3: ML architecture",
      "page": 6
    },
    {
      "caption": "Figure 3: shows the ML architecture, where the input image is",
      "page": 6
    },
    {
      "caption": "Figure 4: Proposed network architecture",
      "page": 7
    },
    {
      "caption": "Figure 5: Accuracy and loss curves",
      "page": 7
    },
    {
      "caption": "Figure 6: FER-2013 predicting results",
      "page": 7
    },
    {
      "caption": "Figure 7: CK+48 predicting results",
      "page": 7
    },
    {
      "caption": "Figure 8: 3RL predicting results",
      "page": 8
    },
    {
      "caption": "Figure 6: , with the first row of results in Table 17.",
      "page": 8
    },
    {
      "caption": "Figure 7: , it is noticeable that most of the",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: State of the art",
      "data": [
        {
          "Network": "CNN\nParticular smile recognition",
          "Features": "Hand-crafted",
          "Dataset": "DISFA",
          "Accuracy": "99.45%"
        },
        {
          "Network": "ECAN\nClassified seven\nemotion classes",
          "Features": "& H\nDeep learning\nOG",
          "Dataset": "JAFFE,\nMMI,\nCK+\nOulu-CASIA\nFER2013 SFEW",
          "Accuracy": "61.94%\n69.89%\n89.69%\n63.97%\n58.21%\n58.21%"
        },
        {
          "Network": "CNN",
          "Features": "Hand-crafted",
          "Dataset": "CK+\nFER2013\nFERG\nJAFFE",
          "Accuracy": "98%\n70.02%\n99.3%\n92.8%"
        },
        {
          "Network": "CNN\nClassified seven\nemotion classes",
          "Features": "",
          "Dataset": "JAFFED + KDEF",
          "Accuracy": "78%"
        },
        {
          "Network": "SVM with genetic algorithm",
          "Features": "Geometric feature (landmark \ncurvature, victories landmark)",
          "Dataset": "8-class\nCK+\n7-class\nCK+\n7-class\nMUG",
          "Accuracy": "93.57%\n95.58%\n96.29%"
        },
        {
          "Network": "SVM",
          "Features": "",
          "Dataset": "JAFFE",
          "Accuracy": "80%"
        },
        {
          "Network": "SVM",
          "Features": "Facial\nLandmarks, BRIEF",
          "Dataset": "",
          "Accuracy": "96.27%"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: FER-2013 dataset 3.2 CK+48",
      "data": [
        {
          "Test": "958",
          "Train": "3995",
          "sum": "4953"
        },
        {
          "Test": "111",
          "Train": "436",
          "sum": "547"
        },
        {
          "Test": "1024",
          "Train": "4097",
          "sum": "5121"
        },
        {
          "Test": "1774",
          "Train": "7215",
          "sum": "8989"
        },
        {
          "Test": "1233",
          "Train": "4965",
          "sum": "6198"
        },
        {
          "Test": "1247",
          "Train": "4830",
          "sum": "6077"
        },
        {
          "Test": "831",
          "Train": "3171",
          "sum": "4002"
        },
        {
          "Test": "7178",
          "Train": "28709",
          "sum": "35888"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: FER-2013 dataset 3.2 CK+48",
      "data": [
        {
          "Images number": "135"
        },
        {
          "Images number": "177"
        },
        {
          "Images number": "75"
        },
        {
          "Images number": "207"
        },
        {
          "Images number": "54"
        },
        {
          "Images number": "84"
        },
        {
          "Images number": "249"
        },
        {
          "Images number": "981"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: FER-2013 dataset 3.2 CK+48",
      "data": [
        {
          "Test": "474",
          "Train": "3190",
          "Sum": "3664"
        },
        {
          "Test": "86",
          "Train": "399",
          "Sum": "485"
        },
        {
          "Test": "493",
          "Train": "1015",
          "Sum": "1508"
        },
        {
          "Test": "1547",
          "Train": "2678",
          "Sum": "4225"
        },
        {
          "Test": "805",
          "Train": "2552",
          "Sum": "3357"
        },
        {
          "Test": "1134",
          "Train": "4228",
          "Sum": "5362"
        },
        {
          "Test": "504",
          "Train": "2342",
          "Sum": "2846"
        },
        {
          "Test": "5043",
          "Train": "16404",
          "Sum": "21447"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 5: Cleaned dataset",
      "data": [
        {
          "Test": "418",
          "Train": "1908",
          "Sum": "2326"
        },
        {
          "Test": "74",
          "Train": "299",
          "Sum": "373"
        },
        {
          "Test": "233",
          "Train": "936",
          "Sum": "1169"
        },
        {
          "Test": "1435",
          "Train": "5744",
          "Sum": "7179"
        },
        {
          "Test": "671",
          "Train": "2688",
          "Sum": "3359"
        },
        {
          "Test": "445",
          "Train": "1782",
          "Sum": "2227"
        },
        {
          "Test": "418",
          "Train": "1676",
          "Sum": "2094"
        },
        {
          "Test": "3694",
          "Train": "15033",
          "Sum": "18727"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: Cleaned dataset",
      "data": [
        {
          "Test": "484",
          "Train": "2069",
          "Sum": "2553"
        },
        {
          "Test": "492",
          "Train": "2039",
          "Sum": "2531"
        },
        {
          "Test": "401",
          "Train": "2000",
          "Sum": "2401"
        },
        {
          "Test": "403",
          "Train": "2031",
          "Sum": "2432"
        },
        {
          "Test": "400",
          "Train": "2046",
          "Sum": "2446"
        },
        {
          "Test": "2180",
          "Train": "10185",
          "Sum": "12365"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: Cleaned dataset",
      "data": [
        {
          "Test": "838",
          "Train": "4040",
          "Sum": "4878"
        },
        {
          "Test": "822",
          "Train": "4061",
          "Sum": "4883"
        },
        {
          "Test": "854",
          "Train": "4070",
          "Sum": "4924"
        },
        {
          "Test": "822",
          "Train": "4057",
          "Sum": "4879"
        },
        {
          "Test": "816",
          "Train": "4014",
          "Sum": "4830"
        },
        {
          "Test": "4152",
          "Train": "20242",
          "Sum": "24394"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 7: , better results were achieved",
      "data": [
        {
          "C": "0.1",
          "Gamma": "0.01",
          "Accuracy": "20%",
          "Test Accuracy": "20%"
        },
        {
          "C": "1",
          "Gamma": "0.1",
          "Accuracy": "99%",
          "Test Accuracy": "58%"
        },
        {
          "C": "1",
          "Gamma": "0.2",
          "Accuracy": "99%",
          "Test Accuracy": "58%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 7: , better results were achieved",
      "data": [
        {
          "Kernel": "RBF",
          "Accuracy": "99%",
          "Test Accuracy": "29%"
        },
        {
          "Kernel": "Linear",
          "Accuracy": "21%",
          "Test Accuracy": "14%"
        },
        {
          "Kernel": "Sigmoid",
          "Accuracy": "24%",
          "Test Accuracy": "29%"
        },
        {
          "Kernel": "Poly",
          "Accuracy": "32%",
          "Test Accuracy": "14%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: , keeping the",
      "data": [
        {
          "Dataset": "FER-2013",
          "Train Accuracy": "99%",
          "Test Accuracy": "29%"
        },
        {
          "Dataset": "CK+48",
          "Train Accuracy": "33%",
          "Test Accuracy": "32%"
        },
        {
          "Dataset": "3RL dataset",
          "Train Accuracy": "99%",
          "Test Accuracy": "58%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , keeping the",
      "data": [
        {
          "Batch": "64",
          "LR": "3e-4",
          "acti-\nvate": "ReLU",
          "Loss \nFN": "cate-\ngorical",
          "epoch": "150",
          "Accu-\nracy": "95%",
          "Test Ac-\ncuracy": "66%"
        },
        {
          "Batch": "32",
          "LR": "1e-4",
          "acti-\nvate": "ReLU \nTanh",
          "Loss \nFN": "Binary",
          "epoch": "50",
          "Accu-\nracy": "88%",
          "Test Ac-\ncuracy": "68%"
        },
        {
          "Batch": "32",
          "LR": "1e-4",
          "acti-\nvate": "",
          "Loss \nFN": "Binary",
          "epoch": "100",
          "Accu-\nracy": "98%",
          "Test Ac-\ncuracy": "90%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , keeping the",
      "data": [
        {
          "Batch": "32",
          "LR": "1e-4  ReLU",
          "activate": "",
          "Loss FN": "categor-\nical",
          "epoch  Accu-": "50",
          "racy": "82%",
          "Test Ac-\ncuracy": "66%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , keeping the",
      "data": [
        {
          "LR \ne-4 \n3e-4 \n3e-4": "",
          "Active  Loss FN  Dropout  Accuracy": "categor-\nical",
          "Test Accu-\nracy": "62%"
        },
        {
          "LR \ne-4 \n3e-4 \n3e-4": "",
          "Active  Loss FN  Dropout  Accuracy": "categor-\nical",
          "Test Accu-\nracy": "62%"
        },
        {
          "LR \ne-4 \n3e-4 \n3e-4": "",
          "Active  Loss FN  Dropout  Accuracy": "binary",
          "Test Accu-\nracy": "92%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 7: The previous figure shows the architecture of the proposed convo-",
      "data": [
        {
          "activate": "tanh",
          "Loss FN": "Binary",
          "epoch": "200",
          "Accuracy": "96%",
          "Test Accuracy": "80%"
        },
        {
          "activate": "sigmoid",
          "Loss FN": "Categor-\nical",
          "epoch": "50",
          "Accuracy": "90.9%",
          "Test Accuracy": "90.4%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 7: The previous figure shows the architecture of the proposed convo-",
      "data": [
        {
          "Active": "ReLU",
          "Loss FN": "binary",
          "Dropout": "0.25 0.1 \n0.25",
          "Accuracy  Test Accuracy": "99.8%"
        },
        {
          "Active": "ReLU",
          "Loss FN": "categorical",
          "Dropout": "0.01 0.1 \n0.25",
          "Accuracy  Test Accuracy": "99.5%"
        },
        {
          "Active": "ReLU",
          "Loss FN": "categorical",
          "Dropout": "0.01 0.5 \n0.25",
          "Accuracy  Test Accuracy": "99%"
        },
        {
          "Active": "ReLU",
          "Loss FN": "categorical",
          "Dropout": "0.01 0.03 \n0.25",
          "Accuracy  Test Accuracy": "99.5%"
        },
        {
          "Active": "tanh",
          "Loss FN": "categorical",
          "Dropout": "0.01 0.5 \n0.25",
          "Accuracy  Test Accuracy": "97.7%"
        },
        {
          "Active": "Sigmoid",
          "Loss FN": "binary",
          "Dropout": "0.25 0.1 \n0.25",
          "Accuracy  Test Accuracy": "99.9%"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 7: The previous figure shows the architecture of the proposed convo-",
      "data": [
        {
          "Dataset": "FER-2013",
          "Accu-\nracy": "99.8%",
          "Test Ac-\ncuracy": "89%",
          "Prediction": "Recognize happy, neutral, \nfear and sad emotion with \nmisclassification in other \nclasses."
        },
        {
          "Dataset": "CK+48",
          "Accu-\nracy": "99.9%",
          "Test Ac-\ncuracy": "95.9%",
          "Prediction": "Only disgust emotion was \nrecognized correctly"
        },
        {
          "Dataset": "3RL dataset",
          "Accu-\nracy": "99.9%",
          "Test Ac-\ncuracy": "91.4",
          "Prediction": "Best prediction results be-\ntween all experiments"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real-time facial expression recognition app development on smart phones",
      "authors": [
        "H Alshamsi",
        "V Kupuska"
      ],
      "year": "2017",
      "venue": "Int J Eng Res Appl"
    },
    {
      "citation_id": "2",
      "title": "Three-dimensional, kinematic, human Behavioral pattern-based features for multimodal emotion recognition",
      "authors": [
        "A Patwardhan"
      ],
      "year": "2017",
      "venue": "Multimodal Technologies and Interaction",
      "doi": "10.3390/mti1030019"
    },
    {
      "citation_id": "3",
      "title": "Auto-optimized multimodal expression recognition framework using 3D kinect data for ASD therapeutic aid",
      "authors": [
        "A Youssef",
        "S Aly",
        "A Ibrahim",
        "A Abbott"
      ],
      "year": "2013",
      "venue": "International Journal of Modeling and Optimization"
    },
    {
      "citation_id": "4",
      "title": "Emotion detection using Kinect 3D facial points",
      "authors": [
        "Z Zhang",
        "L Cui",
        "X Liu",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "2016 IEEE/ WIC/ACM International Conference on Web Intelligence (WI)",
      "doi": "10.1109/WI.2016.0063"
    },
    {
      "citation_id": "5",
      "title": "Facial emotion detection using modified eyemap-mouthmap algorithm on an enhanced image and classification with tensorflow",
      "authors": [
        "A Joseph",
        "P Geetha"
      ],
      "year": "2020",
      "venue": "The Visual Computer",
      "doi": "10.1007/s00371-019-01628-3"
    },
    {
      "citation_id": "6",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "7",
      "title": "Which is the best multiclass SVM method? An empirical study",
      "authors": [
        "K Duan",
        "S Keerthi"
      ],
      "year": "2005",
      "venue": "Multiple Classifier Systems: 6th International Workshop, MCS 2005",
      "doi": "10.1007/11494683_28"
    },
    {
      "citation_id": "8",
      "title": "A Review on Facial Expression Recognition using Deep Learning",
      "authors": [
        "M Chandran",
        "S Naveen"
      ],
      "venue": "A Review on Facial Expression Recognition using Deep Learning"
    },
    {
      "citation_id": "9",
      "title": "Deep learning for smile recognition",
      "authors": [
        "P Glauner"
      ],
      "year": "2016",
      "venue": "Uncertainty Modelling in Knowledge Engineering and Decision Making: Proceedings of the 12th International FLINS Conference",
      "doi": "10.1142/9789813146976_0053"
    },
    {
      "citation_id": "10",
      "title": "HOG-CNN based real time face recognition",
      "authors": [
        "H Ahamed",
        "I Alam",
        "M Islam"
      ],
      "year": "2018",
      "venue": "International Conference on Advancement in Electrical and Electronic Engineering (ICAEEE)",
      "doi": "10.1109/ICAEEE.2018.8642989"
    },
    {
      "citation_id": "11",
      "title": "A deeper look at facial expres-sion dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2973158"
    },
    {
      "citation_id": "12",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "M Minaei",
        "A Abdolrashidi"
      ],
      "year": "2021",
      "venue": "Sensors",
      "doi": "10.3390/s21093046"
    },
    {
      "citation_id": "13",
      "title": "Facial emotions recognition using convolutional neural net",
      "authors": [
        "F Ghaffar"
      ],
      "year": "2020",
      "venue": "Facial emotions recognition using convolutional neural net",
      "doi": "10.48550/arXiv.2001.01456",
      "arxiv": "arXiv:2001.01456"
    },
    {
      "citation_id": "14",
      "title": "Ga-svm-based facial emotion recognition using facial geometric features",
      "authors": [
        "X Liu",
        "X Cheng",
        "K Lee"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal",
      "doi": "10.1109/JSEN.2020.3028075"
    },
    {
      "citation_id": "15",
      "title": "Vision Based Facial Expression Recognition Using Eigenfaces and Multi-SVM Classifier",
      "authors": [
        "H Maw",
        "S Thu",
        "M Mon"
      ],
      "year": "2020",
      "venue": "Advances in Computational Collective Intelligence: 12th International Conference",
      "doi": "10.1007/978-3-030-63119-2_54"
    },
    {
      "citation_id": "16",
      "title": "Facial Expression Recognition, (FERc)",
      "year": "2013",
      "venue": "Facial Expression Recognition, (FERc)"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "; Cohn-Kanade",
        ") Ck+"
      ],
      "year": "2010",
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition using deep convolutional neural networks",
      "authors": [
        "E Correa",
        "A Jonker",
        "M Ozo",
        "R Stolk"
      ],
      "year": "2016",
      "venue": "Emotion recognition using deep convolutional neural networks"
    },
    {
      "citation_id": "19",
      "title": "Cross-database facial expression recognition based on fine-tuned deep convolutional network",
      "authors": [
        "M Zavarez",
        "R Berriel",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "2017 30th SIB-GRAPI Conference on Graphics, Patterns and Images",
      "doi": "10.1109/SIBGRAPI.2017.60"
    }
  ]
}