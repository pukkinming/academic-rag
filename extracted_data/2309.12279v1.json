{
  "paper_id": "2309.12279v1",
  "title": "The Broad Impact Of Feature Imitation: Neural Enhancements Across Financial, Speech, And Physiological Domains",
  "published": "2023-09-21T17:40:44Z",
  "authors": [
    "Reza Khanmohammadi",
    "Tuka Alhanai",
    "Mohammad M. Ghassemi"
  ],
  "keywords": [
    "Feature Imitating Network",
    "Bitcoin Price Prediction",
    "Speech Emotion Recognition",
    "Chronic Neck Pain"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Initialization of neural network weights plays a pivotal role in determining their performance. Feature Imitating Networks (FINs) offer a novel strategy by initializing weights to approximate specific closed-form statistical features, setting a promising foundation for deep learning architectures. While the applicability of FINs has been chiefly tested in biomedical domains, this study extends its exploration into other time series datasets. Three different experiments are conducted in this study to test the applicability of imitating Tsallis entropy for performance enhancement: Bitcoin price prediction, speech emotion recognition, and chronic neck pain detection. For the Bitcoin price prediction, models embedded with FINs reduced the root mean square error by around 1000 compared to the baseline. In the speech emotion recognition task, the FIN-augmented model increased classification accuracy by over 3 percent. Lastly, in the CNP detection experiment, an improvement of about 7 percent was observed compared to established classifiers. These findings validate the broad utility and potency of FINs in diverse applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Deep learning has established itself as a foundational technique across various applications, primarily due to its capability to learn complex patterns and relationships. One of the crucial aspects influencing the efficacy of deep learning models is the initialization of their weights. Proper weight initialization can lead to faster model convergence and enhanced performance  [1] . While the reliance on large datasets and extensive computational resources is vital for determining feature quality and model versatility, correct initialization can offset some of the dependencies on these resources. This offset is especially crucial in domains with limited data and computational capabilities, underlining the importance of leveraging deep learning's potential without a heavy reliance on large datasets and extensive resources. To cater to such scenarios, FINs  [2]  offer an intuitive approach where neural networks are initialized to imitate specific statistical proper-ties. By doing so, FINs provide a more informed starting point, making neural networks less opaque and offering a hint of interpretability in what is often dubbed a \"black box.\" The beauty of FINs lies in their simplicity, allowing researchers to directly incorporate domain-specific knowledge into the model's architecture, fostering both efficacy and understandability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contributions",
      "text": "While FINs have made significant strides in biomedical signal processing  [2, 3, 4] , their applicability in broader domains remains a topic of interest. In this work, we delve into the potential of FINs across three distinct areas: financial, speech, and Electromyography (EMG) time series analysis. Our research aims to demonstrate how integrating a lightweight FIN can enhance the performance of different neural network architectures, regardless of the task or network topology. By investigating their effects across different contexts, we offer insights into the adaptability, benefits, and potential boundaries of using FINs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The Evolution of Transfer Learning Across Domains: Transfer learning has emerged as a potent technique in machine learning, reshaping the paradigm by repurposing pre-trained models to tackle different tasks from their original intent  [5] . Such a strategy has yielded transformative advancements, especially in computer vision  [6] , speech analysis  [7] , and natural language processing (NLP)  [8] . Foundational models like ResNet  [9] , wav2vec  [10] , and BERT  [11]  stand as prime examples of this shift, requiring significantly reduced training data when finetuned for new tasks. Transitioning this approach to the biomedical arena presents unique challenges. There is an inherent lack of large and diverse biomedical datasets  [10]   [12] , which has led to cross-domain adaptations, such as repurposing computer vision models for audio classification  [13] . These adaptations, while novel, often do not achieve the same efficacy as within-domain counterparts, highlighting the pressing need for tailored approaches for biomedical data.\n\nStatistical Feature Imitation Bridges the Transfer Learning Divide in Diverse Specialized Tasks: FINs have established a unique role in addressing this particular challenge  [2] . FINs offer a distinctive approach to neural learning by initializing weights to simulate distinct statistical features, effectively bridging domain knowledge with machine learning. This method has catalyzed notable progress in many fields by showcasing its effectiveness across various tasks. In the seminal work introducing FINs  [2] , the authors showcased the efficacy of this novel approach across three experiments. In Electrocardiogram (ECG) artifact detection, a FIN imitating the Kurtosis feature outperforms standard models in both performance and stability. Similarly, for Electroencephalogram (EEG) artifact detection within the same research, FINs imitating Kurtosis and Shannon's Entropy enhanced results. Moreover, when applied to EEG data for fatigue and drowsiness detection, a FIN based on Shannon's entropy consistently outperformed baselines, while certain models like VGG proved ineffective. Additionally, FINs have shown promise in specialized applications. In biomedical image processing, Ming et al (2023) provided state-of-the-art results across tasks including COVID-19 detection from CT scans and brain tumor identification and segmentation from MRI scans  [3] . In sports analytics, the hybrid architecture of MambaNet  [4]  employed FINs to effectively predict NBA playoff outcomes, showcasing the broad versatility of the FIN approach. Although FINs have shown promise in biomedical applications and sports analytics, their potential in financial and speech time series data is yet to be explored.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Imitating Tsallis Entropy",
      "text": "A FIN is a neural network that is trained to approximate a closed-form statistical feature of choice. In our study, we train a FIN to imitate Tsallis Entropy. Tsallis entropy, a non-extensive generalization of the traditional Shannon entropy, measures the uncertainty or randomness of a system. Uniquely, it takes into account the correlations and higher-order interactions that are often overlooked by the conventional Shannon entropy. This quality makes Tsallis entropy particularly apt for systems exhibiting non-standard statistical behaviors and long-range dependencies. The Influence of q on Tsallis Entropy The distinguishing characteristic of Tsallis entropy is its reliance on the parameter q. The Shannon entropy becomes a special case of Tsallis entropy when q = 1. When q > 1, the entropy gives more weight to lower probabilities, making it more sensitive to rare events. Conversely, for q < 1, the entropy calculation is dominated by higher probabilities. This variability in weighting is encapsulated by the equation for a discrete probability distribution p(i) as influenced by the temperature scaling parameter τ :\n\nWhere u(i) represents the unscaled probabilities from the normalized input. In our implementation, q is set to a default value of 1.5 and further treated as a trainable parameter within our FIN, allowing the model to adaptively finetune its value to optimally capture the inherent complexities and nuances of the dataset.\n\nTemperature Scaling with Parameter τ Another pivotal parameter in our approximation process is τ . This temperature parameter modulates the entropy's sensitivity by scaling the inputs to the softmax function. Specifically, as τ approaches 0, the softmax output mirrors a one-hot encoded distribution, while increasing τ causes the resultant distribution to edge towards uniformity. The introduction of τ in the Tsallis entropy equation underlines its importance in shaping the final probabilities. In the context of our work, τ is initialized with a default value of 1, but like q, it's also trainable within our FIN, allowing the network to adjust it adaptively during the learning phase.\n\nTraining To approximate the Tsallis entropy using neural networks, we generated synthetic signals with uniform random values between 0 and 1. The output regression values for the FIN were the Tsallis Entropy values, which were computed directly on the synthetic signals using the defined closed-form expression in equation 1. This calculation is fundamentally based on a power-law probability distribution. We utilized a simple gradient descent optimizer along with mean absolute error (L1) loss to train this network. Additionally, early stopping was integrated, and the training was optimized with learning rate modifications facilitated by the ReduceLROnPlateau scheduler.\n\nBaseline Model In each of our three experiments, we employed a neural network as a comparative baseline. This network had a representational capability (i.e. number of parameters) that was either equal to or exceeded the FIN-embedded networks introduced in that particular experiment. We investigated multiple network topologies, experimenting with as many as ten variations for each baseline. The model that showcased the best performance on the validation set was subsequently chosen for comparison against the Tsallis Entropy FIN-powered networks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments & Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment I",
      "text": "Objective This experiment focuses on predicting the closing price of Bitcoin on a given day, for the subsequent day.\n\nWe hypothesize that we can achieve enhanced predictive accuracy over traditional baselines by initializing certain neural network weights to imitate Tsallis entropy, followed by finetuning during training.\n\nData and Preprocessing Our study leveraged a publicly accessible dataset 1  that spanned over seven years, from March",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Ii",
      "text": "Objective This experiment aims to enhance speech emotion recognition by leveraging FINs. Unlike the previous experiment, where the input data was fed directly into the FIN, here, we utilize a latent representation of the data-a condensed, yet informative, representation derived from previous layers of a deep neural network. Our hypothesis posits that by feeding this latent representation through the FIN, specifically designed to imitate the Tsallis entropy, and further fine-tuning it during training, we can achieve superior recognition performance. Our target is to surpass the state-of-the-art (SOTA) model, the Acoustic CNN (1D) from the reference study.\n\nData and Preprocessing We used the publicly available modified version 2  of the Sharif Emotional Speech Database (ShEMO)  [15] , which contains 3 hours and 25 minutes of semi-natural speech samples in .wav format. These 3000 samples, recorded by 87 native Farsi speakers, are multilabeled. The reference study  [16]  concentrated on emotions like anger, happiness, sadness, surprise, and neutral. Each speech segment, with an average length of 4.11 seconds, was embedded using wav2vec2  [17]  to enhance its representation in our neural network model. Methods Our method is a deep neural network with a series of fully connected (dense) layers with decreasing units: 512, 256, 128, 64, and 32. Each layer is followed by a ReLU activation function and a dropout layer (rate=0.5) to prevent overfitting. Crucially, after obtaining the 32-unit latent representation from the penultimate layer, the FIN is integrated to compute the Tsallis entropy of this representation. The computed entropy is then concatenated with the 32-unit latent representation and fed into the final fully connected layer to produce the output corresponding to the emotion classes.\n\nResults Our experiment compared three models: our proposed FIN-ENN, the NN-Baseline, and the Acoustic CNN (1D) from the reference study  [16] . The baseline model utilized the emo large feature set, extracting 6552 high-level acoustic features from each audio file using the openSMILE toolkit  [18] . These features arise from high-level statistical functions applied to low-level descriptors. Conversely, our FIN-ENN model adopted two fine-tuned versions of the wav2vec2 model: w2v2-persian-v3 3  and w2v2-persian-ser  4  .\n\nAs shown in Table  2 , the FIN-ENN model's integration of Tsallis FIN contributed to an absolute accuracy improvement of 2.83% for w2v2-persian-v3 and 0.64% for w2v2-persianser compared to their FIN-less counterparts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Iii",
      "text": "Objective This experiment delves into the detection of Chronic Neck Pain (CNP) through EMG data. We hypothesize that embedding a neural network with the FIN, specifically designed to imitate the Tsallis entropy, will improve CNP detection performance compared to traditional models. Data and Preprocessing Our dataset, sourced from Jiménez-Grand et al  [19]  and publicly available on Kaggle 5  , consists of twenty asymptomatic individuals and twenty with Chronic Neck Pain (CNP). Asymptomatic individuals had no significant neck issues in the last two years, while CNP individuals reported notable pain recently. Data was collected as participants walked barefoot along a six-meter rectilinear path, repeated three times at two-minute intervals. Building upon the approach adopted in the original study by Jim'enez-Grand et al.  [19] , we extracted the same four time domain and six frequency domain features from the EMG data. However, instead of analyzing every 500 ms of the signal (as determined by a 1000Hz sampling rate), we segmented the entire signal into five distinct parts, a method inspired by  [20] . Similarly to prior studies, our focus centered on four upper back muscle groups: Trapezius, Sternocleidomastoid, C4 Paraspinal, and Latissimus Dorsi, with each muscle group including both left and right muscles, and features were computed for each side. Methods Jim'enez-Grand et al.  [19]  employed K-NN, SVM, and LDA for classification, processing both raw and Neighbourhood Component Analysis (NCA)-selected features  [21] . In contrast, we used the raw extracted features to train a feed-forward neural network comprising two hidden layers with 256 and 32 units. Drawing inspiration from our previous experiment, the 32-dimensional latent representation from the second hidden layer was channeled into the Tsallis FIN. This processed output was then concatenated with the original 32 features, yielding a 33-dimensional vector that was finally directed to a sigmoid activation to perform the binary classification.\n\nResults As outlined in Table  3 , we compared the performance of our FIN-ENN model against those developed in the original study using accuracy, specificity, and sensitivity. The original study's models, namely K-NN, SVM, and LDA, achieved a maximum accuracy of 55.00% with NCA-selected features. Our NN-Baseline registered an accuracy of 57.50%. However, by leveraging the Tsallis FIN in our architecture, we achieved a superior accuracy of 62.50%. This improvement is also evident in improvements made in both specificity (65.00%) and sensitivity (60.00%). Our results reinforce our initial hypothesis, underscoring the benefits of incorporating the FIN for CNP detection from physiological EMG data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In our experiments, integrating a Feature Imitating Network (FIN) designed to imitate Tsallis entropy consistently enhanced predictive model performances across diverse domains. In predicting Bitcoin's subsequent day's closing price, the enhanced neural network outshone traditional models like Random Forest regression and LSTM. Similarly, in speech emotion recognition, the FIN-augmented model excelled at processing latent representations. In detecting Chronic Neck Pain (CNP) through EMG data, it surpassed established classifiers like K-NN, SVM, and LDA. The consistent edge the FIN provides across these areas underscores its broad utility and efficacy. Future studies can more profoundly investigate influential financial, speech, and physiological features to imitate, aiming to amplify the performance of neural predictive models further.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Acoustic CNN (1D)",
          "Input Feature": "emo large",
          "Accuracy": "66.12"
        },
        {
          "Method": "NN-Baseline\nFIN-ENN",
          "Input Feature": "w2v2-persian-v3\nw2v2-persian-v3",
          "Accuracy": "69.40\n72.23"
        },
        {
          "Method": "NN-Baseline\nFIN-ENN",
          "Input Feature": "w2v2-persian-ser\nw2v2-persian-ser",
          "Accuracy": "94.87\n95.51"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Period": "1",
          "Model": "RF regression\nDeep LSTM\nDeep LSTM + Attention\nNN-Baseline\nFIN-ENN",
          "RMSE": "321.61\n330.26\n283.83\n287.47\n277.45",
          "MAPE": "3.39%\n3.57%\n2.97%\n2.97%\n2.87%"
        },
        {
          "Period": "2",
          "Model": "RF regression\nDeep LSTM\nDeep LSTM + Attention\nNN-Baseline\nFIN-ENN",
          "RMSE": "2096.24\n3045.87\n2014.43\n2127.70\n2001.45",
          "MAPE": "3.29%\n4.68%\n2.96%\n3.18%\n2.96%"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Comparison of classification performance in CNP reported notable pain recently. Data was collected as par-",
      "data": [
        {
          "Method": "K-NN (raw)\nSVM (raw)\nLDA (raw)",
          "Accuracy": "35.00\n32.50\n42.50",
          "Speciﬁcity": "35.00\n31.57\n42.85",
          "Sensitivity": "35.00\n33.33\n42.10"
        },
        {
          "Method": "K-NN (NCA)\nSVM (NCA)\nLDA (NCA)",
          "Accuracy": "55.00\n55.00\n55.00",
          "Speciﬁcity": "54.54\n60.00\n56.25",
          "Sensitivity": "55.55\n54.17\n55.00"
        },
        {
          "Method": "NN-Baseline (raw)\nFIN-ENN (raw)",
          "Accuracy": "57.50\n62.50",
          "Speciﬁcity": "55.00\n65.00",
          "Sensitivity": "60.00\n60.00"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "On weight initialization in deep neural networks",
      "authors": [
        "Krishna Siddharth",
        "Kumar"
      ],
      "year": "2017",
      "venue": "On weight initialization in deep neural networks"
    },
    {
      "citation_id": "3",
      "title": "Feature imitating networks",
      "authors": [
        "Sari Saba-Sadiya",
        "Tuka Alhanai",
        "Mohammad Ghassemi"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Feature imitating networks enhance the performance, reliability and speed of deep learning on biomedical image processing tasks",
      "authors": [
        "Shangyang Min",
        "Mohammad Mahdi Ghassemi",
        "Tuka Alhanai"
      ],
      "year": "2023",
      "venue": "Feature imitating networks enhance the performance, reliability and speed of deep learning on biomedical image processing tasks"
    },
    {
      "citation_id": "5",
      "title": "Mambanet: A hybrid neural network for predicting the nba playoffs",
      "authors": [
        "Reza Khanmohammadi",
        "Sari Saba-Sadiya",
        "Sina Esfandiarpour",
        "Tuka Alhanai",
        "Mohammad Ghassemi"
      ],
      "year": "2022",
      "venue": "Mambanet: A hybrid neural network for predicting the nba playoffs"
    },
    {
      "citation_id": "6",
      "title": "A comprehensive survey on transfer learning",
      "authors": [
        "Fuzhen Zhuang",
        "Zhiyuan Qi",
        "Keyu Duan",
        "Dongbo Xi",
        "Yongchun Zhu",
        "Hengshu Zhu",
        "Hui Xiong",
        "Qing He"
      ],
      "year": "2020",
      "venue": "A comprehensive survey on transfer learning"
    },
    {
      "citation_id": "7",
      "title": "Transfer learning in computer vision tasks: Remember where you come from",
      "authors": [
        "Xuhong Li",
        "Yves Grandvalet",
        "Franck Davoine",
        "Jingchun Cheng",
        "Yin Cui",
        "Hang Zhang",
        "Serge Belongie",
        "Yi-Hsuan Tsai",
        "Ming-Hsuan Yang"
      ],
      "year": "2020",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "8",
      "title": "Deepspectrumlite: A power-efficient transfer learning framework for embedded speech and audio processing from decentralised data",
      "authors": [
        "Shahin Amiriparian",
        "Tobias Hübner",
        "Maurice Gerczuk",
        "Sandra Ottl",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Deepspectrumlite: A power-efficient transfer learning framework for embedded speech and audio processing from decentralised data"
    },
    {
      "citation_id": "9",
      "title": "Transfer learning in natural language processing",
      "authors": [
        "Sebastian Ruder",
        "Matthew Peters",
        "Swabha Swayamdipta",
        "Thomas Wolf"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "10",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "11",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "12",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "13",
      "title": "You snooze, you win: the physionet/computing in cardiology challenge 2018",
      "authors": [
        "Mohammad Mahdi",
        "Benjamin Moody",
        "Li Wei",
        "H Lehman",
        "Christopher Song",
        "Qiao Li",
        "Haoqi Sun",
        "Michael Brandon Westover",
        "Gari Clifford"
      ],
      "year": "2018",
      "venue": "Computing in Cardiology Conference (CinC)"
    },
    {
      "citation_id": "14",
      "title": "Real-time speech emotion recognition using a pre-trained image classification network: Effects of bandwidth reduction and companding",
      "authors": [
        "Margaret Lech",
        "Melissa Stolar",
        "Christopher Best",
        "Robert Bolia"
      ],
      "year": "2020",
      "venue": "Frontiers of Computer Science"
    },
    {
      "citation_id": "15",
      "title": "Analysis of bitcoin price prediction using machine learning",
      "authors": [
        "Junwei Chen"
      ],
      "year": "2023",
      "venue": "Journal of Risk and Financial Management"
    },
    {
      "citation_id": "16",
      "title": "Shemo: a large-scale validated database for persian speech emotion detection",
      "authors": [
        "Mohamad Omid",
        "Paria Nezami",
        "Mansooreh Lou",
        "Karami"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "17",
      "title": "A persian asr-based ser: Modification of sharif emotional speech database and investigation of persian text corpora",
      "authors": [
        "Ali Yazdani",
        "Yasser Shekofteh"
      ],
      "venue": "A persian asr-based ser: Modification of sharif emotional speech database and investigation of persian text corpora"
    },
    {
      "citation_id": "18",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "19",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Muscle network topology analysis for the classification of chronic neck pain based on emg biomarkers extracted during walking",
      "authors": [
        "David Jiménez-Grande",
        "S Farokh Atashzar",
        "Eduardo Martinez-Valdes",
        "Deborah Falla"
      ],
      "year": "2021",
      "venue": "Plos one"
    },
    {
      "citation_id": "21",
      "title": "Biomarker design using combinations of upper back muscles to classify chronic neck pain during curvilinear walking",
      "authors": [
        "Anadi Biswas",
        "Debangshu Dey",
        "Sugata Munshi"
      ],
      "year": "2022",
      "venue": "2022 2nd International Conference on Emerging Frontiers in Electrical and Electronic Technologies (ICEFEET)"
    },
    {
      "citation_id": "22",
      "title": "Neighbourhood components analysis",
      "authors": [
        "Jacob Goldberger",
        "Geoffrey Hinton",
        "Sam Roweis",
        "Russ Salakhutdinov"
      ],
      "year": "2004",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}