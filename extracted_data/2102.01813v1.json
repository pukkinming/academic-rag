{
  "paper_id": "2102.01813v1",
  "title": "Speech Emotion Recognition With Multiscale Area Attention And Data Augmentation",
  "published": "2021-02-03T00:39:09Z",
  "authors": [
    "Mingke Xu",
    "Fan Zhang",
    "Xiaodong Cui",
    "Wei Zhang"
  ],
  "keywords": [
    "speech emotion recognition",
    "convolutional neural network",
    "attention mechanism",
    "data augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In Speech Emotion Recognition (SER), emotional characteristics often appear in diverse forms of energy patterns in spectrograms. Typical attention neural network classifiers of SER are usually optimized on a fixed attention granularity. In this paper, we apply multiscale area attention in a deep convolutional neural network to attend emotional characteristics with varied granularities and therefore the classifier can benefit from an ensemble of attentions with different scales. To deal with data sparsity, we conduct data augmentation with vocal tract length perturbation (VTLP) to improve the generalization capability of the classifier. Experiments are carried out on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. We achieved 79.34% weighted accuracy (WA) and 77.54% unweighted accuracy (UA), which, to the best of our knowledge, is the state of the art on this dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech is an important carrier of emotions in human communication. Speech Emotion Recognition (SER) has wide application perspectives on psychological assessment  [1] , robots  [2] , mobile services  [3] , etc. For example, a psychologist can design a treatment plan according to the emotions hidden/expressed in the patient's speech. Deep learning has accelerated the progress of recognizing human emotions from speech  [4] [5] [6] [7] [8] [9] , but there are still deficiencies in the research of SER, such as data shortage and insufficient model accuracy.\n\nRecently, we proposed Head Fusion Net  [10]  1 which achieved the state-of-the-art performance on the IEMOCAP dataset. However, it does not fully address the above problems. In SER, emotion may display distinct energy patterns in spectrograms with varied granularity of areas. However, typical attention models in SER are usually optimized on a fixed scale, which may limit the model's capability to deal with diverse areas and granularities. Therefore, in this paper, we introduce multiscale area attention to a deep convolutional neural network model based on Head Fusion to improve 1 The code is released at github.com/lessonxmk/head fusion model accuracy. Furthermore, data augmentation is used to address the data scarcity issue.\n\nOur main contributions are as follows:\n\n• To the best of our knowledge, this is the first attempt for applying multiscale area attention to SER.   [11]  used ladder networks to combine the unsupervised auxiliary task and the primary task of predicting emotional attributes.\n\nThere is a recent resurgence of interest on attention-based SER models  [8, 9, 12] . However, those attention mechanisms can only be calculated with a preset granularity which may not adapt dynamically to different areas of interest in spectrogram. Y. Li et al.  [13]  proposed area attention that allows the model to calculate attention with multiple granularities concurrently, an idea that is not yet explored in SER.\n\nInsufficient data hinders progress in SER. Data augmentation has become a popular method to increase training data  [14] [15] [16] [17]  in the related field of Automatic Speech Recognition (ASR). Yet, it has not enjoyed broad attention for SER.\n\nIn this paper, we extend the multiscale area attention to SER with data augmentation. We introduce our method in section 3 and experiment results in section 4 followed by the conclusion with section 5.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "We first introduce our base convolutional neural networks that shares similarity to our Head Fusion Net  [10] , then our newly introduced multiscale area attention that enhances Head Fusion Net, and finally the data augmentation technique.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Convolutional Neural Networks",
      "text": "We designed an attention-based convolutional neural network with 5 convolutional layers, an attention layer, and a fully connected layer. Fig.  1  shows the detailed model structure. First, the Librosa audio processing library  [18]  is used to extract the logMel spectrogram as features, which are fed into two parallel convolutional layers to extract textures from the time axis and frequency axis, respectively. The result is fed into four consecutive convolutional layers and generates an 80-channel representation. Then the attention layer attends on the representation and sends the outputs to the fully connected layer for classification. Batch normalization is applied after each convolutional layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multiscale Area Attention",
      "text": "In this section, We extend the area attention in Y. Li et al.  [13]  to SER. The attention mechanism can be regarded as a soft addressing operation, which uses key-value pairs to represent the content stored in the memory, and the elements are composed of the address (key) and the value (value). Query can match to a key which correspondent value is retrieved from the memory according to the degree of correlation between the query and the key. The query, key, and value are usually first multiplied by a parameter matrix W to obtain Q, K and V. Eq.1 shows the calculation of attention score, where d k is the dimension of K  [19]  to prevent the result from being too large.\n\nIn self-attention, the query, key and value come from a same input X. By calculating self-attention, the model can focus on the connection between different parts of the input. In SER, the distribution of emotion characteristics often crosses a larger scale, and using self-attention in speech emotion recognition improves the accuracy.\n\nHowever, under the conventional attention, the model only uses a preset granularity as the basic unit for calculation, e.g., a word for a word-level translation model, a grid cell for an image-based model, etc. Yet, it is hard to know which granularity is most suitable for a complex task.\n\nArea attention allows the model to attend at multiple scales and granularities and to learn the most appropriate granularities. As shown in Fig.  2 , for a continuous memory block, multiple areas can be created to accommodate for different granularities, e.g., 1x2, 2x1, 2x2 and etc.. In order to calculate attention in units of areas, we need to define the key and value for the area. For example, we can define the mean of an area as the key and the sum of an area as the value, so that the attention can be evaluated in a way similar to ordinary attention. (Eq.1)\n\nExhaustive evaluation of attention on a large memory block may be computationally prohibitive. A maximum length and width is set to an area under investigation. Fig.  2 . Generating Area Multiple areas can be generated by combining adjacent items in a continuous memory block. For a 3x3 memory block, if we set the max area size to 2x2, the memory block can be divided into 1x1,1x2,2x1 and 2x2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation",
      "text": "Given the limited amount of training data in IEMOCAP, we use vocal tract length perturbation (VTLP)  [14]  as means for data augmentation. VTLP increases the number of speakers by perturbing the vocal tract length. We generated additional 7 replicas of the original data with nlpaug library  [20] . The augmented data is only used for training. Interactive Emotional Dyadic Motion Capture (IEMOCAP) et al.  [21]  is the most widely used dataset in the SER field. It contains 12 hours of emotional speech performed by 10 actors from the Drama Department of University of Southern California. The performance is divided into two parts, improvised and scripted, according to whether the actors perform according to a fixed script. The utterances are labeled with 9 types of emotion-anger, happiness, excitement, sadness, frustration, fear, surprise, other and neutral state. Due to the imbalances in the dataset, researchers usually choose the most common emotions, such as neutral state, happiness, sadness, and anger. Because excitement and happiness have a certain degree of similarity and there are too few happy utterances, researchers sometimes replace happiness with excitement or combine excitement and happiness to increase the amount of data  [22] [23] [24] . In addition, previous studies have shown that the accuracy of using improvised data is higher than that of scripted data,  [12, 22]  which can be due to the fact that actors pay more attention to expressing their emotion rather than the script during improvisation.\n\nIn this paper, following other published work, we use improvised data in the IEMOCAP dataset with four types of emotion-neutral state, excitement, sadness, and anger.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We use weighted accuracy (WA) and unweighted accuracy (UA) for evaluation, which have been broadly employed in SER literature. Considering that WA and UA may not reach the maximum in the same model, we calculate the average of WA and UA as the final evaluation criterion (indicated by ACC below), i.e., we save the model with the largest average of WA and UA as the optimal model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We randomly divide the dataset into a training set (80% of data) and a test set (20% of data) for 5-fold cross-validation. Each utterance is divided into 2-second segments, with 1 second (in training) or 1.6 seconds (in testing) overlap between segments. Although divided, the test is still based on the utterance, and the prediction results from the same utterance are averaged as the prediction result of the utterance. Experience shows that a large overlap can make the recognition result of utterance more stable in testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fig. 4. Result Of Modifying The Amount Of Data With Vtlp",
      "text": "The horizontal axis refers to the total amount of data used for training. For example, 8 on the horizontal axis represents original data plus 7 replicas of augmented data. It can be seen that the more augmented data added the higher the accuracy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "Selection of maximum area size Optimal maximum area size is investigated on both the original data and the augmented data with VTLP, respectively. The result is shown in Fig 3 . It can be seen that when trained on the original data set, the model with the max area size of 4x4 achieved the highest ACC, followed by 3x3. When trained on the augmented data   set, the model with the max area size of 3x3 achieved the highest ACC. In most cases, the use of enhanced data brings an accuracy increase of more than 0.5% absolute. Therefore, we suggest using a max area size of 3x3 and using VTLP for data augmentation. Selection of area features Experiments are conducted to investigate the performance of using various area features. For Key, we selected Max, Mean and Sample; for Value, we selected Max, Mean and Sum.The Sample refers to adding a perturbation proportional to the standard deviation on the basis of Mean when training, which is calculated according to Eq.2 where x is a sample and µ and σ are the mean and standard variance, respectively. ξ is a random variable assuming N (0, 1) distribution. We use K-V to represent the model selected K as Key and V as Value.\n\nx = µ + σ * ξ, where ξ ∼ N (0, 1)\n\nTable  1  shows the result. It can be observed that the Sample-Max achieved the highest ACC and the Sample-Mean achieved the lowest ACC. There is little difference in ACC in other cases. We speculate that it is because perturbed Key in training introduces greater randomness.   2  we can see that when the model becomes stronger, the improvement brought by VTLP marginally decreases. This is because VTLP conducts labelpreserving perturbation to improve the robustness of the classifier. When the model gets stronger with attention or multiscale area attention, the model itself becomes more robust, which may offset to certain degree the impact of VTLP.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Table 3. Accuracy Comparison With Existing Ser Results",
      "text": "Method WA(%) UA(%) Year Attention pooling (P. Li et al.)  [22]  71.75 68.06 2018 CTC + Attention (Z. Zhao et al.)  [23]  67.00 69.00 2019 Self attention (L. Tarantino et al.)  [12]  70.17 Comparison with existing results As shown in Table  3 , we compared our accuracy with other published SER results in recent years. These results use the same data set and the evaluation metrics as our experiment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have applyed multiscale area attention to SER and designed an attention-based convolutional neural network, conducted experiments on the IEMOCAP data set with VTLP augmentation, and obtained 79.34% WA and 77.54% UA. The result is state-of-the-art.\n\nIn future research, we will continue to work along the lines by improving the application of attention on SER and apply more data augmentation methods to SER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of the CNN with attention used as a",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the detailed model structure.",
      "page": 2
    },
    {
      "caption": "Figure 2: , for a continuous memory",
      "page": 2
    },
    {
      "caption": "Figure 2: Generating Area Multiple areas can be generated by",
      "page": 2
    },
    {
      "caption": "Figure 3: Result of modifying max area size It can be seen that when trained on the original data set, the model with the max",
      "page": 3
    },
    {
      "caption": "Figure 4: Result of modifying the amount of data with VTLP",
      "page": 3
    },
    {
      "caption": "Figure 3: It can be seen that when trained on the original data set,",
      "page": 3
    },
    {
      "caption": "Figure 5: Feature representation CNN attends more on areas",
      "page": 4
    },
    {
      "caption": "Figure 4: It can be",
      "page": 4
    },
    {
      "caption": "Figure 5: It clearly shows that compared",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: shows the result. It can be seen that the area",
      "data": [
        {
          "Model": "CNN",
          "WA": "0.7467",
          "UA": "0.7222",
          "ACC": "0.7345"
        },
        {
          "Model": "CNN+VTLP",
          "WA": "0.7891",
          "UA": "0.7683",
          "ACC": "0.7787"
        },
        {
          "Model": "Attention",
          "WA": "0.7807",
          "UA": "0.7628",
          "ACC": "0.7718"
        },
        {
          "Model": "Attention+VTLP",
          "WA": "0.7879",
          "UA": "0.7734",
          "ACC": "0.7807"
        },
        {
          "Model": "Area attention",
          "WA": "0.7911",
          "UA": "0.7705",
          "ACC": "0.7808"
        },
        {
          "Model": "Area attention+VTLP",
          "WA": "0.7934",
          "UA": "0.7754",
          "ACC": "0.7844"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows the result. It can be seen that the area",
      "data": [
        {
          "Method": "Attention pooling (P. Li et al.) [22]",
          "WA(%)": "71.75",
          "UA(%)": "68.06",
          "Year": "2018"
        },
        {
          "Method": "CTC + Attention (Z. Zhao et al.) [23]",
          "WA(%)": "67.00",
          "UA(%)": "69.00",
          "Year": "2019"
        },
        {
          "Method": "Self attention (L. Tarantino et al.) [12]",
          "WA(%)": "70.17",
          "UA(%)": "70.85",
          "Year": "2019"
        },
        {
          "Method": "BiGRU (Y. Xu et al.) [7]",
          "WA(%)": "66.60",
          "UA(%)": "70.50",
          "Year": "2020"
        },
        {
          "Method": "Multitask learning + Attention\n(A. Nediyanchath et al.) [9]",
          "WA(%)": "76.40",
          "UA(%)": "70.10",
          "Year": "2020"
        },
        {
          "Method": "Head fusion (Ours) [10]",
          "WA(%)": "76.18",
          "UA(%)": "76.36",
          "Year": "2020"
        },
        {
          "Method": "Area attention (Ours)",
          "WA(%)": "79.34",
          "UA(%)": "77.54",
          "Year": "2020"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Detection of clinical depression in adolescents' speech during family interactions",
      "authors": [
        "Lu-Shih Alex Low",
        "Margaret Namunu C Maddage",
        "Lisa Lech",
        "Nicholas Sheeber",
        "Allen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "3",
      "title": "Application of speech emotion recognition in intelligent household robot",
      "authors": [
        "Xu Huahu",
        "Gao Jue",
        "Yuan Jian"
      ],
      "year": "2010",
      "venue": "2010 International Conference on Artificial Intelligence and Computational Intelligence"
    },
    {
      "citation_id": "4",
      "title": "A study of speech emotion recognition and its application to mobile services",
      "authors": [
        "Won-Joong Yoon",
        "Youn-Ho Cho",
        "Kyu-Sik Park"
      ],
      "year": "2007",
      "venue": "International Conference on Ubiquitous Intelligence and Computing"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "6",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "Xixin Wu",
        "Songxiang Liu",
        "Yuewen Cao",
        "Xu Li",
        "Jianwei Yu",
        "Dongyang Dai",
        "Xi Ma",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Hgfm: A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Yunfeng Xu",
        "Hua Xu",
        "Jiyun Zou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Attention driven fusion for multi-modal emotion recognition",
      "authors": [
        "Darshana Priyasad",
        "Tharindu Fernando",
        "Simon Denman",
        "Sridha Sridharan",
        "Clinton Fookes"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "Anish Nediyanchath",
        "Periyasamy Paramasivam",
        "Promod Yenigalla"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Improve accuracy of speech emotion recognition with attention head fusion",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Samee U Khan"
      ],
      "year": "2020",
      "venue": "2020 10th Annual Computing and Communication Workshop and Conference (CCWC)"
    },
    {
      "citation_id": "12",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio"
    },
    {
      "citation_id": "13",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "Lorenzo Tarantino",
        "Philip Garner",
        "Alexandros Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Area attention",
      "authors": [
        "Yang Li",
        "Lukasz Kaiser",
        "Samy Bengio",
        "Si Si"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Vocal tract length perturbation (vtlp) improves speech recognition",
      "authors": [
        "Navdeep Jaitly",
        "Geoffrey Hinton"
      ],
      "year": "2013",
      "venue": "Proc. ICML Workshop on Deep Learning for Audio, Speech and Language"
    },
    {
      "citation_id": "16",
      "title": "Data augmentation for deep convolutional neural network acoustic modeling",
      "authors": [
        "Xiaodong Cui",
        "Vaibhava Goel",
        "Brian Kingsbury"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Data augmentation for deep neural network acoustic modeling",
      "authors": [
        "Xiaodong Cui",
        "Vaibhava Goel",
        "Brian Kingsbury"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "William Daniel S Park",
        "Yu Chan",
        "Chung-Cheng Zhang",
        "Barret Chiu",
        "Ekin Zoph",
        "Quoc V Cubuk",
        "Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "arxiv": "arXiv:1904.08779"
    },
    {
      "citation_id": "19",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "Daniel Ellis",
        "Oriol Nieto"
      ],
      "year": "2015",
      "venue": "Python in Science Conference"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Nlp augmentation",
      "authors": [
        "Edward Ma"
      ],
      "year": "2019",
      "venue": "Nlp augmentation"
    },
    {
      "citation_id": "22",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "23",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Lirong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "24",
      "title": "Attentionenhanced connectionist temporal classification for discrete speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Zhongtian Bao",
        "Zixing Zhang",
        "Nicholas Cummins",
        "Haishuai Wang",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}