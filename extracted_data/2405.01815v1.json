{
  "paper_id": "2405.01815v1",
  "title": "Toward End-To-End Interpretable Convolutional Neural Networks For Waveform Signals",
  "published": "2024-05-03T02:24:27Z",
  "authors": [
    "Linh Vu",
    "Thu Tran",
    "Wern-Han Lim",
    "Raphael Phan"
  ],
  "keywords": [
    "speech recognition",
    "audio classification",
    "signal processing",
    "interpretable neural networks",
    "convolutional neural networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper introduces a novel convolutional neural networks (CNN) framework tailored for end-to-end audio deep learning models, presenting advancements in efficiency and explainability. By benchmarking experiments on three standard speech emotion recognition datasets with five-fold crossvalidation, our framework outperforms Mel spectrogram features by up to seven percent. It can potentially replace the Mel-Frequency Cepstral Coefficients (MFCC) while remaining lightweight. Furthermore, we demonstrate the efficiency and interpretability of the front-end layer using the PhysioNet Heart Sound Database, illustrating its ability to handle and capture intricate long waveform patterns. Our contributions offer a portable solution for building efficient and interpretable models for raw waveform data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The use of deep learning and representation learning has significantly impacted the signal processing field, similar to the success observed in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Deep neural networks can automatically learn hierarchical representations from raw data. However, simply using deep learning as a one-tool-fitall solution often leads to poor interpretability, i.e., monitoring how the model works is difficult.\n\nRecent developments have led to the creation of efficient and easy-to-understand models for processing raw waveform signals. Cheuk et al.  [1]  have introduced nnAudio, a framework that uses 1D convolutional neural networks to extract spectrograms on-the-fly. Leiber et al.  [2]  have introduced a differentiable modification of the Short-Time Fourier Transform (STFT) that makes it possible to optimize the window length parameter through gradient descent. These works primarily revolve around rethinking the fusion of spectrograms and neural networks, placing emphasis on employing an adapted spectrogram as the initial layer for input signals. Seki et al.  [3]  showed that Gaussian filters with three trainable parameters of gain, bandwidth, and centre frequency can outperform Melfilterbank in both clean and noise-corrupted environments. Inspired by standard filtering in digital signal processing, Ravanelli and Bengio  [4]  proposed a novel CNN called SincNet that uses rectangular band-pass filters as kernels for the first convolution layer with two learnable parameters: high and low cut-off frequencies. Right from the first layer, SincNet can already learn meaningful filters and thus converge faster than the conventional CNN. In a comparison between different parametric modulated kernel-based filters including SincNet (rectangular filters), Sinc2Net (triangular filters), GammaNet (gammatone filters) and GaussNet (Gaussian filters); it is found that SincNet is the best for the speech recognition task  [5] .\n\nAccording to Ravanelli and Bengio  [4] , the frequency gain of the SincNet filters is only defined in subsequent layers of the neural network, which are conventional CNN layers. To further contribute to this research direction, we propose a new interpretable CNN architecture called IConNet that utilizes a finite impulse response (FIR)-based kernel with learnable window functions. FIR filters play a crucial role in signal processing as they enable the extraction of critical information; and different filter designs can enhance or mitigate unwanted effects. With adaptive window functions, the filters can dynamically adjust to varying signal profiles depending on the data and the problem. The primary benefit of this approach is the transparency in the way the model learns -which frequency bands it focuses on and which will be cut off.\n\nWe illustrate the effectiveness of this approach in two healthrelated problems: speech emotion recognition and abnormal heart sound detection.\n\nIn the next section II, we will describe the proposed method. Section III is about the Speech Emotion Recognition (SER) experiment, followed by section IV about abnormal heart sound detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. The Iconnet Architecture",
      "text": "The proposed Interpretable Convolutional Neural Network (IConNet) architecture in this research is designed to leverage insights from audio signal processing; aiming to improve end-to-end deep neural networks' ability to extract features and patterns from raw waveform signals. The foundation of our method draws inspiration from the standard signal processing process, in which the input audio signal undergoes a windowing operation to segment it into smaller frames and simultaneously mitigate spectral leakage to improve frequency resolution. A key novelty lies in using the Generalized Cosine Window function as parametrization for the convolution kernels to enable the neural networks to choose the most suitable shape for each frequency band.\n\nThe convolution layer of the proposed model has restrictedshaped kernels which is a band-pass filter defined by nonlearnable low cut-off frequency f 0 and frequency bandwidth f δ . The filter shape and frequency gain are determined by the window function W with p learnable parameters ϕ p . Let H = {h k : k = 1, ..., K} denotes the kernel (filter) of width K in the time domain. V n denotes the output of the convolution layer for each n input time-domain value. H(k, f 0 , f δ , ϕ) is parametrically modulated by sinc as a non-learnable function and W with learnable parameters.\n\nThus, for each kernel, there are only p parameters ϕ p and two band parameters f 0 and f δ to train via gradient descent. f 0 and f δ can also be non-learnable, which is how we can incorporate prior domain knowledge into the deep neural network design to solve specific problems and control what information is fed into the model to prevent unwanted effects from the first layers.\n\nAfter separating signals into different channels, we use a downsampling layer to reduce the data in the time domain to a lower sample rate. This compression step decreases the data's dimensions, making it easier to process in subsequent steps while retaining the essential features needed for precise analysis and classification. It's worth noting that the convolution filters from the previous step also serve as anti-aliasing filters like traditional signal downsampling. In the second-to-last step, we use an NLRelu activation function recommended in  [6]  to mimic the amplitude-to-decibel conversion since human hearing functions on a logarithmic scale. Finally, we apply the Local Respond Normalization function  [7]  to the output before forwarding it to the next layer. This entire process can be repeated by stacking these blocks on top of each other to achieve further pattern decorrelation from previous decomposed channels while maintaining a more compressed representation. The outputs of the front-end blocks can be combined together as long as they are resampled to the same sampling rate to preserve the temporal characteristic. Depending on the task, these front-end blocks can be incorporated into any deep neural network architecture.\n\nFigure  1  illustrates the proposed architecture, with the part A on the left describing the front-end block. The middle part B of the figure  1  is the high-level deep neural network architecture that incorporates the front-end block to process raw waveform signal data. Part C depicts a simple classifier block consisting of a pooling layer, a two-layer feed-forward neural network with layer norm and a LeakyRelu activation function. The following experiments are designed to demonstrate the feature extraction ability of the front-end blocks and evaluate the effectiveness of this new architecture.\n\nWe have selected two classification problems and designed a classifier consisting of a pooling layer, a two-layer feedforward neural network with layer norm and a LeakyRelu activation function. In both experiments, we use k-fold crossvalidation settings and report three different metrics: unweighted accuracy (UA), unweighted F1 (UF1) and weighted-F1 (F1). For training the neural networks, we employed RAdam optimizer  [8]  with OneCycleLR learning rate scheduler  [9] , Cross Entropy loss and trained each model to up to 60 epochs on each fold.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Speech Emotion Recognition A. Background",
      "text": "Emotions play a pivotal role in revealing essential cues about a speaker's intentions, attitudes, and mental state, particularly in the context of AI health. This significance is underscored by a recent study from Gheorghe et al.  [10]  where a system leveraging deep neural networks successfully identifies depression from speech samples with an unweighted accuracy of 91.25% using Mel-frequency cepstral coefficients (MFCC). MFCC is the most popular feature based on the short-time Fourier Transform spectrogram and the Mel filterbank, which was designed based on human perception of our hearing system. As humans perceive sound on a logarithmic scale, Mel filterbank uses narrower bandwidths at the lower frequencies to capture more information. Its filters have a triangular shape and are used in many tasks. However, according to  [3] ,  [11] , in the data-driven filterbanks, each filterbank's centre frequencies and shapes are different depending on the tasks and the presence of background noise.\n\nBased on these insights, we've developed three IConNet variants detailed in Table  I . These variants assess the influence of adjustable bands versus adaptive windows on CNN model classification outcomes. The front-end kernels in all IConNet models are initialized with the Mel-filterbank, with additional filters allocated to the lower frequency range, prioritizing crucial speech signals.\n\nB. Experiment setup 1) Datasets: Our first experiment incorporates the proposed method, evaluating its performance on three standard Speech Emotion Recognition (SER) datasets with similar settings described in  [12]   1  . Firstly, the RAVDESS dataset  [13]  features high-quality audio speech with eight emotional expressions from 24 North-American actors. Secondly, the CREMA-D dataset  [14]  comprises recordings from 91 speakers with diverse races and ethnicities, representing real-world audio recordings where the recordings are often noise-corrupted. Lastly, IEMOCAP  [15]  is a popular SER dataset with 12hour conversational speech audio from 10 speakers. We use four classes of data, namely happy, sad, angry, neutral, which are shared between the aforementioned three datasets for a fair comparison.\n\n2) Classifiers and evaluation: For the classifier, we use a dense neural network consisting of two layers with 512 nodes in each layer followed by layer norm, as described in the diagram 1. To evaluate the performance of the proposed models, we adopted 5-fold cross-validation with stratified train-test splitting to ensure the proportion of each class in both sets. We trained each model for up to 60 epochs with early stopping, then evaluated the model on the test set and reported the average metrics across folds. We benchmarked IConNet with different settings against Mel and MFCC features extracted using the TorchAudio library  [16]  with different resolutions as described in the table I.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Experiment Results",
      "text": "Table  II  provides detailed experiment result on the three datasets mentioned above when using early stopping. On the RAVDESS dataset, the IConNet model with 456 adaptivewindow FIR kernels achieved an unweighted accuracy of 66.83%, which is 4.83% higher than the adjustable-band-FIR model with the same number of kernels. When the number of kernels was reduced to 256, the UA of the IConNet-W dropped 1%, while the IConNet-B increased 1.73%. For the IConNet-W models and Mel-spectrogram models, higher resolution helped the models make better predictions. The size of Mel-256/IConNet-256 and Mel-456/IConNet-456 models is 1.6 MB and 2 MB, respectively. Despite having the same size, Mel models performed poorly compared to all IConNet models, even after being trained for more epochs.\n\nThe bar chart 2 compares the F1 scores of Mel-456, MFCC-456, and IConNet-456 models on the RAVDESS and CREMA-D datasets. After 60 epochs, the performance of MFCC on the RAVDESS dataset increased from 46.13% to 67.14%, which is a more than 21% improvement. However, the gain for MFCC-456 model on CREMA-D is less than 1%. The IConNet-W model still gave the highest F1 score on both the RAVDESS and CREMA-D datasets at 70.04% and 65.41%, respectively. The table in II clearly indicates that IConNet-W-456 and MFCC-256 attained the highest unweighted accuracy on IEMOCAP with only a 0.01% difference between them. On average, the IConNet-W models outperformed other IConNet variants by roughly 2%. Overall, IConNet models gave better results than Mel-spectrogram and MFCC models, especially on the CREMA-D dataset.\n\nOn the interpretable ability of the IConNet, Figure  3  demonstrates the alterations in the shape of windows that are used to extract important information from different frequency ranges. The window (a) is learned during the model training process for the low-frequency range and exhibits a complex shape, determining the frequency gain at each point. This complex shape enhances the robustness of the model because the low-frequency range contains crucial speech signals that are susceptible to noise. On the other hand, the window (c) is tailored for the higher frequency range, serving as a narrowband filter that effectively mitigates spectral leakage with its ideal sidelobes.\n\nIn summary, the experiment results have proven the appropriate use of the proposed IConNet, especially the IConNet with adaptive-window FIR kernel for SER. Moreover, the IConNet end-to-end models are portable, with the model size being 30% smaller than the hand-crafted feature set models proposed by Vu et al. in  [12] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Abnormal Heart Sound Detection",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Background",
      "text": "Cardiovascular diseases are a leading global health concern, demanding improved diagnostic precision. A heart murmur is an unusual sound heard during the heartbeat cycle, indicating underlying cardiac conditions. Various ML approaches have been developed to identify heart murmurs from stethoscope recordings. As the recording duration is quite long (30 seconds on average), there are often many preprocessing steps, such as heartbeat segmentation to trim the waveform to a smaller size (3 to 5 seconds) and bandpass filtering as the most critical signals lay in the range from 10 Hz to 400 Hz. After that, MFCC and its derivatives are extracted to apply CNN or LSTM models or a combination of both  [17] ,  [18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Proposed Model",
      "text": "This experiment mainly examines the potential role of the IConNet in refining abnormal heart sound detection. Our proposed model is an end-to-end lightweight neural network architecture that eliminates the need for heart sound segmentation, low-pass filtering and MFCC-based feature extraction. We designed an IConNet-W model with two front-end blocks with 128 and 32 kernels respectively, a max-pooling layer and a 2-layer FFN classifier with 256 nodes on each layer. The total number of parameters is below 200K.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Experiment Setup",
      "text": "We employ the widely-used PhysioNet/CinC Challenge dataset  [19]  for heart sound classification evaluation. This dataset comprises 2575 normal and 665 abnormal samples. To validate the effectiveness of the IConNet in identifying relevant features, we resample the waveform from 2000 Hz to 16000 Hz and conduct 4-fold cross-validation, reporting UA, UF1, and F1 metrics. Deng et al.  [20]  serve as our baseline, utilizing a preprocessing pipeline with a bandpass filter, MFCC features, and a CRNN model consisting of three 2D CNN layers and two LSTM layers. We also include the MFCC performance for comparison. Preprocessing for other models involves waveform trimming and downsampling, excluding the IConNet model.  III , it is clear that the baseline model  [20]  performed better than the MFCC + FFN model, thanks to its preprocessing steps that included bandpass filtering and the use of MFCC deltas. The baseline model achieved 90.06% F1 score. However, our proposed architecture surpassed both models with an F1 score of 92.05%, which is 2% higher than the baseline model. While this result still not yet outperforms the state-of-the-art Resnet result reported by Li et al. in  [18] , it has successfully demonstrated the effectiveness of our proposed method in classifying heart  V. CONCLUSIONS Our proposed framework introduces a novel method for constructing end-to-end audio deep-learning models, showcasing its efficacy in healthcare applications such as speech emotion recognition and abnormal heart sound detection. Our findings reveal that the proposed CNN framework surpasses traditional methods utilizing the Mel spectrogram, and potentially MFCC (further experimentation required for confirmation), for both tasks. Moreover, visualization of CNN kernels underscores the value of transparent CNN architectures in healthcare settings, shedding light on the features extracted from input signals in mission-critical tasks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Experiment Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Based On The Results Presented In Table",
      "text": "Insightful observations arise regarding the front-end layer within our proposed framework. Our results indicate that frontend layers featuring learnable windows demonstrate superior performance compared to those employing learnable bands, diverging from the predominant focus in existing literature. We suggest exploring methods to incorporate prior knowledge into the front-end layer to improve performance, especially with the learnable window model. While the proposed framework offers promising results, it requires slightly more parameters and, thus, more training resources.\n\nIn conclusion, this study emphasizes the advantages of employing interpretable CNN for analyzing raw waveform data, showcasing their potential benefits in healthcare settings to foster transparency and trust in medical AI applications.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed IConNet architecture for end-to-end",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates the proposed architecture, with the part A",
      "page": 2
    },
    {
      "caption": "Figure 1: is the high-level deep neural network architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: Result on RAVDESS and CREMA-D datasets after 60",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison of Window Shape and Frequency Re-",
      "page": 3
    },
    {
      "caption": "Figure 3: demonstrates the alterations in the shape of windows that are",
      "page": 4
    },
    {
      "caption": "Figure 4: Frequency response of filters from different bands",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "nnaudio: An on-the-fly gpu audio to spectrogram conversion toolbox using 1d convolutional neural networks",
      "authors": [
        "K Cheuk",
        "H Anderson",
        "K Agres",
        "D Herremans"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "2",
      "title": "A differentiable short-time fourier transform with respect to the window length",
      "authors": [
        "M Leiber",
        "A Barrau",
        "Y Marnissi",
        "D Abboud"
      ],
      "year": "2022",
      "venue": "2022 30th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "3",
      "title": "A deep neural network integrated with filterbank learning for speech recognition",
      "authors": [
        "H Seki",
        "K Yamamoto",
        "S Nakagawa"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Interpretable convolutional filters with sincnet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "Proc. of IRASL@NIPS"
    },
    {
      "citation_id": "5",
      "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
      "authors": [
        "E Loweimi",
        "P Bell",
        "S Renals"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Natural-logarithm-rectified activation function in convolutional neural networks",
      "authors": [
        "Y Liu",
        "J Zhang",
        "C Gao",
        "J Qu",
        "L Ji"
      ],
      "year": "2019",
      "venue": "\" in 2019 IEEE 5th International Conference on Computer and Communications (ICCC)"
    },
    {
      "citation_id": "7",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "On the variance of the adaptive learning rate and beyond",
      "authors": [
        "L Liu",
        "H Jiang",
        "P He",
        "W Chen",
        "X Liu",
        "J Gao",
        "J Han"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Learning Representations"
    },
    {
      "citation_id": "9",
      "title": "in Artificial intelligence and machine learning for multi-domain operations applications",
      "authors": [
        "L Smith",
        "N Topin"
      ],
      "year": "2019",
      "venue": "in Artificial intelligence and machine learning for multi-domain operations applications"
    },
    {
      "citation_id": "10",
      "title": "Using deep neural networks for detecting depression from speech",
      "authors": [
        "M Gheorghe",
        "S Mihalache",
        "D Burileanu"
      ],
      "year": "2023",
      "venue": "2023 31st European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "11",
      "title": "Optimization of data-driven filterbank for automatic speaker verification",
      "authors": [
        "S Sarangi",
        "M Sahidullah",
        "G Saha"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Improved speech emotion recognition based on music-related audio features",
      "authors": [
        "L Vu",
        "R -W. Phan",
        "L Han",
        "D Phung"
      ],
      "year": "2022",
      "venue": "2022 30th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "13",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "14",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "15",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "16",
      "title": "Torchaudio: Building blocks for audio and speech processing",
      "authors": [
        "Y.-Y Yang",
        "M Hira",
        "Z Ni",
        "A Chourdia",
        "A Astafurov",
        "C Chen",
        "C.-F Yeh",
        "C Puhrsch",
        "D Pollack",
        "D Genzel",
        "D Greenberg",
        "E Yang",
        "J Lian",
        "J Mahadeokar",
        "J Hwang",
        "J Chen",
        "P Goldsborough",
        "P Roy",
        "S Narenthiran",
        "S Watanabe",
        "S Chintala",
        "V Quenneville-Bélair",
        "Y Shi"
      ],
      "year": "2021",
      "venue": "Torchaudio: Building blocks for audio and speech processing",
      "arxiv": "arXiv:2110.15018"
    },
    {
      "citation_id": "17",
      "title": "Lightweight end-toend neural network model for automatic heart sound classification",
      "authors": [
        "T Li",
        "Y Yin",
        "K Ma",
        "S Zhang",
        "M Liu"
      ],
      "year": "2021",
      "venue": "Information"
    },
    {
      "citation_id": "18",
      "title": "Heart sound classification based on improved mel-frequency spectral coefficients and deep residual learning",
      "authors": [
        "F Li",
        "Z Zhang",
        "L Wang",
        "W Liu"
      ],
      "year": "2022",
      "venue": "Frontiers in Physiology"
    },
    {
      "citation_id": "19",
      "title": "An open access database for the evaluation of heart sound algorithms",
      "authors": [
        "C Liu",
        "D Springer",
        "Q Li",
        "B Moody",
        "R Juan",
        "F Chorro",
        "F Castells",
        "J Roig",
        "I Silva",
        "A Johnson"
      ],
      "year": "2016",
      "venue": "Physiological measurement"
    },
    {
      "citation_id": "20",
      "title": "Heart sound classification based on improved mfcc features and convolutional recurrent neural networks",
      "authors": [
        "M Deng",
        "T Meng",
        "J Cao",
        "S Wang",
        "J Zhang",
        "H Fan"
      ],
      "year": "2020",
      "venue": "Neural Networks"
    }
  ]
}