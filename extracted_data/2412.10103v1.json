{
  "paper_id": "2412.10103v1",
  "title": "Amused: An Attentive Deep Neural Network For Multimodal Sarcasm Detection Incorporating Bi-Modal Data Augmentation",
  "published": "2024-12-13T12:42:51Z",
  "authors": [
    "Xiyuan Gao",
    "Shubhi Bansal",
    "Kushaan Gowda",
    "Zhu Li",
    "Shekhar Nayak",
    "Nagendra Kumar",
    "Matt Coler"
  ],
  "keywords": [
    "Sarcasm detection",
    "multimodality",
    "data augmentation",
    "attention mechanisms",
    "speech synthesis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Detecting sarcasm effectively requires a nuanced understanding of context, including vocal tones and facial expressions. The progression towards multimodal computational methods in sarcasm detection, however, faces challenges due to the scarcity of data. To address this, we present AMuSeD (Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating bi-modal Data augmentation). This approach utilizes the Multimodal Sarcasm Detection Dataset (MUStARD) and introduces a two-phase bimodal data augmentation strategy. The first phase involves generating varied text samples through Back Translation from several secondary languages. The second phase involves the refinement of a FastSpeech 2-based speech synthesis system, tailored specifically for sarcasm to retain sarcastic intonations. Alongside a cloud-based Text-to-Speech (TTS) service, this Fine-tuned FastSpeech 2 system produces corresponding audio for the text augmentations. We also investigate various attention mechanisms for effectively merging text and audio data, finding self-attention to be the most efficient for bimodal integration. Our experiments reveal that this combined augmentation and attention approach achieves a significant F1score of 81.0% in text-audio modalities, surpassing even models that use three modalities from the MUStARD dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S ARCASM is a complex linguistic phenomenon that is not straightforward to interpret. It frequently communicates feelings that are the opposite of what the words literally mean, using a combination of verbal and non-verbal signals. This complexity poses significant challenges for computational detection, requiring an approach that integrates multiple modes of information. Sarcasm's multidisciplinary nature spans various fields, including linguistics and psychology. Research in these fields has identified cues that are essential for understanding of sarcasm, including tone of voices or facial expressions  [1] , exaggeration of statements  [2] , expectations and the common ground between speakers and hearers  [3] .\n\nManuscript submitted on June 13, 2024. (Corresponding author: Xiyuan  Gao)  X.Gao, Z.Li, S.Nayak and M.Coler are with Campus Fryslân, University of Groningen, Leeuwarden 8911 CE, the Netherlands (e-mails: xiyuan.gao@rug.nl; zhu.li@rug.nl; s.nayak@rug.nl; m.coler@rug.nl) S.Bansal and N.Kumar are with Computer Science and Engineering, Indian Institute of Technology Indore, Indore 453552, India (e-mail: phd2001201007@iiti.ac.in; nagendra@iiti.ac.in).\n\nK.Gowda was with Computer Science and Engineering, Indian Institute of Technology Indore, Indore 453552, India. He is now with Computer Science, Columbia University, New York 10027, USA (e-mail: kg3081@columbia.edu) Drawing insights from these disciplines is crucial for the computational analysis of sarcasm. To better understand sarcasm's nuanced attributes, we present an example from the TV sitcom 'Friends', as depicted in Figure  1 . In this case, the character Chandler feigns surprise in such a way that the listener knows that he is being insincere. We highlight two key aspects of sarcasm: first, there is a divergence between the speaker's actual intention and the literal meaning of their words; second, as Sperber and Wilson  [4]  pointed out, sarcasm serves as a tool to communicate underlying opinions. This leads us to another aspect of sarcasm: the importance of context in deciphering the speaker's intent. As illustrated in Figure  1 , without context, in this case, an unusual flat tone and exaggerated facial expression, Chandler's comment could be misconstrued as genuine surprise upon seeing Joey emerge from the box. However, the sarcastic nature of his statement becomes clear with his tone and facial expression. Context in sarcasm often involves explicit cues like variations in tone, exaggerated emphasis, extended syllables, or a serious facial expression. It can also be implicit, drawing on shared cultural understanding  [5] ,  [6] .\n\nSarcasm represents a significant challenge particularly in fields like Sentiment Analysis and Speech Emotion Recognition (SER). The rise of rich multimedia communication platforms has highlighted the shortcomings of unimodal analyses, thereby shifting the focus towards multimodal methods that integrate text, visual, and audio data. A unimodal approach can erroneously classify utterances, such as the one illustrated in Figure  1 , as \"surprise\" or \"positive\" without taking into account contextual cues, such as tone of voice or facial expressions, derived from the audio and visual modalities. Multimodal sarcasm detection, such as that of Schifanella et al.  [7]  delved into the complex interplay between text and visual data in multimodal contexts, emphasizing the need for a multimodal strategy. An important development in this area was the creation of the MUStARD dataset by Castro et al.  [8] , as it is a primary multimodal dataset containing sarcasm in conversations. This dataset, featuring text, audio, and video data from American TV sitcoms, proved the viability and effectiveness of multimodal data in sarcasm detection. Subsequent research has concentrated on refining methods for integrating these different modalities and improving fusion techniques to enhance detection accuracy. For instance, Wu et al.  [9]  introduced an incongruity-aware attention network that assesses discordance across modalities, prioritizing segments with higher incongruity. This approach, when applied to the MUStARD dataset, significantly improved sarcasm detection. Similarly, Pramanick et al.  [10]  used the same dataset to implement a self-attention mechanism  [11]  that captures both intra-modal and cross-modal relationships.\n\nDespite the progress in multimodal research, the challenge of data scarcity remains a significant barrier. A notable instance of this is the MUStARD dataset, which only includes about 690 utterances, spanning multiple modalities, and with an average audio clip duration of merely 5.2 seconds, cumulatively amounting to less than one hour of speech data. Our approach aims to mitigate this limitation by introducing bimodal data augmentation techniques and enhancing feature fusion through attention mechanisms. Data augmentation is widely recognized for increasing model robustness and preventing overfitting. While it has been extensively applied to unimodal data  [12] -  [15] , its application in bimodal data augmentation remains relatively unexplored  [16] ,  [17] . To our knowledge, the intersection of text and audio in the context of bimodal data augmentation is yet to be fully explored. Moreover, the use of self-attention mechanisms in processing extracted features has been shown to be an effective strategy in recent studies  [10] ,  [18] ,  [19] . These mechanisms are instrumental in identifying and enhancing features that are crucial for sarcasm detection in each modality during the training process, thereby enhancing overall model performance. Building on these foundations, our research proposes the following research questions (RQs).\n\nRQ1: Does our innovative text-audio data augmentation approach enhance the detection of sarcasm in a multimodal context, and which key factors primarily determine its success? RQ2: How does incorporating self-attention mechanisms with our augmentation strategy refine the model's performance, thus pushing the boundaries of multimodal sarcasm detection?\n\nTo address RQ1, we implement a novel data augmentation technique, using Back Translation to expand our text corpus, and speech synthesis to augment our audio data. We focus on two critical aspects: the quantity of augmented data and the effectiveness of fine-tuned speech synthesizers compared to standard models. Addressing RQ2 involves evaluating the influence of the self-attention mechanism on our augmented dataset, particularly its ability to selectively enhance features relevant to sarcasm detection. The main contributions of this research work are:\n\n• We develop a text-audio bimodal data augmentation tech-nique that significantly enhances sarcasm detection. • We introduce AMuSeD, a groundbreaking framework for multimodal sarcasm detection, achieving an F1-score of 81.0% on the MUStARD dataset. The structure of the paper is as follows: Section II provides an overview of the relevant literature, focusing on multimodal sarcasm detection and bimodal data augmentation. Section III details our proposed methodology, including the augmentation process, feature extraction, and fusion. Section IV describes the experimental setup and presents the findings. Section V discusses these results. The paper concludes with Section VI.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "We categorize this review into two parts. The first explores multimodal sarcasm detection, examining the integration of textual and auditory data. This area presents unique challenges and opportunities for methodological innovation. In the second part, we assess the efficacy of our approach by exploring advanced text and audio data augmentation techniques and their impact on each modality.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Sarcasm Detection",
      "text": "In multimodal sarcasm detection, we find that integrating diverse communication modes (text, audio, visual cues) yields a more nuanced understanding of sarcasm. The integration of multiple modalities has been a major focus of research in multimodal sarcasm detection. In this section, we discuss multimodal sarcasm detection based on three major fusion approaches: a late fusion approach, an early fusion approach, and attention-based fusion.\n\n1) Late Fusion Approach: In late fusion, text and audio features are processed independently and then combined for final classification. Schifanella et al.  [7]  employed this method in their study on the interplay of text and images on social media. They noted the crucial role of these modalities in conveying contrasting messages for sarcasm detection. Utilizing MUStARD dataset, Ding et al.  [20]  proposed a fusion framework that concatenated the post-processed features from three modalities (i.e., text, audio, video). For each of the modality, features were extracted and went through a fullyconnected neural network called SubNet. Late fusion approach offers flexibility, allowing each modality to utilize optimal models for feature extraction and classification. However, it may not fully capture the interactions between low-level features across different modalities.\n\n2) Early Fusion Approach: This method involves merging features from various modalities prior to model training. For instance, Castro et al.  [8]  combined text, audio, and video features, feeding them into a Support Vector Machine (SVM) to assess the efficacy of multimodal data integration in sarcasm detection. Hiremath and Patil  [21]  concatenated text, audio, and video features and sent them to a fully-connected neural network to classify sarcasm and non-sarcasm utterances. This approach enables the model to identify correlations between low-level features. Yet, integrating disparate features effectively remains a challenge.\n\n3) Attention-based Fusion: Increasingly popular in multimodal sarcasm detection, attention-based fusion focuses on the relationships within and between modalities using attention mechanisms. It assigns greater weights to more closely related features, potentially enhancing classification accuracy. For example, Chauhan et al.  [22]  applied attention mechanisms to learn the relationship between the feature vector of a segment of an utterance in one modality and another segment of the utterance in a different modality. Zhang et al.  [23]  utilized contrastive attention mechanism to capture the irrelevance between two feature vectors from different modalities. In addition, Zhang et al.  [24]  employed a self attention mechanism to capture the important contextual information of the targeting utterance in each modality, and further built a crossmodal attention mechanism among three modalities to enhance the sarcasm-related features. Attention-based fusion offers nuanced feature weighting but requires careful calibration to ensure efficacy.\n\nWe contend that the complexity of sarcasm transcends textual analysis, and requires an intricate fusion of multiple communicative cues. The challenge lies not only in capturing the essence of sarcasm from each individual modality -be it text, audio, or visual cues -but also in effectively synthesizing these disparate elements. Exploring the integration of these modalities offers insights into the potential of multimodal analysis in overcoming the limitations of unimodal methods, paving the way for more sophisticated and accurate sarcasm detection systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Data Augmentation In Multimodal Sarcasm Detection",
      "text": "Data augmentation, a process of generating additional samples by transforming existing training data, plays a crucial role in enhancing machine learning models' generalization capabilities and robustness. Its significance spans various domains, including image processing, computer vision, audio, and text processing. This subsection considers textual data augmentation, audio data augmentation, and multimodal data augmentation, respectively.\n\n1) Textual Data Augmentation: In text processing, diverse techniques are employed for specific objectives. For instance, Back Translation, popularized by Rico et al.  [12] , involves translating text to a different language and then back to the original. This method maintains the linguistic essence while altering word choice and syntax, proving effective in maintaining label consistency and paraphrasing. Aroyehun and Gelbukh  [25]  and Marivate and Sefara  [13]  used this technique with languages like French, Spanish, and German to enhance datasets in aggression and hate speech detection. Lee et al.  [26]  applied it in sarcasm detection to generate non-sarcastic samples, showcasing its versatility across various natural language processing (NLP) applications.\n\n2) Audio Data Augmentation: In sarcasm detection, Gao et al.  [27]  demonstrated the effectiveness of speed and volume perturbations. While sarcasm-specific research in audio augmentation is limited, related areas like Automatic Speech Recognition and SER have extensively used methods like time shift, pitch shift, and random noise injection. Advances in speech synthesis, such as Generative Adversarial Networks and Tacotron2, offer novel augmentation possibilities. Li et al.  [15]  initiated this approach, employing Tacotron 2 as the TTS model to capture speaker identities from the multispeaker LibriTTS dataset  [28] . Subsequently, Rosenberg et al.  [29]  uses a Tacotron 2-based multispeaker model upon the dataset LibriTTS in different domains, to demonstrate the validity of applying this augmentation method in domain transfer. Notably, synthesized speech still shows performance gaps compared to human speech in recognition tasks, but exhibits promise in replicating specific speech characteristics like sarcasm.\n\n3) Multimodal Data Augmentation: In multimodal sarcasm detection, challenges persist due to data scarcity. Huang et al.  [30]  introduced a method for augmenting data in multimodal emotion classification by segmenting original sequences, though this may overlook inherent sequence associations. In the text-image domain, Xu et al.  [16]  and Ruixue et al.  [17]  employed generative networks for creating text-image pairs and augmenting image data, demonstrating the significance of semantic consistency in applications like Visual Question Answering. Hao et al.  [31]  introduced MixGen, which generates augmented samples by combining images and text. However, text-audio augmentation specifically for sarcasm detection remains unexplored.\n\nWhile attention mechanisms in modality fusion reveal intricate inter-modal relationships, data augmentation techniques like Back Translation and synthesized speech show potential in enhancing model performance. The synergistic application of these methods in text-audio bimodal data augmentation, particularly in sarcasm detection, holds promise for future research.\n\nTransitioning from the foundational understanding of multimodal sarcasm detection, and the challenges and innovations highlighted in bimodal data augmentation and feature fusion, next we present our methodology. This methodology encapsulates the strategic integration of text and audio modalities, employing advanced techniques in data augmentation and feature extraction. Our approach, Attentive Deep Neural Network for MUltimodal Sarcasm dEtection incorporating Bi-modal Data Augmentation (AMuSeD), leverages the strengths of both text and audio data to capture the multifaceted nature of sarcasm more effectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "We introduce an Attentive deep neural network for MUltimodal Sarcasm dEtection incorporating bi-modal Data augmentation (AMuSeD), a sarcasm detection methodology that integrates text and audio modalities. The architecture of AMuSeD is represented in Figure  2 , which provides an overview of the system architecture and components. This approach comprises the following steps: For text, we utilize Back Translation as described by Rico et al.  [12] . For audio, we employ speech synthesis to augment the dataset. 2) Feature extraction: Leveraging state-of-the-art pretrained models, we extract features from each modality. BERT  [32]  is for text to derive high-dimensional embeddings capturing linguistic nuances. For audio, we utilize VGGish  [33] , a model adept at extracting salient audio embeddings. 3) Feature fusion: The core of our methodology lies in the fusion of extracted features. We employ a selfattention mechanism, augmented with skip connections, to synergize the text and audio embeddings. This step allows the model to focus on relevant features from both modalities, enhancing its ability to detect sarcasm. 4) Prediction: The fused features are then concatenated and passed through a fully-connected layer. This layer serves as the decision-making component, outputting the final sarcasm label. This remainder of this section is structured as follows: Subsection A describes our data augmentation strategy. Subsection B explains the feature extraction method. In addition, subsection C illustrates the mechanisms of fusion and classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Data Augmentation Strategy",
      "text": "Our data augmentation approach addresses data scarcity challenges in sarcasm detection by synchronizing text and audio modalities to maintain semantic coherence, as depicted in Figure  3 . This strategy involves the following three steps.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1)",
      "text": "Step 1: Text Augmentation with Back Translation: This process involves generating paraphrased versions of the original sarcastic statements by translating them into a different language and then re-translating them back to the original language. The rationale behind using Back Translation lies in its effectiveness in preserving the sarcasm inherent in the text. Our initial training dataset consists of 690 samples. The back-translation process itself is bifurcated into two primary phases. The first phase involves translating the original text t o into a secondary language. To those ends, we employ backtranslation, as described below:\n\nHere, t o denotes the original English language sample (source language) that is translated into a set of chosen target languages, represented by L. The notation t L b signifies the samples that have been back-translated from these target languages L to English. In our study, L encompasses Greek (Gr), German (Ge), French (F r), and Italian (Ita) as the target languages. These languages were selected because languages closely resembling English tend to yield numerous duplicates post back-translation, diminishing the effectiveness of the experiment. Conversely, languages very remote from English could introduce significant translation errors. Thus, the chosen languages are an attempt to balance minimizing duplicates and translation inaccuracies.\n\nOnce the translation is completed, we reverse the process, translating the data back to English. We utilize Googletrans 1 , a tool that leverages the Google Translate Ajax API, for 1 https://pypi.org/project/googletrans/ translating between the source and target languages. Googletrans employs Google's neural machine translation, which uses a transformer-based encoder and a Recurrent Neural Network decoder. Our initial dataset comprised 690 samples. Post back-translation, we obtained 690 samples for each secondary language, including duplicates that emerged during the process. To ensure model robustness and prevent bias, we systematically removed these duplicates across languages by iteratively comparing utterance pairs. See Table  I .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "2)",
      "text": "Step 2: Speech Augmentation: Our approach to speechbased data augmentation uses speech synthesis and involves generating natural-sounding speech from text. Given the scarcity of sarcastic speech data for training a TTS system from scratch, we explore two alternative speech synthesis models: Amazon Polly and FastSpeech 2.\n\nAmazon Polly (AP) 2 : AP is a neural TTS service by Amazon Web Services. It converts text into lifelike speech across various languages and voices using advanced deep learning techniques. Although the exact size and composition of AP's training dataset are undisclosed, it is known to be extensive, incorporating multilingual and multi-speaker data 2 https://docs.aws.amazon.com/polly/latest/dg/what-is.html to achieve natural-sounding speech synthesis. AP offers a range of languages, male and female voices, accents, and tones. For our study, we selected four voice identities: Emma (English-GB), Sally (English-US), Brian (English-GB), and Joey (English-US), with the first two being female and the latter two male voices.\n\nFastSpeech 2 (FS2): FS2 proposed by Chien et al.  [34] , is a sequence-to-sequence TTS model that predicts mel spectrogram frames directly, facilitating parallelization and faster synthesis. Its architecture includes an encoder for text encoding, a duration predictor for phoneme timing, and a decoder for generating the mel spectrogram frames. We first pre-trained FS2 on a diverse dataset, such as LibriTTS, to generate highquality speech. We then fine-tuned it on the MUStARD dataset to enhance its capability in conveying sarcastic features.\n\nWith the following content, we provide the implementation details of FS2:\n\n• Pre-training FS2: We used a Pre-trained FS2 model to generate speech for sarcasm detection, initially pretraining it on the LibriTTS dataset, which contains recordings from multiple speakers. The synthesized speech was based on back-translated text. • Fine-tuning FS2: This process involved refining the Pretrained FS2 model using the MUStARD dataset. We resampled all speech to 22.05kHz and extracted 80dimensional mel-spectrograms with a filter length of 1,024 and hop length of 256. Phoneme durations were obtained using the Montreal Forced Aligner tool  3  . Employing an open-source FS2 model  4  , we conducted 800,000 iterations for pre-training and 200,000 for fine-tuning.\n\nThe training used the Adam  [35]  optimizer with specific parameters β 1 = 0.9, β 2 = 0.98, ϵ = 10 -9 , and a warmup strategy was implemented for the first 4,000 iterations. Additionally, a well-trained HiFi-GAN  [36]  was utilized as the vocoder. 3) Step 3: Text-audio Biomodal Data Augmentation: Our approach to text-audio bimodal data augmentation involves an initial phase of text back-translation in various secondary languages L = {Gr, Ge, F r, Ita}, resulting in augmented texts t L b . Subsequently, corresponding audio samples a L are generated using Amazon Polly (AP) and FastSpeech 2 (FS2).\n\nTo investigate RQ1, which pertains to the factors influencing the efficacy of our data augmentation method, we create augmented audio datasets of various sizes to examine the impact of data volume. These datasets encompass 4-fold, 16fold, and 20-fold augmentation. It is critical to align all audio samples with their corresponding text during augmentation. For computational efficiency, we restrict the comparison of the three selected synthesis methods (AP, Pre-trained FS2, Finetuned FS2) to the 4-fold augmented dataset.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "4-Fold Augmentation:",
      "text": "This dataset aims to compare the effectiveness of different speech synthesizers, comprising three types of audio samples:\n\n• 2,528 samples generated by applying four AP speaker IDs (Brian, Emma, Joey, Salli) to 632 back-translated texts in Latin 5  • 2,092 samples produced using the Pre-trained FS2, derived from back-translated texts in Greek, German, French, and Italian. • An equivalent set of 2,092 samples created with the Finetuned FS2 model, using the same secondary languages. 16-fold Augmentation: This dataset, comprising 8,368 samples, is generated using the same four AP speaker IDs (Brian, Emma, Joey, Salli), based on 2,092 back-translated texts in Greek, German, French, and Italian.\n\n20-fold Augmentation: The largest dataset, with 10,460 samples, is formed by combining all audio from the 4-fold Fine-tuned FS2 samples and the 16-fold AP samples. This dataset aims to test the compatibility of audios synthesized by different models.\n\nIt is important to note that the terminology used (e.g., 4fold) does not directly reflect the literal sample count, due to the removal of duplicates in the text modality. To ensure clarity and transparency, we have described the terminology and corresponding sample numbers in Table  II .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Feature Extraction Method",
      "text": "In our multimodal approach to sarcasm detection, we employ distinct feature extraction techniques for both textual and audio data. These techniques are pivotal in capturing the nuanced characteristics of sarcasm from each modality. Below, we detail the specific methods used for extracting features from text and audio inputs, highlighting their unique roles in our analysis.\n\n1) Textual Feature Extraction: For encoding textual data, we utilize the Bidirectional Encoder Representations from Transformers (BERT)  [32] , pre-trained on the Toronto BookCorpus 6  (800M words) and English Wikipedia  7  (2,500M words). BERT is instrumental in our approach due to its proven capability in understanding and processing natural language, making it highly effective for generating context-aware embeddings for English text. This effectiveness in capturing textual nuances is critical, as demonstrated in various multimodal sarcasm detection studies  [8] ,  [10] ,  [20] . vii In the preprocessing phase, each text input t i = {t o , t L b }, is framed with (CLS) and (SEP ) tokens to mark the start and end. This procedure is described as:\n\nHere, B denotes the token set derived from the input text sample. We standardize the token sequence length (S) to a maximum of 20, determined by calculating the mean of the second and third quartiles of the lengths of all text samples. The tokenized input is then processed through the BERT encoder:\n\nSince the BERT model generates a 768-dimensional vector for each token, the resultant textual feature matrix is T f ∈ R S×D , where, S = 20 is the number of tokens and D = 768, represents the embedding dimension for every token. The textual feature matrix is later forwarded to the attention module for further processing.\n\n2) Audio Feature Extraction: Our audio feature extraction leverages VGGish  [33] , a Convolutional Neural Network (CNN)-based model specifically designed for audio tasks, pretrained on the expansive YouTube-8M dataset 8  . VGGish's effectiveness in audio classification, particularly in sarcasm detection, has been validated in prior research  [24] ,  [27] ,  [37] ,  [38] . For each audio input a i = {a o , a L AP , a L F S }, we apply VGGish as follows:\n\nEach audio input a i undergoes resampling to 16 kHz, mono format, and conversion into a mel-spectrogram using a 25 ms window and 10 ms hop length. A 64 mel bin is applied to this spectrogram, followed by a log 2 transformation. The data is divided into 0.96-second segments of 96 × 64 dimensions. These segments are processed through VGGish, where we utilize its pre-trained weights, excluding the last two fullyconnected layers, to extract a feature matrix A f of dimensions 24 × 512. This matrix is then prepared for integration with the textual features.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Feature Fusion And Classification",
      "text": "Self-attention models are a class of neural networks that use attention mechanisms to discern relationships within a sequence. Unlike traditional models, they do not depend on recurrence or convolution. Their capability to capture longrange dependencies and parallelize computations enhances both accuracy and computational efficiency. However, these models also present challenges, such as increased memory and computational demands, sensitivity to positional encoding, and limited interpretability. The self-attention mechanism functions by generating a weight vector, which is then applied to the hidden embeddings within the sequence. This process prioritizes more significant elements in the sequence.\n\nIn our experimental approach, we first adjust the dimensions of the textual feature matrix T f from  (20, 768)  to  (20, 512)  to align with the dimensions of the acoustic features. This adjustment to match the dimensions of the acoustic features is essential, as maintaining a consistent feature vector length across different modalities is necessary before implementing the self-attention mechanism. The self-attention mechanism is integrated into our model architecture to enable the model to selectively attend to different parts of the input sequence for each element in the output sequence. This enhances the model's ability to capture dependencies across varying distances in sequential data. Let M be the modality-specific feature matrix where M ∈ R N ×512 , and m is the modality indicator where m ∈ {a, t}. Here, a and t denote the acoustic and textual modality respectively.\n\nThe detailed procedure of the self-attention mechanism is illustrated below. The input matrix M is linearly transformed into three matrices Q, K, and V using the learnable weight matrices W Q , W K , and W V respectively.\n\nThen, attention scores are calculated by taking the dot product of the query Q and key matrices K, scaled by √ d k , where d k is the dimension of the key vectors.\n\nThe final output is obtained by applying the attention scores to the value matrix V and linearly transforming the result using a weight matrix\n\nHere, W O is another learnable matrix. Finally, we use skip connections to obtain the final feature vector ( ms ). Skip connections, used alongside attention mechanisms, ensure that while the network focuses on specific features, it also retains the broader context from the original input, leading to more balanced and effective learning.\n\nWe apply this mechanism individually on the textual and acoustic modality to obtain the self-attended textual feature vector ( ts ∈ R 512 ) and self-attended acoustic feature vector ( ãs ∈ R 512 ) respectively. The attended representations from both text and audio modalities are then concatenated as shown in Equation  13 . p = ts ⊕ ãs\n\nHere, ⊕ signifies concatenation, producing an overall feature vector p with a dimension of 1024. This combined feature vector is then fed into fully-connected layers. The training process involves updating gradients based on the binary crossentropy loss function, and predictions about the sarcasm class are determined using a sigmoid operation:\n\nIn this equation, y represents the predicted class, and p is the concatenated feature vector.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we provide a detailed overview of our experimental implementation and present the results of the experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "This subsection outlines our experimental framework, detailing the dataset utilized, comparison benchmarks, training specifics, evaluation metrics.\n\n1) Dataset Description: Our experiments were conducted using the MUStARD dataset  [8] , comprising 690 audiovisual utterances from American TV sitcoms, evenly split between sarcastic and non-sarcastic instances. This multimodal dataset is particularly suited for sarcasm detection as it includes contextual information and annotated sarcasm labels. Additionally, it addresses speaker bias by including samples from the same speakers across both sarcasm categories. To investigate the impact of varying data augmentation volumes, we created augmented datasets using bimodal data augmentation techniques and evaluated different audio synthesis methods. Table  III  provides a comprehensive overview of the data used in our experiments.\n\n2) Benchmarked Methods: The methods against which AMuSeD is benchmarked follow:\n\nSVM  [39] : SVM is a widely recognized binary linear classifier in multimodal fusion research, known for its robustness and effectiveness in binary classification tasks.\n\nInter-segment Inter-modal Attention and Intra-segment Inter-modal Attention (Ie Attention & Ia Attention)  [22] : These dual attention mechanisms focus on capturing relationships both within (intra) and across (inter) different modalities. Their outputs are applied sequentially, concatenated, and then used as input for a softmax layer, which performs the final classification task.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Contrastive-Attention-Based",
      "text": "Sarcasm Detection (ConAttSD)  [23] : ConAttSD leverages a unique intermodality contrastive attention mechanism. This method emphasizes the importance of incongruity between different modalities, which is a crucial aspect of sarcasm detection.\n\nM2Seq2Seq  [38] : M2Seq2Seq is designed with dual attention mechanisms within its encoder. It enables concurrent learning of both intra-modal and inter-modal dynamics, leading to multi-label classification that encompasses sentiment, emotion, and sarcasm.\n\nGated Emoji-aware Multimodal Attention (GEMA)  [40] : GEMA is an innovative deep learning framework that integrates emoji-awareness into multimodal multitasking. It assigns greater significance to features linked with emojis, thus enhancing the relevancy of input data for the softmax classification layer.\n\n3) Training Details: We train our post self-attention features by a fully-connected neural network. Table  IV  shows the hyper-parameters used in our training setup.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "4) Evaluation Metrics:",
      "text": "The evaluation of our model employs standard metrics: precision (P), recall (R), and F1-score (F1), all of which hinge on the counts of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). These metrics are widely used in sarcasm detection research and have been consistently applied in studies using the MUStARD dataset, facilitating a reliable comparison of our approach against existing methods [8]-  [10] . In alignment with the evaluation protocol established by Castro et al.  [8] , our experiment uses a 5-fold cross-validation method. The dataset is split according to a predefined file accessible on GitHub  9  . Our evaluation follows a speaker-dependent model, where the same speakers appear in both training and testing datasets, a common aspect in sarcasm detection research.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B. Experimental Results",
      "text": "This subsection provides a detailed analysis of the experiments conducted to evaluate the effectiveness of AMuSeD. We compare its performance against state-of-the-art methods, focusing on factors such as the volume of augmented data, the impact of different speech synthesizers, and the efficiency of diverse attention mechanisms.\n\n1) Overall Model Performance: AMuSeD, which uses both text and audio modalities augmented through our bimodal data augmentation process, is evaluated against different benchmark methods. The comparative results are presented in Table V. Notably, our approach achieves an F1-score of 81.0%, a significant outcome even when compared to models that incorporate three modalities from the dataset. This comparison highlights the superior efficacy of attention-based models, including ours, over traditional methods like SVM. This superiority is primarily attributed to the proficiency of attention mechanisms in capturing complex relationships within each modality. The following sections offer more detailed insights into how AMuSeD's self-attention mechanism intricately processes feature vectors by considering intra-modal relationships, and how it stands in contrast to other models employing attention mechanisms.\n\n2) Data Augmentation Analysis: To explore the influence of data augmentation on model performance, we conducted experiments focusing on the size of the augmented dataset. The relationship between the volume of augmented data and model efficacy is depicted in Figure  4 . Our findings indicate a clear positive trend: as the volume of data increases, so does the performance of the model. Notably, when employing the same synthesizer, the model utilizing 16-fold augmented data demonstrated superior performance compared to the 4-fold augmentation. Furthermore, the model with 20-fold augmented data achieved the highest F1-score, reaching 80.98%, which is approximated to 81.0%. This result strongly supports the effectiveness of our data augmentation strategy in enhancing the model's sarcasm detection capabilities. Our investigation extended to the impact of different speech synthesizers on the 4-fold augmented data. Figure  5  presents the results of this analysis. We observed that the choice of synthesizer significantly influenced model performance. Specifically, the   two FS2 based synthesis methods outperformed AP, with the Fine-tuned FS2 version achieving the highest F1-score. This suggests that FS2, especially when fine-tuned, is more effective in capturing speech prosody elements essential for sarcasm classification.\n\nFurthermore, we conducted a subjective evaluation to assess the perceived quality and sarcasm resemblance of audio generated by different synthesizers. For this, we created a listening test comprising 10 natural speech utterances (original group) and their back-translated versions synthesized by AP, Pre-trained FS2, and Fine-tuned FS2 (contrastive group). Fifty listeners with no reported hearing impairments participated in this evaluation. They rated each utterance based on overall quality and sarcasm resemblance on a scale from 1 to 5, using the mean opinion score (MOS) method. The results, depicted in Figure  6 , reveal that Fine-tuned FS2 notably enhanced both speech quality and sarcasm resemblance, significantly outperforming AP and Pre-trained FS2.\n\n3) Significance of Attention Mechanisms: Our analysis extends to models incorporating various attention strategies, including Parallel CoAttention, Word-level attention, Cross x attention, and Self-attention. These models were evaluated using the 20-fold augmented data set. The outcomes of this comparative study, highlighting the effectiveness of each attention mechanism within our model framework, are concisely summarized in Table  VI .\n\nA detailed analysis of models with and without attention mechanisms reveals a performance disparity. Models incorporating attention mechanisms consistently outperform those without, highlighting the critical role these mechanisms play in improving classification accuracy. Further, we delved into the influence of skip connections on different models. This additional layer of analysis yielded model-specific results, as detailed in Table  VII . Notably, while Cross attention models experienced a 1.91% decrease in F1-score upon integrating skip connections, Self-attention models saw an increase of 1.34% in their F1-score, achieving the highest score in our set of experiments. 4) BERT vs. GLOVE: Experiments analyzing text feature extractors were conducted to compare the performance of BERT  [32]  and GLoVE  [43]  on a fully-connected neural network. These experiments were carried out using both original and augmented datasets. The results, as shown in Table  VIII , indicate that BERT  [32]  consistently achieves a higher F1score compared to GLoVE  [43] , regardless of the dataset type. Particularly noteworthy is the enhanced performance of BERT  [32]  with augmented data, underscoring its superior capability in extracting text features that are critical for effective sarcasm detection in our model. 5) The Role of Multimodality: The impact of multimodality on model performance is significant, highlighting the advantages of integrating text and audio data. Table  IX  illustrates this effect on a fully-connected neural network; the fusion of both text and audio modalities results in an F1-score of 68.10%. This outcome accentuates the synergistic benefits derived from the multimodal approach, effectively combining different types of data to enhance sarcasm detection capabilities. V. DISCUSSION\n\nThis study evaluates the effectiveness of our multimodal sarcasm detection method, AMuSeD, which introduces a novel text-audio bimodal data augmentation technique. We utilize Back Translation for text and speech synthesis for audio to produce aligned augmentations for these modalities. Our model employs self-attention with skip connections for effective features fusion to recognize sarcasm. Experimental results demonstrate the method's efficacy, achieving an F1score of 80.98%, surpassing state-of-the-art models across the MUStARD dataset's three modalities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Data Augmentation Insights",
      "text": "Our data augmentation method's effectiveness is validated by examining influential factors such as data size and synthesizer construction. Data size emerged as a critical determinant; larger datasets generally improved model performance. We explored this by generating 4-fold and 16-fold augmented data using the AP synthesizer. The results confirmed the positive impact of data size on performance. Additionally, synthesizer construction played a significant role. Within the 4-fold augmented data, audio samples generated by Fine-tuned FS2, capturing the sarcasm-specific prosody, outperformed other methods. This was corroborated by our listening test, where Fine-tuned FS2 scored highest in sarcasm similarity. These findings align with existing research  [29] ,  [44]  and highlight the importance of fidelity in augmented data for sarcasm detection.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Attention Mechanism Efficacy",
      "text": "The effectiveness of self-attention mechanisms, particularly with skip connections, is evident in our multimodal sarcasm detection model. Self-attention aids in emphasizing relevant features within each modality, while skip connections preserve essential information across network layers. Interestingly, the application of skip connections in Cross Attention mechanisms resulted in a performance drop, emphasizing the nuanced interplay between text and audio modalities in sarcasm detection. This finding indicates that skip connections are not universally beneficial, as they can introduce complexity and potential xi noise into the model. In scenarios without data augmentation, attention-equipped models underperformed traditional methods like SVM and fully-connected neural networks. This suggests that attention mechanisms may not always be superior in text-audio sarcasm detection, especially in smaller datasets where SVM can be more effective. The adaptability of fullyconnected neural networks in handling diverse data types is also evident in these results.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Modality And Feature Analysis",
      "text": "Our findings re-affirm the importance of multimodality in sarcasm detection, resonating with prior research in the field  [22] -  [24] . Sarcasm often relies on contextual information from multiple sources, necessitating a multimodal approach for accurate detection. Additionally, our results reinforce BERT's  [32]  efficacy in extracting text features for sarcasm detection. BERT's transformer-based architecture allows it to capture context-dependent phenomena like sarcasm effectively, unlike GloVe  [43] , which generates static word embeddings without contextual information. This distinction is crucial for tasks like sarcasm detection, where context plays a pivotal role.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Limitations",
      "text": "Our experimental findings shed light on several topics requiring further exploration:\n\nAccuracy of Back Translation: In capturing sarcasm's subtleties, it is not always consistent, especially across languages with diverse syntactic and semantic structures. While translating between closely related languages might result in repetitive samples, very different languages can introduce translation errors, impacting the quality of the augmented dataset.\n\nRefinement of Data Augmentation Techniques: While synthesized audio provides a means to augment data, it falls short in replicating the rich variety of human speech characteristics, such as accents, dialects, and unique speech patterns. This limitation is particularly significant in sarcasm detection, where nuances in vocal expression play a critical role. The quality of audio samples produced through our bimodal data augmentation process should be further refined. Ensuring these samples accurately encapsulate nuanced linguistic characteristics intrinsic to sarcasm is essential for the model's effectiveness.\n\nSynchronization of Text and Audio Modalities: Accurate alignment is crucial in sarcasm detection, where timing and intonation are key to understanding the context. Ensuring that these modalities are effectively synchronized to reflect the intended sarcasm is an area that requires further research and development.\n\nIntegration of Video Modality: A limitation of our current approach is the exclusion of video modality. While our study omits augmenting video data due to its potential overlap with image modality and alignment challenges with text and audio, the inclusion of video is vital. As indicated by Attardo et al.  [45]  and De Vries et al.  [46] , sarcasm often incorporates unique facial expressions, head movements, and body language. Future work should aim to integrate video data effectively, balancing the reduction of redundancy with the preservation of key sarcasm-related features.\n\nEnhancement of Contextual Information: Incorporating contextual elements such as speaker information and surrounding utterances could significantly enhance our model. These factors are critical in sarcasm detection and should be integrated into future model architectures to improve their predictive accuracy.\n\nExploring Advanced Model Architectures: We recommend investigating additional model structures, particularly those like CoAttention, which are known for efficiently managing inter-modal interactions. These sophisticated architectures might offer superior performance and deeper insights into the dynamics of modality relationships.\n\nAcross Cultural and Linguistic Application: Sarcasm is a complex linguistic phenomenon that varies across different cultures and languages. The current methodology and datasets may not adequately represent this diversity, thereby limiting the model's applicability in a global context.\n\nImproving Source Data Quality: The quality and diversity of the source data heavily influence the effectiveness of our approach. Any inherent biases or lack of representativeness in the original dataset can adversely affect both the backtranslation and audio synthesis processes.\n\nComputational Resources: The computational resources required for processing and analyzing multimodal data are considerable. This includes the need for substantial processing power and memory, which could be a constraint in resourcelimited settings. Acknowledging and addressing these computational requirements is essential for the scalability and practical deployment of our methodology.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study marks a significant advance in sarcasm detection by employing a multimodal approach, addressing the unique challenges of sarcasm's pragmatic and context-dependent nature. We have explored sarcasm detection, emphasizing the necessity of multimodality in interpreting sarcastic expressions that often convey meanings contrary to their literal interpretations. Our research answered two RQs: RQ1: Does our innovative text-audio data augmentation approach enhance the detection of sarcasm in a multimodal context, and which key factors primarily determine its success? Our augmentation techniques, utilizing Back Translation and audio synthesis, are effective in boosting multimodal sarcasm detection. We investigated critical factors like data size and the sarcasm resemblance in synthesized speech, providing valuable insights into sarcasm detection methodologies.\n\nRQ2: How does incorporating self-attention mechanisms with our augmentation strategy refine the model's performance, thus pushing the boundaries of multimodal sarcasm detection? The integration of self-attention mechanisms with augmented data has showcased their efficacy in enhancing feature fusion for multimodal sarcasm detection, adding sophistication and potential for improved performance.\n\nOur findings suggest that refining the prosodic qualities in synthesized audio can further enhance our data augmentation approach. Future research should focus on TTS techniques capable of producing speech that more accurately reflects sarcasm's subtleties. Additionally, there is a promising research avenue in exploring the integration of video data within our bimodal augmentation framework, offering a more comprehensive view of sarcasm by incorporating visual cues alongside text and audio.\n\nIn conclusion, our research introduces an innovative textaudio bimodal data augmentation method that significantly boosts sarcasm detection capabilities. We also highlight the effectiveness of self-attention mechanisms in a multimodal context. Our study transcends traditional sentiment analysis boundaries, venturing into the integration of speech data and synthesis tools. This interdisciplinary effort tackles the complexities of sarcasm detection and paves the way for more nuanced and context-sensitive multimodal approaches in the field.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample sarcastic utterance from MUStARD dataset.",
      "page": 1
    },
    {
      "caption": "Figure 1: In this case,",
      "page": 1
    },
    {
      "caption": "Figure 1: , without context, in this case, an unusual flat tone",
      "page": 1
    },
    {
      "caption": "Figure 1: , as “surprise” or “positive” without taking into",
      "page": 1
    },
    {
      "caption": "Figure 2: , which provides an",
      "page": 3
    },
    {
      "caption": "Figure 2: System architecture of the proposed AMuSeD.",
      "page": 4
    },
    {
      "caption": "Figure 3: This strategy involves the following three steps.",
      "page": 4
    },
    {
      "caption": "Figure 3: Schematic overview of data augmentation.",
      "page": 5
    },
    {
      "caption": "Figure 4: The effect of augmented data size.",
      "page": 9
    },
    {
      "caption": "Figure 4: Our findings indicate a",
      "page": 9
    },
    {
      "caption": "Figure 5: presents the results of",
      "page": 9
    },
    {
      "caption": "Figure 5: The effect of synthesizers.",
      "page": 9
    },
    {
      "caption": "Figure 6: Subjective evaluation from the listening test. This figure indicates the",
      "page": 9
    },
    {
      "caption": "Figure 6: , reveal that Fine-tuned FS2 notably enhanced",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No": "4-fold(AP)",
          "Text\nAudio": "Text\nAudio",
          "690\n690": "3,218\n3,218"
        },
        {
          "No": "4-fold(Pre-trained FS2)",
          "Text\nAudio": "Text\nAudio",
          "690\n690": "2,782\n2,782"
        },
        {
          "No": "4-fold(Fine-tuned FS2)",
          "Text\nAudio": "Text\nAudio",
          "690\n690": "2,782\n2,782"
        },
        {
          "No": "16-fold(AP)",
          "Text\nAudio": "Text\nAudio",
          "690\n690": "9,058\n9,058"
        },
        {
          "No": "20-fold(AP+Fine-tuned FS2)",
          "Text\nAudio": "Text\nAudio",
          "690\n690": "11,150\n11,150"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No": "Yes",
          "SVM\nFully-connected Neural\nNetworks": "Parallel CoAttention\nWord level attention\nSelf-attention\nCross attention",
          "60.83\n79.84": "77.94\n77.74\n80.27\n81.10",
          "58.99\n79.42": "77.68\n77.39\n79.71\n80.87",
          "58.36\n79.41": "77.65\n77.31\n79.64\n80.88"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The role of auditory and visual cues in the interpretation of mandarin ironic speech",
      "authors": [
        "S Li",
        "A Chen",
        "Y Chen",
        "P Tang"
      ],
      "year": "2022",
      "venue": "Journal of Pragmatics"
    },
    {
      "citation_id": "2",
      "title": "Tag questions and common ground effects in the perception of verbal irony",
      "authors": [
        "R Kreuz",
        "M Kassler",
        "L Coppenrath",
        "B Mclain Allen"
      ],
      "year": "1999",
      "venue": "Journal of Pragmatics"
    },
    {
      "citation_id": "3",
      "title": "Asymmetries in the use of verbal irony",
      "authors": [
        "R Kreuz",
        "K Link"
      ],
      "year": "2002",
      "venue": "Journal of Language and Social Psychology"
    },
    {
      "citation_id": "4",
      "title": "Irony and use-mention distinction",
      "authors": [
        "D Sperber",
        "D Wilson"
      ],
      "year": "1981",
      "venue": "Irony and use-mention distinction"
    },
    {
      "citation_id": "5",
      "title": "On the psycholinguistics of sarcasm",
      "authors": [
        "R Gibbs"
      ],
      "year": "1986",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "6",
      "title": "How to be sarcastic: The reminder theory of verbal irony",
      "authors": [
        "R Kreuz",
        "S Glucksberg"
      ],
      "year": "1989",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "7",
      "title": "Detecting sarcasm in multimodal social platforms",
      "authors": [
        "R Schifanella",
        "P De Juan",
        "J Tetreault",
        "L Cao"
      ],
      "year": "2016",
      "venue": "Proc. 24th ACM Int. Conf. Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Towards multimodal sarcasm detection (an obviously perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annu. Meeting Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Modeling incongruity between modalities for multimodal sarcasm detection",
      "authors": [
        "Y Wu",
        "Y Zhao",
        "X Lu",
        "B Qin",
        "Y Wu",
        "J Sheng",
        "J Li"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "10",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "S Pramanick",
        "A Roy",
        "V Johns"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV)"
    },
    {
      "citation_id": "11",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. 31st Int. Conf. Neural Inf"
    },
    {
      "citation_id": "12",
      "title": "Improving neural machine translation models with monolingual data",
      "authors": [
        "R Sennrich",
        "B Haddow",
        "A Birch"
      ],
      "year": "2016",
      "venue": "Proc. 54th Annu. Meeting Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Improving short text classification through global augmentation methods",
      "authors": [
        "V Marivate",
        "T Sefara"
      ],
      "year": "2020",
      "venue": "Proc. Int. Cross-Domain Conf. Mach. Learn. and Knowl. Extraction (CD-MAKE)"
    },
    {
      "citation_id": "14",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Audio augmentation for speech recognition"
    },
    {
      "citation_id": "15",
      "title": "Training neural speech recognition systems with synthetic speech augmentation",
      "authors": [
        "J Li",
        "R Gadde",
        "B Ginsburg",
        "V Lavrukhin"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "16",
      "title": "Mda: Multimodal data augmentation framework for boosting performance on sentiment/emotion classification tasks",
      "authors": [
        "N Xu",
        "W Mao",
        "P Wei",
        "D Zeng"
      ],
      "year": "2021",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "17",
      "title": "Semantic equivalent adversarial data augmentation for visual question answering",
      "authors": [
        "R Tang",
        "C Ma",
        "W Zhang",
        "Q Wu",
        "X Yang"
      ],
      "year": "2020",
      "venue": "Proc. 16th Eur. Conf. Comput. Vis. (ECCV)"
    },
    {
      "citation_id": "18",
      "title": "When did you become so smart, oh wise one?! sarcasm explanation in multi-modal multi-party dialogues",
      "authors": [
        "S Kumar",
        "A Kulkarni",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2022",
      "venue": "Proc. 60th Annu. Meeting Assoc. Comput. Linguistics"
    },
    {
      "citation_id": "19",
      "title": "A multimodal corpus for emotion recognition in sarcasm",
      "authors": [
        "A Ray",
        "S Mishra",
        "A Nunna",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Proc. 30th Lang"
    },
    {
      "citation_id": "20",
      "title": "A multimodal fusion method for sarcasm detection based on late fusion",
      "authors": [
        "N Ding",
        "S -W. Tian",
        "L Yu"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "21",
      "title": "Sarcasm detection using cognitive features of visual data by learning model",
      "authors": [
        "B Hiremath",
        "M Patil"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "22",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "D . R",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, Online"
    },
    {
      "citation_id": "23",
      "title": "Multi-modal sarcasm detection based on contrastive attention mechanism",
      "authors": [
        "X Zhang",
        "Y Chen",
        "G Li"
      ],
      "year": "2021",
      "venue": "Proc. 10th Natural Lang"
    },
    {
      "citation_id": "24",
      "title": "Learning multi-task commonness and uniqueness for multimodal sarcasm detection and sentiment analysis in conversation",
      "authors": [
        "Y Zhang",
        "Y Yu",
        "D Zhao",
        "Z Li",
        "B Wang",
        "Y Hou",
        "P Tiwari",
        "J Qin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Aggression detection in social media: Using deep neural networks, data augmentation, and pseudo labeling",
      "authors": [
        "S Aroyehun",
        "A Gelbukh"
      ],
      "year": "2018",
      "venue": "Proc. 1st Workshop Trolling, Aggression and Cyberbullying (TRAC)"
    },
    {
      "citation_id": "26",
      "title": "Augmenting data for sarcasm detection with unlabeled conversation context",
      "authors": [
        "H Lee",
        "Y Yu",
        "G Kim"
      ],
      "year": "2020",
      "venue": "Proc. 2nd Workshop Figurative Lang. Process"
    },
    {
      "citation_id": "27",
      "title": "Deep cnn-based inductive transfer learning for sarcasm detection in speech",
      "authors": [
        "X Gao",
        "S Nayak",
        "M Coler"
      ],
      "year": "2022",
      "venue": "Interspeech 2022, Incheon, Republic of Korea"
    },
    {
      "citation_id": "28",
      "title": "Libritts: A corpus derived from librispeech for text-tospeech",
      "authors": [
        "H Zen",
        "V.-T Dang",
        "R Clark",
        "Y Zhang",
        "R Weiss",
        "Y Jia",
        "Z Chen",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "Libritts: A corpus derived from librispeech for text-tospeech"
    },
    {
      "citation_id": "29",
      "title": "Speech recognition with augmented synthesized speech",
      "authors": [
        "A Rosenberg",
        "Y Zhang",
        "B Ramabhadran",
        "Y Jia",
        "P Moreno",
        "Y Wu",
        "Z Wu"
      ],
      "year": "2019",
      "venue": "Proc. IEEE Automatic Speech Recognit. and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "30",
      "title": "Multimodal continuous emotion recognition with data augmentation using recurrent neural networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "M Niu",
        "M Yang"
      ],
      "year": "2018",
      "venue": "Proc. Audio/Vis. Emotion Challenge and Workshop"
    },
    {
      "citation_id": "31",
      "title": "Mixgen: A new multi-modal data augmentation",
      "authors": [
        "X Hao",
        "Y Zhu",
        "S Appalaraju",
        "A Zhang",
        "W Zhang",
        "B Li",
        "M Li"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis. Workshops (WACVW)"
    },
    {
      "citation_id": "32",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. North American Chapter Assoc. Comput. Linguistics (NAACL)"
    },
    {
      "citation_id": "33",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey"
      ],
      "year": "2017",
      "venue": "Proc. 42nd IEEE Int. Conf. Acoust., Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "34",
      "title": "",
      "authors": [
        "C.-M Chien",
        "J.-H Lin"
      ],
      "venue": ""
    },
    {
      "citation_id": "35",
      "title": "Investigating on incorporating pretrained and learnable speaker representations for multi-speaker multi-style text-to-speech",
      "authors": [
        "P.-C Huang",
        "H.-Y Hsu",
        "Lee"
      ],
      "venue": "Proc. IEEE Int"
    },
    {
      "citation_id": "36",
      "title": "Speech and Signal Process. (ICASSP)",
      "authors": [
        "Conf",
        "Acoust"
      ],
      "year": "2021",
      "venue": "Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "37",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2011",
      "venue": "Proc. 3rd Int. Conf. Learn. Representations (ICLR)"
    },
    {
      "citation_id": "38",
      "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong",
        "J Kim",
        "J Bae"
      ],
      "year": "2020",
      "venue": "Proc. 34th Int. Conf. Neural Inf. Process. Systems"
    },
    {
      "citation_id": "39",
      "title": "A quantum probability driven framework for joint multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "Y Liu",
        "Y Zhang",
        "D Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "J Wang",
        "Y Liu",
        "L Rong",
        "Q Zheng",
        "D Song",
        "P Tiwari",
        "J Qin"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "41",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "42",
      "title": "An emoji-aware multitask framework for multimodal sarcasm detection",
      "authors": [
        "D Chauhan",
        "G Singh",
        "A Arora",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "43",
      "title": "Dropout: A simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Proc. 27th Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "45",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. 2014 Conf. Empirical Methods Natural Lang. Process. (EMNLP)"
    },
    {
      "citation_id": "46",
      "title": "emotional ai for speech emotion recognition: The case for synthetic emotional speech augmentation",
      "authors": [
        "S Latif",
        "A Shahid",
        "J Qadir"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "47",
      "title": "Multimodal markers of irony and sarcasm",
      "authors": [
        "S Attardo",
        "J Eisterhold",
        "J Hay",
        "I Poggi"
      ],
      "year": "2003",
      "venue": "Humor -International Journal of Humor Research"
    },
    {
      "citation_id": "48",
      "title": "Exploring the role body in communicating ironic stance",
      "authors": [
        "C Vries",
        "B Oben",
        "G Brône"
      ],
      "year": "2021",
      "venue": "Languages and Modalities"
    }
  ]
}