{
  "paper_id": "2107.14345v1",
  "title": "Modeling User Empathy Elicited By A Robot Storyteller",
  "published": "2021-07-29T21:56:19Z",
  "authors": [
    "Leena Mathur",
    "Micol Spitale",
    "Hao Xi",
    "Jieyun Li",
    "Maja J Matarić"
  ],
  "keywords": [
    "computational empathy",
    "affective computing",
    "human-robot interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Virtual and robotic agents capable of perceiving human empathy have the potential to participate in engaging and meaningful human-machine interactions that support human well-being. Prior research in computational empathy has focused on designing empathic agents that use verbal and nonverbal behaviors to simulate empathy and attempt to elicit empathic responses from humans. The challenge of developing agents with the ability to automatically perceive elicited empathy in humans remains largely unexplored. Our paper presents the first approach to modeling user empathy elicited during interactions with a robotic agent. We collected a new dataset from the novel interaction context of participants listening to a robot storyteller (46 participants, 6.9 hours of video). After each storytelling interaction, participants answered a questionnaire that assessed their level of elicited empathy during the interaction with the robot. We conducted experiments with 8 classical machine learning models and 2 deep learning models (long short-term memory networks and temporal convolutional networks) to detect empathy by leveraging patterns in participants' visual behaviors while they were listening to the robot storyteller. Our highestperforming approach, based on XGBoost, achieved an accuracy of 69% and AUC of 72% when detecting empathy in videos. We contribute insights regarding modeling approaches and visual features for automated empathy detection. Our research informs and motivates future development of empathy perception models that can be leveraged by virtual and robotic agents during human-machine interactions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "I. Introduction",
      "text": "Advances in affective computing, machine learning, and behavioral signal processing are enabling machines to function as empathic agents, capable of simulating empathy towards users and evoking empathy in users during interactions with them. Empathy is a complex affective and cognitive construct that can be broadly defined as an agent's ability to perceive and relate to another agent's emotional and cognitive states  [1] . While prior research has focused on designing virtual and * equal contribution, alphabetical order robotic agents that can express empathy through verbal and non-verbal communication  [1] -  [4] , the research challenge of developing machines capable of detecting elicited empathy in users remains largely unexplored. Machines with the ability to perceive human empathic response have the potential to participate in engaging human-machine interactions that support human well-being. For example, youth with Autism Spectrum Disorder (ASD) can experience difficulty with empathic skills  [5] ; robots with empathy perception capabilities have the potential to engage in interactions that help youth with ASD and other populations to enhance their empathic skills  [6] ,  [7] .\n\nTo the best of our knowledge, this paper presents the first attempt at modeling user empathy elicited by a robot storyteller. We developed a novel interaction context in which users listened to a robot storyteller (46 university student participants, 6.9 hours of video) and, after listening to the robot's stories, they completed questionnaires that assessed their level of elicited empathy, providing ground truth for our empathy models. We chose storytelling for the humanrobot interaction context because of the potential for robot storytelling to elicit empathy in human listeners  [8] .\n\nTo determine effective modeling approaches for detecting empathy, we conducted experiments with 10 different machine learning models to detect empathy by leveraging participants' visual behaviors while listening to a robot storyteller. Empathy detection in our research involves predicting the results of the participants' empathy questionnaires. This work focused on exploring the potential for leveraging visual behaviors (including eye gaze, facial attributes, and head pose) to detect empathy, because such behaviors have been identified as promising nonverbal indicators of empathic responses  [1] ,  [9] . We experimented with 8 linear and nonlinear classical machine learning models and 2 deep learning models (long short-term memory networks and temporal convolutional networks) that learned and exploited temporal patterns in participants' eye gaze, facial action units, facial landmarks, head pose, and point distribution parameters (shape variations). Our highestperforming approach, based on XGBoost, achieved an accuracy of 69% and AUC of 72% when detecting empathy in videos. We contribute insights regarding effective modeling approaches and visual features for automated empathy detection. Our research demonstrates the potential for advancing machine perception of user empathy and motivates future development of automated empathy detection models that can be leveraged by virtual and robotic agents during humanmachine interactions.\n\nThis paper makes the following contributions:\n\n• A novel automated approach for detecting user empathy elicited by a robot storyteller, based on interpretable visual features from users' eye gaze, facial action units, facial landmarks, head pose, and point distribution parameters.\n\n• An analysis of visual features that were predictive of elicited empathy in our interaction context, informing future research in machine perception of empathy. • A novel empathy dataset of visual features from our human-robot interaction context that is released for the computational empathy research community (with all participants de-identified to protect privacy). This paper is structured as follows: Section II summarizes related work that informed this research, Section III describes the design methodology for collecting and annotating the empathy dataset, Section IV presents our approach to automated empathy detection, Section V discusses modeling results, and Section VI concludes the paper and proposes future research.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section discusses key areas that contribute to our research: (A) theories of empathy, specifically narrative empathy, (B) prior research in developing empathic artificial agents, and (C) robot storytellers in empathic human-robot interactions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Empathy And Narrative Empathy",
      "text": "Many definitions of empathy have been proposed in the psychology and cognitive science literature. Our research is grounded in empathy definitions from social psychology  [10] -  [12] . This work views empathy as (i) an agent's capacity to perceive and share the emotional state of another agent (affective dimension of empathy) and (ii) an agent's identification with a target during a particular context as a result of a shared experience (associative dimension of empathy)  [12] -  [14] . An agent can express empathy towards another agent through appropriate verbal and nonverbal communication that matches the affective and cognitive states of the other agent  [2] ,  [15] . The contextual situation of an interaction (e.g., stimulus, social relationship) influences the amount of empathy elicited in humans  [16] .\n\nThe research in this paper is grounded in narrative empathy theory  [17]  and storytelling. Narrative empathy in an agent can be elicited through reading, viewing, hearing, or imagining stories about another agent's situation. Stories can be told by a narrator through different perspectives, also referred to as narrative voices. A story in first-person narrative voice is told from the perspective of the narrator. A story in third-person narrative voice is told from the perspective of a character in the story. A narrator can be omniscient, with an awareness of the thoughts and feelings of all characters in the story; a narrator can also be objective, with an awareness of only their own thoughts and feelings. We leverage insights from the literature on empathy, narrative empathy, and storytelling to inform the design of our empirical study to model elicited user empathy during interactions with a robot storyteller. While there has been prior research on elicited empathy in human-human storytelling interactions  [18] , to the best of our knowledge, there has been no prior study on user empathy elicited in human-robot interactions with a robot storyteller.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Empathic Artificial Agents",
      "text": "Prior research on empathic artificial agents has focused on developing virtual and robotic agents that can simulate empathy towards users and evoke user empathy. The context of an interaction can influence the extent to which empathy is elicited; Paiva et al.  [19]  created the FearNot! virtual antibullying game that evoked child empathy for virtual robots that were being bullied. Agents that mimic people's facial expressions during interactions have evoked more empathic responses than agents that do not mimic expressions  [20] . In the context of assisted living,  [21]  used a robot to extract the affective states of people during human-robot interactions and simulated an empathic response by the robot. Storytelling can also play a key role in eliciting empathy: virtual and robotic agents that disclose details about themselves through stories have been more effective at eliciting user empathy than agents that did not share stories  [8] ,  [22] .\n\nPrior research in empathic artificial agents has demonstrated the benefits of empathic human-machine interactions. Empathic artificial agents have led to more positive humanmachine interactions: users perceive empathic agents as more expressive and jovial than non-empathic agents  [23] . Empathic human-machine interactions have resulted in improvements in trust  [24] , likeability  [25] , social presence  [15] , reduction of stress  [3] , and increased engagement  [15] . These empathic interactions have the potential to enhance the effectiveness of assistive interventions (e.g., mental health applications  [26] ).\n\nTo the best of our knowledge, no prior research has focused on modeling the level of elicited user empathy during interactions with a virtual or robotic artificial agent. Our paper contributes to research in the area of computational empathy by presenting the first automated approach for detecting user empathy elicited by an artificial agent (in our interaction context, by a robot storyteller).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Robot Storytellers In Empathic Human-Robot Interactions",
      "text": "Robots have been shown to engage users in empathic interactions through storytelling  [8] . User perceptions of a robot's verbal and nonverbal characteristics (i.e., speech, physical appearance) can influence their ability to relate to the robot's stories and engage in empathic responses  [1] . The act of storytelling can personify robots and lead to users perceiving them as social actors and relating to them emotionally  [8] ,  [27] . Increasing the emotional connection between the user and the robot storyteller results in the robot being more likely to elicit empathy as listeners identify with the story's characters or the robot storyteller  [28] ,  [29] . In this work, we chose to collect empathy data in a novel interaction context involving users listening to a robot storyteller, and we contribute a novel approach for detecting user empathy during these interactions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Design Methodology: Empathic Database",
      "text": "We conducted an experimental study of interactions between 46 university students and an autonomous storytelling robot in order to evaluate the ability of the robot storyteller to evoke empathic responses in student listeners. We used a tabletop robot (QT 1 ). The study design included two narrative voice conditions and three stories. We randomly assigned half of the participants to the 1st-person narrative voice condition (1PNV), in which the robot told three stories about itself; the other half of the participants were randomly assigned to the 3rd-person narrative voice condition (3PNV), in which the robot told the same three stories from an external perspective.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Storytelling Activities",
      "text": "The three stories (S1, S2, S3) we created were normalized for style and length and were scripted for both 1PNV and 3PNV. Full stories are available upon request but omitted due to paper length restrictions. The choice of stories was grounded in the theory of non-human narrators  [30] -  [33]  and narrative empathy theory  [17] . The main difference among the three stories was the character with whom the listener could empathize. In S1, the listener is likely to feel empathy for the robot, who is also the narrator in 1PNV. In S2, the listener is 1 https://luxai.com/robot-for-teaching-children-with-autism-at-home likely to feel empathy towards another robot story character distinct from the robot narrator in 1PNV. In S3, the listener is likely to feel empathy for another human character, distinct from the robot narrator.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Data Collection Setup",
      "text": "Participants (N = 46) were recruited via email announcements to university students, and were compensated for participating in the study at US $15 per hour. Participants had an average age of 23.02 years with standard deviation (SD) of 3.09 years; half of the participants self-identified as female and the other half as male. We conducted a single-session experimental study in a private office, as shown in Figure  1 . The robot was placed on a table, and participants were seated on a chair between 1.2 and 2.1 meters from the robot; we determined this distance as most appropriate for oneon-one interpersonal communication  [34] . We recorded the sessions using an RGB-D camera embedded in the robot and an additional external camera. During the interaction, the robot used gestures with its head and arms. Because the robot's head movement affected the quality of the video captured with its head camera, we only used data collected from the external camera. We collected a total of 138 videos (6.9 hours) of story interactions. Each story interaction was 3 minutes in length. Four of the 46 participants were not included, due to technical issues (e.g., Internet connection issues, participants not consistently in the camera frame, etc). Of the 126 videos remaining from 42 participants, 4 videos were discarded due to technical issues, resulting in 122 videos used in the analysis and modeling.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Annotation Procedure",
      "text": "We assessed the participant's empathy via an empathy score questionnaire, adapted from  [35] . The questionnaire items are shown in Table  I . The interaction that we examined between the user and the robot proceeded as follows:\n\n1) Participant entered the room, the robot introduced itself and asked questions about the participant.\n\n2) The robot randomly selected one of the three stories and narrated it in the appropriate narrative voice.\n\n3) The robot asked the participant to fill out the questionnaire, which used a 5-point Likert scale to assess their level of elicited empathy towards the story's characters. 4) The robot repeated Steps 2 and 3 until all three stories were narrated and questionnaires completed. Based on the answers, we assigned an Empathy Score (ES) to each participant for each story ES S1 , ES S2 , and ES S3 . We define ES as:\n\nwhere n is the number of questionnaire items (n = 8) (see Table  I ), x is the value  [1] [2] [3] [4] [5]  from the Likert scale j, and z is the story (S1, S2, and S3). We computed Cronbach's Alpha (0.89) as a measure to evaluate the consistency of the ES definition. Using the computed (ES), we classified the videos into \"empathic\" and \"less-empathic\" classes. We chose the median of our sample (ES median = 24.5) as the cutoff for forming these classes: half of the data were labeled as \"empathic\" and the other half were labeled as \"less-empathic.\" We used a binary label since the limited size of the dataset (122 videos) did not lend itself to finer granularity for modeling. The de-identified dataset is available on GitHub: https://github.com/interaction-lab/empathy-modeling. IV. AUTOMATED EMPATHY DETECTION This section presents our approach for automated detection of user empathy elicited the by robot storyteller. Our method contains the following steps: (1) visual feature extraction, (2) feature preparation for models, and (3) empathy detection with classical machine learning and deep learning models. An overview of our process for automated detection of elicited empathy is shown in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Visual Feature Extraction",
      "text": "To capture participants' visual behaviors while listening to a robot storyteller, we extracted interpretable features that included information from eye gaze, facial attributes, and head pose at each visual frame. The OpenFace 2.2.0 toolkit  [36]  extracted eye gaze directions, the intensity and presence of 17 facial action units (FAUs), facial landmarks, head pose coordinates, and point-distribution model (PDM) parameters for facial location, scale, rotation, and deformation (709 total raw visual features). We chose to leverage these interpretable visual cues for our models, instead of deep visual representations, in order to better identify visual behaviors predictive of empathy  [37] . Figure  1  depicts the robot storyteller's perspective of a participant during an interaction, with visual features extracted by OpenFace. As illustrated in Figure  1 , participants' faces are a key channel of communication in our interaction context. Facial expressions have been recognized as a promising non-verbal empathy index  [9] , motivating our extraction of extensive facial features, in addition to eye gaze and head pose attributes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Feature Preparation For Models",
      "text": "We developed two different approaches to prepare the visual features for classical machine learning models and for deep learning models. The visual features from each video are frame-by-frame and depend on the length of each video. We removed all null and constant features across all videos. Since classical machine learning models require fixed-length feature vector inputs, we prepared our visual features for a binary empathy classification task by representing each feature as a fixed-length vector of the following time-series attributes: mean, median, standard deviation, and autocorrelation (lag 1 second). Our final fixed-length feature vectors representing each video were of length 2836 (709 x 4). For deep learning approaches, we leveraged the raw sequences of visual features, with the goal of learning and leveraging temporal patterns in visual cues that were predictive of empathy. To capture the temporal variability of our data, we re-sampled all feature sequences at 1 second intervals to create final input samples for our deep learning models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Empathy Detection",
      "text": "We formulated empathy detection as a binary classification problem to predict the \"empathic\" or \"less-empathic\" label associated with each video. We experimented with 8 classical linear and nonlinear machine learning models, implemented with scikit-learn  [38] : adaptive boosting (AdaBoost), bagging, decision trees, linear-kernel support vector machine (Linear SVM), logistic regression, random forest, rbf-kernel support vector machine (RBF SVM), and XGBoost. To explore the potential for exploiting temporal information learned from sequences of visual cues to detect empathy, we experimented with two deep learning approaches: long short-term memory networks (LSTM)  [39]  and temporal convolutional networks (TCN)  [40] , both implemented with Keras  [41]  and Tensor-Flow  [42] . We chose to experiment with these two deep models because of their potential to learn and leverage complex, nonlinear long-term dependencies in sequences of data.\n\nTo select and leverage relevant features in our highdimensional input, we implemented feature selection the training sets of each cross-validation fold (SelectKBest with K=25). All modeling was conducted with 5-fold stratified cross-validation, repeated 10 times (50 folds total). Within each cross-validation experiment, all features in the training and testing set of each fold were standardized per the feature distributions of the training set. Given the small size of our dataset, we chose cross-validation in order to avoid obtaining an optimistically-biased estimate of our models' performances  [43] . We chose stratified cross-validation to maintain the same proportion of \"empathic\" and \"less-empathic\" participants in each fold. We also experimented with Bayesian optimization with the Tree Parzen Estimator algorithm  [44]  to tune hyperparameters on training sets with the Optuna framework  [45] . We report the hyperparameters of our highest-performing model in this paper to support reproducibility (detailed in Section V). Hyperparameters for the other models are included in the GitHub repository (link in Section III.C).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "The following four metrics were computed for each model's performance at each of the 50 cross-validation folds: (1) ACC, the classification accuracy over the videos in the test set; (2) AUC, the area under the precision-recall curve, representing the probability of the classifier ranking a randomly-chosen \"empathic\" sample higher than a \"less-empathic\" one; (3) Precision, the proportion of empathic samples among all samples that were classified as empathic; (4) Recall, the proportion of samples correctly classified as \"empathic\" among all empathic samples in the dataset. We used these metrics, averaged across cross-validation folds, to compare the effectiveness of different modeling approaches. Since our dataset was balanced with \"empathic\" and \"less-empathic\" samples, our primary metric for identifying the highest-performing model was ACC. A baseline model for empathy detection (a classifier that always predicts \"empathic\") would achieve 50% accuracy; we defined this baseline for evaluating whether or not our models perform better than chance.\n\nV. RESULTS AND DISCUSSION Modeling results from empathy detection experiments are presented in Table  II . Our highest-performing model, XG-Boost, achieved an accuracy of 69% and AUC of 72%. All classifiers outperformed the chance baseline, demonstrating the potential for leveraging machine learning and visual behaviors to detect elicited empathy in our human-robot interaction context. A comparison of the performance of different modeling approaches is in Section V-A. A discussion of important visual behavioral cues predictive of empathy is found in Section V-B. Statistical significance values regarding differences in model performances were computed with McNemar's test (α = 0.01) with continuity correction  [46] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Comparison Of Modeling Approaches",
      "text": "Our highest-performing model was XGBoost (learning rate = 0.12, max depth = 6, uniform sampling method), a powerful ensemble model that leverages gradient boosting for classification  [47] . XGBoost achieved an accuracy of 69% and AUC of 72%, significantly outperforming the ensemble methods of adaptive boosting and bagging, the decision tree classifiers, and TCN (p < 0.01). XGBoost did not significantly outperform other classical machine learning approaches or the LSTM network; accuracies of these other models ranged from 62% to 65%, and AUCs ranged from 68% to 71%. Our results suggest that classical linear machine learning models (Linear SVM, Logistic Regression) and classical nonlinear machine learning models (Random Forest, RBF SVM, XGBoost) have the potential to perform comparably to deep LSTM networks when predicting empathy in this type of context. The significantly lower ACC and AUC of the TCN may have been influenced by the small nature of this dataset, which may have rendered our model's configuration of filters, dilations, and residual blocks less effective for our specific context. Our TCN achieved a precision of 76%, substantially higher than the precision of other models. In a humanmachine interaction context in which the precision of empathy detection is crucial (e.g., a machine relying on precise empathy judgements to appropriately interact with people), TCN-based approaches may have the potential to outperform other models, but it is worth noting that the TCN accuracy and AUC were substantially lower at 54%. It is also worth noting that TCNs have demonstrated success for modeling affective phenomena in other contexts (e.g., engagement modeling  [48] ).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Analysis Of Feature Contributions",
      "text": "To identify key feature sets and individual features that contributed towards the highest-performing multimodal approach, we examined the features leveraged by the highest-performing model, XGBoost. Table  III  includes the top 25 time-series features, listed in order of contribution. Figure  4  compares the empathy classification performance (accuracy) of XGBoost trained on all visual features with XGBoost trained on each subset of visual features. The top 25 individual visual features contributing to the performance of XGBoost included 19 FAU features, 3 PDM features, 2 eye region landmark features, and 1 eye gaze direction feature. Our findings from individual feature contributions support the potential for leveraging patterns in facial action units for empathy detection in our robot storyteller context. It is likely that participants with higher elicited empathy engaged in facial expressions that were more aligned with the robot's story over the course of the interaction (e.g., smiling or frowning at appropriate times). For example, the intensity of FAU 14, the \"Dimpler\" facial muscle configuration, was the highest contributing individual feature, suggesting that participants who empathized with the robot smiled more. To further examine this phenomenon, we analyzed the distribution of this feature in our dataset by conducting a two-tail independent sample Welch's t-test (without assuming equal variance) and visualizing the average intensity of FAU 14 across the timeseries of all storytelling interactions, depicted in Figure  3 . On average, empathic interactions exhibited a significantly higher mean intensity of FAU 14 compared to less-empathic interactions (p < 0.01). Across all videos, the mean FAU 14 intensity of empathic interactions was 0.23, compared to the mean FAU 14 intensity of less-empathic interactions, which was 0.11. Figure  3  illustrates the higher mean FAU 14 intensity of \"empathic\" interactions, versus \"less-empathic\" interactions, across the time-series of all storytelling interactions. Significant differences (p < 0.01) in feature values between empathic and less-empathic groups for the remaining 24 visual features are indicated in Table  III .\n\nIn addition to FAU 14 (\"Dimpler\"), time-series attributes of 4 other FAUs indicative of smiles and expressive mouth movements were within the top 25 features: FAU 23 (\"Lip Tightener\"), FAU 12 (\"Lip Corner Puller\"), FAU 6 (\"Cheek Raiser\"), and FAU 10 (\"Upper Lip Raiser\"). Across all videos, FAU 23, FAU 12, and FAU 6 exhibited a higher mean presence in empathic interactions versus less-empathic interactions. Temporal patterns in the time-series attributes of these features were leveraged by our highest-performing model to detect empathy. Temporal dynamics of these FAUs have been identified as discriminative features indicative of expressive, smiling expressions  [49] ,  [50] , further suggesting that users in empathic interactions with our robot storyteller exhibited more expressive smiles than users in less empathic interactions. Due to their stronger emotional connection with the robot, it is likely that empathic users engaged in more expressive patterns of visual cues (e.g., smiles) when listening and reacting to the robot's story, compared to less-empathic users  [28] ,  [29] .\n\nOur findings serve as a proof-of-concept demonstrating the potential for machines to leverage discriminative, contextspecific behaviors to perceive empathy elicited in humans during human-machine interactions.\n\nIn addition to analyzing individual features that contributed to the performance of XGBoost, we examined the predictive potential of each individual subset of visual features. We conducted experiments with XGBoost trained on each visual feature subset with the same 5-fold cross-validation methodology detailed in Section IV-C). As illustrated in Figure  4 , an XGBoost model trained on FAU features, alone, achieved an accuracy of 65%. This outperformed XGBoost models trained on the other individual feature subsets, which achieved accuracies of 61% for PDM features, 59% accuracy for facial landmarks, 58% for head pose, and 51% for eye gaze direction vectors. The two highest-performing feature subsets, FAU and PDM, exclusively capture patterns in facial attributes: FAU features capture facial muscle configurations and PDM features include parameters capturing facial location, scale, rotation, and deformation during communication. Our findings further demonstrate the potential for leveraging patterns in facial attributes to predict empathy elicited in humans human-machine interactions. The comparatively lower performance of XGBoost trained on eye gaze direction vectors may have been influenced by the fixed position of the robot in our interaction design: \"empathic\" and \"less-empathic\" participants largely exhibited the same eye gaze patterns towards the stationary robot storyteller.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "This paper presents the first analysis of automated machine learning approaches for detecting user empathy elicited by a robot storyteller. We designed a novel human-robot interaction context and collected the first dataset of elicited empathy in users (46 participants, 6.9 hours of video). Our methodology for collecting the empathy dataset and representing empathy will inform future research in empathic human-machine interaction. A de-identified version of our dataset's visual features is available on GitHub to further contribute to the computational empathy research community.\n\nThrough experiments with eight classical machine learning models and two deep learning models, we demonstrated the potential for leveraging machine learning models, trained on users' visual behaviors, to detect user empathy elicited by robots. Our highest-performing model, XGBoost, achieved an average accuracy of 69% and AUC of 72%, and all of our models performed above chance level when predicting empathy. Our analysis of visual features contributing to the performance of XGBoost supports the potential for exploiting temporal, context-specific patterns in facial features, specifically the intensity and presence of facial action units, to effectively predict elicited user empathy.\n\nFuture research will explore attention mechanisms for automatically learning and leveraging these context-specific features to enable machines to perceive user empathy. Future work will also include expanding our analysis, currently focused on the visual modality, to leverage patterns in multiple modalities of human behavior to predict empathy. While our interaction context was novel, this research was limited by the size of the dataset. Our findings motivate future empathy research in diverse and larger-scale human-machine interaction scenarios, beyond the novel human-robot interaction context of our dataset.\n\nVirtual and robotic agents capable of perceiving human empathy have the potential to participate in engaging and meaningful human-machine interactions that support human well-being. Our research provides a proof-of-concept and motivation for the future development of empathy perception models that can be leveraged by virtual and robotic agents during human-machine interactions.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The human-robot interaction context with a user and a robot storyteller",
      "page": 3
    },
    {
      "caption": "Figure 1: The robot was placed on a table, and participants were",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of the process for automated detection of elicited empathy.",
      "page": 4
    },
    {
      "caption": "Figure 2: A. Visual Feature Extraction",
      "page": 4
    },
    {
      "caption": "Figure 1: depicts the robot storyteller’s",
      "page": 4
    },
    {
      "caption": "Figure 3: Mean feature values for the intensity of FAU 14, computed across",
      "page": 6
    },
    {
      "caption": "Figure 3: On average, empathic interactions exhibited a signiﬁcantly",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates the higher mean FAU 14",
      "page": 6
    },
    {
      "caption": "Figure 4: Empathy classiﬁcation performances of the XGBoost model leveraging",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Item": "1",
          "Sentence": "The robot’s emotions are genuine"
        },
        {
          "Item": "2",
          "Sentence": "I experienced the same emotions\nas the robot when listening to this story"
        },
        {
          "Item": "3",
          "Sentence": "I was in a similar emotional state\nas the robot when listening to this story"
        },
        {
          "Item": "4",
          "Sentence": "I can feel\nthe robot’s emotions"
        },
        {
          "Item": "5",
          "Sentence": "When listening to the story,\nI was fully absorbed"
        },
        {
          "Item": "6",
          "Sentence": "I can relate to what\nthe robot was going\nthrough in the story"
        },
        {
          "Item": "7",
          "Sentence": "I can identify with the situation described\nin the story"
        },
        {
          "Item": "8",
          "Sentence": "I can identify with the robot\nin the story"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Empathy in virtual agents and robots: a survey",
      "authors": [
        "A Paiva",
        "I Leite",
        "H Boukricha",
        "I Wachsmuth"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "2",
      "title": "Emulating empathy in socially assistive robotics",
      "authors": [
        "A Tapus",
        "M Mataric"
      ],
      "year": "2007",
      "venue": "AAAI spring symposium: multidisciplinary collaboration for socially assistive robotics"
    },
    {
      "citation_id": "3",
      "title": "Socially-assistive robots using empathy to reduce pain and distress during peripheral iv placement in children",
      "authors": [
        "M Trost",
        "G Chrysilla",
        "J Gold",
        "M Matarić"
      ],
      "year": "2020",
      "venue": "Pain Research and Management"
    },
    {
      "citation_id": "4",
      "title": "The influence of empathy in human-robot relations",
      "authors": [
        "I Leite",
        "A Pereira",
        "S Mascarenhas",
        "C Martinho",
        "R Prada",
        "A Paiva"
      ],
      "year": "2013",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "5",
      "title": "Affective and cognitive empathy in adolescents with autism spectrum disorder",
      "authors": [
        "M Mazza",
        "M Pino",
        "M Mariano",
        "D Tempesta",
        "M Ferrara",
        "D Berardis",
        "F Masedu",
        "M Valenti"
      ],
      "year": "2014",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2014.00791"
    },
    {
      "citation_id": "6",
      "title": "Can communication with social robots influence how children develop empathy? best-evidence synthesis",
      "authors": [
        "E Pashevich"
      ],
      "year": "2021",
      "venue": "AI & SOCIETY"
    },
    {
      "citation_id": "7",
      "title": "Social robots vs. computer display: does the way social stories are delivered make a difference for their effectiveness on asd children?",
      "authors": [
        "C Pop",
        "R Simut",
        "S Pintea",
        "J Saldien",
        "A Rusu",
        "J Vanderfaeillie",
        "D David",
        "D Lefeber",
        "B Vanderborght"
      ],
      "year": "2013",
      "venue": "Journal of Educational Computing Research"
    },
    {
      "citation_id": "8",
      "title": "Empathic concern and the effect of stories in human-robot interaction",
      "authors": [
        "K Darling",
        "P Nandy",
        "C Breazeal"
      ],
      "year": "2015",
      "venue": "2015 24th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "9",
      "title": "Embodying emotion",
      "year": "2007",
      "venue": "Science"
    },
    {
      "citation_id": "10",
      "title": "Cognitive appraisal, emotion, and empathy",
      "authors": [
        "B Omdahl"
      ],
      "year": "1995",
      "venue": "Cognitive appraisal, emotion, and empathy"
    },
    {
      "citation_id": "11",
      "title": "Empathy and moral development: Implications for caring and justice",
      "authors": [
        "M Hoffman"
      ],
      "year": "2001",
      "venue": "Empathy and moral development: Implications for caring and justice"
    },
    {
      "citation_id": "12",
      "title": "Empathy: A social psychological approach",
      "authors": [
        "M Davis"
      ],
      "year": "1994",
      "venue": "Empathy: A social psychological approach"
    },
    {
      "citation_id": "13",
      "title": "Empathy: Its ultimate and proximate bases",
      "authors": [
        "S Preston",
        "F De Waal"
      ],
      "year": "2002",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "14",
      "title": "The functional architecture of human empathy",
      "authors": [
        "J Decety",
        "P Jackson"
      ],
      "year": "2004",
      "venue": "Behavioral and cognitive neuroscience reviews"
    },
    {
      "citation_id": "15",
      "title": "Empathic robots for long-term interaction",
      "authors": [
        "I Leite",
        "G Castellano",
        "A Pereira",
        "C Martinho",
        "A Paiva"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "16",
      "title": "Empathy and contextual social cognition",
      "authors": [
        "M Melloni",
        "V Lopez",
        "A Ibanez"
      ],
      "year": "2014",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "A theory of narrative empathy",
      "authors": [
        "S Keen"
      ],
      "year": "2006",
      "venue": "Narrative"
    },
    {
      "citation_id": "18",
      "title": "The omg-empathy dataset: Evaluating the impact of affective behavior in storytelling",
      "authors": [
        "P Barros",
        "N Churamani",
        "A Lim",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "Caring for agents and agents that care: Building empathic relations with synthetic agents",
      "authors": [
        "A Paiva",
        "J Dias",
        "D Sobral",
        "R Aylett",
        "P Sobreperez",
        "S Woods",
        "C Zoll",
        "L Hall"
      ],
      "year": "2004",
      "venue": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems"
    },
    {
      "citation_id": "20",
      "title": "An emotional adaption approach to increase helpfulness towards a robot",
      "authors": [
        "B Gonsior",
        "S Sosnowski",
        "M Buß",
        "D Wollherr",
        "K Kühnlenz"
      ],
      "year": "2012",
      "venue": "RSJ International Conference on Intelligent Robots and Systems. IEEE"
    },
    {
      "citation_id": "21",
      "title": "Simulating empathic behavior in a social assistive robot",
      "authors": [
        "B De Carolis",
        "S Ferilli",
        "G Palestra"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "22",
      "title": "Building virtual humans with back stories: Training interpersonal communication skills in medical students",
      "authors": [
        "A Cordar",
        "M Borish",
        "A Foster",
        "B Lok"
      ],
      "year": "2014",
      "venue": "Building virtual humans with back stories: Training interpersonal communication skills in medical students"
    },
    {
      "citation_id": "23",
      "title": "A formal model of emotions for an empathic rational dialog agent",
      "authors": [
        "M Ochs",
        "D Sadek",
        "C Pelachaud"
      ],
      "year": "2012",
      "venue": "Autonomous Agents and Multi-Agent Systems"
    },
    {
      "citation_id": "24",
      "title": "Effects of (in) accurate empathy and situational valence on attitudes towards robots",
      "authors": [
        "H Cramer",
        "J Goddijn",
        "B Wielinga",
        "V Evers"
      ],
      "year": "2010",
      "venue": "2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "25",
      "title": "Computers that care: investigating the effects of orientation of emotion exhibited by an embodied computer agent",
      "authors": [
        "S Brave",
        "C Nass",
        "K Hutchinson"
      ],
      "year": "2005",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "26",
      "title": "Towards an artificially empathic conversational agent for mental health applications: system design and user perceptions",
      "authors": [
        "R Morris",
        "K Kouddous",
        "R Kshirsagar",
        "S Schueller"
      ],
      "year": "2018",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "27",
      "title": "Suspension of disbelief in social robotics",
      "authors": [
        "B Duffy",
        "K Zawieska"
      ],
      "year": "2012",
      "venue": "2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "28",
      "title": "An emotional storyteller robot",
      "authors": [
        "A Chella",
        "R Barone",
        "G Pilato",
        "R Sorbello"
      ],
      "year": "2008",
      "venue": "AAAI spring symposium: emotion, personality, and social behavior"
    },
    {
      "citation_id": "29",
      "title": "Interview with the robot: Question-guided collaboration in a storytelling system",
      "authors": [
        "P Wicke",
        "T Veale"
      ],
      "year": "2018",
      "venue": "ICCC"
    },
    {
      "citation_id": "30",
      "title": "The storied lives of non-human narrators",
      "authors": [
        "L Bernaerts",
        "M Caracciolo",
        "L Herman",
        "B Vervaeck"
      ],
      "year": "2014",
      "venue": "Narrative"
    },
    {
      "citation_id": "31",
      "title": "Believable robot characters",
      "authors": [
        "R Simmons",
        "M Makatchev",
        "R Kirby",
        "M Lee",
        "I Fanaswala",
        "B Browning",
        "J Forlizzi",
        "M Sakr"
      ],
      "year": "2011",
      "venue": "AI Magazine"
    },
    {
      "citation_id": "32",
      "title": "What kind of stories should a virtual human swap",
      "authors": [
        "S Gilani",
        "K Sheetz",
        "G Lucas",
        "D Traum"
      ],
      "year": "2016",
      "venue": "International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "33",
      "title": "Once upon a time... a companion robot that can tell stories",
      "authors": [
        "C Adam",
        "L Cavedon"
      ],
      "year": "2015",
      "venue": "LIG"
    },
    {
      "citation_id": "34",
      "title": "The hidden dimension",
      "authors": [
        "E Hall"
      ],
      "year": "1966",
      "venue": "The hidden dimension"
    },
    {
      "citation_id": "35",
      "title": "On a scale of state empathy during message processing",
      "authors": [
        "L Shen"
      ],
      "year": "2010",
      "venue": "Western Journal of Communication"
    },
    {
      "citation_id": "36",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "37",
      "title": "Multimodal learning for identifying opportunities for empathetic responses",
      "authors": [
        "L Tavabi",
        "K Stefanov",
        "S Nasihati",
        "D Gilani",
        "M Traum",
        "Soleymani"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction, ser. ICMI '19",
      "doi": "10.1145/3340555.3353750"
    },
    {
      "citation_id": "38",
      "title": "API design for machine learning software: experiences from the scikit-learn project",
      "authors": [
        "L Buitinck",
        "G Louppe",
        "M Blondel",
        "F Pedregosa",
        "A Mueller",
        "O Grisel",
        "V Niculae",
        "P Prettenhofer",
        "A Gramfort",
        "J Grobler",
        "R Layton",
        "J Van-Derplas",
        "A Joly",
        "B Holt",
        "G Varoquaux"
      ],
      "year": "2013",
      "venue": "ECML PKDD Workshop: Languages for Data Mining and Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "40",
      "title": "Temporal convolutional networks for action segmentation and detection",
      "authors": [
        "C Lea",
        "M Flynn",
        "R Vidal",
        "A Reiter",
        "G Hager"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Keras",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "42",
      "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "authors": [
        "M Abadi"
      ],
      "venue": "2015, software available from tensorflow.org"
    },
    {
      "citation_id": "43",
      "title": "Model evaluation, model selection, and algorithm selection in machine learning",
      "authors": [
        "S Raschka"
      ],
      "year": "2018",
      "venue": "ArXiv"
    },
    {
      "citation_id": "44",
      "title": "Algorithms for hyperparameter optimization",
      "authors": [
        "J Bergstra",
        "R Bardenet",
        "Y Bengio",
        "B Kégl"
      ],
      "year": "2011",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Optuna: A nextgeneration hyperparameter optimization framework",
      "authors": [
        "T Akiba",
        "S Sano",
        "T Yanase",
        "T Ohta",
        "M Koyama"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "46",
      "title": "Power and sample size calculations. a review and computer program",
      "authors": [
        "W Dupont",
        "W Plummer"
      ],
      "year": "1990",
      "venue": "Controlled clinical trials"
    },
    {
      "citation_id": "47",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16",
      "doi": "10.1145/2939672.2939785"
    },
    {
      "citation_id": "48",
      "title": "Predicting engagement intensity in the wild using temporal convolutional network",
      "authors": [
        "C Thomas",
        "N Nair",
        "D Jayagopi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "49",
      "title": "Dynamics of facial actions for assessing smile genuineness",
      "authors": [
        "M Kawulok",
        "J Nalepa",
        "J Kawulok",
        "B Smolka"
      ],
      "year": "2021",
      "venue": "Plos one"
    },
    {
      "citation_id": "50",
      "title": "Facial action unit recognition by exploiting their dynamic and semantic relationships",
      "authors": [
        "Y Tong",
        "W Liao",
        "Q Ji"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}