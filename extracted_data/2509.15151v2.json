{
  "paper_id": "2509.15151v2",
  "title": "Exploring How Audio Effects Alter Emotion With Foundation Models",
  "published": "2025-09-18T16:57:08Z",
  "authors": [
    "Stelios Katsis",
    "Vassilis Lyberatos",
    "Spyridon Kantarelis",
    "Edmund Dervakos",
    "Giorgos Stamou"
  ],
  "keywords": [
    "Foundation Models",
    "Affect Computing",
    "Audio FX",
    "Model Auditing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Audio effects (FX) such as reverberation, distortion, modulation, and dynamic range processing play a pivotal role in shaping emotional responses during music listening. While prior studies have examined links between low-level audio features and affective perception, the systematic impact of audio FX on emotion remains underexplored. This work investigates how foundation models-large-scale neural architectures pretrained on multimodal data-can be leveraged to analyze these effects. Such models encode rich associations between musical structure, timbre, and affective meaning, offering a powerful framework for probing the emotional consequences of sound design techniques. By applying various probing methods to embeddings from deep learning models, we examine the complex, nonlinear relationships between audio FX and estimated emotion, uncovering patterns tied to specific effects and evaluating the robustness of foundation audio models. Our findings aim to advance understanding of the perceptual impact of audio production practices, with implications for music cognition, performance, and affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The relationship between sound and human emotion has been extensively studied across multiple domains, particularly within affective computing and music cognition. Prior research has demonstrated how low-level audio features-such as timbre, tempo, pitch, and rhythm-correlate with emotional responses in listeners  [1, 2, 3, 4, 5] . Parallel investigations have examined how environmental auditory conditions, including soundscape complexity and the number of simultaneous sound sources, influence affective perception  [6] . However, there remains a notable gap in understanding how audio FX-such as reverberation, distortion, modulation, and dynamic range processing-systematically alter emotional responses in music listening contexts.\n\nUnlike fundamental audio features, audio FX are intentional sound design tools commonly applied in music production and performance to modulate aesthetic and emotional impact. Yet, their role in shaping affective perception has been underexplored. For instance,  [7]  showed that longer reverberation times in recorded music increase ratings of \"sublimity,\" including emotional qualities like awe and nostalgia. Similarly, distortion has been linked to heightened arousal and affective intensity, potentially evoking excitement or even aggression  [2] . Higher rates of vibrato are linked to elevated arousal levels  [8, 9] . Additionally, an increase in the number of perceived sound sources correlates with increased arousal but may reduce pleasantness  [6] .\n\nDespite these isolated findings, there is no comprehensive study that systematically investigates how diverse audio FX in music-individually or in combination-impact emotional perception. Given that audio FX are central to contemporary music production, a focused analysis of their affective consequences is both timely and necessary.\n\nAt the same time, advances in foundation models-largescale neural architectures pre-trained on multimodal datasets, have demonstrated an ability to capture high-level semantic and perceptual relationships across domains, including with audio FX  [10, 11] . In the music domain, foundation models such as MusicLM  [12] , MERT  [13] , Qwen  [14] , and CLAP  [15]  have shown promising abilities in generating emotionally expressive audio, learning disentangled latent representations, and enabling cross-modal retrieval tasks  [16] . These models implicitly encode rich associations between musical structure, timbre, and affective meaning, making them ideal tools to probe novel hypotheses about music-emotion relationships.\n\nIn this work, we leverage foundation models to investigate how specific audio FX influence emotional perception in music. We analyze performance changes, shifts in predicted emotional labels and dimensions, and alterations in model embeddings across three stateof-the-art foundation models and diverse datasets with both categorical and dimensional annotations, using six common audio FX at different intensity levels, including fixed real-world. By combining controlled manipulations with these realistic production scenarios, we identify correlations between audio FX and estimated emotions and uncover properties of how foundation models represent these affective cues.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Material",
      "text": "In this study, we employed a diverse set of datasets and models, spanning both deep and shallow architectures, along with several commonly used audio FXs with varying parameter settings. The code, analytical details, and complete experimental results are available in our GitHub repository 1 .\n\nWe utilized three datasets capturing both categorical and dimensional emotional annotations: EMOPIA  [17] , DEAM  [18] , and witheFlow  [5] . The EMOPIA dataset is a multimodal (audio and MIDI) resource focused on perceived emotion in pop piano music, containing 1,087 clips from 387 songs. Each clip is annotated with   To investigate and interpret musical emotion, we employed three state-of-the-art foundation models, each offering unique capabilities. First, MERT-v1-330M  [13]  is a 330M-parameter model (24 layers with 1024 hidden units) trained on 160,000 hours of music audio using masked language modeling with RVQ-VAE and CQT teacher signals. MERT operates on 24 kHz audio at 75 frames per second and is particularly suited to capturing fine-grained musical and acoustic emotion cues. Second, CLAP  [21]  is a dual-encoder model with roughly 630M parameters, combining a Transformerbased (HTS-AT) audio encoder with a RoBERTa-based text encoder. CLAP was trained on 128K-630K audio-text pairs via contrastive learning, using log-Mel spectrogram inputs at 48 kHz to learn a shared audio-text representation space with strong zero-shot generalization. Finally, Qwen2-Audio-7B  [22]  is a 7B-parameter model featuring a Whisper-style audio encoder and Qwen-style decoder. Qwen2-Audio-7B was trained on over 300K hours of speech, music, and environmental audio to support broad audio-language tasks, including automatic speech recognition, classification, question answering, and conversational interaction.\n\nFor the head classifiers of our foundation models, we employed shallow interpretable models tailored to each task and dataset, training them while keeping the foundation model frozen. For regression on DEAM and witheFlow valence-arousal annotations, we used an XGBRegressor  [23]  to predict continuous emotional dimensions, evaluating performance with MAE, MSE, and R 2 . For single-label classification on EMOPIA, we applied an XGBClassifier, assessing weighted accuracy, precision, recall, and F1-score. For multi-label classification on witheFlow categorical tags, we implemented a OneVsRest strategy with XGBClassifier, measuring F1-micro and F1-macro scores with a threshold of 0.5. This configuration allowed us to effectively adapt gradient-boosted tree models to both dimensional and categorical emotion prediction tasks.\n\nTo apply controlled alterations to our input and observe the behavior of our chosen foundation model, we used six audio FX from the pedalboard library, each scaled in intensity from 1 to 10. Reverb simulates acoustic space by adjusting room size, creating the perception of different environments from small rooms to large halls, which can influence temporal and spectral characteristics of the signal. Delay introduces echoes through delay seconds and feedback, affecting rhythmic perception and reinforcing certain frequencies. Distortion adds harmonic saturation via drive db, enhancing harmonic content and introducing nonlinearities that challenge the model's robustness. EQ shapes the frequency spectrum with low cutoff and high cutoff filters, emphasizing or attenuating spectral bands and altering timbral balance. Chorus thickens the sound by duplicating and slightly detuning the signal with rate, depth, and feedback, producing subtle modulation that can affect perceived pitch and texture. Phaser applies sweeping frequency modulation via rate, depth, and feedback, generating moving notches that alter harmonic relationships over time.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "Using the above material, we conducted four experiments to investigate the relationships between audio effects and estimated emotions. First, we analyzed performance changes to assess how audio manipulations impact model accuracy. Second, we examined shifts in emotion predictions to identify changes in predicted emotional labels or dimensions. Third, we explored embedding space alterations to understand how audio effects influence the internal representations learned by the models. Finally, we investigated how real-world audio FX settings affect emotion estimation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Impact On Model Performance",
      "text": "Inspired by  [10] , that studies the impact of audio effects on foundation models, and  [24] , that examines performance changes in speech emotion recognition models induced by codec processing, we aimed to explore how incorporating emotional information affects performance in emotion recognition tasks. To investigate this, we trained interpretable machine learning classifiers using embeddings extracted from foundation models across all datasets. This approach allowed us to develop models for multi-class classification, multi-label classification, and regression tasks.\n\nAfter training our interpretable classifiers, we applied various audio effects at different intensity levels and analyzed their impact on model performance. All foundation models were consistently used as backbones across tasks and datasets. We hypothesized that any observed performance degradation would indicate a potential correlation between the audio FX and the estimated emotion. Overall, we observed a general decrease in performance, indicating that audio effects may influence emotion recognition. As summarized in Table  1 , phaser and distortion in particular evoked the largest performance declines across multiple intensity levels. All three models exhibited similar trends, with the decrease becoming more pronounced as the intensity of the audio effects increased.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Shifts In Emotion Predictions",
      "text": "Previously trained pipelines were used to run inference on audio samples that had been modified with six common effects applied at varying intensities. Transparent classifiers built on different foundation model backbones generated emotion predictions from these altered inputs (see Section 2). The resulting predictions were compared with those from the original samples, and the observed changes were visualized to reveal how the effects shifted predicted emotions. This approach enabled the examination of potential correlations between specific audio manipulations and estimated emotional responses.\n\nIn Fig.  1 , we examine how emotional predictions in the EMOPIA dataset are affected by different audio effects. Notably, high levels of distortion consistently increase Anger predictions while decreasing Calmness across all models  [25] . In contrast, delay and chorus effects introduce greater variability, suggesting that these manipulations create ambiguity in the models' emotional interpretations  [26] . Interestingly, increasing the chorus effect tends to boost Calmness predictions in the CLAP and MERT models, whereas increasing the delay effect raises Anger predictions in the CLAP and Qwen models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Embedding Space Alterations",
      "text": "In this section, we present an experiment investigating how the embeddings of a foundation model-representations that encode emotional information-are altered as common audio effects are gradually applied. Inspired by the visualization approach of Deng et al.  [10] , we identified the most important features from our shallow emotion classification model. Using the top 25 features, we performed UMAP  [27]  dimensionality reduction and visualized the foundation model's embeddings for inputs modified at different effect intensities. This approach allowed us to observe how audio effects reshape the structure of the embeddings and to identify which models are most sensitive to these changes.\n\nTo ensure stability and comparability, embeddings were first standardized per (dataset, model) using zero-mean/unit-variance scaling, then cleaned via (i) variance thresholding (removing features with variance < 1 × 10 -6 ) and (ii) iterative correlation pruning (dropping features with |r| > 0.95). Feature selection was then performed: for regression-style datasets (DEAM, withe-Flow) we used ElasticNetCV with l1 ratio ∈ {0.5, 0.8}, α values log-spaced from 10 -3 to 10 2 (50 values), 5-fold cross-validation, max iter = 60000, and tol = 2 × 10 -3 , while for classificationstyle datasets (EMOPIA, witheFlow) we used logistic regression with an elastic net penalty (l1 ratio = 0.5), the SAGA solver, C = 0.5, max iter = 6000, and tol = 2×10 -3 . For visualization, we applied UMAP using a cosine metric and spectral initialization; in this experiment, these shallow models were chosen for their linearity and interpretability, with K = 25, n neighbors = 30, and min dist = 0.5. To avoid crowding and label imbalance, we sampled 20 tracks per effect for regression datasets, 5 tracks per label for classification datasets, and 3 tracks per label for the multi-label dataset (witheFlow), using a fixed random seed (42), covering the effects reverb, delay, distortion, EQ, chorus, and phaser.\n\nIn Fig.  2 , we show how embeddings from each foundation model respond to audio effects on the EMOPIA dataset. Our analysis reveals systematic differences in how foundation models encode the impact of audio effects. CLAP embeddings exhibit large, structured displacements as FX levels increase-particularly for delay, chorus, and distortion-indicating strong sensitivity to timbral manipulations. Qwen also shows noticeable shifts, though with less consistent trajectories, whereas MERT remains relatively stable across all effect levels, suggesting robustness to such manipulations, likely due to its training on music-specific tasks. Overall, trajectory length and variance in embedding space can serve as metrics for quantifying the influence of audio FX on emotion, with the magnitude of this impact depending strongly on the choice of foundation model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Real World Scenario",
      "text": "We tested real-world effect chains designed to mimic iconic sounds from Pink Floyd, U2 (emphasizing reverb and delay to create atmospheric textures), and Rage Against the Machine (heavy distortion with moderate chorus) 2 3 . The goal of this experiment was to examine whether, in real-world scenarios, carefully crafted audio FX chains can produce stronger shifts in emotion predictions due to their intentional artistic design.\n\nAs shown in Fig.  3 , moving from isolated effects to complete FX chains reveals how cumulative production choices shift embeddings in latent space. MERT and Qwen produce similar trajectories, both showing structured, directional shifts that reflect the cumulative impact of effects. The distortion-heavy chain of Rage Against the Machine yields almost unidirectional trajectories, consistent with 2 https://www.kitrae.net/music/David Gilmour Effects And Gear.html 3 https://www.guitarchalk.com/guitar-amp-settings-guide/#t-1649265629386 strong spectral shaping imposing a uniform transformation. Reverband delay-heavy chains of U2 and Pink Floyd also follow a generally consistent pattern, though some divergent paths reflect the more complex ways spatial and temporal effects influence latent representations. CLAP, in contrast, shows shorter, scattered movements, suggesting that its sensitivity is dampened when effects are combined. Overall, real-world effect chains generate larger, more coherent shifts, highlighting that artists naturally select combinations to evoke stronger emotional responses. The nature of these trajectories-unidirectional versus multidirectional-thus serves as a proxy for how different categories of audio effects shape emotional perception in foundation models.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "Our study demonstrates that audio effects substantially alter estimated emotion in music. Distortion and phaser, in particular, strongly increase Anger while reducing Calmness, whereas chorus and delay introduce more variability in predictions. Analysis of embedding-space trajectories shows that the magnitude and structure of latent-space shifts reflect the emotional impact of FX, with real-world effect chains producing the most pronounced and coherent changes, likely because they are intentionally designed by artists to evoke strong emotional responses. Among the models, MERT exhibited relative robustness, while CLAP and Qwen were more sensitive to manipulations.\n\nFor future work, we plan to investigate the simultaneous application of multiple audio effects, extend the study to additional datasets and foundation models, explore alternative probing techniques to gain deeper insight into how complex audio transformations influence emotional perception in music, and include comparisons to classical feature-based systems as well as analyses of human ground truth after effects.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Radar plots of emotion predictions for CLAP, Qwen, and",
      "page": 2
    },
    {
      "caption": "Figure 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity",
      "page": 3
    },
    {
      "caption": "Figure 1: , we examine how emotional predictions in the EMOPIA",
      "page": 3
    },
    {
      "caption": "Figure 2: , we show how embeddings from each foundation model",
      "page": 4
    },
    {
      "caption": "Figure 3: , moving from isolated effects to complete",
      "page": 4
    },
    {
      "caption": "Figure 3: UMAP visualization of foundation model embeddings for",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "ABSTRACT\nperceived sound sources correlates with increased arousal but may"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "reduce pleasantness [6]."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "Audio effects (FX) such as reverberation, distortion, modulation, and"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "Despite\nthese\nisolated\nfindings,\nthere\nis\nno\ncomprehensive"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "dynamic range processing play a pivotal role in shaping emotional"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "study that systematically investigates how diverse audio FX in mu-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "responses during music listening. While prior studies have examined"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "sic—individually or in combination—impact emotional perception."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "links between low-level audio features and affective perception,\nthe"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "Given that audio FX are central\nto contemporary music production,"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "systematic impact of audio FX on emotion remains underexplored."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "a focused analysis of their affective consequences is both timely and"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "This work investigates how foundation models—large-scale neural"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "necessary."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "architectures pretrained on multimodal data—can be leveraged to"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "At\nthe\nsame\ntime,\nadvances\nin\nfoundation models—large-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "analyze these effects. Such models encode rich associations between"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "scale neural architectures pre-trained on multimodal datasets, have"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "musical structure, timbre, and affective meaning, offering a powerful"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "demonstrated an ability to capture high-level semantic and percep-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "framework for probing the emotional consequences of sound design"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "tual relationships across domains, including with audio FX [10, 11]."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "techniques.\nBy applying various probing methods to embeddings"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "In the music domain,\nfoundation models\nsuch as MusicLM [12],"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "from deep learning models, we examine the complex, nonlinear re-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "MERT [13], Qwen [14],\nand CLAP [15] have\nshown promising"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "lationships between audio FX and estimated emotion, uncovering"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "abilities\nin generating emotionally expressive audio,\nlearning dis-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "patterns tied to specific effects and evaluating the robustness of foun-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "entangled latent representations, and enabling cross-modal retrieval"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "dation audio models. Our findings aim to advance understanding of"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "tasks\n[16].\nThese models\nimplicitly encode rich associations be-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "the perceptual\nimpact of audio production practices, with implica-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "tween musical\nstructure,\ntimbre,\nand affective meaning, making"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "tions for music cognition, performance, and affective computing."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "them ideal\ntools\nto probe novel hypotheses about music-emotion"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "Index Terms— Foundation Models, Affect Computing, Audio\nrelationships."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "FX, Model Auditing\nIn this work, we leverage foundation models to investigate how"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "specific audio FX influence emotional perception in music. We an-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "alyze performance changes, shifts in predicted emotional labels and"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "1.\nINTRODUCTION"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "dimensions, and alterations in model embeddings across three state-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "of-the-art foundation models and diverse datasets with both categor-\nThe relationship between sound and human emotion has been exten-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "ical and dimensional annotations, using six common audio FX at\nsively studied across multiple domains, particularly within affective"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "different\nintensity levels,\nincluding fixed real-world. By combining\ncomputing and music cognition.\nPrior\nresearch has demonstrated"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "controlled manipulations with these realistic production scenarios,\nhow low-level audio features—such as\ntimbre,\ntempo, pitch,\nand"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "we identify correlations between audio FX and estimated emotions\nrhythm—correlate with emotional responses in listeners [1, 2, 3, 4,"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "and uncover properties of how foundation models represent\nthese\n5].\nParallel\ninvestigations have examined how environmental au-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "affective cues.\nditory conditions,\nincluding soundscape complexity and the num-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "ber of\nsimultaneous\nsound sources,\ninfluence affective perception"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "[6]. However, there remains a notable gap in understanding how au-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "2. MATERIAL"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "dio FX—such as reverberation, distortion, modulation, and dynamic"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "range processing—systematically alter emotional responses in mu-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "In this study, we employed a diverse set of datasets and models, span-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "sic listening contexts."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "ning both deep and shallow architectures, along with several com-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "Unlike\nfundamental\naudio features,\naudio FX are\nintentional"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "monly used audio FXs with varying parameter settings. The code,"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "sound design tools commonly applied in music production and per-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "analytical details, and complete experimental results are available in"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "formance to modulate aesthetic and emotional\nimpact.\nYet,\ntheir"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "our GitHub repository 1."
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "role in shaping affective perception has been underexplored. For in-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "We utilized three datasets capturing both categorical and di-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "stance, [7] showed that longer reverberation times in recorded music"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "mensional emotional annotations: EMOPIA [17], DEAM [18], and"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "increase ratings of “sublimity,” including emotional qualities\nlike"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "witheFlow [5].\nThe EMOPIA dataset\nis a multimodal\n(audio and"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "awe and nostalgia. Similarly, distortion has been linked to height-"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "MIDI) resource focused on perceived emotion in pop piano music,"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "ened arousal and affective intensity, potentially evoking excitement"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "containing 1,087 clips from 387 songs. Each clip is annotated with"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "or even aggression [2]. Higher rates of vibrato are linked to elevated"
        },
        {
          "eddiedervakos@islab.ntua.gr,\ngstam@cs.ntua.gr": "1https://github.com/stelioskt/audioFX\narousal\nlevels\n[8, 9].\nAdditionally,\nan increase in the number of"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for a specific audio FX and dataset.": ""
        },
        {
          "for a specific audio FX and dataset.": ""
        },
        {
          "for a specific audio FX and dataset.": ""
        },
        {
          "for a specific audio FX and dataset.": "Qwen"
        },
        {
          "for a specific audio FX and dataset.": "+0.007"
        },
        {
          "for a specific audio FX and dataset.": "+0.009"
        },
        {
          "for a specific audio FX and dataset.": "+0.006"
        },
        {
          "for a specific audio FX and dataset.": "+0.005"
        },
        {
          "for a specific audio FX and dataset.": "+0.005"
        },
        {
          "for a specific audio FX and dataset.": "+0.020"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "one of\nfour categorical emotions—Excitement, Anger, Sadness, or",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "To investigate and interpret musical emotion, we employed three"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "Calmness—supporting both clip-level classification and song-level",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "state-of-the-art\nfoundation models, each offering unique capabil-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "emotion analysis.\nThe DEAM dataset\ncomprises 1,802 excerpts",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "ities.\nFirst, MERT-v1-330M [13]\nis a 330M-parameter model\n(24"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "and full\nsongs annotated with valence and arousal as continuous,",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "layers with 1024 hidden units)\ntrained on 160,000 hours of music"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "time-varying values per second, as well as aggregated ratings, along",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "audio using masked language modeling with RVQ-VAE and CQT"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "with metadata on duration, genre, and folksonomy tags, enabling",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "teacher signals. MERT operates on 24 kHz audio at 75 frames per"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "regression-based modeling of emotion dynamics.\nThe part of\nthe",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "second and is particularly suited to capturing fine-grained musical"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "witheFlow dataset we used includes 235 solo-instrument recordings,",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "and acoustic emotion cues.\nSecond, CLAP [21]\nis a dual-encoder"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "annotated by 20 listeners with continuous valence-arousal values per",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "model with roughly 630M parameters, combining a Transformer-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "second following the Circumplex Model of Affect\n[19], as well as",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "based (HTS-AT) audio encoder with a RoBERTa-based text encoder."
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "multi-label categorical\ntags based on the Geneva Emotional Music",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "CLAP was trained on 128K–630K audio–text pairs via contrastive"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "Scale (GEMS-9) [20], making it suitable for both dimensional and",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "learning, using log-Mel\nspectrogram inputs at 48 kHz to learn a"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "categorical emotion modeling in music.",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "shared audio–text representation space with strong zero-shot gener-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "alization.\nFinally, Qwen2-Audio-7B [22]\nis a 7B-parameter model"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "featuring a Whisper-style audio encoder and Qwen-style decoder."
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "Qwen2-Audio-7B was trained on over 300K hours of speech, mu-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "sic, and environmental audio to support broad audio-language tasks,"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "including automatic speech recognition, classification, question an-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "swering, and conversational interaction."
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "For the head classifiers of our foundation models, we employed"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "shallow interpretable models\ntailored to each task and dataset,"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "training them while keeping the foundation model\nfrozen.\nFor\nre-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "(a) CLAP-Chorus\n(b) CLAP-Delay\n(c) CLAP-Distortion",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "gression on DEAM and witheFlow valence-arousal annotations, we"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "used an XGBRegressor\n[23]\nto predict\ncontinuous\nemotional di-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "mensions, evaluating performance with MAE, MSE, and R2.\nFor"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "single-label classification on EMOPIA, we applied an XGBClassi-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "fier, assessing weighted accuracy, precision,\nrecall, and F1-score."
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "For multi-label\nclassification\non witheFlow categorical\ntags, we"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "implemented a OneVsRest strategy with XGBClassifier, measuring"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "F1-micro and F1-macro scores with a threshold of 0.5. This config-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "uration allowed us to effectively adapt gradient-boosted tree models"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "(d) Qwen-Chorus\n(e) Qwen-Delay\n(f) Qwen-Distortion",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "to both dimensional and categorical emotion prediction tasks."
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "To apply controlled alterations to our input and observe the be-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "havior of our chosen foundation model, we used six audio FX from"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "the pedalboard library, each scaled in intensity from 1 to 10. Reverb"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "simulates acoustic space by adjusting room size, creating the percep-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "tion of different environments from small rooms to large halls, which"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "can influence temporal and spectral characteristics of the signal. De-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "lay introduces echoes through delay seconds and feedback, affecting"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "(g) MERT-Chorus\n(h) MERT-Delay\n(i) MERT-Distortion",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "rhythmic perception and reinforcing certain frequencies. Distortion"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "adds harmonic saturation via drive db, enhancing harmonic content"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "Fig. 1: Radar plots of emotion predictions for CLAP, Qwen, and",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "and introducing nonlinearities that challenge the model’s robustness."
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "MERT across three audio effects for the EMOPIA dataset. Each level",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "and high cutoff\nEQ shapes the frequency spectrum with low cutoff"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "in the plots depicts the distribution of emotions, normalised based on",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "filters, emphasizing or attenuating spectral bands and altering tim-"
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "the greatest value in a given plot.",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": ""
        },
        {
          "+0.021\n+0.012\n+0.141\n+0.106\n+0.020\nReverb\n+0.010\n-0.012\n+0.015": "",
          "+0.006\n+0.030\n+0.060\n+0.055\n-0.022\n-0.150\n-0.116\n-0.160\n-0.230\n-0.170": "bral balance. Chorus thickens the sound by duplicating and slightly"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , phaser and distortion in particular evoked",
      "data": [
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "after applying audio FX with the intensity ranging from 1 to 10."
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "detuning the signal with rate, depth, and feedback, producing sub-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "tle modulation that can affect perceived pitch and texture.\nPhaser"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "applies sweeping frequency modulation via rate, depth, and feed-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "back, generating moving notches that alter harmonic relationships"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "over time."
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "3. EXPERIMENTS"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "Using the above material, we conducted four experiments to investi-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "gate the relationships between audio effects and estimated emotions."
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "First, we analyzed performance changes to assess how audio ma-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "nipulations impact model accuracy. Second, we examined shifts in"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "emotion predictions to identify changes in predicted emotional\nla-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "bels or dimensions. Third, we explored embedding space alterations"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "to understand how audio effects influence the internal\nrepresenta-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "tions learned by the models. Finally, we investigated how real-world"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "audio FX settings affect emotion estimation."
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "3.1.\nImpact on Model Performance"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "Inspired by [10],\nthat studies the impact of audio effects on foun-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "dation models,\nand [24],\nthat\nexamines performance\nchanges\nin"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "speech emotion recognition models induced by codec processing,"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "we aimed to explore how incorporating emotional\ninformation af-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "fects\nperformance\nin\nemotion\nrecognition\ntasks.\nTo\ninvestigate"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "this, we trained interpretable machine learning classifiers using em-"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "beddings extracted from foundation models across all datasets. This"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "approach allowed us to develop models for multi-class classification,"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "multi-label classification, and regression tasks."
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "After\ntraining our\ninterpretable classifiers, we applied various"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": ""
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "audio effects at different\nintensity levels and analyzed their impact"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "on model performance.\nAll\nfoundation models were consistently"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "used as backbones across tasks and datasets. We hypothesized that"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "any observed performance degradation would indicate a potential"
        },
        {
          "Fig. 2: UMAP visualization of foundation model embeddings for the EMOPIA dataset, showing trajectories generated for each input identity": "correlation between the audio FX and the estimated emotion."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "low emotion classification model.\nUsing the top 25 features, we",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "and delay-heavy chains of U2 and Pink Floyd also follow a gener-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "performed UMAP [27] dimensionality reduction and visualized the",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "ally consistent pattern, though some divergent paths reflect the more"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "foundation model’s embeddings for inputs modified at different ef-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "complex ways\nspatial and temporal effects\ninfluence latent\nrepre-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "fect\nintensities. This approach allowed us to observe how audio ef-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "sentations. CLAP,\nin contrast, shows shorter, scattered movements,"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "fects reshape the structure of the embeddings and to identify which",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "suggesting that\nits\nsensitivity is dampened when effects are com-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "models are most sensitive to these changes.",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "bined. Overall,\nreal-world effect chains generate larger, more co-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "To ensure stability and comparability,\nembeddings were first",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "herent shifts, highlighting that artists naturally select combinations"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "standardized\nper\n(dataset, model)\nusing\nzero-mean/unit-variance",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "to evoke stronger emotional responses. The nature of these trajecto-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "scaling,\nthen cleaned via (i) variance thresholding (removing fea-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "ries—unidirectional versus multidirectional—thus serves as a proxy"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "tures with\nvariance < 1 × 10−6)\nand\n(ii)\niterative\ncorrelation",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "for how different categories of audio effects shape emotional percep-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "pruning (dropping features with |r| > 0.95).\nFeature selection",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "tion in foundation models."
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "was\nthen performed:\nfor\nregression-style datasets\n(DEAM, withe-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "Flow) we used ElasticNetCV with l1 ratio ∈ {0.5, 0.8}, α values",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "4. CONCLUSIONS AND FUTURE WORK"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "log-spaced from 10−3\nto 102\n(50 values), 5-fold cross-validation,",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "max iter = 60000, and tol = 2 × 10−3, while for classification-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "Our\nstudy demonstrates\nthat\naudio effects\nsubstantially alter\nes-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "style datasets\n(EMOPIA, witheFlow) we used logistic\nregression",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "timated emotion in music.\nDistortion and phaser,\nin particular,"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "with an elastic net penalty (l1 ratio = 0.5),\nthe SAGA solver,",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "strongly increase Anger while reducing Calmness, whereas chorus"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "C = 0.5, max iter = 6000, and tol = 2×10−3. For visualization,",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "and delay introduce more variability in predictions.\nAnalysis of"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "we applied UMAP using a cosine metric and spectral\ninitialization;",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "embedding-space trajectories shows that\nthe magnitude and struc-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "in this experiment,\nthese shallow models were chosen for their lin-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "ture of latent-space shifts reflect\nthe emotional\nimpact of FX, with"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "earity and interpretability, with K = 25, n neighbors = 30, and",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "real-world effect chains producing the most pronounced and coher-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "min dist = 0.5. To avoid crowding and label\nimbalance, we sam-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "ent changes, likely because they are intentionally designed by artists"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "pled 20 tracks per effect\nfor\nregression datasets, 5 tracks per\nlabel",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "to evoke strong emotional\nresponses. Among the models, MERT"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "for classification datasets, and 3 tracks per label for the multi-label",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "exhibited relative robustness, while CLAP and Qwen were more"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "dataset\n(witheFlow), using a fixed random seed (42), covering the",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "sensitive to manipulations."
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "effects reverb, delay, distortion, EQ, chorus, and phaser.",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "For\nfuture work, we plan to investigate the simultaneous ap-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "In Fig. 2, we show how embeddings from each foundation model",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "plication of multiple audio effects, extend the study to additional"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "respond to audio effects on the EMOPIA dataset. Our analysis re-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "datasets and foundation models, explore alternative probing tech-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "veals systematic differences in how foundation models encode the",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "niques to gain deeper\ninsight\ninto how complex audio transforma-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "impact of audio effects. CLAP embeddings exhibit large, structured",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "tions influence emotional perception in music, and include compar-"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "displacements as FX levels\nincrease—particularly for delay,\ncho-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "isons to classical feature-based systems as well as analyses of human"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "rus, and distortion—indicating strong sensitivity to timbral manip-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "ground truth after effects."
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "ulations. Qwen also shows noticeable shifts,\nthough with less con-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "sistent\ntrajectories, whereas MERT remains relatively stable across",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "all effect levels, suggesting robustness to such manipulations, likely",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "due to its training on music-specific tasks. Overall, trajectory length",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "and variance in embedding space can serve as metrics for quantify-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "ing the influence of audio FX on emotion, with the magnitude of this",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "impact depending strongly on the choice of foundation model.",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "3.4. Real World Scenario",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "We tested real-world effect chains designed to mimic iconic sounds",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "from Pink Floyd, U2 (emphasizing reverb and delay to create atmo-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "spheric textures), and Rage Against\nthe Machine (heavy distortion",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "with moderate chorus) 2\n3. The goal of this experiment was to ex-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "amine whether,\nin real-world scenarios, carefully crafted audio FX",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "chains can produce stronger shifts in emotion predictions due to their",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "intentional artistic design.",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "As shown in Fig. 3, moving from isolated effects to complete",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "FX chains reveals how cumulative production choices shift embed-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "dings in latent space. MERT and Qwen produce similar trajectories,",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "both showing structured, directional shifts that\nreflect\nthe cumula-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "tive impact of effects. The distortion-heavy chain of Rage Against",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "the Machine yields almost unidirectional trajectories, consistent with",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "Fig. 3: UMAP visualization of\nfoundation model embeddings for"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "2https://www.kitrae.net/music/David Gilmour Effects And Gear.html",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "the witheFlow dataset, showing trajectories generated for each input"
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "3https://www.guitarchalk.com/guitar-amp-settings-guide/#t-",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "",
          "strong spectral shaping imposing a uniform transformation. Reverb-": "after applying real-world scenario audio FX."
        },
        {
          "al.\n[10], we identified the most\nimportant\nfeatures from our shal-": "1649265629386",
          "strong spectral shaping imposing a uniform transformation. Reverb-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "Nam, and Yi-Hsuan Yang, “Emopia: A multi-modal pop piano dataset"
        },
        {
          "5. REFERENCES": "[1] Cyril Laurier, Olivier Lartillot, Tuomas Eerola, and Petri Toiviainen,",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "arXiv\nfor emotion recognition and emotion-based music generation,”"
        },
        {
          "5. REFERENCES": "“Exploring relationships between audio features and emotion in mu-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "preprint arXiv:2108.01374, 2021."
        },
        {
          "5. REFERENCES": "sic,” in Proceedings of the 7th Triennial Conference of European Soci-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[18] Anna Alajanki, Yi-Hsuan Yang, and Mohammad Soleymani,\n“Bench-"
        },
        {
          "5. REFERENCES": "ety for the Cognitive Sciences of Music. Citeseer, 2009, pp. 260–264.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "marking music emotion recognition systems,” PloS one, pp. 835–838,"
        },
        {
          "5. REFERENCES": "[2] Renato Panda, Ricardo Malheiro, and Rui Pedro Paiva,\n“Audio fea-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "2016."
        },
        {
          "5. REFERENCES": "IEEE Transactions on\ntures for music emotion recognition: a survey,”",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[19]\nJames A Russell, “A circumplex model of affect.,” Journal of person-"
        },
        {
          "5. REFERENCES": "Affective Computing, vol. 14, no. 1, pp. 68–88, 2020.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "ality and social psychology, vol. 39, no. 6, pp. 1161, 1980."
        },
        {
          "5. REFERENCES": "[3] Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, and Gior-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[20]\nPeer-Ole Jacobsen, Hannah Strauss, Julia Vigl, Eva Zangerle, and Mar-"
        },
        {
          "5. REFERENCES": "gos Stamou,\n“Perceptual musical features for interpretable audio tag-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "cel Zentner,\n“Assessing aesthetic music-evoked emotions in a minute"
        },
        {
          "5. REFERENCES": "ging,”\nin 2024 IEEE International Conference on Acoustics, Speech,",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "or less: A comparison of the gems-45 and the gems-9,” Musicae Sci-"
        },
        {
          "5. REFERENCES": "and Signal Processing Workshops (ICASSPW). IEEE, 2024, pp. 878–",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "entiae, p. 10298649241256252, 2024."
        },
        {
          "5. REFERENCES": "882.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[21] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-"
        },
        {
          "5. REFERENCES": "[4] Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, and Gior-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "Kirkpatrick, and Shlomo Dubnov,\n“Large-scale contrastive language-"
        },
        {
          "5. REFERENCES": "gos Stamou, “Challenges and perspectives in interpretable music auto-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "audio pretraining with feature fusion and keyword-to-caption augmen-"
        },
        {
          "5. REFERENCES": "tagging using perceptual features,”\nIEEE Access, vol. 13, pp. 60720–",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "tation,”\nin ICASSP 2023-2023 IEEE International Conference on"
        },
        {
          "5. REFERENCES": "60732, 2025.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2023, pp."
        },
        {
          "5. REFERENCES": "[5] Vassilis Lyberatos, Spyridon Kantarelis, Ioanna Zioga, Christina Anag-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "1–5."
        },
        {
          "5. REFERENCES": "nostopoulou, Giorgos Stamou, and Anastasia Georgaki,\n“Music inter-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[22] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo,"
        },
        {
          "5. REFERENCES": "pretation and emotion perception: A computational and neurophysio-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al., “Qwen2-"
        },
        {
          "5. REFERENCES": "logical investigation,” 2025.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "audio technical report,” arXiv preprint arXiv:2407.10759, 2024."
        },
        {
          "5. REFERENCES": "[6]\nZhihui Han, Jian Kang, and Qi Meng,\n“Effect of sound sequence on",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[23]\nTianqi Chen and Carlos Guestrin,\n“Xgboost: A scalable tree boosting"
        },
        {
          "5. REFERENCES": "soundscape emotions,” Applied Acoustics, vol. 207, pp. 109371, 2023.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "the 22nd acm sigkdd international confer-\nsystem,”\nin Proceedings of"
        },
        {
          "5. REFERENCES": "[7] Hannah Wilkie and Peter Harrison,\n“Reverberation time and musical",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "ence on knowledge discovery and data mining, 2016, pp. 785–794."
        },
        {
          "5. REFERENCES": "emotion in recorded music listening,” Music Perception: An Interdis-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "ciplinary Journal, vol. 42, no. 4, pp. 329–344, 2025.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[24] Aaron Albin and Elliot Moore,\n“Objective study of\nthe performance"
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "degradation in emotion recognition through the amr-wb+ codec.,”\nin"
        },
        {
          "5. REFERENCES": "[8] Klaus R Scherer, Johan Sundberg, Lucas Tamarit, and Gl´aucia L Sa-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "INTERSPEECH, 2015, pp. 1319–1323."
        },
        {
          "5. REFERENCES": "lom˜ao, “Comparing the acoustic expression of emotion in the speaking",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "and the singing voice,” Computer Speech & Language, vol. 29, no. 1,",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[25]\nJinxing Gao, Diqun Yan, and Mingyu Dong,\n“Black-box adversar-"
        },
        {
          "5. REFERENCES": "pp. 218–235, 2015.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "ial attacks through speech distortion for speech emotion recognition,”"
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2022,"
        },
        {
          "5. REFERENCES": "[9] Wenyi Song, Anh-Dung Dinh, and Andrew Brian Horner,\n“The emo-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "no. 1, pp. 20, 2022."
        },
        {
          "5. REFERENCES": "tional characteristics of the violin with different pitches, dynamics, and",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "vibrato,” in Proceedings of Meetings on Acoustics. Acoustical Society",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[26] Valerio Cesarini and Giovanni Costantini,\n“Reverb and noise as real-"
        },
        {
          "5. REFERENCES": "of America, 2024, vol. 55, p. 035004.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "world effects in speech recognition models: A study and a proposal of"
        },
        {
          "5. REFERENCES": "",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "a feature set,” Applied Sciences, vol. 14, no. 23, pp. 11446, 2024."
        },
        {
          "5. REFERENCES": "[10] Victor Deng, Changhong Wang, Gael Richard, and Brian McFee, “In-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "vestigating the sensitivity of pre-trained audio embeddings to common",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "[27]\nLeland McInnes, John Healy, and James Melville,\n“Umap: Uniform"
        },
        {
          "5. REFERENCES": "effects,”\nin ICASSP 2025-2025 IEEE International Conference on",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "manifold approximation and projection for dimension reduction,” arXiv"
        },
        {
          "5. REFERENCES": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2025, pp.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": "preprint arXiv:1802.03426, 2018."
        },
        {
          "5. REFERENCES": "1–5.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "[11]\nJui Shah, Yaman Kumar Singla, Changyou Chen, and Rajiv Ratn Shah,",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "“What all do audio transformer models hear?\nprobing acoustic rep-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "arXiv preprint\nresentations for\nlanguage delivery and its structure,”",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "arXiv:2101.00387, 2021.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "[12] Andrea Agostinelli, Timo I. Denk, Zal´an Borsos, Jesse Engel, Mauro",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "Verzetti, Antoine Caillon, Qingqing Huang, Aren\nJansen, Adam",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, and Chris-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "tian Frank, “Musiclm: Generating music from text,” 2023.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "[13] Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Bene-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "tos, et al., “Mert: Acoustic music understanding model with large-scale",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "self-supervised training,” arXiv preprint arXiv:2306.00107, 2023.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "[14]\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.,\n“Qwen tech-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "nical report,” arXiv preprint arXiv:2309.16609, 2023.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "[15] Benjamin Elizalde,\nSoham Deshmukh, Mahmoud Al\nIsmail,\nand",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "Huaming Wang, “Clap: Learning audio concepts from natural language",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "the 2022 International Conference on\nsupervision,”\nin Proceedings of",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "Machine Learning (ICML), 2022.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "[16] Alican Akman, Qiyang Sun, and Bj¨orn W Schuller, “Audio explanation",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "synthesis with generative foundation models,”\nin ICASSP 2025-2025",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "IEEE International Conference on Acoustics, Speech and Signal Pro-",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        },
        {
          "5. REFERENCES": "cessing (ICASSP). IEEE, 2025, pp. 1–5.",
          "[17] Hsiao-Tzu Hung,\nJoann Ching, Seungheon Doh, Nabin Kim,\nJuhan": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Exploring relationships between audio features and emotion in music",
      "authors": [
        "Cyril Laurier",
        "Olivier Lartillot",
        "Tuomas Eerola",
        "Petri Toiviainen"
      ],
      "year": "2009",
      "venue": "Proceedings of the 7th Triennial Conference of European Society for the Cognitive Sciences of Music"
    },
    {
      "citation_id": "3",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Rui Pedro"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Perceptual musical features for interpretable audio tagging",
      "authors": [
        "Spyridon Vassilis Lyberatos",
        "Edmund Kantarelis",
        "Giorgos Dervakos",
        "Stamou"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "5",
      "title": "Challenges and perspectives in interpretable music autotagging using perceptual features",
      "authors": [
        "Spyridon Vassilis Lyberatos",
        "Edmund Kantarelis",
        "Giorgos Dervakos",
        "Stamou"
      ],
      "year": "2025",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Music interpretation and emotion perception: A computational and neurophysiological investigation",
      "authors": [
        "Spyridon Vassilis Lyberatos",
        "Ioanna Kantarelis",
        "Christina Zioga",
        "Giorgos Anagnostopoulou",
        "Anastasia Stamou",
        "Georgaki"
      ],
      "year": "2025",
      "venue": "Music interpretation and emotion perception: A computational and neurophysiological investigation"
    },
    {
      "citation_id": "7",
      "title": "Effect of sound sequence on soundscape emotions",
      "authors": [
        "Zhihui Han",
        "Jian Kang",
        "Qi Meng"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "8",
      "title": "Reverberation time and musical emotion in recorded music listening",
      "authors": [
        "Hannah Wilkie",
        "Peter Harrison"
      ],
      "year": "2025",
      "venue": "Music Perception: An Interdisciplinary Journal"
    },
    {
      "citation_id": "9",
      "title": "Comparing the acoustic expression of emotion in the speaking and the singing voice",
      "authors": [
        "Klaus Scherer",
        "Johan Sundberg",
        "Lucas Tamarit",
        "Gláucia Salomão"
      ],
      "year": "2015",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "10",
      "title": "The emotional characteristics of the violin with different pitches, dynamics, and vibrato",
      "authors": [
        "Wenyi Song",
        "Anh-Dung Dinh",
        "Andrew Horner"
      ],
      "year": "2024",
      "venue": "Proceedings of Meetings on Acoustics"
    },
    {
      "citation_id": "11",
      "title": "Investigating the sensitivity of pre-trained audio embeddings to common effects",
      "authors": [
        "Victor Deng",
        "Changhong Wang",
        "Gael Richard",
        "Brian Mcfee"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "authors": [
        "Jui Shah",
        "Yaman Kumar Singla",
        "Changyou Chen",
        "Rajiv Ratn Shah"
      ],
      "year": "2021",
      "venue": "What all do audio transformer models hear? probing acoustic representations for language delivery and its structure",
      "arxiv": "arXiv:2101.00387"
    },
    {
      "citation_id": "13",
      "title": "Musiclm: Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zalán Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Qingqing Caillon",
        "Aren Huang",
        "Adam Jansen",
        "Marco Roberts",
        "Matt Tagliasacchi",
        "Neil Sharifi",
        "Christian Zeghidour",
        "Frank"
      ],
      "year": "2023",
      "venue": "Musiclm: Generating music from text"
    },
    {
      "citation_id": "14",
      "title": "Mert: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Yizhi Li",
        "Ruibin Yuan",
        "Ge Zhang",
        "Yinghao Ma",
        "Xingran Chen",
        "Hanzhi Yin",
        "Chenghao Xiao",
        "Chenghua Lin",
        "Anton Ragni",
        "Emmanouil Benetos"
      ],
      "year": "2023",
      "venue": "Mert: Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "15",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "16",
      "title": "Clap: Learning audio concepts from natural language supervision",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Mahmoud Ismail",
        "Huaming Wang"
      ],
      "venue": "Proceedings of the 2022 International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "17",
      "title": "Audio explanation synthesis with generative foundation models",
      "authors": [
        "Alican Akman",
        "Qiyang Sun",
        "Björn Schuller"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Emopia: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "Hsiao-Tzu Hung",
        "Joann Ching",
        "Seungheon Doh",
        "Nabin Kim",
        "Juhan Nam",
        "Yi-Hsuan Yang"
      ],
      "year": "2021",
      "venue": "Emopia: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "arxiv": "arXiv:2108.01374"
    },
    {
      "citation_id": "19",
      "title": "Benchmarking music emotion recognition systems",
      "authors": [
        "Anna Alajanki",
        "Yi-Hsuan Yang",
        "Mohammad Soleymani"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "20",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "21",
      "title": "Assessing aesthetic music-evoked emotions in a minute or less: A comparison of the gems-45 and the gems-9",
      "authors": [
        "Peer-Ole Jacobsen",
        "Hannah Strauss",
        "Julia Vigl",
        "Eva Zangerle",
        "Marcel Zentner"
      ],
      "year": "2024",
      "venue": "Musicae Scientiae"
    },
    {
      "citation_id": "22",
      "title": "Large-scale contrastive languageaudio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Qwen2audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "24",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "Tianqi Chen",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "25",
      "title": "Objective study of the performance degradation in emotion recognition through the amr-wb+ codec",
      "authors": [
        "Aaron Albin",
        "Elliot Moore"
      ],
      "year": "2015",
      "venue": "Objective study of the performance degradation in emotion recognition through the amr-wb+ codec"
    },
    {
      "citation_id": "26",
      "title": "Black-box adversarial attacks through speech distortion for speech emotion recognition",
      "authors": [
        "Jinxing Gao",
        "Diqun Yan",
        "Mingyu Dong"
      ],
      "year": "2022",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "27",
      "title": "Reverb and noise as realworld effects in speech recognition models: A study and a proposal of a feature set",
      "authors": [
        "Valerio Cesarini",
        "Giovanni Costantini"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "28",
      "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "Leland Mcinnes",
        "John Healy",
        "James Melville"
      ],
      "year": "2018",
      "venue": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "arxiv": "arXiv:1802.03426"
    }
  ]
}