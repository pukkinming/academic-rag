{
  "paper_id": "2210.03065v2",
  "title": "Mocas: A Multimodal Dataset For Objective Cognitive Workload Assessment On Simultaneous Tasks",
  "published": "2022-10-06T17:20:15Z",
  "authors": [
    "Wonse Jo",
    "Ruiqi Wang",
    "Su Sun",
    "Revanth Krishna Senthilkumaran",
    "Daniel Foti",
    "Byung-Cheol Min"
  ],
  "keywords": [
    "Multimodal Dataset",
    "Cognitive Workload Assessment",
    "Human-robot Teams",
    "Human-machine Systems",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents MOCAS, a multimodal dataset dedicated for human cognitive workload (CWL) assessment. In contrast to existing datasets based on virtual game stimuli, the data in MOCAS was collected from realistic closed-circuit television (CCTV) monitoring tasks, increasing its applicability for real-world scenarios. To build MOCAS, two off-the-shelf wearable sensors and one webcam were utilized to collect physiological signals and behavioral features from 21 human subjects. After each task, participants reported their CWL by completing the NASA-Task Load Index (NASA-TLX) and Instantaneous Self-Assessment (ISA). Personal background (e.g., personality and prior experience) was surveyed using demographic and Big Five Factor personality questionnaires, and two domains of subjective emotion information (i.e., arousal and valence) were obtained from the Self-Assessment Manikin (SAM), which could serve as potential indicators for improving CWL recognition performance. Technical validation was conducted to demonstrate that target CWL levels were elicited during simultaneous CCTV monitoring tasks; its results support the high quality of the collected multimodal signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "H UMANS serve as the core part of any human-machine interaction system; thus, the cognitive workload of human operators is a critical concern in the design and implementation of such systems  [1] . Cognitive workload (CWL) can be defined as the quantitative amount of mental loads exceeding the operator's ability to perform tasks  [2] . It can also be considered as a mental strain to respond to tasks' demands for performing specific tasks  [3] .\n\nPrevious research  [4] ,  [5] ,  [6] ,  [7] ,  [8]  has repeatedly shown that awareness and perception of human CWL can improve the performance of a human-machine interaction system as such awareness allows identifying and alleviating task errors that result from situations of task over-load or under-load. Monitoring the CWL of human operators in a human-robot team, for instance, enables the workload and autonomy levels of the robots to be adjusted as needed to help human operators efficiently and productively maintain their working state  [9] . Similarly, recognizing the CWL of human drivers can improve the safety of human-vehicle systems  [7] .\n\nGenerally, CWL assessments can be categorized as subjective or objective. Subjective CWL estimation focuses on • W. Jo, R.Q.  Wang  the self-assessment of one's workload via subjective questionnaires such as the NASA-Task Load Index (NASA-TLX)  [10] . Objective measurement of CWL relies on quantitative data concerning one's physiological or behavioral responses when facing a certain level of workload. Accordingly, objective assessments can be further categorized as physiological or behavioral based on the data types utilized  [11] . Physiological assessments perceive human biological metrics such as electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS), skin temperature (SKT), and heart rate (HR), which change according to the reaction of the human nervous system to workload fluctuations. Behavioral assessments similarly analyze human behavioral responses such as facial landmarks, eye features, body gestures, and poses in relation to different levels of workload.\n\nHowever, none of the aforementioned signals are optimal in terms of accuracy, robustness, and rapidity due to the drawbacks inherent in utilizing an unimodal data source  [11] . For example, different sensors have different sensitivity to different task scenarios and to different human subjects, therefore it is non-trivial to identify one single metric that is both efficient and accurate for general scenarios  [12] . Moreover, noise and the failure of a singlemodality sensor can lead to serious errors and even invalidity of the recognition system. Multimodal fusion-based CWL assessments have been proposed as a solution to this problem; such assessments combine bio-signals from multiple modalities to provide latent and important information that is unobtainable from a single modal source, and have achieved better performance  [11] ,  [13] ,  [14] ,  [15] ,  [16] ,  [17] ,  [18] . Additionally, personality traits have been shown capable of influencing observed responses when different human subjects face the same level of workload; that is, under a given level of CWL, different personality traits can generate physiological and behavioral signals with different features  [19] ,  [20] ,  [21] ,  [22] . In addition, human emotion has been proven to have a close relation with CWL  [23] ,  [24] ,  [25] ; for instance, one's emotional state may directly affect CWL through expanding or shrinking cognitive resources  [26] . Thus, investigating personal traits, human emotion, and their potential relation with CWL can broaden our awareness of individual differences and suggest additional indicators for use in CWL assessments, thereby improving the accuracy and robustness of those assessments.\n\nEven as multimodal fusion methods are being increasingly utilized in the area of objective CWL assessment, the availability of open-source and sizable multimodal datasets remains limited, making it difficult for researchers to ensure their models and algorithms are fairly reproducible and verifiable. Furthermore, existing open-source datasets  [27] ,  [28] ,  [29] ,  [30] ,  [31] ,  [20] ,  [32]  were collected in the course of participants playing virtual games, such as dual N-back games  [33] ,  [34]  and Multi-Attribute Task Battery (MATB)  [35] ,  [36] , which weakens the applicability of those datasets to real-world CWL recognition in human-machine interactions; these virtual games often feature artificial scenarios or simplified tasks, which may not adequately represent the complexity and dynamics found in real-world situations. Also, they usually focus on specific cognitive processes, such as working memory or attention, and may overlook the interplay between multiple cognitive processes, emotions, and contextual factors that typically arise in real-world contexts  [11] ,  [37] . In addition, according to our best knowledge, no extant dataset covers behavioral data and emotional information (annotations), and only one  [20]  includes the personality traits of the human subjects, but this is limited to considering the cognitive loads and performance for game-based tasks that mean it is not suitable on realistic applications.\n\nTo fill the above-mentioned gaps, we constructed the Multimodal Dataset for Objective Cognitive Workload Assessment on Simultaneous Tasks (MOCAS). To better mimic real-world scenarios, we designed and utilized a simultaneous closed-circuit television (CCTV) monitoring task to elicit target cognitive load, where participants interact with a physical multi-robot system in real-time and may face potential realistic dynamics and latency caused by the robotic system. The data in MOCAS is based upon 21 human subjects and consists of physiological data collected from two wearable sensors, behavioral data obtained from a camera, subjective CWL annotations via the Instantaneous Self-Assessment (ISA)  [38]  and NASA-TLX, subjective emotion annotations via the Self-Assessment Manikin (SAM)  [39] , personal trait background surveyed from the Big Five Factor personality questionnaire  [40] , and raw screen video recordings. MOCAS represents a useful addition to current research in the field of multimodal fusion CWL assessment: to our best knowledge, MOCAS is the first open-access dataset to obtain both physiological and behavioral data along with both CWL and emotion annotations including the personal traits and background of subjects. All data was collected from off-the-shelf and user-friendly sensors; accordingly, an assessment model built from our dataset can be applied to everyday applications with ease and efficiency.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Dataset Design",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Design",
      "text": "MOCAS was designed with consideration of a task scenario typical in real-world human-machine systems (as illustrated in Fig.  1 ), in which one human subject undertakes a simultaneous CCTV monitoring task and multiple sensors track that person's physiological and behavioral metrics. This dataset is intended to serve as an open-access, sizable, and multimodal data source for research in the field of objective assessment of cognitive workload, with the following aims:\n\n• Support the study of cognitive workload recognition with a focus on real-world applications using humanmachine systems. • Encourage research on improving the accuracy of cognitive workload assessment using real-world applications of multimodal fusion approaches, especially combining physiological and behavioral data. • Offer novel opportunities for investigating how awareness of a subject's personality traits and emotion states can improve cognitive workload assessment performance.\n\nDistinct from existing datasets related to cognitive workload, MOCAS evokes a target CWL with realistic CCTV monitoring tasks; it also includes more comprehensive multimodal data collected from both physiological and behavioral sensors, the personal backgrounds of participants (e.g., experience, preference, and personality traits), and emotion information (annotations). The major distinctions between existing CWL datasets and MOCAS are summarized in Table  1 . Compared to other related datasets listed in Table  1 , MOCAS stands out for its inclusion of multimodal data and features from physiological sensors and behavioral devices, including EEG, PPG, GSR, HR, ACC, and TEMP data, as well as facial video and monitor screen recordings. Additionally, MOCAS includes more diverse and realistic activities, which better reflect real-life scenarios. Furthermore, the dataset provides comprehensive annotations, such as labels for different activities, affective states, and cognitive load, that can be used to train and evaluate machine learning models for various applications, such as emotion recognition and mental workload estimation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ethics Statement",
      "text": "This study and the building of MOCAS was reviewed and approved by the Purdue University Institutional Review Board (IRB) with approval number IRB-2021-1813. The informed consent form for participants included the purpose and procedure of data collection, specific types of data collected, possible risks or discomforts, compensation and benefits, protocols to protect privacy and confidentiality, and permission for potential usage of the collected data (publishing the dataset and conducting related research). Each participant was provided with the approved informed consent form upon their arrival at the experimental site for data collection, was asked to fully read and understand the content, and was asked to sign a written signature if s/he agreed to participate in the data collection procedure and approved the potential usage of the collected data. Participants also acknowledged that they had the right to terminate the data collection at any time they wanted.\n\nAll collected raw data, consent forms, and questionnaire forms (on which each participant was assigned and presented with a participant number) are kept separately and only accessible to the investigators of this study or authorized researchers who consent to the End User License Agreement (EULA) governing usage of dataset. Some aspects of participants' electronic data were de-identified (e.g., faces blurred) as befit the permissions they gave.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Participant Recruitment",
      "text": "We reached out to potential participants by posting flyers all over Purdue campus, through social networking services, and by word of mouth. The investigators also used official contact lists (such as of faculty, staff, and students in the department) to recruit participants through emails. When contacted by potential participants, investigators gave further information about the experiment, confirmed participants' availability, and sent back a confirmation email. When participants arrived at the site of data collection, they were provided with the informed consent form, and investigators were responsible for answering all their questions and concerns.\n\nThirty participants were recruited and participated in our data collection study during March and April 2022; however, only 21 had their data included in our dataset based on their permissions. All recruited participants were students and faculty/staff at Purdue University, and their ages range from 18 to 37 years old (mean = 24.3 years, S.D. = 5.2 years). In order to encourage participants' engagement in this experiment, we also compensated $ 15 for each participation and also provided additional compensations (e.g., amazon gift cards as prizes) to three participants who had the highest scores in the given tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stimuli Design",
      "text": "Video game scenarios such as N-back games and the Multi-Attribute Task Battery (MATB) are widely used for eliciting target cognitive workload levels but have limited relevance to real-world applications. In the interest of better mimicking real-world workload scenarios, we proposed a CCTV monitoring task scenario as the stimulus and collected physiological and behavioral signals under different workload levels. Such tasks, in which human operators monitor and control a display interface (or multiple interfaces simultaneously), are widely required in diverse human-machine system task scenarios such as security monitoring  [43] ,  [44] , air traffic management  [45] ,  [46] ,  [47]  and performance checking  [48] ,  [49] .\n\nHere, participants were asked to monitor multiple video streams captured by multiple patrol robot platforms passing by multiple separated rooms located in multiple corridors (Fig.  2 ). As depicted in Fig.  3 , participants were asked to monitor and detect two types of objects: (a) abnormal objects and (b) normal objects. Each room contained a random number of objects with a random assortment of types. The number and speed of robots could be changed to present different levels of stimuli (e.g., low, medium, and high workloads). The thresholds can be determined based on the number and speed of robots. For instance, a low cognitive workload level can be achieved with a small number of robots moving at a slow pace, while a medium cognitive workload level can be achieved with either two robots or faster-moving robots. On the other hand, a high cognitive workload level can be achieved by using many fast-moving robots. These thresholds were determined through multiple beta and pilot tests, which took into account factors such as the number and speed of robots.\n\nDuring the task, the participant was asked to observe the webcam streams captured by the robots and to use a mouse to click on camera views that contain any abnormal objects; the associated graphical user interface (GUI) is shown in Fig.  3a  (for more information refer to  [50] ).\n\nTo incentivize accurate identification and acknowledge participants' attentiveness and precision, we have designed a scoring system in which a participant is awarded 1 point for correctly identifying and clicking on an abnormal object. In contrast, failing to identify and click on an abnormal object results in a deduction of 3 points, which is a more severe penalty than the reward for correct identification. This scoring system aims to emphasize the importance of avoiding errors in the task and to motivate participants to be vigilant and precise in identifying abnormal objects by balancing rewards for correct actions and penalties for incorrect actions.\n\nThe designed CCTV monitoring task was conducted in a room, approximately 5 m x 6 m x 3 m (width x depth x height), with a multi-robot system involving at most four mobile robot platforms  [51]  performing the patrol task (see Fig.  2a ) to provide video streams; also in the room was a desk supporting a 23-inch monitor, a common wireless keyboard, and a mouse (see Fig.  2b ) with which participants performed the monitoring task. During each collection period, only one participant was in the room. participants were not allowed to directly observe the multi-robot system during the experiment, but could hear the sounds generated by its movements. However, we believe that the impact of these sounds on participants' performance is minimal, as the sounds generated were consistent and unobtrusive, avoiding significant distractions. Moreover, the participants were informed about the presence of these sounds before the experiment, allowing them to acclimate to the environment. Furthermore, we intentionally included the presence of noise to simulate real-world conditions, thereby increasing the ecological validity of our study.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Measures",
      "text": "We assessed participants' subjective cognitive workload using both ISA  [38]  and NASA-TLX  [10]  with weights, and also measured their subjective emotional state using SAM  [39] . The ISA is a quick and simple assessment tool to measure mental workload with only five items (e.g., Underutilized, Relaxed, Comfortable, High, and Excessive).\n\nThe NASA-TLX is a widely used assessment tool in phys- iology fields to measure task loads from five dimensions with 7-point scales on mental/physical/temporal demand, performance, effort, and frustration. The SAM is a nonverbal emotion assessment tool that directly measures the three domains related to emotional states. The details of aforementioned three measures can be found in Table  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Apparatus Of Data Collection",
      "text": "During the task, participants were required to wear two off-the-shelf wearable biosensors for physiological data collection, and their behavioral data was recorded through a webcam (i.e., Intel RealSense D435i) mounted on the monitor and a mouse as shown in Fig.  2c :\n\n• Emotiv Insight -captured 5-channel electroencephalogram (EEG), power spectrum (POW) (i.e., theta, alpha, beta, and gamma), and performance metrics generated from the EmotivPro SDK. • Empatica E4 Wristband -captured photoplethysmography (PPG), 9-axis acceleration, skin temperature (SKT), electrodermal activity (GSR, or EDA), heart rate (HR) and the inter-beat interval (IBI) derived from PPG.\n\n• Intel Realsense D435i -captured participant's facial views and eye movements. • Mouse -recorded participant's mouse movements. Additionally, participant's monitor screen, including CCTV video stream, mouse positions and status of pushed mouse buttons, was recorded for the experiment, enabling researchers to easily understand the status of the task performed by each participant. Table  4  summaries collected data in our dataset.\n\nAs illustrated in Fig.  4 , all sensors, devices and GUI programs used in the experiment were connected through Robot Operating System 2 (ROS2), where signals were collected as ROSbag2 files with synchronized time (e.g., ROS2 timestamp). The ROSbag2 format has more benefits than a traditional CSV format file in terms of collecting and analyzing the dataset  [52] , since it can ensure to synchronize the recording all topic data and to easily analyze the dataset by replaying both using a single ROSbag2 file.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Data Collection Procedures",
      "text": "Each participant's data collection procedure was conducted in following three main stages: (1) Introduction stage, (2) Trial stage, and (3) Main stage. Fig.  5  illustrates the overall data collection procedures. Additionally, there is a supplementary video at https://youtu.be/BxVVj7R9b70 that explains the details of this experiment design and procedures.\n\nIntroduction stage: At the beginning of the data collection, investigators checked again if the participant satisfied the qualifications for this study (such as not having medical history on mental, heart disorders, skin allergies, etc.). The satisfied participants then read and signed the informed consent, and filled out demographic and personality questionnaires. The demographic questionnaire collected participants' age, gender, level of education, medical history, experiences in conducting video-based surveillance or monitoring tasks, and daily usages of interacting with a monitor that indicated their capability and ability. The personality questionnaire was based on Big Five Personality Test to categorize participants' personal traits into five personality traits using IPIP Big-Five Factor Markers  [53]  as follows:\n\n• Extraversion: a participant who has high scores tends to be outgoing/talkative/social, whereas one who has low scores tends to be reflective and reserved behavior. • Agreeableness: a participant who has high scores tends to be friendly and optimistic, whereas one who has low scores tends to be critical and aggressive. • Conscientiousness: a participant who has high scores tends to be careful and diligent, whereas one who has low scores tends to be impulsive and disorganized. • Emotional stability (or neuroticism): a participant who has high scores tends to be sensitive and nervous, whereas one who has low scores tends to be resilient and confident. • Intellect/Imagination (or openness to experience): a participant who has high scores tends to be inventive and curious, whereas one who has low scores tends to be traditional and conventional.\n\nAfter that, investigators outlined the experimental process and gave instructions for each of the tasks that the par- Fig.  5 : Overall procedures for the data collection in the experiment. The supplementary video about this procedure is able to be found at https://youtu.be/BxVVj7R9b70. ticipants need to complete, and then helped each participant wear and calibrate the physiological and behavioral sensors.\n\nTrial stage: Following the introduction stage, participants were given time to get familiar with the hardware and software utilized and understand the tasks of this experiment. They conducted a trail experiment with one camera view and the minimum speed of the robots, which would not be included in the main experiment again. This stage was continued until participants fully understood and got familiar with the CCTV monitoring task in this study.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Main Stage:",
      "text": "The main stage has four repeated subphases as illustrated in the dashed box in Fig.  5 : baseline phase (about 1 minute), main phase (about 2 minutes), evaluation phase (about 1 minute), and rest phase (optional). In each baseline phase, participants watched a white cross-line with a black background for 50 seconds, and then a countdown from 10 to 0, which helped participants minimize cognitive workload affected by their previous conditions  [54] ,  [55] ,  [56] ,  [57] .\n\nIn each main phase, the participant operated one CCTV monitoring task selected from nine tasks at different work-load levels decided by different combinations of three different numbers of camera views (one, two, or four cameras) and three different speeds of the multi-robot system (low, medium, and high speed).\n\nDuring each rest phase, participants were given a break if they requested one due to pain caused by the wearable sensors or cognitive load from the tasks. At that time, we provided enough time for participants to relieve stress and pain by removing the wearable sensors. Other participants who did not need rest time continued with the remaining tasks. Since this phase was optional, we did not collect physiological and behavioral signals during this period. Additionally, before conducting the next set of tasks, we changed experimental environment by randomly selecting new positions for the robots and running new experiment programs to collect data. This process took a minimum of 1 minute. The order of nine tasks was randomly selected for each participant. After completing the assigned monitoring task and before the rest phase, the participant moved to the evaluation phase to use the GUI-based questionnaires as shown in Fig.  6  to report subjective cognitive workload  via ISA and NASA-TLX respectively, and subjective emotion state via SAM. After finishing the above three subphases, the participant could ask for a rest based on his/her conditions. When the participant decided to continue, the experiment would start to repeat these four sub-phases.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Records",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset Summary",
      "text": "Table  3  summarizes the MOCAS, which contains multimodal data from 21 participants, including physiological signals, facial camera videos, mouse movement, screen record videos, and subjective questionnaires. Fig.  7  shows examples of physiological and behavioral data from the raw dataset. The total size of the dataset is about 722.4 GB which includes 754 ROSbag2 files. Fig.  8  shows a folder tree of the online repository that displays directory paths and files for the MOCAS dataset. Fig.  8 : A folder tree of the online repository that displays directory paths and files for the MOCAS dataset.\n\nThere are three major folders based on the types of the dataset:",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Pre-Processed And Downsampled Data",
      "text": "The raw ROSbag2 format files were converted to Pickel and CSV format with 100 Hz sampling rate, which were saved to two different folders based on the type of file format (i.e., CSV and Pickle  [58] ). Each file contains raw signals, each channel's EEG signal divided from chunk EEG signals, cleaned BVP and GSR signals, and physiological features from the GSR signals (e.g., tonic and phasic domains).\n\nFor cleaning the raw BVP and GSR signals, we applied a bandpass filter to remove the noise from both the raw BVP and GSR signals through  [60] . For the physiological features, we extracted tonic and phasic features from the raw GSR signals. They can provide valuable insights into the nervous system activity and emotional arousal of an individual  [61] .\n\nThe tonic feature is the baseline level of a physiological signal called Phasic Skin Conductance Response (SCR). To extract the tonic component from the raw GSR signal, we applied a moving averages algorithm for baseline correction to refine the tonic component. The phasic feature refers to the rapid and temporary changes in physiological signals that occur in response to specific stimuli or events, called phasic skin conductance responses (SCRs), which often associated with brief fluctuations in arousal or attention. To identify phasic events, we use peak detection algorithms that detect peaks in the GSR signal. The prominent peaks usually represent the SCRs by analyzing amplitude, rise time, and half-recovery time.\n\nFurthermore, each file contains behavioral features from facial videos (e.g., Action units (AUs), probability and types of facial expression  [62] , and Eye Aspect Ratio (EAR)), as well as the results of the self-reporting questionnaires.\n\nFor extracting AUs and EAR, we first applied the facial detection algorithm to extract the face area from the facial videos and extracted each facial feature from the face area, called AUs (see Fig  7e  and Fig 7f ), through a face landmark (L) detection of Google MediaPipe  [63] . Then, the EAR of each eye is calculated using six landmarks around the eye. The average EAR of eyes is measured using the following Eq. 1  [64] :\n\nThe details of all data in the pre-processed MOCAS dataset can be found in Table  4 . -0.17 -0.05 0.17 -0.18 0.17 -0.17",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Analysis Of Dataset",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Correlation Between Personality Traits And Physiological And Behavioral Signals",
      "text": "We calculated Pearson's correlation coefficient (γ) to find the relationship between the participant's personal traits and the mean of their physiological and behavioral data. γ indicates the relationship between two variables with ranges from -1 to 1. The positive high γ means a proportional relationship between both variables, whereas the negative high γ means an inverse relationship  [65] . Fig.  9  shows the results of the Pearson's correlation coefficient, where the PE is an extraversion marker of the IPIP Big-Five Factor Markers, the PES is emotional stability, the PA is agreeableness, the PC is conscientiousness, and the PII is an intellect/imagination. From the results of Pearson's correlation, we observed a moderate relationship between personal traits and physiological and behavioral features. The moderate positive relationship (γ > 0.3) is found between PE and mean IBI, between PES and mean GSR, SKT, and EAR, between PA and mean HR, and between the PC and mean IBI and EAR. On the other hand, the moderate negative relationship (γ< -0.3) is found between PE and mean HR, between PA and mean EAR and IBI, between PC and mean HR, and between PII and mean GSR, SKR, and EAR.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Relationships Between Personality Traits, Gender, And Performance Measures",
      "text": "We also utilized Pearson's correlation coefficient and independent sample t-test to examine the relationship between the participants' personal characteristics and performance results, such as scores and success rate. As shown in Table  5 , participants with higher PE ratings demonstrated higher scores (γ=.148, p <.05) and a higher number of successful clicks (γ=.236, p <.01) during the experiment. Individuals with higher PA ratings had higher success rates in identifying abnormal objects. Furthermore, there was a positive correlation between participants' PI ratings and the number of successful clicks (γ=-.159, p <.05).\n\nThe correlation coefficients between personality traits and performance, based on gender, are presented in highlighting distinct gender characteristics during the experiment. Among male participants, higher PE scores were positively correlated with higher scores (γ=.204, p <.05) and a higher frequency of successful clicks (γ=.269, p <.01), while higher PC scores were positively associated with a higher number of successful clicks (γ=.184, p <.05). Also, higher PA scores among male participants were negatively correlated with the number of successful clicks (γ=-.191, p <.05). Among female participants, higher PA ratings showed a higher frequency of failure clicks (γ=.314, p <.01), while higher PI scores had a negative correlation with the number of successful clicks (γ=-.242, p <.05).\n\nWe conducted an independent sample t-test between male and female participants to examine the consistency of the observed tendencies in Table  5 . The four performance measures demonstrated normality based on the Shapiro-Wilk test. As shown in Table  6 , there were no significant differences between genders in the performance measures, as supported by minimal effect sizes (Cohen ś d); (score: t 185 = .326, p = .745, success rate: t 185 = -.246, p = .806, number of successful clicks: t 185 = .492, p = .624, and failure clicks: t 185 = .111, p = .912). Therefore, we concluded that there were no significant differences in performancerelated measures between genders.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Subjective Self-Reporting Annotations",
      "text": "We conducted a two-way repeated-measures analysis of variance (rmANOVA) as well as a two-way Friedman test to validate the effects of the two within-subjects factors on participants' cognitive and emotional states. Initially, we carried out the Shapiro-Wilk test to examine whether the obtained subjective self-reporting responses fulfilled the normality assumption. The NASA-TLX and weighted NASA-TLX scores indicated normality, while the arousal and valence ratings in the SAM scale and ISA showed non-normal distribution. Therefore, we employed rmANOVA for the NASA-TLX and weighted NASA-TLX, and the Friedman test for SAM and ISA to demonstrated that the main task evoked distinct responses through the experiment conditions. In the rmANOVA test, the within-subjects factors were robot speed and the number of camera views (e.g., Robot speed and Camera number). We applied the Greenhouse-Geisser correction to address the violation of the sphericity assumption  [66] . The results of the rmANOVA, categorized by the type of subjective questionnaires, are summarized in Table  7 , and Fig.  10 . The two-way Friedman test does not require the assumptions of normality  [67] . Therefore, we conducted a related samples Friedman's two-way analysis of variance by ranks for each self-questionnaire per task.\n\nISA results for measuring cognitive workload: Table  7  presents the results of the two-way Friedman test for the ISA scores reported by participants. The scores were transformed into a range from -2 to 2, representing the stringtype scores: Underutilized (-2), Relaxed (-1), Comfortable (0), High (1), and Excessive (2). The Friedman test revealed significant differences among repeated observations, yielding a Chi-square (χ 2 ) value of 88.336, with a significance level of p<.001. Further analysis of the ISA questionnaire was conducted separately for male and female participants, revealing noticeable differences. For male participants, the Friedman test showed a significant result (χ 2 = 57.313, p<.001), while for female participants, the test yielded a significant result as well (χ 2 = 33.784, p<.001). These findings suggest that the given task elicits distinct responses across different experimental conditions, regardless of the participants' gender.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Nasa-Tlx Results For Measuring Cognitive Workload:",
      "text": "Fig.  10  illustrates the results of the two-way rmANOVA results of the raw and weighted NASA-TLX scores. The weights used in the weighted NASA-TLX score are [5, 0, 4, 3, 2, 1] in which the sequence is mental demand, physical demand, temporal demand, performance, effort, and frustration. From the two-way rmANOVA results using participants' raw and weighted NASA-TLX scores, we found a main effect of the number of the camera views (raw: F 2,38 = 49.12, p<.001, η 2 p = 0.72 and weighted: F 2,38 = 59.29, p<.001, η 2 p = 0.76) and the robot speed on the cognitive workload (raw: F 2,38 = 21.06, p<.001, η 2 p = 0.53 and weighted: F 2,38 = 17.36, p<.001, η 2 p = 0.48), but there is no interaction between the two within-subject factors (raw: F 4,76 = 1.09, p = 0.37, η 2 p = 0.05 and weighted: F 4,76 = 0.86, p = 0.49, η 2 p = 0.04). We also conducted a validation of the results based on gender to determine if there was congruence between male and female participants' responses. For male participants, both the raw and weighted NASA-TLX scores showed a significant main effect for two factors: the number of camera views (raw: F 2,24 = 58.076, p<.001, η 2 p = .829; weighted: F 2,24 = 54.922, p<.001, η 2 p = .821) and the robot speed (raw: F 2,24 = 12.597, p<.001, η 2 p = .512; weighted: F 2,24 = 8.648, p = .003, η 2 p = 0.419). However, there was no interaction observed between these two factors (raw: F 4,48 = .474, p = .695, η 2 p = .038; weighted: F 4,48 = .362, p = .770. Likewise, the responses of female participants demonstrated congruence in the effects of the two factors. Significant main effects were observed for the number of cameras (raw: F 2,12 = 9.067, p = .010 (p<.05), η 2 p = .602; weighted: F 2,12 = 12.728, p = .003 (p<.05), η 2 p = .680) and the robot speed (raw: F 2,12 = 8.006, p = .018, η 2 p = .572; weighted: F 2,12 = 9.256, p = .009, η 2 p = .607) when analyzed separately. However, there was no significant interaction between these two factors (raw: F 4,24 = 1.048, p = .373, η 2 p = .149; weighted: F 4,24 = .931, p = .402, η 2 p = .134). These results indicate that individual factors significantly influence cognitive workload, but the interaction between factors does not significantly impact participants' workload.  8  summarizes the results of the two-way Friedman test conducted on the SAM scores reported by the participants, using a 7point Likert scale. From the test results, it was found that the main experiment significantly differentiated participants' responses in terms of arousal (χ 2 = 49.161, p<.001) and valence (χ 2 = 63.282, p<.001). These statistical findings hold true when the data are analyzed separately for each gender. Specifically, when considering arousal levels, both male participants (χ 2 = 35.113, p<.001) and female participants (χ 2 = 18.719, p = .016) exhibited statistically significant differences. Similarly, in terms of valence, both male participants (χ 2 = 45.116, p<.001) and female participants (χ 2 = 21.553, p = .006) showed statistically signif-",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Sam Scores For Measuring Emotion State: Table",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Classification Evaluation",
      "text": "We conducted a comprehensive evaluation of the MOCAS dataset through a three-class cognitive workload classification. To this end, we utilized pre-processed features avail-TABLE  9 : Evaluation results of unimodal and multimodal fusion three-class classification; Trial-Ind. denotes the trailindependent evaluation scheme, and Subject-Ind. presents the subject-independent, namely, LOSO, scheme.   4  for more information). In order to categorize the subjective cognitive workload annotations obtained from NASA-TLX questionnaires, we assigned them into three categories based on the literature  [68] ,  [69] . Specifically, scores ranging from 0-40 were labeled as \"low\" workload, scores from 40-60 as \"medium\" workload, and scores from 60-100 as \"high\" workload. Furthermore, we established and validated these score thresholds through multiple pilot tests, where the percentile statistics were empirically divided into three equal groups. The 33rd percentile was determined to be 37.78 and the 66th percentile was found to be 55.56. These values were then approximately rounded up to the previously mentioned ranges of 0-40, 40-60, and 60-100.\n\nTaking inspiration from previous studies  [70] ,  [71]  and recognizing the sequential nature of psychological and behavioral signals, we utilized a Long Short-Term Memory network (LSTM)  [72]  for the task of unimodal classification. This approach involved using a single modality as the input. We also employed a Late-Fusion Long Short-Term Memory network (LF-LSTM)  [73] ,  [74]  for multimodal fusion classification, where multiple modalities were utilized as the input. As depicted in Fig.  11 , our LF-LSTM model processed the features from each modality through an LSTM network to extract time-related unimodal features. These processed unimodal features of all modalities were then concatenated and passed through a one-dimensional convolutional neural network (CNN) to generate predictions. For further details and access to the code of our LF-LSTM implementation, refer to the Code Availability section.\n\nTo emulate real-world application scenarios, we constructed input samples for each modality as 2-D feature matrices derived from one-second segments. Our evaluation methodology and data partitioning focused on two primary types: trial-independent and subject-independent schemes. In the trial-independent scheme, all samples were randomly shuffled, followed by k-fold cross-validation (where k=5). This process ensured that samples derived from the same monitoring task of a participant were distributed between the training and test sets. On the other hand, for the subject-independent scheme, we implemented the leaveone-subject-out (LOSO) cross-validation  [75] .\n\nThe mean and standard deviation values of the threeclass accuracy for both unimodal and multimodal fusion classifications under two evaluation schemes are detailed in Table  9 . In the trial-independent evaluation, the highest achieved classification accuracy was 72.33%, reflecting the high quality of the data in the MOCAS dataset. Conversely, in the subject-independent evaluation, the highest accuracy decreased to 46.13%. This reduction is consistent with findings in previous studies  [76] ,  [77] ,  [75]  and can be attributed to the inherent challenges posed by the LOSO validation method. The LOSO approach, influenced by individual variances, complicates the development of a universally robust model. Moreover, our chosen classification network, the LF-LSTM, does not inherently adapt to these individual differences. Consequently, while the model demonstrates strong performance in a trial-independent context, its efficacy is diminished in the subject-independent scenario, reflecting the added complexity of generalizing across diverse individual datasets. This, in turn, provides an exciting venue for future research opportunities. The discrepancy in performance between trial-independent and subject-independent evaluations highlights the importance of developing more sophisticated models and techniques that can effectively account for individual differences. This area of research is particularly promising as it pushes the boundaries of current methodologies, encouraging innovation in personalized and adaptive modeling. Subsequently, we conducted an independent twosample t-test to compare the classification results across different modalities on the trial-independent evaluation. Generally, unimodal cognitive workload recognition using EEG POW modality performs significantly better than using EAR modality (p = .05), EEG modality (p = .01), and other single modalities (p<.001). EEG-related modalities, including EEG and EEG POW, perform well since EEG signals correspond more directly to different brain activities under different workload levels  [78] . Compared with EEG modality, EEG POW modality decomposes the raw EEG signals into component frequency bands, whose features are more intuitive and have fewer noises, leading to better classification results  [79] . Moreover, EAR modality also achieves reasonable performance, we owe this to the fact that when facing different monitoring task levels where different numbers of camera views and robot speed, eye movements of participants would change correspondingly. Furthermore, classification using multimodal fusion significantly achieves better performance than using EEG POW (p = .01) and any other single modalities (p<.001). This reflects the benefits of the multimodal fusion mentioned in Section Background and Summary.\n\nFurthermore, there is considerable scope for enhancing classification performance on the MOCAS dataset. Key areas for improvement include the adoption of more sophisticated classification models, as suggested by recent studies  [18] ,  [80] ,  [81] , and refined data preprocessing techniques. Particularly for improving performance in subject-independent or LOSO evaluations, it is crucial to account for individual differences. Factors such as demographic information, personal traits, and task-specific experience should be considered. Advancements in transfer learning  [77] , user-specific attention mechanisms  [82] ,  [83] , and few-shot learning  [84]  methodologies are poised to make significant contributions in this regard. By integrating these approaches, we can develop more robust models that better accommodate the unique characteristics of individual subjects, thereby improving classification accuracy in diverse and personalized settings.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset Access",
      "text": "In order to protect the sensitive data and privacy of human subjects (e.g., physiological signals and facial views), only authorized researchers who consent to the End User License Agreement (EULA) are allowed to download the MOCAS. The researchers who want to access the MOCAS should visit our website and download the ELUA document; https: //polytechnic.purdue.edu/ahmrs/dataset. After reviewing and filling the document up, they should email it to info@ smart-laboratory.org and then request the access through Zenodo (https://zenodo.org/). Then, our research group will review and grant their access to our Zenodo repository having the downsampled MOCAS dataset, subjective information, and supplementary codes used in this paper. For sharing the raw dataset, we will sequentially invite their email address used in the Zenodo and the EULA document to access raw dataset uploaded on an additional repository (Purdue BOX, https://purdue.box.com/v/mocas-dataset), due to huge size of the raw dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Missing Data",
      "text": "Participant 2 (P2) discontinued experiments due to personal reasons, resulting in missing data for Task C and Task D from P2;\n\n• P 2 mouse cam 1 speed 60 • P 2 mouse cam 2 speed 40 Out of 187 files, one file does not contain the Emotive Insight data due to sensor disconnection. However, it includes other physiological and behavioral signals;\n\n• P 3 mouse cam 4 speed 60 signal 0.db3, where the term mouse in the file name refers to a type of input interface used in user experiments.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Code Availability",
      "text": "The code utilized for data format converting (ROSbag2 to CSV) and preprocessing with the code of LSTM and LF-LSTM used for classification validation is included in the supplementary code file of the dataset repository. Additionally, a command-line tool of the ROS2 middleware for replaying the ROSbag2 format files using the command of ros2 bag play {rosbag2 file name} or rqt bag is also available there.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion & Limitations",
      "text": "We conducted experiments with real multi-robot systems to collect physiological and behavioral signals from human subjects while they watched real-time video streamed by the multi-robot systems. However, we encountered several challenges during the data collection process, including ensuring stable and real-time data collection, keeping the participants engaged, and dealing with discomfort caused by the EEG headset. To address these issues, we used ROS2 for real-time computing, designed realistic missions and provided extra compensation to increase participant engagement, and offered breaks during the experiment to alleviate the discomfort caused by the EEG headset.\n\nThen, we built the MOCAS dataset by collecting behavioral and physiological signals using practical wearable biosensors and devices in real-life scenarios. Compared to other existing datasets listed in Table  1 , our MOCAS dataset provides both raw physiological and behavioral data, as well as pre-processed features in real-time, from more realistic scenarios and stimuli. It also includes annotations of emotional states and cognitive load, which can be useful for training and evaluating machine learning models for different applications, such as emotion recognition and mental workload estimation. Moreover, the MOCAS dataset offers raw ROSbag files recorded during the entire experiment, enabling researchers to easily understand the situation and match it with the raw data. They can also test their prediction algorithms in real-time without requiring further experiments.\n\nHowever, MOCAS dataset still has some limitations on the introduced dataset, so it should be considered before using this dataset as following limitations:\n\nFor the physiological signals, the commercial wearable biosensors used in this dataset tend to easily have unknown noises influenced by the participant's movement. There are available open-source libraries to remove common noises for the biosensors, such as NeuroKit2  [60] , pyphysio  [85]  and BioSPPy  [86] . The downsampled CSV files of our dataset were cleaned using the NeuroKit2 to remove the noises from the BVP and GSR signals.\n\nFor the behavioral data, the raw MOCAS dataset contains behavioral data obtained from the front facial videos of individuals who have agreed to share their data with the public. However, this dataset has a limitation in that it also includes behavioral data recorded from individuals wearing facial masks. This was due to the campus regulations during the data collection experiment, which mandated that everyone wear facial masks during the COVID-19 pandemic. Despite this limitation in the raw MOCAS, we extracted the behavioral features (such as EAR and AUs) using Me-diaPipe which can measure facial landmarks regardless of whether the individual is wearing a facial mask, then add the features on the prepossess MOCAS.\n\nFor the size of dataset and participants, this dataset comprises multimodal signals collected from 21 participants. We acknowledge that the number of subjects in our study is relatively small compared to certain existing dataset, with a total of 21 subjects included in the final analysis. As discussed in Section 3, the sample size was constrained by subject availability and the inclusion criteria for the study. Additionally, explicit informed consent was obtained from all participants, resulting in the exclusion of certain potential subjects from the final analysis. In order to mitigate the limitations imposed by the dataset size, we employed k-fold cross-validation to construct a robust deep learning model for predicting human cognitive workloads. k-fold crossvalidation is a widely recognized technique used to estimate the performance of a model on unseen data. It involves partitioning the dataset into K equally sized folds and training the model k times, with each fold serving as the test set once  [87] . This method is extensively employed in machine learning research and practice to mitigate overfitting and maximize the utilization of available data. By leveraging this validation approach, our deep learning model achieved a predictive accuracy of 74.68% in categorizing three levels of cognitive workloads, namely low, medium, and high.\n\nOther important limitation of this dataset is that it was collected from a restricted age range of participants. Although we aimed to select a diverse sample within this age range through official flyers and snowball sampling  [88] , our findings may not be generalizable to other age groups. This limitation is particularly relevant for phenomena known to vary across the lifespan, such as cognitive or physical abilities  [89] . Therefore, future studies could consider expanding the age range to include a wider range of ages, which would provide a more comprehensive understanding of the phenomenon under investigation  [90] . In addition, future research could explore potential age-related differences in our variables of interest to further enhance our understanding of the phenomenon  [91] . Despite this limitation, our study offers valuable insights within the age range studied and sets the stage for future research to build upon our findings.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a new multimodal dataset for human cognitive workload. The dataset includes physiological signals and behavioral features measured from 21 human subjects conducting the generalized CCTV monitoring task with a real multi-robot system. The physiological signals are acquired by the two wearable sensors, such as EEG, PPG, GSR, HR, IBI, SKT, and motion data. The behavioral features include eye ratios, facial expression, and facial action units extracted from a facial view of the webcam. The proposed dataset consists of raw and downsampled data, a summary of the participants' information, and the results of the subjective questionnaires. The total size of the raw dataset is about 722.4 GB, including 754 rosbag2 files. For the downsampled dataset, '.csv' (Comma Separated Value) and '.pkl' (Pickle) file formats were converted from raw datasets with sampling rates of 100 Hz. The size was 51.9 GB and 32.8 GB respectively.\n\nWe also validated the quality of the dataset by analyzing the correlation between personality traits, physiological signals and behavioral features, evaluating the effects of within-subjects factors on the results of the questionnaires presented to subjects, and applying an LF-LSTM to classify the three-class cognitive workload classifications. As a result, we found that there are significant differences in factors within subjects (e.g., the number of camera views and robot speed) using statistical analysis, and also showed that the classification performance of the multimodal dataset outperforms that of the single-modal dataset through deep learning methods.\n\nAdditionally, we made the MOCAS dataset publicly available by uploading the dataset to the online repositories and codes used in this dataset. We hope that the proposed dataset can become a fundamental resource for other researchers to develop systems and algorithms in human cognitive workloads.",
      "page_start": 14,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the design of MOCAS dataset. MOCAS was designed with consideration of a task scenario typical in",
      "page": 2
    },
    {
      "caption": "Figure 1: ), in which one human subject undertakes a simul-",
      "page": 2
    },
    {
      "caption": "Figure 2: ). As depicted in Fig. 3, participants were asked to",
      "page": 3
    },
    {
      "caption": "Figure 3: a (for more information refer to [50]).",
      "page": 4
    },
    {
      "caption": "Figure 2: a) to provide video streams; also in the room was",
      "page": 4
    },
    {
      "caption": "Figure 2: b) with which partici-",
      "page": 4
    },
    {
      "caption": "Figure 2: Illustration of the designed CCTV monitoring task",
      "page": 4
    },
    {
      "caption": "Figure 3: Illustration of designed CCTV monitoring task show-",
      "page": 5
    },
    {
      "caption": "Figure 4: , all sensors, devices and GUI",
      "page": 5
    },
    {
      "caption": "Figure 5: illustrates the overall",
      "page": 5
    },
    {
      "caption": "Figure 4: Overall system configurations used in this user experiment for data collection and storing process.",
      "page": 6
    },
    {
      "caption": "Figure 5: Overall procedures for the data collection in the experiment. The supplementary video about this procedure is able",
      "page": 6
    },
    {
      "caption": "Figure 5: : baseline",
      "page": 6
    },
    {
      "caption": "Figure 6: to report subjective cognitive workload",
      "page": 6
    },
    {
      "caption": "Figure 6: Graphical user interfaces for subjective questionnaires used in the user experiment: (a) SAM for measuring emotional",
      "page": 7
    },
    {
      "caption": "Figure 7: Examples of the raw and cleaned collected physiological and behavioral data: raw and cleaned (a) BVP data and",
      "page": 7
    },
    {
      "caption": "Figure 8: shows a folder tree of the online repository that",
      "page": 7
    },
    {
      "caption": "Figure 8: A folder tree of the online repository that displays",
      "page": 7
    },
    {
      "caption": "Figure 7: e and Fig 7f), through a face landmark",
      "page": 8
    },
    {
      "caption": "Figure 9: Correlation matrix for the correlation between per-",
      "page": 9
    },
    {
      "caption": "Figure 9: shows the results",
      "page": 9
    },
    {
      "caption": "Figure 10: The two-way Friedman test does not require",
      "page": 10
    },
    {
      "caption": "Figure 10: illustrates the results of the two-way rmANOVA",
      "page": 10
    },
    {
      "caption": "Figure 10: The data distribution by variables of (a) raw NASA-TLX scores and (b) weighted NASA-TLX scores (***: p<.001).",
      "page": 11
    },
    {
      "caption": "Figure 11: Illustration of LF-LSTM model for multimodal fu-",
      "page": 11
    },
    {
      "caption": "Figure 11: , our LF-LSTM model processed",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Facial features, \neye features\nBVP, GSR, HR, \nIBI, SK, ACC\nEEG, ACC": "",
          "Screen view\nMouse position and buttons": ""
        },
        {
          "Facial features, \neye features\nBVP, GSR, HR, \nIBI, SK, ACC\nEEG, ACC": ": Physiological signals\n: Behavioral signals",
          "Screen view\nMouse position and buttons": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Number of participants": "Age",
          "21 (7 females and 14 males)": "from 18 to 37 (mean=24.3 age, std.=5.2 age)"
        },
        {
          "Number of participants": "Average time\nper experimental task",
          "21 (7 females and 14 males)": "mean= 3.52 mins, std.=0.28 mins"
        },
        {
          "Number of participants": "Personality traits",
          "21 (7 females and 14 males)": "Extraversion, Emotional stability, Agreeableness, Conscientiousness, and Intellect/Imagination"
        },
        {
          "Number of participants": "Physiological signals",
          "21 (7 females and 14 males)": "Empatica E4: BVP, GSR, HR, IBI, SKT\nEmotiv Insight: raw 5-channel EEGs, EEG band powers (i.e., α, β, γ, and θ), performance metrics"
        },
        {
          "Number of participants": "Behavioral features",
          "21 (7 females and 14 males)": "facial view (30 Hz), facial features & expressions (30 Hz),\nand Mouse positions & button clicking status (True or False)"
        },
        {
          "Number of participants": "Experiment status",
          "21 (7 females and 14 males)": "0=loading phase, 1=baseline phase, 2=main phase, 4=Evaluation phase (SAM),\n5=Evaluation phase (ISA), 6=Evaluation phase (NASA-TLX), and 7=Score phase"
        },
        {
          "Number of participants": "Experiment recording",
          "21 (7 females and 14 males)": "Screen record video (30 Hz)"
        },
        {
          "Number of participants": "Experiment scores",
          "21 (7 females and 14 males)": "Obtained scores, Success click, Failure click, and Success rate (=success clicks/all clicks)"
        },
        {
          "Number of participants": "Subjective annotations",
          "21 (7 females and 14 males)": "SAM: two categories; valence (from negative to positive) and\narousal domain (from calm to excited) with a range from -4 to +4\nISA: five categories; Underutilized (-2), Relaxed (-1), Comfortable (0), High (1),\nand Excessive (2)\nNASA-TLX: Seven categories for measuring workloads; Mental demand,\nPhysical demand, Temporal demand, Performance, Effort, and Frustration\nwith a range from 1 to 7"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: Results of independent sample t-test of",
      "data": [
        {
          "Device/Source": "Empatica E4 Wristband",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "64 Hz\nBVP (PPG)\n1\nN/A"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "4 Hz\n[0.01µS, 100µS]\nGSR\n1"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "1 Hz\nHR (from BVP)\n1\nN/A"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "1 Hz\nIBI (from BVP)\n1\nN/A"
        },
        {
          "Device/Source": "Emotive Insight",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "128 Hz\nEEG (with Contact Quality)\n6\nN/A"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "8 Hz\n[0, 100]\nEEG band powers\n25"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "1 Hz\n[0, 1]\nPerformance metrics\n7"
        },
        {
          "Device/Source": "Intel RealSense D435i",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "1 Hz\nAction Unit (AU)\n3 (x, y, average)\nN/A"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "1 Hz\nEye Aspect Ratio (EAR)\n3 (left, right, average)\nN/A"
        },
        {
          "Device/Source": "Subjective Annotations",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "Cognitive Load (from ISA)\n1\nN/A\n[-2,2]"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "Cognitive Load (from NASA-TLX-Weighted)\n1\nN/A\n[0,100]"
        },
        {
          "Device/Source": "",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "Emotion (from SAM)\n2 (arousal, valence)\nN/A\n[-4,4]"
        },
        {
          "Device/Source": "GUI program",
          "Collected data\nChannels\nSampling rate\nSignal range [min, max]": "[0, 7]\nExperiment states\n1\nN/A"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: Results of independent sample t-test of",
      "data": [
        {
          "0.31\n-0.37\n0.07\n0.34\n0.29\n-0.56\n-0.12\n-0.29\n-0\n0.19": "-0.14\n0.33\n0.03\n-0.01"
        },
        {
          "0.31\n-0.37\n0.07\n0.34\n0.29\n-0.56\n-0.12\n-0.29\n-0\n0.19": "-0.31\n-0.01\n0.37\n-0.57\n0.34\n-0.07\n-0.41\n0.57\n0.14\n0.49\n0.21\n0.23"
        },
        {
          "0.31\n-0.37\n0.07\n0.34\n0.29\n-0.56\n-0.12\n-0.29\n-0\n0.19": "0.21\n0.51\n-0.37\n0.4\n-0.17\n-0.05\n0.17\n-0.18\n0.17"
        },
        {
          "0.31\n-0.37\n0.07\n0.34\n0.29\n-0.56\n-0.12\n-0.29\n-0\n0.19": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Critical considerations for human-robot interface development",
      "authors": [
        "J Adams"
      ],
      "year": "2002",
      "venue": "Proceedings of 2002 AAAI Fall Symposium"
    },
    {
      "citation_id": "2",
      "title": "Multiple resources and performance prediction",
      "authors": [
        "C Wickens"
      ],
      "year": "2002",
      "venue": "Theoretical issues in ergonomics science"
    },
    {
      "citation_id": "3",
      "title": "A review of the mental workload literature",
      "authors": [
        "B Cain"
      ],
      "year": "2007",
      "venue": "A review of the mental workload literature"
    },
    {
      "citation_id": "4",
      "title": "The index of cognitive activity: Measuring cognitive workload",
      "authors": [
        "S Marshall"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE 7th conference on Human Factors and Power Plants"
    },
    {
      "citation_id": "5",
      "title": "Driver experience and cognitive workload in different traffic environments",
      "authors": [
        "C Patten",
        "A Kircher",
        "J Östlund",
        "L Nilsson",
        "O Svenson"
      ],
      "year": "2006",
      "venue": "Accident Analysis & Prevention"
    },
    {
      "citation_id": "6",
      "title": "Using mobile telephones: cognitive workload and attention resource allocation",
      "authors": [
        "C Patten",
        "A Kircher",
        "J Östlund",
        "L Nilsson"
      ],
      "year": "2004",
      "venue": "Accident analysis & prevention"
    },
    {
      "citation_id": "7",
      "title": "Neuroindices of cognitive workload: Neuroimaging, pupillometric and event-related potential studies of brain work",
      "authors": [
        "M Just",
        "P Carpenter",
        "A Miyake"
      ],
      "year": "2003",
      "venue": "Theoretical Issues in Ergonomics Science"
    },
    {
      "citation_id": "8",
      "title": "Pupil dilation as an indicator of cognitive workload in human-computer interaction",
      "authors": [
        "M Pomplun",
        "S Sunkara"
      ],
      "year": "2003",
      "venue": "Proceedings of the International Conference on HCI"
    },
    {
      "citation_id": "9",
      "title": "Adaptive workload allocation for multi-human multi-robot teams for independent and homogeneous tasks",
      "authors": [
        "T Mina",
        "S Kannan",
        "W Jo",
        "B.-C Min"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Development of nasa-tlx (task load index): Results of empirical and theoretical research",
      "authors": [
        "S Hart",
        "L Staveland"
      ],
      "year": "1988",
      "venue": "Advances in psychology"
    },
    {
      "citation_id": "11",
      "title": "Multimodal fusion for objective assessment of cognitive workload: a review",
      "authors": [
        "E Debie",
        "R Rojas",
        "J Fidock",
        "M Barlow",
        "K Kasmarik",
        "S Anavatti",
        "M Garratt",
        "H Abbass"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "12",
      "title": "Multimodal behavior and interaction as indicators of cognitive load",
      "authors": [
        "F Chen",
        "N Ruiz",
        "E Choi",
        "J Epps",
        "M Khawaja",
        "R Taib",
        "B Yin",
        "Y Wang"
      ],
      "year": "2013",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "13",
      "title": "Measuring workload using a combination of electroencephalography and near infrared spectroscopy",
      "authors": [
        "E Coffey",
        "A.-M Brouwer",
        "J Van Erp"
      ],
      "year": "2012",
      "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting"
    },
    {
      "citation_id": "14",
      "title": "Hybrid fnirs-eeg based classification of auditory and visual perception processes",
      "authors": [
        "F Putze",
        "S Hesslinger",
        "C.-Y Tse",
        "Y Huang",
        "C Herff",
        "C Guan",
        "T Schultz"
      ],
      "year": "2014",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "15",
      "title": "Mental workload classification with concurrent electroencephalography and functional nearinfrared spectroscopy",
      "authors": [
        "Y Liu",
        "H Ayaz",
        "P Shewokis"
      ],
      "year": "2017",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "16",
      "title": "Multisubject \"learning\" for mental workload classification using concurrent eeg, fnirs, and physiological measures",
      "year": "2017",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Identification of driver cognitive workload using support vector machines with driving performance, physiology and eye movement in a driving simulator",
      "authors": [
        "J Son",
        "H Oh",
        "M Park"
      ],
      "year": "2013",
      "venue": "International Journal of Precision Engineering and Manufacturing"
    },
    {
      "citation_id": "18",
      "title": "Husformer: A multi-modal transformer for multi-modal human state recognition",
      "authors": [
        "R Wang",
        "W Jo",
        "D Zhao",
        "W Wang",
        "B Yang",
        "G Chen",
        "B.-C Min"
      ],
      "year": "2022",
      "venue": "Husformer: A multi-modal transformer for multi-modal human state recognition",
      "arxiv": "arXiv:2209.15182"
    },
    {
      "citation_id": "19",
      "title": "The influence of personality traits and cognitive load on the use of adaptive user interfaces",
      "authors": [
        "K Gajos",
        "K Chauncey"
      ],
      "year": "2017",
      "venue": "Proceedings of the 22nd International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "20",
      "title": "Datasets for cognitive load inference using wearable sensors and psychological traits",
      "authors": [
        "M Gjoreski",
        "T Kolenik",
        "T Knez",
        "M Luštrek",
        "M Gams",
        "H Gjoreski",
        "V Pejović"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "21",
      "title": "Capacity and comprehension: Spontaneous stereotyping under cognitive load",
      "authors": [
        "D Wigboldus",
        "J Sherman",
        "H Franzese",
        "A Knippenberg"
      ],
      "year": "2004",
      "venue": "Social Cognition"
    },
    {
      "citation_id": "22",
      "title": "Immersion measurement in watching videos using eye-tracking data",
      "authors": [
        "Y Choi",
        "J Kim",
        "J.-H Hong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Emotion, cognitive load and learning outcomes during simulation training",
      "authors": [
        "K Fraser",
        "I Ma",
        "E Teteris",
        "H Baxter",
        "B Wright",
        "K Mclaughlin"
      ],
      "year": "2012",
      "venue": "Medical education"
    },
    {
      "citation_id": "24",
      "title": "Tuning down the emotional brain: an fmri study of the effects of cognitive load on the processing of affective images",
      "authors": [
        "L Van Dillen",
        "D Heslenfeld",
        "S Koole"
      ],
      "year": "2009",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "25",
      "title": "Interaction of cognitive and affective load within a virtual city",
      "authors": [
        "T Parsons",
        "J Asbee",
        "C Courtney"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Four ways of considering emotion in cognitive load theory",
      "authors": [
        "J Plass",
        "S Kalyuga"
      ],
      "year": "2019",
      "venue": "Educational Psychology Review"
    },
    {
      "citation_id": "27",
      "title": "Stew: Simultaneous task eeg workload data set",
      "authors": [
        "W Lim",
        "O Sourina",
        "L Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "28",
      "title": "Quasar's qstates cognitive gauge performance in the cognitive state assessment competition 2011",
      "authors": [
        "N Mcdonald",
        "W Soussou"
      ],
      "year": "2011",
      "venue": "2011 Annual International Conference of the IEEE"
    },
    {
      "citation_id": "29",
      "title": "Wauc: a multi-modal database for mental workload assessment under physical activity",
      "authors": [
        "I Albuquerque",
        "A Tiwari",
        "M Parent",
        "R Cassani",
        "J.-F Gagnon",
        "D Lafond",
        "S Tremblay",
        "T Falk"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "30",
      "title": "A data set of real world driving to assess driver workload",
      "authors": [
        "S Schneegass",
        "B Pfleging",
        "N Broy",
        "F Heinrich",
        "A Schmidt"
      ],
      "year": "2013",
      "venue": "Proceedings of the 5th international conference on automotive user interfaces and interactive vehicular applications"
    },
    {
      "citation_id": "31",
      "title": "Maus: A dataset for mental workload assessmenton n-back task using wearable sensor",
      "authors": [
        "W.-K Beh",
        "Y.-H Wu"
      ],
      "year": "2021",
      "venue": "Maus: A dataset for mental workload assessmenton n-back task using wearable sensor",
      "arxiv": "arXiv:2111.02561"
    },
    {
      "citation_id": "32",
      "title": "Mmod-cog: A database for multimodal cognitive load classification",
      "authors": [
        "I Mijić",
        "M Šarlija",
        "D Petrinović"
      ],
      "year": "2019",
      "venue": "2019 11th international symposium on image and signal processing and analysis (ispa)"
    },
    {
      "citation_id": "33",
      "title": "Improving fluid intelligence with training on working memory",
      "authors": [
        "S Jaeggi",
        "M Buschkuehl",
        "J Jonides",
        "W Perrig"
      ],
      "year": "2008",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "34",
      "title": "Nback working memory paradigm: A meta-analysis of normative functional neuroimaging studies",
      "authors": [
        "A Owen",
        "K Mcmillan",
        "A Laird",
        "E Bullmore"
      ],
      "year": "2005",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "35",
      "title": "Loaded with fun? the impact of enjoyment and cognitive load on brand retention in digital games",
      "authors": [
        "T Vyvey",
        "E Castellar",
        "J Van Looy"
      ],
      "year": "2018",
      "venue": "Journal of Interactive Advertising"
    },
    {
      "citation_id": "36",
      "title": "Evaluating and managing cognitive load in games",
      "authors": [
        "S Kalyuga",
        "J Plass"
      ],
      "year": "2009",
      "venue": "Handbook of research on effective electronic gaming in education"
    },
    {
      "citation_id": "37",
      "title": "Noise and the reality gap: The use of simulation in evolutionary robotics",
      "authors": [
        "N Jakobi",
        "P Husbands",
        "I Harvey"
      ],
      "year": "1995",
      "venue": "European Conference on Artificial Life"
    },
    {
      "citation_id": "38",
      "title": "Instantaneous self-assessment of workload technique (isa)",
      "authors": [
        "C Jordan",
        "S Brennen"
      ],
      "year": "1992",
      "venue": "Defence Research Agency"
    },
    {
      "citation_id": "39",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "40",
      "title": "The six factor personality questionnaire",
      "authors": [
        "D Jackson",
        "P Tremblay"
      ],
      "year": "2002",
      "venue": "The six factor personality questionnaire"
    },
    {
      "citation_id": "41",
      "title": "Clas: A database for cognitive load, affect and stress recognition",
      "authors": [
        "V Markova",
        "T Ganchev",
        "K Kalinkov"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Biomedical Innovations and Applications (BIA)"
    },
    {
      "citation_id": "42",
      "title": "The tufts fnirs mental workload dataset & benchmark for brain-computer interfaces that generalize",
      "authors": [
        "Z Huang",
        "L Wang",
        "G Blaney",
        "C Slaughter",
        "D Mckeon",
        "Z Zhou",
        "R Jacob",
        "M Hughes"
      ],
      "year": "2021",
      "venue": "Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks"
    },
    {
      "citation_id": "43",
      "title": "Machine learning for real-time singletrial eeg-analysis: from brain-computer interfacing to mental state monitoring",
      "authors": [
        "M K.-R. M Üller",
        "G Tangermann",
        "M Dornhege",
        "G Krauledat",
        "B Curio",
        "Blankertz"
      ],
      "year": "2008",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "44",
      "title": "Affective computing and biometrics based hci surveillance system",
      "authors": [
        "R Wang",
        "B Fang"
      ],
      "year": "2008",
      "venue": "2008 international symposium on information science and engineering"
    },
    {
      "citation_id": "45",
      "title": "Cognitive evaluation of humanhuman and human-machine cooperation modes in air traffic control",
      "authors": [
        "J.-M Hoc",
        "M.-P Lemoine"
      ],
      "year": "1998",
      "venue": "The International Journal of Aviation Psychology"
    },
    {
      "citation_id": "46",
      "title": "Common work space for human-machine cooperation in air traffic control",
      "authors": [
        "M Pacaux-Lemoine",
        "S Debernard"
      ],
      "year": "2002",
      "venue": "Control Engineering Practice"
    },
    {
      "citation_id": "47",
      "title": "The neuroergonomic evaluation of human machine interface design in air traffic control using behavioral and eeg/erp measures",
      "authors": [
        "L Giraudet",
        "J.-P Imbert",
        "M Berenger",
        "S Tremblay",
        "M Causse"
      ],
      "year": "2015",
      "venue": "Behavioural brain research"
    },
    {
      "citation_id": "48",
      "title": "Queueing network-model human processor (qn-mhp) a computational architecture for multitask performance in human-machine systems",
      "authors": [
        "Y Liu",
        "R Feyen",
        "O Tsimhoni"
      ],
      "year": "2006",
      "venue": "ACM Transactions on Computer-Human Interaction (TOCHI)"
    },
    {
      "citation_id": "49",
      "title": "A framework to guide the assessment of human-machine systems",
      "authors": [
        "K Stowers",
        "J Oglesby",
        "S Sonesh",
        "K Leyva",
        "C Iwig",
        "E Salas"
      ],
      "year": "2017",
      "venue": "Human factors"
    },
    {
      "citation_id": "50",
      "title": "Smart-teleload: A new graphic user interface to generate affective loads for teleoperation",
      "authors": [
        "W Jo",
        "G.-E Cha",
        "D Foti",
        "B.-C Min"
      ],
      "year": "2024",
      "venue": "SoftwareX"
    },
    {
      "citation_id": "51",
      "title": "Smartmbot: A ros2-based low-cost and open-source mobile robot platform",
      "authors": [
        "W Jo",
        "J Kim",
        "R Wang",
        "J Pan",
        "R Senthilkumaran",
        "B.-C Min"
      ],
      "year": "2022",
      "venue": "Smartmbot: A ros2-based low-cost and open-source mobile robot platform",
      "arxiv": "arXiv:2203.08903"
    },
    {
      "citation_id": "52",
      "title": "ROSbagbased multimodal affective dataset for emotional and cognitive states",
      "authors": [
        "W Jo",
        "S Kannan",
        "G.-E Cha",
        "A Lee",
        "B.-C Min"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)"
    },
    {
      "citation_id": "53",
      "title": "An alternative\" description of personality\": the big-five factor structure",
      "authors": [
        "L Goldberg"
      ],
      "year": "1990",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "54",
      "title": "Mental fatigue and the control of cognitive processes: Effects on perseveration and planning",
      "authors": [
        "D Van Der Linden",
        "M Frese",
        "T Meijman"
      ],
      "year": "2003",
      "venue": "Acta Psychologica"
    },
    {
      "citation_id": "55",
      "title": "The effect of mental stress on heart rate variability and blood pressure during computer work",
      "authors": [
        "N Hjortskov",
        "D Rissén",
        "A Blangsted",
        "N Fallentin",
        "U Lundberg",
        "K Søgaard"
      ],
      "year": "2004",
      "venue": "European journal of applied physiology"
    },
    {
      "citation_id": "56",
      "title": "Taking micro-breaks at work: effects of watching funny short-form videos on subjective experience, phys-iological stress, and task performance",
      "authors": [
        "Y Liu",
        "Q Gao",
        "L Ma"
      ],
      "year": "2021",
      "venue": "Cross-Cultural Design. Applications in Arts, Learning, Well-being, and Social Development: 13th International Conference, CCD 2021, Held as Part of the 23rd HCI International Conference, HCII 2021, Virtual Event"
    },
    {
      "citation_id": "57",
      "title": "Investigating the effect of deictic movements of a multi-robot",
      "authors": [
        "A Lee",
        "W Jo",
        "S Kannan",
        "B.-C Min"
      ],
      "year": "2021",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "58",
      "title": "pickle -python object serializationpython 3.10.5 documentation",
      "authors": [
        "G Van Rossum"
      ],
      "year": "2020",
      "venue": "pickle -python object serializationpython 3.10.5 documentation"
    },
    {
      "citation_id": "59",
      "title": "mmappickle: Python 3 module to store memorymapped numpy array in pickle format",
      "authors": [
        "L Fasnacht"
      ],
      "year": "2018",
      "venue": "Journal of Open Source Software"
    },
    {
      "citation_id": "60",
      "title": "NeuroKit2: A python toolbox for neurophysiological signal processing",
      "authors": [
        "D Makowski",
        "T Pham",
        "Z Lau",
        "J Brammer",
        "F Lespinasse",
        "H Pham",
        "C Sch",
        "S Chen"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "61",
      "title": "Arousal effects on pupil size, heart rate, and skin conductance in an emotional face task",
      "authors": [
        "C.-A Wang",
        "T Baird",
        "J Huang",
        "J Coutinho",
        "D Brien",
        "D Munoz"
      ],
      "year": "2018",
      "venue": "Frontiers in neurology"
    },
    {
      "citation_id": "62",
      "title": "Performance evaluation and comparison of software for face recognition, based on dlib and opencv library",
      "authors": [
        "N Boyko",
        "O Basystiuk",
        "N Shakhovska"
      ],
      "year": "2018",
      "venue": "2018 IEEE Second International Conference on Data Stream Mining & Processing"
    },
    {
      "citation_id": "63",
      "title": "Mediapipe: A framework for building perception pipelines",
      "authors": [
        "C Lugaresi",
        "J Tang",
        "H Nash",
        "C Mcclanahan",
        "E Uboweja",
        "M Hays",
        "F Zhang",
        "C.-L Chang",
        "M Yong",
        "J Lee"
      ],
      "year": "2019",
      "venue": "Mediapipe: A framework for building perception pipelines",
      "arxiv": "arXiv:1906.08172"
    },
    {
      "citation_id": "64",
      "title": "Efficient online engagement analytics algorithm toolkit that can run on edge",
      "authors": [
        "S Thiha",
        "J Rajasekera"
      ],
      "year": "2023",
      "venue": "Algorithms"
    },
    {
      "citation_id": "65",
      "title": "Pisani, R. Purves",
      "authors": [
        "D Freedman",
        "R Pisani",
        "R Purves"
      ],
      "year": "2007",
      "venue": "Pisani, R. Purves"
    },
    {
      "citation_id": "66",
      "title": "Statistical principles in experimental design",
      "authors": [
        "B Winer"
      ],
      "year": "1971",
      "venue": "Statistical principles in experimental design"
    },
    {
      "citation_id": "67",
      "title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance",
      "authors": [
        "M Friedman"
      ],
      "year": "1937",
      "venue": "Journal of the american statistical association"
    },
    {
      "citation_id": "68",
      "title": "How high is high? a meta-analysis of nasa-tlx global workload scores",
      "authors": [
        "R Grier"
      ],
      "year": "2015",
      "venue": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting"
    },
    {
      "citation_id": "69",
      "title": "High fidelity simulation to assess task load index and performance: a prospective observational study",
      "authors": [
        "J Favre-Félix",
        "M Dziadzko",
        "C Bauer",
        "A Duclos",
        "J.-J Lehot",
        "T Rimmelé",
        "M Lilot"
      ],
      "year": "2021",
      "venue": "High fidelity simulation to assess task load index and performance: a prospective observational study"
    },
    {
      "citation_id": "70",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "71",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "72",
      "title": "Supervised sequence labelling with recurrent neural networks",
      "authors": [
        "A Graves"
      ],
      "year": "2012",
      "venue": "Supervised sequence labelling with recurrent neural networks"
    },
    {
      "citation_id": "73",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "74",
      "title": "Dual-stream recurrent neural network for video captioning",
      "authors": [
        "N Xu",
        "A.-A Liu",
        "Y Wong",
        "Y Zhang",
        "W Nie",
        "Y Su",
        "M Kankanhalli"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "75",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen",
        "S Li",
        "K Hou",
        "B Hu",
        "J Gao",
        "T Zhang"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "76",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "77",
      "title": "Personalzscore: Eliminating individual difference for eeg-based crosssubject emotion recognition",
      "authors": [
        "H Chen",
        "S Sun",
        "J Li",
        "R Yu",
        "N Li",
        "X Li",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "78",
      "title": "Using electroencephalography to measure cognitive load",
      "authors": [
        "P Antonenko",
        "F Paas",
        "R Grabner",
        "T Van Gog"
      ],
      "year": "2010",
      "venue": "Educational psychology review"
    },
    {
      "citation_id": "79",
      "title": "The utility of eeg band power analysis in the study of infancy and early childhood",
      "authors": [
        "J Saby",
        "P Marshall"
      ],
      "year": "2012",
      "venue": "Developmental neuropsychology"
    },
    {
      "citation_id": "80",
      "title": "Multimodal spatiotemporal representation for automatic depression level detection",
      "authors": [
        "M Niu",
        "J Tao",
        "B Liu",
        "J Huang",
        "Z Lian"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "81",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "J Zhou",
        "G Cui",
        "S Hu",
        "Z Zhang",
        "C Yang",
        "Z Liu",
        "L Wang",
        "C Li",
        "M Sun"
      ],
      "year": "2020",
      "venue": "AI Open"
    },
    {
      "citation_id": "82",
      "title": "Nrpa: neural recommendation with personalized attention",
      "authors": [
        "H Liu",
        "F Wu",
        "W Wang",
        "X Wang",
        "P Jiao",
        "C Wu",
        "X Xie"
      ],
      "year": "2019",
      "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "83",
      "title": "Initial task assignment in multi-human multi-robot teams: An attentionenhanced hierarchical reinforcement learning approach",
      "authors": [
        "R Wang",
        "D Zhao",
        "A Gupte",
        "B.-C Min"
      ],
      "year": "2023",
      "venue": "Initial task assignment in multi-human multi-robot teams: An attentionenhanced hierarchical reinforcement learning approach",
      "arxiv": "arXiv:2310.04979"
    },
    {
      "citation_id": "84",
      "title": "Few-shot learning in emotion recognition of spontaneous speech using a siamese neural network with adaptive sample pair formation",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "85",
      "title": "pyphysio: A physiological signal processing library for data science approaches in physiology",
      "authors": [
        "A Bizzego",
        "A Battisti",
        "G Gabrieli",
        "G Esposito",
        "C Furlanello"
      ],
      "year": "2019",
      "venue": "SoftwareX"
    },
    {
      "citation_id": "86",
      "title": "BioSPPy: Biosignal processing in Python",
      "authors": [
        "C Carreiras",
        "A Alves",
        "A Lourenc ¸o",
        "F Canento",
        "H Silva",
        "A Fred"
      ],
      "year": "2015",
      "venue": "BioSPPy: Biosignal processing in Python"
    },
    {
      "citation_id": "87",
      "title": "",
      "authors": [
        "Online"
      ],
      "venue": ""
    },
    {
      "citation_id": "88",
      "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection",
      "authors": [
        "R Kohavi"
      ],
      "year": "1995",
      "venue": "Ijcai"
    },
    {
      "citation_id": "89",
      "title": "Snowball sampling: Problems and techniques of chain referral sampling",
      "authors": [
        "P Biernacki",
        "D Waldorf"
      ],
      "year": "1981",
      "venue": "Sociological methods & research"
    },
    {
      "citation_id": "90",
      "title": "Major Issues in Cognitive Aging",
      "authors": [
        "T Salthouse"
      ],
      "year": "2019",
      "venue": "Major Issues in Cognitive Aging"
    },
    {
      "citation_id": "91",
      "title": "Normal-appearing cerebral white matter in healthy adults: Mean change over 2 years and individual differences in change",
      "authors": [
        "A Bender",
        "N Raz"
      ],
      "year": "2012",
      "venue": "Neurobiology of Aging"
    },
    {
      "citation_id": "92",
      "title": "Wonse Jo received the B.S. in robotics engineering from Hoseo University, South Korea in 2013 and M.S. degrees in electronic engineering from the Kyung-Hee University, South Korea",
      "authors": [
        "E Laukka",
        "G Kalpouzos",
        "A Salami",
        "T.-Q Li",
        "L Fratiglioni",
        "L Bäckman"
      ],
      "year": "2015",
      "venue": "His research interests includes affective robotics/computing, human multi-robot interaction, environmental robotics, and field robotics"
    }
  ]
}