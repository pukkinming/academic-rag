{
  "paper_id": "2408.09186v2",
  "title": "Eeg-Scmm: Soft Contrastive Masked Modeling For Cross-Corpus Eeg-Based Emotion Recognition",
  "published": "2024-08-17T12:35:13Z",
  "authors": [
    "Qile Liu",
    "Weishan Ye",
    "Lingli Zhang",
    "Zhen Liang"
  ],
  "keywords": [
    "EEG",
    "Emotion Recognition",
    "Soft Contrastive Learning",
    "Masked Modeling",
    "Cross-Corpus"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition using electroencephalography (EEG) signals has attracted increasing attention in recent years. However, existing methods often lack generalization in cross-corpus settings, where a model trained on one dataset is directly applied to another without retraining, due to differences in data distribution and recording conditions. To tackle the challenge of cross-corpus EEG-based emotion recognition, we propose a novel framework termed Soft Contrastive Masked Modeling (SCMM). Grounded in the theory of emotional continuity, SCMM integrates soft contrastive learning with a hybrid masking strategy to effectively capture emotion dynamics (refer to short-term continuity). Specifically, in the self-supervised learning stage, we propose a soft weighting mechanism that assigns similarity scores to sample pairs, enabling fine-grained modeling of emotional transitions and capturing the temporal continuity of human emotions. To further enhance representation learning, we design a similarity-aware aggregator that fuses complementary information from semantically related samples based on pairwise similarities, thereby improving feature expressiveness and reconstruction quality. This dual design contributes to a more discriminative and transferable representation, which is crucial for robust cross-corpus generalization. Extensive experiments on the SEED, SEED-IV, and DEAP datasets show that SCMM achieves state-of-the-art (SOTA) performance, outperforming the secondbest method by an average accuracy of 4.26% under both same-class and different-class cross-corpus settings. The source code is available at https://github.com/Kyler-RL/SCMM. \n CCS CONCEPTS â€¢ Human-centered computing â†’ HCI design and evaluation methods; â€¢ Computing methodologies â†’ Artificial intelligence.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are human attitudinal experiences and behavioral responses to objective things, closely related to an individual's health conditions and behavioral patterns  [45] . Compared to speech  [40] , gestures  [31] , and facial expressions  [2] , electroencephalography (EEG) offers a more direct and objective measurement of human emotions by capturing brain activity across various scalp locations  [16] . Therefore, researchers have increasingly emphasized EEGbased emotion recognition in recent years  [50, 51, 53, 56] , aiming to advance the development of affective brain-computer interfaces (aBCIs). However, three critical challenges remain to be addressed in current approaches.\n\n(1) Insufficient Generalization Capability. Most existing EEGbased emotion recognition methods are typically designed for a single dataset, necessitating model retraining when the dataset changes. This requirement significantly limits the scalability and generalizability of the model, hindering its application on different datasets. To address this issue, the concept of cross-corpus has been proposed, which is designed to be generalized across multiple datasets. A cross-corpus model is trained on one dataset and can be directly applied to another without retraining from scratch. This concept, which originated in natural language processing  [38, 52] , has been extended to various domains in recent years  [5, 35, 37] . Although existing EEG-based emotion recognition methods, such as BiDANN  [28] , TANN  [26] , and PR-PL  [57] , have demonstrated superior performance in within-subject or cross-subject tasks within a single dataset, their effectiveness significantly degrades in crosscorpus scenarios, where the differences in data distribution across datasets far exceed the intra-dataset variability  [35] .\n\n(2) Modeling Strategy Limitation. Recently, researchers have explored domain adaptation techniques to address cross-corpus EEG-based emotion recognition  [14, 58] . This is motivated by their  efficacy to solve the problem of domain shift  [10] . Despite initial success, such approaches often require prior access to all labeled source data and unlabeled target data for model training. Considering the difficulties in collecting EEG signals as well as the time and expertise required to label them, the modeling strategy limitation of domain adaptation techniques poses a significant challenge to real-world aBCI applications.\n\n(3) Ignorance of Emotional Continuity. Unlike domain adaptation techniques, contrastive learning (CL) achieves superior performance without relying on labeled data, and has shown great potential in various fields  [4, 9, 34] . Current CL-based methods for EEG-based emotion recognition, such as CLISA  [39]  and JCFA  [29] , consider an anchor and its augmented views as positive pairs, while treating all other samples as negatives. When computing the contrastive loss, the weights for positives and negatives are set to 1 and 0, respectively, as shown in Fig.  1 (b) (Hard CL). However, psychological and neuroscientific studies suggest that emotion analysis using brain signals should account for dynamic changes  [6, 15] . Specifically, emotions exhibit significant \"short-term continuity\" characteristics, meaning that human emotions are relatively stable over certain periods, with sudden changes being rare. As illustrated in Fig.  1 (a), a high cosine similarity is maintained between an anchor sample ğ‘¥ ğ‘– and its neighboring sample ğ‘¥ ğ‘— , and even a distant sample ğ‘¥ ğ‘˜ separated by extended periods (e.g., 60 seconds). Given this nature of emotions, we propose that the definition of positive pairs in CL-based EEG emotion analysis should extend beyond just the anchor and its augmented views. Instead, it should include a broader range of similar samples, especially those that are temporally proximal, as shown in Fig.  1 (b) (SCMM). In contrast, existing methods following the traditional CL paradigm  [4]  may incorrectly pull apart similar but not identical samples, thus failing to capture the emotional continuity inherent in EEG signals.\n\nTo tackle the aforementioned three critical issues, we propose a novel Soft Contrastive Masked Modeling (SCMM) framework for cross-corpus EEG-based emotion recognition. Unlike traditional hard CL shown in Fig.  1 (b), SCMM considers emotional continuity and incorporates soft assignments of sample pairs. This approach enables the model to identify the fine-grained relationships between different samples in a self-supervised manner, thereby enhancing the generalizability of EEG representations. Comprehensive experiments on three well-recognized datasets show that SCMM consistently achieves state-of-the-art (SOTA) performance, demonstrating its superior capability and stability. In summary, the main contributions of SCMM are outlined as follows:\n\nâ€¢ We propose a novel SCMM framework to address three key challenges (insufficient generalization capability, modeling strategy limitation, and ignorance of emotional continuity) in cross-corpus EEG-based emotion recognition.\n\nâ€¢ Inspired by the nature of emotions, we introduce a soft weighting mechanism that assigns similarity scores to sample pairs to capture the similarity relationships between different samples. As a result, better feature representations of EEG signals are learned in a self-supervised manner. â€¢ We develop a new hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships, which is essential for enhancing contrastive learning. In addition, we introduce a similarity-aware aggregator to fuse complementary information from semantically related samples, enabling fine-grained feature learning and improving the model's overall capability. â€¢ We conduct extensive experiments on three well-known datasets (SEED, SEED-IV, and DEAP), demonstrating that SCMM achieves SOTA performance against 10 baselines, with an average accuracy improvement of 4.26% under both same-class and different-class cross-corpus settings.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work 2.1 Eeg-Based Emotion Recognition",
      "text": "Current approaches for EEG-based emotion recognition mainly rely on two types of experimental protocols: (1) subject-dependent and (2) subject-independent.\n\n(1) The subject-dependent protocol trains and tests models using EEG data from the same subject within a single dataset. For example, Duan et al.  [7]  extracted various emotion-related features from EEG signals and used support vector machine (SVM) and k-nearest neighbors (KNN) for emotion recognition. Similarly, Alsolamy et al.  [1]  inputted power spectral density (PSD) features into an SVM classifier to predict emotions while listening to the Quran. In terms of deep learning models, Zheng et al.  [55]  trained a deep belief network (DBN) using differential entropy (DE) features extracted from multi-channel EEG signals for subject-dependent emotion classification. Additionally, Song et al.  [41]  proposed a dynamical graph convolutional network (DGCNN) that dynamically learns the intrinsic relationship between different EEG channels to enhance the model's discriminative ability. In general, the subject-dependent protocol tends to achieve superior performance due to its potential to introduce information leakage. However, this protocol fails to take into account the significant individual differences of EEG signals, thus limiting its practical applications.\n\n(2) The subject-independent protocol trains and tests models using EEG data from different subjects within a single dataset. Since transfer learning has demonstrated its potential in addressing the problem of domain shift, a series of methods have adopted it for subject-independent EEG-based emotion recognition. For example, Li et al.  [28]  proposed a bi-hemispheres domain adversarial neural network (BiDANN) that considers distribution shift between training and testing data and cerebral hemispheres. Following this, a novel transferable attention neural network (TANN)  [26]  was introduced to learn the emotional discriminative information of EEG signals. To simultaneously adapt the marginal distribution and the conditional distribution, Li et al.  [22]  proposed a joint distribution adaptation network (JDA) for subject-independent EEG-based emotion recognition. Similarly, Chen et al.  [3]  introduced a multi-source marginal distribution adaptation network (MS-MDA) to capture both domain-invariant and domain-specific features of emotional EEG signals. Further, Zhou et al.  [57]  proposed a novel prototypical representation-based pairwise learning framework (PR-PL) to address individual differences and noisy labeling in emotional EEG signals. Despite the progress made in dealing with individual differences, these methods struggle to mitigate the distributional differences across datasets, resulting in significant performance degradation in cross-corpus scenarios.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cross-Corpus Eeg Emotion Recognition",
      "text": "To alleviate the large distributional differences across datasets, researchers have attempted to use domain adaptation techniques for cross-corpus EEG-based emotion recognition. For example, He et al.  [14]  proposed an adversarial discriminative temporal convolutional network (AD-TCN) that integrates the adversarial discriminative learning into a temporal convolutional network for enhancing distribution matching. Meanwhile, Li et al.  [24]  proposed a novel distillation-based domain generalization network (DBDG) to learn the discriminative and generalizable emotional features. Moreover, Zhou et al.  [58]  introduced an EEG-based emotion style transfer network (E 2 STN) that contains the content information of the source domain and the style information of the target domain, achieving superior performance in cross-corpus scenarios. However, these methods require prior access to all labeled source data and unlabeled target data for model training, which is not feasible in practical applications due to the difficulties in collecting and labeling EEG data. Therefore, recent studies have sought to leverage contrastive learning for cross-corpus EEG-based emotion recognition. A noteworthy attempt is JCFA  [29] , which performs joint contrastive learning across three domains to align the time-and frequency-based embeddings of the same EEG sample in the latent time-frequency space, achieving SOTA performance in cross-corpus EEG-based emotion recognition tasks. However, such approaches following the traditional CL paradigm  [4]  fail to capture the emotional continuity inherent in EEG signals, resulting in relatively limited performance in cross-corpus scenarios. Therefore, this study aims to develop a sufficiently generalized model without relying on labeled data, which can be directly applied to different EEG emotion datasets without retraining from scratch, and achieve accurate and efficient cross-corpus generalization.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Problem Formulation",
      "text": "Given an unlabeled pre-training EEG emotion dataset ğ‘‹ = {ğ‘¥ ğ‘– } ğ‘ ğ‘–=1 with ğ‘ samples, where each sample ğ‘¥ ğ‘– âˆˆ R ğ¶ Ã—ğ¹ contains ğ¶ channels and ğ¹ -dimensional features, the goal is to learn a nonlinear embedding function ğ‘“ ğœƒ . This function is designed to map ğ‘¥ ğ‘– to its representation â„ ğ‘– that best describes itself by leveraging the emotional continuity inherent in EEG signals. Ultimately, the pre-trained model is capable of producing generalizable EEG representations that can be effectively used across different datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "The overall framework of SCMM is illustrated in Fig.  2 , which includes three main modules: hybrid masking, soft contrastive learning, and aggregate reconstruction. Below, we will detail the specific design of each module and the pre-training process of SCMM.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hybrid Masking",
      "text": "The selection of masking strategies is crucial for CL and masked modeling. For an input EEG sample ğ‘¥ ğ‘– âˆˆ ğ‘‹ , existing methods use random masking  [51]  or channel masking  [23]  to generate the masked sample ğ‘¥ ğ‘– . The random masking strategy masks samples along the feature dimension, ignoring the inter-channel relationships of multi-channel EEG signals. While a large masking ratio (e.g., 75%) can mask entire portions of certain channels, it complicates the modeling process due to significant information loss. Conversely, the channel masking strategy masks features across all dimensions of the selected channels, losing the relationships between different dimensional features. Neither approach captures both channel and feature relationships simultaneously. Therefore, we develop a new hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships.\n\nSpecifically, we first generate a random masking matrix Mask ğ‘… âˆˆ {0, 1} with dimensions ğ¶Ã—ğ¹ and a channel masking matrix Mask ğ¶ âˆˆ {0, 1} with dimensions ğ¶ Ã— ğ¹ , both derived from binomial distributions with the same masking ratio ğ‘Ÿ âˆˆ (0, 1). Here, the element values in each row of Mask ğ¶ are either all 1s or all 0s. Next, we generate a probability matrix ğ‘ˆ âˆˆ [0, 1] with dimensions ğ¶ Ã— 1 for hybrid masking, which is drawn from a uniform distribution. The hybrid masking process is defined as:\n\nwhere âŠ™ denotes element-wise multiplication. ğ‘¥ ğ‘–,ğ‘ represents the DE features of the ğ‘-th channel of ğ‘¥ ğ‘– , and ğ‘¥ ğ‘–,ğ‘ is the corresponding",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Soft Cl",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Similarity Matrix",
      "text": "Aggregation",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Soft Contrastive Learning",
      "text": "Traditional hard CL treats the same sample and its augmented views as positive pairs, while treating all other samples as negatives  [4] .\n\nDuring the computation of the contrastive loss, hard values (1 or 0) are assigned to sample pairs, as illustrated in Fig.  1 (b) (Hard CL). However, we argue that this approach fails to account for the \"short-term continuity\" characteristic inherent in human emotions, leading to inaccurate modeling of inter-sample relationships and hindering the generalizability of the learned embeddings.\n\nTo address this issue, we propose defining soft assignments for different sample pairs, as shown in Fig.  1 (b) (SCMM). We first input ğ‘¥ ğ‘– and ğ‘¥ ğ‘– into an encoder ğ¸ that maps samples to embeddings, denoted as â„ ğ‘– = ğ¸ (ğ‘¥ ğ‘– ) and â„ ğ‘– = ğ¸ ( ğ‘¥ ğ‘– ). These embeddings are then projected into a latent space Z using a projector ğ‘ƒ, resulting in ğ‘§ ğ‘– = ğ‘ƒ (â„ ğ‘– ) and ğ‘§ ğ‘– = ğ‘ƒ ( â„ ğ‘– ). Next, we perform soft contrastive learning in Z using ğ‘§ ğ‘– and ğ‘§ ğ‘– . Specifically, for a given pair of samples (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ), we first calculate the normalized distance ğ· (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) between ğ‘¥ ğ‘– and ğ‘¥ ğ‘— in the original data space as:\n\nwhere ğ·ğ‘–ğ‘ ğ‘¡ (â€¢, â€¢) is a metric function used to measure the distance between sample pairs, and ğ‘ğ‘œğ‘Ÿğ‘š(â€¢) denotes min-max normalization. Based on the normalized distance ğ· (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ), we then define a soft assignment ğ‘¤ (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) for each pair of samples (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) using the sigmoid function ğœ (ğ‘¥) = 1/(1 + exp(-ğ‘¥)):\n\nwhere ğ›¼ âˆˆ [0, 1] is a boundary parameter that controls the upper bound of soft assignments. ğœ s is a sharpness parameter, where smaller values of ğœ s result in greater differences in ğ‘¤ (â€¢, â€¢) between  sample pairs, and vice versa. Figure  4  illustrates the differences between soft assignments ğ‘¤ (â€¢, â€¢) with different sharpness ğœ s .\n\nLeveraging the soft assignments for all sample pairs, we propose a soft contrastive loss to refine the traditional hard contrastive loss. Specifically, for a pair of projected embeddings (ğ‘§ ğ‘– , ğ‘§ ğ‘– ), we first calculate the softmax probability of the relative similarity among all similarities as:\n\nwhere sim(â€¢, â€¢) refers to the cosine similarity, and ğœ c is a temperature parameter used to adjust the scale. Based on ğ‘ (ğ‘§ ğ‘– , ğ‘§ ğ‘– ), the soft contrastive loss is then defined as:\n\nwhere X = ğ‘‹ âˆª ğ‘‹ represents the union of the data spaces of the original and masked samples. By assigning soft weights to different sample pairs, the model is encouraged to better capture the inherent correlations across different samples. During the training process, the final soft contrastive loss L C is computed by summing and averaging L C,ğ‘– across all samples within a mini-batch. Notably, when âˆ€ğ‘¤ (ğ‘¥ ğ‘– , ğ’™) = 0, the soft contrastive loss reduces to the traditional hard contrastive loss.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Aggregate Reconstruction",
      "text": "To further capture the fine-grained relationships between different samples, we incorporate an aggregator for weighted aggregation and reconstruction. Current approaches for masked EEG modeling typically reconstruct the masked portion based on the unmasked portion of a single masked sample  [20, 33] , following the learning paradigm of MAE  [12] . However, this single-sample reconstruction strategy overlooks the interactions between samples, leading to a complex and inadequate reconstruction process.\n\nTo overcome this limitation, we introduce a similarity-aware aggregator that improves the traditional single-sample reconstruction process. Specifically, we first calculate the cosine similarity between each pair of projected embeddings (ğ‘§ ğ‘– , ğ‘§ ğ‘— ) within a mini-batch, resulting in a similarity matrix S. Based on the pairwise similarities in S, the aggregator then performs weighted aggregation of the embedding â„ ğ‘– , defined as:\n\nwhere â„ â€² âˆˆ H \\{â„ ğ‘– } represents the encoded embedding corresponding to the projected embedding ğ‘§ â€² , and H denotes the embedding space of the encoder ğ¸. This approach allows for a more comprehensive reconstruction by aggregating complementary information and incorporating similar features from different samples during the reconstruction process. Finally, the reconstructed embedding â„ ğ‘Ÿ ğ‘– is fed into a lightweight decoder ğ· to obtain the reconstructed sample ğ‘¥ ğ‘Ÿ ğ‘– . Following the masked modeling paradigm, we use Mean Squared Error (MSE) as the reconstruction loss for model optimization, which is defined as:\n\nSimilar to the soft contrastive loss L C , the final reconstruction loss L R is computed by summing and averaging L R,ğ‘– across all samples within a mini-batch.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "The Pre-Training Process Of Scmm",
      "text": "During the pre-training process, SCMM is trained by jointly optimizing L C and L R . The overall pre-training loss is defined as:\n\nwhere ğœ† C and ğœ† R are trade-off hyperparameters that are adaptively adjusted according to the homoscedastic uncertainty of each loss item  [18] . Algorithm 1 details the pre-training process of SCMM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Algorithm 1",
      "text": "The pre-training process of SCMM.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Require:",
      "text": "-Unlabeled pre-training EEG emotion dataset ğ‘‹ = {ğ‘¥ ğ‘– } ğ‘ ğ‘–=1 . The number of pre-training epochs. Ensure:\n\n1: Randomly initialize the model parameters ğœƒ ; 2: for ğ‘’ğ‘ğ‘œğ‘â„ = 1 to epochs do // All operations are performed within a mini-batch 3:\n\nGenerate the masked sample ğ‘¥ ğ‘– of each input EEG sample ğ‘¥ ğ‘– using hybrid masking in Eq. (1);\n\nGenerate â„ ğ‘– and â„ ğ‘– by feeding ğ‘¥ ğ‘– and ğ‘¥ ğ‘– into ğ¸;\n\n5:\n\nGenerate ğ‘§ ğ‘– and ğ‘§ ğ‘– by feeding â„ ğ‘– and â„ ğ‘– into ğ‘ƒ;\n\nCompute the normalized distance ğ· (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) for each pair of samples (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) using Eq. (2);\n\nGenerate the soft assignment ğ‘¤ (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) for each pair of samples (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) using Eq. (  3 );\n\nCompute the soft contrastive loss L C using Eq. (  5 );",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "9:",
      "text": "Compute the pairwise cosine similarity for each pair of projected embeddings (ğ‘§ ğ‘– , ğ‘§ ğ‘— ); Compute the reconstruction loss L R using Eq. (  7 );\n\n13:\n\nCompute the pre-training loss L pret using Eq. (  8 );\n\n14:\n\nUpdate the model parameters ğœƒ ; 15: end for 16: return The pre-trained SCMM model ğ‘“ ğœƒ .\n\nTable  1 : Experimental results on SEED and SEED-IV under two cross-corpus conditions: (1) same-class and (2) different-class. \" â€ \" and \" â€¡\" represent that the model uses DE features and raw EEG signals as inputs, respectively. \"*\" indicates that the results are reproduced by ourselves. A â†’ B denotes that A is the pre-training dataset, while B is the dataset for model fine-tuning and testing. Best results are highlighted in bold, while the second-best results are underlined.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Same-Class",
      "text": "Different-Class",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments 5.1 Datasets",
      "text": "We conduct extensive experiments on three public datasets, SEED  [55] , SEED-IV  [54] , and DEAP  [19] , to evaluate the model performance of SCMM in cross-corpus EEG-based emotion recognition tasks. These datasets are diverse in terms of EEG equipment, emotional stimuli, data specifications, labeling approaches, and subjects, making them well-suited for assessing the model's efficacy in cross-corpus scenarios. In the experiments, we use 1-s (SEED and DEAP) and 4-s (SEED-IV) DE features as inputs, respectively. Detailed descriptions of the datasets and pre-processing procedures are provided in Appendix A.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "In the pre-training stage, we set ğ‘Ÿ to 0.5 and ğœ‡ to 0.1 for hybrid masking. We use the negative of cosine similarity as ğ· (â€¢, â€¢), and we set ğ›¼ to 0.5, ğœ s to 0.05, and ğœ c to 0.5 for soft CL. We use Adam as optimizer with an initial learning rate of 5e-4 and a weight decay of 3e-4. The pre-training process is conducted over 200 epochs with a batch size of 256. We save the model parameters ğœƒ from the final epoch as the pre-trained SCMM. In the fine-tuning stage, we input the encoded embedding â„ ğ‘– into an emotion classifier consisting of a 2-layer fully connected network for final emotion recognition. For efficient deployment and testing of the model, the pre-trained SCMM is optimized solely using cross-entropy loss during fine-tuning. The fine-tuning process is conducted over 50 epochs with a batch size of 128. All experiments are conducted using Python 3.9 with PyTorch 1.13 on an NVIDIA GeForce RTX 3090 GPU. We release the source code of SCMM at https://github.com/Kyler-RL/SCMM. Further implementation details can be found in Appendix B.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Models And Experimental Settings",
      "text": "We compare the proposed SCMM against ten competitive baselines, including five transfer learning methods: DANN  [10] , BiDANN  [28] , TANN  [26] , PR-PL  [57] , and E 2 STN  [58] , as well as five selfsupervised learning models: SimCLR  [4, 43] , Mixup  [46, 49] , TS-TCC  [9] , MAE  [12] , and JCFA  [29] . Notably, E 2 STN and JCFA are two SOTA methods designed for cross-corpus EEG-based emotion recognition. In the experiments, we adopt a cross-corpus subjectindependent protocol consistent with JCFA and use a leave-trialsout cross-validation strategy for fine-tuning and testing. We calculate the average accuracy and standard deviation (ACC / STD %) across all subjects in the test set to evaluate the model performance of SCMM. More details about baseline models and experimental settings are provided in Appendix C.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Results Analysis And Comparison",
      "text": "To fully validate the model performance of SCMM, we conduct extensive experiments under two cross-corpus conditions: (1) sameclass and (2) different-class. Appendix E presents the withindataset validation experimental results of SCMM on SEED and SEED-IV. Full results are provided in Appendix F.\n\n(1) Same-Class. We first conduct two experiments on the SEED and SEED-IV 3-category datasets: pre-training on SEED-IV and fine-tuning on SEED (SEED-IV 3 â†’ SEED 3 ), and pre-training on SEED and fine-tuning on SEED-IV (SEED 3 â†’ SEED-IV 3 ). In both experiments, all samples corresponding to fear emotions in the SEED-IV dataset are excluded. The left two columns in Table  1  present the comparison results, indicating that SCMM achieves SOTA performance in both experiments. Specifically, our model achieves classification accuracies of 91.61% and 87.24% with standard deviations of 7.56% and 8.35% in the SEED-IV 3 â†’ SEED 3 and  3 experiments, outperforming the second-best method MAE by accuracies of 5.12% and 3.37%, respectively. In addition, the proposed SCMM is significantly better than transfer learning methods, highlighting its superiority.\n\n(2) Different-Class. We then conduct two experiments on the SEED and SEED-IV 4-category datasets, denoted as SEED-IV 4 â†’ SEED 3 and SEED 3 â†’ SEED-IV 4 . These experiments aim to evaluate the model performance when the pre-training and fine-tuning datasets contain different emotion categories. In the experiments, we exclude transfer learning methods since they are not suitable for scenarios where the training and testing datasets contain different emotion categories. Experimental results in the right two columns of Table  1  demonstrate that SCMM achieves the best performance in both experiments. Specifically, our model achieves classification accuracies of 91.26% and 80.89% with standard deviations of 7.91% and 8.69% in the SEED-IV 4 â†’ SEED 3 and SEED 3 â†’ SEED-IV 4 experiments, surpassing the second-best method MAE by 5.24% and 4.15% in accuracies, respectively. Additionally, the traditional CLbased models SimCLR, Mixup, TS-TCC, and JCFA exhibit relatively poor performance across all experiments due to their use of raw EEG signals as inputs.\n\nTo further validate the generalization capability of SCMM, we conduct additional experiments on the SEED and DEAP datasets, denoted as DEAP â†’ SEED 3 , SEED 3 â†’ DEAP (Valence), and SEED 3 â†’ DEAP (Arousal). Note that the EEG acquisition equipment, emotional stimuli, data specifications, labeling approaches, and subjects are completely different between the two datasets. Table  2  presents the experimental results of SCMM compared to existing methods. Specifically, for the DEAP â†’ SEED 3 experiment, SCMM achieves an accuracy of 91.70% with a standard deviation of 8.07%, outperforming the second-best method MAE by an accuracy of 8.01%. For the SEED 3 â†’ DEAP (Valence) and SEED 3 â†’ DEAP (Arousal) experiments, SCMM achieves classification accuracies of 73.96% and 72.66% with standard deviations of 6.75% and 5.67%, surpassing the second-best method MAE by 1.77% and 2.16% in accuracy. The results show that our model maintains excellent performance even when the pre-training and fine-tuning datasets are completely different, highlighting its superior generalization capability. Further, the comparative analysis of Table  1  and Table  2  reveals that the model performance of SCMM on the same fine-tuning dataset remains stable when pre-training on different datasets. This suggests that our model effectively captures generalizable emotional EEG representations that are robust to dataset variations.\n\nIn summary, extensive experimental results on SEED, SEED-IV, and DEAP confirm that our model exhibits superior performance and stability in cross-corpus EEG-based emotion recognition tasks under both same-class and different-class conditions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Discussions 6.1 Ablation Study",
      "text": "To assess the validity of each module in SCMM, we conduct a comprehensive ablation study on the SEED and SEED-IV datasets. Table  3  presents the results of ablation experiments. Specifically, we design three different models below. (1) w/o L C : the first configuration removes the soft contrastive loss and trains the model using only the reconstruction loss. The results show that the model performs the worst without the contrastive learning constraint. However, it still outperforms the MAE model based on the single-sample reconstruction paradigm. This suggests that our aggregate reconstruction strategy effectively improves the model performance by capturing fine-grained inter-sample relationships. (2) w/o L R : the second configuration removes the reconstruction loss and trains the model using only the soft contrastive loss. Experimental results show that this configuration outperforms the masked modeling approach, indicating that contrastive learning is more effective for EEG-based emotion recognition by capturing high-level semantic features of EEG signals.   configuration achieves the best performance in all experiments, indicating that SCMM significantly enhances the model performance and stability by combining soft contrastive learning and aggregate reconstruction. This improvement is evident under different cross-corpus conditions, demonstrating the feasibility of extending SCMM to real-life aBCI applications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Performance With Limited Data",
      "text": "We investigate the model performance of SCMM on the SEED and SEED-IV datasets when fine-tuning with limited labeled data. Specifically, we randomly select 1%, 5%, 10%, and 20% of labeled samples from the fine-tuning dataset for model fine-tuning, while the remaining samples are used for testing. While soft CL has been explored across various domains, most methods focus on computing soft assignments for contrastive loss in the embedding space  [8, 48] . However, we argue that utilizing similarities in the original data space can provide better self-supervision    5  presents the experimental results, demonstrating that soft CL in the original data space consistently outperforms the embedding space in all experiments. Furthermore, this approach allows offline pre-computation of cosine similarities of different sample pairs in the original data space, thus reducing computational costs and improving training efficiency.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Hyperparameter Sensitivity Analysis",
      "text": "To analyze the hyperparameter sensitivity of SCMM, we conduct experiments on SEED and SEED-IV under the same-class condition, as shown in Fig.  6 . Specifically, the examined hyperparameters are divided into two groups: Experimental results show that the proposed SCMM achieves the best performance when the masking ratio and threshold are set to ğ‘Ÿ = 0.5 and ğœ‡ = 0.1 (i.e., the ratio of random masking and channel masking is 9:1), respectively. In addition, we find that our model performs better when using hybrid masking compared to random masking or channel masking in most settings, demonstrating the effectiveness of the proposed hybrid masking strategy in enhancing the model performance.\n\n(2) Soft Contrastive Learning. Figures  6(c ) -(f) depict the classification curves of SCMM using different metric function ğ·ğ‘–ğ‘ ğ‘¡ (â€¢, â€¢), upper bound ğ›¼, sharpness ğœ s , and temperature ğœ c , respectively. Specifically, our model performs best on the SEED and SEED-IV 3-category datasets when using cosine similarity as the metric function ğ·ğ‘–ğ‘ ğ‘¡ (â€¢, â€¢), as shown in Fig.  6(c ). Additionally, Figures  6(d) -(f)  show that SCMM achieves the best performance when the upper bound, sharpness and temperature are set to ğœ‡ = 0.5, ğœ s = 0.05 and ğœ c = 0.5, respectively. In summary, the results demonstrate that SCMM maintains excellent performance under different hyperparameter settings, indicating that its superior generalization ability is not significantly affected by hyperparameter changes.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Sample-Wise Relationships.",
      "text": "To evaluate whether samplewise relationships are preserved in the encoder, we randomly select 100 test samples from the SEED dataset and visualize the pairwise cosine similarity between sample pairs. Additionally, we select all test samples of one subject from the SEED dataset and visualize the learned embeddings of SCMM using t-SNE  [44] . Figure  7(a)  presents heat maps of pairwise similarity matrices, where darker colors indicate higher similarity between samples. Traditional hard CL identifies only coarse-grained relationships across samples from different emotion categories, especially for the most challengingto-recognize negative and neutral emotions. In contrast, SCMM effectively captures the fine-grained relationships between samples of different categories. Moreover, the results of t-SNE visualization in Fig.  7  6.5.2 Intra-and Inter-Class Similarities. To assess the quality of the embeddings learned by SCMM, we randomly select one subject from the SEED dataset and calculate both the average intra-and interclass cosine similarities between the learned embeddings of all test samples, as shown in Fig.  8 . It is evident that the proposed SCMM produces embeddings with higher intra-class similarity compared to traditional hard CL. In addition, the average inter-class similarity of the embeddings learned by SCMM is significantly lower than that of hard CL. In summary, visualization results confirm that the soft contrastive learning strategy designed in SCMM effectively clusters samples within the same category while distinctly separating samples from different categories, thus enhancing the model's discriminative capabilities.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Reconstruction Quality.",
      "text": "To verify the effectiveness of the similarity-aware aggregator designed in SCMM, we compare the reconstruction quality of the single-sample reconstruction paradigm (MAE) with the aggregate reconstruction paradigm (SCMM) on the DEAP dataset. For clarity, we flatten both the original input sample and the reconstructed sample into one-dimensional vectors with dimensions ğ¶ Ã— ğ¹ (channels Ã— features). The results depicted in Fig.  9  illustrate that our model achieves lower reconstruction loss (MSE) and better sample reconstruction, which is crucial to improving the model performance.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusions",
      "text": "This paper proposes a novel SCMM framework to tackle the critical challenge of cross-corpus generalization in the field of EEG-based emotion recognition. Unlike traditional CL-based models, SCMM integrates soft contrastive learning with a hybrid masking strategy to effectively capture the \"short-term continuity\" characteristics inherent in human emotions and produce stable and generalizable EEG representations. Additionally, a similarity-aware aggregator is introduced to fuse complementary information from semantically related samples, thereby enhancing the fine-grained feature representation capability of the model. Extensive experimental results on three well-recognized datasets show that SCMM consistently achieves SOTA performance in cross-corpus EEG-based emotion recognition tasks under both same-class and different-class conditions. Comprehensive ablation study and hyperparameter sensitivity analysis confirm the superiority and robustness of SCMM.\n\nVisualization results indicate that our model effectively reduces the distance between similar samples within the same category and captures more fine-grained relationships across samples. These findings suggest that SCMM enhances the feasibility of extending the proposed method to real-life aBCI applications.\n\n(a) Intra-Class Cosine Similarity (b) Inter-Class Cosine Similarity",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Datasets A.1 Dataset Description",
      "text": "We conduct extensive experiments on three well-known datasets, SEED  [55] , SEED-IV  [54] , and DEAP  [19] , to evaluate the model performance of SCMM in cross-corpus EEG-based emotion recognition tasks. These datasets encompass different EEG equipment, emotional stimuli, data specifications, labeling approaches, and subjects, making them exceptionally suitable for evaluating the model's efficacy in cross-corpus scenarios. Table  6  provides a detailed description of the three datasets.\n\n(1) SEED  [55]  was developed by the Center for Brain-like Computing and Machine Intelligence (BCMI) of Shanghai Jiao Tong University. The dataset used a 62-channel ESI NeuroScan System based on the international 10-20 system to record EEG signals from 15 subjects (7 males and 8 females) under different video stimuli at a sampling rate of 1 kHz. Each subject participated in 3 sessions. In each session, each subject was required to watch 15 movie clips consisting of 3 different emotional states: negative, neutral, and positive. Each emotional state contains a total of 5 movie clips, corresponding to 5 trials.\n\n(2) SEED-IV  [54]  used the same EEG equipment as the SEED dataset, but with different video stimuli, emotion categories, and subjects. The dataset recorded EEG signals from 15 subjects under different video stimuli at a sampling rate of 1 kHz. Each subject participated in 3 sessions. In each session, each subject was required to watch 24 movie clips containing 4 different emotions: sad, neutral, fear, and happy. Each emotion contains a total of 6 movie clips, corresponding to 6 trials. (3) DEAP  [19]  was constructed by Queen Mary University of London. The dataset has completely different acquisition device, emotional stimuli, data specifications, labeling approaches, experimental protocols, and subjects from the SEED-series datasets. Specifically, the DEAP dataset used a 128-channel Biosemi ActiveTwo System to record EEG signals from specific 32 channels of 32 subjects (16 males and 16 females) while watching 40 one-minute music videos at a sampling rate of 512 Hz. The 40 videos elicited different emotions according to the valence-arousal dimension. Specifically, the valence-arousal emotional model proposed by Russell  [36]  places each emotional state on a two-dimensional scale. The first dimension represents valence, ranging from negative to positive, and the second dimension represents arousal, ranging from calm to exciting. Participants rated valence and arousal using a continuous scale of 1 to 9 after watching each video clip.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "A.2 Pre-Processing Procedures",
      "text": "For the SEED and SEED-IV datasets, the raw EEG signals were first downsampled to 200 Hz and filtered through a bandpass filter of 0.3-50 Hz to remove noise and artifacts. Then, the data were divided into multiple non-overlapping segments using a sliding window of 1s (SEED) and 4s (SEED-IV), respectively. After that, we extracted differential entropy (DE) features for each channel of each segment at five frequency bands: Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-14 Hz), Beta (14-31 Hz), and Gamma  (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) . Finally, the DE features from 62 channels and 5 bands were formed into a feature matrix of shape 62 Ã— 5, which serves as input to the SCMM model. The extraction of DE features can be expressed as:\n\nwhere ğ‘’ is the Euler constant. ğ‘¥ is an EEG signal of a specific length that approximately obeys a Gaussian distribution ğ‘ (ğœ‡, ğœ 2 ). Here, ğœ‡ and ğœ are the mean and standard deviation of ğ‘¥, respectively. For the DEAP dataset, the raw EEG signals were initially downsampled to 128 Hz and denoised by a bandpass filter of 4-45 Hz. Subsequently, the data were segmented into multiple non-overlapping segments using a sliding window of 1s. Similar to the SEED and SEED-IV datasets, DE features were extracted for each channel of each segment at five frequency bands. Finally, the DE features from 32 channels and 5 bands were formed into a feature matrix of shape 32 Ã— 5 as input to the model. In the experiments, we divided the continuous labels using a fixed threshold of 5 to convert them to binary classification tasks (low/high).",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "A.3 Handling Different Number Of Channels",
      "text": "Since the SEED-series datasets and the DEAP dataset contain different numbers of electrodes (channels), we require channel processing before inputting DE features into the model. Specifically, we consider the fine-tuning dataset as the standard. When the number of channels in the fine-tuning dataset is less than in the pre-training dataset, we select data from the corresponding channels in the pretraining dataset and drop the data from the redundant channels as inputs (e.g., pre-training on SEED and fine-tuning on DEAP). Conversely, when the number of channels in the fine-tuning dataset is greater than in the pre-training dataset, we fill the missing channel data with zeros in the pre-training dataset to match the fine-tuning dataset (e.g., pre-training on DEAP and fine-tuning on SEED).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B Implementation Details",
      "text": "To reduce computational load while maintaining model performance, we adopt a lightweight design for each module of SCMM. Specifically, we use a 3-layer 1D CNN for the encoder ğ¸ and a 2-layer MLP for the projector ğ‘ƒ. For the lightweight decoder ğ·, we utilize a single-layer MLP for reconstruction. For the hyperparameter selection in the pre-training stage, we set ğ‘Ÿ to 0.5 and ğœ‡ to 0.1 for hybrid masking, i.e., the ratio of random masking and channel masking is 9:1. We use the negative of cosine similarity as the metric function ğ·ğ‘–ğ‘ ğ‘¡ (â€¢, â€¢), and we set ğ›¼ to 0.5, ğœ ğ‘¤ to 0.05 and ğœ ğ‘ to 0.5 for soft contrastive learning. We use Adam optimizer with an initial learning rate of 5 Ã— 10 -4 and an L2-norm penalty coefficient  -4 . The number of fine-tuning epochs is set to 50 for SEED and SEED-IV and 500 for DEAP, with a batch size of 128. For efficient deployment and testing of the model, the pre-trained SCMM is optimized solely using cross-entropy loss during fine-tuning. All experiments are conducted using Python 3.9 with PyTorch 1.13 on an NVIDIA GeForce 3090 GPU. Table  7  summarizes the hyperparameter settings of SCMM.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C Baselines And Experimental Setup",
      "text": "We compare the proposed SCMM against ten competitive baselines, including five transfer learning methods: DANN  [10] , BiDANN  [28] , TANN  [26] , PR-PL  [57] , and E 2 STN  [58] , as well as five selfsupervised learning models: SimCLR  [4, 43] , Mixup  [46, 49] , TS-TCC  [9] , MAE  [12] , and JCFA  [29] . Note that E 2 STN and JCFA are two state-of-the-art (SOTA) methods designed for cross-corpus EEGbased emotion recognition. To ensure a fair comparison, we adopt the same encoder, projector, decoder, and classifier structures as SCMM for SimCLR, Mixup, TS-TCC, and MAE. We use the default hyperparameters reported in the original papers for all models in the experiments, unless otherwise specified. Additionally, for DANN, BiDANN, TANN, PR-PL, E 2 STN, MAE, and SCMM, the inputs are preprocessed 1-s DE features. In contrast, SimCLR, Mixup, TS-TCC, and JCFA use preprocessed 1-s EEG signals as inputs, in accordance with the specific design of each model.\n\nIn the experiments, we adopt a cross-corpus subject-independent protocol, consistent with the setup used by JCFA. Specifically, samples from one dataset are used for pre-training, while samples of each subject from another dataset are used individually for finetuning and testing. During the fine-tuning process, we use a leavetrials-out setting, where samples from a part of the trials in each session of each subject in the fine-tuning dataset are used for finetuning, and the remaining trials are used for testing. This approach effectively avoids information leakage. We calculate the average accuracy and standard deviation (ACC / STD %) across all subjects in the test set to evaluate the model performance of SCMM.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D Masking Strategy",
      "text": "This paper introduces a novel hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships. To compare our approach with traditional masking strategies, we explore three different masking rules: random masking, channel masking, and hybrid masking. Figure  10  presents examples of generated masked samples using three strategies.\n\nâ€¢ Random Masking: Generate masks using a binomial distribution to randomly mask samples along the feature dimension, setting the values of masked features to 0. â€¢ Channel Masking: Generate masks using a binomial distribution to randomly mask samples along the channel dimension, setting the values of all features within the masked channels to 0. â€¢ Hybrid Masking: Generate a probability matrix using a uniform distribution that proportionally mixes masks generated by random masking and channel masking. To assess the impact of various masking strategies, we conduct comparative experiments on the SEED and SEED-IV 3-category datasets using four strategies: random, channel, parallel, and hybrid masking. Specifically, the parallel masking strategy indicates that each sample is augmented randomly using one of the random masking or channel masking, and a threshold ğœ‡ is used to control the probability of the two masking strategies being selected. Table  9  presents the experimental results, showing that the hybrid masking strategy achieves the highest accuracy and lowest standard deviation in both experiments. This suggests that the integration of different masking approaches significantly improves the model performance and stability. In addition, parallel masking increases the richness of augmented samples by combining different strategies, which is slightly better than using a single masking approach. However, this strategy makes the model training process unstable, resulting in large standard deviations. In summary, our proposed hybrid masking strategy is highly flexible and can be extended to various datasets by integrating multiple masking strategies using different ratios, which is exceptionally suitable for data with rich semantic information. This strategy effectively generates more diverse masked samples, encouraging the model to comprehensively capture the inherent relationships of the data.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "E Within-Dataset Validation",
      "text": "To evaluate the model performance of SCMM for EEG-based emotion recognition within a single dataset, we conduct additional experiments on SEED and SEED-IV based on two experimental protocols: subject-dependent and subject-independent. In the experiments, we compare the proposed SCMM with three different types of models: machine learning, deep learning, and self-supervised learning. Additionally, we explore the model performance of SCMM using two different training strategies: pre-training-fine-tuning (P-T) and end-to-end (E-E). Table  10  presents the experimental results, indicating that our model achieves competitive results compared with various advanced deep learning models and significantly outperforms traditional machine learning methods and self-supervised learning models. In addition, the SCMM model using the end-to-end training strategy is better than the pre-training-fine-tuning strategy due to the introduction of the emotion classifier for joint learning. In summary, comprehensive experimental results demonstrate the effectiveness of the proposed SCMM in within-dataset EEG-based emotion recognition tasks.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "F Full Results",
      "text": "Tables 11 to 14 present the full experimental results of SCMM.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) An illustration of emotional continuity. We take the sample ğ‘¥ğ‘–at second ğ‘¡within an EEG trial as the anchor,",
      "page": 2
    },
    {
      "caption": "Figure 1: (b) (Hard CL). However,",
      "page": 2
    },
    {
      "caption": "Figure 1: (a), a high cosine similarity is maintained between an an-",
      "page": 2
    },
    {
      "caption": "Figure 1: (b) (SCMM). In contrast, existing",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), SCMM considers emotional continu-",
      "page": 2
    },
    {
      "caption": "Figure 2: , which in-",
      "page": 3
    },
    {
      "caption": "Figure 2: The overall framework of SCMM. The pre-training process of SCMM involves three modules: (1) hybrid masking, (2)",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the differences between the three masking strategies.",
      "page": 4
    },
    {
      "caption": "Figure 3: Comparison of different masking strategies. The",
      "page": 4
    },
    {
      "caption": "Figure 1: (b) (SCMM). We first input",
      "page": 4
    },
    {
      "caption": "Figure 4: Heatmaps of soft assignments ğ‘¤(Â·, Â·) with different",
      "page": 5
    },
    {
      "caption": "Figure 4: illustrates the differences",
      "page": 5
    },
    {
      "caption": "Figure 5: Model performance with limited labeled data for",
      "page": 8
    },
    {
      "caption": "Figure 5: shows the",
      "page": 8
    },
    {
      "caption": "Figure 6: Hyperparameter sensitivity analysis. (a) - (f) represent the masking ratio ğ‘Ÿ, threshold ğœ‡, metric function ğ·ğ‘–ğ‘ ğ‘¡(Â·, Â·),",
      "page": 9
    },
    {
      "caption": "Figure 6: Specifically, the examined hyperparameters are",
      "page": 9
    },
    {
      "caption": "Figure 6: (c). Additionally, Figures 6(d) - (f)",
      "page": 9
    },
    {
      "caption": "Figure 7: (b) indicate that our model better clusters samples within",
      "page": 10
    },
    {
      "caption": "Figure 7: (a) Heat maps of pairwise similarity matrices. (b)",
      "page": 10
    },
    {
      "caption": "Figure 8: It is evident that the proposed SCMM",
      "page": 10
    },
    {
      "caption": "Figure 9: illustrate that our model achieves lower reconstruction",
      "page": 10
    },
    {
      "caption": "Figure 8: Intra- and inter-class cosine similarities of embeddings learned by hard CL and soft CL. (a) and (b) represent the",
      "page": 11
    },
    {
      "caption": "Figure 9: Comparison of reconstruction quality. We visualize the reconstruction results to compare the single-sample recon-",
      "page": 11
    },
    {
      "caption": "Figure 1: 0 presents",
      "page": 14
    },
    {
      "caption": "Figure 10: Examples of generated masked samples using different masking strategies. The masking ratio is set to ğ‘Ÿ= 0.5, and",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "89.68/09.32\n90.73/08.48",
          "SEED3â†’SEED-IV3": "84.24/11.90\n85.07/11.05",
          "SEED-IV4â†’SEED3": "89.45/09.10\n90.96/08.36",
          "Column_5": ""
        },
        {
          "Column_1": "SCMM",
          "Column_2": "91.61/07.56",
          "SEED3â†’SEED-IV3": "87.24/08.35",
          "SEED-IV4â†’SEED3": "91.26/07.91",
          "Column_5": "80.89/08.69"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "90.30/07.94",
          "SEED3â†’SEED-IV3": "85.95/08.74",
          "SEED-IV4â†’SEED3": "90.91/08.61",
          "Column_5": ""
        },
        {
          "Column_1": "SoftCL",
          "Column_2": "91.61/07.56",
          "SEED3â†’SEED-IV3": "87.24/08.35",
          "SEED-IV4â†’SEED3": "91.25/07.91",
          "Column_5": "80.89/08.69"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "SoftCL",
          "90.87/08.58": "91.70/08.01",
          "73.10/07.34": "73.96/06.75",
          "71.98/06.02": "72.66/05.67",
          "Column_5": "-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "89.99/10.25",
          "SEED3â†’SEED-IV3": "85.75/14.00",
          "SEED-IV4â†’SEED3": "90.31/08.59",
          "Column_5": ""
        },
        {
          "Column_1": "OS",
          "Column_2": "91.61/07.56",
          "SEED3â†’SEED-IV3": "87.24/08.35",
          "SEED-IV4â†’SEED3": "91.25/07.91",
          "Column_5": "80.89/08.69"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "OS",
          "90.64/07.97": "91.70/08.01",
          "72.75/07.06": "73.96/06.75",
          "71.58/05.72": "72.66/05.67",
          "Column_5": "-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Subjects": "15\n15\n32",
          "SessionsÃ—Trials": "3Ã—15\n3Ã—24\n1Ã—40",
          "Channels": "62\n62\n32",
          "SamplingRate": "1kHz\n1kHz\n512Hz"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-": "0.5"
        },
        {
          "-": "0.1"
        },
        {
          "-": "0.5"
        },
        {
          "-": "0.05"
        },
        {
          "-": "0.5"
        },
        {
          "-": "200"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SEED-IV3â†’SEED3": "90.30/08.80\n90.25/08.68\n90.34/08.20\n91.61/07.56"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "83.99/09.72\n-\n-\n-",
          "SEED-IV": "56.61/20.02\n-\n-\n-",
          "SEED": "56.73/16.29\n63.38/14.88\n71.31/14.09\n84.57/09.49"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "81.79/11.15\n76.58/10.72\n83.32/09.20\n89.18/09.74": "92.19/07.68\n93.02/06.67",
          "52.47/11.57\n49.40/10.99\n63.59/19.82\n65.61/17.33": "81.41/08.17\n81.60/08.12",
          "63.45/15.96\n58.26/15.05\n67.52/12.73\n76.04/11.91": "83.84/06.22\n84.88/05.85"
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion estimation from EEG signals during listening to Quran using PSD features",
      "authors": [
        "Mashail Alsolamy",
        "Anas Fattouh"
      ],
      "year": "2016",
      "venue": "2016 7th International Conference on Computer Science and Information Technology (CSIT)"
    },
    {
      "citation_id": "2",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "Felipe Zago Canal",
        "Tobias Rossi MÃ¼ller",
        "Cristine Jhennifer",
        "Gustavo Matias",
        "Antonio Gino Scotton",
        "Junior Reis De Sa",
        "Eliane Pozzebon",
        "Antonio Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "3",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Ming Hao Chen",
        "Zhunan Jin",
        "Cunhang Li",
        "Jinpeng Fan",
        "Huiguang Li",
        "He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "4",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Cross corpus physiological-based emotion recognition using a learnable visual semantic graph convolutional network",
      "authors": [
        "Woan-Shiuan Chien",
        "Hao-Chun Yang",
        "Chi-Chun Lee"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Affective style and affective disorders: Perspectives from affective neuroscience",
      "authors": [
        "Davidson Richard"
      ],
      "year": "1998",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "7",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "8",
      "title": "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations",
      "authors": [
        "Debidatta Dwibedi",
        "Yusuf Aytar",
        "Jonathan Tompson",
        "Pierre Sermanet",
        "Andrew Zisserman"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "9",
      "title": "Time-Series Representation Learning via Temporal and Contextual Contrasting",
      "authors": [
        "Emadeldeen Eldele",
        "Mohamed Ragab",
        "Zhenghua Chen",
        "Min Wu",
        "Chee Keong Kwoh",
        "Xiaoli Li",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21"
    },
    {
      "citation_id": "10",
      "title": "Journal of Machine Learning Research",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Pascal Germain",
        "Hugo Larochelle",
        "FranÃ§ois Laviolette",
        "Mario March",
        "Victor Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "11",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "Boqing Gong",
        "Yuan Shi",
        "Fei Sha",
        "Kristen Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr DollÃ¡r",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition",
      "authors": [
        "Zhipeng He",
        "Yongshi Zhong",
        "Jiahui Pan"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "15",
      "title": "The relation between short-term emotion dynamics and psychological well-being: A metaanalysis",
      "authors": [
        "Marlies Houben",
        "Wim Van Den",
        "Peter Noortgate",
        "Kuppens"
      ],
      "year": "2015",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "16",
      "title": "Ten challenges for EEG-based affective computing",
      "authors": [
        "Xin Hu",
        "Jingjing Chen",
        "Fei Wang",
        "Dan Zhang"
      ],
      "year": "2019",
      "venue": "Brain Science Advances"
    },
    {
      "citation_id": "17",
      "title": "PGCN: Pyramidal graph convolutional network for EEG emotion recognition",
      "authors": [
        "Ming Jin",
        "Changde Du",
        "Huiguang He",
        "Ting Cai",
        "Jinpeng Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "Alex Kendall",
        "Yarin Gal",
        "Roberto Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "CE-MOAE: A Dynamic Autoencoder with Masked Channel Modeling for Robust EEG-Based Emotion Recognition",
      "authors": [
        "Yu-Ting Lan",
        "Wei-Bang Jiang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "He Li",
        "Yi-Ming Jin",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "22",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "Jinpeng Li",
        "Shuang Qiu",
        "Changde Du",
        "Yixin Wang",
        "Huiguang He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "23",
      "title": "A multi-view spectral-spatial-temporal masked autoencoder for decoding emotions with selfsupervised learning",
      "authors": [
        "Rui Li",
        "Yiting Wang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Distillation-Based Domain Generalization for Cross-Dataset EEG-Based Emotion Recognition",
      "authors": [
        "Wei Li",
        "Siyi Wang",
        "Shitong Shao",
        "Kaizhu Huang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "25",
      "title": "GMSS: Graph-based multi-task self-supervised learning for EEG emotion recognition",
      "authors": [
        "Yang Li",
        "Ji Chen",
        "Fu Li",
        "Boxun Fu",
        "Hao Wu",
        "Youshuo Ji",
        "Yijin Zhou",
        "Yi Niu",
        "Guangming Shi",
        "Wenming Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "A novel transferability attention neural network model for EEG emotion recognition",
      "authors": [
        "Yang Li",
        "Boxun Fu",
        "Fu Li",
        "Guangming Shi",
        "Wenming Zheng"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "27",
      "title": "A novel bi-hemispheric discrepancy model for EEG emotion recognition",
      "authors": [
        "Yang Li",
        "Lei Wang",
        "Wenming Zheng",
        "Yuan Zong",
        "Lei Qi",
        "Zhen Cui",
        "Tong Zhang",
        "Tengfei Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "28",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for EEG emotion recognition",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Zhen Cui",
        "Tong Zhang",
        "Yuan Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "29",
      "title": "Joint Contrastive Learning with Feature Alignment for Cross-Corpus EEG-based Emotion Recognition",
      "authors": [
        "Qile Liu",
        "Zhihao Zhou",
        "Jiyuan Wang",
        "Zhen Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 1st International Workshop on Brain-Computer Interfaces (BCI) for Multimedia Understanding"
    },
    {
      "citation_id": "30",
      "title": "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition",
      "authors": [
        "Ting Luo",
        "Jing Zhang",
        "Yingwei Qiu",
        "Li Zhang",
        "Yaohua Hu",
        "Zhuliang Yu",
        "Zhen Liang"
      ],
      "year": "2025",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "31",
      "title": "Survey on emotional body gesture recognition",
      "authors": [
        "Fatemeh Noroozi",
        "Adrian Ciprian",
        "Dorota Corneanu",
        "Tomasz KamiÅ„ska",
        "Sergio SapiÅ„ski",
        "Gholamreza Escalera",
        "Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "Ivor Sinno Jialin Pan",
        "James Tsang",
        "Qiang Kwok",
        "Yang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "33",
      "title": "Multi-Scale Masked Autoencoders for Cross-Session Emotion Recognition",
      "authors": [
        "Miaoqi Pang",
        "Hongtao Wang",
        "Jiayang Huang",
        "Chi-Man Vong",
        "Zhiqiang Zeng",
        "Chuangquan Chen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "34",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "35",
      "title": "Cross-corpus EEG-based emotion recognition",
      "authors": [
        "Soheil Rayatdoost",
        "Mohammad Soleymani"
      ],
      "year": "2018",
      "venue": "2018 IEEE 28th International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "37",
      "title": "In search of a robust facial expressions recognition model: A large-scale visual cross-corpus study",
      "authors": [
        "Elena Ryumina",
        "Denis Dresvyanskiy",
        "Alexey Karpov"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "38",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "Bjorn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "Andre Stuhlsatz",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "Xinke Shen",
        "Xianggen Liu",
        "Xin Hu",
        "Dan Zhang",
        "Sen Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Beer Youddha",
        "Shivani Singh",
        "Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "41",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "A Johan",
        "Joos Suykens",
        "Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "43",
      "title": "contrastive learning in human activity recognition for healthcare",
      "authors": [
        "Ian Chi",
        "Ignacio Tang",
        "Dimitris Perez-Pozuelo",
        "Mascolo Spathis"
      ],
      "year": "2020",
      "venue": "contrastive learning in human activity recognition for healthcare",
      "arxiv": "arXiv:2011.11542"
    },
    {
      "citation_id": "44",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "45",
      "title": "DMMR: Cross-subject domain generalization for EEG-based emotion recognition via denoising mixed mutual reconstruction",
      "authors": [
        "Yiming Wang",
        "Bin Zhang",
        "Yujiao Tang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Mixing up contrastive learning: Self-supervised representation learning for time series",
      "authors": [
        "Kristoffer WickstrÃ¸m",
        "Michael Kampffmeyer",
        "Karl Ã˜yvind Mikalsen",
        "Robert Jenssen"
      ],
      "year": "2022",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "47",
      "title": "A novel solution for EEG-based emotion recognition",
      "authors": [
        "Zhuofan Xie",
        "Mingzhang Zhou",
        "Haixin Sun"
      ],
      "year": "2021",
      "venue": "2021 IEEE 21st International Conference on Communication Technology (ICCT)"
    },
    {
      "citation_id": "48",
      "title": "Neighborhood contrastive learning applied to online patient monitoring",
      "authors": [
        "Hugo YÃ¨che",
        "Gideon Dresdner",
        "Francesco Locatello",
        "Matthias HÃ¼ser",
        "Gunnar RÃ¤tsch"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "49",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "50",
      "title": "Unsupervised time-aware sampling network with deep reinforcement learning for eeg-based emotion recognition",
      "authors": [
        "Yongtao Zhang",
        "Yue Pan",
        "Yulin Zhang",
        "Min Zhang",
        "Linling Li",
        "Li Zhang",
        "Gan Huang",
        "Lei Su",
        "Honghai Liu",
        "Zhen Liang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "GANSER: A self-supervised data augmentation framework for EEG-based emotion recognition",
      "authors": [
        "Zhi Zhang",
        "Yan Liu",
        "Sheng-Hua Zhong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised learning in cross-corpus acoustic emotion recognition",
      "authors": [
        "Zixing Zhang",
        "Felix Weninger",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "53",
      "title": "Plug-and-play domain adaptation for cross-subject EEG-based emotion recognition",
      "authors": [
        "Li-Ming Zhao",
        "Xu Yan",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "55",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "56",
      "title": "EEG-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using EEG signals",
      "authors": [
        "Rushuang Zhou",
        "Zhiguo Zhang",
        "Hong Fu",
        "Li Zhang",
        "Linling Li",
        "Gan Huang",
        "Fali Li",
        "Xin Yang",
        "Yining Dong",
        "Yuan-Ting Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Enhancing Cross-Dataset EEG Emotion Recognition: A Novel Approach with Emotional EEG Style Transfer Network",
      "authors": [
        "Yijin Zhou",
        "Fu Li",
        "Yang Li",
        "Youshuo Ji",
        "Lijian Zhang",
        "Yuanfang Chen",
        "Huaning Wang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}