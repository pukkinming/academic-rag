{
  "paper_id": "2103.13372v1",
  "title": "Affective Processes: Stochastic Modelling Of Temporal Context For Emotion And Facial Expression Recognition",
  "published": "2021-03-24T17:48:19Z",
  "authors": [
    "Enrique Sanchez",
    "Mani Kumar Tellamekala",
    "Michel Valstar",
    "Georgios Tzimiropoulos"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Temporal context is key to the recognition of expressions of emotion. Existing methods, that rely on recurrent or selfattention models to enforce temporal consistency, work on the feature level, ignoring the task-specific temporal dependencies, and fail to model context uncertainty. To alleviate these issues, we build upon the framework of Neural Processes to propose a method for apparent emotion recognition with three key novel components: (a) probabilistic contextual representation with a global latent variable model; (b) temporal context modelling using task-specific predictions in addition to features; and (c) smart temporal context selection. We validate our approach on four databases, two for Valence and Arousal estimation (SEWA and AffWild2), and two for Action Unit intensity estimation (DISFA and BP4D). Results show a consistent improvement over a series of strong baselines as well as over state-ofthe-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In this paper, we address the problem of facial behaviour recognition from video, in particular, the problem of recognising apparent emotions in terms of Valence and Arousal  [51] , and facial expressions in terms of Action Unit intensity  [11] . This is a longstanding problem in video recognition which has been extensively studied by the computer vision community  [26, 64, 41, 21, 33, 28, 67, 74, 77, 71, 55, 53, 54, 39, 46] . Nevertheless, even recent methods  [27, 33, 77]  struggle to achieve high accuracy on the most difficult datasets including SEWA  [34] , Aff-Wild2  [29] , BP4D  [75]  and DISFA  [42] . Hence, there is still great progress to be made towards solving this challenging problem. In this paper, we show that effective temporal context modelling is a key feature for significantly advancing the state-of-the-art. But what makes emotion (and facial expression) recognition such a difficult problem? There are many, often interrelated, reasons for this including: (a) annotation of emotions is often both subjective and laborious making it hard to annotate it consistently  [56] ; (b) there exist only small-and medium-size emotion video datasets and, due to point (a) above, the annotations for these datasets are often noisy; (c) emotions are subtle, acting as unobserved, latent variables that only partially explain facial appearance over time  [2] . They often require temporal as well as multi-modal context in order to be robustly recognised. From these challenges, this paper focuses on solving problem (c). In particular, it proposes a completely unexplored perspective for effective incorporation of temporal context into emotion recognition.\n\nPrevious work in temporal modelling of emotions has primarily focused on modelling facial expression dynamics, i.e., the way that facial expressions evolve over time for recognition purposes. A typical deep learning pipeline for this has been a Convolution Neural Network (CNN) followed by a Recurrent Neural Network (RNN), usually an LSTM or GRU  [32, 28, 26, 71] . Although these dynamics can be particularly useful in recognising specific facial expressions (e.g.  [63, 64, 21, 41, 70] ), we believe that their importance has been over-emphasised for the problem of emotion recognition.\n\nWe posit that temporal context is more important than facial dynamics for emotion recognition. The cues for inferring a person's apparent emotion from their facial behaviour are often sparsely and non-regularly distributed over a temporal window of interest and collecting such distributed contextual cues is critical to inferring emotions robustly. In addition, due to the person-specific variability of facial expressions but, most importantly, due to their subjective annotation, context must be modelled in a stochastic manner.\n\nTo address the aforementioned challenges, in this work, and for the first time to the best of our knowledge, we build upon the framework of Neural Processes  [16, 17]  to propose a model for emotion recognition with 3 key components:\n\n(1) stochastic contextual representation with a global latent variable model;  (2)  task-aware temporal context modelling not only by using features but also task-specific predictions; and (3) effective temporal context selection.\n\nFig.  1  depicts an overview of the working flow our method (bottom), RNN-based methods (top), and methods based on self-attention  [66]  (middle). Note methods for context modelling based on self-attention do not satisfy any of (1)-(3): They are deterministic and model long-term dependencies not globally, but by using pair-wise feature similarities. Moreover, they do not use task-aware context modelling and do not perform context selection.\n\nOverall, we make the following 3 contributions:\n\n1. We propose Affective Processes: the very first model for emotion recognition with three key properties: (a) global stochastic contextual representation; (b) taskaware temporal context modelling; and (c) temporal context selection. We conduct a large number of ablation studies illustrating the contribution of each of our model's key features and components.\n\n2. We show that our model is more effective in modelling temporal context not only than CNNs+RNNs but also than a strong baseline based on self-attention. Our model outperforms both baselines by large margin.\n\n3. We validated our approach on the most difficult databases for emotion recognition in terms of Valence and Arousal estimation, namely SEWA  [34]  and Af-fWild2  [29] . We further show that our approach can be effective for the problem of Action Unit intensity estimation on DISFA  [42]  and BP4D  [75]  datasets. On all datasets used, we show a consistent and often significant improvement over state-of-the-art methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section reviews related work from temporal apparent emotion recognition, Action Unit intensity estimation and neural modelling of Stochastic Processes.\n\nTemporal Modelling in Emotion Recognition: Most existing methods model the temporal dynamics of continuous emotions using deterministic approaches such as Time-Delay Neural Networks  [43] , RNNs, LSTMs and GRUs  [32, 28, 26, 71, 8, 59] , multi-head attention models  [20] , 3D Convolutions  [76, 35] , 3D ConvLSTMs  [19] , and temporal-hourglass CNNs  [10] . While these deterministic models are capable of effectively learning the temporal dynamics, they do not take the inherent stochastic nature of the continuous emotion labels into account. Deviating from this trend, recently,  [47]  applied variational RNNs  [5]  to the valence estimation problem to add stochasticity to the temporal model of emotion recognition. Similarly, in  [48] , emotion is treated as a dynamical latent system state which is estimated using Gaussian processes coupled with Kalman filters, while Deep Extended Kalman filters are used in  [49] . These probabilistic methods have better latent structurediscovery capabilities than the the deterministic temporal models, however, they focus only on the temporal dynamics of emotions, ignoring the temporal context. In contrast, our method primarily focuses on directly learning the global temporal context in the input sequence using stochastic processes with data-driven 'priors'. AU Intensity Estimation: In order to model the subjectivevariability of the contextual factors in AU intensity estimation, several works advocate for probabilistic models  [67, 54, 6, 53, 13, 14, 55] . Particularly, Gaussian Processes based models  [13, 14]  and variational latent variable models  [12, 39, 68]  have been extensively used for this purpose.\n\nOther probabilistic approaches to facial affect modelling include Conditional Ordinal Random Fields (CORF)  [67, 54, 53]  and Markov Random Fields (MRF)  [55] . To learn the task-specific temporal context, context-aware feature fusion and label fusion modules are proposed in  [77]  to model AU intensity temporal dynamics. Although we also propose to learn the task-specific temporal context, our context learning method is completely different to that of  [77] .\n\nStochastic Process Modelling  [72, 69, 61, 62] : Our method primarily builds on the recently proposed stochastic regression frameworks of Neural Processes (NPs). NPs  [16, 17, 23, 18, 58, 38]  are a family of predictive stochastic processes that aim to learn distributions over functions by adapting their priors to the data using context observations. Application of NPs in Computer Vision has been limited to basic image completion and super-resolution tasks  [23]  as proof-of-concept experiments. To our knowledge, we are the first to show how NPs can be used to solve challenging real-world regression tasks like that of emotion recognition. The NP Encoder is an MLP q φ that computes an individual feature representation r c = q φ (x c , y c ) ∈ R d (d = 128 in our work) for each input context pair, followed by a permutation-invariant aggregation operation. We choose simple summation for aggregation to compute a global deterministic context representation",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "with C the number of context points. To account for the context stochasticity, r C is used to parameterise a Gaussian distribution from which a global latent variable z can be drawn. To this end, two linear layers are used to compute the mean µ C (r C ) and standard deviation σ C (r C ). The global latent variable z ∼ N (µ C , σ C ) ∈ R d represents a different realisation of the SP conditioned on the context.\n\nThe NP Decoder is an MLP f θ that samples functions f ∼ P through the latent variable z. For a given z, function samples are generated at all target points as y t = f θ (x t , z). Assuming some i.i.d. Gaussian observation noise with learnable variance  [22] , the output of f θ can be that of a mean and standard deviation of a Gaussian distribution:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Affective Processes",
      "text": "Let us assume that we are given a sequence of images {I t } T t=1 , and that we want to estimate facial affect, in terms of Valence and Arousal, on a per-frame basis y t ∈ R 2 1  . The framework of NPs cannot be applied to emotion recognition as is. Actually, application of NPs in Computer Vision has been limited to basic image completion and superresolution tasks  [23]  as proof-of-concept experiments. One of our main contributions is to show how such a framework can be applied for challenging real-world temporal regression tasks like the one of emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Input Features And Pseudo-Labels",
      "text": "The original NP formulation would directly operate on the sequence of images {I t }. Furthermore, it would require ground truth labels for the context points y c both at training and test time. While one can assume the ground-truth labels to be available for training, their need at test time would limit the application of NPs to real-world problems. To alleviate these issues, we firstly propose to train a backbone CNN, comprising a feature extractor Φ(.) and a classifier W, in a standard supervised manner, to provide independent per-frame estimates of Valence and Arousal ŷt ∈ R 2 .\n\nThe purpose of this network is two-fold: Firstly, the feature extractor Φ(.) is used to provide a per-frame feature representation x t = Φ(I t ). Then, we can use the sequence of features x t as input to the NP instead of the image sequence. Secondly, we propose to use the output of the classifier ŷt as pseudo-ground truth labels to be fed as input to the NP. These labels are noisy, and lack temporal consistency, yet they still provide some information regarding the true labels. In this paper, we choose as our backbone an architecture inspired by that of  [46, 60, 73] . We refer the reader to the Supplementary Material for further details. Frozen Backbone: It is important to remark that Φ(.) and W are kept frozen during the training of our AP. While one could opt for finetuning the backbone while training the AP, the change in Φ(.) and W would lead to changing both the input space and the pseudolabels, leading to poorer performance. When minimising the AP objective (see below), W is no longer learned through the error of the values ŷ, and thus it can lead to produce pseudolabels that lie out of the space Y, i.e. that are not valid. Recall that NPs rely on estimating f (x t ) for the target points, provided some observed values y c = f (x c ) for the context. In this paper the values y c are replaced by the predicted values ŷc provided by W. If these values change over the course of training, and become non plausible values of f , the whole NP framework becomes invalid. We experimentally tried fine-tuning the backbone, and observed a poor performance.\n\nOverall, to our knowledge, we are the first to show in NP literature that pseudo-ground truth generated from an independent network can be effectively used for defining the labels for the context points used as input to the NP.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Context Selection",
      "text": "Our first and obvious option for context selection is uniform random sampling from the sequence, which would imply the strong assumption that the errors of the backbone are consistent. Considering that the backbone works in a perframe basis, it is likely that some of the input features are more indicative of the context than others. In this paper, we propose to delve into the properties of the NPs to investigate whether we can infer which frames are most optimal for context representation.\n\nGiven that the number of context points can be any number between 1 and the length of the sequence, the choice of r C = r c for a single context point c also results in a valid latent function. Building on this concept and the fact that σ C (r C ) represents the uncertainty of the latent functions, we propose to select the context by first estimating the set of latent functions that each of the individual representations r c would propose, and select those with lowest uncertainty. That is, we propose to first estimate r c and their corresponding σ C (r c ) for all the frames in a given sequence, and select the C with lowest uncertainty. The context set C is then defined as the C frames for which σ c (r c ) is the lowest. Note that this step is used to select the context points, only. Once context points are selected, they are passed through the aggregation function to compute r C in the standard way. This adds a negligible extra complexity to the framework (the values of r c do not need to be computed again). Finally, we emphasize that we apply context selection only at test time.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Complete Architecture",
      "text": "The complete architecture is shown in Fig.  2 . We use as input the features x t extracted from the pre-trained backbone, as well as the corresponding predictions. The encoder q φ is a three-layer MLP that maps each of the input pairs (x c , ŷc ) into a latent representation r c ∈ R 128 . The input pseudolabels ŷt are firstly upsampled to match the dimension of the feature representation. Once r C has been computed from the average of the selected context points, it is forwarded to a latent encoder, which comprises a common linear layer and two independent linear layers, to compute µ C and σ C , respectively. The decoder f θ is another threelayer MLP that receives the individual features x t and the latent representation z, and produces two outputs ỹµ t and ỹσ t , parameterising the output distribution. During training, z ∼ N (µ C , σ C ), and ỹt ∼ N (ỹ µ t , ỹσ t ). At test time, we use the predictive mean and set z = µ C and ỹt = ỹµ t .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Losses",
      "text": "Learning in our model comprises learning the parameters of the encoder q φ and the decoder f θ . For a given set of annotated sequences of varying length, a random subset of frames are used as context, and the learning is the task of reconstructing the labels for the target points. In the NPs, the goal is to maximise the evidence lower-bound (ELBO) of log p(y T |x T , x C , y C ). Because Valence and Arousal take values in [-1, 1], we also consider that the labels y are normally distributed around the origin, and propose a regularisation term for the output distribution as\n\n). The full objective to minimise is:\n\nwith L nll = -E q φ (z|C) [log p(y T |x T , z)] and L kl = D KL (q φ (z|T ) q φ (z|C)). The optimisation can be efficiently done through the reparametrization trick  [25] . In this paper λ kl and λ reg are set to 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Other Methods",
      "text": "It is important to remark that our model comes with the following benefits: a) it allows to automatically infer the global context; this adds a new key to existing Neural Processes where context is manually given, b) it allows to model the uncertainty on the estimated context, and c) it allows to draw coherent samples; note that while the decoder works on a per-frame basis, the fact that it is conditioned on a global latent variable whose value can be drawn from a latent distribution, makes all of the samples be dependent on it  [7] .\n\nVs. RNN: The task of temporal regression could be approached by using a parametric form for f , by means of an RNN. However, this assumes that the variables y t are drawn from Markov chains, which in practice disregard long-term dependencies needed for temporal context modelling. Furthermore, it can be shown  [17]  that the joint distribution p(y T ) is exchangeable, i.e. it is permutation invariant. This offers a new insight into temporal modelling that differentiates from RNNs that condition a current observation on the previous hidden state, i.e. where order matters. We argue that, provided a given static image, the corresponding labels should not be position-dependent. If we shuffle a given sequence and pick up a frame at random, the corresponding annotation is not expected to change. Our approach seeks an orderless modelling of sequences, by looking at them as a whole through the global latent variable z. Vs. Self-attention: Self-attention models  [66]  can also model long-term dependencies using pair-wise feature similarities. On the contrary, we choose NPs to estimate f by sampling from a global latent random variable. This allows for a more direct and holistic modelling of the temporal context. Furthermore, unlike self-attention models, in NPs both input signal x c , and labels y c are used to learn context.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets And Performance Metrics",
      "text": "We validate our approach for emotion recognition on SEWA  [34]  and Aff-Wild2  [26, 29, 30, 28, 31, 27, 74] . We also validate our approach for the task of Action Unit intensity estimation on BP4D  [75, 65]  and DISFA  [42] .\n\nSEWA  [34]  is a large video dataset for affect estimation inthe-wild. It comprises over 2, 000 minutes of video data annotated with valence and arousal values. It contains subjects from 6 different cultures. We follow the evaluation protocol of  [34, 33] , and apply a 8:1:1 dataset partition We follow a 3-fold cross-validation protocol with 18 subjects used for training and 9 subjects used for testing.\n\nPerformance Metrics: For Valence and Arousal, we follow the AVEC Challenge series  [52]  and report the Concordance Correlation Coefficient  [36] . For Action Units, we follow the ranking criteria used in FERA challenges  [65] , and report the Intra Class Correlation (ICC  [57] ). A full definition of these metrics can be found in the Supp. Material.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "All experiments are developed in PyTorch  [50] . First, we pre-train the backbone architecture Φ and W on a per-frame basis for each corresponding dataset (see Supplementary Material). We then use as input the features x t extracted from the pre-trained backbone, as well as the corresponding predictions. The dimension of input features x t is 512.\n\nValence and Arousal: To train the AP for Valence and Arousal, we randomly sample sequences between 35 and 70 frames from the corresponding training set. The range 35 -70 can be seen as an augmentation parameter: using a shorter range yielded worse results due to lack of generalisation, whereas no improvement was observed when using a longer range of sequences. The number of context points is randomly chosen from 3 to the length of the given sequence. During training, the target labels are the groundtruth annotations. For the context labels, we use the groundtruth annotations or the predictions from W with probability 0.5. The batch size is 16 videos, and the learning rate and weight decay are set to 0.00025 and 0.0001, respectively. The AP is trained for 25 epochs, each of 1000 iterations, using Adam  [24]  with (β 1 , β 2 ) = (0.9, 0.999). We use a Cosine Annealing strategy for the learning rate  [40] .\n\nAt test time, we use a fixed sequence length of 70 frames.\n\nAction Units: We used the same setting as for Valence and Arousal, with the following differences: the batch size is set to 6 videos, and the learning rate and weight decay are set to 0.0001 and 0.0005, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "We use the validation partition of SEWA to perform our ablation studies. We study the effect of our proposed context selection method, the contribution of the pseudo labels, the effect of the different losses used for the AP training, and the effect of the latent variable. Finally, we explore a different alternative to the AP configuration  [37] . We study in Sec. 6 the complexity of our approach. Effect of context selection: Firstly, we investigate the effect of the number of selected context points and the selection method proposed in Sec. 3.2. For a given trained model, we study the effect of choosing the context points (a) randomly, (b) according to the lowest values of σ c , and (c) according to the largest values of σ c . Fig.  3  shows that the best results are attained when choosing 40 context points according to the lowest value of σ c . We fix for the remaining experiments the number of context points to 40.\n\nImpact of task-aware context modelling: One of the contributions of our method is that of task-aware context modelling in the form of pseudo-ground truth labels. We conducted an experiment where the AP model is trained without the predictions provided by W. The architecture, training, losses, learning rate and weight decay remain the same as for the main AP. The results, reported in Table  1  as AP w/o W , show that the performance drops significantly without the aid of the noisy task-specific predictions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Effect Of Global Latent Random Variable:",
      "text": "We firstly investigate the effect of the latent variable on regressing coherent functions. Recall that our proposed approach consists of a probabilistic regression of coherent functions, i.e. it produces a set of functional proposals. An alternative to this approach is to use a non-probabilistic approach where the latent representation does not parameterise a latent random distribution, but is rather deterministic. The architecture of a deterministic AP is similar to that shown in Fig.  2  although with a single-branch latent encoder and no sampling. With a deterministic AP, one dispenses with the coherent sampling i.e. the dependence among the target outputs is lost, and assumed to be i.i.d. The learning burden is put only into capturing the observation variance ỹσ to account for the output uncertainty (i.e. it is probabilistic in the output, but not in the latent representation). The results of the deterministic AP are shown in Table  1  (Deterministic AP). We observe that the performance degrades substantially, in particular regarding the Arousal predictions. This illustrates the importance of the latent variable to model the underlying variability for the joint prediction of Valence and Arousal. Finally, we also investigated the dimensionality of z, observing little effect when having more dimensions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effect Of Losses:",
      "text": "The training of the APs is carried out by minimising the objective shown in Eqn. 1. Recent findings  [15]  suggest that training an NP using only a maximum likelihood approach (i.e. by minimising the negative log-likelihood L nll ) can improve performance and avoid the practical issues of using amortised variational inference (i.e. L nll + L kl ). We studied the effect of each case along with our proposed loss. The results are shown in Table  1 . We refer with AP nll to the maximum likelihood approach, with AP nll+kl to the one that minimises L nll + L kl , with AP nll+reg to the one that minimises L nll + L reg , and with AP nll+reg+kl to the one that minimises the full loss in Eqn. 1. The numerical results do not shed light into whether one is better than the other. We observe that a combination of the maximum likelihood approach with our proposed regularisation slightly improves the numerical results, although we acknowledge these not to be significant enough. If we use the three losses together we observe a performance degradation. These results align with recent findings that advocate for a further investigation of which of the losses is a better objective for Neural Processes  [15] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Different Network Configurations:",
      "text": "We also study different network configurations. We first evaluate the perfor-",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison With State-Of-The-Art",
      "text": "We compare the results of our models w.r.t. the most recent works in emotion and Action Unit intensity estimation. The bulk of our results are shown in Tables  2  and 3  for Valence and Arousal, and Tables  4  and 5  for Action Units (MSE is also reported in the Supplementary Material).\n\nIn-house baselines: We trained a set of baselines based on GRU  [4]  and Attention  [66] , using, for each case, the same backbone as the one used in AP to provide the input features. We also optimised for each task and model the sequence length (herein denoted by L).   2 3 4 5 indicate that our AP outperforms both recurrent and attentionbased models. We also experimented with using as input both the features and the labels, as well as finetuning the backbone, observing a performance degradation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Different Backbones:",
      "text": "We validate that our AP can be used with other backbone choices. On SEWA, we first train a ResNet-50, yielding a CCC of 0.567, and then train our AP on top yielding a CCC of 0.655. On AffWild-2, we train our AP using as backbone the publicly available model of  [35] : the backbone of  [35]  reports a CCC score on the AffWild-2 test set of 0.43, whereas our AP yields a CCC score of 0.47.\n\nAnalysis: On SEWA, we compare against Mitekova et al.  [44]  and Kossaifi et al.  [33] . Notably, our backbone is already on par with the state of the art results of  [33] . We can observe that our method based on temporal context offers the largest improvement. The same behaviour is observed",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model",
      "text": "Valence Arousal Mean Mitekova et al.  [44]  0.44 0.39 0.42 Kossafi et al.  [33]  0  on AffWild2, where our AP significantly improves the Valence CCC. On DISFA, our backbone is again on par with the state of the art methods VGP-AE  [12]  and 2DC  [39] . However, our AP brings the largest improvement setting a new state-of-the-art ICC of 0.58, which improves over the Heatmap Regression method of Ntinou et al.  [46] . Finally, we compare our method against the winner and runner-up methods of FERA 2015, ISIR  [45]  and CDL  [1] , respectively, as well as against the Heatmap Regression method of  [46] . Our method sets up a new state-of-the-art result with an ICC score of 0.74. We include a set of qualitative results in the Supplementary Material.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusions",
      "text": "We proposed a novel temporal context learning approach to facial affect recognition, by modelling the stochastic nature of affect labels. To model uncertainty in the temporal context of the affective signals, our proposed method, called Affective Processes, builds on Neural Processes to learn a distribution of functions. Our method addresses a key limitation of the NPs -that of requiring ground-truth labels at inference time -by proposing the use of pseudo-labels provided by a pre-trained backbone and by using a novel method for automatic context selection. We believe that the key elements of our method pave the way to apply NPs to large-scale supervised learning problems which are inherently stochastic and for which deterministic temporal models like RNNs and self-attention could fall short.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Backbone",
      "text": "This appendix is used to describe the backbone structure and training, left out of the main document due to lack of space. It also includes a formal definition of the main performance metrics used in the paper, as well as the Mean Squared Error reported for the Action Unit intensity estimation task. We first describe the architecture of the backbone (Sec. A.1), and then the training details for each of the databases used in the paper (Sec. A.2). Sec. B and Sec. C are devoted to describing the performance metrics used in the paper and to reporting the MSE results on DISFA and BP4D, respectively.\n\nA.1. Architecture\n\nThe structure of the backbone for both Valence and Arousal and Action Unit recognition is depicted in Fig.  5 . Note that both are depicted in the same figure for the sake of clarity, although the corresponding subnetworks consisting of the Emotion Head or the Action Units Head are trained independently using the task specific datasets. Both networks share a common module, referred to as Face Alignment Module, which is pre-trained for the task of facial landmark localisation, and kept frozen for the subsequent training steps. For both Valence and Arousal and Action Unit estimation, the backbone is decomposed into three main components, namely a) Face Alignment Module, b) Taskspecific Feature Module, and c) Task-specific Head.\n\nThe Face Alignment Module is a lightweight version of the Face Alignment Network of  [3] . It starts with a 2d convolutional layer (referred to as Conv2d) and a set of 4 convolutional blocks (ConvBlock, depicted in Fig.  6 ) that bring down the resolution of the input image from 256 to 64 and the number of channels from 3 to 128. This set of ConvBlocks is followed by an Hourglass, a four layer set of 128-channel ConvBlocks with skip connections, that aggregate the features at different spatial scales. The Hourglass is followed by another ConvBlock and two Conv2d layers that produce a set of 68 Heatmaps, corresponding to the position of the facial landmarks. In this paper, rather than using the facial landmarks to register the face, we directly concatenate the produced features at both an early and late stage of the network with the Heatmaps. The output is then a 128 + 128 + 68 tensor of 64 × 64, resulting from concatenating the features computed after the fourth Con-vBlock, the features computed after the last ConvBlock, and the produced Heatmaps. This way, the Heatmaps help the subsequent network locally attend to the extracted coarse and fine features  [46, 60, 73] . The benefits of this approach are twofold: a) it dispenses with the need of registering the faces according to detected landmarks, and b) because of a) we can directly use the features from the Face Alignment Network and have shallower networks in the front-end for the subsequent tasks.\n\nThe Task specific Feature Module consists of a mere set of 4 ConvBlocks, each followed by a max pooling layer, that produce a tensor of 128 × 4 × 4. To form the features x t that will be used as input to our AP network, we further downsample that tensor through an average pooling operation with a 2 × 2 kernel. The 128 × 2 × 2 output is flattened to form the 512-d feature vector x t .\n\nThe Task specific head for Valence and Arousal is composed of four independent Conv2d layers, each with 4 × 4 filters (i.e. equal to the spatial resolution of the input tensor). The first Conv2d layer is the corresponding Valence and Arousal classifier W mentioned in the main document. The output of this layer is a 2-d vector ŷt , corresponding to the values of Valence and Arousal, respectively. In order to boost the performance of the network for the task of predicting the continuous values of Valence and Arousal (ŷ), we approach the backbone training in a Multi-task manner (see below), where the goal is to also classify the basic (discrete) emotion, as well as the bin where both Valence and Arousal would lie in a discretised space. For the basic emotion (happiness, sadness, fear, anger, surprise, disgust and neutral), we include a second Conv2d which outputs the logits corresponding to each of the 7 target classes. For the discretised Valence (v) and Arousal (â), we use two Conv2d layers with 20 outputs each, i.e. we discretise the continuous space in 20 bins, and we treat the task of predicting the corresponding bin as a classification task (see below). Note that these extra heads, as well as the emotion head, are used to reinforce the learning of the regression head tasked with predicting ŷ. Once the network is trained, the heads corresponding to the discrete emotion and the discretised Valence and Arousal are removed from the backbone.\n\nThe Task specific head for Action Unit intensity estimation is also composed of 4 ConvBlocks as for the Valence and Arousal head. The output features, a tensor of 128 × 4 × 4 are also spatially downsampled with average pooling and flattened to form the input features to the AP x t . The Action Units classifier W is a Conv2d with a 4 × 4 filter that maps the 128 × 4 × 4 into either the 5 or 12 target AUs, for BP4D and DISFA, respectively.",
      "page_start": 12,
      "page_end": 14
    },
    {
      "section_name": "A.2. Training",
      "text": "Data processing The faces are first cropped according to a face bounding box, provided by the off-the-shelf face detector RetinaFace  [9] . Given that the first block of the backbone is a Face Alignment Network that is used to provide the features to the subsequent networks, no face registration step is applied. During training, for image augmentation we applied random cropping (224×224), random horizontal flipping, random rotation(-20 • to +20 • ), color jittering, and random gray scaling operations.   6 , our ConvBlocks are of 128 channels, rather than the original 256 used in  [3] . The Face Alignment Network is pre-trained and kept frozen, and returns a set of features resulting from concatenating the output of the last ConvBlock before the Hourglass, the output of the last ConvBlock of the network, and the produced Heatmaps. Then, the Emotion and Action Unit heads follow for each corresponding task. Both have a similar Feature Extraction composed of ×4 ConvBlocks followed by Average Pooling. The output of this module is a 128 × 4 × 4 tensor, which is used as input to the corresponding classifiers, as well as to compute the final feature representation xt that will be used along with ŷt as input to our proposed AP.\n\nFace Alignment Network The Face Alignment Module was trained on the 300W-LP dataset  [3]  using standard Heatmap Regression, and was kept frozen afterwards.\n\nValence and Arousal For Valence and Arousal, the network is trained in a Multi-task way. Let ŷ = (ŷ v , ŷa ) be the Valence and Arousal prediction, ê ∈ R 7 be the output of the discrete emotion layer, and v ∈ R 20 and â ∈ R 20 the output of the Valence and Arousal classes, respectively. We denote by y and e the corresponding Valence and Arousal and Emotion ground-truth values. The loss is defined as:\n\nwhere L mse = ŷy is the standard MSE loss for Valence and Arousal, L ccc = 1 -CCC(ŷv,yv)+CCC(ŷa,ya) The values of the loss weights are all set to 1 except for the MSE loss that is set to λ mse = 0.5. For both SEWA and AffWild2, the training is performed for 20 epochs, using Adam with learning rate 0.0001, (β 1 , β 2 ) = (0.9, 0.999) and weight decay 0.000001. The learning rate is reduced by a factor of 10 after every 5 epochs. For AffWild2 we used the sequences that were annotated with both discrete emotion and Valence and Arousal. Considering that SEWA has not been annotated with the basic emotions, we train our SEWA backbone by extending it with the sequences of AffWild2 containing such annotations. We backpropagate w.r.t. the emotion head using images from AffWild2, and w.r.t. the remaining heads using only images from SEWA. We apply the same 8:1:1 partition described in the paper, and choose the backbone according to the best validation CCC score.\n\nAction Units For Action Unit intensity estimation, Mean Squared Error is used as the loss function to train the corresponding models in this work (for BP4D and DISFA). The AU intensities are normalised from -1 to 1 to align with the L reg used in the AP framework described in the main document. Adam optimizer with a learning rate of 0.0003, (β 1 , β 2 ) = (0.9, 0.999), and an L2 weight decay of 0.00001 is used to train the Action Unit head. To tune the initial learning rate, cyclic learning rate scheduler with a cycle length of 2 is used. After 80 epochs, the best model is selected based on the ICC score on the validation set.\n\nFor BP4D, the model is trained using the official train/validation/test partitions. For DISFA, the model is trained using the three-fold cross validation method described in the main document, using exactly the same generated partitions.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Performance Metrics",
      "text": "For Valence and Arousal, we report the Concordance Correlation Coefficient  [36] , which is used to rank participants in the AVEC Challenge series  [52] . It is a global measure of both correlation and proximity, and is defined as:\n\nwhere µ, σ, and ρ refer to the mean value, (co-)variance, and Pearson Correlation Coefficient, respectively. For Action Unit intensity, we follow the standard ranking criteria used in FERA challenges  [65] , and we report the Intra Class Correlation (ICC  [57] ). For an AU j with ground-truth labels {y j i } N i=1 , and predictions {ỹ j i } N i=1 , the ICC score is defined as ICC j = W j -S j W j +S j , with W j = 1 N i (y j i -ŷj ) 2 + (ỹ j i -ŷj ) 2 , S j = i (y j i -ỹj i ) 2 , and ŷj = 1 2N i (y j i + ỹj i ).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C. Mean Squared Error Results",
      "text": "The additional Mean Squared Error results for DISFA and BP4D are reported in Table  6",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Different ways of modelling temporal context in the liter-",
      "page": 1
    },
    {
      "caption": "Figure 1: depicts an overview of the working ﬂow our",
      "page": 2
    },
    {
      "caption": "Figure 2: We use as",
      "page": 4
    },
    {
      "caption": "Figure 2: Architecture of AP. The encoder takes as input individ-",
      "page": 4
    },
    {
      "caption": "Figure 3: Accuracy on SEWA validation w.r.t. the number of con-",
      "page": 6
    },
    {
      "caption": "Figure 4: Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the",
      "page": 7
    },
    {
      "caption": "Figure 2: Qualitative results: We show a conceptual demonstra-",
      "page": 7
    },
    {
      "caption": "Figure 4: The blue lines represent",
      "page": 7
    },
    {
      "caption": "Figure 2: comprises only ∼330K parameters, and requires only ∼20",
      "page": 8
    },
    {
      "caption": "Figure 5: Note that",
      "page": 12
    },
    {
      "caption": "Figure 5: Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion",
      "page": 13
    },
    {
      "caption": "Figure 5: The ∗64 inscribed",
      "page": 13
    },
    {
      "caption": "Figure 6: , our ConvBlocks are of 128 channels, rather than the original 256 used in [3]. The",
      "page": 13
    },
    {
      "caption": "Figure 6: The Convolutional Block (ConvBlock), used in",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Temporal context is key to the recognition of expressions of"
        },
        {
          "Abstract": "emotion. Existing methods,\nthat rely on recurrent or self-"
        },
        {
          "Abstract": "attention models to enforce temporal consistency, work on"
        },
        {
          "Abstract": "the feature level, ignoring the task-speciﬁc temporal depen-"
        },
        {
          "Abstract": "dencies, and fail\nto model context uncertainty. To alleviate"
        },
        {
          "Abstract": "these issues, we build upon the framework of Neural Pro-"
        },
        {
          "Abstract": "cesses\nto propose a method for apparent emotion recog-"
        },
        {
          "Abstract": "nition with three\nkey novel\ncomponents:\n(a) probabilis-"
        },
        {
          "Abstract": "tic contextual representation with a global\nlatent variable"
        },
        {
          "Abstract": "model;\n(b)\ntemporal context modelling using task-speciﬁc"
        },
        {
          "Abstract": "predictions\nin addition to features; and (c) smart\ntempo-"
        },
        {
          "Abstract": "ral context\nselection. We validate our approach on four"
        },
        {
          "Abstract": "databases,\ntwo for Valence and Arousal estimation (SEWA"
        },
        {
          "Abstract": "and AffWild2), and two for Action Unit intensity estimation"
        },
        {
          "Abstract": "(DISFA and BP4D). Results show a consistent improvement"
        },
        {
          "Abstract": "over a series of strong baselines as well as over state-of-"
        },
        {
          "Abstract": "the-art methods."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1. Introduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "In this paper, we address the problem of\nfacial behaviour"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "recognition\nfrom video,\nin\nparticular,\nthe\nproblem of"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "recognising apparent\nemotions\nin\nterms of Valence\nand"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Arousal\n[51],\nand facial\nexpressions\nin terms of Action"
        },
        {
          "Abstract": "Unit\nintensity [11].\nThis\nis\na\nlongstanding problem in"
        },
        {
          "Abstract": "video recognition which has been extensively studied by"
        },
        {
          "Abstract": "the computer vision community [26, 64, 41, 21, 33, 28,"
        },
        {
          "Abstract": "67, 74, 77, 71, 55, 53, 54, 39, 46]. Nevertheless, even re-"
        },
        {
          "Abstract": "cent methods [27, 33, 77] struggle to achieve high accuracy"
        },
        {
          "Abstract": "on the most difﬁcult datasets including SEWA [34], Aff-"
        },
        {
          "Abstract": "Wild2 [29], BP4D [75] and DISFA [42]. Hence, there is still"
        },
        {
          "Abstract": "great progress to be made towards solving this challenging"
        },
        {
          "Abstract": "problem. In this paper, we show that effective temporal con-"
        },
        {
          "Abstract": "text modelling is a key feature for signiﬁcantly advancing"
        },
        {
          "Abstract": "the state-of-the-art."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "elling and do not perform context selection."
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "Overall, we make the following 3 contributions:"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "1. We propose Affective Processes:\nthe very ﬁrst model"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "for emotion recognition with three key properties:\n(a)"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "global\nstochastic contextual\nrepresentation;\n(b)\ntask-"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "aware temporal context modelling;\nand (c)\ntemporal"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "context selection. We conduct a large number of abla-"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "tion studies illustrating the contribution of each of our"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "model’s key features and components."
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": ""
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "2. We show that our model is more effective in modelling"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "temporal context not only than CNNs+RNNs but also"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "than a strong baseline based on self-attention.\nOur"
        },
        {
          "ilarities. Moreover, they do not use task-aware context mod-": "model outperforms both baselines by large margin."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in order to be robustly recognised. From these challenges,": "this paper focuses on solving problem (c).\nIn particular,\nit",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "databases for emotion recognition in terms of Valence"
        },
        {
          "in order to be robustly recognised. From these challenges,": "proposes a completely unexplored perspective for effective",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "and Arousal estimation, namely SEWA [34] and Af-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "incorporation of temporal context into emotion recognition.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "fWild2 [29]. We further show that our approach can be"
        },
        {
          "in order to be robustly recognised. From these challenges,": "Previous work in temporal modelling of emotions has",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "effective for the problem of Action Unit intensity esti-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "primarily focused on modelling facial expression dynam-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "mation on DISFA [42] and BP4D [75] datasets. On all"
        },
        {
          "in order to be robustly recognised. From these challenges,": "ics,\ni.e.,\nthe way that\nfacial expressions evolve over\ntime",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "datasets used, we show a consistent and often signiﬁ-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "for recognition purposes. A typical deep learning pipeline",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "cant improvement over state-of-the-art methods."
        },
        {
          "in order to be robustly recognised. From these challenges,": "for this has been a Convolution Neural Network (CNN) fol-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "lowed by a Recurrent Neural Network (RNN), usually an",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "2. Related Work"
        },
        {
          "in order to be robustly recognised. From these challenges,": "LSTM or GRU [32, 28, 26, 71]. Although these dynamics",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "This section reviews related work from temporal apparent"
        },
        {
          "in order to be robustly recognised. From these challenges,": "can be particularly useful\nin recognising speciﬁc facial ex-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "emotion recognition, Action Unit\nintensity estimation and"
        },
        {
          "in order to be robustly recognised. From these challenges,": "pressions (e.g.\n[63, 64, 21, 41, 70]), we believe that\ntheir",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "neural modelling of Stochastic Processes."
        },
        {
          "in order to be robustly recognised. From these challenges,": "importance has been over-emphasised for\nthe problem of",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "Temporal Modelling\nin Emotion Recognition:\nMost"
        },
        {
          "in order to be robustly recognised. From these challenges,": "emotion recognition.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "existing methods model\nthe\ntemporal dynamics of\ncon-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "We posit that temporal context is more important than fa-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "tinuous emotions using deterministic approaches\nsuch as"
        },
        {
          "in order to be robustly recognised. From these challenges,": "cial dynamics for emotion recognition. The cues for infer-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "Time-Delay Neural Networks\n[43], RNNs, LSTMs\nand"
        },
        {
          "in order to be robustly recognised. From these challenges,": "ring a person’s apparent emotion from their facial behaviour",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "GRUs\n[32, 28, 26, 71, 8, 59], multi-head attention mod-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "are often sparsely and non-regularly distributed over a tem-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "els [20], 3D Convolutions [76, 35], 3D ConvLSTMs [19],"
        },
        {
          "in order to be robustly recognised. From these challenges,": "poral window of\ninterest\nand collecting such distributed",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "and temporal-hourglass CNNs [10]. While these determin-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "contextual cues is critical to inferring emotions robustly. In",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "istic models are capable of effectively learning the temporal"
        },
        {
          "in order to be robustly recognised. From these challenges,": "addition, due to the person-speciﬁc variability of facial ex-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "dynamics,\nthey do not\ntake the inherent stochastic nature"
        },
        {
          "in order to be robustly recognised. From these challenges,": "pressions but, most\nimportantly, due to their subjective an-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "of\nthe continuous emotion labels into account. Deviating"
        },
        {
          "in order to be robustly recognised. From these challenges,": "notation, context must be modelled in a stochastic manner.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "from this trend, recently, [47] applied variational RNNs [5]"
        },
        {
          "in order to be robustly recognised. From these challenges,": "To address the aforementioned challenges,\nin this work,",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "to the valence estimation problem to add stochasticity to the"
        },
        {
          "in order to be robustly recognised. From these challenges,": "and for the ﬁrst time to the best of our knowledge, we build",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "temporal model of emotion recognition. Similarly,\nin [48],"
        },
        {
          "in order to be robustly recognised. From these challenges,": "upon the framework of Neural Processes [16, 17] to propose",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "emotion is treated as a dynamical latent system state which"
        },
        {
          "in order to be robustly recognised. From these challenges,": "a model\nfor emotion recognition with 3 key components:",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "is estimated using Gaussian processes coupled with Kalman"
        },
        {
          "in order to be robustly recognised. From these challenges,": "(1) stochastic contextual representation with a global latent",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "ﬁlters, while Deep Extended Kalman ﬁlters are used in [49]."
        },
        {
          "in order to be robustly recognised. From these challenges,": "variable model; (2) task-aware temporal context modelling",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "These probabilistic methods have better\nlatent\nstructure-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "not only by using features but also task-speciﬁc predictions;",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "discovery capabilities\nthan the the deterministic temporal"
        },
        {
          "in order to be robustly recognised. From these challenges,": "and (3) effective temporal context selection.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "models, however,\nthey focus only on the temporal dynam-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "Fig.\n1\ndepicts\nan\noverview of\nthe working ﬂow our",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "ics of emotions, ignoring the temporal context.\nIn contrast,"
        },
        {
          "in order to be robustly recognised. From these challenges,": "method (bottom), RNN-based methods (top), and methods",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "our method primarily focuses on directly learning the global"
        },
        {
          "in order to be robustly recognised. From these challenges,": "based on self-attention [66]\n(middle).\nNote methods\nfor",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "temporal context in the input sequence using stochastic pro-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "context modelling based on self-attention do not satisfy any",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "cesses with data-driven ‘priors’."
        },
        {
          "in order to be robustly recognised. From these challenges,": "of (1)-(3): They are deterministic and model long-term de-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "AU Intensity Estimation: In order to model the subjective-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "pendencies not globally, but by using pair-wise feature sim-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "variability of\nthe contextual\nfactors\nin AU intensity esti-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "ilarities. Moreover, they do not use task-aware context mod-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "mation,\nseveral works\nadvocate\nfor probabilistic models"
        },
        {
          "in order to be robustly recognised. From these challenges,": "elling and do not perform context selection.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "[67, 54, 6, 53, 13, 14, 55]. Particularly, Gaussian Processes"
        },
        {
          "in order to be robustly recognised. From these challenges,": "Overall, we make the following 3 contributions:",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "based models [13, 14] and variational\nlatent variable mod-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "els [12, 39, 68] have been extensively used for this purpose."
        },
        {
          "in order to be robustly recognised. From these challenges,": "1. We propose Affective Processes:\nthe very ﬁrst model",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "Other probabilistic approaches to facial affect modelling in-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "for emotion recognition with three key properties:\n(a)",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "clude Conditional Ordinal Random Fields (CORF) [67, 54,"
        },
        {
          "in order to be robustly recognised. From these challenges,": "global\nstochastic contextual\nrepresentation;\n(b)\ntask-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "53] and Markov Random Fields (MRF) [55]. To learn the"
        },
        {
          "in order to be robustly recognised. From these challenges,": "aware temporal context modelling;\nand (c)\ntemporal",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "task-speciﬁc temporal context, context-aware feature fusion"
        },
        {
          "in order to be robustly recognised. From these challenges,": "context selection. We conduct a large number of abla-",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "and label fusion modules are proposed in [77] to model AU"
        },
        {
          "in order to be robustly recognised. From these challenges,": "tion studies illustrating the contribution of each of our",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "intensity temporal dynamics. Although we also propose to"
        },
        {
          "in order to be robustly recognised. From these challenges,": "model’s key features and components.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": ""
        },
        {
          "in order to be robustly recognised. From these challenges,": "",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "learn the task-speciﬁc temporal context, our context\nlearn-"
        },
        {
          "in order to be robustly recognised. From these challenges,": "2. We show that our model is more effective in modelling",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "ing method is completely different to that of [77]."
        },
        {
          "in order to be robustly recognised. From these challenges,": "temporal context not only than CNNs+RNNs but also",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "Stochastic Process Modelling\n[72,\n69,\n61,\n62]:\nOur"
        },
        {
          "in order to be robustly recognised. From these challenges,": "than a strong baseline based on self-attention.\nOur",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "method primarily builds on the recently proposed stochastic"
        },
        {
          "in order to be robustly recognised. From these challenges,": "model outperforms both baselines by large margin.",
          "3. We\nvalidated\nour\napproach\non\nthe most\ndifﬁcult": "regression frameworks of Neural Processes (NPs). NPs [16,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "processes that aim to learn distributions over functions by",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "Let us\nassume\nthat we\nare given a\nsequence of\nimages"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "adapting their priors to the data using context observations.",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "{It}T\nt=1, and that we want to estimate facial affect, in terms"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Application of NPs in Computer Vision has been limited to",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "of Valence and Arousal, on a per-frame basis yt ∈ R2 1."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "basic image completion and super-resolution tasks [23] as",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "The framework of NPs cannot be applied to emotion recog-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "proof-of-concept experiments.\nTo our knowledge, we are",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "nition as is. Actually, application of NPs in Computer Vi-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "the ﬁrst to show how NPs can be used to solve challenging",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "sion has been limited to basic image completion and super-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "real-world regression tasks like that of emotion recognition.",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "resolution tasks [23] as proof-of-concept experiments. One"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "of our main contributions is to show how such a framework"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "3. Method",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "can be applied for challenging real-world temporal regres-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "sion tasks like the one of emotion recognition."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "3.1. Problem Formulation and Background",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Temporal Regression: Given as input signal\nthe sequence",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "3.2.1\nInput Features and Pseudo-labels"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "xt representing, for example, a sequence of images, and the",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "corresponding output\nlabels yt ∈ RN , we wish to learn a",
          "3.2. Affective Processes": "The original NP formulation would directly operate on the"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "function from X to label space Y as f : X → Y.",
          "3.2. Affective Processes": "sequence of\nFurthermore,\nit would require\nimages {It}."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Stochastic Process\n(SP): Unlike previous work in deter-",
          "3.2. Affective Processes": "ground truth labels for the context points yc both at training"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "ministic regression, we are not tasked with estimating a sin-",
          "3.2. Affective Processes": "and test time. While one can assume the ground-truth labels"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "gle function f but rather a distribution of functions P, i.e. a",
          "3.2. Affective Processes": "to be available for\ntraining,\ntheir need at\ntest\ntime would"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Stochastic Process (SP), from which we can sample f ∼ P.",
          "3.2. Affective Processes": "limit\nthe application of NPs to real-world problems. To al-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Each observed sequence in a training set is considered a re-",
          "3.2. Affective Processes": "leviate these issues, we ﬁrstly propose to train a backbone"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "alisation of the SP.",
          "3.2. Affective Processes": "CNN, comprising a feature extractor Φ(.) and a classiﬁer"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "W,\nin a standard supervised manner,\nto provide indepen-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Neural Process (NP): NP uses an encoder-decoder archi-",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "tecture to learn how to sample functions f ∼ P (i.e.\nfrom",
          "3.2. Affective Processes": "dent per-frame estimates of Valence and Arousal ˆyt ∈ R2."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "The purpose of this network is two-fold: Firstly, the fea-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "an SP) conditioned on a set of Context Points. Speciﬁcally,",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "ture extractor Φ(.) is used to provide a per-frame feature"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "a given sequence is split\ninto two parts:\nthe context C for",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "is assumed that\nthe ground truth labels are avail-\nwhich it",
          "3.2. Affective Processes": "representation xt = Φ(It). Then, we can use the sequence"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "of\nto the NP instead of\nthe image se-\nfeatures xt as input"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "and the target T\npoints,\nfor which only\nable {xc, yc}c∈C",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "quence. Secondly, we propose to use the output of the clas-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "is available. Given a function f drawn\nthe input {x}t∈T",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "from an SP conditioned on the context C,\nthe goal\nis to es-",
          "3.2. Affective Processes": "siﬁer ˆyt as pseudo-ground truth labels to be fed as input to"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "the NP. These labels are noisy, and lack temporal consis-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "timate f (xt) for each xt ∈ T . Later on, we propose a way",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "tency, yet they still provide some information regarding the"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "to deal with the unavailability of context ground truth labels",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "true labels.\nIn this paper, we choose as our backbone an"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "{yc}c∈C for the practical scenario of emotion recognition.",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "architecture inspired by that of\n[46, 60, 73]. We refer\nthe"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "The NP Encoder is an MLP qφ that computes an individ-",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "reader to the Supplementary Material for further details."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "ual feature representation rc = qφ(xc, yc) ∈ Rd (d = 128",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "Frozen Backbone:\nIt\nis important\nto remark that Φ(.) and"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "in our work)\nfor\neach input\ncontext pair,\nfollowed by a",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "W are kept frozen during the training of our AP. While one"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "permutation-invariant\naggregation operation. We\nchoose",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "could opt for ﬁnetuning the backbone while training the AP,"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "simple summation for aggregation to compute a global de-",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "(cid:80)",
          "3.2. Affective Processes": "the change in Φ(.) and W would lead to changing both the"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "terministic context\nrepresentation rC = 1\nc rc ∈ Rd,\nC",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "input space and the pseudolabels,\nleading to poorer perfor-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "with C the number of context points.\nTo account\nfor\nthe",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "mance. When minimising the AP objective (see below), W"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "context\nstochasticity, rC is used to parameterise a Gaus-",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "is no longer learned through the error of the values ˆy, and"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "sian distribution from which a global\nlatent variable z can",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "thus it can lead to produce pseudolabels that\nlie out of the"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "be drawn. To this end,\ntwo linear layers are used to com-",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "space Y, i.e.\nthat are not valid. Recall that NPs rely on esti-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "pute the mean µC(rC) and standard deviation σC(rC). The",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "mating f (xt) for the target points, provided some observed"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "global\nlatent variable z ∼ N (µC, σC) ∈ Rd represents a",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "In this paper the values\nvalues yc = f (xc) for the context."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "different realisation of the SP conditioned on the context.",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "yc are replaced by the predicted values ˆyc provided by W."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "that samples functions\nThe NP Decoder is an MLP fθ",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "If these values change over the course of training, and be-"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "f ∼ P through the latent variable z. For a given z, function",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "come non plausible values of f ,\nthe whole NP framework"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "samples are generated at all target points as yt = fθ(xt, z).",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "becomes invalid. We experimentally tried ﬁne-tuning the"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "Assuming some\ni.i.d.\nGaussian observation noise with",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "backbone, and observed a poor performance."
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "learnable variance\n[22],\ncan be\nthat of\nthe output of fθ",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "a mean and standard deviation of a Gaussian distribution:",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "",
          "3.2. Affective Processes": "1For our experiments, we will also consider\nthe task of Action Unit"
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "yt ∼ N (f µ",
          "3.2. Affective Processes": ""
        },
        {
          "17, 23, 18, 58, 38] are a family of predictive stochastic": "θ (xt, z)).\nθ (xt, z), f σ",
          "3.2. Affective Processes": "intensity estimation."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "NP literature that pseudo-ground truth generated from an"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "independent network can be effectively used for deﬁning"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "the labels for the context points used as input to the NP."
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "3.2.2\nContext Selection"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "Our ﬁrst and obvious option for context selection is uniform"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "random sampling from the sequence, which would imply"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "the strong assumption that\nthe errors of\nthe backbone are"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "consistent. Considering that\nthe backbone works in a per-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "frame basis,\nit\nis likely that some of the input features are"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "more indicative of the context than others. In this paper, we"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "propose to delve into the properties of the NPs to investigate"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "whether we can infer which frames are most optimal\nfor"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "context representation."
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "Given that the number of context points can be any num-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "ber between 1 and the length of the sequence, the choice of"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "rC = rc for a single context point c also results in a valid"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "latent function. Building on this concept and the fact\nthat"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "the latent\nfunctions,\nσC(rC) represents the uncertainty of"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "we propose to select the context by ﬁrst estimating the set of"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "latent functions that each of the individual representations"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "rc would propose, and select those with lowest uncertainty."
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "That is, we propose to ﬁrst estimate rc and their correspond-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "ing σC(rc) for all the frames in a given sequence, and select"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "the C with lowest uncertainty. The context set C is then de-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "ﬁned as the C frames for which σc(rc) is the lowest. Note"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "that this step is used to select the context points, only. Once"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "context points are selected, they are passed through the ag-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "gregation function to compute rC in the standard way. This"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "adds a negligible extra complexity to the framework (the"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "values of rc do not need to be computed again). Finally, we"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "emphasize that we apply context selection only at test time."
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "3.2.3\nThe Complete Architecture"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "The complete architecture is shown in Fig. 2. We use as"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "input\nthe features xt extracted from the pre-trained back-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "bone, as well as the corresponding predictions. The encoder"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "the input pairs\nqφ is a three-layer MLP that maps each of"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "(xc, ˆyc) into a latent representation rc ∈ R128. The input"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "pseudolabels ˆyt are ﬁrstly upsampled to match the dimen-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "sion of the feature representation. Once rC has been com-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "puted from the average of the selected context points,\nit\nis"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "forwarded to a latent encoder, which comprises a common"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "linear layer and two independent\nlinear layers,\nto compute"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": ""
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "µC and σC, respectively. The decoder fθ is another three-"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "layer MLP that receives the individual features xt and the"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "latent\nrepresentation z, and produces two outputs ˜yµ\nand\nt"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "yσ\nt , parameterising the output distribution. During training,"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "test\ntime, we\nz ∼ N (µC, σC), and ˜yt ∼ N (˜yµ\nt , ˜yσ\nt ). At"
        },
        {
          "Overall,\nto our knowledge, we are the ﬁrst\nto show in": "use the predictive mean and set z = µC and ˜yt = ˜yµ\nt ."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "performing 8 different tasks. It is the main corpus of FERA"
        },
        {
          "3.3. Comparison with other methods": "It is important to remark that our model comes with the fol-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "2015 challenge [65]. We use the ofﬁcial train (168 videos),"
        },
        {
          "3.3. Comparison with other methods": "lowing beneﬁts: a) it allows to automatically infer the global",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "validation (160 videos),\nand test\n(159 videos) partitions,"
        },
        {
          "3.3. Comparison with other methods": "context;\nthis adds a new key to existing Neural Processes",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "which contain annotations for 5 different AUs."
        },
        {
          "3.3. Comparison with other methods": "where context\nis manually given, b) it allows to model\nthe",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "DISFA [42] contains 27 videos of 27 different\nsubjects,"
        },
        {
          "3.3. Comparison with other methods": "uncertainty on the estimated context,\nand c)\nit allows\nto",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "with 4,844 frames each. It contains annotations for 12 AUs."
        },
        {
          "3.3. Comparison with other methods": "draw coherent samples; note that while the decoder works",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "We follow a 3-fold cross-validation protocol with 18 sub-"
        },
        {
          "3.3. Comparison with other methods": "on a per-frame basis,\nthe fact\nthat\nit\nis conditioned on a",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "jects used for training and 9 subjects used for testing."
        },
        {
          "3.3. Comparison with other methods": "global\nlatent variable whose value can be drawn from a la-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Performance Metrics: For Valence and Arousal, we follow"
        },
        {
          "3.3. Comparison with other methods": "tent distribution, makes all of the samples be dependent on",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "the AVEC Challenge series [52] and report the Concordance"
        },
        {
          "3.3. Comparison with other methods": "it [7].",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Correlation Coefﬁcient [36]. For Action Units, we follow"
        },
        {
          "3.3. Comparison with other methods": "Vs. RNN: The task of\ntemporal\nregression could be ap-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "the ranking criteria used in FERA challenges [65], and re-"
        },
        {
          "3.3. Comparison with other methods": "proached by using a parametric form for f , by means of an",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "port the Intra Class Correlation (ICC [57]). A full deﬁnition"
        },
        {
          "3.3. Comparison with other methods": "RNN. However, this assumes that the variables yt are drawn",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "of these metrics can be found in the Supp. Material."
        },
        {
          "3.3. Comparison with other methods": "from Markov chains, which in practice disregard long-term",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "dependencies needed for temporal context modelling. Fur-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "4.2. Implementation details"
        },
        {
          "3.3. Comparison with other methods": "thermore,\nit can be shown [17]\nthat\nthe joint distribution",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "All experiments are developed in PyTorch [50].\nFirst, we"
        },
        {
          "3.3. Comparison with other methods": "it is permutation invariant. This\np(yT ) is exchangeable, i.e.",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "pre-train the backbone architecture Φ and W on a per-frame"
        },
        {
          "3.3. Comparison with other methods": "offers a new insight into temporal modelling that differenti-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "basis\nfor each corresponding dataset\n(see Supplementary"
        },
        {
          "3.3. Comparison with other methods": "ates from RNNs that condition a current observation on the",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Material). We then use as input\nthe features xt extracted"
        },
        {
          "3.3. Comparison with other methods": "previous hidden state,\ni.e. where order matters. We argue",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "from the pre-trained backbone, as well as the correspond-"
        },
        {
          "3.3. Comparison with other methods": "that, provided a given static image, the corresponding labels",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "ing predictions. The dimension of input features xt is 512."
        },
        {
          "3.3. Comparison with other methods": "should not be position-dependent.\nIf we shufﬂe a given se-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Valence and Arousal:\nTo train the AP for Valence and"
        },
        {
          "3.3. Comparison with other methods": "quence and pick up a frame at random,\nthe corresponding",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Arousal, we randomly sample sequences between 35 and"
        },
        {
          "3.3. Comparison with other methods": "annotation is not expected to change. Our approach seeks",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "70 frames from the corresponding training set. The range"
        },
        {
          "3.3. Comparison with other methods": "an orderless modelling of sequences, by looking at them as",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "35 − 70 can be seen as an augmentation parameter: using"
        },
        {
          "3.3. Comparison with other methods": "a whole through the global latent variable z.",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "a shorter\nrange yielded worse results due to lack of gen-"
        },
        {
          "3.3. Comparison with other methods": "Vs.\nSelf-attention:\nSelf-attention models\n[66] can also",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "eralisation, whereas no improvement was observed when"
        },
        {
          "3.3. Comparison with other methods": "model long-term dependencies using pair-wise feature sim-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "using a longer range of sequences. The number of context"
        },
        {
          "3.3. Comparison with other methods": "ilarities. On the contrary, we choose NPs to estimate f by",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "points is randomly chosen from 3 to the length of the given"
        },
        {
          "3.3. Comparison with other methods": "sampling from a global latent random variable. This allows",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "sequence. During training, the target labels are the ground-"
        },
        {
          "3.3. Comparison with other methods": "for a more direct and holistic modelling of the temporal con-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "truth annotations. For the context labels, we use the ground-"
        },
        {
          "3.3. Comparison with other methods": "text. Furthermore, unlike self-attention models, in NPs both",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "truth annotations or the predictions from W with probabil-"
        },
        {
          "3.3. Comparison with other methods": "input signal xc, and labels yc are used to learn context.",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "ity 0.5. The batch size is 16 videos, and the learning rate"
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "and weight decay are set\nto 0.00025 and 0.0001,\nrespec-"
        },
        {
          "3.3. Comparison with other methods": "4. Experimental Settings",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "tively. The AP is trained for 25 epochs, each of 1000 iter-"
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "ations, using Adam [24] with (β1, β2) = (0.9, 0.999). We"
        },
        {
          "3.3. Comparison with other methods": "4.1. Datasets and Performance Metrics",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "use a Cosine Annealing strategy for the learning rate [40]."
        },
        {
          "3.3. Comparison with other methods": "We\nvalidate\nour\napproach\nfor\nemotion\nrecognition\non",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "At test time, we use a ﬁxed sequence length of 70 frames."
        },
        {
          "3.3. Comparison with other methods": "SEWA [34] and Aff-Wild2 [26, 29, 30, 28, 31, 27, 74].",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Action Units: We used the same setting as for Valence and"
        },
        {
          "3.3. Comparison with other methods": "We also validate our approach for the task of Action Unit",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Arousal, with the following differences:\nthe batch size is set"
        },
        {
          "3.3. Comparison with other methods": "intensity estimation on BP4D [75, 65] and DISFA [42].",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "to 6 videos, and the learning rate and weight decay are set"
        },
        {
          "3.3. Comparison with other methods": "SEWA [34] is a large video dataset for affect estimation in-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "to 0.0001 and 0.0005, respectively."
        },
        {
          "3.3. Comparison with other methods": "the-wild.\nIt comprises over 2, 000 minutes of video data",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "annotated with valence and arousal values.\nIt contains sub-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "5. Ablation Studies"
        },
        {
          "3.3. Comparison with other methods": "jects from 6 different cultures. We follow the evaluation",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "We use the validation partition of SEWA to perform our ab-"
        },
        {
          "3.3. Comparison with other methods": "protocol of\n[34, 33], and apply a 8:1:1 dataset partition2.",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "lation studies. We study the effect of our proposed context"
        },
        {
          "3.3. Comparison with other methods": "AffWild2 [27] is a large-scale video dataset divided in train-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "selection method, the contribution of the pseudo labels, the"
        },
        {
          "3.3. Comparison with other methods": "ing, validation, and test partitions. Because the labels for",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "effect of the different\nlosses used for the AP training, and"
        },
        {
          "3.3. Comparison with other methods": "the latter are not made publicly available, we use the train-",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "the effect of the latent variable. Finally, we explore a dif-"
        },
        {
          "3.3. Comparison with other methods": "ing and validation only for our evaluation purposes.",
          "BP4D [75, 65] contains videos collected from 41 subjects,": ""
        },
        {
          "3.3. Comparison with other methods": "",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "ferent alternative to the AP conﬁguration [37]. We study in"
        },
        {
          "3.3. Comparison with other methods": "2The same partitions were kindly provided by the data owners.",
          "BP4D [75, 65] contains videos collected from 41 subjects,": "Sec. 6 the complexity of our approach."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: (Determinis-",
      "data": [
        {
          "Model": "Backbone",
          "Valence": "0.576",
          "Arousal": "0.632",
          "Mean": "0.604"
        },
        {
          "Model": "APnll",
          "Valence": "0.645",
          "Arousal": "0.676",
          "Mean": "0.660"
        },
        {
          "Model": "APnll+kl",
          "Valence": "0.649",
          "Arousal": "0.669",
          "Mean": "0.659"
        },
        {
          "Model": "APnll+reg",
          "Valence": "0.649",
          "Arousal": "0.676",
          "Mean": "0.662"
        },
        {
          "Model": "APnll+reg+kl",
          "Valence": "0.643",
          "Arousal": "0.628",
          "Mean": "0.635"
        },
        {
          "Model": "Deterministic AP",
          "Valence": "0.650",
          "Arousal": "0.576",
          "Mean": "0.613"
        },
        {
          "Model": "AP + Det.",
          "Valence": "0.649",
          "Arousal": "0.658",
          "Mean": "0.652"
        },
        {
          "Model": "APw/o W",
          "Valence": "0.600",
          "Arousal": "0.454",
          "Mean": "0.527"
        },
        {
          "Model": "AP + Det. + Att",
          "Valence": "0.662",
          "Arousal": "0.672",
          "Mean": "0.667"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: (Determinis-",
      "data": [
        {
          "0.600\n0.454\n0.527\nAPw/o W": "0.662\n0.667\nAP + Det. + Att\n0.672"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "Table 1. Ablation studies on the validation set of SEWA."
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "although with a single-branch latent encoder and no sam-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "pling. With a deterministic AP, one dispenses with the co-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "herent sampling i.e.\nthe dependence among the target out-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "puts is lost, and assumed to be i.i.d. The learning burden is"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "put only into capturing the observation variance ˜yσ to ac-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "count\nfor\nthe output uncertainty (i.e.\nit\nis probabilistic in"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "the output, but not in the latent representation). The results"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "of the deterministic AP are shown in Table 1 (Determinis-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "tic AP). We observe that the performance degrades substan-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "tially,\nin particular regarding the Arousal predictions. This"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "illustrates the importance of the latent variable to model the"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "underlying variability for the joint prediction of Valence and"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "Arousal. Finally, we also investigated the dimensionality of"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "z, observing little effect when having more dimensions."
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "Effect of\nlosses:\nThe training of\nthe APs is carried out"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "by minimising the objective shown in Eqn. 1. Recent ﬁnd-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "ings [15] suggest\nthat\ntraining an NP using only a maxi-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "mum likelihood approach (i.e. by minimising the negative"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "log-likelihood Lnll) can improve performance and avoid"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "the practical\nissues of using amortised variational\ninfer-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "ence (i.e. Lnll + Lkl). We studied the effect of each case"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "along with our proposed loss. The results are shown in Ta-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "to the maximum likelihood ap-\nble 1. We refer with APnll"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "proach, with APnll+kl to the one that minimises Lnll + Lkl,"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "with APnll+reg to the one that minimises Lnll + Lreg, and"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "to the one that minimises the full\nloss\nwith APnll+reg+kl"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "in Eqn. 1.\nThe numerical\nresults do not\nshed light\ninto"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "whether one is better\nthan the other. We observe that a"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "combination of the maximum likelihood approach with our"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "proposed regularisation slightly improves the numerical re-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "sults, although we acknowledge these not\nto be signiﬁcant"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "enough.\nIf we use the three losses together we observe a"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "performance degradation.\nThese results align with recent"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "ﬁndings that advocate for a further\ninvestigation of which"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "of the losses is a better objective for Neural Processes [15]."
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": ""
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "Different network conﬁgurations:\nWe also study differ-"
        },
        {
          "0.600\n0.454\n0.527\nAPw/o W": "ent network conﬁgurations. We ﬁrst evaluate the perfor-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "lowest values of (cid:107)σc(cid:107). Blue line represents the ground-truth labels. The green line represents the predictions from the backbone. The"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "black dots represent the frames selected as context using σc. The light red curves represent different realisations of f (., z) for 10 different"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "(sampled) z, and the red curve represents the process returned by the predictive mean µC . The variation in the functions corresponds to"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "the global latent uncertainty given the selected context."
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "mance of using a combined latent and deterministic layer"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "denoted as AP + Det.. The performance of the AP + Det."
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "is on par with that of the single AP. We then explore the use"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "of Attention in the AP + Det. where self-attention is used"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "to replace the aggregation of the individual representations"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "(attention is permutation-invariant). Considering the trade-"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "off between complexity and accuracy, we opt\nfor keeping"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "the simplest architecture, described in Fig. 2."
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "Qualitative\nresults: We\nshow a\nconceptual demonstra-"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "tion of\nthe AP capabilities for both context selection and"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "functional proposals\nin Fig. 4.\nThe blue lines\nrepresent"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "the ground-truth annotations for different segments of 70"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "frames. The green lines correspond to the individual predic-"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "tions ˜yt given by the backbone W. The black dots represent"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "the frames selected as context according to the lowest val-"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "ues of (cid:107)σc(cid:107). For the sake of clarity, we used only 10 points"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "to select context.\nThe light\nred curves represent different"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "realisations of the functions fθ(., z) given by different sam-"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "ples of z, with the function corresponding to µC represented"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "in dark red. We can see that\nthe proposed functions are"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "able to better approximate the sequential predictions from"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "the noisy individual representations,\nleading to much more"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "stable predictions. We can observe that\nthe model\ntends to"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "select frames that correspond to lower error, indicating how"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "the individual latent representations can represent their con-"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "ﬁdence in the context."
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": ""
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "6. Comparison with state-of-the-art"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "We compare the results of our models w.r.t.\nthe most recent"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "works in emotion and Action Unit intensity estimation. The"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "bulk of our results are shown in Tables 2 and 3 for Valence"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "and Arousal, and Tables 4 and 5 for Action Units (MSE is"
        },
        {
          "Figure 4. Examples of modelling Valence and Arousal on SEWA dataset, when using as context only 10 points, selected according to the": "also reported in the Supplementary Material)."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Mitekova et al. [44]",
          "Valence": "0.44",
          "Arousal": "0.39",
          "Mean": "0.42"
        },
        {
          "Model": "Kossaﬁ et al. [33]",
          "Valence": "0.75",
          "Arousal": "0.52",
          "Mean": "0.64"
        },
        {
          "Model": "Ours backbone",
          "Valence": "0.69",
          "Arousal": "0.61",
          "Mean": "0.65"
        },
        {
          "Model": "ConvGRU [4]†",
          "Valence": "0.72",
          "Arousal": "0.62",
          "Mean": "0.67"
        },
        {
          "Model": "Self-Attn [66]†",
          "Valence": "0.70",
          "Arousal": "0.61",
          "Mean": "0.65"
        },
        {
          "Model": "Ours AP",
          "Valence": "0.75",
          "Arousal": "0.64",
          "Mean": "0.69"
        },
        {
          "Model": "",
          "Valence": "",
          "Arousal": "",
          "Mean": ""
        },
        {
          "Model": "",
          "Valence": "",
          "Arousal": "",
          "Mean": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours AP": "",
          "0.75": "",
          "0.64": "",
          "0.69": ""
        },
        {
          "Ours AP": "Table 2.",
          "0.75": "",
          "0.64": "",
          "0.69": ""
        },
        {
          "Ours AP": "test partition. † denotes in-house implementation.",
          "0.75": "",
          "0.64": "",
          "0.69": ""
        },
        {
          "Ours AP": "AU",
          "0.75": "2",
          "0.64": "4",
          "0.69": "6"
        },
        {
          "Ours AP": "VGP-AE [12]",
          "0.75": "0.47",
          "0.64": "0.62",
          "0.69": "0.50"
        },
        {
          "Ours AP": "2DC [39]",
          "0.75": "0.55",
          "0.64": "0.69",
          "0.69": "0.59"
        },
        {
          "Ours AP": "HR [46]",
          "0.75": "0.52",
          "0.64": "0.75",
          "0.69": "0.51"
        },
        {
          "Ours AP": "Ours backbone",
          "0.75": "0.15",
          "0.64": "0.76",
          "0.69": "0.50"
        },
        {
          "Ours AP": "BiGRU [4]†",
          "0.75": "0.13",
          "0.64": "0.77",
          "0.69": "0.53"
        },
        {
          "Ours AP": "Self-Attn\n[66]†",
          "0.75": "0.17",
          "0.64": "0.78",
          "0.69": "0.51"
        },
        {
          "Ours AP": "Ours AP",
          "0.75": "0.19",
          "0.64": "0.78",
          "0.69": "0.52"
        },
        {
          "Ours AP": "",
          "0.75": "",
          "0.64": "",
          "0.69": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ours AP": "",
          "0.35": "",
          "0.19": "",
          "0.78": "",
          "0.73": "",
          "0.65\n0.52": "",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": ""
        },
        {
          "Ours AP": "AU",
          "0.35": "10",
          "0.19": "12",
          "0.78": "14",
          "0.73": "17",
          "0.65\n0.52": "Avg.",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "parison,\nthe ConvGRU model has ∼789K parameters, and"
        },
        {
          "Ours AP": "CDL [1]",
          "0.35": "0.73",
          "0.19": "0.83",
          "0.78": "0.50",
          "0.73": "0.37",
          "0.65\n0.52": "0.62",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "requires ∼758 MFLOPs for L = 30 frames. The PyTorch"
        },
        {
          "Ours AP": "ISIR [45]",
          "0.35": "0.80",
          "0.19": "0.86",
          "0.78": "0.71",
          "0.73": "0.44",
          "0.65\n0.52": "0.72",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "built-in 2-layer BiGRU has ∼ 1.2M parameters, and needs"
        },
        {
          "Ours AP": "HR [46]",
          "0.35": "0.82",
          "0.19": "0.80",
          "0.78": "0.71",
          "0.73": "0.50",
          "0.65\n0.52": "0.73",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "∼71 MFLOPs\nfor L = 60 frames.\nThe Multi-Head at-"
        },
        {
          "Ours AP": "Ours backbone",
          "0.35": "0.74",
          "0.19": "0.82",
          "0.78": "0.62",
          "0.73": "0.39",
          "0.65\n0.52": "0.67",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "tention model has ∼4.2M parameters, and requires ∼125"
        },
        {
          "Ours AP": "BiGRU [4]†",
          "0.35": "0.76",
          "0.19": "0.83",
          "0.78": "0.62",
          "0.73": "0.50",
          "0.65\n0.52": "0.70",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "MFLOPs for L = 30 frames."
        },
        {
          "Ours AP": "Self-Attn [66]†",
          "0.35": "0.78",
          "0.19": "0.79",
          "0.78": "0.60",
          "0.73": "0.42",
          "0.65\n0.52": "0.68",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": ""
        },
        {
          "Ours AP": "Ours AP",
          "0.35": "0.80",
          "0.19": "0.86",
          "0.78": "0.69",
          "0.73": "0.51",
          "0.65\n0.52": "0.74",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": ""
        },
        {
          "Ours AP": "",
          "0.35": "",
          "0.19": "",
          "0.78": "",
          "0.73": "",
          "0.65\n0.52": "",
          "0.61\n0.28\n0.58\n0.81\n0.49\n0.92\n0.67": "7. Conclusions"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "Self-Attn [66]†\n0.80\n0.78\n0.79\n0.60\n0.42\n0.68",
          "MFLOPs for L = 30 frames.": ""
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "0.82\n0.86\n0.51\n0.74\nOurs AP\n0.80\n0.69",
          "MFLOPs for L = 30 frames.": ""
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "",
          "MFLOPs for L = 30 frames.": "7. Conclusions"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "Table 5. Results on the test partition of BP4D dataset (in ICC val-",
          "MFLOPs for L = 30 frames.": ""
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "",
          "MFLOPs for L = 30 frames.": "We proposed a novel temporal context learning approach to"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "ues). † denotes in-house evaluation",
          "MFLOPs for L = 30 frames.": ""
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "",
          "MFLOPs for L = 30 frames.": "facial affect recognition, by modelling the stochastic nature"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "",
          "MFLOPs for L = 30 frames.": "of affect\nlabels. To model uncertainty in the temporal con-"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "on AffWild2, where our AP signiﬁcantly improves the Va-",
          "MFLOPs for L = 30 frames.": "text of\nthe affective signals, our proposed method, called"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "lence CCC. On DISFA, our backbone is again on par with",
          "MFLOPs for L = 30 frames.": "Affective Processes, builds on Neural Processes to learn a"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "the state of\nthe art methods VGP-AE [12] and 2DC [39].",
          "MFLOPs for L = 30 frames.": "distribution of functions. Our method addresses a key lim-"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "However, our AP brings the largest\nimprovement setting a",
          "MFLOPs for L = 30 frames.": "itation of\nthe NPs — that of\nrequiring ground-truth labels"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "new state-of-the-art ICC of 0.58, which improves over the",
          "MFLOPs for L = 30 frames.": "at\ninference time — by proposing the use of pseudo-labels"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "Heatmap Regression method of Ntinou et al. [46]. Finally,",
          "MFLOPs for L = 30 frames.": "provided by a pre-trained backbone and by using a novel"
        },
        {
          "BiGRU [4]†\n0.78\n0.76\n0.83\n0.62\n0.50\n0.70": "we compare our method against\nthe winner and runner-up",
          "MFLOPs for L = 30 frames.": "method for automatic context selection. We believe that the"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Yann Dubois, James Requeima, and Richard E Turner. Meta-"
        },
        {
          "References": "[1] Tadas Baltruˇsaitis, Marwa Mahmoud, and Peter Robinson.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "learning stationary stochastic process prediction with convo-"
        },
        {
          "References": "Cross-dataset learning and person-speciﬁc normalisation for",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Inform. Process.\nlutional neural processes.\nIn Adv. Neural"
        },
        {
          "References": "automatic action unit detection.\nIn IEEE Conf. and Work-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Syst., 2020. 6"
        },
        {
          "References": "shops on Auto. Face and Gesture Recog., volume 6, pages",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[16] Marta Garnelo, Dan Rosenbaum, Christopher Maddison,"
        },
        {
          "References": "1–6. IEEE, 2015. 8",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye"
        },
        {
          "References": "[2] Lisa\nFeldman Barrett,\nRalph Adolphs,\nStacy Marsella,",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Teh, Danilo Rezende,\nand SM Ali Eslami.\nConditional"
        },
        {
          "References": "Aleix M Martinez, and Seth D Pollak. Emotional expressions",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "neural processes.\nIn International Conference on Machine"
        },
        {
          "References": "reconsidered: Challenges to inferring emotion from human",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Learning, pages 1704–1713, 2018. 2, 3"
        },
        {
          "References": "facial movements. Psychological science in the public inter-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[17] Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio"
        },
        {
          "References": "est, 20(1):1–68, 2019. 1",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye"
        },
        {
          "References": "[3] Adrian Bulat and Georgios Tzimiropoulos. How far are we",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Teh. Neural processes.\nIn ICML Workshop on Theoretical"
        },
        {
          "References": "from solving the 2d & 3d face alignment problem?(and a",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Foundations and Applications of Deep Generative Models,"
        },
        {
          "References": "dataset of 230,000 3d facial\nlandmarks).\nIn Proceedings",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "2018. 2, 3, 5"
        },
        {
          "References": "of\nthe IEEE International Conference on Computer Vision,",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[18]\nJonathan Gordon, Wessel P Bruinsma, Andrew YK Foong,"
        },
        {
          "References": "pages 1021–1030, 2017. 12, 13, 14",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "James Requeima, Yann Dubois, and Richard E Turner. Con-"
        },
        {
          "References": "[4] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre,",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "volutional\nconditional neural processes.\nIn International"
        },
        {
          "References": "Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Conference on Learning Representations, 2020. 3"
        },
        {
          "References": "Yoshua Bengio.\nLearning phrase representations using rnn",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[19]\nJian Huang, Ya Li, Jianhua Tao, Zheng Lian, and Jiangyan"
        },
        {
          "References": "arXiv\nencoder-decoder\nfor\nstatistical machine translation.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Yi. End-to-end continuous emotion recognition from video"
        },
        {
          "References": "preprint arXiv:1406.1078, 2014. 7, 8, 14",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "using 3d convlstm networks.\nIn ICASSP, pages 6837–6841."
        },
        {
          "References": "[5]\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "IEEE, 2018. 2"
        },
        {
          "References": "Goel, Aaron C Courville, and Yoshua Bengio. A recurrent",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[20]\nJian Huang, Jianhua Tao, Bin Liu, Zhen Lian, and Mingyue"
        },
        {
          "References": "latent variable model for sequential data.\nIn Adv. Neural In-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Niu. Efﬁcient modeling of long temporal contexts for contin-"
        },
        {
          "References": "form. Process. Syst., pages 2980–2988, 2015. 2",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "uous emotion recognition.\nIn Int. Conf. Affec. Comput. and"
        },
        {
          "References": "[6] Arnaud Dapogny, Kevin Bailly,\nand Severine Dubuisson.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Intel. Inter., pages 185–191. IEEE, 2019. 2"
        },
        {
          "References": "Pairwise\nconditional\nrandom forests\nfor\nfacial\nexpression",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[21]\nShashank Jaiswal and Michel Valstar. Deep learning the dy-"
        },
        {
          "References": "recognition.\nIn Int. Conf. Comput. Vis., pages 3783–3791,",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "namic appearance and shape of facial action units.\nIn IEEE"
        },
        {
          "References": "2015. 2",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Winter Conf. on Appl. of Comput. Vis., pages 1–8.\nIEEE,"
        },
        {
          "References": "[7] Bruno De Finetti. La pr´evision: ses lois logiques, ses sources",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "2016. 1, 2"
        },
        {
          "References": "subjectives. Annales de l’institut Henri Poincar´e, 7(1):1–68,",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[22] Alex Kendall and Yarin Gal. What uncertainties do we need"
        },
        {
          "References": "1937. 5",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "in bayesian deep learning for computer vision? In I. Guyon,"
        },
        {
          "References": "[8] Didan Deng, Zhaokang Chen, and Bertram E Shi. Multitask",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-"
        },
        {
          "References": "emotion recognition with incomplete labels.\nIn IEEE Conf.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "wanathan, and R. Garnett, editors, Adv. Neural Inform. Pro-"
        },
        {
          "References": "and Workshops on Auto. Face and Gesture Recog., pages",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "cess. Syst., pages 5574–5584. Curran Associates, Inc., 2017."
        },
        {
          "References": "828–835, 2020. 2",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "3"
        },
        {
          "References": "[9]\nJiankang Deng,\nJia Guo, Yuxiang Zhou,\nJinke Yu,\nIrene",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[23] Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Gar-"
        },
        {
          "References": "Kotsia,\nand\nStefanos\nZafeiriou.\nRetinaface:\nSingle-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "nelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals,\nand"
        },
        {
          "References": "arXiv preprint\nstage dense face localisation in the wild.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Yee Whye Teh. Attentive neural processes.\nIn International"
        },
        {
          "References": "arXiv:1905.00641, 2019. 12",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Conference on Learning Representations, 2018. 3"
        },
        {
          "References": "[10] Zhengyin Du, Suowei Wu, Di Huang, Weixin Li, and Yun-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[24] Diederik P Kingma and Jimmy Ba. Adam: A method for"
        },
        {
          "References": "hong Wang. Spatio-temporal encoder-decoder fully convolu-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "stochastic optimization.\narXiv preprint arXiv:1412.6980,"
        },
        {
          "References": "tional network for video-based dimensional emotion recog-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "2014. 5"
        },
        {
          "References": "nition.\nIEEE Trans. on Affect. Comput., 2019. 2",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "[11]\nPaul Ekman. Facial action coding system. 1977. 1",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[25] Diederik P Kingma and Max Welling. Auto-encoding varia-"
        },
        {
          "References": "",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "tional bayes. arXiv preprint arXiv:1312.6114, 2013. 4"
        },
        {
          "References": "[12]\nStefanos Eleftheriadis, Ognjen Rudovic, Marc Peter Deisen-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": ""
        },
        {
          "References": "roth, and Maja Pantic.\nVariational gaussian process auto-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[26] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-"
        },
        {
          "References": "encoder\nfor ordinal prediction of\nfacial\naction units.\nIn",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "ing Zhao, and Stefanos Zafeiriou. Recognition of affect\nin"
        },
        {
          "References": "ACCV, pages 154–170. Springer, 2016. 2, 8, 14",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "the wild using deep neural networks. In IEEE Conf. Comput."
        },
        {
          "References": "[13]\nStefanos Eleftheriadis, Ognjen Rudovic, Marc Peter Deisen-",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Vis. Pattern Recog. Worksh., pages 1972–1979. IEEE, 2017."
        },
        {
          "References": "roth,\nand Maja Pantic.\nGaussian process domain experts",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "1, 2, 5"
        },
        {
          "References": "for modeling of facial affect.\nIEEE Trans. Image Process.,",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[27] Dimitrios Kollias, Attila Schulc, Elnar Hajiyev, and Stefanos"
        },
        {
          "References": "26(10):4697–4711, 2017. 2",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Zafeiriou.\nAnalysing affective behavior\nin the ﬁrst abaw"
        },
        {
          "References": "[14]\nStefanos Eleftheriadis, Ognjen Rudovic,\nand Maja Pantic.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "2020 competition.\narXiv preprint arXiv:2001.11409, 2020."
        },
        {
          "References": "Discriminative shared gaussian processes for multiview and",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "1, 5"
        },
        {
          "References": "IEEE Trans.\nview-invariant\nfacial expression recognition.",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "[28] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "References": "Image Process., 24(1):189–204, 2014. 2",
          "[15] Andrew YK Foong, Wessel P Bruinsma, Jonathan Gordon,": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "in-the-wild: Aff-wild database and challenge, deep architec-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Deng,\nJinkuang Cheng,\nand John P Cosmas.\nTime-delay"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "tures, and beyond.\nInt. J. Comput. Vis., pages 1–23, 2019. 1,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "neural network for continuous emotional dimension predic-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "2, 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "tion from facial expression sequences. IEEE transactions on"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[29] Dimitrios Kollias and Stefanos Zafeiriou.\nAff-wild2: Ex-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "cybernetics, 46(4):916–929, 2015. 2"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "arXiv\ntending the aff-wild database for affect\nrecognition.",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[44] Anna Mitenkova, Jean Kossaiﬁ, Yannis Panagakis, and Maja"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "preprint arXiv:1811.07770, 2018. 1, 2, 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Pantic. Valence and arousal estimation in-the-wild with ten-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[30] Dimitrios Kollias and Stefanos Zafeiriou. A multi-task learn-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "sor methods.\nIn IEEE Conf. and Workshops on Auto. Face"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "ing & generation framework: Valence-arousal, action units",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "and Gesture Recog., pages 1–7. IEEE, 2019. 7, 8"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "& primary expressions.\narXiv preprint arXiv:1811.07771,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[45]\nJeremie Nicolle, Kevin Bailly, and Mohamed Chetouani. Fa-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "2018. 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "cial action unit\nintensity prediction via hard multi-task met-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[31] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "ric learning for kernel regression.\nIn IEEE Conf. and Work-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "action unit\nrecognition: Aff-wild2, multi-task learning and",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "shops on Auto. Face and Gesture Recog., volume 6, pages"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "arcface. arXiv preprint arXiv:1910.04855, 2019. 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "1–6. IEEE, 2015. 8, 14"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[32] Dimitrios Kollias\nand Stefanos P Zafeiriou.\nExploiting",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[46]\nI. Ntinou, E. Sanchez, A. Bulat, M. Valstar,\nand G. Tz-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "multi-cnn features\nin cnn-rnn based dimensional\nemotion",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "imiropoulos.\nA transfer\nlearning approach to heatmap re-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "IEEE Trans. on\nrecognition on the omg in-the-wild dataset.",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "IEEE Transac-\ngression for action unit intensity estimation."
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Affect. Comput., 2020. 2",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "tions on Affective Computing, pages 1–1, 2021.\n1, 3, 8, 12,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[33]\nJean Kossaiﬁ, Antoine Toisoul, Adrian Bulat, Yannis Pana-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "14"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "gakis, Timothy M. Hospedales, and Maja Pantic.\nFactor-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[47] Desmond Ong, Zhengxuan Wu, Zhi-Xuan Tan, Marianne"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "ized higher-order cnns with an application to spatio-temporal",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Reddan,\nIsabella Kahhale, Alison Mattek, and Jamil Zaki."
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "emotion estimation.\nIn IEEE Conf. Comput. Vis. Pattern",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Modeling emotion in complex stories:\nthe stanford emo-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Recog., June 2020. 1, 5, 7, 8",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "tional narratives dataset.\nIEEE Trans. on Affect. Comput.,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[34]\nJean Kossaiﬁ, Robert Walecki, Yannis Panagakis, Jie Shen,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "2019. 2"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Maximilian Schmitt, Fabien Ringeval,\nJing Han, Vedhas",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[48] Meshia C´edric Oveneke, Isabel Gonzalez, Valentin Enescu,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Pandit, Antoine Toisoul, Bjoern W Schuller, et al. Sewa db:",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Dongmei Jiang, and Hichem Sahli. Leveraging the bayesian"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "A rich database for audio-visual emotion and sentiment re-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "ﬁltering paradigm for vision-based facial affective state es-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "search in the wild.\nIEEE Trans. Pattern Anal. Mach. Intell.,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "timation.\nIEEE Trans. on Affect. Comput., 9(4):463–477,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "2019. 1, 2, 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "2017. 2"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[35]\nF Kuhnke, L Rumberg, and J Ostermann. Two-stream aural-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[49] Meshia C´edric Oveneke, Yong Zhao, Ercheng Pei, Abel D`ıaz"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "visual affect analysis in the wild.\nIn IEEE Conf. and Work-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Berenguer, Dongmei Jiang, and Hichem Sahli. Leveraging"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "shops on Auto. Face and Gesture Recog., pages 366–371,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "the deep learning paradigm for continuous affect estimation"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "2020. 2, 7",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "from facial expressions.\nIEEE Trans. on Affect. Comput.,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[36]\nI Lawrence and Kuei Lin. A concordance correlation coefﬁ-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "2019. 2"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "cient to evaluate reproducibility. Biometrics, pages 255–268,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[50] Adam Paszke,\nSam Gross,\nSoumith Chintala, Gregory"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "1989. 5, 14",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[37] Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosen-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "ban Desmaison, Luca Antiga, and Adam Lerer. Automatic"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "baum,\nJonathan Schwarz,\nand Yee Whye Teh.\nEmpirical",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "differentiation in pytorch.\nIn Autodiff workshop - NeurIPS,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "evaluation of neural process objectives.\nIn Adv. Neural In-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "2017. 5"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "form. Process. Syst. Worksh., 2018. 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[51]\nJonathan Posner,\nJames A Russell,\nand Bradley S Peter-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[38]\nJuho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "son.\nThe circumplex model of affect: An integrative ap-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Hwang, and Yee Whye Teh. Bootstrapping neural processes.",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "proach\nto\naffective\nneuroscience,\ncognitive\ndevelopment,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Adv. Neural Inform. Process. Syst., 33, 2020. 3",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "and psychopathology.\nDevelopment and psychopathology,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[39] Dieu Linh Tran, Robert Walecki,\nStefanos Eleftheriadis,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "17(3):715, 2005. 1"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Bjorn Schuller, Maja Pantic,\net\nal.\nDeepcoder:\nSemi-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[52]\nFabien Ringeval, Bj¨orn Schuller, Michel Valstar, Nicholas"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "parametric variational autoencoders for automatic facial ac-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Cummins, Roddy Cowie, Leili Tavabi, Maximilian Schmitt,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "tion coding.\nIn Int. Conf. Comput. Vis., pages 3190–3199,",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Sina Alisamir,\nShahin Amiriparian,\nEva-Maria Messner,"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "2017. 1, 2, 8, 14",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "et al. Avec 2019 workshop and challenge: state-of-mind, de-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[40]\nIlya\nLoshchilov\nand\nFrank Hutter.\nSgdr:\nStochas-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "tecting depression with ai, and cross-cultural affect recogni-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "arXiv\npreprint\ntic\ngradient\ndescent with warm restarts.",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "tion. In Proceedings of the 9th International on Audio/Visual"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "arXiv:1608.03983, 2016. 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "Emotion Challenge and Workshop, pages 3–12, 2019. 5, 14"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[41] Brais Martinez, Michel F Valstar, Bihan Jiang,\nand Maja",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[53] Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic. Ker-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Pantic. Automatic analysis of facial actions: A survey. IEEE",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "nel conditional ordinal random ﬁelds for temporal segmenta-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Trans. on Affect. Comput., 2017. 1, 2",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "tion of facial action units.\nIn Eur. Conf. Comput. Vis., pages"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "[42]\nS Mohammad Mavadati, Mohammad H Mahoor, Kevin",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "260–269. Springer, 2012. 1, 2"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Bartlett, Philip Trinh, and Jeffrey F Cohn. Disfa: A sponta-",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "[54] Ognjen Rudovic, Vladimir Pavlovic, and Maja Pantic. Multi-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "neous facial action intensity database. IEEE Trans. on Affect.",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "output\nlaplacian dynamic ordinal\nregression for\nfacial ex-"
        },
        {
          "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction": "Comput., 4(2):151–160, 2013. 1, 2, 5",
          "[43] Hongying Meng,\nNadia\nBianchi-Berthouze,\nYangdong": "pression recognition and intensity estimation. In IEEE Conf."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "1, 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "ian Q Weinberger, and Andrew Gordon Wilson. Exact gaus-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[55] Georgia Sandbach,\nStefanos Zafeiriou,\nand Maja Pantic.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "sian processes on a million data points.\nIn Adv. Neural In-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Markov random ﬁeld structures for facial action unit\ninten-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "form. Process. Syst., pages 14648–14659, 2019. 2"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "sity estimation.\nIn Int. Conf. Comput. Vis. Worksh., pages",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[70]\nShangfei Wang, Zhuangqiang Zheng, Shi Yin, Jiajia Yang,"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "738–745, 2013. 1, 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "and Qiang Ji. A novel dynamic model capturing spatial and"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "temporal patterns for facial expression analysis. IEEE Trans."
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[56] Vidhyasaharan Sethu, Emily Mower Provost,\nJulien Epps,",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Pattern Anal. Mach. Intell., 2019. 2"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Carlos Busso, Nicholas Cummins, and Shrikanth Narayanan.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "arXiv\nThe\nambiguous world of\nemotion representation.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[71]\nShu-Hui Wang and Chiou-Ting Hsu. Ast-net: An attribute-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "preprint arXiv:1909.00360, 2019. 1",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "based\nsiamese\ntemporal\nnetwork\nfor\nreal-time\nemotion"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "recognition.\nIn Brit. Mach. Vis. Conf., 2017. 1, 2"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[57]\nP.E. Shrout and J.L. Fleiss.\nIntraclass correlations: uses in",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[72] Christopher KI Williams\nand Carl\nEdward Rasmussen."
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "assessing rater reliability.\nPsychological bulletin, 1979.\n5,",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Gaussian processes for machine learning, volume 2. MIT"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "14",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "press Cambridge, MA, 2006. 2"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[58] Gautam Singh,\nJaesik Yoon, Youngsung Son, and Sungjin",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[73]\nJing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Fan-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Inform.\nAhn.\nSequential neural processes.\nIn Adv. Neural",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "face: a simple orthogonal\nimprovement\nto deep face recog-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Process. Syst., pages 10254–10264, 2019. 3",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "nition.\nIn AAAI Conference on Artiﬁcial Intelligence, 2020."
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[59] Mani Kumar Tellamekala and Michel Valstar.\nTemporally",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "3, 12"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "coherent visual representations for dimensional affect recog-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[74]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "nition.\nIn 2019 8th International Conference on Affective",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Computing and Intelligent\nInteraction (ACII), pages 1–7.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "IEEE, 2019. 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "In IEEE Conf. Comput. Vis. Pattern Recog. Worksh.\nIEEE,"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[60] Antoine Toisoul, Jean Kossaiﬁ, Adrian Bulat, Georgios Tz-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "2017. 1, 5"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "imiropoulos, and Maja Pantic. Estimation of continuous va-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[75] Xing Zhang, Lijun Yin,\nJeffrey F Cohn, Shaun Canavan,"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "lence and arousal levels from faces in naturalistic conditions.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Michael Reale, Andy Horowitz, Peng Liu, and Jeffrey M Gi-"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Nature Machine Intelligence, 2021. 3, 12",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "rard. Bp4d-spontaneous:\na high-resolution spontaneous 3d"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[61] Martin Trapp, Robert Peharz, Franz Pernkopf, and Carl Ed-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Image and Vision Com-\ndynamic facial expression database."
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "ward Rasmussen. Deep structured mixtures of gaussian pro-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "puting, 32(10):692–706, 2014. 1, 2, 5"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "cesses. In International Conference on Artiﬁcial Intelligence",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[76] Y Zhang, R Huang, J Zeng, and S Shan. M3f: Multi-modal"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "and Statistics, pages 2251–2261. PMLR, 2020. 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "continuous valence-arousal estimation in the wild.\nIn IEEE"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[62] Volker Tresp. Mixtures of gaussian processes. In Adv. Neural",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Conf. and Workshops on Auto. Face and Gesture Recog.,"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Inform. Process. Syst., pages 654–660, 2001. 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "pages 617–621. 2"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[63] Michel Valstar and Maja Pantic. Fully automatic facial action",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "[77] Yong Zhang, Haiyong Jiang, Baoyuan Wu, Yanbo Fan, and"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "unit detection and temporal analysis. In IEEE Conf. Comput.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "Qiang Ji. Context-aware feature and label fusion for facial"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Vis. Pattern Recog. Worksh., pages 149–149. IEEE, 2006. 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "action unit\nintensity estimation with partially labeled data."
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[64] Michel Franc¸ois Valstar.\nTiming is everything: A spatio-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": "In Int. Conf. Comput. Vis., pages 733–742, 2019. 1, 2"
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "PhD\ntemporal approach to the analysis of\nfacial actions.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Dissertation, 2008. 1, 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[65] Michel F Valstar, Timur Almaev,\nJeffrey M Girard, Gary",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "McKeown, Marc Mehu, Lijun Yin, Maja Pantic,\nand Jef-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "frey F Cohn. Fera 2015-second facial expression recognition",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "and analysis challenge.\nIn IEEE Conf. and Workshops on",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Auto. Face and Gesture Recog., volume 6, pages 1–8. IEEE,",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "2015. 5, 14",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Polosukhin. Attention is all you need. In Adv. Neural Inform.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Process. Syst., pages 5998–6008, 2017. 2, 5, 7, 8, 14",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[67] Robert Walecki, Vladimir Pavlovic, Bj¨orn Schuller, Maja",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Pantic, et al. Deep structured learning for facial action unit",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "intensity estimation.\nIn IEEE Conf. Comput. Vis. Pattern",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Recog., pages 3405–3414, 2017. 1, 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "[68] Robert Walecki, Ognjen Rudovic, Vladimir Pavlovic,\nand",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Maja Pantic. Variable-state latent conditional random ﬁelds",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "for\nfacial expression recognition and action unit detection.",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "In IEEE Conf. and Workshops on Auto. Face and Gesture",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        },
        {
          "Comput. Vis. Pattern Recog., pages 2634–2641. IEEE, 2012.": "Recog., volume 1, pages 1–8. IEEE, 2015. 2",
          "[69] Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kil-": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "of 4 ConvBlocks, each followed by a max pooling layer,"
        },
        {
          "A. Backbone": "This appendix is used to describe the backbone structure",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "that produce a tensor of 128 × 4 × 4. To form the features"
        },
        {
          "A. Backbone": "and training,\nleft out of the main document due to lack of",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "that will be used as input to our AP network, we further\nxt"
        },
        {
          "A. Backbone": "space.\nIt also includes a formal deﬁnition of the main per-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "downsample that\ntensor through an average pooling opera-"
        },
        {
          "A. Backbone": "formance metrics used in the paper, as well as the Mean",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "tion with a 2 × 2 kernel. The 128 × 2 × 2 output is ﬂattened"
        },
        {
          "A. Backbone": "Squared Error\nreported for\nthe Action Unit\nintensity esti-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "to form the 512-d feature vector xt."
        },
        {
          "A. Backbone": "mation task. We ﬁrst describe the architecture of the back-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "The Task speciﬁc head for Valence and Arousal\nis com-"
        },
        {
          "A. Backbone": "bone (Sec. A.1), and then the training details for each of the",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "posed of four independent Conv2d layers, each with 4 × 4"
        },
        {
          "A. Backbone": "databases used in the paper (Sec. A.2). Sec. B and Sec. C",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "ﬁlters (i.e.\nequal\nto the spatial resolution of the input\nten-"
        },
        {
          "A. Backbone": "are devoted to describing the performance metrics used in",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "sor). The ﬁrst Conv2d layer\nis the corresponding Valence"
        },
        {
          "A. Backbone": "the paper and to reporting the MSE results on DISFA and",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "and Arousal classiﬁer W mentioned in the main document."
        },
        {
          "A. Backbone": "BP4D, respectively.",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "The output of\nthis layer\nis a 2-d vector ˆyt, corresponding"
        },
        {
          "A. Backbone": "A.1. Architecture",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "to the values of Valence and Arousal, respectively.\nIn order"
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "to boost the performance of the network for the task of pre-"
        },
        {
          "A. Backbone": "The structure of the backbone for both Valence and Arousal",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "dicting the continuous values of Valence and Arousal (ˆy),"
        },
        {
          "A. Backbone": "and Action Unit recognition is depicted in Fig. 5. Note that",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "we approach the backbone training in a Multi-task manner"
        },
        {
          "A. Backbone": "both are depicted in the same ﬁgure for the sake of clarity,",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "(see below), where the goal is to also classify the basic (dis-"
        },
        {
          "A. Backbone": "although the corresponding subnetworks consisting of\nthe",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "crete) emotion, as well as the bin where both Valence and"
        },
        {
          "A. Backbone": "Emotion Head or\nthe Action Units Head are trained inde-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "Arousal would lie in a discretised space. For the basic emo-"
        },
        {
          "A. Backbone": "pendently using the task speciﬁc datasets. Both networks",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "tion (happiness, sadness,\nfear, anger, surprise, disgust and"
        },
        {
          "A. Backbone": "share a common module,\nreferred to as Face Alignment",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "neutral), we include a second Conv2d which outputs\nthe"
        },
        {
          "A. Backbone": "Module, which is pre-trained for\nthe task of\nfacial\nland-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "logits corresponding to each of the 7 target classes. For the"
        },
        {
          "A. Backbone": "mark localisation, and kept frozen for the subsequent train-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "discretised Valence (ˆv) and Arousal (ˆa), we use two Conv2d"
        },
        {
          "A. Backbone": "ing steps.\nFor both Valence and Arousal and Action Unit",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "layers with 20 outputs each,\ni.e. we discretise the continu-"
        },
        {
          "A. Backbone": "estimation,\nthe backbone is decomposed into three main",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "ous space in 20 bins, and we treat the task of predicting the"
        },
        {
          "A. Backbone": "components, namely a) Face Alignment Module, b) Task-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "corresponding bin as a classiﬁcation task (see below). Note"
        },
        {
          "A. Backbone": "speciﬁc Feature Module, and c) Task-speciﬁc Head.",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "that these extra heads, as well as the emotion head, are used"
        },
        {
          "A. Backbone": "The Face Alignment Module is a lightweight version of",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "to reinforce the learning of the regression head tasked with"
        },
        {
          "A. Backbone": "the Face Alignment Network of\n[3].\nIt\nstarts with a 2d",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "predicting ˆy. Once the network is trained,\nthe heads corre-"
        },
        {
          "A. Backbone": "convolutional\nlayer\n(referred to as Conv2d) and a set of 4",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "sponding to the discrete emotion and the discretised Valence"
        },
        {
          "A. Backbone": "convolutional blocks (ConvBlock, depicted in Fig. 6)\nthat",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "and Arousal are removed from the backbone."
        },
        {
          "A. Backbone": "bring down the resolution of\nthe input\nimage from 256 to",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "64 and the number of channels from 3 to 128. This set of",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "speciﬁc head for Action Unit\nintensity\nesti-\nThe Task"
        },
        {
          "A. Backbone": "ConvBlocks is followed by an Hourglass, a four\nlayer set",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "mation is also composed of 4 ConvBlocks as for\nthe Va-"
        },
        {
          "A. Backbone": "of 128-channel ConvBlocks with skip connections, that ag-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "lence and Arousal head.\nThe output\nfeatures, a tensor of"
        },
        {
          "A. Backbone": "gregate the features at different spatial scales.\nThe Hour-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "128 × 4 × 4 are also spatially downsampled with average"
        },
        {
          "A. Backbone": "glass is followed by another ConvBlock and two Conv2d",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "pooling and ﬂattened to form the input\nfeatures to the AP"
        },
        {
          "A. Backbone": "layers that produce a set of 68 Heatmaps, corresponding to",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "xt. The Action Units classiﬁer W is a Conv2d with a 4 × 4"
        },
        {
          "A. Backbone": "the position of\nthe facial\nlandmarks.\nIn this paper,\nrather",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "ﬁlter that maps the 128 × 4 × 4 into either the 5 or 12 target"
        },
        {
          "A. Backbone": "than using the facial\nlandmarks to register the face, we di-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "",
          "The Task speciﬁc Feature Module consists of a mere set": "AUs, for BP4D and DISFA, respectively."
        },
        {
          "A. Backbone": "rectly concatenate the produced features at both an early and",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "late stage of the network with the Heatmaps. The output is",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "then a 128 + 128 + 68 tensor of 64 × 64,\nresulting from",
          "The Task speciﬁc Feature Module consists of a mere set": "A.2. Training"
        },
        {
          "A. Backbone": "concatenating the features computed after\nthe fourth Con-",
          "The Task speciﬁc Feature Module consists of a mere set": ""
        },
        {
          "A. Backbone": "vBlock, the features computed after the last ConvBlock, and",
          "The Task speciﬁc Feature Module consists of a mere set": "Data processing\nThe faces are ﬁrst cropped according to"
        },
        {
          "A. Backbone": "the produced Heatmaps. This way,\nthe Heatmaps help the",
          "The Task speciﬁc Feature Module consists of a mere set": "a face bounding box, provided by the off-the-shelf face de-"
        },
        {
          "A. Backbone": "subsequent network locally attend to the extracted coarse",
          "The Task speciﬁc Feature Module consists of a mere set": "tector RetinaFace [9]. Given that the ﬁrst block of the back-"
        },
        {
          "A. Backbone": "and ﬁne features [46, 60, 73]. The beneﬁts of this approach",
          "The Task speciﬁc Feature Module consists of a mere set": "bone is a Face Alignment Network that\nis used to provide"
        },
        {
          "A. Backbone": "are twofold: a) it dispenses with the need of registering the",
          "The Task speciﬁc Feature Module consists of a mere set": "the features to the subsequent networks, no face registration"
        },
        {
          "A. Backbone": "faces according to detected landmarks, and b) because of a)",
          "The Task speciﬁc Feature Module consists of a mere set": "step is applied. During training,\nfor\nimage augmentation"
        },
        {
          "A. Backbone": "we can directly use the features from the Face Alignment",
          "The Task speciﬁc Feature Module consists of a mere set": "we applied random cropping (224×224), random horizon-"
        },
        {
          "A. Backbone": "Network and have shallower networks in the front-end for",
          "The Task speciﬁc Feature Module consists of a mere set": "tal ﬂipping, random rotation(−20◦ to +20◦), color jittering,"
        },
        {
          "A. Backbone": "the subsequent tasks.",
          "The Task speciﬁc Feature Module consists of a mere set": "and random gray scaling operations."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Head and Action Units Head are depicted together, despite these being different networks, trained separately. The grey modules represent"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "2-d convolutions (Conv2d), whereas the blue blocks represent Convolutional Blocks (ConvBlocks), described in Fig. 5. The ∗64 inscribed"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "in the ﬁrst ConvBlock corresponds to a slightly different conﬁguration that uses a skip connection to upsample the number of channels"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "from 64 to 128.\nThe backbone includes a Face Alignment Network, an Hourglass-like architecture that\ntakes the input\nimage I, and"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "produces a set 68 Heatmaps corresponding to the position of the facial\nlandmarks. The Hourglass comprises four layers of ConvBlocks"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "with downsampling, and skip connections (for\nthe sake of clarity we illustrate three layers, where each smaller block corresponds to"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "halving the spatial resolution). As shown in Fig. 6, our ConvBlocks are of 128 channels,\nrather than the original 256 used in [3]. The"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Face Alignment Network is pre-trained and kept frozen, and returns a set of features resulting from concatenating the output of the last"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "ConvBlock before the Hourglass,\nthe output of the last ConvBlock of the network, and the produced Heatmaps. Then,\nthe Emotion and"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Action Unit heads follow for each corresponding task. Both have a similar Feature Extraction Module, composed of ×4 ConvBlocks"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "followed by Average Pooling. The output of this module is a 128 × 4 × 4 tensor, which is used as input to the corresponding classiﬁers, as"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "well as to compute the ﬁnal feature representation xt that will be used along with ˆyt as input to our proposed AP."
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Face Alignment Network\nThe Face Alignment Module\nwhere Lmse = (cid:107)ˆy − y(cid:107) is the standard MSE loss for Va-"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "was\ntrained on the 300W-LP dataset\n[3] using standard"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "is\nlence and Arousal, Lccc = 1 − CCC(ˆyv,yv)+CCC(ˆya,ya)\n2"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Heatmap Regression, and was kept frozen afterwards.\nthe CCC score between the predicted Valence and Arousal"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "is the\nvalues and corresponding ground-truth, Lxent−emo"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "standard cross\nentropy loss between the predicted emo-"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Valence and Arousal\nFor Valence and Arousal,\nthe net-"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "tion ˆe and the corresponding ground-truth e. We deﬁne"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "work is trained in a Multi-task way. Let ˆy = (ˆyv, ˆya) be"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "the cross\nLxent−va = Lxent−v + Lxent−a, with Lxent−v"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "the Valence and Arousal prediction, ˆe ∈ R7 be the output"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "entropy loss between the 20-d output of\nthe Valence head"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "of the discrete emotion layer, and ˆv ∈ R20 and ˆa ∈ R20 the"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "and the corresponding ground-truth bin, and equivalently"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "output of the Valence and Arousal classes, respectively. We"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Lxent−a for Arousal. The ground-truth bin results from uni-"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "denote by y and e the corresponding Valence and Arousal"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "formly discretising the Valence and Arousal spaces, which"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "and Emotion ground-truth values. The loss is deﬁned as:"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "lie within the [−1, 1] space, into 20 bins each."
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "The values of the loss weights are all set\nto 1 except for"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "L =λmseLmse + λcccLccc"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "the MSE loss that is set to λmse = 0.5. For both SEWA and"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "(2)\n+ λxent−emoLxent−emo\nAffWild2,\nthe training is performed for 20 epochs, using"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "Adam with learning rate 0.0001,\n(β1, β2) = (0.9, 0.999)"
        },
        {
          "Figure 5. Architecture of the Backbone used in our AP pipeline described in the main document. For the sake of clarity, both the Emotion": "+ λxent−vaLxent−va"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 7: Results on the test partition of BP4D dataset (in MSE",
      "data": [
        {
          "AU": "VGP-AE [12]",
          "1": "0.51",
          "2": "0.32",
          "4": "1.13",
          "5": "0.08",
          "6": "0.56",
          "9": "0.31",
          "12": "0.47",
          "15": "0.20",
          "17": "0.28",
          "20": "0.16",
          "25": "0.49",
          "26": "0.44",
          "Avg.": "0.41"
        },
        {
          "AU": "2DC [39]",
          "1": "0.32",
          "2": "0.39",
          "4": "0.53",
          "5": "0.26",
          "6": "0.43",
          "9": "0.30",
          "12": "0.25",
          "15": "0.27",
          "17": "0.61",
          "20": "0.18",
          "25": "0.37",
          "26": "0.55",
          "Avg.": "0.37"
        },
        {
          "AU": "HR [46]",
          "1": "0.41",
          "2": "0.37",
          "4": "0.70",
          "5": "0.08",
          "6": "0.44",
          "9": "0.30",
          "12": "0.29",
          "15": "0.14",
          "17": "0.26",
          "20": "0.16",
          "25": "0.24",
          "26": "0.39",
          "Avg.": "0.32"
        },
        {
          "AU": "Ours backbone",
          "1": "0.93",
          "2": "0.90",
          "4": "0.51",
          "5": "0.04",
          "6": "0.44",
          "9": "0.19",
          "12": "0.30",
          "15": "0.13",
          "17": "0.21",
          "20": "0.17",
          "25": "0.23",
          "26": "0.29",
          "Avg.": "0.36"
        },
        {
          "AU": "BiGRU [4]†",
          "1": "0.85",
          "2": "0.79",
          "4": "0.48",
          "5": "0.06",
          "6": "0.47",
          "9": "0.19",
          "12": "0.34",
          "15": "0.18",
          "17": "0.23",
          "20": "0.21",
          "25": "0.30",
          "26": "0.40",
          "Avg.": "0.37"
        },
        {
          "AU": "Self-Attn [66]†",
          "1": "0.76",
          "2": "0.71",
          "4": "0.52",
          "5": "0.04",
          "6": "0.42",
          "9": "0.17",
          "12": "0.35",
          "15": "0.14",
          "17": "0.21",
          "20": "0.19",
          "25": "0.28",
          "26": "0.36",
          "Avg.": "0.34"
        },
        {
          "AU": "Ours AP",
          "1": "0.68",
          "2": "0.59",
          "4": "0.40",
          "5": "0.03",
          "6": "0.49",
          "9": "0.15",
          "12": "0.26",
          "15": "0.13",
          "17": "0.22",
          "20": "0.20",
          "25": "0.35",
          "26": "0.17",
          "Avg.": "0.30"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 7: Results on the test partition of BP4D dataset (in MSE",
      "data": [
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "is selected based on the ICC score on the validation set."
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "For BP4D,\nthe model\nis\ntrained\nusing\nthe\nofﬁcial"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "train/validation/test partitions.\nFor DISFA,\nthe model\nis"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "trained using the\nthree-fold cross validation method de-"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "scribed in the main document, using exactly the same gen-"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "erated partitions."
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "B. Performance Metrics"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "For Valence and Arousal, we report\nthe Concordance Cor-"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "relation Coefﬁcient [36], which is used to rank participants"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "in the AVEC Challenge series [52].\nIt\nis a global measure"
        },
        {
          "0.15\n0.13\n0.17\n0.30\n0.26\n0.22\n0.20\n0.35": "of both correlation and proximity, and is deﬁned as:"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 7: Results on the test partition of BP4D dataset (in MSE",
      "data": [
        {
          "relation Coefﬁcient [36], which is used to rank participants": "in the AVEC Challenge series [52].\nIt\nis a global measure"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "of both correlation and proximity, and is deﬁned as:"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "2σyσˆyρyˆy"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "CCC(y, ˆy) =\n(3)"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "σ2\ny + σ2\ny + (µy − µˆy)2 ,"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "where µ, σ, and ρ refer\nto the mean value,\n(co-)variance,"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "and Pearson Correlation Coefﬁcient, respectively."
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "For Action Unit\nintensity, we follow the standard rank-"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "ing criteria used in FERA challenges[65],\nand we report"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "the Intra Class Correlation (ICC [57]).\nFor an AU j with"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "ground-truth labels {yj\nthe\ni=1, and predictions {˜yj\ni=1,\ni }N\ni }N"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "ICC score is deﬁned as ICC j = W j −Sj"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "W j +Sj , with W j ="
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "(cid:16)"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "(cid:80)"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "1N\n(yj\n, Sj = (cid:80)\ni − ˆyj)2 + (˜yj\ni − ˆyj)2(cid:17)\ni − ˜yj\ni )2,\ni\ni(yj"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "(cid:80)"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "and ˆyj = 1"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "i ).\ni(yj\n2N"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "C. Mean Squared Error Results"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "The additional Mean Squared Error results for DISFA and"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "BP4D are reported in Table 6 and Table 7."
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "AU\n6\n10\n12\n14\n17\nAvg."
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "ISIR [45]\n0.83\n0.80\n0.62\n1.14\n0.84\n0.85"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "0.68\n0.80\n0.98\nHR [46]\n0.79\n0.64\n0.78"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "Ours backbone\n0.80\n0.87\n0.74\n1.23\n0.89\n0.90"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "BiGRU [4]†\n0.79\n0.85\n0.76\n1.19\n0.78\n0.87"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "Self-Attn\n[66]†\n0.82\n0.88\n0.70\n1.22\n0.80\n0.88"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "0.60\n0.57\n0.77\nOurs AP\n0.72\n0.84\n1.13"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "Table 7. Results on the test partition of BP4D dataset\n(in MSE"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": "values) † denotes in-house evaluation"
        },
        {
          "relation Coefﬁcient [36], which is used to rank participants": ""
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "Tadas Baltrušaitis",
        "Marwa Mahmoud",
        "Peter Robinson"
      ],
      "year": "2015",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "2",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "Lisa Feldman",
        "Ralph Adolphs",
        "Stacy Marsella",
        "Aleix Martinez",
        "Seth Pollak"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "3",
      "title": "How far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)",
      "authors": [
        "Adrian Bulat",
        "Georgios Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merriënboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "5",
      "title": "A recurrent latent variable model for sequential data",
      "authors": [
        "Junyoung Chung",
        "Kyle Kastner",
        "Laurent Dinh",
        "Kratarth Goel",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Adv"
    },
    {
      "citation_id": "6",
      "title": "Pairwise conditional random forests for facial expression recognition",
      "authors": [
        "Arnaud Dapogny",
        "Kevin Bailly",
        "Severine Dubuisson"
      ],
      "year": "2015",
      "venue": "Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "7",
      "title": "La prévision: ses lois logiques, ses sources subjectives",
      "authors": [
        "Bruno De"
      ],
      "year": "1937",
      "venue": "Annales de l'institut Henri Poincaré"
    },
    {
      "citation_id": "8",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "9",
      "title": "Singlestage dense face localisation in the wild",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Yuxiang Zhou",
        "Jinke Yu",
        "Irene Kotsia",
        "Stefanos Zafeiriou",
        "Retinaface"
      ],
      "year": "2019",
      "venue": "Singlestage dense face localisation in the wild",
      "arxiv": "arXiv:1905.00641"
    },
    {
      "citation_id": "10",
      "title": "Spatio-temporal encoder-decoder fully convolutional network for video-based dimensional emotion recognition",
      "authors": [
        "Zhengyin Du",
        "Suowei Wu",
        "Di Huang",
        "Weixin Li",
        "Yunhong Wang"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "11",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1977",
      "venue": "Facial action coding system"
    },
    {
      "citation_id": "12",
      "title": "Variational gaussian process autoencoder for ordinal prediction of facial action units",
      "authors": [
        "Stefanos Eleftheriadis",
        "Ognjen Rudovic",
        "Marc Peter Deisenroth",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "ACCV"
    },
    {
      "citation_id": "13",
      "title": "Gaussian process domain experts for modeling of facial affect",
      "authors": [
        "Stefanos Eleftheriadis",
        "Ognjen Rudovic",
        "Marc Peter Deisenroth",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "14",
      "title": "Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition",
      "authors": [
        "Stefanos Eleftheriadis",
        "Ognjen Rudovic",
        "Maja Pantic"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "15",
      "title": "Metalearning stationary stochastic process prediction with convolutional neural processes",
      "authors": [
        "Andrew Yk Foong",
        "P Wessel",
        "Jonathan Bruinsma",
        "Yann Gordon",
        "James Dubois",
        "Richard Requeima",
        "Turner"
      ],
      "year": "2020",
      "venue": "In Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "16",
      "title": "Conditional neural processes",
      "authors": [
        "Marta Garnelo",
        "Dan Rosenbaum",
        "Christopher Maddison",
        "Tiago Ramalho",
        "David Saxton",
        "Murray Shanahan",
        "Yee Teh",
        "Danilo Rezende",
        "Eslami Ali"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Neural processes",
      "authors": [
        "Marta Garnelo",
        "Jonathan Schwarz",
        "Dan Rosenbaum",
        "Fabio Viola",
        "Danilo Rezende",
        "S Ali Eslami",
        "Yee Teh"
      ],
      "year": "2018",
      "venue": "ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models"
    },
    {
      "citation_id": "18",
      "title": "Convolutional conditional neural processes",
      "authors": [
        "Jonathan Gordon",
        "P Wessel",
        "Andrew Yk Bruinsma",
        "James Foong",
        "Yann Requeima",
        "Richard Dubois",
        "Turner"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "End-to-end continuous emotion recognition from video using 3d convlstm networks",
      "authors": [
        "Jian Huang",
        "Ya Li",
        "Jianhua Tao",
        "Zheng Lian",
        "Jiangyan Yi"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Efficient modeling of long temporal contexts for continuous emotion recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zhen Lian",
        "Mingyue Niu"
      ],
      "year": "2019",
      "venue": "In Int. Conf. Affec. Comput. and Intel. Inter"
    },
    {
      "citation_id": "21",
      "title": "Deep learning the dynamic appearance and shape of facial action units",
      "authors": [
        "Shashank Jaiswal",
        "Michel Valstar"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conf. on Appl. of Comput. Vis"
    },
    {
      "citation_id": "22",
      "title": "What uncertainties do we need in bayesian deep learning for computer vision?",
      "authors": [
        "Alex Kendall",
        "Yarin Gal ; In",
        "I Guyon",
        "U Luxburg",
        "S Bengio",
        "H Wallach",
        "R Fergus",
        "S Vishwanathan",
        "R Garnett"
      ],
      "year": "2017",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "23",
      "title": "Attentive neural processes",
      "authors": [
        "Hyunjik Kim",
        "Andriy Mnih",
        "Jonathan Schwarz",
        "Marta Garnelo",
        "Ali Eslami",
        "Dan Rosenbaum",
        "Oriol Vinyals",
        "Yee Teh"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "24",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "25",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "26",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "Dimitrios Kollias",
        "A Mihalis",
        "Irene Nicolaou",
        "Guoying Kotsia",
        "Stefanos Zhao",
        "Zafeiriou"
      ],
      "year": "2005",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog. Worksh"
    },
    {
      "citation_id": "27",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Dimitrios Kollias",
        "Attila Schulc",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw 2020 competition",
      "arxiv": "arXiv:2001.11409"
    },
    {
      "citation_id": "28",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2005",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "29",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2005",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "30",
      "title": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "31",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "32",
      "title": "Exploiting multi-cnn features in cnn-rnn based dimensional emotion recognition on the omg in-the-wild dataset",
      "authors": [
        "Dimitrios Kollias",
        "P Stefanos",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "33",
      "title": "Factorized higher-order cnns with an application to spatio-temporal emotion estimation",
      "authors": [
        "Jean Kossaifi",
        "Antoine Toisoul",
        "Adrian Bulat",
        "Yannis Panagakis",
        "Timothy Hospedales",
        "Maja Pantic"
      ],
      "year": "2008",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "34",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "Bjoern Schuller"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "35",
      "title": "Two-stream auralvisual affect analysis in the wild",
      "authors": [
        "Kuhnke",
        "Rumberg",
        "Ostermann"
      ],
      "year": "2020",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "36",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "Kuei Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "37",
      "title": "Empirical evaluation of neural process objectives",
      "authors": [
        "Anh Tuan",
        "Hyunjik Le",
        "Marta Kim",
        "Dan Garnelo",
        "Jonathan Rosenbaum",
        "Yee Schwarz",
        "Teh"
      ],
      "year": "2018",
      "venue": "Adv. Neural Inform. Process. Syst. Worksh"
    },
    {
      "citation_id": "38",
      "title": "Bootstrapping neural processes",
      "authors": [
        "Juho Lee",
        "Yoonho Lee",
        "Jungtaek Kim",
        "Eunho Yang",
        "Sung Hwang",
        "Yee Teh"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "39",
      "title": "Deepcoder: Semiparametric variational autoencoders for automatic facial action coding",
      "authors": [
        "Linh Dieu",
        "Robert Tran",
        "Stefanos Walecki",
        "Bjorn Eleftheriadis",
        "Maja Schuller",
        "Pantic"
      ],
      "year": "2017",
      "venue": "Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "40",
      "title": "Sgdr: Stochastic gradient descent with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2016",
      "venue": "Sgdr: Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "41",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "Brais Martinez",
        "Michel Valstar",
        "Bihan Jiang",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "42",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Kevin Mahoor",
        "Philip Bartlett",
        "Jeffrey Trinh",
        "Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "43",
      "title": "Time-delay neural network for continuous emotional dimension prediction from facial expression sequences",
      "authors": [
        "Hongying Meng",
        "Nadia Bianchi-Berthouze",
        "Yangdong Deng",
        "Jinkuang Cheng",
        "John Cosmas"
      ],
      "year": "2015",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "44",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "Anna Mitenkova",
        "Jean Kossaifi",
        "Yannis Panagakis",
        "Maja Pantic"
      ],
      "year": "2019",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "45",
      "title": "Facial action unit intensity prediction via hard multi-task metric learning for kernel regression",
      "authors": [
        "Jeremie Nicolle",
        "Kevin Bailly",
        "Mohamed Chetouani"
      ],
      "year": "2015",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "46",
      "title": "A transfer learning approach to heatmap regression for action unit intensity estimation",
      "authors": [
        "I Ntinou",
        "E Sanchez",
        "A Bulat",
        "M Valstar",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Modeling emotion in complex stories: the stanford emotional narratives dataset",
      "authors": [
        "Desmond Ong",
        "Zhengxuan Wu",
        "Zhi-Xuan Tan",
        "Marianne Reddan",
        "Isabella Kahhale",
        "Alison Mattek",
        "Jamil Zaki"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "48",
      "title": "Leveraging the bayesian filtering paradigm for vision-based facial affective state estimation",
      "authors": [
        "Meshia Cédric Oveneke",
        "Isabel Gonzalez",
        "Valentin Enescu",
        "Dongmei Jiang",
        "Hichem Sahli"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "49",
      "title": "Leveraging the deep learning paradigm for continuous affect estimation from facial expressions",
      "authors": [
        "Meshia Cédric Oveneke",
        "Yong Zhao",
        "Ercheng Pei",
        "Abel Berenguer",
        "Dongmei Jiang",
        "Hichem Sahli"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "50",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Soumith Chintala",
        "Gregory Chanan",
        "Edward Yang",
        "Zachary Devito",
        "Zeming Lin",
        "Alban Desmaison",
        "Luca Antiga",
        "Adam Lerer"
      ],
      "year": "2017",
      "venue": "Autodiff workshop -NeurIPS"
    },
    {
      "citation_id": "51",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "Jonathan Posner",
        "James Russell",
        "Bradley Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "52",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Leili Tavabi",
        "Maximilian Schmitt",
        "Sina Alisamir",
        "Shahin Amiriparian",
        "Eva-Maria Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "53",
      "title": "Kernel conditional ordinal random fields for temporal segmentation of facial action units",
      "authors": [
        "Ognjen Rudovic",
        "Vladimir Pavlovic",
        "Maja Pantic"
      ],
      "year": "2012",
      "venue": "Eur. Conf. Comput. Vis"
    },
    {
      "citation_id": "54",
      "title": "Multioutput laplacian dynamic ordinal regression for facial expression recognition and intensity estimation",
      "authors": [
        "Ognjen Rudovic",
        "Vladimir Pavlovic",
        "Maja Pantic"
      ],
      "venue": "IEEE Conf"
    },
    {
      "citation_id": "55",
      "title": "Pattern Recog",
      "authors": [
        "Comput",
        "Vis"
      ],
      "year": "2012",
      "venue": "Pattern Recog"
    },
    {
      "citation_id": "56",
      "title": "Markov random field structures for facial action unit intensity estimation",
      "authors": [
        "Georgia Sandbach",
        "Stefanos Zafeiriou",
        "Maja Pantic"
      ],
      "year": "2013",
      "venue": "Int. Conf. Comput. Vis. Worksh"
    },
    {
      "citation_id": "57",
      "title": "The ambiguous world of emotion representation",
      "authors": [
        "Vidhyasaharan Sethu",
        "Emily Provost",
        "Julien Epps",
        "Carlos Busso",
        "Nicholas Cummins",
        "Shrikanth Narayanan"
      ],
      "year": "2019",
      "venue": "The ambiguous world of emotion representation",
      "arxiv": "arXiv:1909.00360"
    },
    {
      "citation_id": "58",
      "title": "Intraclass correlations: uses in assessing rater reliability",
      "authors": [
        "P Shrout",
        "J Fleiss"
      ],
      "year": "1979",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "59",
      "title": "Sequential neural processes",
      "authors": [
        "Gautam Singh",
        "Jaesik Yoon",
        "Youngsung Son",
        "Sungjin Ahn"
      ],
      "year": "2019",
      "venue": "In Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "60",
      "title": "Temporally coherent visual representations for dimensional affect recognition",
      "authors": [
        "Mani Kumar",
        "Michel Valstar"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "61",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "62",
      "title": "Deep structured mixtures of gaussian processes",
      "authors": [
        "Martin Trapp",
        "Robert Peharz",
        "Franz Pernkopf",
        "Carl Rasmussen"
      ],
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "63",
      "title": "Mixtures of gaussian processes",
      "authors": [
        "Tresp Volker"
      ],
      "year": "2001",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "64",
      "title": "Fully automatic facial action unit detection and temporal analysis",
      "authors": [
        "Michel Valstar",
        "Maja Pantic"
      ],
      "year": "2006",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog. Worksh"
    },
    {
      "citation_id": "65",
      "title": "Timing is everything: A spatiotemporal approach to the analysis of facial actions",
      "authors": [
        "Michel Franc",
        "Valstar"
      ],
      "year": "2008",
      "venue": "PhD Dissertation"
    },
    {
      "citation_id": "66",
      "title": "Fera 2015-second facial expression recognition and analysis challenge",
      "authors": [
        "Timur Michel F Valstar",
        "Jeffrey Almaev",
        "Gary Girard",
        "Marc Mckeown",
        "Lijun Mehu",
        "Maja Yin",
        "Jeffrey Pantic",
        "Cohn"
      ],
      "year": "2015",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "67",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Adv. Neural Inform. Process. Syst"
    },
    {
      "citation_id": "68",
      "title": "Deep structured learning for facial action unit intensity estimation",
      "authors": [
        "Robert Walecki",
        "Vladimir Pavlovic",
        "Björn Schuller",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog"
    },
    {
      "citation_id": "69",
      "title": "Variable-state latent conditional random fields for facial expression recognition and action unit detection",
      "authors": [
        "Robert Walecki",
        "Ognjen Rudovic",
        "Vladimir Pavlovic",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "70",
      "title": "Exact gaussian processes on a million data points",
      "authors": [
        "Ke Wang",
        "Geoff Pleiss",
        "Jacob Gardner",
        "Stephen Tyree",
        "Kilian Weinberger",
        "Andrew Gordon"
      ],
      "year": "2019",
      "venue": "Adv. Neural Inform. Process. Syst."
    },
    {
      "citation_id": "71",
      "title": "A novel dynamic model capturing spatial and temporal patterns for facial expression analysis",
      "authors": [
        "Shangfei Wang",
        "Zhuangqiang Zheng",
        "Jiajia Shi Yin",
        "Qiang Yang",
        "Ji"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "72",
      "title": "Ast-net: An attributebased siamese temporal network for real-time emotion recognition",
      "authors": [
        "Shu-Hui Wang",
        "Chiou-Ting Hsu"
      ],
      "year": "2017",
      "venue": "Ast-net: An attributebased siamese temporal network for real-time emotion recognition"
    },
    {
      "citation_id": "73",
      "title": "Gaussian processes for machine learning",
      "authors": [
        "K Christopher",
        "Carl Williams",
        "Rasmussen"
      ],
      "year": "2006",
      "venue": "Gaussian processes for machine learning"
    },
    {
      "citation_id": "74",
      "title": "Fanface: a simple orthogonal improvement to deep face recognition",
      "authors": [
        "Jing Yang",
        "Adrian Bulat",
        "Georgios Tzimiropoulos"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "75",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "IEEE Conf. Comput. Vis. Pattern Recog. Worksh"
    },
    {
      "citation_id": "76",
      "title": "spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Shaun Jeffrey F Cohn",
        "Michael Canavan",
        "Andy Reale",
        "Peng Horowitz",
        "Jeffrey Liu",
        "Girard"
      ],
      "year": "2005",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "77",
      "title": "M3f: Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "R Zhang",
        "J Huang",
        "Zeng",
        "Shan"
      ],
      "venue": "IEEE Conf. and Workshops on Auto. Face and Gesture Recog"
    },
    {
      "citation_id": "78",
      "title": "Context-aware feature and label fusion for facial action unit intensity estimation with partially labeled data",
      "authors": [
        "Yong Zhang",
        "Haiyong Jiang",
        "Baoyuan Wu",
        "Yanbo Fan",
        "Qiang Ji"
      ],
      "year": "2019",
      "venue": "Int. Conf. Comput. Vis"
    }
  ]
}