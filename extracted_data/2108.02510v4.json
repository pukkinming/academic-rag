{
  "paper_id": "2108.02510v4",
  "title": "Improved Speech Emotion Recognition Using Transfer Learning And Spectrogram Augmentation",
  "published": "2021-08-05T10:39:39Z",
  "authors": [
    "Sarala Padi",
    "Seyed Omid Sadjadi",
    "Dinesh Manocha",
    "Ram D. Sriram"
  ],
  "keywords": [
    "Attentive pooling",
    "IEMOCAP",
    "ResNet",
    "spectrogram augmentation",
    "speech emotion recognition (SER)",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic speech emotion recognition (SER) is a challenging task that plays a crucial role in natural human-computer interaction. One of the main challenges in SER is data scarcity, i.e., insufficient amounts of carefully labeled data to build and fully explore complex deep learning models for emotion classification. This paper aims to address this challenge using a transfer learning strategy combined with spectrogram augmentation. Specifically, we propose a transfer learning approach that leverages a pre-trained residual network (ResNet) model including a statistics pooling layer from speaker recognition trained using large amounts of speaker-labeled data. The statistics pooling layer enables the model to efficiently process variable-length input, thereby eliminating the need for sequence truncation which is commonly used in SER systems. In addition, we adopt a spectrogram augmentation technique to generate additional training data samples by applying random time-frequency masks to log-mel spectrograms to mitigate overfitting and improve the generalization of emotion recognition models. We evaluate the effectiveness of our proposed approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that the transfer learning and spectrogram augmentation approaches improve the SER performance, and when combined achieve state-of-the-art results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition plays a key role in human-computer interaction where it can enrich the next-generation AI with emotional intelligence by grasping the emotion from voice and words  [32, 37] . The motivation behind developing algorithms to analyze emotions is to design computer interfaces that mimic and embed realistic emotions in synthetically generated responses  [6] . Furthermore, research studies have shown that emotions play a critical role in the decision-making process for humans  [6] . Hence, there is a growing demand to develop automatic systems that understand and recognize human emotions.\n\nHumans express emotions in several ways, and speech is considered the most effective communication method to express feelings. For speech emotion recognition (SER), traditionally, machine learning (ML) models were developed using hand-crafted and engineered features such as mel-frequency cepstral coefficients (MFCC), Chroma-based features, pitch, energy, entropy, and zero-crossing rate  [16, 21, 43] , to mention a few. However, the performance of such ML models depends on the type and diversity of the features used. Although it remains unclear which features correlate most with various emotions, the research is still ongoing to explore additional features and new algorithms to model the dynamics of feature streams representing human emotions. On the other hand, the recent advancements in deep learning, along with the available computational capabilities, have enabled the research community to build end-to-end systems for SER. A big advantage of such systems is that they can directly learn the features from spectrograms or raw waveforms  [12, 23, 36, 41, 45] , thereby obviating the need for extracting a large set of hand-crafted features  [13] . Recent studies have proposed the use of convolutional neural network (CNN) models combined with long short-term memory (LSTM) built on spectrograms and raw waveforms, showing improved SER performance  [19, 23, 24, 26, 35, 36, 46] . However, building such complex systems requires large amounts of labeled training data. Also, the insufficient labeled training data can potentially make the models overfit to specific data conditions and domains, resulting in poor generalization on unseen data.\n\nThis paper addresses the insufficient data problem using a transfer learning approach combined with a spectrogram augmentation strategy. We re-purpose a residual network (ResNet) model  [17]  developed for speaker recognition using large amounts of speakerlabeled data and use it as a feature descriptor for SER. The model includes a statistics pooling layer that enables processing of variable length segments without a need for truncation. Also, we increase the training data size by generating more data samples using spectrogram augmentation  [30] . We evaluate the effectiveness of our proposed system on the interactive emotional dyadic motion capture (IEMOCAP) dataset  [3] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Recently, neural network based modeling approaches along with different variations of attention mechanism (e.g., plain  [18] , local  [24] , and self  [40] ) have shown promise for SER. Among them, techniques such as bidirectional LSTMs (BLSTM)  [10, 18, 24, 31, 42]  and time-delay neural networks (TDNN)  [44] , which can effectively model relatively long contexts compared to their DNN counterparts, have been successfully applied for SER on the IEMOCAP. Nevertheless, as discussed previously, the lack of large amounts of carefully labeled data to build complex models for emotion classification remains a main challenge in SER  [1] . To address this, two approaches are commonly used: data augmentation and transfer learning.\n\nData augmentation methods generate additional training data by perturbing, corrupting, mimicking, and masking the original data samples to enable the development of complex ML models. For example,  [4, 29, 35]  applied signal-based transformations such as speed perturbation, time-stretch, pitch shift, as well as added noise to original speech waveforms. One disadvantage of these approaches is that they require signal-level modifications, thereby increasing the computational complexity and storage requirements of the subsequent front-end processing. They can also lead to model overfitting due to potentially similar samples in the training set, while random balance can potentially remove useful information  [4] . For example, in  [9]  a vocal tract length perturbation (VTLP) approach was explored for data augmentation along with a CNN model, and was found to result in a lower accuracy compared to a baseline model due to overfitting issues.\n\nSince generative adversarial network (GAN) based models have demonstrated remarkable success in computer vision, several studies have recently incorporated this idea to address the data scarcity problem and to generate additional data samples for SER  [4, 8] . For instance,  [4]  addressed the data imbalance using signal-based transformations and GAN based models for generating high-resolution spectrograms to train a VGG19 model for an emotion classification task and showed that GAN-generated spectrograms outperformed signal-based transformations. However, GAN generated features are strongly dependent on the data used during training and may not generalize to other datasets. Another challenge with GAN-based augmentation is that it is difficult to train and optimize.\n\nAnother effective way to address challenges related to data scarcity is transfer learning  [2, 11, 14, 28, 49] . Transfer learning can leverage the information and knowledge learned from one related task and domain to another. Several recent studies have proposed transfer learning methods to improve SER performance and have shown these methods to outperform prior methods in recognizing emotions even for unseen scenarios, individuals, and conditions  [15] . It has been shown that transfer learning can increase feature learning abilities, and that the transferred knowledge can further enhance the SER classification accuracy  [7, 13, 22, 39] . To further improve the SER performance, transfer learned features have been used in combination with deep belief networks (DBN)  [22] , recurrent neural networks (RNN)  [13] , CNN  [39] , temporal convolutional network (TCN)  [49] , and sparse autoencoder  [7] . However, transfer learning methods have not been fully explored and analyzed for emotion recognition. Particularly, it is unclear whether and how ML models trained for other data-rich speech applications such as speaker recognition would perform for SER.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed System",
      "text": "Figure  4  shows the block diagram of the proposed system for speech emotion recognition. We use an end-to-end system with a ResNet34 model  [17]  to perform emotion classification. ResNet models, originally developed for computer vision applications  [17] , have recently gained interest for speech applications such as speaker recognition  [48] . The residual blocks introduced in ResNet models allow us to train much deeper models that are otherwise difficult, if not impossible, to train due to vanishing and exploding gradient problems. The ResNet models also allow the higher layers to learn the identity function so that higher-level features perform equally well on unseen data compared to the lower layers of the model. In our proposed system, the convolutional layers in the model learn feature representations (feature maps) and reduce the spectral variations into compact representations, while the fully connected (FC) layers take the contextual features and generate predictions for emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Input Data",
      "text": "Although SER systems traditionally used a large set of low-level time-and frequency-domain features to capture and represent the various emotions in speech, in recent years many state-of-the-art SER systems use complex neural network models that learn directly from spectrograms, or even raw waveforms. Accordingly, in this study, we build and explore a ResNet based system using log-mel spectrograms as input features. We extract high-resolution spectrograms to enable the model to not only learn the spectral envelope structure, but also the coarse harmonic structure for the various emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transfer Learning",
      "text": "As noted previously, transfer learning is a ML method where a model initially developed for one task or domain is re-purposed, partly or entirely, for a different but related task/domain. It has recently gained interest for SER  [11] . In this study, we re-purpose a model initially developed for speaker recognition to serve as a feature descriptor for SER. More specifically, we first train a ResNet34 model on large amounts of speaker-labeled audio data. Then, we replace the FC layers of the pre-trained model with new randomly initialized FC layers. Finally, we re-train the new FC layers for an SER task on the IEMOCAP dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Statistics Pooling",
      "text": "As shown in Figure  4 , the proposed system employs a statistics pooling layer  [38]  that aggregates the frame-level information over time and reduces the sequence of frames to a single vector by concatenating the mean and standard deviation computed over frames. Accordingly, the convolutional layers in the ResNet model work at the frame-level, while the FC layers work at the segmentlevel. This enables the system to efficiently model variable-length sequences of frames, thereby eliminating the need for truncating the sequence of frames to a pre-specified length to match that of the segments used during training. The sequence-truncation approach, which is commonly adopted in neural network based SER systems, can have a deleterious impact on SER performance as potentially informative frames are dropped out from the input. It is worth noting here that the statistics pooling can be viewed as an attention mechanism with equal weights for all frames, which also appends second order statistics (i.e., standard deviation) to capture long-term temporal variability over the duration of segments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spectrogram Augmentation",
      "text": "Currently, the majority of the features and methods for SER are adapted from speech recognition, speaker recognition, or speech synthesis fields  [20] . There has been recent success in applying a computationally efficient data augmentation strategy, termed spectrogram augmentation, for speech recognition tasks  [30] . The spectrogram augmentation technique generates additional training data samples by applying random time-frequency masks to spectrograms to mitigate the overfitting issue and improve the generalization of speech recognition models. Motivated by promising results seen with the spectrogram augmentation in the speech recognition field, we augment the training data using spectro-temporally modified versions of the original spectrograms (see Figure  2 ). Because the time-frequency masks are applied directly to spectrograms, the augmentation can be conveniently applied on-the-fly, eliminating the necessity to create and store new data files as commonly done in many augmentation approaches for speech applications.\n\nSimilar to the approach taken in  [30] , we consider two policies to systematically apply spectrogram augmentation for SER, namely conservative and aggressive. The frequency masking is applied over ùëì consecutive frequency channels in the range [ùëì 0 , ùëì 0 + ùëì ), where ùëì is sampled from a uniform distribution [0, ùêπ ] and ùëì 0 is sampled from [0, ùúà -ùëì ]. Here, ùêπ and ùúà denote the maximum width of frequency masks and the total number of frequency channels, respectively. The time masking, on the other hand, is applied over ùë° consecutive frames in the range [ùë° 0 , ùë° 0 + ùë°), where ùë° is selected from a uniform distribution [0,ùëä ] and ùë° 0 is sampled from [0,ùëá -ùë°]. Similarly, ùëä and ùëá denote the maximum width of time masks and the number of time frames, respectively. An upper bound is also applied on the width of the time masks such that ùëä = min(ùëä , ùëùùëá ), i.e., the width of a mask cannot be longer than ùëù times the number of time frames. This is to ensure sufficient speech content after masking, in particular for shorter segments. Table  1  summarizes the various parameters for the two spectrogram augmentation policies used in this paper.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments 4.1 Dataset",
      "text": "We evaluate the effectiveness of the proposed SER system on the IEMOCAP dataset  [3] , which contains improvised and scripted multimodal dyadic conversations between actors of opposite gender. It consists of 12 hours of speech data from 10 subjects, presegmented into short cuts that were judged by three annotators to generate emotion labels. It includes nine categorical emotions and 3-dimensional labels. In our experiments, we only consider the speech segments for which at least two annotators agree on the emotion label. In an attempt to replicate the experimental protocols used in a number of prior studies, we conduct three experiments on the full dataset (i.e., the combined improvised and scripted portions): Exp 1, using four categorical emotions: \"angry\", \"happy\", \"neutral\", \"sad\"; Exp 2, using the same categories as in Exp 1, but replacing the \"happy\" category with \"excited\"; Exp 3, by merging the \"happy\" and \"excited\" categories from Exp 1 and Exp 2. The total number of examples used for Exp 1 is 4490 and the number of examples per category is 1103, 595, 1708, and 1084, respectively. The number of examples in the \"excited\" category is 1041, making the total number of examples in the merged category (i.e., Exp 3) 1636. Table 2 summarizes the data statistics in the IEMOCAP dataset for the three experimental setups considered in this study.\n\nThe IEMOCAP dataset comprises five sessions, and the speakers in the sessions are non-overlapping. Therefore, there are 10 speakers in the dataset, i.e., 5 female and 5 male speakers. To conduct the experiments in a speaker-independent fashion, we use a leave-onesession-out (LOSO) cross-validation strategy, which results in 5 different train-test splits/folds. For each fold, we use the data from 4 sessions for training and the remaining one session for model evaluation. Since the dataset is multi-label and imbalanced, in addition to the overall accuracy, termed weighted accuracy (WA), we report the average recall over the different emotion categories, termed unweighted accuracy (UA), to present our findings. Additionally, to understand and visualize the performance of the proposed system within and across the various emotion categories, we compute and report confusion matrices for the three experiments. Note that for",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Setup And Configuration",
      "text": "For speech parameterization, high resolution 128-dimensional logmel spectrograms are extracted from 25 ms frames at a 100 Hz frame rate (i.e., every 10 ms). For feature normalization, a segment level mean and variance normalization is applied  1  . Note that this is not ideal as typically the normalization is applied at the recording/conversation level. We have found that normalizing the segments using statistics computed at the conversation level significantly improves the SER performance on the IEMOCAP. Nevertheless, this violates the independence assumption for the speech segments, hence it is not considered in this study. The front-end processing, including feature extraction and feature normalization, is performed using the NIST speaker and language recognition evaluation (SLRE)  [33, 34]  toolkit. While training the model, we select ùëá -frame chunks using random offsets over original speech segments where ùëá is randomly sampled from the set {150, 200, 250, 300} for each batch. For speech segments shorter than ùëá frames, signal padding is applied. On the other hand, while evaluating the model, we feed the entire duration of the test segments because the statistics pooling layer enables the model to consume variable-length inputs.\n\nAs noted previously, the proposed end-to-end SER system uses a pre-trained ResNet34 model built on a speaker recognition task. We train the ResNet34 model on millions of speech samples from more than 7000 speakers available in the VoxCeleb corpus  [25] . To build the speaker recognition model, we apply the same frontend processing described above to extract high-resolution log-mel Table  3 : Performance comparison of our proposed approach with prior methods that use the LOSO strategy for experiments on the full IEMOCAP dataset (i.e., both the improvised and scripted portions). Abbreviations: A-Angry, H-Happy, N-Neutral, E-Excited, H+E: Happy and Excited merged. Blanks (-) indicate unreported values.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiment (Emotion Classes) Approach Ua [%] Wa [%]",
      "text": "Exp 1 (A, H, S, N)\n\nBLSTM+attention  [18]  49.96 59.33 Transformer+self-attention  [40]  58.01 59.43 BLSTM+local attention  [24]  58.8 63.5 BLSTM+attention  [31]  59.6 62.5 Proposed 61.61 66.02\n\nExp 2 (A, E, S, N) CTC-BLSTM  [5]  54 -BLSTM+attention  [42]  55.65 -Transformer+self-attention  [40]  64.79 64.33 Proposed 65.56 65.62\n\nExp 3 (A, H+E, S, N)\n\nBLSTM+transfer learnig  [13]  51.86 50.47 VGG19+GAN augmentation  [4]  54.6 -CNN+attention+multi-task learning  [26]  -56.10 BLSTM+self-attention  [10]  57.0 55.7 CNN+attention+multi-task/transfer learning  [27]  59.54 -ResTDNN+self-attention  [44]  61.32 60.64 Proposed 64.14 63.61 spectrograms from VoxCeleb data. We conduct experiments using models with and without transfer learning and spectrogram augmentation. For each original speech segment, we generate and augment two spectro-temporally modified versions according to the augmentation policies defined in Table  1 . This is applied for both speaker and emotion recognition systems during training. To study the impact of the statistics pooling layer, we also evaluate these models with and without this layer. For all the experiments, we use a categorical cross-entropy loss as the objective function to train the models. The number of channels in the first block of the ResNet model is set to 32. The model is trained using Pytorch 2 and the stochastic gradient descent (SGD) optimizer with momentum (0.9), an initial learning rate of 10 -2 , and a batch size of 32. The learning rate remains constant for the first 8 epochs, after which it is halved every other epoch. We use parametric rectified linear unit (PReLU) activation functions in all layers (except for the output), and utilize a layer-wise batch normalization to accelerate the training process and improve the generalization properties of the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Table  3  presents the performance comparison of our proposed system with several prior approaches for the three experimental setups (i.e., Exp 1, 2, and 3) described in Section 4. The results are obtained using the combined system that utilizes the ResNet model with the statistics pooling layer trained using the transfer learning and spectrogram augmentation approaches described in Section 3. All studies referenced in the table adopt the LOSO strategy to conduct experiments on both the improvised and scripted portions of the 2 https://github.com/pytorch/pytorch IEMOCAP dataset  3  . It can be seen from the table that the proposed system consistently provides competitive performance across the three experiments, achieving state-of-the-art results. In the case of Exp 2, the proposed system outperforms a system that uses 384 engineered features  [40] , while for the other two experiments, our proposed system outperforms systems that use a large set of engineered features (e.g.,  [24]  and  [31] ).\n\nTo visualize the performance of the proposed system within and across the different emotion categories, confusion matrices for the three experimental setups are shown in Figure  3 . It is observed from Figure  3 (a) that the system confuses the \"happy\" class (H) with the \"neutral\" class (N) quite often, while performing the best on the \"angry\" class (A). This is consistent with observations reported in other studies on IEMOCAP  [26, 47] . Our informal listening experiments confirm that the \"happy\" and \"neutral\" classes are indeed confusable emotion pairs in the IEMOCAP dataset. The system performance balance is improved in Figure  3 (b) where we replace the less pronounced \"happy\" category with the \"excited\" category (E). Combining the \"happy\" and \"excited\" categories in Exp 3 further improves the performance balance across the various emotions, at the expense of increasing the confusion between the \"angry\" (A) and \"excited\" plus \"happy\" (H+E) categories.\n\nTo investigate and quantify the contribution of the various system components proposed in this study for improved SER, we further conduct ablation experiments to measure the system performance with and without the transfer learning, the spectrogram augmentation, and the statistics pooling layer. For these ablation  where we use 80% of the data for training and 20% for testing the system. This process is repeated 5 times to reduce possible partition-dependencies. Figure  4  shows the average overall classification accuracy (WA) computed across 5 folds (or 5 runs). The height of the bars represents the average accuracy, and the error bars denote the standard deviations computed over the 5 runs. It is observed that the proposed components, both individually and in combination, consistently provide performance gains across the three experimental setups (i.e., Exp 1, 2 and 3). The statistics pooling approach seems to have the greatest impact on performance, followed by the transfer learning and spectrogram augmentation methods. Furthermore, the model that combines all the system components not only consistently achieves the best performance, but also relatively smaller variation across the 5 runs as evidenced by the error bars.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we explored a transfer learning approach along with a spectrogram augmentation strategy to improve the SER performance. Specifically, we re-purposed a pre-trained ResNet model from speaker recognition that was trained using large amounts of speaker-labeled data. The convolutional layers of the ResNet model were used to extract features from high-resolution log-mel spectrograms. In addition, we adopted a spectrogram augmentation technique to generate additional training data samples by applying random time-frequency masks to log-mel spectrograms to mitigate overfitting and improve the generalization of emotion recognition models. We evaluated the proposed system using three different experimental settings and compared the performance against that of several prior studies. The proposed system consistently provided competitive performance across the three experimental setups, achieving state-of-the-art results on two settings. The state-of-theart results were achieved without the use of engineered features. It was also shown that incorporating the statistics pooling layer to accommodate variable-length audio segments improved the emotion recognition performance. Results from this study suggest that, for practical applications, simplified front-ends with only spectrograms can be as effective for SER, and that models trained for data-rich speech applications such as speaker recognition can be re-purposed using transfer learning to improve the SER performance under data scarcity constraints. In the future, to further enhance the emotion recognition accuracy, we will extend our work along these lines by exploring more data augmentation methods, incorporating other transfer learning paradigms, and evaluating the proposed system across different datasets.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Acknowledgement",
      "text": "Experiments and analyses were performed, in part, on the NIST Enki HPC cluster.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of the proposed SER system. ùëádenotes the number of frames.",
      "page": 2
    },
    {
      "caption": "Figure 4: shows the block diagram of the proposed system for speech",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) original spectrogram, and (b) spectrogram modified using (multiple) masking blocks of consecutive time steps (vertical masks)",
      "page": 3
    },
    {
      "caption": "Figure 4: , the proposed system employs a statistics",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Because",
      "page": 3
    },
    {
      "caption": "Figure 3: It is observed from",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) that the system confuses the ‚Äúhappy‚Äù class (H) with the",
      "page": 5
    },
    {
      "caption": "Figure 3: (b) where we replace the",
      "page": 5
    },
    {
      "caption": "Figure 3: Performance confusion matrices of the proposed SER system for the three experiments conducted in this study using",
      "page": 6
    },
    {
      "caption": "Figure 4: Performance (WA) of the proposed approach with and without transfer learning (TL), spectrogram augmentation",
      "page": 6
    },
    {
      "caption": "Figure 4: shows the average overall classi-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Exp 1": "",
          "Angry\nHappy\nNeutral\nSad": "Total",
          "1103\n595\n1708\n1084": "4490"
        },
        {
          "Exp 1": "Exp 2",
          "Angry\nHappy\nNeutral\nSad": "Angry\nExcited\nNeutral\nSad",
          "1103\n595\n1708\n1084": "1103\n1041\n1708\n1084"
        },
        {
          "Exp 1": "",
          "Angry\nHappy\nNeutral\nSad": "Total",
          "1103\n595\n1708\n1084": "4936"
        },
        {
          "Exp 1": "Exp 3",
          "Angry\nHappy\nNeutral\nSad": "Angry\nExcited+Happy\nNeutral\nSad",
          "1103\n595\n1708\n1084": "1103\n1636\n1708\n1084"
        },
        {
          "Exp 1": "",
          "Angry\nHappy\nNeutral\nSad": "Total",
          "1103\n595\n1708\n1084": "5531"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Exp 1 (A, H, S, N)": "",
          "BLSTM+attention [18]\nTransformer+self-attention [40]\nBLSTM+local attention [24]\nBLSTM+attention [31]": "Proposed",
          "49.96\n58.01\n58.8\n59.6": "61.61",
          "59.33\n59.43\n63.5\n62.5": "66.02"
        },
        {
          "Exp 1 (A, H, S, N)": "Exp 2 (A, E, S, N)",
          "BLSTM+attention [18]\nTransformer+self-attention [40]\nBLSTM+local attention [24]\nBLSTM+attention [31]": "CTC-BLSTM [5]\nBLSTM+attention [42]\nTransformer+self-attention [40]",
          "49.96\n58.01\n58.8\n59.6": "54\n55.65\n64.79",
          "59.33\n59.43\n63.5\n62.5": "‚Äì\n‚Äì\n64.33"
        },
        {
          "Exp 1 (A, H, S, N)": "",
          "BLSTM+attention [18]\nTransformer+self-attention [40]\nBLSTM+local attention [24]\nBLSTM+attention [31]": "Proposed",
          "49.96\n58.01\n58.8\n59.6": "65.56",
          "59.33\n59.43\n63.5\n62.5": "65.62"
        },
        {
          "Exp 1 (A, H, S, N)": "Exp 3 (A, H+E, S, N)",
          "BLSTM+attention [18]\nTransformer+self-attention [40]\nBLSTM+local attention [24]\nBLSTM+attention [31]": "BLSTM+transfer learnig [13]\nVGG19+GAN augmentation [4]\nCNN+attention+multi-task learning [26]\nBLSTM+self-attention [10]\nCNN+attention+multi-task/transfer learning [27]\nResTDNN+self-attention [44]",
          "49.96\n58.01\n58.8\n59.6": "51.86\n54.6\n‚Äì\n57.0\n59.54\n61.32",
          "59.33\n59.43\n63.5\n62.5": "50.47\n‚Äì\n56.10\n55.7\n‚Äì\n60.64"
        },
        {
          "Exp 1 (A, H, S, N)": "",
          "BLSTM+attention [18]\nTransformer+self-attention [40]\nBLSTM+local attention [24]\nBLSTM+attention [31]": "Proposed",
          "49.96\n58.01\n58.8\n59.6": "64.14",
          "59.33\n59.43\n63.5\n62.5": "63.61"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "Arsha Samuel Albanie",
        "Andrea Nagrani",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. ACM ICM"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition among elderly individuals using multimodal fusion and transfer learning",
      "authors": [
        "George Boateng",
        "Tobias Kowatsch"
      ],
      "year": "2020",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Data augmentation using GANs for speech emotion recognition",
      "authors": [
        "Aggelina Chatziagapi",
        "Georgios Paraskevopoulos",
        "Dimitris Sgouropoulos"
      ],
      "year": "2019",
      "venue": "Georgios Pantazopoulos, Malvina Nikandrou, Theodoros Giannakopoulos, Athanasios Katsamanis, Alexandros Potamianos, and Shrikanth Narayanan"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "Vladimir Chernykh",
        "Pavel Prikhodko"
      ],
      "year": "2017",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "7",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "Jun Deng",
        "Zixing Zhang",
        "Erik Marchi",
        "Bj√∂rn Schuller"
      ],
      "year": "2013",
      "venue": "Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "8",
      "title": "GAN-based data generation for speech emotion recognition",
      "authors": [
        "Dimitrios Sefik Emre Eskimez",
        "Robert Dimitriadis",
        "Kenichi Gmyr",
        "Kumanati"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "CNN+LSTM architecture for speech emotion recognition with data augmentation",
      "authors": [
        "Caroline Etienne",
        "Guillaume Fidanza",
        "Andrei Petrovskii",
        "Laurence Devillers",
        "Benoit Schmauch"
      ],
      "year": "2018",
      "venue": "CNN+LSTM architecture for speech emotion recognition with data augmentation",
      "arxiv": "arXiv:1802.05630"
    },
    {
      "citation_id": "10",
      "title": "End-to-End speech emotion recognition combined with acoustic-to-word ASR model",
      "authors": [
        "Han Feng",
        "Sei Uno",
        "Tatsuya Kawahara"
      ],
      "year": "2020",
      "venue": "Proc. IN-TERSPEECH"
    },
    {
      "citation_id": "11",
      "title": "A review of generalizable transfer learning in automatic emotion recognition",
      "authors": [
        "Kexin Feng",
        "Theodora Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "12",
      "title": "End-to-end speech emotion recognition based on one-dimensional convolutional neural network",
      "authors": [
        "Mengna Gao",
        "Jing Dong",
        "Dongsheng Zhou",
        "Qiang Zhang",
        "Deyun Yang"
      ],
      "year": "2019",
      "venue": "Proc. ACM ICIAI"
    },
    {
      "citation_id": "13",
      "title": "Representation Learning for Speech Emotion Recognition",
      "authors": [
        "Sayan Ghosh",
        "Eugene Laksana",
        "Louis-Philippe Morency",
        "Stefan Scherer"
      ],
      "year": "2016",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "14",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "John Gideon",
        "Soheil Khorram",
        "Zakaria Aldeneh",
        "Dimitrios Dimitriadis",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)",
      "authors": [
        "John Gideon",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "17",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Attention assisted discovery of sub-utterance structure in speech emotion recognition",
      "authors": [
        "Che-Wei Huang",
        "Shri Narayanan"
      ],
      "year": "2016",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "19",
      "title": "Convolutional RNN: an enhanced model for extracting features from sequential data",
      "authors": [
        "Gil Keren",
        "Bj√∂rn Schuller"
      ],
      "year": "2016",
      "venue": "Proc. IEEE IJCNN"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Sreenivasa Koolagudi",
        "Rao"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "Oh-Wook Kwon",
        "Kwokleung Chan",
        "Jiucang Hao",
        "Te-Won Lee"
      ],
      "year": "2003",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "22",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Shahzad Younis",
        "Junaid Qadir",
        "Julien Epps"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "Xi Ma",
        "Zhiyong Wu",
        "Jia Jia",
        "Mingxing Xu",
        "Helen Meng",
        "Lianhong Cai"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "24",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "25",
      "title": "Voxceleb: Large-scale speaker verification in the wild",
      "authors": [
        "Arsha Nagrani",
        "Son Joon",
        "Weidi Chung",
        "Andrew Xie",
        "Zisserman"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "26",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "27",
      "title": "Improving Speech Emotion Recognition with Unsupervised Representation Learning on Unlabeled Speech",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Group-level speech emotion recognition utilising deep spectrum features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Bj√∂rn Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "29",
      "title": "X-Vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Tianzi Wang",
        "Jesus Villalba",
        "Nanxin Chen",
        "Najim Dehak"
      ],
      "year": "2020",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "30",
      "title": "SpecAugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "Daniel Park",
        "William Chan",
        "Yu Zhang",
        "Chung-Cheng Chiu",
        "Barret Zoph",
        "Ekin Cubuk",
        "V Quoc",
        "Le"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "31",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "Gaetan Ramet",
        "Philip Garner",
        "Michael Baeriswyl",
        "Alexandros Lazaridis"
      ],
      "year": "2018",
      "venue": "Proc. IEEE SLT Workshop"
    },
    {
      "citation_id": "32",
      "title": "Being Human: Human-Computer Interaction in The Year",
      "authors": [
        "R Harper Richard",
        "R Tom",
        "Yvonne",
        "Abigail"
      ],
      "year": "2008",
      "venue": "Being Human: Human-Computer Interaction in The Year"
    },
    {
      "citation_id": "33",
      "title": "The 2019 NIST audio-visual speaker recognition evaluation",
      "authors": [
        "Omid Seyed",
        "Craig Sadjadi",
        "Elliot Greenberg",
        "Douglas Singer",
        "Lisa Reynolds",
        "Jaime Mason",
        "Hernandez-Cordero"
      ],
      "year": "2020",
      "venue": "Proc. Speaker Odyssey Workshop"
    },
    {
      "citation_id": "34",
      "title": "The 2017 NIST language recognition evaluation",
      "authors": [
        "Omid Seyed",
        "Timothee Sadjadi",
        "Audrey Kheyrkhah",
        "Craig Tong",
        "Elliot Greenberg",
        "Douglas Singer",
        "Lisa Reynolds",
        "Jaime Mason",
        "Hernandez-Cordero"
      ],
      "year": "2018",
      "venue": "Proc. Speaker Odyssey Workshop"
    },
    {
      "citation_id": "35",
      "title": "Emotion identification from raw speech signals using DNNs",
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "36",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Bj√∂rn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "38",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "David Snyder",
        "Daniel Garcia-Romero",
        "Gregory Sell",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition using transfer learning",
      "authors": [
        "Peng Song",
        "Yun Jin",
        "Li Zhao",
        "Minghai Xin"
      ],
      "year": "2014",
      "venue": "IEICE Trans. Information and Systems"
    },
    {
      "citation_id": "40",
      "title": "Selfattention for speech emotion recognition",
      "authors": [
        "Lorenzo Tarantino",
        "Philip Garner",
        "Alexandros Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "41",
      "title": "Adieu features? end-toend speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Bj√∂rn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "42",
      "title": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning",
      "authors": [
        "Samarth Tripathi",
        "Tripathi Sarthak",
        "Homayoon Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "43",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "Dimitrios Ververidis",
        "Constantine Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "44",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "45",
      "title": "Predicting arousal and valence from waveforms and spectrograms using deep neural networks",
      "authors": [
        "Zixiaofan Yang",
        "Julia Hirschberg"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "46",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "Promod Yenigalla",
        "Abhay Kumar",
        "Suraj Tripathi",
        "Chirag Singh",
        "Sibsambhu Kar",
        "Jithendra Vepa"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "47",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "Proc. IEEE SLT"
    },
    {
      "citation_id": "48",
      "title": "BUT system description to VoxCeleb speaker recognition challenge",
      "authors": [
        "Hossein Zeinali",
        "Shuai Wang",
        "Anna Silnova",
        "Pavel Matƒõjka",
        "Old≈ôich Plchot"
      ],
      "year": "2019",
      "venue": "BUT system description to VoxCeleb speaker recognition challenge",
      "arxiv": "arXiv:1910.12592"
    },
    {
      "citation_id": "49",
      "title": "Self-attention transfer networks for speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Zhongtian Bao",
        "Zixing Zhang",
        "Nicholas Cummins",
        "Shihuang Sun",
        "Haishuai Wang",
        "Jianhua Tao",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "Virtual Reality & Intelligent Hardware"
    }
  ]
}