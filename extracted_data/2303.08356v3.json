{
  "paper_id": "2303.08356v3",
  "title": "Leveraging Tcn And Transformer For Effective Visual-Audio Fusion In Continuous Emotion Recognition",
  "published": "2023-03-15T04:15:57Z",
  "authors": [
    "Weiwei Zhou",
    "Jiada Lu",
    "Zhaolong Xiong",
    "Weifeng Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotion recognition plays an important role in human-computer interaction. In this paper, we present our approach to the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective Behavior Analysis in-thewild (ABAW). Specifically, we propose a novel multi-modal fusion model that leverages Temporal Convolutional Networks (TCN) and Transformer to enhance the performance of continuous emotion recognition. Our model aims to effectively integrate visual and audio information for improved accuracy in recognizing emotions. Our model outperforms the baseline and ranks 3 in the Expression Classification challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial Expression Recognition (FER) can be used in a variety of applications, such as emotion recognition in videos, facial recognition for security purposes, and even in virtual reality applications. Many facial-related tasks have achieved high accuracies, such as face recognition and face attribute recognition. Despite this, the capacity to comprehend the emotions of a person is still not adequate. The subtle distinctions between emotional expressions can lead to ambiguity or uncertainty in the perception of emotions, which makes it harder to assess the emotion of a person. Therefore, the scale of most of the FER datasets are not sufficient to build a robust model.\n\nThe appearance of AffWild and AffWild2 dataset and the corresponding challenges  [5-12, 12-14, 30]  boost the development of affective recognition study. The Aff-Wild2 dataset contains about 600 videos with around 3M frames. The dataset is annotated with three different affect attributes: a) dimensional affect with valence and arousal; b) six basic categorical affect; c) action units of facial mus-* These authors contributed equally to this work. cles. To facilitate the utilization of the Aff-Wild2 dataset, the ABAW5 2023 competition was organized for affective behavior analysis in the wild.\n\nMulti-modal emotion recognition has been proven to be a more effective approach than single-modality emotion recognition, as it can utilize the complementary information between modalities to capture a more complete emotional state while being less susceptible to various noises. This improved recognition ability and generalization ability of the model can lead to more accurate and reliable results.\n\nConsidering the fact that visual and audio information contains much emotional information, we propose to use multi-modal features for continuous facial emotion recognition and design a network structure based on TCN and Transformer for feature fusion. Visual and audio features are first fed into their respective TCN modules, then the features are concatenated and fed into the Transformer encoder for learning, and finally, an MLP is used for prediction. Our approach can unify visual and audio features into a temporal model, designing an efficient emotion recognition network with Transformer, thereby improving the evaluation accuracy of Valence-Arousal Estimation, Action Unit Detection, and Expression Classification.\n\nThe remaining parts of the paper are presented as follows: Sec 2 describe the study of facial emotion recognition and multi-modal fusion technique. Sec 3 describes our methodology; Sec 4 describes the experiment details and the result; Sec 5 is the conclusion of the paper.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Many previous studies were focusing on the fusion of visual and audio features for emotion recognition. Juan et al.  [19]  presented a network that used traditional audio features and visual features extracted with a pre-trained CNN model. Vu et al.  [28]  built a multi-task model for valencearousal estimation and facial expressions prediction. The authors applied the distillation knowledge architecture for training and prediction because the dataset does not include labels for all two tasks. One of the approaches using the arXiv:2303.08356v3 [cs.CV] 6 Sep 2023 multi-modal mechanism for facial emotion recognition was proposed by Tzirakis et al.  [26] , where the visual and audio features are extracted with the CNN module and are concatenated to feed into the LSTM network. Nguyen et al.  [18]  proposed a network consisting of a two-stream autoencoder and an LSTM to integrate visual and audio signals for emotion recognition. Zhang et al.  [31]  proposed a multi-modal multi-feature approach that extracts visual features from 3D-CNN and audio features from a bidirectional recurrent neural network. Srinivas et al.  [21]  propose a transformer architecture with encoder layers to integrate audio-visual features for expression tracking. Tzirakis et al.  [25]  use attention-based methods to fuse the visual and audio features.\n\nPrevious studies have proposed some useful networks on the Aff-wild2 dataset. Kuhnke et al.  [15]  combine vision and audio information in the video and construct a twostream network for emotion recognition and achieving high performance. Yue Jin et al.  [4]  propose a transformer-based model to merge audio and visual feature.\n\nTemporal Convolutional Network (TCN) was proposed by Colin Lea et al.  [16] , which hierarchically captures relationships at low-, intermediate-, and high-level time scales. Jin Fan et al.  [3]  proposed a model with a spatial-temporal attention mechanism to catch dynamic internal correlations with stacked TCN backbones to extract features from different window sizes.\n\nThe Transformer mechanism proposed by Vaswani et al.  [27]  has achieved high performance in many tasks, so many researchers exploit Transfomer for affective behavior studies. Zhao et al.  [32]  proposed a model with spatial and temporal Transformer for facial expression analysis. Jacob et al.  [17]  proposed a network to learn the relationship between action units with transformer correlation module.\n\nInspired by the previous work, in this paper we proposed a multi-modal fusion model with TCN and Transformer to enhance the performance of emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we describe in detail our proposed method for tackling the three challenging tasks of affective behavior analysis in the wild that are addressed by the 5th ABAW Competition: Valence-Arousal Estimation, EXPR Classification, and AU Detection. We explain how we design our model architecture, data processing, and training strategy for each task and how we leverage multi-modal to improve our performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing",
      "text": "We extract the audio stream from the video and preprocess it by converting it to a mono channel with a sample rate of 16, 000 Hz. This allows us to reduce the noise and complexity of the audio signal. Some of the video frames do not contain valid faces, either due to missing or not detected by the face detector. To handle this issue, we replace these frames with the closest frame that has valid face detection. This ensures that we have a consistent sequence of facial images for each video.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio Features",
      "text": "We use Wav2Vec2-emotion  [22]  to extract the audio features that capture the emotional content of speech.\n\nWav2Vec2-emotion is a model based on Wav2Vec2-Large-Robust, which is pre-trained on 960 hours of Lib-riSpeech audio with a sampling rate of 16kHz. The model is then fine-tuned on 284 instances of MSP-Podcast data, which contains emotional speech from different speakers and scenarios. The feature vector dimension is 512, which represents a high-level representation of the acoustic signal.\n\nTo align the audio features with the video frames, we resize the features to match the length of each frame using interpolation. This ensures that we have a consistent temporal resolution for both modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Features",
      "text": "We extract four visual feature vectors using different models that capture various aspects of facial appearance and expression.\n\nThe first feature vector is extracted using ArcFace  [2]  from insightface, which has been pre-trained on the Glint360K dataset  [1]  for face recognition. This vector encodes the identity and pose of the face with a dimension of 512.\n\nThe second feature vector is extracted using EfficientNet-b2  [23, 24] , which has been pre-trained on the VGGFace2 dataset  [20]  for face identification and fine-tuned on the AffectNet8 dataset. This vector captures the facial attributes and expressions with a dimension of 1280.\n\nThe third and fourth feature vectors are extracted using a model from DAN  [29] , pre-trained on MSCeleb, and finetuned on RAF-DB and AffectNet8. These vectors represent the global and local features of the face with a dimension of 512 each.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Split Videos",
      "text": "Videos are first split into segments with a window size w and stride s. Given the segment window w and stride s, a video with n frames would be split into [n/s] + 1 segments, where the i-th segment contains frames F (i-1) * s+1 , . . . , F (i-1) * s+w .\n\nIn other words, videos are cut into some overlapping chunks, each with a fixed number of frames. The purpose of doing this is to break down the video into smaller parts that are easier to process and analyze. Each chunk has some degree of overlap with the previous and next ones so that no information in the video is missed.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modeling",
      "text": "We denote audio features as f a i and visual features as f v i corresponding to the i-th segment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Convolutional Network",
      "text": "Each feature is fed into a dedicated Temporal Convolutional Network (TCN) for temporal encoding, which can be formulated as follows:\n\nwhere g v i denotes visual features, g a i denotes audio features. Then, visual features and audio features are concatenated, denotes as g c i .\n\nThis means that we use a special type of neural network that can capture the temporal patterns and dependencies of the features over time. The TCN takes the input feature vector and applies a series of convolutional layers with different kernel sizes and dilation rates to produce an output feature vector. The output feature vector has the same length as the input feature vector but contains more information about the temporal context. For example, the TCN can learn how the sound and image change over time in each segment of the video. The output feature vectors for both sound and image are then combined together by concatenating them along a dimension. This creates a new feature vector that contains both audio and visual information for each segment of the video.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Encoder",
      "text": "We utilize a transformer encoder to model the temporal information in the video segment as well, which can be formulated as follows:\n\n. The Transformer encoder only models the context within a single segment, thereby ignoring the dependencies between frames across segments. To account for the context of different frames, overlapping between consecutive segments can be employed, thus enabling the capture of the dependencies between frames across segments, which means s ≤ w.\n\nWe use another type of neural network that can learn the relationships and interactions among the features within each segment. The transformer encoder takes the input feature vector that contains both audio and visual information and applies a series of self-attention layers and feed-forward layers to produce an output feature vector. The output feature vector has more semantic meaning and representation power than the input feature vector. For example, the transformer encoder can learn how different parts of the sound and image relate to each other in each segment of the video. However, the transformer encoder does not consider how different segments of the video are connected or influenced by each other. To solve this problem, we can make some segments overlap with each other so that some frames are shared by two or more segments. This way, we can capture some information about how different segments affect each other. The degree of overlap is controlled by two parameters: s is the length of a segment and w is the sliding window size. If s is smaller than or equal to w, then there will be some overlap between consecutive segments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prediction",
      "text": "After the temporal encoder, the features h i are finally fed into MLP for regression, which can be formulated as fol-lows:\n\nwhere y i are the predictions of i-th segment. For VA challenge, y i ∈ R l×2 . For EXPR challenge, y i ∈ R l×8 . For AU challenge, y i ∈ R l×12 .\n\nThe prediction vector contains the values that we want to estimate for each segment. The MLP consists of several layers of neurons that can learn non-linear transformations of the input. The MLP can be trained to minimize the error between the prediction vector and the ground truth vector. The ground truth vector is the actual values that we want to predict for each segment. Depending on what kind of challenge we are solving, we have different types of ground truth vectors and prediction vectors. For the VA challenge, we want to predict two values: valence and arousal. Valence measures how positive or negative an emotion is. Arousal measures how active or passive an emotion is. For the EXPR challenge, we want to predict eight values: one for each basic expression (anger, disgust, fear, happiness, sadness, and surprise) plus neutral and other expressions. For the AU challenge, we want to predict twelve values: one for each action unit (AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, AU26).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Functions",
      "text": "VA challenge: We use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the measure, which is defined as in Eq 1. It measures the correlation between two sequences x and y and ranges between -1 and 1, where -1 means perfect anticorrelation, 0 means no correlation, and 1 means perfect correlation. The loss is calculated as Eq 2.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ccc(X, Y)",
      "text": "EXPR challenge: We use the cross-entropy loss as the loss function, which is defined as in Eq 3.\n\nwhere y ic is a binary indicator (0 or 1) if class c is the correct classification for observation i. p ic is the predicted probability of observation i being in class c, M is the number of classes. The multiclass cross entropy loss function measures how well a model predicts the true probabilities of each class for a given observation. It penalizes wrong predictions by taking the logarithm of the predicted probabilities. The lower the loss, the better the model.\n\nAU challenge: We employ BCEWithLogitsLoss as the loss function, which integrates a sigmoid layer and binary cross-entropy, which is defined as in Eq 4.\n\n(4) where N is the number of samples, y i is the target label for sample i, x i is the input logits for sample i, σ is the sigmoid function The advantage of using BCEWithLogitsLoss over BCELoss with sigmoid is that it can avoid numerical instability and improve performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments Settings",
      "text": "All models are trained on an Nvidia GeForce GTX 3090 GPU which has 24GB of memory. We use AdamW optimizer and cosine learning rate schedule with the first epoch warmup. The learning rate is 3e -5, the weight decay is 1e -5, the dropout prob is 0.3, and the batch size is 32.\n\nFor VA Challenge, we use Wav2Vec2-emotion, Eff, RAF-DB, and AffectNet8 as the input features.\n\nFor EXPR Challenge, we use two types of input features: Eff and AffectNet8 as described above.\n\nFor AU Challenge, we use three types of input features: Eff, RAF-DB, and AffectNet8 as described above.\n\nFor all three challenges, we split videos using a segment window w = 300 and a stride s = 200. This means we divide each video into segments of 300 frames with an overlap of 100 frames between consecutive segments. This helps us capture the temporal dynamics of facial expressions and emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overall Results",
      "text": "Table  1  displays the experimental results of our proposed method on the validation set of the VA, EXPR, and AU Challenge, where the Concordance Correlation Coefficient (CCC) is utilized as the evaluation metric for both valence and arousal prediction, and F1-score is used to evaluate the result of EXPR and AU challenge. As demonstrated in the table, our proposed method outperforms the baseline significantly. These results show that our proposed approach using TCN and Transformer-based model effectively integrates visual and audio information for improved accuracy in recognizing emotions on this dataset.\n\nTable  2 , Table  3 , and Table  4  display the overall test results on the three challenges. Notably, Netease Fuxi and SituTech achieved the first and second highest scores in all three challenges, surpassing other teams significantly, indicating their exceptional performance in these challenges.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we perform several ablation studies on these three experiments to compare the contribution of different features. From Table  6 , it can be seen that almost every feature contributes to the VA prediction task, and the combination of 4 visual features: Eff, ArcFace, AffectNet8, RAF-DB, and the audio features: Wav2Vec2-emotion reach the highest CCC score on VA experiment. Table  7  shows that the use of Eff and AffectNet8 can reach the highest F1score in the EXPR experiments. Table  8  shows that Eff, AffectNet8, and RAF-DB can reach the highest F1-score in the EXPR and AU experiments. The cross-validation result of the VA, EXPR, and AU experiments are reported in Table 5. Fold 0 is exactly the original data from the ABAW dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Our proposed approach utilizes a combination of a Temporal Convolutional Network (TCN) and a Transformerbased model to integrate visual and audio information for improved accuracy in recognizing emotions. The TCN captures relationships at low-, intermediate-, and high-level time scales, while the Transformer mechanism merges audio and visual features. We conducted our experiment on the Aff-Wild2 dataset, which is a widely used benchmark team ranks fourth in the VA challenge, third in the EXPR challenge, and sixth in the AU challenge.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of our proposed model. The model consists of four components: pre-trained feature extractors for audio and",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Chinatelecom Cloud": "{zhouweiwei,lujiada,xiongzl12,wangweifeng}@chinatelecom.cn"
        },
        {
          "Chinatelecom Cloud": "Abstract"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "Human emotion recognition plays an important role in"
        },
        {
          "Chinatelecom Cloud": "human-computer\ninteraction.\nIn this paper, we present"
        },
        {
          "Chinatelecom Cloud": "our approach to the Valence-Arousal (VA) Estimation Chal-"
        },
        {
          "Chinatelecom Cloud": "lenge, Expression (Expr) Classification Challenge, and Ac-"
        },
        {
          "Chinatelecom Cloud": "tion Unit\n(AU) Detection Challenge of\nthe 5th Workshop"
        },
        {
          "Chinatelecom Cloud": "and Competition\non Affective Behavior Analysis\nin-the-"
        },
        {
          "Chinatelecom Cloud": "wild (ABAW). Specifically, we propose a novel multi-modal"
        },
        {
          "Chinatelecom Cloud": "fusion model\nthat\nleverages Temporal Convolutional Net-"
        },
        {
          "Chinatelecom Cloud": "works (TCN) and Transformer to enhance the performance"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "of\ncontinuous\nemotion recognition.\nOur model aims\nto"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "effectively integrate visual and audio information for\nim-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "proved accuracy in recognizing emotions. Our model out-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "performs the baseline and ranks 3 in the Expression Classi-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "fication challenge."
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "1. Introduction"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "Facial Expression Recognition (FER)\ncan be used in"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "a variety of applications,\nsuch as emotion recognition in"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "videos, facial recognition for security purposes, and even in"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "virtual reality applications. Many facial-related tasks have"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "achieved high accuracies, such as face recognition and face"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "attribute recognition. Despite this,\nthe capacity to compre-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "hend the emotions of a person is still not adequate.\nThe"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "subtle distinctions between emotional expressions can lead"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "to ambiguity or uncertainty in the perception of emotions,"
        },
        {
          "Chinatelecom Cloud": "which makes it harder\nto assess the emotion of a person."
        },
        {
          "Chinatelecom Cloud": "Therefore,\nthe scale of most of\nthe FER datasets are not"
        },
        {
          "Chinatelecom Cloud": "sufficient to build a robust model."
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "The appearance of AffWild and AffWild2 dataset and the"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "corresponding challenges [5–12, 12–14, 30] boost\nthe de-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "velopment of affective recognition study.\nThe Aff-Wild2"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "dataset contains about 600 videos with around 3M frames."
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "The\ndataset\nis\nannotated with\nthree\ndifferent\naffect\nat-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "tributes: a) dimensional affect with valence and arousal; b)"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "six basic categorical affect; c) action units of\nfacial mus-"
        },
        {
          "Chinatelecom Cloud": ""
        },
        {
          "Chinatelecom Cloud": "*These authors contributed equally to this work."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multi-modal mechanism for facial emotion recognition was": "proposed by Tzirakis et al. [26], where the visual and audio",
          "do not contain valid faces, either due to missing or not de-": "tected by the face detector. To handle this issue, we replace"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "features are extracted with the CNN module and are con-",
          "do not contain valid faces, either due to missing or not de-": "these frames with the closest frame that has valid face de-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "catenated to feed into the LSTM network. Nguyen et al.",
          "do not contain valid faces, either due to missing or not de-": "tection. This ensures that we have a consistent sequence of"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "[18] proposed a network consisting of a two-stream auto-",
          "do not contain valid faces, either due to missing or not de-": "facial images for each video."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "encoder and an LSTM to integrate visual and audio sig-",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "nals for emotion recognition. Zhang et al.\n[31] proposed",
          "do not contain valid faces, either due to missing or not de-": "3.2. Audio Features"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "a multi-modal multi-feature approach that extracts visual",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "We use Wav2Vec2-emotion [22] to extract the audio fea-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "features from 3D-CNN and audio features from a bidirec-",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "tures that capture the emotional content of speech."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "tional recurrent neural network. Srinivas et al. [21] propose",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "Wav2Vec2-emotion\nis\na model\nbased\non Wav2Vec2-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "a transformer architecture with encoder layers to integrate",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "Large-Robust, which is pre-trained on 960 hours of Lib-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "audio-visual\nfeatures for expression tracking.\nTzirakis et",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "riSpeech audio with a sampling rate of 16kHz. The model"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "al.\n[25] use attention-based methods to fuse the visual and",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "is then fine-tuned on 284 instances of MSP-Podcast data,"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "audio features.",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "which contains emotional\nspeech from different\nspeakers"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "Previous studies have proposed some useful networks on",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "and scenarios. The feature vector dimension is 512, which"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "the Aff-wild2 dataset. Kuhnke et al.\n[15] combine vision",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "represents a high-level representation of the acoustic signal."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "and audio information in the video and construct a two-",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "To align the audio features with the video frames, we re-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "stream network for emotion recognition and achieving high",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "size the features to match the length of each frame using"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "performance. Yue Jin et al. [4] propose a transformer-based",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "interpolation. This ensures that we have a consistent\ntem-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "model to merge audio and visual feature.",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "poral resolution for both modalities."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "Temporal Convolutional Network (TCN) was proposed",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "by Colin Lea et al. [16], which hierarchically captures rela-",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "3.3. Visual Features"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "tionships at low-, intermediate-, and high-level time scales.",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "Jin Fan et al.\n[3] proposed a model with a spatial-temporal",
          "do not contain valid faces, either due to missing or not de-": "We\nextract\nfour visual\nfeature vectors using different"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "attention mechanism to catch dynamic internal correlations",
          "do not contain valid faces, either due to missing or not de-": "models that capture various aspects of facial appearance and"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "with stacked TCN backbones to extract\nfeatures from dif-",
          "do not contain valid faces, either due to missing or not de-": "expression."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "ferent window sizes.",
          "do not contain valid faces, either due to missing or not de-": "The\nfirst\nfeature\nvector\nis\nextracted\nusing ArcFace"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "The Transformer mechanism proposed by Vaswani et al.",
          "do not contain valid faces, either due to missing or not de-": "[2]\nfrom insightface, which has been pre-trained on the"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "[27] has achieved high performance in many tasks, so many",
          "do not contain valid faces, either due to missing or not de-": "Glint360K dataset [1] for face recognition. This vector en-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "researchers exploit Transfomer for affective behavior stud-",
          "do not contain valid faces, either due to missing or not de-": "codes the identity and pose of the face with a dimension of"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "ies.\nZhao et al.\n[32] proposed a model with spatial and",
          "do not contain valid faces, either due to missing or not de-": "512."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "temporal Transformer for facial expression analysis.\nJacob",
          "do not contain valid faces, either due to missing or not de-": "The\nsecond\nfeature\nvector\nis\nextracted\nusing"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "et al.\n[17] proposed a network to learn the relationship be-",
          "do not contain valid faces, either due to missing or not de-": "EfficientNet-b2\n[23,\n24],\nwhich\nhas\nbeen\npre-trained"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "tween action units with transformer correlation module.",
          "do not contain valid faces, either due to missing or not de-": "on the VGGFace2 dataset\n[20]\nfor\nface identification and"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "Inspired by the previous work, in this paper we proposed",
          "do not contain valid faces, either due to missing or not de-": "fine-tuned on the AffectNet8 dataset. This vector captures"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "a multi-modal fusion model with TCN and Transformer to",
          "do not contain valid faces, either due to missing or not de-": "the facial attributes and expressions with a dimension of"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "enhance the performance of emotion recognition.",
          "do not contain valid faces, either due to missing or not de-": "1280."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "The third and fourth feature vectors are extracted using"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "3. Methodology",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "a model from DAN [29], pre-trained on MSCeleb, and fine-"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "tuned on RAF-DB and AffectNet8. These vectors represent"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "In\nthis\nsection, we\ndescribe\nin\ndetail\nour\nproposed",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "the global and local features of the face with a dimension of"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "method for tackling the three challenging tasks of affective",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "512 each."
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "behavior analysis in the wild that are addressed by the 5th",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "ABAW Competition: Valence-Arousal Estimation, EXPR",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "3.4. Split Videos"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "Classification, and AU Detection. We explain how we de-",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "sign our model architecture, data processing, and training",
          "do not contain valid faces, either due to missing or not de-": "Videos\nare\nfirst\nsplit\ninto\nsegments with\na window"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "strategy for each task and how we leverage multi-modal\nto",
          "do not contain valid faces, either due to missing or not de-": "size w and\nstride\ns.\nGiven\nthe\nsegment window w"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "improve our performance.",
          "do not contain valid faces, either due to missing or not de-": "and stride s,\na video with n frames would be split\ninto"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "[n/s] + 1\nsegments, where\nthe\ni-th\nsegment\ncontains"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "3.1. Preprocessing",
          "do not contain valid faces, either due to missing or not de-": ""
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "",
          "do not contain valid faces, either due to missing or not de-": "(cid:9).\nframes(cid:8)F(i−1)∗s+1, . . . , F(i−1)∗s+w"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "We extract\nthe audio stream from the video and prepro-",
          "do not contain valid faces, either due to missing or not de-": "In other words, videos are cut\ninto some overlapping"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "cess it by converting it\nto a mono channel with a sample",
          "do not contain valid faces, either due to missing or not de-": "chunks, each with a fixed number of frames. The purpose"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "rate of 16, 000 Hz. This allows us to reduce the noise and",
          "do not contain valid faces, either due to missing or not de-": "of doing this is to break down the video into smaller parts"
        },
        {
          "multi-modal mechanism for facial emotion recognition was": "complexity of the audio signal. Some of the video frames",
          "do not contain valid faces, either due to missing or not de-": "that are easier to process and analyze. Each chunk has some"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Visual Eeatures": "Temporal Block\nTemporal Block"
        },
        {
          "Visual Eeatures": "Extractor"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "Figure 1. The architecture of our proposed model. The model consists of four components: pre-trained feature extractors for audio and"
        },
        {
          "Visual Eeatures": "visual features, TCN with three temporal blocks, Transformer encoder, and MLP for final prediction."
        },
        {
          "Visual Eeatures": "degree of overlap with the previous and next ones so that no"
        },
        {
          "Visual Eeatures": "information in the video is missed."
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "3.5. Modeling"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "We denote audio features as f a\nand visual features as f v"
        },
        {
          "Visual Eeatures": "i\ni"
        },
        {
          "Visual Eeatures": "corresponding to the i-th segment."
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "3.5.1\nTemporal Convolutional Network"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "Each feature is fed into a dedicated Temporal Convolutional"
        },
        {
          "Visual Eeatures": "Network (TCN) for temporal encoding, which can be for-"
        },
        {
          "Visual Eeatures": "mulated as follows:"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "gv"
        },
        {
          "Visual Eeatures": "i = TCN (f v\ni )"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "ga\ni = TCN (f a\ni )"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "where gv\ni denotes visual features, ga\ni denotes audio features."
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "Then, visual features and audio features are concatenated,"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "denotes as gc\ni ."
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "gc\ni = [gv\ni , ga\ni ]"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "This means that we use a special type of neural network that"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "can capture the temporal patterns and dependencies of the"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "features over time. The TCN takes the input feature vector"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "and applies a series of convolutional\nlayers with different"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "kernel sizes and dilation rates to produce an output feature"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "vector. The output feature vector has the same length as the"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "input feature vector but contains more information about the"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "temporal context. For example, the TCN can learn how the"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "sound and image change over time in each segment of the"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "video. The output feature vectors for both sound and image"
        },
        {
          "Visual Eeatures": "are then combined together by concatenating them along a"
        },
        {
          "Visual Eeatures": ""
        },
        {
          "Visual Eeatures": "dimension. This creates a new feature vector that contains"
        },
        {
          "Visual Eeatures": "both audio and visual\ninformation for each segment of the"
        },
        {
          "Visual Eeatures": "video."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lows:": "yi = MLP(hi)",
          "predictions by taking the logarithm of the predicted proba-": "bilities. The lower the loss, the better the model."
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "AU challenge: We employ BCEWithLogitsLoss as the"
        },
        {
          "lows:": "where yi are the predictions of i-th segment. For VA chal-",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "loss function, which integrates a sigmoid layer and binary"
        },
        {
          "lows:": "lenge, yi ∈ Rl×2. For EXPR challenge, yi ∈ Rl×8. For AU",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "cross-entropy, which is defined as in Eq 4."
        },
        {
          "lows:": "challenge, yi ∈ Rl×12 .",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "The prediction vector contains the values that we want to",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "estimate for each segment. The MLP consists of several lay-",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "1 N\n(cid:88) i\n[yi · log(σ(xi)) + (1 − yi) · log(1 − σ(xi))]\nLAU = −"
        },
        {
          "lows:": "ers of neurons that can learn non-linear transformations of",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "the input. The MLP can be trained to minimize the error be-",
          "predictions by taking the logarithm of the predicted proba-": "(4)"
        },
        {
          "lows:": "tween the prediction vector and the ground truth vector. The",
          "predictions by taking the logarithm of the predicted proba-": "is the target label for\nwhere N is the number of samples, yi"
        },
        {
          "lows:": "ground truth vector is the actual values that we want to pre-",
          "predictions by taking the logarithm of the predicted proba-": "is the input\nlogits for sample i, σ is the sig-\nsample i, xi"
        },
        {
          "lows:": "dict for each segment. Depending on what kind of challenge",
          "predictions by taking the logarithm of the predicted proba-": "moid function The advantage of using BCEWithLogitsLoss"
        },
        {
          "lows:": "we are solving, we have different types of ground truth vec-",
          "predictions by taking the logarithm of the predicted proba-": "over BCELoss with sigmoid is that\nit can avoid numerical"
        },
        {
          "lows:": "tors and prediction vectors. For the VA challenge, we want",
          "predictions by taking the logarithm of the predicted proba-": "instability and improve performance."
        },
        {
          "lows:": "to predict\ntwo values: valence and arousal. Valence mea-",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "sures how positive or negative an emotion is. Arousal mea-",
          "predictions by taking the logarithm of the predicted proba-": "4. Experiments and Results"
        },
        {
          "lows:": "sures how active or passive an emotion is.\nFor\nthe EXPR",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "4.1. Experiments Settings"
        },
        {
          "lows:": "challenge, we want to predict eight values: one for each ba-",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "sic expression (anger, disgust, fear, happiness, sadness, and",
          "predictions by taking the logarithm of the predicted proba-": "All models are trained on an Nvidia GeForce GTX 3090"
        },
        {
          "lows:": "surprise) plus neutral and other expressions.\nFor\nthe AU",
          "predictions by taking the logarithm of the predicted proba-": "GPU which has 24GB of memory. We use AdamW opti-"
        },
        {
          "lows:": "challenge, we want\nto predict\ntwelve values: one for each",
          "predictions by taking the logarithm of the predicted proba-": "mizer and cosine learning rate schedule with the first epoch"
        },
        {
          "lows:": "action unit\n(AU1, AU2, AU4, AU6, AU7, AU10, AU12,",
          "predictions by taking the logarithm of the predicted proba-": "warmup. The learning rate is 3e − 5,\nthe weight decay is"
        },
        {
          "lows:": "AU15, AU23, AU24, AU25, AU26).",
          "predictions by taking the logarithm of the predicted proba-": "1e − 5, the dropout prob is 0.3, and the batch size is 32."
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "For VA Challenge, we\nuse Wav2Vec2-emotion,\nEff,"
        },
        {
          "lows:": "3.6. Loss Functions",
          "predictions by taking the logarithm of the predicted proba-": "RAF-DB, and AffectNet8 as the input features."
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "For EXPR Challenge, we use two types of input features:"
        },
        {
          "lows:": "VA challenge: We use the Concordance Correlation Co-",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "Eff and AffectNet8 as described above."
        },
        {
          "lows:": "efficient\n(CCC) between the predictions\nand the ground",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "For AU Challenge, we use three types of input features:"
        },
        {
          "lows:": "truth labels as\nthe measure, which is defined as\nin Eq 1.",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "Eff, RAF-DB, and AffectNet8 as described above."
        },
        {
          "lows:": "It measures the correlation between two sequences x and y",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "For all three challenges, we split videos using a segment"
        },
        {
          "lows:": "and ranges between -1 and 1, where -1 means perfect anti-",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "window w = 300 and a stride s = 200. This means we di-"
        },
        {
          "lows:": "correlation, 0 means no correlation, and 1 means perfect",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "vide each video into segments of 300 frames with an overlap"
        },
        {
          "lows:": "correlation. The loss is calculated as Eq 2.",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "of 100 frames between consecutive segments.\nThis helps"
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "us capture the temporal dynamics of facial expressions and"
        },
        {
          "lows:": "2 ∗ cov(x, y)",
          "predictions by taking the logarithm of the predicted proba-": "emotions."
        },
        {
          "lows:": "CCC(x, y) =",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "σ2\nx + σ2\ny + (µx − µy)2",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "(1)",
          "predictions by taking the logarithm of the predicted proba-": "4.2. Overall Results"
        },
        {
          "lows:": "(cid:88)",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "where cov(x, y) =\n(x − µx) ∗ (y − µy)",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "Table 1 displays the experimental results of our proposed"
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "method on the validation set of\nthe VA, EXPR, and AU"
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "Challenge, where the Concordance Correlation Coefficient"
        },
        {
          "lows:": "(2)\nLVA = 1 − CCC",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "(CCC) is utilized as the evaluation metric for both valence"
        },
        {
          "lows:": "EXPR challenge: We use the cross-entropy loss as the",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "and arousal prediction, and F1-score is used to evaluate the"
        },
        {
          "lows:": "loss function, which is defined as in Eq 3.",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "result of EXPR and AU challenge. As demonstrated in the"
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "table, our proposed method outperforms the baseline sig-"
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "nificantly. These results show that our proposed approach"
        },
        {
          "lows:": "1 N\n(cid:88) i\nM(cid:88) c\n(3)\nyic log(pic)\nLEXPR = −",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "using TCN and Transformer-based model effectively inte-"
        },
        {
          "lows:": "=1",
          "predictions by taking the logarithm of the predicted proba-": ""
        },
        {
          "lows:": "",
          "predictions by taking the logarithm of the predicted proba-": "grates visual and audio information for improved accuracy"
        },
        {
          "lows:": "is a binary indicator (0 or 1) if class c is the\nwhere yic",
          "predictions by taking the logarithm of the predicted proba-": "in recognizing emotions on this dataset."
        },
        {
          "lows:": "correct classification for observation i. pic is the predicted",
          "predictions by taking the logarithm of the predicted proba-": "Table 2, Table 3, and Table 4 display the overall\ntest re-"
        },
        {
          "lows:": "probability of observation i being in class c, M is the num-",
          "predictions by taking the logarithm of the predicted proba-": "sults on the three challenges. Notably, Netease Fuxi and"
        },
        {
          "lows:": "ber of classes. The multiclass cross entropy loss function",
          "predictions by taking the logarithm of the predicted proba-": "SituTech achieved the first and second highest scores in all"
        },
        {
          "lows:": "measures how well a model predicts the true probabilities",
          "predictions by taking the logarithm of the predicted proba-": "three challenges,\nsurpassing other\nteams significantly,\nin-"
        },
        {
          "lows:": "of each class for a given observation.\nIt penalizes wrong",
          "predictions by taking the logarithm of the predicted proba-": "dicating their exceptional performance in these challenges."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 6: , it can be seen that almost",
      "data": [
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "CCC-V"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.6193"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.6486"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.5526"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.5008"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.5234"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.4818"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.4622"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.5043"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.4578"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.3245"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": "0.211"
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": ""
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": ""
        },
        {
          "Table 1. Performance of our method on the validation dataset of three experiments": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 6: , it can be seen that almost",
      "data": [
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "ACCC\n0.3776"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "Table 2. The overall\ntest results on VA challenge. The bold fonts",
          "USC IHP\n0.4292": "baseline\n0.365"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "indicate the best results.",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "Table 4. The overall\ntest results on AU challenge. The bold fonts"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "indicate the best results."
        },
        {
          "baseline\n0.201\n0.211\n0.191": "Teams\nF1",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "4.3. Ablation Study"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "0.4121\nNetease Fuxi",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "In this section, we perform several ablation studies on"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "SituTech\n0.4072",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "these three experiments to compare the contribution of dif-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "Ours\n0.3532",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "ferent\nfeatures.\nFrom Table 6,\nit can be seen that almost"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "HFUT-MAC\n0.3337",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "every feature contributes to the VA prediction task, and the"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "HSE-NN-SberAI\n0.3292",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "combination of 4 visual features: Eff, ArcFace, AffectNet8,"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "AlphaAff\n0.3218",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "RAF-DB, and the audio features: Wav2Vec2-emotion reach"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "USTC-IAT-United\n0.3075",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "the highest CCC score on VA experiment. Table 7 shows"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "SSSIHL DMACS\n0.3047",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "that the use of Eff and AffectNet8 can reach the highest F1-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "SCLAB CNU\n0.2949",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "score in the EXPR experiments.\nTable 8 shows that Eff,"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "Wall Lab\n0.2913",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "AffectNet8, and RAF-DB can reach the highest F1-score in"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "ACCC\n0.2846",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "the EXPR and AU experiments. The cross-validation result"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "RT IAI\n0.2834",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "of the VA, EXPR, and AU experiments are reported in Ta-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "DGU-IPL\n0.2278",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "ble 5. Fold 0 is exactly the original data from the ABAW"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "baseline\n0.2050",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "dataset."
        },
        {
          "baseline\n0.201\n0.211\n0.191": "Table 3.\nThe overall\ntest\nresults on EXPR challenge.\nThe bold",
          "USC IHP\n0.4292": ""
        },
        {
          "baseline\n0.201\n0.211\n0.191": "fonts indicate the best results.",
          "USC IHP\n0.4292": "5. Conclusion"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "Our proposed approach utilizes a combination of a Tem-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "poral Convolutional Network (TCN)\nand a Transformer-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "",
          "USC IHP\n0.4292": "based model\nto integrate visual and audio information for"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "Our\nteam ranks\nfourth in the VA challenge,\nthird in the",
          "USC IHP\n0.4292": "improved accuracy in recognizing emotions. The TCN cap-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "EXPR challenge, and sixth in the AU challenge. Our team’s",
          "USC IHP\n0.4292": "tures\nrelationships\nat\nlow-,\nintermediate-,\nand high-level"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "performance demonstrates our competitive standing in the",
          "USC IHP\n0.4292": "time scales, while the Transformer mechanism merges au-"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "challenges, with notable achievements\nin the VA, EXPR,",
          "USC IHP\n0.4292": "dio and visual\nfeatures. We conducted our experiment on"
        },
        {
          "baseline\n0.201\n0.211\n0.191": "and AU challenges.",
          "USC IHP\n0.4292": "the Aff-Wild2 dataset, which is a widely used benchmark"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 7: Ablation study of features on the validation dataset of Table8.AblationstudyoffeaturesonthevalidationdatasetofAU",
      "data": [
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.5505",
          "Fold 1": "0.6455",
          "Fold 2": "0.5889",
          "Fold 3": "0.5394",
          "Fold 4": "0.5406"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.24",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "Valence",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.5504",
          "Fold 1": "0.4979",
          "Fold 2": "0.5008",
          "Fold 3": "0.4979",
          "Fold 4": "0.4875"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Test",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.211",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "CCC",
          "Partition": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.6809",
          "Fold 1": "0.6259",
          "Fold 2": "0.6539",
          "Fold 3": "0.6468",
          "Fold 4": "0.6591"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.20",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "Arousal",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.5805",
          "Fold 1": "0.5396",
          "Fold 2": "0.6325",
          "Fold 3": "0.5037",
          "Fold 4": "0.5569"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Test",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.191",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.4138",
          "Fold 1": "0.4350",
          "Fold 2": "0.3614",
          "Fold 3": "0.3959",
          "Fold 4": "0.4234"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.23",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "EXPR",
          "Evaluation Metric": "F1-score",
          "Partition": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.3406",
          "Fold 1": "0.2979",
          "Fold 2": "0.3532",
          "Fold 3": "0.3293",
          "Fold 4": "0.3427"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Test",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.2050",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.5248",
          "Fold 1": "0.5524",
          "Fold 2": "0.5000",
          "Fold 3": "0.5060",
          "Fold 4": "0.5393"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Validation",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.39",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        },
        {
          "Task": "AU",
          "Evaluation Metric": "F1-score",
          "Partition": "",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Ours",
          "Fold 0": "0.4735",
          "Fold 1": "0.4822",
          "Fold 2": "0.4887",
          "Fold 3": "0.4818",
          "Fold 4": "0.4720"
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "Test",
          "Method": "",
          "Fold 0": "",
          "Fold 1": "",
          "Fold 2": "",
          "Fold 3": "",
          "Fold 4": ""
        },
        {
          "Task": "",
          "Evaluation Metric": "",
          "Partition": "",
          "Method": "Baseline",
          "Fold 0": "0.365",
          "Fold 1": "-",
          "Fold 2": "-",
          "Fold 3": "-",
          "Fold 4": "-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 7: Ablation study of features on the validation dataset of Table8.AblationstudyoffeaturesonthevalidationdatasetofAU",
      "data": [
        {
          "Baseline\n0.365\n-": "Table 5. Results for the five folds of three tasks",
          "-": ""
        },
        {
          "Baseline\n0.365\n-": "Audio Features",
          "-": "Arousal"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6054"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6629"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6579"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6467"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6519"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6532"
        },
        {
          "Baseline\n0.365\n-": "None",
          "-": "0.6613"
        },
        {
          "Baseline\n0.365\n-": "Wav2Vec2-emotion",
          "-": "0.6809"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 7: Ablation study of features on the validation dataset of Table8.AblationstudyoffeaturesonthevalidationdatasetofAU",
      "data": [
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "Audio Features"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "None"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "Wav2Vec2-emotion"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": "features on the validation dataset of"
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": ""
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": ""
        },
        {
          "Table 6. Ablation study of features on the validation dataset of VA experiment.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "puter Vision, pages 3652–3660, 2021. 1"
        },
        {
          "References": "[1] Xiang An, Jiangkang Deng, Jia Guo, Ziyong Feng, Xuhan",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[15]\nFelix Kuhnke, Lars Rumberg, and J¨orn Ostermann.\nTwo-"
        },
        {
          "References": "Zhu, Yang Jing, and Liu Tongliang. Killing two birds with",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "stream aural-visual affect analysis in the wild.\nIn 2020 15th"
        },
        {
          "References": "one stone: Efficient and robust\ntraining of face recognition",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "IEEE International Conference on Automatic Face and Ges-"
        },
        {
          "References": "the IEEE Conference\ncnns by partial fc.\nIn Proceedings of",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "ture Recognition (FG 2020), pages 600–605. IEEE, 2020. 2"
        },
        {
          "References": "on Computer Vision and Pattern Recognition, 2022. 2",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[16] Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager."
        },
        {
          "References": "[2]\nJiankang Deng,\nJia Guo,\nNiannan Xue,\nand\nStefanos",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Temporal convolutional networks: A unified approach to ac-"
        },
        {
          "References": "Zafeiriou. Arcface: Additive angular margin loss for deep",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "tion segmentation.\nIn Computer Vision–ECCV 2016 Work-"
        },
        {
          "References": "the\nIEEE/CVF con-\nface\nrecognition.\nIn Proceedings of",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "shops: Amsterdam, The Netherlands, October 8-10 and 15-"
        },
        {
          "References": "ference on computer vision and pattern recognition, pages",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "16, 2016, Proceedings, Part\nIII 14, pages 47–54. Springer,"
        },
        {
          "References": "4690–4699, 2019. 2",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "2016. 2"
        },
        {
          "References": "[3]\nJin Fan, Ke Zhang, Yipan Huang, Yifei Zhu, and Baiping",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[17] Geethu Miriam Jacob and Bj¨orn Stenger. Facial action unit"
        },
        {
          "References": "Chen. Parallel spatio-temporal attention-based tcn for multi-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "detection with transformers.\nIn 2021 IEEE/CVF Conference"
        },
        {
          "References": "variate time series prediction. Neural Computing and Appli-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "on Computer Vision and Pattern Recognition (CVPR), pages"
        },
        {
          "References": "cations, pages 1–10, 2021. 2",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "7676–7685, 2021. 2"
        },
        {
          "References": "[4] Yue\nJin, Tianqing Zheng, Chao Gao,\nand Guoqiang Xu.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[18] Dung Nguyen, Duc Thanh Nguyen, Rui Zeng, Thanh Thi"
        },
        {
          "References": "A multi-modal\nand multi-task\nlearning method\nfor\nac-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Nguyen, Son N Tran, Thin Nguyen, Sridha Sridharan, and"
        },
        {
          "References": "arXiv\npreprint\ntion\nunit\nand\nexpression\nrecognition.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Clinton Fookes. Deep auto-encoders with sequential\nlearn-"
        },
        {
          "References": "arXiv:2107.04187, 2021. 2",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "IEEE\ning for multimodal dimensional emotion recognition."
        },
        {
          "References": "[5] Dimitrios\nKollias.\nAbaw:\nLearning\nfrom synthetic",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Transactions on Multimedia, 24:1313–1324, 2021. 2"
        },
        {
          "References": "arXiv\npreprint\ndata & multi-task\nlearning\nchallenges.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[19]\nJuan DS Ortega, Patrick Cardinal, and Alessandro L Koerich."
        },
        {
          "References": "arXiv:2207.01138, 2022. 1",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Emotion recognition using fusion of audio and video fea-"
        },
        {
          "References": "[6] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "tures.\nIn 2019 IEEE International Conference on Systems,"
        },
        {
          "References": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Man and Cybernetics (SMC), pages 3847–3852. IEEE, 2019."
        },
        {
          "References": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "1"
        },
        {
          "References": "ference on Computer Vision and Pattern Recognition, pages",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[20] O. M. Parkhi, A. Vedaldi,\nand A. Zisserman.\nDeep face"
        },
        {
          "References": "2328–2336, 2022.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "recognition.\nIn British Machine Vision Conference, 2015."
        },
        {
          "References": "[7] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "2"
        },
        {
          "References": "affective behavior\nin the first abaw 2020 competition.\nIn",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[21]\nSrinivas Parthasarathy and Shiva Sundaram. Detecting ex-"
        },
        {
          "References": "2020\n15th\nIEEE International Conference\non Automatic",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "pressions with multimodal transformers.\nIn 2021 IEEE Spo-"
        },
        {
          "References": "Face and Gesture Recognition (FG 2020)(FG), pages 794–",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "ken Language Technology Workshop (SLT), pages 636–643."
        },
        {
          "References": "800, 2020.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "IEEE, 2021. 2"
        },
        {
          "References": "[8] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[22] Leonardo Pepino, Pablo Riera, and Luciana Ferrer.\nEmo-"
        },
        {
          "References": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "tion recognition from speech using wav2vec 2.0 embeddings."
        },
        {
          "References": "arXiv preprint\nfect and action units\nin a single network.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv preprint arXiv:2104.03502, 2021. 2"
        },
        {
          "References": "arXiv:1910.11111, 2019.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[23] Andrey V. Savchenko. Video-based frame-level facial anal-"
        },
        {
          "References": "[9] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "ysis of affective behavior on mobile devices using efficient-"
        },
        {
          "References": "Zafeiriou.\nDistribution matching for heterogeneous multi-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "nets.\nIn Proceedings of the IEEE/CVF Conference on Com-"
        },
        {
          "References": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "puter Vision and Pattern Recognition (CVPR) Workshops,"
        },
        {
          "References": "arXiv:2105.03790, 2021.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "pages 2359–2366, June 2022. 2"
        },
        {
          "References": "[10] Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[24] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models"
        },
        {
          "References": "Cowen, and Stefanos Zafeiriou. Abaw: Valence-arousal esti-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "and faster training.\nIn International conference on machine"
        },
        {
          "References": "mation, expression recognition, action unit detection & emo-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "learning, pages 10096–10106. PMLR, 2021. 2"
        },
        {
          "References": "tional reaction intensity estimation challenges, 2023.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[25]\nPanagiotis Tzirakis,\nJiaxin Chen, Stefanos Zafeiriou,\nand"
        },
        {
          "References": "[11] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Bj¨orn Schuller.\nEnd-to-end multimodal affect\nrecognition"
        },
        {
          "References": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "in real-world environments.\nInformation Fusion, 68:46–53,"
        },
        {
          "References": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "2021. 2"
        },
        {
          "References": "in-the-wild: Aff-wild database and challenge, deep architec-",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[26]\nPanagiotis Tzirakis, George Trigeorgis, Mihalis A Nico-"
        },
        {
          "References": "tures, and beyond. International Journal of Computer Vision,",
          "ings of\nthe IEEE/CVF International Conference on Com-": "laou, Bj¨orn W Schuller,\nand Stefanos Zafeiriou.\nEnd-to-"
        },
        {
          "References": "pages 1–23, 2019.",
          "ings of\nthe IEEE/CVF International Conference on Com-": "end multimodal emotion recognition using deep neural net-"
        },
        {
          "References": "[12] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "works.\nIEEE Journal of selected topics in signal processing,"
        },
        {
          "References": "action unit\nrecognition: Aff-wild2, multi-task learning and",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "11(8):1301–1309, 2017. 2"
        },
        {
          "References": "arcface. arXiv preprint arXiv:1910.04855, 2019. 1",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-"
        },
        {
          "References": "[13] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia"
        },
        {
          "References": "in-the-wild: Valence-arousal, expressions, action units and a",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Polosukhin. Attention is all you need. Advances in neural"
        },
        {
          "References": "unified framework. arXiv preprint arXiv:2103.15792, 2021.",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "information processing systems, 30, 2017. 2"
        },
        {
          "References": "[14] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "[28] Manh Tu Vu, Marie Beurton-Aimar, and Serge Marchand."
        },
        {
          "References": "tive behavior in the second abaw2 competition.\nIn Proceed-",
          "ings of\nthe IEEE/CVF International Conference on Com-": ""
        },
        {
          "References": "",
          "ings of\nthe IEEE/CVF International Conference on Com-": "Multitask multi-database emotion recognition.\nIn Proceed-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "puter Vision, pages 3637–3644, 2021. 1"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "[29] Zhengyao Wen, Wenzhong Lin, Tao Wang,\nand Ge Xu."
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "Distract\nyour\nattention:\nMulti-head\ncross\nattention\nnet-"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv\npreprint\nwork\nfor\nfacial\nexpression\nrecognition."
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "arXiv:2109.07270, 2021. 2"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "[30]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "and Pattern Recognition Workshops\nIn Computer Vision"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "IEEE, 2017. 1"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "[31] Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, and Shiguang"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "Shan. M 3 f: Multi-modal continuous valence-arousal esti-"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "mation in the wild. In 2020 15th IEEE International Confer-"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "ence on Automatic Face and Gesture Recognition (FG 2020),"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "pages 632–636. IEEE, 2020. 2"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "[32] Zengqun Zhao and Qingshan Liu.\nFormer-dfer: Dynamic"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "facial expression recognition transformer.\nIn Proceedings"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "of\nthe 29th ACM International Conference on Multimedia,"
        },
        {
          "ings of\nthe IEEE/CVF International Conference on Com-": "pages 1553–1561, 2021. 2"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Killing two birds with one stone: Efficient and robust training of face recognition cnns by partial fc",
      "authors": [
        "Xiang An",
        "Jiangkang Deng",
        "Jia Guo",
        "Ziyong Feng",
        "Xuhan Zhu",
        "Yang Jing",
        "Liu Tongliang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Niannan Xue",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Parallel spatio-temporal attention-based tcn for multivariate time series prediction",
      "authors": [
        "Jin Fan",
        "Ke Zhang",
        "Yipan Huang",
        "Yifei Zhu",
        "Baiping Chen"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "4",
      "title": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "authors": [
        "Jin Yue",
        "Tianqing Zheng",
        "Chao Gao",
        "Guoqiang Xu"
      ],
      "year": "2021",
      "venue": "A multi-modal and multi-task learning method for action unit and expression recognition",
      "arxiv": "arXiv:2107.04187"
    },
    {
      "citation_id": "5",
      "title": "Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias",
        "Abaw"
      ],
      "year": "2022",
      "venue": "Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "6",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "8",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "11",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "13",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "14",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceed-ings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jörn Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "16",
      "title": "Temporal convolutional networks: A unified approach to action segmentation",
      "authors": [
        "Colin Lea",
        "Rene Vidal",
        "Austin Reiter",
        "Gregory Hager"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016 Workshops"
    },
    {
      "citation_id": "17",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Björn Jacob",
        "Stenger"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "18",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "Dung Nguyen",
        "Thanh Duc",
        "Rui Nguyen",
        "Thanh Zeng",
        "Son Nguyen",
        "Thin Tran",
        "Sridha Nguyen",
        "Clinton Sridharan",
        "Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "Juan Ds Ortega",
        "Patrick Cardinal",
        "Alessandro Koerich"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)"
    },
    {
      "citation_id": "20",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "21",
      "title": "Detecting expressions with multimodal transformers",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "23",
      "title": "Video-based frame-level facial analysis of affective behavior on mobile devices using efficientnets",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "24",
      "title": "Efficientnetv2: Smaller models and faster training",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "25",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiaxin Chen",
        "Stefanos Zafeiriou",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "26",
      "title": "End-toend multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "27",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Multitask multi-database emotion recognition",
      "authors": [
        "Tu Manh",
        "Marie Vu",
        "Serge Beurton-Aimar",
        "Marchand"
      ],
      "year": "2021",
      "venue": "Proceed-ings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "authors": [
        "Zhengyao Wen",
        "Wenzhong Lin",
        "Tao Wang",
        "Ge Xu"
      ],
      "year": "2021",
      "venue": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "arxiv": "arXiv:2109.07270"
    },
    {
      "citation_id": "30",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "31",
      "title": "M 3 f: Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Yuan-Hang Zhang",
        "Rulin Huang",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "32",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    }
  ]
}