{
  "paper_id": "2202.08974v1",
  "title": "Multimodal Emotion Recognition Using Transfer Learning From Speaker Recognition And Bert-Based Models",
  "published": "2022-02-16T00:23:42Z",
  "authors": [
    "Sarala Padi",
    "Seyed Omid Sadjadi",
    "Dinesh Manocha",
    "Ram D. Sriram"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic emotion recognition plays a key role in computer-human interaction as it has the potential to enrich the next-generation artificial intelligence with emotional intelligence. It finds applications in customer and/or representative behavior analysis in call centers, gaming, personal assistants, and social robots, to mention a few. Therefore, there has been an increasing demand to develop robust automatic methods to analyze and recognize the various emotions. In this paper, we propose a neural network-based emotion recognition framework that uses a late fusion of transfer-learned and fine-tuned models from speech and text modalities. More specifically, we i) adapt a residual network (ResNet) based model trained on a large-scale speaker recognition task using transfer learning along with a spectrogram augmentation approach to recognize emotions from speech, and ii) use a fine-tuned bidirectional encoder representations from transformers (BERT) based model to represent and recognize emotions from the text. The proposed system then combines the Resnet and BERT-based model scores using a late fusion strategy to further improve the emotion recognition performance. The proposed multimodal solution addresses the data scarcity limitation in emotion recognition using transfer learning, data augmentation, and fine-tuning, thereby improving the generalization performance of the emotion recognition models. We evaluate the effectiveness of our proposed multimodal approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that both audio and text-based models improve the emotion recognition performance and that the proposed multimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As machines and computer-based applications and interactions are continuously progressing, and becoming a part of our daily lives, natural human-computer interaction (HCI) has become increasingly critical  [1, 2] . As a result, understanding and responding to the emotional state of individuals is a necessary step in natural HCI  [3] . Apart from this, there are other applications where automatic detection and classification of human emotion plays a critical role and can be useful in psychological and nano-physiological studies of human emotional expression. Similarly, emotion information plays an important role in automatic tutoring systems, call center services, gaming, personal assistants, to mention a few. Another potential application can be alerting drivers by detecting their stress or anger levels that could lead to major accidents and impair their driving capabilities  [4, 3] .\n\nThere are a number of modalities with which humans con-vey their emotions. Examples include speech, text, facial expressions, hand gestures, and so on. Although there are scenarios where facial expressions could be a preferred and more effective way of communicating emotions, speech and text data are abundantly available and more conveniently captured as compared to other modalities. In particular, speech plays a crucial role in conveying the emotional state of a human in the form of prosody and/or paralinguistic context. For speech emotion recognition (SER), traditionally, machine learning (ML) models were developed using engineered features such as melfrequency cepstral coefficients (MFCC), Chroma-based features, pitch, energy, entropy, and zero-crossing rate  [5, 6, 7] , to mention a few. However, the performance of such ML models depends on the type and diversity of the features used. Although it remains unclear which features correlate most with various emotions, the research is still ongoing to explore additional features and new algorithms to model the dynamics of feature streams representing human emotions. On the other hand, deep learning-based models can directly learn the task-relevant features from spectrograms or raw waveforms  [8, 9, 10, 11] , thereby obviating the need for extracting a large set of engineered features  [12] . Recent studies have proposed the use of convolutional neural network (CNN) models combined with long short-term memory (LSTM) built on spectrograms and raw waveforms, showing improved SER performance  [9, 10, 12, 13, 14, 15, 16] . However, building such complex systems requires large amounts of labeled training data. Also, insufficient labeled training data can potentially make the models overfit to specific data conditions and domains, resulting in poor generalization performance on unseen data conditions.\n\nAnother important modality for conveying and capturing emotions is text. To represent and model the textual data for solving natural language processing (NLP) related tasks, different approaches have been developed. The word and sentence embeddings have emerged as the most effective representation and have shown breakthroughs in improving the performance of deep learning models for various NLP applications. The variations of such embeddings include Word to Vector (Word2Vec)  [17] , Global Vectors (GloVe)  [18] , and bidirectional encoder representations from transformers (BERT)  [19] . Although these methods provide a compact representation of textual data, there are some limitations and challenges in effectively and efficiently leveraging these methods for various tasks. The main challenge is that building such models requires large amounts of data for the task at hand. To overcome this problem, transfer learning and fine-tuning the available pretrained models have shown promise. Nonetheless, the pretrained models are typically trained and tuned for out-of-domain tasks. Accordingly, the tweaked features may not perform well on target tasks such as emotion recognition, where the vocabulary used to extract the emotion is different from that used for the source task.\n\nThis paper presents a multimodal framework for emotion recognition from speech and text using transfer learning and fine-tuning of speaker ResNet and BERT-based models. This paper leverages the advantages of a BERT-based model combined with acoustic feature representations to improve emotion recognition performance. In particular, we use a BERT model to not only extract sentence embeddings from the text but also to fine-tune the model to improve the emotion recognition performance. In addition, to address the data scarcity limitations in speech-based emotion recognition, we use a transfer learning approach combined with a spectrogram augmentation strategy. Specifically, we re-purpose a ResNet  [20]  model developed for speaker recognition using large amounts of speaker labeled data and use it as a feature descriptor for SER. The model includes a statistics pooling layer that enables the processing of variablelength segments without a need for truncation. Also, we increase the training data size by generating more data samples using spectrogram augmentation  [21] . The proposed framework effectively leverages the complementary information from text and speech modalities and combines the scores produced to improve emotion recognition performance. We evaluate the effectiveness of our proposed systems on the interactive emotional dyadic motion capture (IEMOCAP) dataset  [22]  using speechonly, text-only, and multimodal settings and show that the fusion of the two complementary modalities results in state-ofthe-art emotion recognition performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In recent years, deep neural network based systems have shown a tremendous success in recognizing the emotions from the speech signal  [24, 16, 25] . Specifically, techniques such as bidirectional LSTMs (BLSTM)  [23, 15, 24, 25]  and time-delay neural networks (TDNN)  [26] , which can effectively model relatively long contexts compared to their DNN counterparts, have been successfully applied for SER. Nevertheless, as discussed previously, the lack of large amounts of carefully labeled data for building complex models for emotion classification remains a main challenge in SER  [27] . To address this, data augmentation methods are used to generate additional training data by perturbing, corrupting, mimicking, and masking the original data samples to enable the development of complex ML models. For example,  [16, 28, 29]  applied signal-based transformations such as speed perturbation, time-stretch, pitch shift, as well as added noise to original speech waveforms. One disadvantage of these approaches is that they require signal-level modifications, thereby increasing the computational complexity and storage requirements of the subsequent front-end processing. They can also lead to model overfitting due to potentially similar samples in the training set, while random balance can potentially remove useful information  [28] . Furthermore, retaining performance on relatively clean conditions has shown to be a challenge while using signal-based transformations  [29] .\n\nAnother effective way to address data scarcity is transfer learning  [30, 31, 32] . Transfer learning can leverage the information and knowledge learned from one related task and domain to another, as long as the input remains the same. Several studies have have shown that transfer learning approaches outperform prior methods in recognizing emotions even for unseen scenarios, individuals, and conditions  [33] . It was also shown that transfer learning increases the feature learning abilities which leads to improved SER performance  [34, 35, 36, 37] . However, transfer learning methods have not been fully explored and analyzed for emotion recognition. Particularly, it is unclear whether and how ML models trained for other datarich speech applications such as speaker recognition would perform for SER. We recently proposed an SER framework based on transfer learning from speaker recognition along with data augmentation  [38]  which achieved promising improvements in SER performance on the IEMOCAP. In this study, we explore the impact of full network fine-tuning (as opposed to only training the fully connected classification layers) combined with transfer learning on SER performance.\n\nAs for text based approaches, word embeddings have emerged as the de facto representation in many natural language processing tasks. The most widely used word embedding models are Word2Vec  [17]  and GloVe  [18] . Both these models are unsupervised and have shown great success for various NLP tasks including sentiment analysis, document indexing, and topic model analysis. However, a major limitation of these models is that the word order is not taken into account which leads to a loss of syntactic and semantic understanding of words in sentences. In addition, fine-tuning these word embeddings for different target tasks and applications is not feasible. To circumvent these limitations, BERT models are used for extracting representations from text data to capture the context. Several studies have successfully combined BERT-based embeddings with the speech-based representations to improve the emotion recognition performance  [39, 40, 26] . However, BERT-based embeddings are context dependent and the representations extracted using the pretrained models may not capture the salient domain-dependent information for emotion recognition. This study investigates fine-tuning of a BERT model for text-based emotion recognition\n\nIn recent years, ML approaches have been explored where models are trained using multiple modalities and scores are combined by adding an extra layer (i.e., a meta learning layer) either by concatenating (stacking) the embeddings or scores, or using a weighted sum of the embeddings or scores  [41, 42, 43] . Multimodal processing and recognition have shown promise in many practical machine learning applications  [44, 45] . Since spoken data is composed of audio and text content, prior studies have also investigated the combination of acoustic features/representations and linguistic information for emotion recognition. Particularly, multimodal emotion recognition models have been developed from audio, video, images, or textual data. The models have been combined either at the feature/representation level (aka the early fusion) or at the decision/score level (aka the late fusion)  [46, 26, 39] . In this study, we investigate the latter within a multimodal emotion recognition framework using speech and text models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Multimodal Framework",
      "text": "Figure  1  shows the block diagram of the proposed multimodal emotion recognition framework. The following subsections provide a brief description of the speech and text-based models and systems used in this framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Text System",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bert",
      "text": "BERT is a language model trained on large amounts of text data that has achieved state-of-art results on many NLP tasks. Because of its bidirectionality and use of transformers with the attention mechanism and positional encoding  [47] , the BERT model learns the contextual information from input sentences. The BERT models are context-dependent. So, to use these models for downstream tasks, fine-tuning is required to generate the vectors/embeddings based on the word context. Unlike GloVe, it represents the input as subwords and learns sub-word embeddings (as opposed to whole word embeddings). By using the sub-word representation, the BERT approach overcomes the out of vocabulary limitation of character (ELMO)  [48]  and wordbased representations (GloVe or Word2Vec)  [17, 18] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transfer Learning",
      "text": "In the BERT setup, preprocessing of the input text is performed to generate tokens and are mapped to indices. As shown in Figure  1 , there are two additional tokens called the [CLS] classification token and the [SEP] separate segment token, which are appended at the beginning and end of each sentence, respectively. First, the embedding layer receives a list of input tokens and generates word embeddings for each token by adding positional embeddings to maintain the word order in a sequence. Next, the output of the embedding layer is fed to the multi-head self-attention sublayer to model temporal dependencies. Attention masks are used to exclude the paddings in sequences from attention weight calculations. Finally, we extend the BERT-base model by adding a new classification head, including randomly initialized fully connected (FC) layers with a softmax to get the prediction probabilities for the various emotions. We also finetune the network parameters on the training set for the emotion recognition task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech System",
      "text": "Although SER systems traditionally used a large set of lowlevel time-and frequency-domain features to capture and represent the various emotions in speech, in recent years, many state-of-the-art SER systems have used complex neural network models that learn directly from spectrograms or even raw waveforms. In this study, we build and explore an end-to-end ResNet  [20]  based system using log-mel spectrograms as input features. ResNet models originally, developed for computer vi-sion applications  [20] , have recently gained interest in speech applications such as speaker recognition  [49] . We extract highresolution spectrograms to enable the model to learn the spectral envelope and the coarse harmonic structures for the various emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Transfer Learning",
      "text": "As noted previously, transfer learning is an ML method where a model initially developed for one task or domain is re-purposed, partly or entirely, for a different but related task/domain. It has recently gained interest in SER  [30] . In this study, we repurpose a model initially developed for speaker recognition to serve as a feature descriptor for SER. We first train a ResNet34 model on large amounts of speaker-labeled audio data, and then we replace the FC layers of the pre-trained model with new randomly initialized FC layers. Finally, we 1) only re-train the new FC layers for an SER task on the IEMOCAP dataset ( linear probing  [50] ), and 2) re-train the FC layers and fine-tune the convolutional layers using two different learning rates. As shown in Figure  1 , the proposed system employs a statistics pooling layer  [51]  that aggregates the frame-level information over time and reduces the sequence of frames to a single vector by concatenating the mean and standard deviation computed over frames. The convolutional layers in the ResNet model work at the frame level, while the FC layers work at the segment level.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spectrogram Augmentation",
      "text": "There has been recent success in applying a computationally efficient data augmentation strategy, termed spectrogram augmentation, for speech recognition tasks  [21] . The spectrogram augmentation technique generates additional training data samples by applying random time-frequency masks to spectrograms to mitigate the over-fitting issue and improve the generalization of speech recognition models. Motivated by promising results seen with the spectrogram augmentation in the speech recognition field, we augment the training data using spectrotemporally modified versions of the original spectrograms. Be-cause the time-frequency masks are applied directly to spectrograms, we can conveniently apply the augmentation on the fly by eliminating the need to create and store new data files. Similar to the approach taken in  [21] , we consider two policies, conservative and aggressive, to generate spectrogram augmentations for SER. The parameter settings for the two spectrogram augmentation policies are like  [38] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "To leverage the complementary information captured by the two modalities, we use a weighted average of the scores (S speech and Stext) obtained from the speech and text models as follows:\n\nThe scores are calculated by applying the natural logarithm to outputs of the softmax layer (i.e., class posteriors). Here, w1 and w2 are fixed weights assigned to the text and speech modalities, respectively. We constrain the weights to sum to 1, i.e., w2 = 1 -w1. The weights determine the degree to which each modality contributes to the final score. For emotion classification, we select the emotion category with the highest score.\n\nWe investigate two approaches for finding the fusion parameters. In the first approach, we directly use the raw scores from each modality and search for the best setting for w1 on a hold-out set (w2 is simply 1 -w1). In the second approach, we first pre-processed the scores for each modality to have zero mean and unit variance as follows:\n\nwhere the normalization statistics µS and σS are calculated on a hold out set. The final fusion score is then obtained by averaging the normalized scores using w1 = w2 = 0.5 (i.e., equal weight fusion).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate the effectiveness of the proposed multimodal emotion recognition system on the IEMOCAP dataset  [22] , which contains improvised and scripted multimodal dyadic conversations between actors of opposite genders. It has 12 hours of speech data from 10 subjects and is pre-segmented into shortcuts. It includes nine categorical emotions and 3-dimensional labels. Our experiments include categorical emotions and speech segments with majority labeling, where at least two annotators agree in the annotation labeling. To replicate the experimental protocols used in many prior studies, we consider four categorical emotions: \"angry\", \"happy\", \"neutral\", and \"sad\" where we merged \"happy\" and \"excited\" into one category and the number of examples per category is 1103, 1636, 1708, and 1084, respectively. Table  1  summarizes the data statistics in the IEMOCAP dataset for the experimental setup considered in this study.\n\nThe IEMOCAP dataset comprises five sessions, and the speakers in the sessions are non-overlapping. Therefore, there are ten speakers in the dataset, i.e., five female and five male speakers. To conduct the experiments in a speakerindependent fashion, we use a leave-one-session-out (LOSO) cross-validation strategy, which results in 5 different train-test splits/folds. For each fold, we use the data from 4 sessions for training and the remaining session for model evaluation. Since the dataset is multi-label and imbalanced, besides the overall accuracy and termed weighted accuracy (WA), we report the average recall over the different emotion categories, termed unweighted accuracy (UA), to present our findings. To understand and visualize the performance of the proposed system within and across the various emotion categories, we compute and report confusion matrices for the experiments. We use similar setups for the speech-only, text-only, and combined model. For speech parameterization, we extract high resolution 128dimensional log-mel spectrograms from 25 ms frames at a 100 Hz frame rate (i.e., every 10 ms). For feature normalization, we apply a segment level mean and variance normalization  1  which is not ideal because we applied a typical normalization at the recording/conversation level. We have found that normalizing the segments using statistics computed at the conversation level significantly improves the SER performance on the IEMOCAP. However, this violates the independence assumption for the speech segments and did not consider in this study.\n\nFor the front-end processing, including feature extraction and feature normalization, we use the NIST speaker and language recognition evaluation (SLRE)  [52, 53]  toolkit. While training the model, we select T -frame chunks using random offsets over original speech segments, where T is randomly sampled from the set {150, 200, 250, 300} for each batch. We apply signal padding for speech segments shorter than T frames. While evaluating the model, we feed the entire duration of the test segments because the statistics pooling layer enables the model to consume variable-length inputs.\n\nAs noted previously, the proposed end-to-end SER system uses a pre-trained ResNet34 model built on a speaker recognition task. We train the ResNet34 model with a 512-dimensional embedding layer on millions of speech samples from over 7000 speakers available in the VoxCeleb corpus  [54] . To build the speaker recognition model, we apply the same front-end processing described above to extract high-resolution log-mel spectrograms from VoxCeleb data. We conduct experiments using models with and without transfer learning and spectrogram augmentation. For each original speech segment, we generate and augment two spectro-temporally modified versions according to the augmentation policies defined in  [38]  for both speaker and speech emotion recognition systems during training. We also evaluate our models with and without the statistics pooling layer to study their impact on the emotion recognition task. We use a categorical cross-entropy loss as the objective function to   [55]  63.5 -BERT+self-attention  [26]  58.53 59.20 Glove+self-attention  [26]  61.27 62.27 BERT  [39]  55.2 -BERT+CNN  [40]  66.1 67.0 Proposed 70.33 70.24",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech",
      "text": "BLSTM+transfer learnig  [34]  51.86 50.47 VGG19+GAN augmentation  [28]  54.6 -Recurrent encoder  [55]  54.6 -CNN+multi-task learning  [13]  -56.10 BLSTM+attention  [15]  58.8 63.5 CNN+transfer learning  [56]  59.54 -ResTDNN+attention  [26]  61.32 60.64 LSTM+attention  [46]  63.4 57.4 TDNN+attention  [26]  60.64 61.32 wav2vec-base  [57]  63.43 -wav2vec-large  [57]  65.64 -HuBERT-base  [57]  64.92 -HuBERT-large  [57]  67.62 -WavLM-base  [58]  65.94 -WavLM-large  [58]  70.03 -Proposed (w/o fine-tuning) 64.28 63.74 Proposed (w/ fine-tuning) 65.97 65.4",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech + Text",
      "text": "Recurrent encoder  [55]  71.8 -LSTM+attention+Concat  [46]  70.4 69.5 Audio+GloVe  [26]  65.53 66.43 BERT+CNN Fusion  [39]  65.1 -Wav2vec+CNN+BERT  [40]  73.0 73.5 Proposed (w1 = 0.94) 76.07 75.76 Proposed (w1 = w2 = 0.5) 75.97 75.01 train the models. The number of channels in the first block of the ResNet model is set to 32. Pytorch 2  is used for model implementation, the stochastic gradient descent (SGD) optimizer with momentum (0.9), and a batch size of 32. For transfer learning with frozen convolutional weights, an initial learning rate of 10 -1 is used. To fine-tune the convolutional layers, we use a learning rate of 10 -3 . The learning rate remains constant for the first 8 epochs and is halved for every other epoch. We use parametric rectified linear unit (PReLU) activation functions in all layers (except for the output) and utilize layer-wise batch normalization to accelerate the training process and improve the generalization properties of the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Parameter Settings For Text Model",
      "text": "The most commonly used pre-trained BERT models are BERTbase and BERT-large. The former has 12 encoders with 12 bidirectional self-attention heads, while the latter has 24 encoders with 16 bidirectional self-attention heads. These models were trained on unlabeled data collected from the BooksCorpus (800 million words) and English Wikipedia (2500 million words). In our experiments, we use a pretrained BERT-base model, specifically the BERT-base-uncased model with 12-layered transformer blocks with each block containing 12-head self-attention layers and 768 hidden outputs. This model has 110 million parameters in total. We use randomly initialized fully connected layers with 768 × 4 parameters along with softmax, and finetune the model for emotion recognition on the IEMOCAP using a categorical cross-entropy loss as the objective function, an Adam optimizer with a learning rate of 2 × 10 -5 , and a batch size of 32. We use the Pytorch framework for the BERT model training.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Evaluations",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "Table  2  presents the performance comparison of the proposed speech-only, text-only and multimodal systems with several prior approaches for the experimental setup described in Section 4. The fine-tuned BERT model outperforms prior textbased emotion recognition methods on the IEMOCAP with 70.33% UA and 70.24% WA, respectively. The fine-tuned ResNet34 model described in Section 3.2 performs favorably compared to all SER approaches considered in this paper, except for HuBERT-large and WavLM-large with nearly 300 million parameters that use head-to-toe fusion of embeddings from 24 encoder layers. The proposed SER system comfortably outperforms a system that uses 384 engineered features  [59] . We refer the reader to  [38]  for the results from the ablation study of the various components (i.e., transfer learning, data augmentation, statistics pooling) of the proposed SER system. In case of multimodal emotion recognition from speech and text, our proposed system outperforms prior methods and achieves stateof-the-art results using both fusion strategies. We note that combining the speech and text-based emotion recognition systems provides remarkable improvements with approximately 6% absolute improvement compared to the best performing unimodal system (76.07% vs 70.33% UA).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Error Analysis",
      "text": "To visualize the performance of the proposed system within and across different emotion categories, confusion matrices for the two modalities as well as their fusion are shown in Figure  2 .\n\nIt is observed from Figure  2 (a) and (b) that the system confuses the \"happy\" class (H+E) with the \"neutral\" class (N) quite often, while performing the best on the \"angry\" emotion (A). This is consistent with observations reported in other studies on IEMOCAP  [13, 55] . Our informal listening experiments confirm that the \"happy\" and \"neutral\" classes are indeed confusable emotion pairs in the IEMOCAP dataset. Combining the Resnet34 and BERT model scores further improves the performance. Moreover, we observe that multimodal fusion reduces the confusion between \"Neutral\" (N) and \"Happy\" (H+E) emotions, performing best for \"angry\" class and achieving accuracies of over 70% for the other three emotion categories.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross Comparisons",
      "text": "All studies referenced in     2  for comparison. The emotion recognition performance on the improvised portion is known to be better than that on the full dataset (e.g., see  [25, 24, 59, 34, 13] ). The latest HuBERT-base results reported in  [57]  uses a weighted average of embeddings from all transformer layers to achieve 67.62% UA on the IEMOCAP task. However, the first version 3 of the paper reported 62.94% UA only using the embeddings from the last layer. This is more comparable to our proposed SER system with 65.97% UA. Furthermore, from Table 3, we can notice that the HuBERT-large and WavLM-large models each have more than 300 million parameters as compared to our ResNet34 model with nearly 20 million parameters. Although HuBERT-large and WavLM-large models outperform 3 https://arxiv.org/pdf/2105.01051v1.pdf our SER system, these models are computationally expensive and require large amounts of data for self-supervised training.\n\nSimilarly for the text modality, our fine-tuned BERT model outperformed prior methods. Note that although  [26]  reported a performance of 71.22% UA, the experimental setup uses a context window [-3,3] including 3 preceding and succeeding sentences to generate the emotion prediction score for each sentence. This violates the independence assumption for the segments, hence it is not considered for comparison in this study. In addition, although this setup may be applicable to the IEMO-CAP where context is inferred from the data, it may not generalize well to other datasets.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "In this study, we presented a multimodal emotion recognition framework using transfer learning and fine-tuning of pre-trained speaker recognition and BERT models. Specifically, we repurposed a pre-trained ResNet model from speaker recognition that was trained using large amounts of speaker-labeled data. Further, we fine-tuned the pretrained ResNet34 model to extract features from high-resolution log-mel spectrograms to improve the speech emotion recognition performance. In addition, we adopted a spectrogram augmentation technique to generate additional training data samples by applying random timefrequency masks to log-mel spectrograms to mitigate overfitting and improve the generalization of emotion recognition models. We also explored a BERT-based emotion recognition system and fine-tuned the pretrained weights to improve the emotion recognition performance. We further improved the emotion recognition performance by fusing the complementary information available from speech and text modalities. We evaluated the proposed system using speech-only, text-only and multimodal settings and compared the performance of our system against that of several prior state-of-the-art studies. The proposed system consistently provided competitive performance across the two modalities as well as their fusion, achieving state-of-theart results using text and multimodal settings. Results from this study show that models trained for data-rich applications such as speaker recognition and language modeling can be repurposed using transfer learning to improve the emotion recognition performance under data scarcity constraints. In the future, we plan to extend our work to other datasets and other languages. We also plan to explore more efficient alternatives to network fine-tuning such as head-to-toe probing  [51] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Disclaimer",
      "text": "These results presented in this paper are not to be construed or represented as endorsements of any participant's system, methods, or commercial product, or as official findings on the part of NIST or the U.S. Government.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the block diagram of the proposed multimodal",
      "page": 2
    },
    {
      "caption": "Figure 1: Block diagram of the proposed multimodal emotion recognition framework.",
      "page": 3
    },
    {
      "caption": "Figure 1: , the proposed system employs a statistics",
      "page": 3
    },
    {
      "caption": "Figure 2: It is observed from Figure 2(a) and (b) that the system con-",
      "page": 5
    },
    {
      "caption": "Figure 2: Confusion matrices of the proposed approach for the speech-only, text-only, and multimodal experiments conducted in this",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Performance comparison of our proposed approach millionwords)andEnglishWikipedia(2500millionwords).In",
      "data": [
        {
          "Text": "",
          "Haiyang Xu et.al [46]\nRecurrent encoder [55]\nBERT+self-attention [26]\nGlove+self-attention [26]\nBERT [39]\nBERT+CNN [40]": "Proposed",
          "60.3\n63.5\n58.53\n61.27\n55.2\n66.1": "70.33",
          "54.8\n–\n59.20\n62.27\n–\n67.0": "70.24"
        },
        {
          "Text": "Speech",
          "Haiyang Xu et.al [46]\nRecurrent encoder [55]\nBERT+self-attention [26]\nGlove+self-attention [26]\nBERT [39]\nBERT+CNN [40]": "BLSTM+transfer learnig [34]\nVGG19+GAN augmentation [28]\nRecurrent encoder [55]\nCNN+multi-task learning [13]\nBLSTM+attention [15]\nCNN+transfer learning [56]\nResTDNN+attention [26]\nLSTM+attention [46]\nTDNN+attention [26]\nwav2vec-base [57]\nwav2vec-large [57]\nHuBERT-base [57]\nHuBERT-large [57]\nWavLM-base [58]\nWavLM-large [58]",
          "60.3\n63.5\n58.53\n61.27\n55.2\n66.1": "51.86\n54.6\n54.6\n–\n58.8\n59.54\n61.32\n63.4\n60.64\n63.43\n65.64\n64.92\n67.62\n65.94\n70.03",
          "54.8\n–\n59.20\n62.27\n–\n67.0": "50.47\n–\n–\n56.10\n63.5\n–\n60.64\n57.4\n61.32\n–\n–\n–\n–\n–\n–"
        },
        {
          "Text": "",
          "Haiyang Xu et.al [46]\nRecurrent encoder [55]\nBERT+self-attention [26]\nGlove+self-attention [26]\nBERT [39]\nBERT+CNN [40]": "Proposed (w/o ﬁne-tuning)\nProposed (w/ ﬁne-tuning)",
          "60.3\n63.5\n58.53\n61.27\n55.2\n66.1": "64.28\n65.97",
          "54.8\n–\n59.20\n62.27\n–\n67.0": "63.74\n65.4"
        },
        {
          "Text": "+ T\nSpeech\next",
          "Haiyang Xu et.al [46]\nRecurrent encoder [55]\nBERT+self-attention [26]\nGlove+self-attention [26]\nBERT [39]\nBERT+CNN [40]": "Recurrent encoder [55]\nLSTM+attention+Concat [46]\nAudio+GloVe [26]\nBERT+CNN Fusion [39]\nWav2vec+CNN+BERT [40]",
          "60.3\n63.5\n58.53\n61.27\n55.2\n66.1": "71.8\n70.4\n65.53\n65.1\n73.0",
          "54.8\n–\n59.20\n62.27\n–\n67.0": "–\n69.5\n66.43\n–\n73.5"
        },
        {
          "Text": "",
          "Haiyang Xu et.al [46]\nRecurrent encoder [55]\nBERT+self-attention [26]\nGlove+self-attention [26]\nBERT [39]\nBERT+CNN [40]": "Proposed (w1 = 0.94)\nProposed (w1 = w2 = 0.5)",
          "60.3\n63.5\n58.53\n61.27\n55.2\n66.1": "76.07\n75.97",
          "54.8\n–\n59.20\n62.27\n–\n67.0": "75.76\n75.01"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: forcompar- tion,weadoptedaspectrogramaugmentationtechniquetogen-",
      "data": [
        {
          "wav2vec-Base[57]\nwav2vec-Large[57]\nHuBERT-Base[57]\nHuBERT-Large [57]\nWavLM-Large[58]": "Proposed",
          "Audio\nAudio\nAudio\nAudio\nAudio": "Audio",
          "63.43\n65.64\n64.92\n67.62\n70.03": "65.97",
          "95.04M\n317.38M\n94.68M\n316.61M\n316.62M": "21.5M"
        },
        {
          "wav2vec-Base[57]\nwav2vec-Large[57]\nHuBERT-Base[57]\nHuBERT-Large [57]\nWavLM-Large[58]": "BERT+attention[26]\nBERT+CNN [40]",
          "Audio\nAudio\nAudio\nAudio\nAudio": "Text\nText",
          "63.43\n65.64\n64.92\n67.62\n70.03": "58.53\n66.1",
          "95.04M\n317.38M\n94.68M\n316.61M\n316.62M": "–\n–"
        },
        {
          "wav2vec-Base[57]\nwav2vec-Large[57]\nHuBERT-Base[57]\nHuBERT-Large [57]\nWavLM-Large[58]": "Proposed",
          "Audio\nAudio\nAudio\nAudio\nAudio": "Text",
          "63.43\n65.64\n64.92\n67.62\n70.03": "70.33",
          "95.04M\n317.38M\n94.68M\n316.61M\n316.62M": "110M"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "3",
      "title": "Being Human: Human-Computer Interaction in The Year",
      "authors": [
        "H Richard",
        "R Tom",
        "R Yvonne",
        "S Abigail"
      ],
      "year": "2008",
      "venue": "Being Human: Human-Computer Interaction in The Year"
    },
    {
      "citation_id": "4",
      "title": "The role of affect and emotion in HCI",
      "authors": [
        "R Beale",
        "C Peter"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "6",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "Predicting arousal and valence from waveforms and spectrograms using deep neural networks",
      "authors": [
        "Z Yang",
        "J Hirschberg"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "10",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Proc. IN-TERSPEECH"
    },
    {
      "citation_id": "12",
      "title": "End-to-end speech emotion recognition based on onedimensional convolutional neural network",
      "authors": [
        "M Gao",
        "J Dong",
        "D Zhou",
        "Q Zhang",
        "D Yang"
      ],
      "year": "2019",
      "venue": "Proc. ACM ICIAI"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "14",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Convolutional RNN: an enhanced model for extracting features from sequential data",
      "authors": [
        "G Keren",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proc. IEEE IJCNN"
    },
    {
      "citation_id": "16",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Emotion identification from raw speech signals using DNNs",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "18",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space"
    },
    {
      "citation_id": "19",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "21",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "SpecAugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "23",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "24",
      "title": "Attention assisted discovery of sub-utterance structure in speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "25",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "Proc. IEEE SLT Workshop"
    },
    {
      "citation_id": "26",
      "title": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning",
      "authors": [
        "S Tripathi",
        "T Sarthak",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proc. ACM ICM"
    },
    {
      "citation_id": "29",
      "title": "Data augmentation using GANs for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "30",
      "title": "X-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "31",
      "title": "A review of generalizable transfer learning in automatic emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "32",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Proc. INTER-SPEECH"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition among elderly individuals using multimodal fusion and transfer learning",
      "authors": [
        "G Boateng",
        "T Kowatsch"
      ],
      "year": "2020",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "34",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "36",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "37",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "38",
      "title": "Speech emotion recognition using transfer learning",
      "authors": [
        "P Song",
        "Y Jin",
        "L Zhao",
        "M Xin"
      ],
      "year": "2014",
      "venue": "IEICE Trans. Information and Systems"
    },
    {
      "citation_id": "39",
      "title": "Improved speech emotion recognition using transfer learning and spectrogram augmentation",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "R Sriram",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "Proc. ACM ICMI"
    },
    {
      "citation_id": "40",
      "title": "Fusion approaches for emotion recognition from speech using acoustic and text-based features",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "41",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Multimodal emotion recognition with high-level speech and text features"
    },
    {
      "citation_id": "42",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "arxiv"
    },
    {
      "citation_id": "43",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Multimodal emotion recognition",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Handbook of pattern recognition and computer vision"
    },
    {
      "citation_id": "46",
      "title": "Multimodal emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2011",
      "venue": "Machine audition: principles, algorithms and systems"
    },
    {
      "citation_id": "47",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "48",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "49",
      "title": "Deep contextualized word representations",
      "authors": [
        "M Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Deep contextualized word representations"
    },
    {
      "citation_id": "50",
      "title": "BUT system description to VoxCeleb speaker recognition challenge",
      "authors": [
        "H Zeinali",
        "S Wang",
        "A Silnova",
        "P Matějka",
        "O Plchot"
      ],
      "year": "2019",
      "venue": "BUT system description to VoxCeleb speaker recognition challenge",
      "arxiv": "arXiv:1910.12592"
    },
    {
      "citation_id": "51",
      "title": "Head2Toe: Utilizing intermediate representations for better transfer learning",
      "authors": [
        "U Evci",
        "V Dumoulin",
        "H Larochelle",
        "M Mozer"
      ],
      "year": "2022",
      "venue": "Head2Toe: Utilizing intermediate representations for better transfer learning"
    },
    {
      "citation_id": "52",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "53",
      "title": "The 2019 NIST audio-visual speaker recognition evaluation",
      "authors": [
        "S Sadjadi",
        "C Greenberg",
        "E Singer",
        "D Reynolds",
        "L Mason",
        "J Hernandez-Cordero"
      ],
      "year": "2020",
      "venue": "Proc. Speaker Odyssey Workshop"
    },
    {
      "citation_id": "54",
      "title": "The 2017 NIST language recognition evaluation",
      "authors": [
        "S Sadjadi",
        "T Kheyrkhah",
        "A Tong",
        "C Greenberg",
        "E Singer",
        "D Reynolds",
        "L Mason",
        "J Hernandez-Cordero"
      ],
      "year": "2018",
      "venue": "Proc. Speaker Odyssey Workshop"
    },
    {
      "citation_id": "55",
      "title": "Voxceleb: Large-scale speaker verification in the wild",
      "authors": [
        "A Nagrani",
        "J Chung",
        "W Xie",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "56",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Proc. IEEE SLT Workshop"
    },
    {
      "citation_id": "57",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Proc. IEEE ICASSP"
    },
    {
      "citation_id": "58",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "59",
      "title": "Wavlm: Largescale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2021",
      "venue": "Wavlm: Largescale self-supervised pre-training for full stack speech processing",
      "arxiv": "arXiv:2110.13900"
    },
    {
      "citation_id": "60",
      "title": "Selfattention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. IN-TERSPEECH"
    },
    {
      "citation_id": "61",
      "title": "CNN+LSTM architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "CNN+LSTM architecture for speech emotion recognition with data augmentation",
      "arxiv": "arXiv:1802.05630"
    }
  ]
}