{
  "paper_id": "2111.07094v1",
  "title": "Speech Emotion Recognition Using Deep Sparse Auto-Encoder Extreme Learning Machine With A New Weighting Scheme And Spectral/Spectro-Temporal Features Along With Classical Feature Selection And A New Quantum-Inspired Dimension Reduction Method",
  "published": "2021-11-13T11:09:38Z",
  "authors": [
    "Fatemeh Daneshfar",
    "Seyed Jahanshah Kabudian"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In today's world, affective computing is very important in the relationship between man and machine. In this paper, a multi-stage system for speech emotion recognition (SER) based on speech signal is proposed, which uses new techniques in different stages of processing. The system consists of three stages: feature extraction, feature selection/dimension reduction, and finally feature classification. In the first stage, a complex set of long-termstatistics features is extracted from both the speech signal and the glottal-waveform signal using a combination of new and diverse features such as prosodic features, spectral features, and spectro-temporal features. One of the challenges of the SER systems is to distinguish correlated emotions. These features are good discriminators for speech emotions and increase the SER's ability to recognize similar and different emotions. The data augmentation technique is also used to increase the number of training samples. This feature vector with a large number of dimensions naturally has redundancy. In the second stage, using classical feature selection techniques as well as a new quantum-inspired technique to reduce the feature vector dimensionality (proposed by the authors), the number of feature vector dimensions is reduced. In the third stage, the optimized feature vector is classified by a weighted deep sparse extreme learning machine (ELM) classifier. The classifier performs classification in three steps: sparse random feature learning, orthogonal random projection using the singular value decomposition (SVD) technique, and discriminative classification in the last step using the generalized Tikhonov regularization technique. Also, many existing emotional datasets suffer from the problem of data imbalanced distribution, which in turn increases the classification error and decreases system performance. In this paper, a new weighting method has also been proposed to deal with class imbalance, which is more efficient than existing weighting methods. The proposed method is evaluated on three standard emotional databases EMODB, SAVEE, and IEMOCAP. According to our latest information, the system proposed in this paper is more accurate in recognizing emotions than the latest state-of-the-art methods. emotion, have also been used. The set of prosodic, spectral and spectro-temporal feature vectors becomes a very large vector and long-term statistics are extracted from it. One of the most important issues in pattern classification is feature selection or dimension reduction. Unfortunately, classification error is not directly considered as cost function in current feature selection/dimension reduction algorithms. In this paper, in addition to using the classical feature selection methods, a new meta-heuristic method developed by the authors  [5, 6]  and inspired by quantum theory-called point-mass quantum particle swarm optimization (pQPSO)-is used to estimate the dimension reduction matrix. After dimension reduction, the feature vector must be classified. This paper uses a deep classifier. One of the problems with deep learning is that they take a long time to train. The hierarchical adaptive weighted ELM (H-AWELM) classifier is used here, which is very fast. This classifier uses both random unsupervised feature learning as well as random orthogonal projections and supervised learning. This classification strategy is very fast because it uses random mappings in the unsupervised layers and the fastregularized weighted pseudo-inverse (generalized Tikhonov regularization) technique in the supervised layer. Since emotional databases are typically imbalanced, a new sample-wise weighting method is proposed which is more accurate than the current weighting methods. Also, in general, different speech emotions may have similar acoustical features. The correlation coefficient index can be used to examine the degree of dependence and relationship between different emotions. Fig.  1  shows a correlation diagram of seven different emotions of the EMODB dataset. From this diagram, it can be concluded that sad has the highest similarity with bore (compared to other emotions), happiness has the highest similarity with anger and fear has the highest correlation with disgust among other emotions. The reason for this is, the similarity in energy, pitch, duration, and other prosodic features in these emotions. Also, the pairs (fear, anger) and (disgust, happiness) have a low correlation coefficient and are independent. Recognizing and differentiating these correlated emotions is one of the challenges of today's speech emotion recognition systems, which in this study has been done well.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "1-Introduction",
      "text": "Recognition of emotions from speech signal or Speech Emotion Recognition (SER) is one of the research fields in affective computing. The purpose of the SER is to analyze human speech signal and to extract the emotional state of the person (fear, joy, sadness, etc). The SER systems usually have three stages: feature extraction, dimension reduction/feature selection, and classification. In the feature extraction stage, prosodic and spectral features are usually used. These features alone are not able to discriminate unstable transitions in the speech signal in different emotions. For this purpose, in this paper, spectro-temporal features such as Gabor filter bank (GBFB) and separate Gabor filter bank (SGBFB) features  [1]  have been used, which have more discriminative power. New spectral features such as constant-Q cepstral coefficient (CQCC)  [2] , Single frequency cepstral coefficient (SFCC)  [3]  and IIR-CQT Mel-frequency cepstral coefficient (ICMC)  [4]  that have not previously been used to identify The importance of the results of this paper and its contributions are summarized as follows:\n\n In the feature extraction stage, spectro-temporal features such as GBFB and SGBFB are used, as well as new spectral features such as CQCC, SFCC, and ICMC, which have not yet been used in SER. These features help to better identify similar emotions with the same and different speakers by providing spectro-temporal contrast at all times. They will also improve the results of speech/non-speech classification by eliminating redundant information between feature components  [1] .  In the feature dimension reduction stage, both classical methods and a new quantum-inspired metaheuristic algorithm proposed by the authors are used to estimate the projection matrix.  In the classification stage, the H-AWELM classifier is used along with a new weighting method. This new weighting method proposed by the authors leads to higher performance on three popular databases than the current weighting methods.  According to our latest information, the proposed speech emotion recognition system has the best accuracy for the two well-known datasets EMODB and SAVEE compared to all other state-of-the-art systems.\n\nThe rest of the paper is organized as follows: Section 2 will review the research work done in the field of the SER. Section 3 describes the SER system, the algorithms and block details. The results of the experiments and performance evaluation of the proposed system, are presented in Sections 4 and 5 respectively, and finally we will conclude in the last section.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "2-Literature Review",
      "text": "Numerous works have been recently published on emotional speech feature extraction. In  [7] , a new architecture has been proposed that extracts Mel-frequency cepstral coefficients, Mel spectrogram, and spectral contrast features from audio files and uses them as input to a deep convolutional neural network (CNN) to identify the emotions of the EMODB, IEMOCAP, and RAVDESS datasets. In this study, to improve the accuracy of classification, an incremental method has been used to modify the initial model. The final recognition rates obtained by this model were 86.1% and 64.3% on the EMODB and IEMOCAP datasets. In this study, correlated emotions (happiness, anger) are well differentiated (100%, 100%), while (bore, sad) and (disgust, fear) are not well distinguished (76.92%, 87.5%) and (90%, 66.67%) and classification error occurred. The model presented by  [8] , extracts important discriminative salient features in parallel and learns long-term conceptual dependencies from speech, using a multi-learning strategy. In this study, deep learning (DL) is used to find the speech emotional features. This architecture consists of a Dilated CNN (DCNN) to learn the basic features of the input speech signal. It then uses one-dimensional residual blocks (RB) and a Bidirectional gated recurrent unit (BIGRU) to improve learned features. Also, the spatial and temporal information of the network output is connected using a fusion layer, and finally, after passing through a fully connected network (FCN) with a softmax layer, the final decision is made. This model has reached 90% and 73% recognition rates on EMODB and IEMOCAP datasets. Although it distinguishes the correlated emotions (bore, sad) and (disgust, fear) with high recognition rates (92.7%, 95.2%) and (90.9%, 91.3%) respectively, it still failed to distinguish (anger, happiness) well (98.1%, 66.67%).\n\nIn a study presented by  [9] , a deep CNN network was used to extract emotional features. Then, a correlationbased method was used to select the most appropriate and discriminative features for SER, and finally, methods such as support vector machine (SVM), random forests (RF), and neural networks were used as classifiers. In this study, using fast Fourier transform, the input signal spectrum of emotional data is used to detect the output emotional class. The results of this study on the EMODB and SAVEE datasets were 90.5% and 66.9%, respectively. Also, this paper has been able to distinguish the correlated emotions (bore, sad), (fear, disgust), and (happiness, anger) with almost high rates (87.65%, 93.54%), (92.06%,98.91%), and (92.12%,92.3%). But it did not recognize the neutral emotion well (89.87%). Mehmet Bilal Er  [10]  has increased the accuracy of classification by using deep and acoustic features. First, acoustic features such as root mean square (RMS) energy and Mel frequency cepstral coefficients are extracted from the input signal. The main audio signal spectrum is then given as the input of the DNN and deep features are extracted. Then a hybrid feature vector is created and after selecting features with ReliefF algorithm and SVM classifier, the SER system will be designed. In this paper, the recognition rate on the EMODB was 90.21%. This study also distinguished correlated emotion pairs at almost high rates, (bore, sad) with (92%, 89%), (fear, disgust) with (96%, 89%) and (anger, happiness) with (91%, 88%), but it has recognized neutral emotion at a lower rate of the other references (88%).\n\nThe study presented by  [11]  did not use the speech acoustic features but used a residual CNN to classify speech emotions. In this model, the speaker gender information is also added to the proposed algorithm to increase the recognition rate. In this study, the emotion recognition rate on the EMODB, was 90.3%, but it did not distinguish correlated emotions with a high recognition rate. In this paper, emotions (bore, sad) with rates (100%, 80%), (fear, disgust) with rates (77.78%, 100%), and (happiness, anger) with rates (84.62%, 90%) have been recognized. In fact, the proposed method in this study has a classification error in distinguishing between similar emotions. The method proposed by  [12]  has used a new framework for recognizing speech emotions using key sequence segment selection based on the similarity of radial base function (RBF) networks in different clusters. Then, a CNN and an RBF network were used to extract the discriminative and important features, and a long short-term memory (LSTM) network was used to classify them. Finally, the emotion recognition rates on the EMODB and IEMOCAP datasets were reported to be 85.57% and 72.25%, respectively. This paper also recognizes the correlated emotions (bore, sad) and (fear, disgust) with high rates (90%, 88%) and (87%, 92%), but it could not distinguish (happiness, anger) at high rate (66%, 91%). Also, in the research presented by  [13]  to solve the problem of lack of data in the emotional dataset, a module based on simple deep learning has been used. In addition to the above methods, which use deep learning in the recognition of speech emotions, many methods have done this using generative adversarial networks (GAN). For example, a semi-supervised GAN has been used by  [14]  to extract the features of both labeled and unlabeled datasets. Yi and Mak  [15] , used an adversarial network and an auto-encoder (AE) as a classifier of emotional features. This model has a recognition rate of 84.49% and 66.92% on the EMODB and IEMOCAP datasets. In this study, the recognition rate of correlated emotions has not been reported. Also, an adversarial AE has been presented by  [16]  to solve the problem of lack of emotional data which reported a recognition rate 64.5% on the IEMOCAP dataset. Meanwhile, some studies have used wavelet transform to better recognize speech emotion. For example, Wang et al.  [17] , used wavelet analysis to extract features and achieved a recognition rate of 79.2% on the EMODB dataset. It has identified the pair (anger, happiness) with high rates (93.7%, 98.6%), while it can't distinguish well other related emotions (bore, sad) and (fear, disgust), ((75.3%, 96.8%) and (55.07%, 80.4%)). This method has also been used by  [18] , and its result on the EMODB was 90.09%. In this study, correlated emotions (fear, disgust) and (happiness, anger) were distinguished at a reasonable rate (88.41%, 82.61%) and (96.06%, 85.92%), while correlated emotions (bore, sad) was not well distinguished (82.72% ,95.16%).\n\nReddy and Vijayarajan  [19]  also used Mel-frequency cepstral coefficient and discrete wavelet transform as features of the SER system and examined their performance using the SVM and K-nearest neighbor (KNN) classifiers. Ali Bakhshi et al.  [20] , have used cepstral features along with a binary hidden Markov model (HMM) classifier to detect speech emotions. In the architecture proposed by  [23] , high-dimensional vector of emotional features including fundamental frequency, zero cross rate, Mel-frequency cepstral coefficient (MFCC), energy and harmonic noise ratio using the c-means fuzzy clustering algorithm (FCM), are divided into different subsections and are classified using multiple RF algorithm. The recognition rate of the proposed model on the EMODB set was 85.61%. Also, this method could not distinguish well between correlated emotions (bore, sad), (fear, disgust) and (happiness, anger).\n\nIn the model proposed by  [24] , a new regression method called robust discriminative sparse regression is used to select appropriate discriminative features. The results on EMODB was 86.19% and only the pairs (bore, sad) could not be distinguished well.\n\nThe model proposed by  [25]  compares the performance of two different classifications in SER systems. In  [26] , an ensemble learning model random forest algorithm has been used to find the importance of different features . In this method, the weighted binary cuckoo algorithm is used to select the features and the decision tree, RF and SVM classifiers are used for classification. It has finally a recognition rate of 83.7% on EMODB. This method also distinguishes correlated emotions (bore, sad) at a better rate than (fear, disgust) and (happiness, anger). In the research presented by  [27] , a discriminative non-negative matrix factorization (DSNMF) method has been used to reduce the dimensions of input features. This method reached 82.8% on EMODB but could not differentiate the correlated emotions (happiness, anger) well. In the model proposed by  [28]  the Hilbert-Huang-Hurst coefficient vector (HHHC) has been used as one of the nonlinear features of the vocal source, due to its effect on speech production mechanism, to better display emotional states. In this study, gaussian mixture model (GMM), HMM, DNN, CNN and convolutional recurrent neural network (CRNN) have also been used as classifiers. This model achieved recognition rate of 81.8% on EMODB and it could not distinguish any of the correlated emotions too. And finally in the model proposed by  [29]  an adaptive domain-aware representation learning method is used to better identify and extract domain-dependent features. This model has a rate of 73.02% on IEMOCAP. Table  1  summarizes the recent proposed SER methods. All of them evaluated with both criteria, weighted average recall (WAR) and unweighted average recall (UAR).   [7]  90.01 N/A CNN as classifier  [8]  N/A 90.21 DL to extract features  [10]  N/A 90.30 RCNN as classifier  [11]  N/A 85.57 CNN for feature extraction and bidirectional deep neural network LSTM for feature classification  [12]  90.01 N/A DL to extract features  [13]  N/A 90.5 DCNN for feature extraction  [9]  68.00 65.20 semi-supervised GAN as classifier  [14]  83.31 84.49 GAN and AE as classifier  [15]  66.70 N/A adversarial AE for features discriminative recognition  [16]  79.20 N/A Wavelet analysis to extract features  [17]  N/A 90.09 Tunable Q wavelet transform to extract features  [18]  N/A 87.29 BHMM as classifier  [20]  N/A 77.08 Feature extraction using a triangular filter bank  [21]  N/A 81.80 Use HHH coefficients to extract features  [28]  65.86 73.02 Use adaptive domain-aware representation learning method to extract domain-dependent features  [29]  90.01 N/A Use sequential learning to find dependencies between features  [13]  76.90 N/A Use the active feature selection method to select the feature  [30]  N/A 72.19 Use the feature weighting method based on emotional groups to select the features  [22]  N/A 85.61 CMFC to select features  [23]  86.19 N/A Use of robust discriminative sparse regression to select discriminative features  [24]  N/A 83.70 Using RF with group learning model and weighted binary cuckoo algorithm to select superior features  [26]  83.30 82.80 Using discriminative non-negative matrix factorization to reduce features dimensions  [27]  Besides, there have been a large number of research studies into the area of ELMs  [31] [32] [33] [34] [35] . For example,  [36]  used a self-adaptive evolutionary ELM for optimizing network hidden node parameters,  [37]  developed an online sequential learning algorithm for single-hidden layer feedforward neural networks (SLFNs),  [38]  introduced multi-layer ELM which could learn feature representations using ELM-based auto-encoder. In  [39] , a new learning algorithm, called bidirectional ELM, had been proposed, in which the number of hidden nodes could be further reduced without affecting learning effectiveness, and finally  [40]  extended ELMs for both semi-supervised and unsupervised tasks based on manifold regularization. So far, many weighting algorithms have been proposed for imbalanced data classification with ELMs including  [41] , in which a weighted ELM had been proposed to deal with data with imbalanced class distribution,  [42]  that presented an effective method based on a fast ELM classifier and fuzzy membership of samples for classimbalanced learning, and  [43]  wherein previous weighted ELMs could be improved with decay-weight matrix setting for balanced and optimized learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "3-System Description",
      "text": "As illustrated in Fig.  2 , the overall SER system consists of three main parts: preprocessing and feature extraction, feature selection, and feature classification. In this section, the SER system designed using the proposed method will be elaborated.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3-1 Feature Extraction",
      "text": "It is an important stage of an efficient SER system. At first, the preprocessing of the speech signal before feature extraction has been considered. Pre-emphasizing, framing, windowing, and voice activity detection are three common techniques used in signal preprocessing. The preprocessed signal will be then fed to the feature extraction module. In this stage, the necessary and emotion-relevant features like the spectro-temporal features will be extracted from the signal, which will be explained below.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "3-1-1 Gbfb Features",
      "text": "There is an uncertainty relation between a signal's specificity in time and frequency. Many approaches show that both spectral and temporal information of a spectro-temporal representation of a speech signal is useful for speech processing and limiting redundant features  [44] . Dennis Gabor defined a family of signals (called Gabor filters) to optimize spectral and temporal trade-off. The Gabor elementary function is the sum of Gaussian cosine and Gaussian sine functions that can minimize the space (time)-uncertainty product  [1] . The Gabor filter h(t) is a Gaussian function modulated with a sinusoidal function with frequency u0 as below (Fig.  3 ), ( )\n\nWhere s(t) shows a complex sinusoidal function:\n\nAnd, g(t) refers to a Gaussian function, also known as envelope:\n\nThus, the Gabor filter can be written as:\n\nAnd, the frequency response of the filter is: However, a Gabor filter in a 2D space is a sinusoidal plane of particular frequency and orientation, modulated by a Gaussian envelope (Fig.  3 ). It can be written as:\n\n( , ) ( , ) ( , ) h x y s x y g x y \n\nThe Gabor filter can be also written as:\n\nTherefore, the frequency response of the filter is:\n\nx y x y\n\nu0 and v0 denote Gabor filter spatial center frequency. σx and σy represent the SD of the Gaussian envelope along x and y directions and determine the filter bandwidth. An example of a two-dimensional (2D) Gabor filter is shown in Fig.  4 .\n\nBecause of the locality property, the Gabor representation is physically more realistic than the Fourier one. It characterizes a band-limited signal of finite duration; in contrast, the Fourier representation of such a signal requires an infinite superposition of nonlocal signals. Also, the spectro-temporal features derived from the GBFB are endowed with good discriminative properties in both spectral and temporal domains and are also considered as a powerful tool for improving robustness, and augmenting performance of SER systems, and limiting redundancy between their features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Calculation Of Gbfb Features",
      "text": "The process of GBFB feature extraction is illustrated in Fig.  5 . First, a logarithmically-scaled mel-spectrogram, i.e. with logarithmic amplitude and mel-frequency scaling or any similar spectro-temporal representation is calculated from the speech (Fig.  5a ). Then, 41 Gabor filters (Fig.  5b ) are generated for all pairs of spectral and temporal modulation frequencies except for the redundant ones. After that, the log mel-spectrogram (LMspec) will be filtered with GBFB filters (convolved with each of the 41 Gabor filters) to model the response and to represent the spectro-temporal patterns. However, the resulting feature vector is relatively high-dimensional when all the filter outputs of 41 filters are used. Subsequently, the filtered LMspec are spectrally sub-sampled at a rate of a quarter of the extent of the spectral width of the corresponding filter because the filter output between adjacent channels is highly correlated once the filter has a large spectral extent. This can reduce redundancy from the filtered LMspec. Finally, the filtered and sub-sampled LMspecs are also concatenated and form a 455-dimensional feature vector, referred to as GBFB features (Fig.  5c )  [1] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Calculation Of Sgbfb Features",
      "text": "It has been confirmed in  [1]  that if a separate spectral and temporal processing with two 1D GBFB can be used to extract features, it will improve robustness in contrast to complex joint 2D spectro-temporal GBFB approach. On the other hand, the 2D GBFB can be decomposed into a spectral 1D GBFB and a temporal 1D GBFB such that the inseparable 2D patterns of the GBFB will be replaced with separable ones (a 1D Gabor filter has been illustrated in Fig.  6 ). SGBFB features are also those extracted with these two 1D GBFBs, instead of 2D Gabor filters. An example of an input LMspec after filtering with spectral filter, temporal filter, and spectro-temporal filter has been displayed in Fig.  7 .\n\nThe experiment results in  [1]  revealed that since spectral and temporal processing can be performed independently in SGBFB, then it is more robust to noise compared with GBFB features.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "3-1-2 Statistical Functions",
      "text": "After all the features have been extracted from the input speech signal and its glottal waveform, a feature matrix, whose rows and columns are respectively frames and different feature vector elements, will be obtained.\n\nConsidering that the input signal extracted features are per frame, the evaluation of a system for SER is usually based on the whole emotional utterance. For this purpose and in order to facilitate the comparison of recognition rates, statistical data are used to find the features of related utterances.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "3-2 Feature Selection/Dimension Reduction",
      "text": "The purpose of feature selection and dimension reduction methods is to reduce the dimension of the feature space to select more influential features and to reduce redundancy. So, today, these methods have become an integral component of the learning process in order to deal with high-dimensional data. Besides, a proper feature selection/dimension reduction method can improve the learning process in a variety of ways including learning speed and generalization capacity. In this experiment, due to the high dimensions of the extracted features and in order to reduce redundancy, some feature selection/dimension reduction methods will be used. They are as follows,\n\n mRMR  [45]  denotes a feature selection method for selecting features according to the mutual information in a way that there is minimum redundancy between selected features and maximum relevance between such features and their targets.  Correlation-based feature selection (CFS)  [46]  ranks features based on correlations. Good feature subsets contain features, which are correlated with the class label, but uncorrelated to each other.  Local learning-based clustering (LLC)  [47]  is a local learning approach for feature clustering. The basic idea is that a good clustering result should have the property that the cluster label of each feature point can be well predicted based on its neighboring features and their cluster labels.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "3-2-1 Pqpso",
      "text": "So far, many changes have been made to the standard QPSO algorithm, such as the weighted QPSO (wQPSO) algorithm  [48] . Point mass function-weighted QPSO (called pQPSO) is another modified version of the standard QPSO, which has been proposed by the authors in this paper  [5, 6] .\n\nIn this modified-QPSO, mean best and global best position in the standard QPSO, is replaced by a randomlyselected particle, Lucky Global (LG) best, which is selected from a set of top best particles found in each iteration according to  (13)  and is then generated with point mass function distribution ( ) according to  (14)  to  (16) .\n\nIn  (14) , and represent highest and lowest values of cost function for the list members, and is particles' relative fitness factor in list. After generating with probability ( ), this value replaces both and .\n\nIn the standard QPSO algorithm, new particles may be created outside the desired interval. Since these particles represent invalid solutions to the problem, they should be eliminated or moved to a valid range (that is, repaired).\n\nRemoving or repairing the new particles also slows down algorithm convergence. To cope with this problem in the proposed pQPSO, truncated Laplace distribution (TLD) is used for generating new particle positions within the valid range. Assuming that the random variable is in the interval ≤ ≤ , with mean and standard deviation , in the TLD; the probability density function (PDF) and cumulative distribution function (CDF) of are as follows.\n\n,\n\nThen, particle position is generated randomly using the following equations.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Sgn(F ( )",
      "text": "), U(0,1)\n\nwherein, sgn stands for the sign function. Moreover, in this paper, an adaptive technique is proposed for determining contraction expansion value in each iteration instead of using constant or linearly-varying method for contraction expansion (CE) coefficient; in which, its value is calculated proportional to error reduction rate in the previous iteration, as follows.\n\nin which, 0: is initial value, 1: refers to final value of CE coefficient, : represents number of iterations, : shows current iteration, : is maximum number of unsuccessful attempts to improve cost function more than a specified value relatively, : indicates current number of unsuccessful attempts. If the algorithm fails to reduce relative error by more than in successive iterations, it is terminated.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "3-3 Classification",
      "text": "Following feature selection, a classification method will be utilized to organize the selected features. This method will be trained on selected training features and then it is ready for classification of test features. The final accuracy rate on the test set actually indicates the performance of the proposed algorithm. In this section, the unweighted ELM and the proposed weighted ELM have been described.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "3-3-1 Unweighted Elm",
      "text": "ELMs are random SLFNs whose purpose is to solve the problem of slow-learning back-propagation (BP) method in neural networks (NNs). In these machines, the weights between the input layer and the hidden layer are randomly assigned. Then, unlike the BP method, the weights between the hidden layer and the output layer are obtained using the regularized least squares (LS) technique to resolve the problem of minimizing the training error and the norm of output weights  [49] .\n\nFor example, in an ELM network having L nodes in the hidden layer as well as N different training pairs ( , )\n\ni i\n\nx t , the output of the hidden layer for input vector i x can be represented as follows:\n\nSo that,\n\n[ , ,..., ] 1, 2,..,\n\n[ , ,..., ] 1,2,..,\n\nM refers to number of classes. Then, a simple ELM network can be represented by the following relations:\n\n[ , ,..., ]\n\nThe output vector yi for input xi is calculated as follows:\n\n( )",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Y H   (33)",
      "text": "The matrix Y is horizontally-stacked output vectors of the ELM for all training samples. As well, the matrix H is horizontally-stacked output vectors of the hidden layer for all training samples, the matrix T is horizontallystacked target vectors for all training samples (also known as indicator matrix or one-hot matrix), and the matrix β is vertically-stacked weight vectors of the output layer for all classes.\n\nAs stated in Bartlett's theory  [50] , the purpose of ELM is to reduce training error\n\n(the Frobenius norm of error matrix) and to maximize marginal distance between classes, or to minimize the norm of output weights 2  . Accordingly, the classification problem is mathematically written as:\n\n[ , ,... ]\n\nHere, i  is the error vector for training sample i x . So, β is calculated in the ELM as follows:\n\nSuch that † H is Moore-Penrose (generalized) matrix inverse of H and C refers to regularization parameter for better generalization  [49] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "3-3-2 Weighted Elm",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Previous Work",
      "text": "Suppose the data distribution has two classes of majorities and minorities; where the majority class sample numbers are more than average and the minorities are below average. If the unweighted ELM method is used, the samples in the majority and minority classes will have the same weight. Therefore, the majority class boundary will be increased to reduce the training error and to have the same misclassification costs for each sample, and some examples of the minority classes will be then mistakenly classified as the majority ones. The aim of weighted ELM design is to moderate the majority class boundary, to enlarge the boundary of the minority one, and consequently to diminish the training error. Then, the weight of the samples is used in ELM training to balance data distribution and (  34 ) is changed into:\n\ndiag( , , ..., )\n\nWhere W is an N N  diagonal positive matrix and ii W corresponds to training sample i x . The purpose of this type of ELM is, if xi belongs to a minority class, its weight will be greater than that of the majority class to balance the distribution of data. In this case, the relation  (38)  will change as follows:\n\nSo far, several algorithms have been used as weighting methods to balance the class boundaries. The following two weighting methods are proposed in  [41] :\n\nWhere With this weighting method, the greater the number of samples in a class, the lower the weight of those samples will be, and vice versa. In the second weighting method, this ratio is further reduced and the samples from the majority classes will gain less weight than the first method.\n\nOne of the problems with this weighting method is that, the weight of the samples of the majority classes decreases relative to the number of samples; then, some of the samples from the majority classes will be misclassified as minority ones. This problem will be more severe in the second method because the weight of the majority classes is further decreased.\n\nTo explain this problem, a new method has been proposed in  [42] , which has the following weighting relation in multi-class and binary classification cases:\n\nThis weighting method increases the weight of classes by a factor proportional to the number of samples in each class to avoid the incorrect classification of the samples in the majority class into the minority one. Thus, the weight of both majority and minority classes is added in a slight manner. Since this increase is proportional to the number of instances in each class, it will be higher for the majority classes than for the minority ones. As a result, the weight loss of the majority classes is lower than the W1 and W2 methods and the misclassification rate of the majority class samples is reduced compared with that of two previous methods. The coefficient d is called decaying parameter, so the greater the value, the higher the importance of the minority classes. On the other hand, in the method presented in  [43] , weights are obtained from the following equation:\n\nWhere, the number of samples in different classes i.e.  \n\nis sorted in an ascending order to achieve\n\nFor example, in a binary classification, the weight of the two classes with negative and positive samples as majority and minority classes (i.e.\n\n), is obtained as follows:",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Proposed Weighting Method",
      "text": "As explained before, one of the problems with the W1 weighting method is that, the weight of the samples of the majority classes decreases relative to the number of samples, then some of the samples from the majority classes will be misclassified as minority ones. This problem will be more severe in W2 method because the weight of majority classes is further decreased. In the method presented in this study, the main objective is to boost the boundary of the majority classes more than the W1 method and to reduce the boundary of the minority ones less than W1. In this weighting method, the importance of majority classes is slightly higher than the methods W1 and W2 and the misclassification will be demoted. The proposed formula for weighting different classes in this study is as follows:\n\n1,..., max(\n\n)\n\nIn a binary mode, with negative and positive samples as majority and minority classes, it is as follows:",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "3-3-3 H-Elm",
      "text": "The H-ELM, is kind of ELM with more than one hidden layer. Each hidden layer is an auto-encoder ELM (AE-ELM). Therefore, as shown in Fig.  8 , the H-ELM training consists of two successive stages, an unsupervised hierarchical feature representation and a supervised feature classification. In the first phase, an ELM random feature space exploits hidden information among training samples and an N-layer unsupervised-learning ELMbased sparse auto-encoder extracts multilayer sparse features of the input data. Within this phase, each hidden layer of the H-ELM is an independent module functioning as a separated feature extractor. After unsupervised hierarchical training, the outputs of the N-th layer are also viewed as the high-level features extracted from the input data. They are thus utilized as the inputs of the supervised ELM-based regression to obtain the final results of the whole network. On the other hand, the supervised feature classification stage is performed for final decisionmaking. Algorithm 1 presents the multi-layer unsupervised-learning ELM-based sparse auto-encoder pseudo-code in detail. The proposed H-ELM structure achieves more compact and meaningful feature representations than the original ELM. Exploiting the advantages of ELM random feature mapping, the hierarchically-encoded outputs are randomly projected before final decision-making, leading to a better generalization with faster learning speed unlike the greedy layer-wise training of DL, in which the hidden layers are trained in a forward manner with unsupervised initialization and need to be retrained (that is, fine-tuned) iteratively using back-propagation (BP)based NNs. However, in H-ELM, the weights of the current layer are fixed without fine-tuning once the previous layer is established. Thus, H-ELM training would be much faster than that of DL.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "4-Experiments 4-1 Emotional Datasets",
      "text": "Many emotional databases have been designed to test the performance of SER systems. In this study, three common databases including Berlin Database of Emotional Speech (EMODB)  [51] , Surrey Audio-Visual Expressed Emotion (SAVEE)  [52] , and the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [53]  were employed to evaluate the effectiveness of the proposed system, whose specifications are listed in Table  2 .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "4-2 Feature Extraction",
      "text": "Since the best and the most effective features for SER have not yet been identified, many experiments have been done in this study to select the best set of features, whose results are presented in Table  3 . Given that such features are obtained in diverse ways, each one describes a speech signal from a different perspective. In Table  2 , the number of dimensions examined for each feature as well as related references and their recognition rates (i.e.\n\nWAR) are also illustrated. All the experiments have been conducted on EMODB corpus in a 10-fold crossvalidation case (that is, leave one speaker out (LOSO)) using the SVM classifier.\n\nGenerate a Temporary Uniformly-Distributed Temporary Weight Matrix of the Layer\n\nCompute Temporary Output of the Layer\n\nCompute the Final Sparse Weight Matrix of the Layer i  by Solving the following sparse least-squares problem (sparse linear inverse problem):\n\narg min ( )\n\nCompute Final Output of the Layer\n\nend for // Second Stage: Orthogonal Random Projection Layer Base on SVD Add a Vector of 1's to the Input Matrix of the Layer (for Considering Bias)\n\nGenerate a Temporary Uniformly-Distributed Random Weight Matrix:\n\nCompute Orthonormal Basis for the Range of the Matrix tmp 1 U   Using Compact SVD as Follows:\n\nFeatures evaluated in this study include MFCC  [54] , perceptual linear predictive cepstral (PLPC) [55],\n\nperceptual minimum-variance distortionless response cepstral coefficient (PMVDR)  [56] , pitch (F0)  [54] , glottal waveform signal  [54] , CQCC  [2] , ICMC  [4] , frequency domain linear prediction (FDLP)  [57] , cochlear filter cepstral coefficient (CFCC)  [58] , GBFB  [1] , SGBFB  [1] , wavelet cepstral coefficient (WCC)  [59] , and SFCC  [60]  along with their first-and second-order derivatives (i.e. Δ, Δ 2 ) for the input speech signal. Considering the impact of features evaluated, the following feature set are selected from the list of the mentioned features (the numbers in parentheses represent the number of dimensions of each feature), MFCC  (20) , PLPC  (7) , CQCC  (20) , ICMC  (20) , SFCC  (13) , and their first and second-order derivatives as well as GBFB (455) and SGBFB (1020) features. The extracted feature vector dimensionality is equal to 1715. In addition to the above feature vector, the same features will be extracted from the pitch-shifted speech signal (by 0.9 and 1.1) and added to the input training samples which thereby increase the accuracy of the SER system.",
      "page_start": 14,
      "page_end": 16
    },
    {
      "section_name": "4-2-1 Statistical Functions",
      "text": "There are many statistical functions to extract statistical data from the frames of features. To find the best appropriate ones, some experiments have been performed in this study. Table  4  shows some statistical functions examined including their relations, where x is a random variable having n observations (i = 1, …, n). These functions and their combinations have been also applied to the extracted feature vector obtained from the previous sub-section. The recognition rates have been illustrated in Fig 9 . Moreover, all the experiments have been performed on EMODB corpus using the SVM classifier in LOSO case. According to the results, the statistical functions that reach the best recognition rate consist of mean, SD, skewness, kurtosis that can prepare the related feature vectors of utterances. Therefore, the dimension of the extracted feature vector will increase from 1715 to 6860 after applying the mentioned statistical functions.\n\n)\n\nPercentile percentile (here =50) percentile rank 100 -th percentile",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Zero Crossing (Zc)",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "4-3 Feature Selection/Dimension Reduction",
      "text": "In this experiment, due to the high dimensions of the extracted features and in order to reduce redundancy, a feature selection method will be used. Before applying this algorithm, in order to find the best number of selected dimensions, several experiments have been performed on EMODB corpus by the SVM classifier and in LOSO case, the diagram of the cumulative sum of their scores is shown in Fig.  10 . Since the highest cumulative sum of weights is for 3000 and 5000 dimension numbers, the subsequent experiments will be then performed with both 3000 and 5000 dimensions. Then, in order to find the best feature selection/dimension reduction method, the proposed H-AWELM model will be implemented using different feature selection/dimension reduction methods.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "4-3-1 Classical Feature Selection Methods",
      "text": "The related recognition rates for different feature selection methods are listed in Table  5 . Among these, the mRMR method has had better results than other feature selection methods.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "4-3-2 Dimension Reduction Methods",
      "text": "In this case, the proposed model is implemented with different feature dimension reduction methods such as pQPSO, wQPSO and QPSO and the results are shown in Table  6 . Meanwhile, the pQPSO method has had better results than other dimension reduction methods.  However, since the recognition rate obtained by the mRMR method was higher than pQPSO and other feature selection methods (Table  5 6 ), in the final experiments, the mRMR algorithm was used for feature selection.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "4-4 Classification",
      "text": "In this section, in order to find the most appropriate structure for the mentioned H-ELM, some experiments have been performed on different layers and various neuron numbers (  Fig 11) . Based on the results presented on EMODB corpus, the best structure of unsupervised layer is belonging to a structure with three different layers with 1000, 1000, and 20000 neurons. On the other hand, in the last layer of the H-ELM, diverse functions can be used as an ELM kernel function. The results for some functions are shown in Fig.  12 . According to the results, the tangent hyperbolic transfer function has produced better recognition rates compared with other functions. Therefore, the experiments will continue with these hidden layer neurons and kernel function including the proposed adaptive weighted algorithm in the last layer of the H-ELM.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "5-Performance Evaluation",
      "text": "After the feature extraction stage of the training and test signals, a final model of the three-layer H-ELM with the tangent hyperbolic kernel and the proposed adaptive weighting approach has been learned on the selected training features to classify the selected test features. The final recognition rate on the test features actually indicates the performance of the proposed algorithm. In this section, for better comparison of the obtained results, the criterion of imbalance ratio (IR) is used. This criterion is used to evaluate the degree of dataset imbalance and in the case of multiclass, is obtained from the following relation  [43] , #( ) refers to the number of instances of class . This ratio has been shown for different datasets in Table  7 . As it turns out, all three datasets have an IR less than 1 and they suffer from a kind of imbalance. Since these datasets have different imbalance ratios, they are suitable for evaluating the performance of the proposed weighting algorithm.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "5-1 Comparison Of Results On Different Weighting Algorithms",
      "text": "The results of the proposed H-AWELM model concerning three different databases have been illustrated in Table  8 . Here, different weighting algorithms (previously presented in Section 3) implemented with the H-ELM method and their results on EMODB, SAVEE and IEMOCAP datasets have been respectively presented for both 3000 and 5000 feature selection numbers for further comparisons. Since the weighted and unweighted ELM network are randomly initialized, all the presented results in this paper have been repeated 10 times with 10 different random number generator seeds and the results for 10 runs are averaged whose means and standard deviations are reported (    ). They are also evaluated with both criteria, WAR and UAR, for the purpose of accuracy measurement. As shown in Table  8  the weighting method W2 is less efficient in both feature selection modes than the others. The main reason is that, in this case, the weight given to the majority classes is less than that specified by other methods. Also, the weighting method W1 does not yield relatively good results because the number of samples in each set increases and the related weight will be moderated, so the weight given to the majority classes is reduced by the same proportion. Since this weight is higher than that obtained by the W2 method, it also yields relatively better results in both feature selection modes in contrast to W1. In this research, the overall purpose of the proposed method is to decrease the training classification error. Therefore, the proposed adaptive weighting method provides higher recognition rates than the weighting methods presented in the literature review section in both 3000 and 5000 selected features number cases. Also, in comparison with the unweighted classification method, the obtained results have shown that the use of weighted classification with any of the weighting methods has achieved better results than the unweighted classification. In other words, H-AWELM can not only improve the minority classes' classification but can also maintain the majority classes' classification at the same ELM level. In addition, the G-mean criterion is used to better understand and evaluate the classification of unbalanced datasets. This criterion, as defined below, is the geometric mean of the recall values of all classes,\n\nWhere Rj is the recall value of class j. In fact, it measures the balance/trade off ratio between different emotion recall values. For example, in a binary classification problem with 99% majority sample and 1% minority sample, an unweighted classification achieves 99% accuracy by classifying all samples in the majority class. But its Gmean value will be zero because the value of the minority class recall is zero  [41, 43] . Fig.  13  shows the value of the G-mean variable for the different weighting methods. As shown in this figure, the proposed method has a better value for this criterion.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "5-2 Comparisons With State-Of-The-Art Results",
      "text": "The results presented in this paper have also improved significantly compared with the findings provided by many papers in this field. Tables 9 to 11 better compare these results in terms of both WAR and UAR criteria. N/A indicates that the paper has presented its results only in WAR form. In Table  9 , both accuracy rates (WAR and UAR) of the currently published papers on EMODB corpus are compared with the results of the presented method in this paper. Obviously, the proposed algorithm has produced a higher accuracy rate than many previous methods and these results are even better than those published earlier by the authors. The recognition rates of 82.82% and 79.94% obtained in  [5]  and  [6]  were less than the 91.29% obtained in this paper. Also Fig.  14  compares the presented proposed method with other recently published methods on the EMODB dataset. In this comparison, the results of the weighted ELM classifier were better than the deep and adversarial classifiers.\n\nIn addition, Table  10  compares the results for SAVEE with the findings reported in recently published papers in terms of both WAR and UAR criteria. However, the results of the proposed system and the weighting method have been better than those in many recent papers. These results are also better than the two previous works reported by the authors. Recognition rates of 60.79%  [5]  and 59.38%  [6]  can be thus compared with 66.94% stated in this paper. Also, in Table  11 , this comparison is made for the IEMOCAP corpus and recently published papers.\n\nAccording to the results presented in Tables  9 to 11 , since the EMODB dataset has a more unbalanced distribution of data than the SAVEE and that part of the IEMOCAP examined in this study, the weighted classifier H-AWELM was more accurate in recognizing the emotions of the EMODB (91.29%) than the SAVEE (66.94%) and IEMOCAP (67.68%) datasets. In addition, the presented method is compared with the recent references in terms of execution time and response time per one second of the audio file in Fig.  15 . The execution time of  [5] , was 0.75 hours for each EMODB fold and 7.5 hours on all 10 folds, on (Corei3-8GB RAM) processor (Fig.  15(a) ). Despite the high execution time, it has a short response time, 0.6 sec (Fig.  15(b) ). On the other hand,  [6]  has more execution time on the same processor. The use of GMM statistical classifier with high components (1024) has been one of the most important reasons for the high execution time of this algorithm. However, the time required to run the proposed H-AWELM algorithm on each fold of EMODB with the same processor specifications is 0.005 hours, which eventually reaches 0.05 hours on 10 folds (the response time of this model is almost zero). In addition, while H-AWELM takes less time to train the network, it also has less response time than other methods. Compared to other references, although the processors tested were not the same and many references used graphical processing unit (GPUs) like Yi 2020  [15] , the fast ELM classifier in the H-AWELM method, has a short",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "G-Mean Value",
      "text": "Dataset/Weighting method execution time and less response time compared to recent methods. In fact, the method presented in this paper has not only achieved better results than deep methods, but also has less execution time and faster response. In addition, bore is most similar to sad compared to the other emotions due to the similarity of their prosodic features and glottal waveforms (91.89% to 5.43%). Anger is also the most similar to happiness among other emotions (93.91% to 6.08%), this similarity is also due to the similarity of their prosodic features (both of them have more energy than the other emotions) and consequently the similarity in their glottal waveforms too. In addition, in the SAVEE confusion matrix, the most similar emotion to anger is happiness (60.61% to 9.52%), this can also be seen in the confusion matrix for the IEMOCAP dataset too (58.64% to 23.66%). In fact, in the method presented in this paper, due to the use of features based on Mel frequency and perceptual linear coefficients inspired by human auditory structure, different emotions of happiness, sad, anger, fear, neutral, bore and disgust are well discernible and distinct in all three emotional datasets.   Compared to the other proposed architectures,  [17]  has achieved 79.12% accuracy using features based on wavelet packet analysis, along with the linear SVM classifier on the EMODB. Also, an accuracy of 76.9% was obtained in  [30]  using a feature selection method based on Fisher criteria on the same dataset. Paper  [106]  achieved 89% and 62% recognition rates on the EMODB and SAVEE datasets respectively by reducing inter-class variance and increasing intra-class variance, in order to train SER systems on emotional datasets with incomplete data and knowledge transfer between them. One reason for the high accuracy of H-AWELM compared to the mentioned studies is the use of a multi-layer architecture based on an ELM-AE to discover and extract more complex features from the input features. In addition, using the glottal waveform and SFCC, ICMC, CQCC features increase the ability to distinguish between different emotions and even recognize the same emotions for both groups of different speakers and the same speakers (see Table  9  to 11). While  [17]  only using features based on wavelet packet analysis, it has not been able to distinguish seven different emotions precisely (low recognition rate of fear 55.05% and neutral 56.96%, compared to 95.27% and 94.97% in H-AWELM). In addition, the paper  [25]  has failed to detect the emotion of sad (8.86%) in the EMODB (although it has been successful in recognizing the SAVEE dataset emotions). As well as the paper  [24]  has produced a SER system with a small recognition rate for anger (83.26%) compared to (93.91%) in H-AWELM, by extracting a high-dimensional feature vector (6373) and using a proposed feature selection algorithm to reduce it. Also, the recognition rate of the proposed H-AWELM network on the SAVEE and IEMOCAP datasets are comparable to many of the recent state of the works so far in the same experimental conditions (Table  10  to 11). However, recent results published by the authors  [5] , using the pQPSO dimension reduction method and the Gaussian classifier, have achieved higher accuracy on this emotional database. It seems that the proposed weighted classifier H-AWELM in this case, has not been able to more accurately detect different emotions on IEMOCAP. However, in the proposed architecture, due to the use of ELM, the response time of the algorithm is less than many other classifiers, including the Gaussian classifier.\n\nIn the following, Table  12  shows the recognition rate of the H-AWELM network for ten different folds of EMODB. According to the features used in this study, the emotions expressed by each speaker (with any gender), are well recognizable. In fact, the emotions expressed by the female speaker (92.55%) and the emotions expressed by the male speaker (92.04%) are recognized well by the proposed architecture too. Actually, using the ICMC, CQCC and SFCC features that have a good ability to confirm and recognize speakers, is one of the reasons for recognizing emotions in different speakers. In addition, the use of glottal waveform features has led to better recognition and differentiation of different genders' emotions too. Despite the high accuracy of the proposed architecture, one of its problems and limitations is the high memory consumption due to the hierarchical structure and high dimensions of the ELM's output layer weights due to the high dimensions of the input feature vector. In the H-AWELM model, the memory consumed during the training, is equal to the sum of the weights of the hierarchical ELM-AE's layers (βi) with the output layer weights of the ELM classifier. Fig.  19  shows the memory consumption in the various H-AWELM network configurations. The best result obtained on the EMODB dataset, requires 3.232 GB of memory consumption. Due to the memory consumed during training, it will not be possible to further examine the results by different configurations of this network.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "5-3 Multilingual Results",
      "text": "Recently, interest in SER systems has been increased from monolingual scenarios to multilingual ones so these systems are able to recognize speech emotions across languages like any humans. On the one hand, this interest can promote full development of SER systems in the real world and provide an opportunity to examine similarities between languages so that other speech instances from different datasets can be used in the absence of training data in emotional models. This is also useful when emotional dataset samples are low; therefore, the other datasets will be used in the training set to boost efficiency and recognition rates.\n\nIn order to investigate the performance of the proposed system in a multilingual mode, exploiting datasets with different languages can be very useful and effective. It should be noted that EMODB is in German and SAVEE and the IEMOCAP are in British English and American English; respectively. To evaluate the proposed system in a cross-speaker mode, all datasets must be included in the training set and only one speaker will be in the test set. The accuracy rate for each dataset will be also equal to the average of the results for all speakers of that particular dataset. Since all three datasets will be employed as training and test sets in this experiment, they should have the same level of participation and emotions. For this purpose, the following four emotions of neutral, happiness, anger, and sad presented in all corpora will be used. Table  13  shows details of the utterances' numbers chosen from each corpus. The proposed system is only useful if it can produce comparable results in a multilingual mode compared with a monolingual system. As cited in Table  14 , the recognition rate of 75.94% is achieved in the multilingual mode for EMODB corpus compared with 94.16% in monolingual case. This recognition rate for SAVEE corpus in multilingual mode is 47.23% compared with monolingual accuracy of 75.33%. But the difference for the IEMOCAP dataset is smaller than the other two, 66.11% vs. 67.68% in multilingual and monolingual cases; respectively. The reason for this small difference is that the IEMOCAP corpus has more utterances than the other two datasets; therefore, it has a larger volume of training set and the difference of accuracy rates in multilingual and monolingual modes is less than that of two other corpora. Also, it was expected that the accuracy rate in the multilingual mode would be better than the monolingual one, because the data in the training set has increased. However, according to the results presented in Table  14 , it seems useless to add English data to the German EMODB training set or adding German data to the English SAVEE and IEMOCAP training sets. The reason for this decrease in accuracy rate is that the extracted features are dependent on the environment and specified languages; however, it is better to use features that are independent of environmental and linguistic conditions in future work.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "5-4 Statistical Analysis Of H-Awelm",
      "text": "In this section, the proposed method is evaluated in terms of speed, accuracy, generality and output richness criteria. Statistical analysis of the presented results is used to organize, summarize and further compare it, by descriptive statistical methods.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "5-4-1 T-Test",
      "text": "The purpose of this section is to compare the best results reported in the paper with the best results reported so far in other references through t-test evaluation. Table  15  shows the best recognition rate ever obtained for each emotional data set in LOSO mode. Also, the results of p-value evaluation using t-test with confidence value (∝) of 0.01 are also shown in Table  15 . In this test, we want to check whether the mean of the best recognition rate obtained by the H-AWELM ( ) on emotional datasets is less than or equal to the mean of the best recognition rate obtained by the competitor ( ) (hypothesis zero or ), or not (opposite hypothesis or )? This is shown in relation  (52) , : ≤ : >  (52)  Considering the p-values for EMODB, it can be concluded that the recognition rate obtained by the H-AWELM, with a high confidence value, has a significant difference compared to the best method presented so far on this dataset and in LOSO mode. In the case of SAVEE dataset, although the mean obtained in this study is higher than the average of the competitor, this superiority cannot be claimed with high reliability (p = 0.46). Also, in the case of the IEMOCAP method, the average obtained from the proposed method was lower than those published earlier by the authors.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "5-4-2 Dataset'S Recognition Rates",
      "text": "As shown in Fig  20  and in comparison, with those published earlier by the authors, the highest mean and median value among the three different emotional datasets is related to EMODB and the lowest value is related to SAVEE. Due to the small number of samples of the training set compared to the samples of the test set in SAVEE dataset compared to EMODB and IEMOCAP, a significant decrease in the recognition rate in LOSO mode is evident in this dataset. The H-AWELM method also had the highest recognition rates on EMODB and SAVEE, while  [5]  method produced better results on IEMOCAP. Fig.  20  Comparison of the datasets' recognition rate of the presented methods by the authors using measures of central tendency One of the reasons for the high recognition rate of IEMOCAP using the pQPSO method in  [5] , is the high utterances' number (5531 samples) in this dataset compared to EMODB and SAVEE datasets (535 and 480 samples respectively) which has achieved better results by using the GMM statistical classifier with more training samples. In addition, these results indicate that the long-term features used in the H-AWELM method have improved results on the acted datasets (EMODB and SAVEE). However, the short-term features in  [5]  have yielded better results on IEMOCAP with both improvisations scenarios. In addition, the short-term features used in  [5]  are suitable for longer speech formats (the utterances' average length of IEMOCAP in these experiments is 4.73 ± 0.18 seconds). While the long-term features in the H-AWELM are more suitable for speech formats with shorter average length (the utterances' average length of EMODB and SAVEE is 2 to 3 and 3.85 ± 0.33 seconds respectively). Therefore, it can be concluded that the presented method in  [5]  will have a higher recognition rate on datasets with more samples but will distinguish less emotions (4 different emotions in IEMOCAP in comparison to 7 emotions in EMODB and SAVEE). While the long-term features and the H-AWELM method has better recognition rate on the shorter-length datasets and will differentiate more emotions (7 emotions in EMODB and SAVEE in comparison to 4 different emotions in IEMOCAP).",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "5-4-3 Emotions' Recognition Rates",
      "text": "Fig.  21  shows the recognition rate of different emotions for the proposed methods by the authors, and for different emotion sets. According to Fig.  21 (a ), all the proposed methods identified anger and sad emotions at a higher rate on the EMODB dataset, due to the large number of its samples. This is quite evident, for the neutral emotion, which has many samples in SAVEE and IEMOCAP datasets too (Fig.  21  (b) and Fig.  21 (c )). In addition, sad and anger emotions with fewer samples in IEMOCAP has lower recognition rates (Fig.  21 (c) ).\n\nAccording to Fig.  21 (d) , bore, neutral, happiness, and fear emotions has higher recognition rates using the longterm features and the H-AWELM method. While neutral, anger, disgust, and surprise emotions have better results with short-term features and the pQPSO method  [5] . Emotions with instantaneous changes (such as anger and surprise) appear to have higher recognition rates using short-term features and the GMM statistical classifier  [5] . Also, emotions that change slightly over time (such as bore and neutral) are more recognizable with long-term features and H-AWELM method. In addition, because the disgust emotion has fewer acoustic features and low sample numbers in emotional dataset, its detection seems to be more complex than other emotions, and the most researches did not report its recognition rate.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "5-4-4 Correlated Emotions' Recognition Rates",
      "text": "Fig.  22  provides a comparison on the effect of H-AWELM features, on distinguishing the correlated emotions for the EMODB dataset (these emotions have already been identified in Fig.  1 ). In this evaluation, an experiment was performed with H-AWELM model without SFCC, ICMC, CQCC, GBFB, and glottal features, respectively, and the recognition rate of the correlated emotions in each case, are shown in Fig.  22 (a to c ). As it is clear in Fig.  22  (a), the absence of any of these features has a negative impact on the distinction of correlated pairs (bore, sad). The SFCC and glottal waveform features have the most effects on differentiation of these two emotions than the other features. Also, GBFB and CQCC features has a little effect on distinguishing of (fear, disgust) pair (Fig.  22  (b )), while the glottal waveform features have the greatest effect on (fear, disgust) and (anger, happiness) recognition rates too. Fig.  23  shows a comparison between the correlated emotions' classification error, compared to the most recently presented results for the EMODB dataset.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Current Methods",
      "text": "According to Fig.  23 (d) , many recent methods have failed to distinguish all of the correlated emotions well at high rates, and a classification error has been occurred. Compared to the recent works, the H-AWELM method has produced a high recognition rate for similar and correlated emotions. However, the method proposed by Tuncer  [18]  has also been able to differentiate correlated and similar emotions at a higher rate than other recently methods.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "6-Discussion",
      "text": "In this paper, a new method for SER has been proposed based on the H-ELM with a new weighting method as a classifier that makes the final system more accurate in contrast to the recently weighting algorithms as well as many recently published SER systems. Using the glottal waveform features -which will have a great impact on the speaker's speech style, behavior and emotional states-the ability to identify and recognize similar emotions has been done well. In addition, the use of CQCC, ICMC and SFCC features, which have not been used in SER systems so far, allows better recognition and differentiation of the emotions of different speakers with different genders. This paper also, utilizes the spectro-temporal GBFB and SGBFB features of input speech signal which are efficient in reducing redundancy between feature components and have good discriminative properties in both spectral and temporal domains. In this research, various experiments have been conducted to find appropriate features, statistics, feature selection numbers, and classification methods with their kernels and structures. For example, the results of the experiments indicate that the set of features has the most impact on the accuracy rate of the system including MFCC, PLPC, CQCC, ICMC, and SFCC along with their first-and second-order derivatives and also GBFB and SGBFB features. Likewise, a good set of long-term statistics (i.e. mean, SD, skewness, and kurtosis), suitable for extracting features related to utterances, was obtained through numerous experiments. Due to the high dimensional feature vector in the proposed algorithm, classical feature selection methods and a new quantum-inspired dimension reduction method with a variety of feature selection numbers could be used in which, according to the experiments, the best selected feature numbers with the best accuracy rates were 3000 and 5000. In addition, the effects of various classifiers including different SVM structures, deep NNs, and different ELM-based methods with various kernels and structures have been additionally investigated. Finally, the H-ELM method has been selected with hyperbolic tangent activation function as kernel and a significant simple structure which learns considerably faster than existing learning methods. With respect to the new weighting method, the proposed hierarchical adaptive weighted ELM network is able to minimize the total misclassification costs with low computational complexity and yield a more accurate model.\n\nTo prove this, various experiments have been done on four different recent weighting methods compare to the proposed weighting algorithm on three different emotional speech databases in both 3000 and 5000 number of selected features. Despite the above-mentioned advantages and as it is clear from the experimental results, the ELM shows a very sensitive performance regarding to the number of hidden layers and the number of neurons in each layer, so it is necessary to use a faster method to find the optimal number of layers and neurons. In addition to the optimal neuron numbers, due to the proposed hierarchical structure, memory is also a critical issue for this strategy and high memory requirement is a major drawback of the proposed method that can increase the implementation costs. Then, in this case, the training phase can be executed on parallel computers to fix high memory usage issue.\n\nAnother limitation presented in this study is related to exploiting the environmental and language-dependent features that are not fully robust and make the results in multilingual mode not as good as the monolingual one.\n\nTo avoid this, it is better to improve feature sets to be language-independent that are more robust features in the future.\n\nAll the mentioned experimental results on three famous emotional databases have been presented to verify the efficacy of the proposed method and to show improved and better results compared with the recently-published works.\n\nFor future work, the authors would like to establish deep NNs instead of layers of the proposed H-AWELM, to improve and use more robust and language-independent features, and to examine more emotional corpora in both multilingual and cross-corpus cases. The purpose of this experiment compared with previous ones is to find the relationship between emotions and languages as well as speech features among languages.",
      "page_start": 29,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of emotion correlation rates using correlation coefficient index",
      "page": 2
    },
    {
      "caption": "Figure 2: , the overall SER system consists of three main parts: preprocessing and feature extraction,",
      "page": 5
    },
    {
      "caption": "Figure 2: Overall proposed SER system",
      "page": 5
    },
    {
      "caption": "Figure 3: Gabor filters",
      "page": 6
    },
    {
      "caption": "Figure 3: ). It can be written as:",
      "page": 7
    },
    {
      "caption": "Figure 4: Because of the locality property, the Gabor representation is physically more realistic than the Fourier one. It",
      "page": 7
    },
    {
      "caption": "Figure 5: First, a logarithmically-scaled mel-spectrogram,",
      "page": 7
    },
    {
      "caption": "Figure 5: a). Then, 41 Gabor filters (Fig. 5b) are generated for all pairs of spectral and",
      "page": 7
    },
    {
      "caption": "Figure 4: a) Real part of a two-dimensional (2D) Gabor filter, b) Absolute values of filter’s transfer function",
      "page": 7
    },
    {
      "caption": "Figure 5: Feature extraction scheme with GBFB, a) Input log mel-spectrogram b) 41 filters of GBFB, c) Resulting 455-dimensional feature",
      "page": 8
    },
    {
      "caption": "Figure 6: ). SGBFB features are also those extracted with these two 1D GBFBs, instead of 2D Gabor",
      "page": 8
    },
    {
      "caption": "Figure 7: The experiment results in [1] revealed that since spectral and temporal processing can be performed independently",
      "page": 8
    },
    {
      "caption": "Figure 6: 1D Gabor filter, absolute (blue), real (red), and imaginary (yellow) parts",
      "page": 8
    },
    {
      "caption": "Figure 7: Feature extraction scheme with 1D GBFB, a) Input LMspec, b) 2D convolved with spectral 1D GBFB, c) 2D convolved with",
      "page": 8
    },
    {
      "caption": "Figure 8: , the H-ELM training consists of two successive stages, an unsupervised",
      "page": 14
    },
    {
      "caption": "Figure 8: H-ELM structure (details are explained in the algorithm 1)",
      "page": 14
    },
    {
      "caption": "Figure 9: Moreover, all the experiments have been",
      "page": 16
    },
    {
      "caption": "Figure 9: Statistical functions and recognition rates",
      "page": 17
    },
    {
      "caption": "Figure 10: Since the highest cumulative sum of weights is for 3000 and 5000 dimension numbers, the subsequent experiments",
      "page": 17
    },
    {
      "caption": "Figure 10: .The cumulative sum of features’ scores for three different feature selection methods",
      "page": 17
    },
    {
      "caption": "Figure 11: ). Based on the results presented on",
      "page": 18
    },
    {
      "caption": "Figure 12: According to the results, the tangent hyperbolic transfer function",
      "page": 18
    },
    {
      "caption": "Figure 11: H-ELM hidden layer and neuron numbers",
      "page": 18
    },
    {
      "caption": "Figure 12: Recognition rates of different kernel functions",
      "page": 18
    },
    {
      "caption": "Figure 13: shows the value of",
      "page": 20
    },
    {
      "caption": "Figure 13: The G-mean value of different weighting methods",
      "page": 20
    },
    {
      "caption": "Figure 14: compares the presented proposed method with other recently published",
      "page": 20
    },
    {
      "caption": "Figure 15: The execution time of [5], was",
      "page": 20
    },
    {
      "caption": "Figure 15: (b)). On the other hand, [6] has more",
      "page": 20
    },
    {
      "caption": "Figure 14: Comparison of H-AWELM recognition rate with state of the art (EMODB)",
      "page": 21
    },
    {
      "caption": "Figure 16: to 18 show the confusion matrices related to the EMODB, SAVEE, and IEMOCAP databases. According",
      "page": 22
    },
    {
      "caption": "Figure 15: Comparison (a) Execution time and (b) Response time with state of the art (EMODB)",
      "page": 22
    },
    {
      "caption": "Figure 16: Confusion matrix of the proposed method for EMODB",
      "page": 23
    },
    {
      "caption": "Figure 17: Confusion matrix of the proposed method for SAVEE",
      "page": 23
    },
    {
      "caption": "Figure 18: Confusion matrix of proposed method for the IEMOCAP",
      "page": 23
    },
    {
      "caption": "Figure 19: shows the memory consumption in the various H-AWELM network configurations. The",
      "page": 24
    },
    {
      "caption": "Figure 19: Comparison of memory consumption in different H-AWELM configurations",
      "page": 24
    },
    {
      "caption": "Figure 20: and in comparison, with those published earlier by the authors, the highest mean and median",
      "page": 26
    },
    {
      "caption": "Figure 20: Comparison of the datasets’ recognition rate of the presented methods by the authors using measures of central tendency",
      "page": 26
    },
    {
      "caption": "Figure 21: shows the recognition rate of different emotions for the proposed methods by the authors, and for different",
      "page": 26
    },
    {
      "caption": "Figure 21: (a), all the proposed methods identified anger and sad emotions at a higher",
      "page": 26
    },
    {
      "caption": "Figure 21: (b) and Fig. 21 (c)). In addition, sad and",
      "page": 27
    },
    {
      "caption": "Figure 21: (d), bore, neutral, happiness, and fear emotions has higher recognition rates using the long-",
      "page": 27
    },
    {
      "caption": "Figure 21: Comparison of the emotions’ recognition rates of the presented method by the authors",
      "page": 27
    },
    {
      "caption": "Figure 22: provides a comparison on the effect of H-AWELM features, on distinguishing the correlated emotions for",
      "page": 27
    },
    {
      "caption": "Figure 1: ). In this evaluation, an experiment was",
      "page": 27
    },
    {
      "caption": "Figure 22: (a to c). As it is clear in Fig. 22",
      "page": 27
    },
    {
      "caption": "Figure 22: (b)), while the glottal waveform features have the greatest effect on (fear, disgust) and (anger, happiness)",
      "page": 27
    },
    {
      "caption": "Figure 23: shows a comparison between the correlated emotions' classification error, compared to the most recently",
      "page": 27
    },
    {
      "caption": "Figure 23: (a) shows the classification error of the recently methods for (bore, sad) emotions; Fig. 23 (b) shows the",
      "page": 28
    },
    {
      "caption": "Figure 23: (c) shows classification error of (anger, happiness)",
      "page": 28
    },
    {
      "caption": "Figure 23: (d) shows the total number of correlated emotion classification errors.",
      "page": 28
    },
    {
      "caption": "Figure 22: Comparison of the effect of different features on the recognition rate (WAR) of EMODB correlated emotions (a) (bore, sad) (b)",
      "page": 28
    },
    {
      "caption": "Figure 23: Comparison of EMODB correlated emotion classification error in the proposed method and recent researchs (a) bore and sad (b)",
      "page": 28
    },
    {
      "caption": "Figure 23: (d), many recent methods have failed to distinguish all of the correlated emotions well at",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Current researches to solve the SER system challenges",
      "data": [
        {
          "Reference": "[7]",
          "Year": "2020",
          "Proposed method": "DCNN as classifier",
          "WAR": "86.10",
          "UAR": "N/A"
        },
        {
          "Reference": "[8]",
          "Year": "2020",
          "Proposed method": "CNN as classifier",
          "WAR": "N/A",
          "UAR": "90.01"
        },
        {
          "Reference": "[10]",
          "Year": "2020",
          "Proposed method": "DL to extract features",
          "WAR": "90.21",
          "UAR": "N/A"
        },
        {
          "Reference": "[11]",
          "Year": "2020",
          "Proposed method": "RCNN as classifier",
          "WAR": "90.30",
          "UAR": "N/A"
        },
        {
          "Reference": "[12]",
          "Year": "2020",
          "Proposed method": "CNN for feature extraction and bidirectional deep neural network LSTM for feature classification",
          "WAR": "85.57",
          "UAR": "N/A"
        },
        {
          "Reference": "[13]",
          "Year": "2021",
          "Proposed method": "DL to extract features",
          "WAR": "N/A",
          "UAR": "90.01"
        },
        {
          "Reference": "[9]",
          "Year": "2020",
          "Proposed method": "DCNN for feature extraction",
          "WAR": "90.5",
          "UAR": "N/A"
        },
        {
          "Reference": "[14]",
          "Year": "2020",
          "Proposed method": "semi-supervised GAN as classifier",
          "WAR": "65.20",
          "UAR": "68.00"
        },
        {
          "Reference": "[15]",
          "Year": "2020",
          "Proposed method": "GAN and AE as classifier",
          "WAR": "84.49",
          "UAR": "83.31"
        },
        {
          "Reference": "[16]",
          "Year": "2020",
          "Proposed method": "adversarial AE for features discriminative recognition",
          "WAR": "N/A",
          "UAR": "66.70"
        },
        {
          "Reference": "[17]",
          "Year": "2020",
          "Proposed method": "Wavelet analysis to extract features",
          "WAR": "N/A",
          "UAR": "79.20"
        },
        {
          "Reference": "[18]",
          "Year": "2021",
          "Proposed method": "Tunable Q wavelet transform to extract features",
          "WAR": "90.09",
          "UAR": "N/A"
        },
        {
          "Reference": "[20]",
          "Year": "2020",
          "Proposed method": "BHMM as classifier",
          "WAR": "87.29",
          "UAR": "N/A"
        },
        {
          "Reference": "[21]",
          "Year": "2020",
          "Proposed method": "Feature extraction using a triangular filter bank",
          "WAR": "77.08",
          "UAR": "N/A"
        },
        {
          "Reference": "[28]",
          "Year": "2020",
          "Proposed method": "Use HHH coefficients to extract features",
          "WAR": "81.80",
          "UAR": "N/A"
        },
        {
          "Reference": "[29]",
          "Year": "2020",
          "Proposed method": "Use adaptive domain-aware representation learning method to extract domain-dependent features",
          "WAR": "73.02",
          "UAR": "65.86"
        },
        {
          "Reference": "[13]",
          "Year": "2021",
          "Proposed method": "Use sequential learning to find dependencies between features",
          "WAR": "N/A",
          "UAR": "90.01"
        },
        {
          "Reference": "[30]",
          "Year": "2020",
          "Proposed method": "Use the active feature selection method to select the feature",
          "WAR": "N/A",
          "UAR": "76.90"
        },
        {
          "Reference": "[22]",
          "Year": "2021",
          "Proposed method": "Use the feature weighting method based on emotional groups to select the features",
          "WAR": "72.19",
          "UAR": "N/A"
        },
        {
          "Reference": "[23]",
          "Year": "2020",
          "Proposed method": "CMFC to select features",
          "WAR": "85.61",
          "UAR": "N/A"
        },
        {
          "Reference": "[24]",
          "Year": "2020",
          "Proposed method": "Use of robust discriminative sparse regression to select discriminative features",
          "WAR": "N/A",
          "UAR": "86.19"
        },
        {
          "Reference": "[26]",
          "Year": "2021",
          "Proposed method": "Using RF with group learning model and weighted binary cuckoo algorithm to select superior features",
          "WAR": "83.70",
          "UAR": "N/A"
        },
        {
          "Reference": "[27]",
          "Year": "2020",
          "Proposed method": "Using discriminative non-negative matrix factorization to reduce features dimensions",
          "WAR": "82.80",
          "UAR": "83.30"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: Current researches to solve the SER system challenges",
      "data": [
        {
          "Reference": "[7]",
          "Year": "2020",
          "Proposed method": "DCNN as classifier",
          "WAR": "86.10",
          "UAR": "N/A"
        },
        {
          "Reference": "[8]",
          "Year": "2020",
          "Proposed method": "CNN as classifier",
          "WAR": "N/A",
          "UAR": "90.01"
        },
        {
          "Reference": "[10]",
          "Year": "2020",
          "Proposed method": "DL to extract features",
          "WAR": "90.21",
          "UAR": "N/A"
        },
        {
          "Reference": "[11]",
          "Year": "2020",
          "Proposed method": "RCNN as classifier",
          "WAR": "90.30",
          "UAR": "N/A"
        },
        {
          "Reference": "[12]",
          "Year": "2020",
          "Proposed method": "CNN for feature extraction and bidirectional deep neural network LSTM for feature classification",
          "WAR": "85.57",
          "UAR": "N/A"
        },
        {
          "Reference": "[13]",
          "Year": "2021",
          "Proposed method": "DL to extract features",
          "WAR": "N/A",
          "UAR": "90.01"
        },
        {
          "Reference": "[9]",
          "Year": "2020",
          "Proposed method": "DCNN for feature extraction",
          "WAR": "90.5",
          "UAR": "N/A"
        },
        {
          "Reference": "[14]",
          "Year": "2020",
          "Proposed method": "semi-supervised GAN as classifier",
          "WAR": "65.20",
          "UAR": "68.00"
        },
        {
          "Reference": "[15]",
          "Year": "2020",
          "Proposed method": "GAN and AE as classifier",
          "WAR": "84.49",
          "UAR": "83.31"
        },
        {
          "Reference": "[16]",
          "Year": "2020",
          "Proposed method": "adversarial AE for features discriminative recognition",
          "WAR": "N/A",
          "UAR": "66.70"
        },
        {
          "Reference": "[17]",
          "Year": "2020",
          "Proposed method": "Wavelet analysis to extract features",
          "WAR": "N/A",
          "UAR": "79.20"
        },
        {
          "Reference": "[18]",
          "Year": "2021",
          "Proposed method": "Tunable Q wavelet transform to extract features",
          "WAR": "90.09",
          "UAR": "N/A"
        },
        {
          "Reference": "[20]",
          "Year": "2020",
          "Proposed method": "BHMM as classifier",
          "WAR": "87.29",
          "UAR": "N/A"
        },
        {
          "Reference": "[21]",
          "Year": "2020",
          "Proposed method": "Feature extraction using a triangular filter bank",
          "WAR": "77.08",
          "UAR": "N/A"
        },
        {
          "Reference": "[28]",
          "Year": "2020",
          "Proposed method": "Use HHH coefficients to extract features",
          "WAR": "81.80",
          "UAR": "N/A"
        },
        {
          "Reference": "[29]",
          "Year": "2020",
          "Proposed method": "Use adaptive domain-aware representation learning method to extract domain-dependent features",
          "WAR": "73.02",
          "UAR": "65.86"
        },
        {
          "Reference": "[13]",
          "Year": "2021",
          "Proposed method": "Use sequential learning to find dependencies between features",
          "WAR": "N/A",
          "UAR": "90.01"
        },
        {
          "Reference": "[30]",
          "Year": "2020",
          "Proposed method": "Use the active feature selection method to select the feature",
          "WAR": "N/A",
          "UAR": "76.90"
        },
        {
          "Reference": "[22]",
          "Year": "2021",
          "Proposed method": "Use the feature weighting method based on emotional groups to select the features",
          "WAR": "72.19",
          "UAR": "N/A"
        },
        {
          "Reference": "[23]",
          "Year": "2020",
          "Proposed method": "CMFC to select features",
          "WAR": "85.61",
          "UAR": "N/A"
        },
        {
          "Reference": "[24]",
          "Year": "2020",
          "Proposed method": "Use of robust discriminative sparse regression to select discriminative features",
          "WAR": "N/A",
          "UAR": "86.19"
        },
        {
          "Reference": "[26]",
          "Year": "2021",
          "Proposed method": "Using RF with group learning model and weighted binary cuckoo algorithm to select superior features",
          "WAR": "83.70",
          "UAR": "N/A"
        },
        {
          "Reference": "[27]",
          "Year": "2020",
          "Proposed method": "Using discriminative non-negative matrix factorization to reduce features dimensions",
          "WAR": "82.80",
          "UAR": "83.30"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Table 2. Specifications of emotional datasets",
      "data": [
        {
          "Dataset": "",
          "Language": "",
          "Total no. of \nSpeakers": "",
          "Emotions": "Anger \nDisgust \nFear \nHappiness \nSad \nBore  \nSurprise \nNeutral \n(A) \n(D) \n(F) \n(H) \n(SA) \n(B) \n(SU) \n(N)"
        },
        {
          "Dataset": "EMODB",
          "Language": "German",
          "Total no. of \nSpeakers": "10",
          "Emotions": "127 \n46 \n69 \n71 \n62 \n81 \n- \n79"
        },
        {
          "Dataset": "SAVEE",
          "Language": "English",
          "Total no. of \nSpeakers": "4",
          "Emotions": "60 \n60 \n60 \n60 \n60 \n- \n60 \n120"
        },
        {
          "Dataset": "IEMOCAP",
          "Language": "English",
          "Total no. of \nSpeakers": "10",
          "Emotions": "1103 \n- \n- \n1636 \n1084 \n- \n- \n1708"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 3: Evaluated feature extraction methods and their recognition rates (WAR) on EMODB dataset",
      "data": [
        {
          "Statistical Function": "Mean",
          "Relation": "n\n1\n \nx\nx\ni\n \nn\ni\n\n1"
        },
        {
          "Statistical Function": "Quadratic mean",
          "Relation": "n\n1\n2\n \nx\ni\n\n1\nn"
        },
        {
          "Statistical Function": "Harmonic mean",
          "Relation": "n\n1\n\n1\n\n1\n \n(\nx\n)\ni\n\n1\nn"
        },
        {
          "Statistical Function": "Standard deviation (std)",
          "Relation": "n\n1\n2\n \n\n\n(\nx\n\nx\n)\ni\nn\n \ni\n\n1"
        },
        {
          "Statistical Function": "Skewness",
          "Relation": "n\nx\n\nx\n1\n \ni\n3\nSkew\n(\n)\n \nn\n\ni\n\n1"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 3: Evaluated feature extraction methods and their recognition rates (WAR) on EMODB dataset",
      "data": [
        {
          "Statistical \nRelation \nFunction": "Kurtosis"
        },
        {
          "Statistical \nRelation \nFunction": "Percentile"
        },
        {
          "Statistical \nRelation \nFunction": "Zero crossing \n(ZC)"
        },
        {
          "Statistical \nRelation \nFunction": "Geometric \nmean"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 5: Among these, the mRMR",
      "data": [
        {
          "Feature selection method": "LLC",
          "Feature selection No": "5000",
          "WAR": "89.58%"
        },
        {
          "Feature selection method": "CFS",
          "Feature selection No": "5000",
          "WAR": "90.06%"
        },
        {
          "Feature selection method": "mRMR",
          "Feature selection No": "3000",
          "WAR": "91.29%"
        },
        {
          "Feature selection method": "",
          "Feature selection No": "5000",
          "WAR": "91.24%"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 7: The degree of IR of different datasets",
      "data": [
        {
          "DB": "",
          "Weighting Method": "",
          "Feature Selection Number": "5000"
        },
        {
          "DB": "",
          "Weighting Method": "",
          "Feature Selection Number": "WAR (%)"
        },
        {
          "DB": "EMODB",
          "Weighting Method": "W1",
          "Feature Selection Number": "91.14±0.34"
        },
        {
          "DB": "",
          "Weighting Method": "W2",
          "Feature Selection Number": "90.23±0.3"
        },
        {
          "DB": "",
          "Weighting Method": "W3",
          "Feature Selection Number": "91.2±0.28"
        },
        {
          "DB": "",
          "Weighting Method": "W4",
          "Feature Selection Number": "90.97±0.36"
        },
        {
          "DB": "",
          "Weighting Method": "Without Weighting",
          "Feature Selection Number": "88.81±1.71"
        },
        {
          "DB": "",
          "Weighting Method": "Proposed Weighting",
          "Feature Selection Number": "91.24±0.32"
        },
        {
          "DB": "SAVEE",
          "Weighting Method": "W1",
          "Feature Selection Number": "65.5±1.18"
        },
        {
          "DB": "",
          "Weighting Method": "W2",
          "Feature Selection Number": "65±0.91"
        },
        {
          "DB": "",
          "Weighting Method": "W3",
          "Feature Selection Number": "64.9±0.41"
        },
        {
          "DB": "",
          "Weighting Method": "W4",
          "Feature Selection Number": "60.31±1.18"
        },
        {
          "DB": "",
          "Weighting Method": "Without Weighting",
          "Feature Selection Number": "57.24±2.15"
        },
        {
          "DB": "",
          "Weighting Method": "Proposed Weighting",
          "Feature Selection Number": "65.73±1"
        },
        {
          "DB": "IEMOCAP",
          "Weighting Method": "W1",
          "Feature Selection Number": "63.63±0.39"
        },
        {
          "DB": "",
          "Weighting Method": "W2",
          "Feature Selection Number": "52.2±0.35"
        },
        {
          "DB": "",
          "Weighting Method": "W3",
          "Feature Selection Number": "65.76±0.26"
        },
        {
          "DB": "",
          "Weighting Method": "W4",
          "Feature Selection Number": "65.38±0.27"
        },
        {
          "DB": "",
          "Weighting Method": "Proposed Weighting",
          "Feature Selection Number": "67.68±0.33"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 9: Recognition rates (%) on EMODB reported in literature",
      "data": [
        {
          "Reference": "Haider et al., 2020 [30]",
          "WAR": "84.62",
          "Reference \nUAR": "Bhargava 2013 [78] \n76.90",
          "UAR \nReference": "N/A \nÖzseven 2019 [96]",
          "UAR": "N/A"
        },
        {
          "Reference": "Zhao et al., 2019 [61]",
          "WAR": "85.10",
          "Reference \nUAR": "Badshah et al., 2019 [79] \n79.70",
          "UAR \nReference": "N/A \nDeb 2017 [97]",
          "UAR": "N/A"
        },
        {
          "Reference": "Chen et al., 2018 [62]",
          "WAR": "85.32",
          "Reference \nUAR": "Zhang et al., 2013 [80] \n82.82",
          "UAR \nReference": "N/A \nMeng et al., 2019 [98]",
          "UAR": "N/A"
        },
        {
          "Reference": "Song et al., 2020 [24]",
          "WAR": "85.54",
          "Reference \nUAR": "Wu et al., 2011 [81] \n86.19",
          "UAR \nReference": "N/A \nVasuki 2020 [99]",
          "UAR": "N/A"
        },
        {
          "Reference": "Sidorov et al., 2016 [63]",
          "WAR": "85.57",
          "Reference \nUAR": "Sun 2015 [82] \nN/A",
          "UAR \nReference": "N/A \nSajjad 2020 [12]",
          "UAR": "N/A"
        },
        {
          "Reference": "Aghajani et al., 2020 [64]",
          "WAR": "85.61",
          "Reference \nUAR": "Sun et al., 2015 [83] \nN/A",
          "UAR \nReference": "N/A \nChen et al., 2020 [23]",
          "UAR": "N/A"
        },
        {
          "Reference": "Li et al., 2021 [22]",
          "WAR": "86.00",
          "Reference \nUAR": "N/A \nXu et al., 2015 [84]",
          "UAR \nReference": "N/A \nEyben et al., 2015 [100]",
          "UAR": "N/A"
        },
        {
          "Reference": "Yüncü et al., 2014 [65]",
          "WAR": "86.10",
          "Reference \nUAR": "Vieira et al., 2020 [28] \nN/A",
          "UAR \nReference": "N/A \nIssa et al., 2020 [7]",
          "UAR": "N/A"
        },
        {
          "Reference": "Khan 2017 [66]",
          "WAR": "86.36",
          "Reference \nUAR": "Man-Wai 2016 [85] \nN/A",
          "UAR \nReference": "N/A \nSingh et al., 2020 [25]",
          "UAR": "N/A"
        },
        {
          "Reference": "Sinith et al., 2015 [67]",
          "WAR": "86.44",
          "Reference \nUAR": "Stuhlsatz et al., 2011 [86] \nN/A",
          "UAR \nReference": "N/A \nJiang et al., 2019 [101]",
          "UAR": "84.53"
        },
        {
          "Reference": "Deb 2017 [68]",
          "WAR": "87.29",
          "Reference \nUAR": "Wen et al., 2017 [87] \nN/A",
          "UAR \nReference": "N/A \nBakhshi et al., 2020 [20]",
          "UAR": "N/A"
        },
        {
          "Reference": "Deb 2016 [69]",
          "WAR": "87.31",
          "Reference \nUAR": "Lotfidereshgi 2017 [88] \nN/A",
          "UAR \nReference": "N/A \nZhang et al., 2017 [102]",
          "UAR": "86.30"
        },
        {
          "Reference": "Tao et al., 2016 [70]",
          "WAR": "87.55",
          "Reference \nUAR": "Sun 2017 [89] \nN/A",
          "UAR \nReference": "N/A \nSun et al., 2019 [103]",
          "UAR": "N/A"
        },
        {
          "Reference": "Kadiri et al., 2015 [71]",
          "WAR": "88.60",
          "Reference \nUAR": "Tzinis et al., 2018 [90] \nN/A",
          "UAR \nReference": "N/A \nHook et al., 2019 [104]",
          "UAR": "N/A"
        },
        {
          "Reference": "Shirani et al., 2016 [72]",
          "WAR": "88.80",
          "Reference \nUAR": "Kalinli 2016 [91] \nN/A",
          "UAR \nReference": "N/A \nJalili et al., 2018 [105]",
          "UAR": "N/A"
        },
        {
          "Reference": "Bashirpour 2016 [73]",
          "WAR": "89.00",
          "Reference \nUAR": "Hou et al., 2020 [27] \nN/A",
          "UAR \nReference": "83.30 \nNguyen et al., 2020 [106]",
          "UAR": "N/A"
        },
        {
          "Reference": "Sugan et al., 2020 [21]",
          "WAR": "89.70",
          "Reference \nUAR": "Daneshfar 2020 [5] \nN/A",
          "UAR \nReference": "N/A \nSun 2019 [107]",
          "UAR": "N/A"
        },
        {
          "Reference": "Luengo 2010 [74]",
          "WAR": "90.09",
          "Reference \nUAR": "Zhang 2021 [26] \nN/A",
          "UAR \nReference": "N/A \nTuncer et al. 2021 [18]",
          "UAR": "N/A"
        },
        {
          "Reference": "Rintala 2020 [75]",
          "WAR": "90.21",
          "Reference \nUAR": "Yi 2019 [92] \nN/A",
          "UAR \nReference": "N/A \nEr 2020 [10]",
          "UAR": "N/A"
        },
        {
          "Reference": "Wang et al., 2020 [17] \nHassan 2012 [76]",
          "WAR": "90.30 \n90.50",
          "Reference \nUAR": "Deb 2018 [93] \nN/A \nYi 2020 [15] \nN/A",
          "UAR \nReference": "N/A \nSun 2020 [11] \n83.31 \nFarooq et al. 2020 [9]",
          "UAR": "N/A \nN/A"
        },
        {
          "Reference": "Daneshfar et al., 2020 [6] \nZao 2014 [77]",
          "WAR": "91.29",
          "Reference \nUAR": "Tawari 2010 [94] \n76.81 \nÖzseven 2018 [95] \nN/A",
          "UAR \nReference": "N/A \nProposed \nN/A",
          "UAR": "88.95"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 9: Recognition rates (%) on EMODB reported in literature",
      "data": [
        {
          "Reference": "Haider et al., 2020 [30]",
          "WAR": "N/A",
          "UAR": "42.4"
        },
        {
          "Reference": "Papakostas et al., 2017 [108]",
          "WAR": "44.00",
          "UAR": "N/A"
        },
        {
          "Reference": "Liu et al., 2018 [109]",
          "WAR": "44.18",
          "UAR": "N/A"
        },
        {
          "Reference": "Noroozi et al., 2017 [110]",
          "WAR": "45.51",
          "UAR": "N/A"
        },
        {
          "Reference": "Vásquez-Correa et al., 2016 [111]",
          "WAR": "47.30",
          "UAR": "N/A"
        },
        {
          "Reference": "Sun et al., 2015 [83]",
          "WAR": "50.00",
          "UAR": "N/A"
        },
        {
          "Reference": "Sun 2017 [89]",
          "WAR": "51.46",
          "UAR": "49.33"
        },
        {
          "Reference": "Wen et al., 2017 [87]",
          "WAR": "53.60",
          "UAR": "N/A"
        },
        {
          "Reference": "Tzinis et al., 2018 [90]",
          "WAR": "54.00",
          "UAR": "53.80"
        },
        {
          "Reference": "Sugan et al., 2020 [21]",
          "WAR": "55.83",
          "UAR": "N/A"
        },
        {
          "Reference": "Sinith et al., 2015 [67]",
          "WAR": "57.50",
          "UAR": "N/A"
        },
        {
          "Reference": "Sun 2015 [82]",
          "WAR": "58.76",
          "UAR": "N/A"
        },
        {
          "Reference": "Daneshfar et al., 2020 [6]",
          "WAR": "59.38",
          "UAR": "55.00"
        },
        {
          "Reference": "Zhang 2021 [26]",
          "WAR": "60.16",
          "UAR": "N/A"
        },
        {
          "Reference": "Daneshfar 2020 [5]",
          "WAR": "60.79",
          "UAR": "N/A"
        },
        {
          "Reference": "Nguyen et al., 2020 [106]",
          "WAR": "62.00",
          "UAR": "N/A"
        },
        {
          "Reference": "Jiang et al., 2019 [101]",
          "WAR": "62.49",
          "UAR": "59.40"
        },
        {
          "Reference": "Wang et al., 2020 [17]",
          "WAR": "66.20",
          "UAR": "81.8"
        },
        {
          "Reference": "Farooq et al., 2020 [9]",
          "WAR": "66.90",
          "UAR": "N/A"
        },
        {
          "Reference": "Proposed",
          "WAR": "66.94±0.65",
          "UAR": "67.90±0.62"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 11: Recognition rates (%) on the IEMOCAP reported in literature",
      "data": [
        {
          "Reference": "Latif et al., 2019 [112]",
          "WAR": "65.20",
          "Reference \nUAR": "Hou et al., 2020 [27] \n60.23",
          "UAR \nReference": "63.80 \nZhao et al., 2019 [61]",
          "UAR": "68.00"
        },
        {
          "Reference": "Zong et al., 2018 [113]",
          "WAR": "65.71",
          "Reference \nUAR": "Tzinis et al., 2018 [90] \n64.80",
          "UAR \nReference": "65.20 \nDaneshfar 2020 [5]",
          "UAR": "65.73"
        },
        {
          "Reference": "Latif et al., 2020 [16]",
          "WAR": "66.80",
          "Reference \nUAR": "Li et al., 2015 [121]  \n66.70",
          "UAR \nReference": "N/A \nDeb 2018 [93]",
          "UAR": "N/A"
        },
        {
          "Reference": "Kwon 2020 [8]",
          "WAR": "66.80",
          "Reference \nUAR": "Vieira et al., 2020 [28] \n73.01",
          "UAR \nReference": "N/A \nYi 2019 [92]",
          "UAR": "62.83"
        },
        {
          "Reference": "Ghosh et al., 2016 [114]",
          "WAR": "67.10",
          "Reference \nUAR": "Jiang et al., 2019 [101] \nN/A",
          "UAR \nReference": "N/A \nLiu et al., 2018 [109]",
          "UAR": "66.20"
        },
        {
          "Reference": "Xie et al., 2019 [115]",
          "WAR": "67.68",
          "Reference \nUAR": "Tzinis 2017 [122] \nN/A",
          "UAR \nReference": "N/A \nProposed",
          "UAR": "63.13"
        },
        {
          "Reference": "Li et al., 2020 [116]",
          "WAR": "69.00",
          "Reference \nUAR": "Deb 2017 [97] \n59.91",
          "UAR \nReference": "N/A \nYeh et al., 2020 [126]",
          "UAR": "70.10"
        },
        {
          "Reference": "Zhao et al., 2018 [117]",
          "WAR": "71.45",
          "Reference \nUAR": "Chen et al., 2018 [62] \n60.10",
          "UAR \nReference": "N/A \nYi 2020 [15]",
          "UAR": "64.22"
        },
        {
          "Reference": "Huang et al., 2018 [118]",
          "WAR": "71.50",
          "Reference \nUAR": "Han et al., 2018 [123] \nN/A",
          "UAR \nReference": "65.70 \nSun 2020 [11]",
          "UAR": "N/A"
        },
        {
          "Reference": "Li et al., 2021 [22]",
          "WAR": "71.75",
          "Reference \nUAR": "Issa et al. 2020 [7] \nN/A",
          "UAR \nReference": "N/A \nLi et al., 2018 [127]",
          "UAR": "N/A"
        },
        {
          "Reference": "Xia 2015 [119]",
          "WAR": "73.02",
          "Reference \nUAR": "Etienne et al., 2018 [124] \n62.40",
          "UAR \nReference": "61.70 \nFan et al., 2020 [128]",
          "UAR": "65.86"
        },
        {
          "Reference": "Zhao et al., 2019 [61]",
          "WAR": "74.80",
          "Reference \nUAR": "Fayek et al., 2017 [125] \nN/A",
          "UAR \nReference": "60.90 \nDaneshfar et al., 2020 [6]",
          "UAR": "N/A"
        },
        {
          "Reference": "Mao et al., 2019 [120]",
          "WAR": "",
          "Reference \nUAR": "Shirani et al., 2016 [72] \n58.02",
          "UAR \nReference": "N/A",
          "UAR": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 9: to 11). While [17] only using features based on wavelet",
      "data": [
        {
          "60.6\n1": "3.9",
          "9.14": "62.0\n1",
          "8.33": "6.10",
          "9.52": "5.21",
          "2.97": "10.5\n8",
          "4.06": "5.36",
          "5.32": "6.71"
        },
        {
          "60.6\n1": "2.18",
          "9.14": "2.32",
          "8.33": "71.5\n0",
          "9.52": "7.29",
          "2.97": "0.31",
          "4.06": "5.01",
          "5.32": "11.3\n5"
        },
        {
          "60.6\n1": "8.60",
          "9.14": "2.29",
          "8.33": "2.81",
          "9.52": "66.6\n7",
          "2.97": "1.33",
          "4.06": "4.99",
          "5.32": "13.2\n8"
        },
        {
          "60.6\n1": "1.64",
          "9.14": "4.52",
          "8.33": "1.45",
          "9.52": "1.25",
          "2.97": "79.2\n9",
          "4.06": "11.5\n1",
          "5.32": "0.30"
        },
        {
          "60.6\n1": "0",
          "9.14": "5.02",
          "8.33": "10.4\n8",
          "9.52": "3.95",
          "2.97": "8.70",
          "4.06": "67.2\n1",
          "5.32": "4.62"
        },
        {
          "60.6\n1": "0",
          "9.14": "1.15",
          "8.33": "19.4\n4",
          "9.52": "10.8\n1",
          "2.97": "0",
          "4.06": "0.63",
          "5.32": "67.9\n5"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 9: to 11). While [17] only using features based on wavelet",
      "data": [
        {
          "95.2\n7": "0",
          "0": "0",
          "3.72": "0",
          "1": "4.1"
        },
        {
          "95.2\n7": "3.03",
          "0": "0",
          "3.72": "85.8\n6",
          "1": "9.83"
        },
        {
          "95.2\n7": "0",
          "0": "5.43",
          "3.72": "0",
          "1": "0"
        },
        {
          "95.2\n7": "0.7",
          "0": "0.80",
          "3.72": "1.43",
          "1": "0"
        },
        {
          "95.2\n7": "0.90",
          "0": "91.6\n6",
          "3.72": "0",
          "1": "0"
        },
        {
          "95.2\n7": "0",
          "0": "0",
          "3.72": "6.08",
          "1": "93.9\n1"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 9: to 11). While [17] only using features based on wavelet",
      "data": [
        {
          "58.64": "9.12",
          "23.66": "65.23",
          "14.50": "13.94",
          "3.18": "11.69"
        },
        {
          "58.64": "7.47",
          "23.66": "16.14",
          "14.50": "68.83",
          "3.18": "7.54"
        },
        {
          "58.64": "13.79",
          "23.66": "22.51",
          "14.50": "5.49",
          "3.18": "58.18"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 12: shows the recognition rate of the H-AWELM network for ten different folds of",
      "data": [
        {
          "fold no (gender)": "1 (m)",
          "WAR": "94.55"
        },
        {
          "fold no (gender)": "2 (f)",
          "WAR": "88.82"
        },
        {
          "fold no (gender)": "3 (f)",
          "WAR": "89.97"
        },
        {
          "fold no (gender)": "4 (m)",
          "WAR": "90.25"
        },
        {
          "fold no (gender)": "5 (m)",
          "WAR": "93.68"
        },
        {
          "fold no (gender)": "6 (m)",
          "WAR": "89.93"
        },
        {
          "fold no (gender)": "7 (f)",
          "WAR": "90.58"
        },
        {
          "fold no (gender)": "8 (f)",
          "WAR": "92.54"
        },
        {
          "fold no (gender)": "9 (m)",
          "WAR": "91.79"
        },
        {
          "fold no (gender)": "10 (f)",
          "WAR": "90.85"
        },
        {
          "fold no (gender)": "Mean (m)",
          "WAR": "92.04"
        },
        {
          "fold no (gender)": "Mean (f)",
          "WAR": "90.55"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 13: shows details of the utterances’ numbers",
      "data": [
        {
          "Train": "EMODB",
          "Test": "EMODB",
          "WAR": "94.16+0.43"
        },
        {
          "Train": "SAVEE",
          "Test": "SAVEE",
          "WAR": "75.33±1.09"
        },
        {
          "Train": "IEMOCAP",
          "Test": "IEMOCAP",
          "WAR": "67.68±0.33"
        },
        {
          "Train": "EMODB, SAVEE, IEMOCAP",
          "Test": "EMODB",
          "WAR": "75.94±1.72"
        },
        {
          "Train": "",
          "Test": "SAVEE",
          "WAR": "47.23±1.19"
        },
        {
          "Train": "",
          "Test": "IEMOCAP",
          "WAR": "66.11±0.31"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 13: shows details of the utterances’ numbers",
      "data": [
        {
          "Dataset": "H-AWELM WAR",
          "EMODB": "91.29±0.43%",
          "SAVEE": "66.94±0.65%",
          "IEMOCAP": "66.94±0.65%"
        },
        {
          "Dataset": "The best competitor WAR",
          "EMODB": "Farooq et al., 2020 [9] \n90.50%",
          "SAVEE": "Farooq et al., 2020 [9] \n66.90%",
          "IEMOCAP": "Daneshfar 2020 [5] \n74.80±0.11%"
        },
        {
          "Dataset": "p-value",
          "EMODB": "\u0000 < 0.01",
          "SAVEE": "\u0000 > 0.01",
          "IEMOCAP": "\u0000 > \u0000. \u0000\u0000"
        },
        {
          "Dataset": "Hypothesis confirmed",
          "EMODB": "\u0000 > \u0000\u0000",
          "SAVEE": "\u0000 ≤ \u0000\u0000",
          "IEMOCAP": "\u0000 ≤ \u0000\u0000"
        }
      ],
      "page": 25
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Separable spectro-temporal Gabor filter bank features: Reducing the complexity of robust features for automatic speech recognition",
      "authors": [
        "Marc Schädler",
        "Birger René",
        "Kollmeier"
      ],
      "year": "2015",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "2",
      "title": "Constant Q cepstral coefficients: A spoofing countermeasure for automatic speaker verification",
      "authors": [
        "Massimiliano Todisco",
        "Héctor Delgado",
        "Nicholas Evans"
      ],
      "year": "2017",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "3",
      "title": "Single frequency filtering approach for discriminating speech and nonspeech",
      "authors": [
        "G Aneeja",
        "B Yegnanarayana"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Further optimisations of constant Q cepstral processing for integrated utterance and text-dependent speaker verification",
      "authors": [
        "Héctor Delgado",
        "Massimiliano Todisco",
        "Md Sahidullah",
        "Achintya Sarkar",
        "Nicholas Evans",
        "Tomi Kinnunen",
        "Zheng-Hua Tan"
      ],
      "year": "2016",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using discriminative dimension reduction by employing a modified quantum-behaved particle swarm optimization algorithm",
      "authors": [
        "Fatemeh Daneshfar",
        "Seyed Kabudian",
        "Jahanshah"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications",
      "doi": "10.1007/s11042-019-08222-8"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using hybrid spectralprosodic features of speech signal/glottal waveform, metaheuristic-based dimensionality reduction, and Gaussian elliptical basis function network classifier",
      "authors": [
        "Fatemeh Daneshfar",
        "Seyed Jahanshah Kabudian",
        "Abbas Neekabadi"
      ],
      "year": "2020",
      "venue": "Applied Acoustics",
      "doi": "10.1016/j.apacoust.2020.107360"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "8",
      "title": "MLT-DNet: Speech emotion recognition using 1D dilated CNN based on multi-learning trick approach",
      "authors": [
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "9",
      "title": "Impact of Feature Selection Algorithm on Speech Emotion Recognition Using Deep Convolutional Neural Network",
      "authors": [
        "M Farooq",
        "F Hussain",
        "N Baloch",
        "F Raja",
        "H Yu",
        "Y Zikria"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "A Novel Approach for Classification of Speech Emotions Based on Deep and Acoustic Features",
      "authors": [
        "M Er"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "11",
      "title": "End-to-End speech emotion recognition with gender information",
      "authors": [
        "T Sun"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "12",
      "title": "Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "13",
      "title": "Att-Net: Enhanced emotion recognition system using lightweight self-attention module",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "14",
      "title": "Robust Semisupervised Generative Adversarial Networks for Speech Emotion Recognition via Distribution Smoothness",
      "authors": [
        "H Zhao",
        "Y Xiao",
        "Z Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Improving Speech Emotion Recognition With Adversarial Data Augmentation Network",
      "authors": [
        "L Yi",
        "M Mak"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "16",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Wavelet packet analysis for speaker-independent emotion recognition",
      "authors": [
        "K Wang",
        "G Su",
        "L Liu",
        "S Wang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques"
    },
    {
      "citation_id": "19",
      "title": "Audio compression with multi-algorithm fusion and its impact in speech emotion recognition",
      "authors": [
        "A Reddy",
        "V Vijayarajan"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "20",
      "title": "Recognition of emotion from speech using evolutionary cepstral coefficients",
      "authors": [
        "A Bakhshi",
        "S Chalup",
        "A Harimi",
        "S Mirhassani"
      ],
      "year": "2020",
      "venue": "Recognition of emotion from speech using evolutionary cepstral coefficients"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using cepstral features extracted with novel triangular filter banks based on bark and ERB frequency scales",
      "authors": [
        "N Sugan",
        "N Srinivas",
        "L Kumar",
        "M Nath",
        "A Kanhe"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Exploiting the potentialities of features for speech emotion recognition",
      "authors": [
        "D Li",
        "Y Zhou",
        "Z Wang",
        "D Gao"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "23",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "24",
      "title": "Speech Emotion Recognition Based on Robust Discriminative Sparse Regression",
      "authors": [
        "P Song",
        "W Zheng",
        "Y Yu",
        "S Ou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "25",
      "title": "An Efficient Language-Independent Acoustic Emotion Classification System",
      "authors": [
        "R Singh",
        "H Puri",
        "N Aggarwal",
        "V Gupta"
      ],
      "year": "2020",
      "venue": "Arabian Journal for Science and Engineering"
    },
    {
      "citation_id": "26",
      "title": "Speech feature selection and emotion recognition based on weighted binary cuckoo search",
      "authors": [
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "27",
      "title": "A supervised non-negative matrix factorization model for speech emotion recognition",
      "authors": [
        "M Hou",
        "J Li",
        "G Lu"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "28",
      "title": "Hilbert-Huang-Hurst-based non-linear acoustic feature vector for emotion classification with stochastic models and learning systems",
      "authors": [
        "V Vieira",
        "R Coelho",
        "F De Assis"
      ],
      "year": "2020",
      "venue": "IET Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Adaptive Domain-Aware Representation Learning for Speech Emotion Recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "D Huang"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition in low-resource settings: An evaluation of automatic feature selection methods",
      "authors": [
        "F Haider",
        "S Pollak",
        "P Albert",
        "S Luz"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "31",
      "title": "Hybrid intelligent deep kernel incremental extreme learning machine based on differential evolution and multiple population grey wolf optimization methods",
      "authors": [
        "Di Wu",
        "Zong Qu",
        "Feng Jiao Guo",
        "Xiao Lin Zhu",
        "Qin Wan"
      ],
      "year": "2019",
      "venue": "Automatika"
    },
    {
      "citation_id": "32",
      "title": "Sparse Deep Tensor Extreme Learning Machine for Pattern Classification",
      "authors": [
        "Jin Zhao",
        "Licheng Jiao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "33",
      "title": "Regularized ensemble neural networks models in the Extreme Learning Machine framework",
      "authors": [
        "Carlos Perales-González",
        "Mariano Carbonero-Ruz",
        "David Becerra-Alonso",
        "Javier Pérez-Rodríguez",
        "Francisco Fernández-Navarro"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "34",
      "title": "Multilayer one-class extreme learning machine",
      "authors": [
        "Haozhen Dai",
        "Jiuwen Cao",
        "Tianlei Wang",
        "Muqing Deng",
        "Zhixin Yang"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "35",
      "title": "Stacked Denoising Extreme Learning Machine Autoencoder Based on Graph Embedding for Feature Representation",
      "authors": [
        "Hongwei Ge",
        "Weiting Sun",
        "Mingde Zhao",
        "Yao Yao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "36",
      "title": "Self-adaptive evolutionary extreme learning machine",
      "authors": [
        "Jiuwen Cao",
        "Zhiping Lin",
        "Guang-Bin Huang"
      ],
      "year": "2012",
      "venue": "Neural processing letters"
    },
    {
      "citation_id": "37",
      "title": "A fast and accurate online sequential learning algorithm for feedforward networks",
      "authors": [
        "Nan Liang",
        "Guang-Bin Ying",
        "Paramasivan Huang",
        "Narasimhan Saratchandran",
        "Sundararajan"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on neural networks"
    },
    {
      "citation_id": "38",
      "title": "Representational learning with extreme learning machine for big data",
      "authors": [
        "Liyanaarachchi Kasun",
        "Hongming Lekamalage Chamara",
        "Guang-Bin Zhou",
        "Chi Huang",
        "Vong"
      ],
      "year": "2013",
      "venue": "IEEE intelligent systems"
    },
    {
      "citation_id": "39",
      "title": "Bidirectional extreme learning machine for regression problem and its learning effectiveness",
      "authors": [
        "Yimin Yang",
        "Yaonan Wang",
        "Xiaofang Yuan"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "40",
      "title": "Semi-supervised and unsupervised extreme learning machines",
      "authors": [
        "Gao Huang",
        "Shiji Song",
        "N Jatinder",
        "Cheng Gupta",
        "Wu"
      ],
      "year": "2014",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "41",
      "title": "Weighted extreme learning machine for imbalance learning",
      "authors": [
        "Weiwei Zong",
        "Guang-Bin Huang",
        "Yiqiang Chen"
      ],
      "year": "2013",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "42",
      "title": "Decay-weighted extreme learning machine for balance and optimization learning",
      "authors": [
        "Qing Shen",
        "Xiaojuan Ban",
        "Ruoyi Liu",
        "Yu Wang"
      ],
      "year": "2017",
      "venue": "Machine Vision and Applications"
    },
    {
      "citation_id": "43",
      "title": "Ensemble based fuzzy weighted extreme learning machine for gene expression classification",
      "authors": [
        "Yang Wang",
        "Anna Wang"
      ],
      "year": "2019",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Discrimination of speech from nonspeech based on multiscale spectro-temporal modulations",
      "authors": [
        "Nima Mesgarani",
        "Malcolm Slaney",
        "Shihab Shamma"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "45",
      "title": "Feature selection based on mutual information: criteria of max-dependency, maxrelevance, and min-redundancy",
      "authors": [
        "Hanchuan Peng",
        "Fuhui Long",
        "Chris Ding"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "46",
      "title": "Correlation-based feature selection of discrete and numeric class machine learning",
      "authors": [
        "M Hall"
      ],
      "year": "2000",
      "venue": "Correlation-based feature selection of discrete and numeric class machine learning"
    },
    {
      "citation_id": "47",
      "title": "Feature selection and kernel learning for local learning-based clustering",
      "authors": [
        "H Zeng",
        "Y Cheung"
      ],
      "year": "2010",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "48",
      "title": "An improved quantum-behaved particle swarm optimization algorithm with weighted mean best position",
      "authors": [
        "M Xi",
        "J Sun",
        "W Xu"
      ],
      "year": "2008",
      "venue": "Applied Mathematics and Computation"
    },
    {
      "citation_id": "49",
      "title": "Extreme learning machine: theory and applications",
      "authors": [
        "Guang-Bin Huang",
        "Qin-Yu Zhu",
        "Chee-Kheong Siew"
      ],
      "year": "2006",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "50",
      "title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
      "authors": [
        "Peter Bartlett"
      ],
      "year": "1998",
      "venue": "IEEE transactions on Information Theory"
    },
    {
      "citation_id": "51",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "52",
      "title": "Machine audition: principles, algorithms and systems",
      "authors": [
        "Sanaul Haq",
        "Philip Jb Jackson"
      ],
      "year": "2011",
      "venue": "Machine audition: principles, algorithms and systems"
    },
    {
      "citation_id": "53",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "54",
      "title": "COVAREP-A collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "55",
      "title": "Perceptual linear predictive (PLP) analysis of speech",
      "authors": [
        "Hynek Hermansky"
      ],
      "year": "1990",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "56",
      "title": "A new perceptually motivated MVDR-based acoustic front-end (PMVDR) for robust automatic speech recognition",
      "authors": [
        "Umit Yapanel",
        "John Hl Hansen"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "57",
      "title": "Feature extraction using 2-D autoregressive models for speaker recognition",
      "authors": [
        "Sriram Ganapathy",
        "Samuel Thomas",
        "Hynek Hermansky"
      ],
      "year": "2012",
      "venue": "Odyssey 2012-The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "58",
      "title": "An auditory-based feature extraction algorithm for robust speaker identification under mismatched conditions",
      "authors": [
        "Qi Li",
        "Yan Huang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "59",
      "title": "Gammatone wavelet cepstral coefficients for robust speech recognition",
      "authors": [
        "Aniruddha Adiga",
        "Mathew Magimai",
        "Chandra Sekhar"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference of IEEE Region"
    },
    {
      "citation_id": "60",
      "title": "Single frequency filtering approach for discriminating speech and nonspeech",
      "authors": [
        "G Aneeja",
        "B Yegnanarayana"
      ],
      "year": "2015",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "61",
      "title": "Compact Convolutional Recurrent Neural Networks via Binarization for Speech Emotion Recognition",
      "authors": [
        "Huan Zhao",
        "Yufeng Xiao",
        "Jing Han",
        "Zixing Zhang"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "3-D convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "63",
      "title": "Speech-based emotion recognition and static speaker representation",
      "authors": [
        "Maxim Sidorov",
        "Wolfgang Minker",
        "Eugene Stanislavovich"
      ],
      "year": "2016",
      "venue": "Journal of the Siberian Federal University. The series \"Mathematics and Physics"
    },
    {
      "citation_id": "64",
      "title": "Speech Emotion Recognition Using Scalogram Based Deep Structure",
      "authors": [
        "K Aghajani",
        "I Esmaili Paeen Afrakoti"
      ],
      "year": "2020",
      "venue": "International Journal of Engineering"
    },
    {
      "citation_id": "65",
      "title": "Automatic speech emotion recognition using auditory models with binary decision tree and svm",
      "authors": [
        "Enes Yüncü",
        "Hüseyin Hacihabiboglu",
        "Cem Bozsahin"
      ],
      "year": "2014",
      "venue": "22nd International Conference on Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "Emotion recognition using prosodic and spectral features of speech and Naïve Bayes Classifier",
      "authors": [
        "Atreyee Khan",
        "Uttam Kumar"
      ],
      "year": "2017",
      "venue": "2017 international conference on wireless communications, signal processing and networking"
    },
    {
      "citation_id": "67",
      "title": "Emotion recognition from audio signals using Support Vector Machine",
      "authors": [
        "M Sinith",
        "E Aswathi",
        "T Deepa",
        "C Shameema",
        "Shiny Rajan"
      ],
      "year": "2015",
      "venue": "IEEE Recent Advances in Intelligent Computational Systems (RAICS)"
    },
    {
      "citation_id": "68",
      "title": "Exploration of phase information for speech emotion classification",
      "authors": [
        "Suman Deb",
        "Samarendra Dandapat"
      ],
      "year": "2017",
      "venue": "2017 Twenty-third National Conference on Communications (NCC)"
    },
    {
      "citation_id": "69",
      "title": "Emotion classification using residual sinusoidal peak amplitude",
      "authors": [
        "Suman Deb",
        "S Dandapat"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Signal Processing and Communications"
    },
    {
      "citation_id": "70",
      "title": "Spectral features based on local Hu moments of Gabor spectrograms for speech emotion recognition",
      "authors": [
        "Huawei Tao",
        "Ruiyu Liang",
        "Cheng Zha",
        "Xinran Zhang",
        "Li Zhao"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "71",
      "title": "Analysis of excitation source features of speech for emotion recognition",
      "authors": [
        "Sudarsana Kadiri",
        "P Reddy",
        "Gangamohan",
        "V Suryakanth",
        "Bayya Gangashetty",
        "Yegnanarayana"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "72",
      "title": "Speech Emotion Recognition based on SVM as Both Feature Selector and Classifier",
      "authors": [
        "Amirreza Shirani",
        "Ahmad Reza",
        "Naghsh Nilchi"
      ],
      "year": "2016",
      "venue": "International Journal of Image, Graphics & Signal Processing"
    },
    {
      "citation_id": "73",
      "title": "Speech emotion recognition based on power normalized cepstral coefficients in noisy conditions",
      "authors": [
        "M Bashirpour",
        "M Geravanchizadeh"
      ],
      "year": "2016",
      "venue": "Iranian Journal of Electrical and Electronic Engineering"
    },
    {
      "citation_id": "74",
      "title": "Feature analysis and evaluation for automatic emotion identification in speech",
      "authors": [
        "Iker Luengo",
        "Eva Navas",
        "Inmaculada Hernáez"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "75",
      "title": "Speech Emotion Recognition from Raw Audio using Deep Learning",
      "authors": [
        "J Rintala"
      ],
      "year": "2020",
      "venue": "Speech Emotion Recognition from Raw Audio using Deep Learning"
    },
    {
      "citation_id": "76",
      "title": "Classification of emotional speech using 3DEC hierarchical classifier",
      "authors": [
        "Ali Hassan",
        "Robert Damper"
      ],
      "year": "2012",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "77",
      "title": "Time-frequency feature and AMS-GMM mask for acoustic emotion classification",
      "authors": [
        "L Zao",
        "D Cavalcante",
        "R Coelho"
      ],
      "year": "2014",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "78",
      "title": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "authors": [
        "Mayank Bhargava",
        "Tim Polzehl"
      ],
      "year": "2012",
      "venue": "Proc. International Conference on Emerging Computation and Information Technologies"
    },
    {
      "citation_id": "79",
      "title": "Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "Abdul Badshah",
        "Nasir Malik",
        "Noor Rahim",
        "Jamil Ullah",
        "Khan Ahmad",
        "Mi Muhammad",
        "Soonil Lee",
        "Sung Kwon",
        "Baik"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "80",
      "title": "Speech emotion recognition using an enhanced kernel isomap for human-robot interaction",
      "authors": [
        "Shiqing Zhang",
        "Xiaoming Zhao",
        "Bicheng Lei"
      ],
      "year": "2013",
      "venue": "International Journal of Advanced Robotic Systems"
    },
    {
      "citation_id": "81",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "Wu",
        "Siqing",
        "H Tiago",
        "Wai-Yip Falk",
        "Chan"
      ],
      "year": "2011",
      "venue": "Speech communication"
    },
    {
      "citation_id": "82",
      "title": "Emotion recognition using semi-supervised feature selection with speaker normalization",
      "authors": [
        "Yaxin Sun",
        "Guihua Wen"
      ],
      "year": "2015",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "83",
      "title": "Weighted spectral features based on local Hu moments for speech emotion recognition",
      "authors": [
        "Yaxin Sun",
        "Guihua Wen",
        "Jiabing Wang"
      ],
      "year": "2015",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "84",
      "title": "Dimensionality reduction for speech emotion features by multiscale kernels",
      "authors": [
        "Xinzhou Xu",
        "Jun Deng",
        "Wenming Zheng",
        "Li Zhao",
        "Björn Schuller"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "85",
      "title": "Feature Selection and Nuisance Attribute Projection for Speech Emotion Recognition",
      "year": "2016",
      "venue": "Technical Report and Lecture Note Series"
    },
    {
      "citation_id": "86",
      "title": "Deep neural networks for acoustic emotion recognition: raising the benchmarks",
      "authors": [
        "André Stuhlsatz",
        "Christine Meyer",
        "Florian Eyben",
        "Thomas Zielke",
        "Günter Meier",
        "Björn Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "87",
      "title": "Random deep belief networks for recognizing emotions from speech signals",
      "authors": [
        "Guihua Wen",
        "Huihui Li",
        "Jubing Huang",
        "Danyang Li",
        "Eryang Xun"
      ],
      "year": "2017",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "88",
      "title": "Biologically inspired speech emotion recognition",
      "authors": [
        "Reza Lotfidereshgi",
        "Philippe Gournay"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "89",
      "title": "Ensemble softmax regression model for speech emotion recognition",
      "authors": [
        "Yaxin Sun",
        "Guihua Wen"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "90",
      "title": "Integrating Recurrence Dynamics for Speech Emotion Recognition",
      "authors": [
        "Efthymios Tzinis",
        "Georgios Paraskevopoulos"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "91",
      "title": "Analysis of Multi-Lingual Emotion Recognition Using Auditory Attention Features",
      "authors": [
        "Ozlem Kalinli"
      ],
      "year": "2016",
      "venue": "Analysis of Multi-Lingual Emotion Recognition Using Auditory Attention Features"
    },
    {
      "citation_id": "92",
      "title": "Adversarial data augmentation network for speech emotion recognition",
      "authors": [
        "Lu Yi",
        "Man-Wai Mak"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "93",
      "title": "Multiscale amplitude feature and significance of enhanced vocal tract information for emotion classification",
      "authors": [
        "Suman Deb",
        "Samarendra Dandapat"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "94",
      "title": "Speech emotion analysis: Exploring the role of context",
      "authors": [
        "Ashish Tawari",
        "Mohan Trivedi"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on multimedia"
    },
    {
      "citation_id": "95",
      "title": "Investigation of the effect of spectrogram images and different texture analysis methods on speech emotion recognition",
      "authors": [
        "Turgut Özseven"
      ],
      "year": "2018",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "96",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "Turgut Özseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "97",
      "title": "Emotion classification using segmentation of vowel-like and non-vowel-like regions",
      "authors": [
        "Suman Deb",
        "Samarendra Dandapat"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "98",
      "title": "Speech Emotion Recognition From 3D Log-Mel Spectrograms With Deep Learning Network",
      "authors": [
        "Meng",
        "Tianhao Hao",
        "Fei Yan",
        "Hongwei Yuan",
        "Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "99",
      "title": "Hierarchical classifier design for speech emotion recognition in the mixed-cultural environment",
      "authors": [
        "P Vasuki",
        "Chandrabose Aravindan"
      ],
      "year": "2020",
      "venue": "Journal of Experimental & Theoretical Artificial Intelligence"
    },
    {
      "citation_id": "100",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "101",
      "title": "Parallelized Convolutional Recurrent Neural Network With Spectral Features for Speech Emotion Recognition",
      "authors": [
        "Pengxu Jiang",
        "Hongliang Fu",
        "Huawei Tao",
        "Peizhi Lei",
        "Li Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "102",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "103",
      "title": "Decision tree SVM model with Fisher feature selection for speech emotion recognition",
      "authors": [
        "Linhui Sun",
        "Sheng Fu",
        "Fu Wang"
      ],
      "year": "2019",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "104",
      "title": "Automatic speech based emotion recognition using paralinguistics features",
      "authors": [
        "Joosep Hook",
        "Fatemeh Noroozi",
        "Onsen Toygar",
        "Gholamreza Anbarjafari"
      ],
      "year": "2019",
      "venue": "Bulletin of the Polish Academy of Sciences. Technical Sciences"
    },
    {
      "citation_id": "105",
      "title": "Speech Emotion Recognition Using Cyclostationary Spectral Analysis",
      "authors": [
        "Amin Jalili",
        "Sadid Sahami",
        "Chong-Yung Chi",
        "Rassoul Amirfattahi"
      ],
      "year": "2018",
      "venue": "2018 IEEE 28th International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "106",
      "title": "Joint Deep Cross-Domain Transfer Learning for Emotion Recognition",
      "authors": [
        "Dung Nguyen",
        "Sridha Sridharan",
        "Thanh Duc",
        "Simon Nguyen",
        "Son Denman",
        "Rui Tran",
        "Clinton Zeng",
        "Fookes"
      ],
      "year": "2020",
      "venue": "Joint Deep Cross-Domain Transfer Learning for Emotion Recognition",
      "arxiv": "arXiv:2003.11136"
    },
    {
      "citation_id": "107",
      "title": "Sparse Autoencoder with Attention Mechanism for Speech Emotion Recognition",
      "authors": [
        "Ting- Sun",
        "An-Yeu Andy Wei",
        "Wu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "108",
      "title": "Deep visual attributes vs. hand-crafted audio features on multidomain speech emotion recognition",
      "authors": [
        "Michalis Papakostas",
        "Evaggelos Spyrou"
      ],
      "year": "2017",
      "venue": "Theodoros Giannakopoulos, Giorgos Siantikos, Dimitrios Sgouropoulos, Phivos Mylonas, and Fillia Makedon"
    },
    {
      "citation_id": "109",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Zhen-Tao Liu",
        "Qiao Xie",
        "Min Wu",
        "Wei-Hua Cao",
        "Ying Mei",
        "Jun-Wei Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "110",
      "title": "Vocal-based emotion recognition using random forests and decision tree",
      "authors": [
        "Fatemeh Noroozi",
        "Tomasz Sapiński",
        "Dorota Kamińska",
        "Gholamreza Anbarjafari"
      ],
      "year": "2017",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "111",
      "title": "Wavelet-based time-frequency representations for automatic recognition of emotions from speech",
      "authors": [
        "Juan Vásquez-Correa",
        "Tomas Camilo",
        "Juan Arias-Vergara",
        "Jesús Rafael Orozco-Arroyave",
        "Francisco Vargas-Bonilla",
        "Elmar Nöth"
      ],
      "year": "2016",
      "venue": "Speech Communication; 12. ITG Symposium. VDE"
    },
    {
      "citation_id": "112",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Julien Epps"
      ],
      "year": "2019",
      "venue": "Direct Modelling of Speech Emotion from Raw Speech",
      "arxiv": "arXiv:1904.03833"
    },
    {
      "citation_id": "113",
      "title": "Multi-Channel Auto-Encoder for Speech Emotion Recognition",
      "authors": [
        "Zefang Zong",
        "Hao Li",
        "Qi Wang"
      ],
      "year": "2018",
      "venue": "Multi-Channel Auto-Encoder for Speech Emotion Recognition",
      "arxiv": "arXiv:1810.10662"
    },
    {
      "citation_id": "114",
      "title": "Representation Learning for Speech Emotion Recognition",
      "authors": [
        "Sayan Ghosh",
        "Eugene Laksana",
        "Louis-Philippe Morency",
        "Stefan Scherer"
      ],
      "year": "2016",
      "venue": "Representation Learning for Speech Emotion Recognition"
    },
    {
      "citation_id": "115",
      "title": "Attention-Based Dense LSTM for Speech Emotion Recognition",
      "authors": [
        "Yue Xie",
        "Ruiyu Liang",
        "Zhenlin Liang",
        "Li Zhao"
      ],
      "year": "2019",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "116",
      "title": "Speaker-Invariant Affective Representation Learning via Adversarial Training",
      "authors": [
        "Haoqi Li",
        "Ming Tu",
        "Jing Huang",
        "Shrikanth Narayanan",
        "Panayiotis Georgiou"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "117",
      "title": "Exploring Spatio-Temporal Representations by Integrating Attention-based Bidirectional-LSTM-RNNs and FCNs for Speech Emotion Recognition",
      "authors": [
        "Ziping Zhao",
        "Yu Zheng",
        "Zixing Zhang",
        "Haishuai Wang",
        "Yiqin Zhao",
        "Chao Li"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech 2018"
    },
    {
      "citation_id": "118",
      "title": "Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function",
      "authors": [
        "Jian Huang",
        "Ya Li",
        "Jianhua Tao",
        "Zhen Lian"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "119",
      "title": "A multi-task learning framework for emotion recognition using 2D continuous space",
      "authors": [
        "Rui Xia",
        "Yang Liu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "120",
      "title": "Revisiting Hidden Markov Models for Speech Emotion Recognition",
      "authors": [
        "Shuiyang Mao",
        "Dehua Tao",
        "Guangyan Zhang",
        "P Ching",
        "Tan Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "121",
      "title": "From simulated speech to natural speech, what are the robust features for emotion recognition?",
      "authors": [
        "Ya Li",
        "Linlin Chao",
        "Yazhu Liu",
        "Wei Bao",
        "Jianhua Tao"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "122",
      "title": "Segment-based speech emotion recognition using recurrent neural networks",
      "authors": [
        "Efthymios Tzinis",
        "Alexandras Potamianos"
      ],
      "year": "2017",
      "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "123",
      "title": "Towards Temporal Modelling of Categorical Speech Emotion Recognition",
      "authors": [
        "Wenjing Han",
        "Huabin Ruan",
        "Xiaomin Chen",
        "Zhixiang Wang",
        "Haifeng Li",
        "Björn Schuller"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "124",
      "title": "CNN+ LSTM Architecture for Speech Emotion Recognition with Data Augmentation",
      "authors": [
        "Caroline Etienne",
        "Guillaume Fidanza",
        "Andrei Petrovskii",
        "Laurence Devillers",
        "Benoit Schmauch"
      ],
      "year": "2018",
      "venue": "Proc. Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "125",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "Haytham Fayek",
        "Margaret Lech",
        "Lawrence Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "126",
      "title": "A Dialogical Emotion Decoder for Speech Motion Recognition in Spoken Dialog",
      "authors": [
        "S Yeh",
        "Y Lin",
        "C Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "127",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Li-Rong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "128",
      "title": "Adaptive Domain-Aware Representation Learning for Speech Emotion Recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "D Huang"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "129",
      "title": "Confidence measure improvement using useful predictor features and support vector machines",
      "authors": [
        "Shekofteh Yasser",
        "Kabudian Jahanshah",
        "Goodarzi Mohammad Mohsen",
        "Sarraf Iman"
      ],
      "year": "2012",
      "venue": "20th Iranian Conference on Electrical Engineering",
      "doi": "10.1109/IranianCEE.2012.6292531"
    },
    {
      "citation_id": "130",
      "title": "Sarraf Rezaei Iman. Feature bandwidth extension for Persian conversational telephone speech recognition",
      "authors": [
        "Goodarzi Mohammad Mohsen",
        "Almasganj Farshad",
        "Kabudian Jahanshah",
        "Shekofteh Yasser"
      ],
      "year": "2012",
      "venue": "20th Iranian Conference on Electrical Engineering",
      "doi": "10.1109/IranianCEE.2012.6292541"
    },
    {
      "citation_id": "131",
      "title": "MiGSA: a new simulated annealing algorithm with mixture distribution as generating function",
      "authors": [
        "Seyed Mirhosseini",
        "Hasan Hanif",
        "Jahanshah Yarmohamadi",
        "Kabudian"
      ],
      "year": "2014",
      "venue": "th International conference on computer and knowledge engineering (ICCKE)",
      "doi": "10.1109/ICCKE.2014.6993413"
    },
    {
      "citation_id": "132",
      "title": "Time-inhomogeneous hidden Bernoulli model: An alternative to hidden Markov model for automatic speech recognition",
      "authors": [
        "Jahanshah Kabudian",
        "M Mehdi",
        "S Homayounpour",
        "Ahadi"
      ],
      "year": "2008",
      "venue": "IEEE international conference on acoustics, speech and signal processing",
      "doi": "10.1109/ICASSP.2008.4518556"
    }
  ]
}