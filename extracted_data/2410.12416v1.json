{
  "paper_id": "2410.12416v1",
  "title": "Enhancing Speech Emotion Recognition Through Segmental Average Pooling Of Self-Supervised Learning Features",
  "published": "2024-10-16T10:00:57Z",
  "authors": [
    "Jonghwan Hyeon",
    "Yung-Hwan Oh",
    "Ho-Jin Choi"
  ],
  "keywords": [
    "speech emotion recognition",
    "human-computer interaction",
    "self-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) analyzes human emotions expressed through speech. Self-supervised learning (SSL) offers a promising approach to SER by learning meaningful representations from a large amount of unlabeled audio data. However, existing SSL-based methods rely on Global Average Pooling (GAP) to represent audio signals, treating speech and nonspeech segments equally. This can lead to dilution of informative speech features by irrelevant non-speech information. To address this, the paper proposes Segmental Average Pooling (SAP), which selectively focuses on informative speech segments while ignoring non-speech segments. By applying both GAP and SAP to SSL features, our approach utilizes overall speech signal information from GAP and specific information from SAP, leading to improved SER performance. Experiments show state-of-the-art results on the IEMOCAP for English and superior performance on KEMDy19 for Korean datasets in both unweighted and weighted accuracies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is an active area of research in the field of speech processing, aiming to automatically recognize the emotional state of a speaker from their speech signal. SER has gained significant attention due to its potential applications in various domains such as human-computer interaction, virtual assistants, and affective computing where understanding the emotional context can greatly enhance the interaction between humans and machines. However, accurately recognizing emotions from speech signals remains a challenging task due to the complex nature of human emotions and the variability of speech signals across different speakers and contexts.\n\nOne of the key challenges in SER is to extract and utilize meaningful and effective features from speech signals for accurate emotion recognition. Traditionally, SER systems rely on handcrafted features, such as Mel-frequency cepstral coefficients (MFCCs), spectral features, and prosody features, which are designed to capture specific aspects of speech signals. However, these features are limited in their ability to capture the complex and dynamic nature of emotions conveyed through speech because they do not capture the higher-level abstractions that are essential for emotion recognition.\n\nRecently, self-supervised learning (SSL) has gained significant success in the natural language processing field, where models are trained on large amounts of unlabeled text data and learn to capture complex contextual relationships between words and phrases. Inspired by this success, researchers have explored the use of SSL models to extract more abstract and informative features from speech signals. These models are trained on large amounts of unlabeled speech data and can learn to capture a wide range of speech characteristics, including phonetic, syntactic, and semantic information potentially capturing more comprehensive and contextualized information.\n\nMeanwhile, speech signals inherently vary in length, resulting in features extracted from SSL models also having variable lengths. To leverage these variable-length SSL features in machine learning models, which typically require fixed-length representations for input, it is essential to transform them into a fixed-length format. The traditional approach for this transformation is to apply Global Average Pooling (GAP) on SSL features across the temporal dimension. However, speech signals primarily consist of two types of segments: speech segments, which convey meaning through words and phrases, and non-speech segments, which consist of silence and background noise. Since GAP treats all segments equally, whether they are speech or non-speech, it can lead to the dilution of informative features extracted from speech segments by irrelevant information contained within non-speech segments. Consequently, this can negatively impact the performance of SER models that use SSL features.\n\nTo solve this problem, we propose Segmental Average Pooling (SAP), which focuses only on speech segments of speech signals, while ignoring non-speech segments. By applying both GAP and SAP on SSL features, our proposed model can utilize overall information of the speech signal from the GAP representation and specific information of the speech signal from the SAP representation.\n\nWe evaluate our proposed approach on two datasets, IEMO-CAP  [1]  for English and KEMDy19  [2, 3]  for Korean, using both unweighted and weighted accuracy. We perform the leaveone-speaker-out cross-validation to measure performance independently of speaker characteristics. Our proposed approach, which combines GAP and SAP, achieves better performance on both datasets compared to relying solely on GAP. Furthermore, we achieve state-of-the-art performance on both datasets, demonstrating the effectiveness of our proposed approach.\n\nOur main contributions are as follows:  (1)  We propose a novel pooling method, SAP, which focuses only on speech segments and ignores non-speech segments to prevent the dilution of informative features.  (2)  We demonstrate that combining GAP and SAP improves the performance of SER models that use SSL features.  (3)  We achieve state-of-the-art performance on the IEMOCAP for English and superior performance on the KEMDy19 for Korean using our proposed approach. arXiv:2410.12416v1 [cs.SD] 16 Oct 2024 2. Related Works",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Speech emotion recognition (SER) is an active area of research that aims to detect the emotional state of a speaker based on characteristics of their speech signal. Over the years, various machine learning techniques have been employed on different types of acoustic features extracted from the speech. Early SER systems utilized Gaussian Mixture Models (GMMs) trained on low-level descriptors such as pitch, energy, and Mel-Frequency Cepstral Coefficients (MFCCs)  [4, 5, 6] .\n\nWith the advent of deep learning, neural network architectures such as convolutional neural networks (CNNs)  [7]  and long short-term memory (LSTM) networks  [8]  have achieved state-of-the-art performance by learning discriminative feature representations directly from the raw audio. Some studies have also explored using auxiliary modalities like text transcripts  [9]  or visual facial expressions  [10, 11]  to complement the acoustic speech data. Recently, advances in self-supervised learning  [12, 13]  and transformer architectures  [14]  have further enhanced SER performance. However, challenges still persist in real-world deployment scenarios with noisy inputs and underrepresented emotion classes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Supervised Learning",
      "text": "Self-supervised learning (SSL) has emerged as a powerful technique that leverages large amounts of unlabeled data to train models through carefully designed self-supervision tasks. This pre-training process enables models to learn the intrinsic characteristics and patterns present in the data, acquiring rich, contextualized representations. These learned representations can then be effectively fine-tuned for various downstream tasks using a relatively small amount of labeled data and a limited number of training epochs, achieving competitive or even state-ofthe-art performance.\n\nIn recent years, various SSL models have been introduced in the speech processing field, such as Wav2Vec 2.0  [15] , Hu-BERT  [16] , and WavLM  [17] . These models have demonstrated significant improvements in performance compared to previous approaches across a wide range of downstream speech tasks, including automatic speech recognition, keyword spotting, speaker identification, as shown in  [18] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Approach",
      "text": "In this paper, we propose a novel approach to enhance speech emotion recognition (SER) by applying both Global Average Pooling (GAP) and Segmental Average Pooling (SAP) on selfsupervised learning (SSL) features. An overall architecture of our proposed approach is illustrated in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Learning Features",
      "text": "SSL models, which are pre-trained on large-scale audio data, allow us to obtain contextualized speech features directly from a raw speech signal of a given utterance. Let X be a raw speech signal of an utterance u. To feed X into SSL models, X is first divided into a sequence of frames X = (x1, x2, . . . , xT ), where xi ∈ R w represents the i-th frame of the utterance u, and T is the number of frames determined by the length of the raw speech signal, the window size w and the stride s. Given a pre-trained SSL model fSSL(•),",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Global Average Pooling",
      "text": "Since the primary objective of SER is to recognize the emotion conveyed by the entire utterance u, rather than the emotion at the individual frame level xi, it becomes crucial to aggregate these frame-level speech features obtained by fSSL(•) into a single utterance-level speech feature. A traditional approach to achieve this aggregation is to apply Global Average Pooling (GAP) across the temporal dimension of these frame-level speech features as follows:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ci",
      "text": "(2)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Segmental Average Pooling",
      "text": "Speech signals primarily consist of two types of segments: speech segments, which convey meaning through words and phrases, and non-speech segments, which consist of silence and background noise. However, in equation 2, GAP treats all frames equally, regardless of whether they are speech segments or non-speech segments. This can lead to the dilution of informative features extracted from speech segments by irrelevant information contained within non-speech segments. Consequently, this may negatively impact the performance of SER models that utilize SSL features.\n\nTo address this issue, we propose Segmental Average Pooling (SAP), which focuses only on speech segments of speech signals, while ignoring non-speech segments. This selective approach ensures that only informative features extracted from speech segments contribute to the final utterance-level feature. To define SAP(•), it is necessary to determine whether a given frame contains speech. For this purpose, we utilize the voice activity detection (VAD) algorithm. 1\n\nUsing VAD(•), we collect frame-level SSL features only from speech segments. Then, we apply multi-head selfattention (MHSA) to these features to capture additional relationships among speech segments as described below:\n\nAs a result, we define SAP(•) as follows:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Hi",
      "text": "(5)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Combining Gap And Sap",
      "text": "Our proposed approach leverages the complementary strengths of both GAP and SAP representations. GAP(•) captures the overall, global information of the speech signal, providing a broad context that includes the average characteristics of the entire signal. Conversely, SAP(•) focuses on specific, salient features of the speech signal, which are crucial for accurately distinguishing between nuanced phonetic elements or speech characteristics. Therefore, we define the final speech representation, SR(•), as the concatenation of GAP(•) and SAP(•):",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "Human emotions are complex, and available SER datasets are often limited in size. This necessitates a strategy that maximizes the information extracted from each data sample. To address this challenge, we adopt a multi-task learning (MTL) approach, which aims to concurrently predict both continuous and discrete emotions. The total loss L is defined as:\n\nwhere Ldiscrete is the weighted cross-entropy loss 2 for predicting discrete emotions, and Lvalence and Larousal are the mean absolute error losses for predicting continuous valence and arousal emotions, respectively. The coefficients α, β, and γ balance the contribution of each loss component to the total loss. 1 Our proposed approach employs the voice activity detection algorithm developed by Google for the WebRTC project. 2 Class weights are calculated using the label distribution in the training dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We conduct our experiments on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [1]  dataset for English and the Korean Emotion Multimodal Database in 2019 (KEMDy19)  [2, 3]  for Korean.\n\nThe IEMOCAP dataset consists of five sessions in total, where each session features one male and one female speaker engaged in a conversation. Similar to previous studies, the utterances labeled as \"excited\" are merged into \"happy\", and only four emotion classes {angry, happy, neutral, and sad} are considered. As a result, the number of utterances representing angry, happy, neutral, and sad are 1103, 1636, 1708, and 1084, respectively. To compare our performance with existing studies under the same conditions, we employ the leave-one-speakerout 10-fold cross-validation approach, where 8, 1, 1 folds are used as training, validation, and test sets, respectively.\n\nSimilarly, the KEMDy19 dataset includes twenty sessions, with each session featuring one male and one female speaker engaged in a conversation. We also consider only four emotion classes {angry, happy, neutral, sad}. As a result, the number of utterances representing angry, happy, neutral, and sad are 1530, 1313, 4328, and 773, respectively. To evaluate the performance independently of speaker characteristics, we perform the leaveone-speaker-out 40-fold cross-validation, where 38, 1, 1 folds are used as training, validation and test sets, respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Evaluation metrics: Following previous studies  [19, 20, 21, 22] , we use unweighted accuracy (UA) and weighted accuracy (WA) as our evaluation criteria. Self-supervised learning model: We use WavLM Large  [17]  as our self-supervised learning (SSL) model fSSL which has achieved competitive performance in the SER task on the SU-PERB benchmark  [18] . According to WavLM Large, it uses the window size w of 25ms and the stride s of 20ms. Multi-task learning: We use α, β and γ as 0.5, 0.25 and 0.25, respectively. Projection dimension: The final speech representation SR(•) is projected into 32 dimensions before feeding it to the classifier for discrete emotion and the regressors for continuous emotions. Implementation details: Our code is implemented using Py-Torch  [23]  and HuggingFace Transformers  [14] . Due to limited memory capacity, utterances exceeding 19 seconds in the IEMOCAP dataset and 16 seconds in the KEMDy19 dataset are truncated. We employ an epoch of 30, a batch size of 64, a learning rate of 3e-5, a warm-up ratio of 0.1, and the cosine learning rate scheduler. Additionally, we utilize early stopping with a patience of 5, monitoring the total loss L on a validation set. Our model has approximately 316M trainable parameters. We conduct our experiments using NVIDIA A100 40GB and the estimated training time is about 8 hours in the IEMOCAP dataset and about 40 hours in the KEMDy19 dataset for each experiment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iemocap",
      "text": "Table  1  presents a comparison of the performance of three different methods on the IEMOCAP dataset, employing a leaveone-speaker-out 10-fold cross-validation setting. Compared to GAP(•) which is a traditional approach for aggregating SSL features, our proposed method, SR(•), which combines GAP(•) and SAP(•), demonstrates superior performance on both UA DRN-MHSA  [24]  67.40 audio-BRE  [19]  65.20 64.60 Audio-CNN-xvector  [20]  68.40 66.60 HNSD  [21]  72.50 70.50 MHSA-FACA  [22]  72.83 72.01 SCL-kNN  [25]  75.14 74.13\n\nPropposed 75.57 74.77\n\nand WA. However, we observe that SAP(•) alone does not show an improvement in performance. This indicates that the overall information of the speech signal remains important for accurately recognizing emotions conveyed through speech signals.\n\nTable  2  presents a comparison of our proposed method with recent state-of-the-art (SOTA) approaches. For a fair comparison, we only consider previous works performing a leave-onespeaker-out 10-fold cross-validation. The results show that our proposed approach, which combines both GAP(•) and SAP(•), achieves a relative improvement of 0.43% and 0.64% on UA and WA, respectively, compared to other SOTA approaches, demonstrating the effectiveness of our method.   3  presents a comparison of the performance of three different methods on the KEMDy19 dataset, employing a leaveone-speaker-out 40-fold cross-validation setting. Similar to the findings with the IEMOCAP dataset, we observe that our proposed approach, SR(•), shows superior performance in both UA and WA compared to GAP(•). Since this paper is the first to conduct a leave-one-speaker-out 40-fold cross-validation to evaluate performance independently of speaker characteristics, we are unable to compare our results directly with previous works fairly. However, the performance improvements from the proposed approach, SR(•), compared to the traditional approach, GAP(•), demonstrate the effectiveness of our proposed method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Kemdy19",
      "text": "Figure  2  shows the confusion matrix generated from our proposed method. According to this matrix, our proposed approach achieves highest accuracy in the angry class on the IEMOCAP dataset and in the neutral class on the KEMDy19  dataset. In contrast, our approach exhibits the lowest accuracy in the happy class on both datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel Segmental Average Pooling (SAP) method designed to enhance speech emotion recognition by effectively utilizing self-supervised learning (SSL) speech features. SAP selectively focuses on informative speech segments while ignoring non-speech segments like silence and background noise. By combining SAP with Global Average Pooling (GAP), our approach leverages both overall information from the entire speech signal through the GAP representation and specific information from speech segments through the SAP representation. Our experimental results on two datasets, the IEMOCAP for English and the KEMDy19 for Korean, demonstrate that our proposed approach achieves superior performance compared to relying solely on GAP. Notably, our proposed method achieves state-of-the-art performance on the IEMOCAP dataset and highly competitive results on the KEMDy19 dataset, highlighting its effectiveness in capturing the complex and dynamic nature of emotions conveyed through speech.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 3.1. Self-supervised learning features",
      "page": 2
    },
    {
      "caption": "Figure 1: An overall architecture of our proposed approach",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the confusion matrix generated from our",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion matrix on IEMOCAP and KEMDy19",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "jonghwanhyeon@kaist.ac.kr,"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "Abstract"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "Speech Emotion Recognition (SER) analyzes human emotions"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "expressed through speech.\nSelf-supervised learning (SSL) of-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "fers a promising approach to SER by learning meaningful rep-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "resentations from a large amount of unlabeled audio data. How-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "ever, existing SSL-based methods rely on Global Average Pool-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "ing (GAP) to represent audio signals,\ntreating speech and non-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "speech segments equally. This can lead to dilution of informa-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "tive speech features by irrelevant non-speech information. To"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "address\nthis,\nthe paper proposes Segmental Average Pooling"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "(SAP), which selectively focuses on informative speech seg-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "ments while ignoring non-speech segments. By applying both"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "GAP and SAP to SSL features, our approach utilizes overall"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "speech signal\ninformation from GAP and specific information"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "from SAP, leading to improved SER performance. Experiments"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "show state-of-the-art results on the IEMOCAP for English and"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "superior performance on KEMDy19 for Korean datasets in both"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "unweighted and weighted accuracies."
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "Index Terms:\nspeech emotion recognition, human-computer"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "interaction, self-supervised learning"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "1.\nIntroduction"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "Speech emotion recognition (SER) is an active area of research"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "in the field of speech processing, aiming to automatically rec-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "ognize the emotional state of a speaker from their speech signal."
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "SER has gained significant attention due to its potential applica-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "tions in various domains such as human-computer interaction,"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "virtual assistants, and affective computing where understanding"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "the emotional context can greatly enhance the interaction be-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": ""
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "tween humans and machines. However, accurately recognizing"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "emotions from speech signals remains a challenging task due"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "to the complex nature of human emotions and the variability of"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "speech signals across different speakers and contexts."
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "One of the key challenges in SER is to extract and utilize"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "meaningful and effective features from speech signals for ac-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "curate emotion recognition.\nTraditionally, SER systems\nrely"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "on handcrafted features, such as Mel-frequency cepstral coeffi-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "cients (MFCCs), spectral features, and prosody features, which"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "are designed to capture specific aspects of speech signals. How-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "ever,\nthese features are limited in their ability to capture the"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "complex and dynamic nature of emotions conveyed through"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "speech because they do not capture the higher-level abstractions"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "that are essential for emotion recognition."
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "Recently, self-supervised learning (SSL) has gained signif-"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "icant\nsuccess\nin the natural\nlanguage processing field, where"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "models\nare\ntrained on large\namounts of unlabeled text data"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "and learn to capture complex contextual relationships between"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "words and phrases.\nInspired by this success,\nresearchers have"
        },
        {
          "1School of Computing, KAIST, Daejeon, South Korea": "explored the use of SSL models to extract more abstract and"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "2.1.\nSpeech emotion recognition"
        },
        {
          "2. Related Works": "Speech emotion recognition (SER) is an active area of research"
        },
        {
          "2. Related Works": "that aims to detect\nthe emotional state of a speaker based on"
        },
        {
          "2. Related Works": "characteristics of\ntheir speech signal. Over\nthe years, various"
        },
        {
          "2. Related Works": "machine learning techniques have been employed on different"
        },
        {
          "2. Related Works": "types of acoustic features extracted from the speech. Early SER"
        },
        {
          "2. Related Works": "systems utilized Gaussian Mixture Models (GMMs) trained on"
        },
        {
          "2. Related Works": "low-level descriptors such as pitch, energy, and Mel-Frequency"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "Cepstral Coefficients (MFCCs) [4, 5, 6]."
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "With the advent of deep learning, neural network architec-"
        },
        {
          "2. Related Works": "tures\nsuch as convolutional neural networks\n(CNNs)\n[7] and"
        },
        {
          "2. Related Works": "long short-term memory (LSTM) networks [8] have achieved"
        },
        {
          "2. Related Works": "state-of-the-art performance by learning discriminative feature"
        },
        {
          "2. Related Works": "representations directly from the raw audio. Some studies have"
        },
        {
          "2. Related Works": "also explored using auxiliary modalities like text transcripts [9]"
        },
        {
          "2. Related Works": "or visual facial expressions [10, 11] to complement\nthe acous-"
        },
        {
          "2. Related Works": "tic speech data.\nRecently, advances\nin self-supervised learn-"
        },
        {
          "2. Related Works": "ing [12, 13] and transformer architectures [14] have further en-"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "hanced SER performance. However, challenges still persist\nin"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "real-world deployment scenarios with noisy inputs and under-"
        },
        {
          "2. Related Works": "represented emotion classes."
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "2.2.\nSelf-supervised learning"
        },
        {
          "2. Related Works": "Self-supervised learning (SSL) has emerged as a powerful tech-"
        },
        {
          "2. Related Works": "nique that\nleverages large amounts of unlabeled data to train"
        },
        {
          "2. Related Works": "models through carefully designed self-supervision tasks. This"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "pre-training process enables models to learn the intrinsic char-"
        },
        {
          "2. Related Works": "acteristics and patterns present\nin the data, acquiring rich, con-"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "textualized representations. These learned representations can"
        },
        {
          "2. Related Works": "then be effectively fine-tuned for various downstream tasks us-"
        },
        {
          "2. Related Works": "ing a relatively small amount of labeled data and a limited num-"
        },
        {
          "2. Related Works": "ber of training epochs, achieving competitive or even state-of-"
        },
        {
          "2. Related Works": "the-art performance."
        },
        {
          "2. Related Works": "In recent years, various SSL models have been introduced"
        },
        {
          "2. Related Works": "in the speech processing field, such as Wav2Vec 2.0 [15], Hu-"
        },
        {
          "2. Related Works": "BERT [16],\nand WavLM [17].\nThese models have demon-"
        },
        {
          "2. Related Works": "strated significant\nimprovements in performance compared to"
        },
        {
          "2. Related Works": "previous approaches across a wide range of downstream speech"
        },
        {
          "2. Related Works": "tasks,\nincluding automatic speech recognition, keyword spot-"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "ting, speaker identification, as shown in [18]."
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "3. Proposed Approach"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "In this paper, we propose a novel approach to enhance speech"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "emotion recognition (SER) by applying both Global Average"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "Pooling (GAP) and Segmental Average Pooling (SAP) on self-"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "supervised learning (SSL) features. An overall architecture of"
        },
        {
          "2. Related Works": "our proposed approach is illustrated in Figure 1."
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "3.1.\nSelf-supervised learning features"
        },
        {
          "2. Related Works": "SSL models, which are pre-trained on large-scale audio data,"
        },
        {
          "2. Related Works": ""
        },
        {
          "2. Related Works": "allow us to obtain contextualized speech features directly from"
        },
        {
          "2. Related Works": "a raw speech signal of a given utterance. Let X be a raw speech"
        },
        {
          "2. Related Works": "signal of an utterance u.\nTo feed X into SSL models, X is"
        },
        {
          "2. Related Works": "first divided into a sequence of frames X = (x1, x2, . . . , xT ),"
        },
        {
          "2. Related Works": "the utterance u,\nwhere xi ∈ Rw represents the i-th frame of"
        },
        {
          "2. Related Works": "and T is the number of frames determined by the length of the"
        },
        {
          "2. Related Works": "raw speech signal, the window size w and the stride s. Given a"
        },
        {
          "2. Related Works": "pre-trained SSL model fSSL(·),"
        },
        {
          "2. Related Works": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "models that utilize SSL features.": "To address this issue, we propose Segmental Average Pool-",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "We\nconduct\nour\nexperiments\non\nthe\nInteractive Emotional"
        },
        {
          "models that utilize SSL features.": "ing (SAP), which focuses only on speech segments of speech",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "Dyadic Motion Capture\n(IEMOCAP)\n[1]\ndataset\nfor\nEn-"
        },
        {
          "models that utilize SSL features.": "signals, while ignoring non-speech segments.\nThis\nselective",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "glish and the Korean Emotion Multimodal Database in 2019"
        },
        {
          "models that utilize SSL features.": "approach ensures that only informative features extracted from",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "(KEMDy19) [2, 3] for Korean."
        },
        {
          "models that utilize SSL features.": "speech segments contribute to the final utterance-level feature.",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "The IEMOCAP dataset consists of five sessions\nin total,"
        },
        {
          "models that utilize SSL features.": "To define SAP(·),\nit\nis necessary to determine whether a given",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "where each session features one male and one female speaker"
        },
        {
          "models that utilize SSL features.": "frame contains speech.\nFor\nthis purpose, we utilize the voice",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "engaged in a conversation. Similar to previous studies,\nthe ut-"
        },
        {
          "models that utilize SSL features.": "activity detection (VAD) algorithm.1",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "terances labeled as ”excited” are merged into ”happy”, and only"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "four emotion classes {angry, happy, neutral, and sad} are con-"
        },
        {
          "models that utilize SSL features.": "(cid:40)",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "1\nif x contains speech",
          "4. Experiments": "sidered. As a result,\nthe number of utterances representing an-"
        },
        {
          "models that utilize SSL features.": "VAD(x) =\n(3)",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "gry, happy, neutral, and sad are 1103, 1636, 1708, and 1084,"
        },
        {
          "models that utilize SSL features.": "0\notherwise",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "respectively. To compare our performance with existing studies"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "under\nthe same conditions, we employ the leave-one-speaker-"
        },
        {
          "models that utilize SSL features.": "Using VAD(·), we collect\nframe-level SSL features only",
          "4. Experiments": "out 10-fold cross-validation approach, where 8, 1, 1 folds are"
        },
        {
          "models that utilize SSL features.": "from speech\nsegments.\nThen, we\napply multi-head\nself-",
          "4. Experiments": "used as training, validation, and test sets, respectively."
        },
        {
          "models that utilize SSL features.": "attention (MHSA)\nto these features to capture additional\nrela-",
          "4. Experiments": "Similarly,\nthe KEMDy19 dataset\nincludes twenty sessions,"
        },
        {
          "models that utilize SSL features.": "tionships among speech segments as described below:",
          "4. Experiments": "with each session featuring one male and one female speaker"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "engaged in a conversation. We also consider only four emotion"
        },
        {
          "models that utilize SSL features.": "g(X) = [ci|ci ∈ fSSL(X), VAD(xi) = 1]",
          "4. Experiments": "classes {angry, happy, neutral, sad}. As a result, the number of"
        },
        {
          "models that utilize SSL features.": "(4)",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "utterances representing angry, happy, neutral, and sad are 1530,"
        },
        {
          "models that utilize SSL features.": "h(X) = MHSA(g(X))",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "1313, 4328, and 773, respectively. To evaluate the performance"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "independently of speaker characteristics, we perform the leave-"
        },
        {
          "models that utilize SSL features.": "As a result, we define SAP(·) as follows:",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "one-speaker-out 40-fold cross-validation, where 38, 1, 1 folds"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "are used as training, validation and test sets, respectively."
        },
        {
          "models that utilize SSL features.": "1\n(cid:88)",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "SAP(X) =\n(5)\nhi",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "|h(X)|",
          "4. Experiments": "4.1. Experimental setup"
        },
        {
          "models that utilize SSL features.": "hi∈h(X)",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "Evaluation metrics:\nFollowing previous studies [19, 20, 21,"
        },
        {
          "models that utilize SSL features.": "3.4. Combining GAP and SAP",
          "4. Experiments": "22], we use unweighted accuracy (UA) and weighted accuracy"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "(WA) as our evaluation criteria."
        },
        {
          "models that utilize SSL features.": "Our proposed approach leverages the complementary strengths",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "Self-supervised learning model: We use WavLM Large [17]"
        },
        {
          "models that utilize SSL features.": "of both GAP and SAP representations.\nGAP(·) captures\nthe",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "as our\nself-supervised learning (SSL) model fSSL which has"
        },
        {
          "models that utilize SSL features.": "overall, global\ninformation of\nthe speech signal, providing a",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "achieved competitive performance in the SER task on the SU-"
        },
        {
          "models that utilize SSL features.": "broad context\nthat\nincludes\nthe average characteristics of\nthe",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "PERB benchmark [18]. According to WavLM Large, it uses the"
        },
        {
          "models that utilize SSL features.": "entire signal. Conversely, SAP(·) focuses on specific, salient",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "window size w of 25ms and the stride s of 20ms."
        },
        {
          "models that utilize SSL features.": "features of\nthe speech signal, which are crucial\nfor accurately",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "Multi-task learning: We use α, β and γ as 0.5, 0.25 and 0.25,"
        },
        {
          "models that utilize SSL features.": "distinguishing between nuanced phonetic elements or\nspeech",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "respectively."
        },
        {
          "models that utilize SSL features.": "characteristics. Therefore, we define the final speech represen-",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "Projection dimension: The final speech representation SR(·) is"
        },
        {
          "models that utilize SSL features.": "tation, SR(·), as the concatenation of GAP(·) and SAP(·):",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "projected into 32 dimensions before feeding it\nto the classifier"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "for discrete emotion and the regressors for continuous emotions."
        },
        {
          "models that utilize SSL features.": "SR(X) = Concat(GAP(X), SAP(X))\n(6)",
          "4. Experiments": "Implementation details: Our code is implemented using Py-"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "Torch [23] and HuggingFace Transformers [14]. Due to lim-"
        },
        {
          "models that utilize SSL features.": "3.5. Multi-task learning",
          "4. Experiments": "ited memory capacity, utterances exceeding 19 seconds in the"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "IEMOCAP dataset and 16 seconds\nin the KEMDy19 dataset"
        },
        {
          "models that utilize SSL features.": "Human emotions are complex, and available SER datasets are",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "are truncated. We employ an epoch of 30, a batch size of 64,"
        },
        {
          "models that utilize SSL features.": "often limited in size. This necessitates a strategy that maximizes",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "a learning rate of 3e-5, a warm-up ratio of 0.1, and the cosine"
        },
        {
          "models that utilize SSL features.": "the information extracted from each data sample.\nTo address",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "learning rate scheduler. Additionally, we utilize early stopping"
        },
        {
          "models that utilize SSL features.": "this challenge, we adopt a multi-task learning (MTL) approach,",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "with a patience of 5, monitoring the total loss L on a validation"
        },
        {
          "models that utilize SSL features.": "which aims to concurrently predict both continuous and discrete",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "set. Our model has approximately 316M trainable parameters."
        },
        {
          "models that utilize SSL features.": "emotions. The total loss L is defined as:",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "We conduct our experiments using NVIDIA A100 40GB and"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "the estimated training time is about 8 hours in the IEMOCAP"
        },
        {
          "models that utilize SSL features.": "(7)\nL = αLdiscrete + βLvalence + γLarousal",
          "4. Experiments": "dataset and about 40 hours in the KEMDy19 dataset\nfor each"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "experiment."
        },
        {
          "models that utilize SSL features.": "where Ldiscrete is the weighted cross-entropy loss 2 for predicting",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "discrete emotions, and Lvalence and Larousal are the mean absolute",
          "4. Experiments": "4.2. Results"
        },
        {
          "models that utilize SSL features.": "error losses for predicting continuous valence and arousal emo-",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "4.2.1.\nIEMOCAP"
        },
        {
          "models that utilize SSL features.": "tions,\nrespectively.\nThe coefficients α, β, and γ balance the",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "contribution of each loss component to the total loss.",
          "4. Experiments": "Table 1 presents a comparison of the performance of three dif-"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "ferent methods on the IEMOCAP dataset, employing a leave-"
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "one-speaker-out 10-fold cross-validation setting. Compared to"
        },
        {
          "models that utilize SSL features.": "1Our proposed approach employs the voice activity detection algo-",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "",
          "4. Experiments": "GAP(·) which is a traditional approach for aggregating SSL fea-"
        },
        {
          "models that utilize SSL features.": "rithm developed by Google for the WebRTC project.",
          "4. Experiments": ""
        },
        {
          "models that utilize SSL features.": "2Class weights are calculated using the label distribution in the train-",
          "4. Experiments": "tures, our proposed method, SR(·), which combines GAP(·)"
        },
        {
          "models that utilize SSL features.": "ing dataset.",
          "4. Experiments": "and SAP(·), demonstrates\nsuperior performance on both UA"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "and WA. However, we observe that SAP(·) alone does not show",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "information of",
          "75.57\n74.77": "remains important"
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "and WA,",
          "75.57\n74.77": "compared to other SOTA approaches,"
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "4.2.2. KEMDy19",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        },
        {
          "Propposed": "",
          "75.57\n74.77": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sad": "",
          "2.40": "Angry",
          "4.70": "Happy",
          "15.96": "Neutral",
          "76.94": "Sad",
          "8.93": "Angry",
          "3.88": "Happy",
          "19.02": "Neutral",
          "68.18": "Sad"
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "Predicted",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "Predicted",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "(a) IEMOCAP",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "(b) KEMDy19",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "Figure 2: Confusion matrix on IEMOCAP and KEMDy19",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "In contrast, our approach exhibits the lowest accuracy",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "in the happy class on both datasets.",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "5. Conclusion",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "In this paper, we propose a novel Segmental Average Pooling",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "(SAP) method designed to enhance speech emotion recognition",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "by effectively utilizing self-supervised learning (SSL) speech",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "SAP selectively focuses on informative speech seg-",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "ments while",
          "4.70": "",
          "15.96": "ignoring non-speech segments",
          "76.94": "",
          "8.93": "",
          "3.88": "like",
          "19.02": "silence",
          "68.18": "and"
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "background noise.",
          "4.70": "",
          "15.96": "",
          "76.94": "By combining SAP with Global Average",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "Pooling (GAP), our approach leverages both overall",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": "informa-"
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "tion from the entire speech signal",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "through the GAP represen-",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "tation and specific information from speech segments through",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "the SAP representation.",
          "4.70": "",
          "15.96": "",
          "76.94": "Our",
          "8.93": "experimental",
          "3.88": "",
          "19.02": "results",
          "68.18": "on\ntwo"
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "datasets, the IEMOCAP for English and the KEMDy19 for Ko-",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "rean, demonstrate that our proposed approach achieves supe-",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "rior performance compared to relying solely on GAP. Notably,",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "our proposed method achieves state-of-the-art performance on",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "the IEMOCAP dataset and highly competitive results on the",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "KEMDy19 dataset, highlighting its effectiveness in capturing",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "the complex and dynamic nature of emotions conveyed through",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        },
        {
          "Sad": "",
          "2.40": "",
          "4.70": "",
          "15.96": "",
          "76.94": "",
          "8.93": "",
          "3.88": "",
          "19.02": "",
          "68.18": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Performance on KEMDy19": "UA (%)\nWA (%)",
          "speech.": "6. References"
        },
        {
          "Table 3: Performance on KEMDy19": "Method",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "Mean\n95% CI\nMean\n95% CI",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "[1] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "Table 3: Performance on KEMDy19": "GAP(·)\n64.34\n(62.62, 66.05)\n66.62\n(64.95, 68.28)",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "Table 3: Performance on KEMDy19": "SAP(·)\n65.72\n(64.32, 67.12)\n67.75\n(66.59, 68.92)",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "resources and evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "Table 3: Performance on KEMDy19": "SR(·)\n66.26\n(64.59, 67.93)\n68.27\n(66.82, 69.72)",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "[2] K. J. Noh, C. Y. Jeong, J. Lim, S. Chung, G. Kim, J. M. Lim, and"
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "H. Jeong, “Multi-path and group-loss-based network for speech"
        },
        {
          "Table 3: Performance on KEMDy19": "Table 3 presents a comparison of the performance of three",
          "speech.": "emotion recognition in multi-domain datasets,” Sensors, vol. 21,"
        },
        {
          "Table 3: Performance on KEMDy19": "different methods on the KEMDy19 dataset, employing a leave-",
          "speech.": "no. 5, p. 1579, 2021."
        },
        {
          "Table 3: Performance on KEMDy19": "one-speaker-out 40-fold cross-validation setting. Similar to the",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "[3] K. Noh and H. Jeong, “Emotion-aware speaker identification with"
        },
        {
          "Table 3: Performance on KEMDy19": "findings with the IEMOCAP dataset, we observe that our pro-",
          "speech.": "transfer learning,” IEEE Access, 2023."
        },
        {
          "Table 3: Performance on KEMDy19": "posed approach, SR(·),\nshows\nsuperior performance in both",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "[4] C. M. Lee and S. S. Narayanan, “Toward detecting emotions in"
        },
        {
          "Table 3: Performance on KEMDy19": "UA and WA compared to GAP(·). Since this paper is the first",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "spoken dialogs,” IEEE transactions on speech and audio process-"
        },
        {
          "Table 3: Performance on KEMDy19": "to conduct a leave-one-speaker-out 40-fold cross-validation to",
          "speech.": "ing, vol. 13, no. 2, pp. 293–303, 2005."
        },
        {
          "Table 3: Performance on KEMDy19": "evaluate performance independently of speaker characteristics,",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "[5] M. M. El Ayadi, M. S. Kamel, and F. Karray, “Speech emotion"
        },
        {
          "Table 3: Performance on KEMDy19": "we are unable to compare our\nresults directly with previous",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "recognition using gaussian mixture vector autoregressive models,”"
        },
        {
          "Table 3: Performance on KEMDy19": "works\nfairly.\nHowever,\nthe performance improvements\nfrom",
          "speech.": "in 2007 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "Table 3: Performance on KEMDy19": "the proposed approach, SR(·), compared to the traditional ap-",
          "speech.": "Signal Processing-ICASSP’07, vol. 4.\nIEEE, 2007, pp. IV–957."
        },
        {
          "Table 3: Performance on KEMDy19": "proach, GAP(·), demonstrate the effectiveness of our proposed",
          "speech.": ""
        },
        {
          "Table 3: Performance on KEMDy19": "",
          "speech.": "[6] H. Tang, S. M. Chu, M. Hasegawa-Johnson, and T. S. Huang,"
        },
        {
          "Table 3: Performance on KEMDy19": "method.",
          "speech.": "“Emotion recognition from speech via boosted gaussian mixture"
        },
        {
          "Table 3: Performance on KEMDy19": "Figure 2 shows the confusion matrix generated from our",
          "speech.": "models,” in 2009 IEEE International Conference on Multimedia"
        },
        {
          "Table 3: Performance on KEMDy19": "proposed method. According to this matrix, our proposed ap-",
          "speech.": "and Expo.\nIEEE, 2009, pp. 294–297."
        },
        {
          "Table 3: Performance on KEMDy19": "proach achieves highest\naccuracy in the\nangry class on the",
          "speech.": "[7]\nP. Tzirakis, J. Zhang, and B. Schuller, “End-to-end speech emo-"
        },
        {
          "Table 3: Performance on KEMDy19": "IEMOCAP dataset and in the neutral class on the KEMDy19",
          "speech.": "tion recognition using a deep convolutional\nrecurrent network,”"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Acoustics, Speech and Signal Processing,\nICASSP, Calgary, AB,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "nition Through Focus and Calibration Attention Mechanisms,” in"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Canada, 2018, pp. 15–20.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "Proc. Interspeech 2022, 2022, pp. 136–140."
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[8] C.-W. Huang and S. S. Narayanan, “Attention assisted discovery",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "[23] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "of sub-utterance structure in speech emotion recognition.” in In-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al.,\n“Pytorch:"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "terspeech, 2016, pp. 1387–1391.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "An imperative style, high-performance deep learning library,” Ad-"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "vances in neural information processing systems, vol. 32, 2019."
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[9]\nS. Yoon, S. Byun,\nand K.\nJung,\n“Multimodal\nspeech emotion",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "recognition using audio and text,” in 2018 IEEE spoken language",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "[24] R. Li, Z. Wu,\nJ.\nJia, S. Zhao, and H. Meng, “Dilated residual"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "technology workshop (SLT).\nIEEE, 2018, pp. 112–118.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "network with multi-head self-attention for speech emotion recog-"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "nition,” in ICASSP 2019-2019 IEEE International Conference on"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[10] H.-J. Go, K.-C. Kwak, D.-J. Lee,\nand M.-G. Chun,\n“Emotion",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019,"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "recognition from the facial\nimage and speech signal,”\nin SICE",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "pp. 6675–6679."
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "2003 Annual Conference\n(IEEE Cat. No. 03TH8734),\nvol. 3.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "IEEE, 2003, pp. 2890–2895.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "[25] X. Wang, S. Zhao, and Y. Qin, “Supervised Contrastive Learning"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "with Nearest Neighbor Search for Speech Emotion Recognition,”"
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[11] C.\nBusso,\nZ. Deng,\nS. Yildirim, M.\nBulut,\nC. M.\nLee,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": "in Proc. INTERSPEECH 2023, 2023, pp. 1913–1917."
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "A. Kazemzadeh, S. Lee, U. Neumann, and S. Narayanan, “Anal-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "ysis of emotion recognition using facial expressions, speech and",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "the 6th international\nmultimodal\ninformation,” in Proceedings of",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "conference on Multimodal interfaces, 2004, pp. 205–211.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[12]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“BERT:",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Pre-training\nof\ndeep\nbidirectional\ntransformers\nfor\nlanguage",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "the 2019 Conference of\nthe\nunderstanding,”\nin Proceedings of",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "North American Chapter of\nthe Association for Computational",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Linguistics:\nHuman Language Technologies, Volume 1 (Long",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "and Short Papers).\nMinneapolis, Minnesota: Association for",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Computational Linguistics, Jun. 2019, pp. 4171–4186. [Online].",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Available: https://aclanthology.org/N19-1423",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[13] Y. Liu, M. Ott, N. Goyal,\nJ. Du, M.\nJoshi, D. Chen, O. Levy,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "M. Lewis,\nL. Zettlemoyer,\nand V.\nStoyanov,\n“Roberta:\nA",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "arXiv\npreprint\nrobustly\noptimized\nbert\npretraining\napproach,”",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "arXiv:1907.11692, 2019.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[14]\nT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., “Transformers:",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "State-of-the-art natural\nlanguage processing,” in Proceedings of",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "the 2020 conference on empirical methods in natural\nlanguage",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "processing: system demonstrations, 2020, pp. 38–45.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[15] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "2.0: A framework for self-supervised learning of speech repre-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "sentations,” Advances in neural\ninformation processing systems,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "vol. 33, pp. 12 449–12 460, 2020.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[16] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "dinov, and A. Mohamed, “Hubert:\nSelf-supervised speech rep-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "resentation\nlearning\nby masked\nprediction\nof\nhidden\nunits,”",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "cessing, vol. 29, pp. 3451–3460, 2021.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[17]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen,\nJ. Li,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "N. Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "supervised pre-training for\nfull\nstack speech processing,” IEEE",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Journal of Selected Topics in Signal Processing, vol. 16, no. 6,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "pp. 1505–1518, 2022.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[18]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Y\n. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "W.-C. Tseng, K.\ntik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Processing Universal PERformance Benchmark,” in Proc. Inter-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "speech 2021, 2021, pp. 1194–1198.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[19]\nS. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recogni-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "tion using multi-hop attention mechanism,” in ICASSP 2019-2019",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "IEEE International Conference on Acoustics, Speech and Signal",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Processing (ICASSP).\nIEEE, 2019, pp. 2822–2826.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[20]\nZ. Peng, Y. Lu, S. Pan, and Y. Liu, “Efficient\nspeech emotion",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "recognition using multi-scale cnn and attention,” in ICASSP 2021-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "2021 IEEE International Conference on Acoustics, Speech and",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "Signal Processing (ICASSP).\nIEEE, 2021, pp. 3020–3024.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "[21] Q. Cao, M. Hou, B. Chen, Z. Zhang, and G. Lu, “Hierarchical",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "network based on the fusion of static and dynamic features for",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "speech emotion recognition,” in ICASSP 2021-2021 IEEE Inter-",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "national Conference on Acoustics, Speech and Signal Processing",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        },
        {
          "the 2018 IEEE International Conference on\nin Proceedings of": "(ICASSP).\nIEEE, 2021, pp. 6334–6338.",
          "[22]\nJ. Kim, Y. An, and J. Kim, “Improving Speech Emotion Recog-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Multi-path and group-loss-based network for speech emotion recognition in multi-domain datasets",
      "authors": [
        "K Noh",
        "C Jeong",
        "J Lim",
        "S Chung",
        "G Kim",
        "J Lim",
        "H Jeong"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "4",
      "title": "Emotion-aware speaker identification with transfer learning",
      "authors": [
        "K Noh",
        "H Jeong"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using gaussian mixture vector autoregressive models",
      "authors": [
        "M El Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from speech via boosted gaussian mixture models",
      "authors": [
        "H Tang",
        "S Chu",
        "M Hasegawa-Johnson",
        "T Huang"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "8",
      "title": "End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Attention assisted discovery of sub-utterance structure in speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Attention assisted discovery of sub-utterance structure in speech emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE spoken language technology workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from the facial image and speech signal",
      "authors": [
        "H.-J Go",
        "K.-C Kwak",
        "D.-J Lee",
        "M.-G Chun"
      ],
      "year": "2003",
      "venue": "SICE 2003 Annual Conference"
    },
    {
      "citation_id": "12",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Z Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th international conference on Multimodal interfaces"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "14",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "15",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "16",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong"
      ],
      "venue": ""
    },
    {
      "citation_id": "20",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Li",
        "A Watanabe",
        "H Mohamed",
        "Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Efficient speech emotion recognition using multi-scale cnn and attention",
      "authors": [
        "Z Peng",
        "Y Lu",
        "S Pan",
        "Y Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms",
      "authors": [
        "J Kim",
        "Y An",
        "J Kim"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Dilated residual network with multi-head self-attention for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
      "authors": [
        "X Wang",
        "S Zhao",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    }
  ]
}