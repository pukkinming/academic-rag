{
  "paper_id": "2312.13556v1",
  "title": "Multi-Level Knowledge Distillation For Speech Emotion Recognition In Noisy Conditions",
  "published": "2023-12-21T03:49:46Z",
  "authors": [
    "Yang Liu",
    "Haoqin Sun",
    "Geng Chen",
    "Qingyue Wang",
    "Zhen Zhao",
    "Xugang Lu",
    "Longbiao Wang"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Knowledge Distillation",
    "Wav2vec-2.0",
    "Noise"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) performance deteriorates significantly in the presence of noise, making it challenging to achieve competitive performance in noisy conditions. To this end, we propose a multi-level knowledge distillation (MLKD) method, which aims to transfer the knowledge from a teacher model trained on clean speech to a simpler student model trained on noisy speech. Specifically, we use clean speech features extracted by the wav2vec-2.0 as the learning goal and train the distil wav2vec-2.0 to approximate the feature extraction ability of the original wav2vec-2.0 under noisy conditions. Furthermore, we leverage the multi-level knowledge of the original wav2vec-2.0 to supervise the single-level output of the distil wav2vec-2.0. We evaluate the effectiveness of our proposed method by conducting extensive experiments using five types of noise-contaminated speech on the IEMOCAP dataset, which show promising results compared to state-of-the-art models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the continuous development of artificial intelligence technology, the field of affective computing is gradually attracting the interest of many researchers. Speech emotion recognition (SER), as a very important branch of affective computing, is penetrating into people's lives, such as improving humancomputer interaction  [1] , helping psychological diagnosis  [2]  and enhancing customer service  [3] . Currently, research on SER systems in laboratory environments has yielded good performance  [4, 5] . However, in practical applications, SER systems face more complex and demanding conditions, such as noise from different sources. Therefore, a SER system with better robustness is vital for practical applications.\n\nIn previous studies, various methods have been developed to address the problem of reducing the effects of environmental noise while retaining valid emotional information, including robust acoustic features extraction  [6] , speech enhancement techniques  [7]  and blind source separation  [8] . With the development and widespread application of deep learning, researchers have proposed neural network models with more complex scales in order to improve the accuracy of SER in noisy environment. For example, Wijayasingha et al.  [9]  added synthetic noise to the training data and combined the amplitude spectrogram with a modified delay spectrogram to extract spatio-temporal features to reduce the effect of noise using convolutional neural networks (CNN) and attention mechanism.\n\nZhang et al.  [10]  adopted an autoencoder with long short-term memory (LSTM) for feature enhancement in noisy condition, which significantly improved the accuracy of SER. However, these techniques require complex processing of speech signals before they could be utilized by SER systems for emotion prediction. The additional time overhead associated with the complex processing of the speech signal is very unfriendly to practical applications.\n\nIn order to solve the above problems, the knowledge distillation (KD)  [11]  method may provide a possible solution, which has received a great attention in the field of SER  [12, 13] . For example, Tong et al.  [14]  proposed a framework combining KD and transfer learning to compress the model and improve the accuracy of emotion recognition with a small amount of data and unbalanced emotion categories. Albanie et al.  [15]  and Li et al.  [16]  proposed a cross-modal emotion distillation method to transfer the emotion knowledge learned from vision and text modality to speech modality, respectively, which improves the robustness of SER models. In general, KD focuses on training a simpler student model by utilizing additional information in soft labels provided by a teacher model, which also helps the simpler student model still perform well when dealing with emotional speech in noisy conditions. In this paper, we adopt the KD-based architecture to achieve the trade-off between model complexity and performance for SER in noisy conditions. This paper proposes a framework based on multi-level knowledge distillation (MLKD) for SER in noisy environments. The training phase of our approach is divided into two steps. In the first step, we train the teacher model in a clean speech corpus. In the second step, we train a distil wav2vec-2.0 as student model with a relatively simple structure in noisy speech corpus. Finally, to ensure that the transferred distilled knowledge has a positive impact on the student model, we use the intermediate layer features of the teacher model to guide the student model in enhancing feature extraction capabilities. Experimental results on the benchmark dataset IEMOCAP show that the absolute gains of our proposed method achieves 18.23% on unweighted accuracy (UA) on average compared to the baseline system, across all types of noise.\n\nThe main contributions of this study are summarized as follows:\n\n(1) We propose a novel multi-level knowledge distillation framework, aiming to train a student model on noisy speech corpus to approximate the capability of the teacher model trained on clean speech corpus to classify emotions of noisy speech.\n\n(2) We take advantage of the multi-level knowledge of the teacher model to supervise single-level output of the student model, which is a lighter pre-trained distil wav2vec-2.0.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "The overall structure of the proposed method is shown in Fig.  1 . First, in the teacher model, we use the wav2vec-2.0 containing feature encoder and contextual network to extract the comprehensive emotion feature representation of the raw waveform. Next, after the teacher model is trained, we use the distilled knowledge of the teacher model to provide guidance to the student model on how to obtain better denoising and feature extraction capability in the noisy speech corpus.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Level Knowledge Distillation Framework",
      "text": "Our proposed MLKD framework consists of a teacher model, a student model, and a multi-level loss block, which aims to enable the structurally simplified student model to imitate the feature extraction ability of the teacher model when trained on a noisy speech corpus. In the MLKD framework, the teacher model is obtained by training on a clean speech corpus and the student model learns features from multiple intermediate layers of the teacher model, which could fully exploit the feature extraction capability of the teacher model, and learn the feature information of the teacher model from shallow to deep.\n\nThe learning process consists of two steps: (1) Model training: we train the teacher model in the clean speech corpus.\n\n(2) Distillation: the student model performs two tasks, where the first one is to predict emotion classification, and the second one is to perform a feature regression task using the extracted knowledge in order to simulate the output of the teacher network. We present more details about the knowledge distillation framework below.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Teacher Model",
      "text": "Pre-trained wav2vec-2.0: The pre-trained wav2vec-2.0 consists of feature encoder and context network. First, the original audio waveform x ∈ R N is encoded by the feature encoder consisting of seven 1D convolutional layers as f ϕ , which represents the low-level embedding features of the original waveform. Then, the channel dimension of f ϕ is normalized and then activated by the Gelu function. Next, f ϕ is fed into the context network, which consists of twelve transformer blocks, each containing twelve attention heads. Finally, we take the feature vector z ∈ R L b of the last hidden layer of the context network as the input of the fully connected layer, where L b represents the hidden dimension, which is set to 768.\n\nFully Connected Layer: In the final stage of the teacher model, we use a fully connected layer to compress the feature vector z to obtain the soft label y t for the emotion classification prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Student Model",
      "text": "Inspired by the idea of distil-BERT  [17] , we compress the context network of wav2vec-2.0 from the twelve transformer blocks to six blocks. In the traning process of the student model, first, we add noise with different Signal to Noise Ratios (SNRs) to the clean speech corpus as the input of the student model. Then, the wav2vec-2.0 feature encoder encodes the noisy audio waveforms as low-level embedding features. Next, we initialize the transformer block in student model with the even layer weights of the context network of the teacher model. Finally, the soft prediction y s is obtained by the fully connected layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Level Loss Block",
      "text": "We compute the loss of the six transformer blocks so that the student model could learn more feature knowledge from the hidden states of multiple intermediate layers of the teacher model. We then use the transformer blocks of the even layers of the context network of teacher model to guide the student model. The intermediate layer of the teacher model is denoted as It,i, and the intermediate layer of the student model is denoted as Is,i, where It,i = {2, 4, 6, 8, 10, 12}, and Is,i = {1,2,3,4,5,6}. For the i-th intermediate layer, h t i and h s i represents the hidden state of the teacher model at the It,i-th layer and student model at the Is,i-th layer, respectively. We use the mean square error (MSE) to measure the distance between distillation features. The multi-level loss function is calculated as follow.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Training Objective",
      "text": "In the final stage of the distillation framework, we follow the previous work  [18]  to transfer knowledge of the decision level of the teacher model to the student model by minimizing the distance between the soft label and soft prediction. Specifically, we use Kullback-Leibler divergence to measure the distance of the emotional probability distribution between the soft label and soft prediction. The distillation loss LKL is calculated as follows.\n\nwhere, p t i represents the soft label for the i-th sample generated by the teacher model. p s i represents the soft prediction for the i-th sample generated by the student model. T represents the distillation temperature coefficient, which is used to soften the probability distribution of the soft label.\n\nAlthough the knowledge of the teacher model far exceeds that of the student model, it still has the potential to make errors. If the student model could correctly recognize emotions in addition to the teacher's guidance, it could effectively reduce the possibility of occasional errors by the teacher model. Therefore, the final objective function L is obtained by weighting the distillation loss and the student loss.\n\nwhere LC represents cross-entropy loss function, which is defined as student loss. y i,k represents the true probability distribution and the true label of the i-th sample as k. p i,k represents the probability output of the softmax layer and the probability that the i-th sample is predicted to be the k-th label.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Evaluation Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Dataset Iemocap:",
      "text": "The IEMOCAP corpus  [19]  contains 10,039 utterances with a sampling rate of 16 kHz. Each utterance in the corpus is annotated with an emotion. Four emotions are selected: happiness (combined with the excitement class), anger, neutral and sadness, for a total of 5531 utterances in our experiments.\n\nNoisex-92 database: The Noisex-92 database  [20]  is noise data measured in the field by the Speech Research Unit of the Institute of Perceptual Technology in the UK, where all noise files are of 235 seconds duration, using a sampling rate of 19.98 KHz. It consists of 15 categories of noise. We have used 5 different types of noise (voice: babble, F-16 fighter-jet, factory, HF radio and volvo 340) in our experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "We choose both the distillation temperature T and the weight of distillation loss α by the grid search method and obtain the best performance of the student model when T and α are set to 7 and 0.7, respectively. The pre-trained wav2vec-2.0 is used in our experiments, which is implemented based on the Huggingface transformers repository  [21] . We use 10-speaker-independent Leave-One-Speaker-Out cross-validation strategy. We choose the AdamW optimizer and the learning rate is 5 × 10 -5 . We set the epoch to 200 to train the teacher and student models, and the batch size is 8.\n\nIn the experiment, we add noise to the clean signal using the FANT toolkit  [22]  at levels of SNRs of 0 dB, 5 dB, 10 dB, 15 dB, and 20 dB. We randomly select a noise to add to each clean speech. Voice activity detector + Non-negative Matrix Factorization (VAD + NMF)  [23]  is chosen as the baseline to compare with our proposed method. UA is chosen to evaluate the performance of the proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation",
      "text": "To evaluate the effectiveness of the proposed MLKD framework for SER, two different approaches are implemented to train the student model. The first approach is to train the student model independently, while the second approach is to train it under the supervision of the teacher model. After training, the student model is tested under factory noise conditions with six SNR levels. The implementation results show that the student model under the supervision of the teacher model ourperforms the student model trained independently. The absolute improvement of UA is 6.10% (0 dB), 4.42% (5 dB), 3.37% (10 dB), 1.32% (15 dB), 1.57% (20 dB), and 1.68% (clean speech), with an average improvement of 3.08% across all types of noise. The teacher model focuses on how to extract comprehensive emotion information in the clean speech. The student model could learn the knowledge of emotion feature representations from the teacher model and apply it to the noisy speech, thus alleviating the effect of noise on emotion recognition. It should be noted that when the SNR is above 15 dB, the contaminated speech is close to clean speech, and in this case, the recognition performance of the student model is lower than that of the teacher model. However, the performance of student model gradually surpasses that of teacher model as the SNR decreases. Therefore, our proposed method performs better at low SNRs, while it is slightly inferior to the teacher model at high SNRs. These findings suggest that our proposed MLKD framework could effectively improve the performance of the student model, which is more suitable for SER in noisy conditions.\n\nTo evaluate the computation complexity of the proposed student model, we compare the number of parameters and reasoning time between the teacher model and the student model, as presented in Table  3 . The teacher model occupies 360.17 MB of memory space, whereas the student model occupies only 197 MB of memory space, indicating that the proposed student model has 45% fewer parameters compared to the teacher model. Furthermore, the inference time of both models is compared on the same epoch using an Intel(R) Xeon(R) CPU Silver 4110 of 2.10 GHz 64-bit processor with 16 GB RAM. The teacher model takes 926 seconds to recognize 507 samples, while the student model takes only 444 seconds. The results show that in the case of clean speech, the proposed student",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison With State-Of-The-Art Approaches",
      "text": "To demonstrate the effectiveness of our proposed MLKD framework for SER, we conducted experiments to compare it with a baseline system (VAD + NMF). The results are showed in Table  2 , which shows the emotion recognition accuracy of our proposed method under five types of noise conditions (Babble, F-16, Factory, HF-channel, Volvo) at five different SNR levels (0 dB, 5 dB, 10 dB, 15 dB, 20 dB). For all SNR levels of each type of noise, the average absolute improvement on UA using our proposed method over the baseline is 14.57% (Babble), 21.99% (F-16), 21.44% (Factory), 18.34% (HF-channel), 14.79% (Volvo) (with an average improvement of 18.23% across all types of noise). Specifically, when the SNR is reduced from 20 dB to 15 dB, the UA of our supervised student model decreases by only 1.00%, while the UA of the baseline decreases by 2.22%. Although the performance of both models decreases, the degree of decrease of our student model is less significant, indicating the superiority of our student model. Furthermore, when the SNR is reduced to 10 dB, the performance of both methods continues to decrease. However, the performance of our supervised student model is the least affected, where the UA remains close to 68%. In addition, the advantage of our proposed method is more evident at SNRs of 0 dB and 5 dB. The baseline method shows an average perfor-mance degradation of about 8%, while the performance of our proposed method only decreases by 4%. Therefore, our student model is more robust and has better generalization ability. Moreover, the proposed MLKD approach enables the student model to extract clean emotion features from the corpus in noisy environments, which allows our student model to maintain impressive performance in the noisy speech corpus. Overall, the experimental results show that our proposed method outperforms the baseline system, and the student model trained with MLKD is more suitable for SER in noisy environments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we present a multi-level knowledge distillation framework that includes a teacher model, a student model, and a multi-level loss block. Our framework aims to transfer the feature extraction ability of the teacher model in the clean speech corpus to a more lightweight student model trained with noisy speech corpus. We first use the wav2vec-2.0 model to extract comprehensive acoustic features. Next, we design a distilled student model, distil wav2vec-2.0, where we utilize the transformer block weights of the even layer of the teacher model to initialize the student model to enhance its training and convergence. Lastly, we use the multi-level loss block, which uses multiple intermediate layer representations of the teacher model to guide the output of intermediate layers of the student model. Our series of experimental results show that our student model maintains a relatively impressive ability for emotion recognition of contaminated speech under the guidance of the teacher model. Compared to the state-of-the-art approach, the absolute gains of our proposed method achieve an average of 18.23% on UA across all types of noise.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of MLKD, which consists of a teacher model, a student model, and a multi-level loss block. The",
      "page": 2
    },
    {
      "caption": "Figure 1: First, in the teacher model, we use the wav2vec-2.0 con-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Noisy types": "Method\nSNRs",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "baseline"
        },
        {
          "Noisy types": "0 dB",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "45.12"
        },
        {
          "Noisy types": "5 dB",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "46.87"
        },
        {
          "Noisy types": "10 dB",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "53.01"
        },
        {
          "Noisy types": "15 dB",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "55.81"
        },
        {
          "Noisy types": "20 dB",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "56.12"
        },
        {
          "Noisy types": "Mean",
          "Babble\nF16\nFactory\nHF-channel\nVolvo": "67.58\n66.64\n67.45\n66.46\n69.55\n53.01\n44.65\n46.01\n48.12\n54.76"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in humancomputer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "3",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "4",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "5",
      "title": "A discriminative feature representation method based on cascaded attention network with adversarial strategy for speech emotion recognition",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Y Li",
        "M Unoki",
        "Z Zhao"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using non-linear teager energy based features in noisy environments",
      "authors": [
        "A Georgogiannis",
        "V Digalakis"
      ],
      "year": "2012",
      "venue": "Speech emotion recognition using non-linear teager energy based features in noisy environments"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion analysis in noisy real-world environment",
      "authors": [
        "A Tawari",
        "M Trivedi"
      ],
      "year": "2010",
      "venue": "2010 20th International Conference on Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Channel mapping using bidirectional long short-term memory for dereverberation in hands-free voice controlled devices",
      "authors": [
        "Z Zhang",
        "J Pinto",
        "C Plahl",
        "B Schuller",
        "D Willett"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "10",
      "title": "Robustness to noise for speech emotion classification using cnns and attention mechanisms",
      "authors": [
        "L Wijayasingha",
        "J Stankovic"
      ],
      "year": "2021",
      "venue": "Smart Health"
    },
    {
      "citation_id": "11",
      "title": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with lstm neural networks",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "J Han",
        "J Deng",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings INTERSPEECH 2016, 17th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "12",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "13",
      "title": "End-to-end emotional speech recognition using acoustic model adaptation based on knowledge distillation",
      "authors": [
        "H.-I Yun",
        "J.-S Park"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "14",
      "title": "Electroglottograph-based speech emotion recognition via cross-modal distillation",
      "authors": [
        "L Chen",
        "J Ren",
        "X Mao",
        "Q Zhao"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "15",
      "title": "Multimodal music emotion recognition method based on the combination of knowledge distillation and transfer learning",
      "authors": [
        "G Tong"
      ],
      "year": "2022",
      "venue": "Scientific Programming"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition via multilevel cross-modal distillation",
      "authors": [
        "R Li",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "19",
      "title": "Learning small-size dnn with output-distribution-based criteria",
      "authors": [
        "J Li",
        "R Zhao",
        "J.-T Huang",
        "Y Gong"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "The noisex-92 study on the effect of additive noise on automatic speech recognition",
      "authors": [
        "A Varga"
      ],
      "year": "1992",
      "venue": "The noisex-92 study on the effect of additive noise on automatic speech recognition"
    },
    {
      "citation_id": "22",
      "title": "Huggingface's transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "23",
      "title": "Fant-filtering and noise adding tool",
      "authors": [
        "H Hirsch"
      ],
      "year": "2005",
      "venue": "Niederrhein University of Applied Sciences"
    },
    {
      "citation_id": "24",
      "title": "Robust front-end processing for emotion recognition in noisy speech",
      "authors": [
        "M Pandharipande",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2018",
      "venue": "2018 11th International Symposium on Chinese Spoken Language Processing"
    }
  ]
}