{
  "paper_id": "2411.09080v1",
  "title": "Language Models For Music Medicine Generation",
  "published": "2024-11-13T23:17:47Z",
  "authors": [
    "Emmanouil Nikolakakis",
    "Joann Ching",
    "Emmanouil Karystinaios",
    "Gabrielle Sipin",
    "Gerhard Widmer",
    "Razvan Marinescu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Music therapy has been shown in recent years to provide multiple health benefits related to emotional wellness. In turn, maintaining a healthy emotional state has proven to be effective for patients undergoing treatment, such as Parkinson's patients or patients suffering from stress and anxiety. We propose fine-tuning MusicGen, a music-generating transformer model, to create short musical clips that assist patients in transitioning from negative to desired emotional states. Using low-rank decomposition fine-tuning on the MTG-Jamendo Dataset with emotion tags, we generate 30-second clips that adhere to the iso principle, guiding patients through intermediate states in the valence-arousal circumplex. The generated music is evaluated using a music emotion recognition model to ensure alignment with intended emotions. By concatenating these clips, we produce a 15-minute \"music medicine\" resembling a music therapy session. Our approach is the first model to leverage Language Models to generate music medicine. Ultimately, the output is intended to be used as a temporary relief between music therapy sessions with a board-certified therapist.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music therapy has proved to be a highly effective alternative treatment to traditional medication since the late nineteenth century. For most of the twentieth century, music therapy's positive effects in treating patients were investigated empirically, and the results were presented from a qualitative perspective. Nevertheless, recent advances in sensor technology have allowed clinicians to perform experiments that measure changes in heart rate, electromyogram, respiration, and skin conductance. Moreover, neuroimaging methodologies, such as Positron Emission Tomography and functional Magnetic Resonance Imaging  [1]  enable the visualization of the signals transmitted between neurons and the cerebral blood flow within the brain during sessions. Therefore, the patient's response to music has been studied from a cognitive perspective.\n\nWith the advent of generative models such as Transformers and Diffusion models, various works have investigated the quality and diversity of AI-generated music for different applications  [2] [3] [4] . Some examples of tasks that can be accomplished involve style change  [5] , orchestration  [6] , and novel music generation pieces from a given text prompt  [7] .\n\nFollowing the popularity of generative AI, Williams et al.  [8]  evaluated the effectiveness of a Markov model to generate therapeutic music for a patient by measuring their Galvanic Skin Response at any given moment and confirmed its effects. Following those results, Hou et al.  [9]  and Li et al.  [10]  presented two long-short term memory (LSTM) based generative models adapted for music therapy given a treatment scenario. More recently, music generation in the symbolic domain has shown promising results for music therapy  [11] .\n\nIn this work, we propose a generative music medicine model 1 by fine-tuning MusicGen  [7]  using low-rank decomposition  [12]  with prompts that contain emotion labels. We aim to generate a \"therapy session\" that follows the iso principle  [13] , which aims to alter a person's mood by playing music matching their current mood and then gradually shifting to music that represents a desired positive state. Iso is a commonly used practice in music therapy, with its effectiveness demonstrated in recent studies  [14, 15] . We use the term music medicine for the output of our model, as music therapy is always conducted in coordination with a licensed music therapist  [16] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology 2.1 Fine-Tuning Musicgen",
      "text": "We leverage the MusicGen model  [7] , a single Language Model (LM) for continuous conditional music generation. MusicGen was trained on 20K hours of licensed instrumentonly music and approximately 400K tracks, thus creating an awareness of different genres and instruments. We fine-tune MusicGen with the MTG-Jamendo Dataset  [17] , specifically using the subset where mood/theme tags are available,    [18]  creating an emotion-aware music generation model.\n\nWe use parameter-efficient fine-tuning training with Low-Rank Adaptation (LoRA)  [12]  to fine-tune the Mu-sicGen model without adapting the original weights provided by  [7] . As each piece in the subset is associated with mood tags, we can map them to specific emotions from the circumplex  [18]  (Fig.  1 ). A pile sorting experiment was conducted to assign an individual mapping of moods to emotions by three independent researchers and one by Anthropic's Claude AI. We obtained an inter-rater agreement score (Fleiss Kappa) of 0.2471, confirming that our individual ranking corresponds to a fair agreement. Finally, the mapping is verified by a music therapist.\n\nWe validate the fine-tuning between a set number of iterations by extracting the audio output's emotion using an emotion and theme recognition model  [19] . We then compare whether the extracted emotion matches the intended emotion of our prompt.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Continuous Prompt Engineering",
      "text": "Our model creates an audio output of around 15 minutes by concatenating multiple 30-second audio clips generated at each mood state. The audio clips are each generated with a given prompt to the fine-tuned model containing comma-separated tags concerning the mood tag, emotion, instrumentation, and genre (e.g. \"sad, piano, classical\"). To follow the iso principle, we determine a path between the initial and desired state, generating audio segments along the intermediate emotional states. Our approach is designed to begin with a text input for the first time step. For the following time steps, the model's input consists of part of the audio output of the previous time step, along with a text prompt consisting of the subsequent emotional state, the preferred instrument, and the genre of music. We introduce a temperature variable that increases the chance of changing the instrumentation and genre between audio clips according to the statistical distribution of these labels for a given mood in the MTG-Jamendo Dataset subset used for fine-tuning. To ensure high-quality generation, we trim any silence at beginning and end of the 30-sec generated clips before using them for conditioning the next generation.\n\nAfter obtaining the generated clips, we normalize them individually. Subsequently, we crossfade them with an overlap of a quarter length from the previous one, therefore combining them into a single audio. We further post-process the concatenated audio by applying a high-pass filter and dynamic denoising using spectral gating. Therefore, we aim to have a final audio output that maximizes auditory pleasure and comfort for the individual undergoing the therapy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "We fine-tuned and experimented with three different variations of MusicGen: 1) MusicGen-Small, 2) MusicGen-Medium, and 3) MusicGen-Large  [7] . After experimenting, we opted to fine-tune the model for 2 epochs with a learning rate of 7 * 10 6 and a linear decay scheduler. All training and inference were performed on a single NVIDIA A40 GPU.\n\nThe generated output from each model successfully traverses states in the circumplex, starting and ending at the initial and desired state, respectively. To quantitatively evaluate the output (see Table  1 ), we use the following metrics: 1) Contrastive Language-Audio Pretraining (CLAP)  [20] , which evaluates the relevance of the audio output to the text prompt input, 2) Area under the precision-recall curve (AUPRC) and 3) Hamming Score; the latter two are used to evaluate the mood of the generated audio segment by passing it to an emotion recognition model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusions",
      "text": "This work presented a novel approach to generative music medicine. The generated audio follows the iso principle, a well-established methodology in music therapy, dynamically adapting instruments and genres to reflect the patient's evolving emotional state. All steps of our work have been evaluated and approved by a licensed music therapist. Our future direction involves conducting a user study to evaluate the model's effectiveness in mental wellness improvement. Moreover, we wish to make a more controllable system by passing continuous coordinates on the circumplex as input to the prompt rather than discrete states to generate a smoother transition between states and audio segments.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotion Circumplex Model [18]",
      "page": 2
    },
    {
      "caption": "Figure 1: ). A pile sorting experiment",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "firstname.lastname@jku.at": "ABSTRACT"
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "Music therapy has been shown in recent years to provide"
        },
        {
          "firstname.lastname@jku.at": "multiple health benefits related to emotional wellness.\nIn"
        },
        {
          "firstname.lastname@jku.at": "turn, maintaining a healthy emotional state has proven to be"
        },
        {
          "firstname.lastname@jku.at": "effective for patients undergoing treatment, such as Parkin-"
        },
        {
          "firstname.lastname@jku.at": "son’s patients or patients suffering from stress and anxi-"
        },
        {
          "firstname.lastname@jku.at": "ety. We propose fine-tuning MusicGen, a music-generating"
        },
        {
          "firstname.lastname@jku.at": "transformer model, to create short musical clips that assist"
        },
        {
          "firstname.lastname@jku.at": "patients in transitioning from negative to desired emotional"
        },
        {
          "firstname.lastname@jku.at": "states. Using low-rank decomposition fine-tuning on the"
        },
        {
          "firstname.lastname@jku.at": "MTG-Jamendo Dataset with emotion tags, we generate"
        },
        {
          "firstname.lastname@jku.at": "30-second clips that adhere to the iso principle, guiding"
        },
        {
          "firstname.lastname@jku.at": "patients through intermediate states in the valence-arousal"
        },
        {
          "firstname.lastname@jku.at": "circumplex. The generated music is evaluated using a mu-"
        },
        {
          "firstname.lastname@jku.at": "sic emotion recognition model to ensure alignment with in-"
        },
        {
          "firstname.lastname@jku.at": "tended emotions. By concatenating these clips, we produce"
        },
        {
          "firstname.lastname@jku.at": "a 15-minute \"music medicine\" resembling a music therapy"
        },
        {
          "firstname.lastname@jku.at": "session. Our approach is the first model\nto leverage Lan-"
        },
        {
          "firstname.lastname@jku.at": "guage Models to generate music medicine. Ultimately, the"
        },
        {
          "firstname.lastname@jku.at": "output is intended to be used as a temporary relief between"
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "music therapy sessions with a board-certified therapist."
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "1.\nINTRODUCTION"
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "Music therapy has proved to be a highly effective alternative"
        },
        {
          "firstname.lastname@jku.at": "treatment to traditional medication since the late nineteenth"
        },
        {
          "firstname.lastname@jku.at": "century. For most of the twentieth century, music therapy’s"
        },
        {
          "firstname.lastname@jku.at": "positive effects in treating patients were investigated em-"
        },
        {
          "firstname.lastname@jku.at": "pirically, and the results were presented from a qualitative"
        },
        {
          "firstname.lastname@jku.at": "perspective. Nevertheless, recent advances in sensor tech-"
        },
        {
          "firstname.lastname@jku.at": "nology have allowed clinicians to perform experiments that"
        },
        {
          "firstname.lastname@jku.at": "measure changes in heart rate, electromyogram, respiration,"
        },
        {
          "firstname.lastname@jku.at": "and skin conductance. Moreover, neuroimaging methodolo-"
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "gies, such as Positron Emission Tomography and functional"
        },
        {
          "firstname.lastname@jku.at": "Magnetic Resonance Imaging [1] enable the visualization"
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "* Equal contribution among authors."
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "© E. Nikolakakis, E. Karystinaios, J. Ching, G. Sipin, G."
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "Widmer, R. Marinescu. Licensed under a Creative Commons Attribution"
        },
        {
          "firstname.lastname@jku.at": "4.0 International License (CC BY 4.0). Attribution:\nE. Nikolakakis, E."
        },
        {
          "firstname.lastname@jku.at": "Karystinaios, J. Ching, G. Sipin, G. Widmer, R. Marinescu, “Language"
        },
        {
          "firstname.lastname@jku.at": "Models for Music Medicine Generation”, in Extended Abstracts for the"
        },
        {
          "firstname.lastname@jku.at": ""
        },
        {
          "firstname.lastname@jku.at": "Late-Breaking Demo Session of the 25th Int. Society for Music Information"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": "-"
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        },
        {
          "Table 1: Evaluation scores for the three fine-tuned models and their percentage improvements over the non-finetuned version.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "and comfort for the individual undergoing the therapy."
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "creating an emotion-aware music generation model.",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "3. RESULTS"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "We use parameter-efficient fine-tuning training with",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "Low-Rank Adaptation (LoRA) [12] to fine-tune the Mu-",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "We fine-tuned and experimented with three different vari-"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "sicGen model without adapting the original weights pro-",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "ations of MusicGen: 1) MusicGen-Small, 2) MusicGen-"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "vided by [7]. As each piece in the subset is associated with",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "Medium, and 3) MusicGen-Large [7]. After experimenting,"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "mood tags, we can map them to specific emotions from",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "we opted to fine-tune the model for 2 epochs with a learning"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "the circumplex [18]\n(Fig. 1). A pile sorting experiment",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "rate of 7 ∗ 106 and a linear decay scheduler. All training and"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "was conducted to assign an individual mapping of moods",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "inference were performed on a single NVIDIA A40 GPU."
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "to emotions by three independent researchers and one by",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "The generated output from each model successfully tra-"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "Anthropic’s Claude AI. We obtained an inter-rater agree-",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "verses states in the circumplex, starting and ending at the"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "ment score (Fleiss Kappa) of 0.2471, confirming that our",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "initial and desired state, respectively. To quantitatively eval-"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "individual ranking corresponds to a fair agreement. Finally,",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "uate the output (see Table 1), we use the following metrics:"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "the mapping is verified by a music therapist.",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "1) Contrastive Language-Audio Pretraining (CLAP) [20],"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "We validate the fine-tuning between a set number of it-",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "which evaluates the relevance of the audio output\nto the"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "erations by extracting the audio output’s emotion using an",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "text prompt input, 2) Area under the precision-recall curve"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "emotion and theme recognition model [19]. We then com-",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "(AUPRC) and 3) Hamming Score; the latter two are used"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "pare whether the extracted emotion matches the intended",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "to evaluate the mood of the generated audio segment by"
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "emotion of our prompt.",
          "have a final audio output that maximizes auditory pleasure": ""
        },
        {
          "Figure 1: Emotion Circumplex Model [18]": "",
          "have a final audio output that maximizes auditory pleasure": "passing it to an emotion recognition model."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "peutic music based on the iso principle,” in Summit on"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Our implementation functions as a proof of concept, but it",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Music Intelligence.\nSpringer, 2023, pp. 32–45."
        },
        {
          "5. ACKNOWLEDGEMENTS": "must still be evaluated experimentally before being clini-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "cally applied as medical treatment. This work is supported",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[12] E.\nJ. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li,"
        },
        {
          "5. ACKNOWLEDGEMENTS": "by the European Research Council (ERC) under the EU’s",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "S. Wang, L. Wang, and W. Chen, “Lora: Low-rank"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Horizon 2020 research & innovation programme, grant",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "adaptation of large language models,” in International"
        },
        {
          "5. ACKNOWLEDGEMENTS": "agreement No. 101019375 (Whither Music?).",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Conference on Learning Representations (ICLR), 2021."
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[13] W. B. Davis, K. E. Gfeller, and M. H. Thaut, “An intro-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "6. REFERENCES",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "duction to music therapy: Theory and practice,” Ameri-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "[1] A. J. Blood and R. J. Zatorre, “Intensely pleasurable",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "can Music Therapy Association, 2008."
        },
        {
          "5. ACKNOWLEDGEMENTS": "responses to music correlate with activity in brain re-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[14] A. Heiderscheit and A. Madson, “Use of the iso princi-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "gions implicated in reward and emotion,” Proceedings",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "ple as a central method in mood management: A music"
        },
        {
          "5. ACKNOWLEDGEMENTS": "of\nthe National Academy of Sciences, vol. 98, no. 20,",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "psychotherapy clinical case study,” Music therapy per-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "pp. 11 818–11 823, 2001.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "spectives, vol. 33, no. 1, pp. 45–52, 2015."
        },
        {
          "5. ACKNOWLEDGEMENTS": "[2] C.-Z. A. Huang, A. Vaswani,\nJ. Uszkoreit, N. M.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[15] K. Starcke, J. Mayr, and R. von Georgi, “Emotion mod-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Shazeer,\nI. Simon, C. Hawthorne, A. M. Dai, M. D.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "ulation through music after sadness induction—the iso"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Hoffman, M. Dinculescu, and D. Eck, “Music trans-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "principle in a controlled experimental study,” Interna-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "former: Generating music with long-term structure,” in",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "tional Journal of Environmental Research and Public"
        },
        {
          "5. ACKNOWLEDGEMENTS": "International Conference on Learning Representations",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Health, vol. 18, no. 23, p. 12486, 2021."
        },
        {
          "5. ACKNOWLEDGEMENTS": "(ICLR), 2018.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[16] T. Stegemann, M. Geretsegger, E. Phan Quoc, H. Riedl,"
        },
        {
          "5. ACKNOWLEDGEMENTS": "[3] Z. Wang, L. Min, and G. G. Xia, “Whole-song hierarchi-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "and M. Smetana, “Music therapy and other music-based"
        },
        {
          "5. ACKNOWLEDGEMENTS": "cal generation of symbolic music using cascaded diffu-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "interventions\nin pediatric health care:\nan overview,”"
        },
        {
          "5. ACKNOWLEDGEMENTS": "sion models,” in International Conference on Learning",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Medicines, vol. 6, no. 1, p. 25, 2019."
        },
        {
          "5. ACKNOWLEDGEMENTS": "Representations (ICLR), 2024.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[17] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and"
        },
        {
          "5. ACKNOWLEDGEMENTS": "[4] D. Jeong, T. Kwon, and J. Nam, “Virtuosonet : A hier-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "X. Serra, “The MTG-jamendo dataset\nfor automatic"
        },
        {
          "5. ACKNOWLEDGEMENTS": "archical attention rnn for generating expressive piano",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "music tagging,” in Machine Learning for Music Discov-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "performance from music score,” in 32nd Conference on",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "ery Workshop, International Conference on Machine"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Neural Information Processing Systems (NIPS), 2018.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Learning (ICML), 2019."
        },
        {
          "5. ACKNOWLEDGEMENTS": "[5] S.-L. Wu and Y.-H. Yang, “MuseMorphose: Full-song",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[18]\nJ. Posner,\nJ. A. Russell,\nand B. S. Peterson,\n“The"
        },
        {
          "5. ACKNOWLEDGEMENTS": "and fine-grained piano music style transfer with one",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "circumplex model of affect: An integrative approach"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Transformer VAE,” IEEE/ACM Transactions on Audio,",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "to affective neuroscience, cognitive development, and"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Speech, and Language Processing, 2023.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "psychopathology,” Development and psychopathology,"
        },
        {
          "5. ACKNOWLEDGEMENTS": "[6]\nJ. Zhao, G. Xia, and Y. Wang, “Q&A: Query-based",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "vol. 17, no. 3, pp. 715–734, 2005."
        },
        {
          "5. ACKNOWLEDGEMENTS": "representation learning for multi-track symbolic music",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[19] M. Mayerl, M. Vötter, A. Peintner, G. Specht, and"
        },
        {
          "5. ACKNOWLEDGEMENTS": "re-arrangement,” in International Joint Conference on",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "E. Zangerle, “Recognizing Song Mood and Theme:"
        },
        {
          "5. ACKNOWLEDGEMENTS": "Artificial Intelligence (IJCAI), 2023.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Clustering-based Ensembles,” in MediaEval 2021 Work-"
        },
        {
          "5. ACKNOWLEDGEMENTS": "J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Syn-\n[7]",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "shop, 2021."
        },
        {
          "5. ACKNOWLEDGEMENTS": "naeve, Y. Adi, and A. Défossez, “Simple and control-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "lable music generation,” Advances in Neural Informa-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "[20] B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang,"
        },
        {
          "5. ACKNOWLEDGEMENTS": "tion Processing Systems, vol. 36, 2024.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "“CLAP: Learning audio concepts from natural language"
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "supervision,”\nin IEEE International Conference on"
        },
        {
          "5. ACKNOWLEDGEMENTS": "[8] D. Williams, V. J. Hodge, and C.-Y. Wu, “On the use of",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "Acoustics, Speech and Signal Processing (ICASSP),"
        },
        {
          "5. ACKNOWLEDGEMENTS": "AI for generation of functional music to improve mental",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": "2023."
        },
        {
          "5. ACKNOWLEDGEMENTS": "health,” Frontiers in Artificial\nIntelligence, vol. 3, p.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "497864, 2020.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "[9] Y. Hou, “AI music therapist: A study on generating",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "specific therapeutic music based on deep generative",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "adversarial network approach,” in 2nd International",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "Conference on Electronic Technology, Communication",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "and Information (ICETCI).\nIEEE, 2022.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "[10] Y. Li, X. Li, Z. Lou, and C. Chen, “Long short-term",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "memory-based music analysis system for music ther-",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        },
        {
          "5. ACKNOWLEDGEMENTS": "apy,” Frontiers in Psychology, vol. 13, p. 928048, 2022.",
          "[11] Z. Qiu, R. Yuan, W. Xue, and Y. Jin, “Generated thera-": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Intensely pleasurable responses to music correlate with activity in brain regions implicated in reward and emotion",
      "authors": [
        "A Blood",
        "R Zatorre"
      ],
      "year": "2001",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "3",
      "title": "Music transformer: Generating music with long-term structure",
      "authors": [
        "C.-Z Huang",
        "A Vaswani",
        "J Uszkoreit",
        "N Shazeer",
        "I Simon",
        "C Hawthorne",
        "A Dai",
        "M Hoffman",
        "M Dinculescu",
        "D Eck"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "4",
      "title": "Whole-song hierarchical generation of symbolic music using cascaded diffusion models",
      "authors": [
        "Z Wang",
        "L Min",
        "G Xia"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "5",
      "title": "Virtuosonet : A hierarchical attention rnn for generating expressive piano from music score",
      "authors": [
        "D Jeong",
        "T Kwon",
        "J Nam"
      ],
      "year": "2018",
      "venue": "32nd Conference on Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "6",
      "title": "MuseMorphose: Full-song and fine-grained piano music style transfer with one Transformer VAE",
      "authors": [
        "S.-L Wu",
        "Y.-H Yang"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Q&A: Query-based representation learning for multi-track symbolic music re-arrangement",
      "authors": [
        "J Zhao",
        "G Xia",
        "Y Wang"
      ],
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "8",
      "title": "Simple and controllable music generation",
      "authors": [
        "J Copet",
        "F Kreuk",
        "I Gat",
        "T Remez",
        "D Kant",
        "G Synnaeve",
        "Y Adi",
        "A Défossez"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "On the use of AI for generation of functional music to improve mental health",
      "authors": [
        "D Williams",
        "V Hodge",
        "C.-Y Wu"
      ],
      "year": "2020",
      "venue": "Frontiers in Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "AI music therapist: A study on generating specific therapeutic music based on deep generative adversarial network approach",
      "authors": [
        "Y Hou"
      ],
      "year": "2022",
      "venue": "2nd International Conference on Electronic Technology, Communication and Information (ICETCI)"
    },
    {
      "citation_id": "11",
      "title": "Long short-term memory-based music analysis system for music therapy",
      "authors": [
        "Y Li",
        "X Li",
        "Z Lou",
        "C Chen"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "12",
      "title": "Generated therapeutic music based on the iso principle",
      "authors": [
        "Z Qiu",
        "R Yuan",
        "W Xue",
        "Y Jin"
      ],
      "year": "2023",
      "venue": "Summit on Music Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "14",
      "title": "An introduction to music therapy: Theory and practice",
      "authors": [
        "W Davis",
        "K Gfeller",
        "M Thaut"
      ],
      "year": "2008",
      "venue": "An introduction to music therapy: Theory and practice"
    },
    {
      "citation_id": "15",
      "title": "Use of the iso principle as a central method in mood management: A music psychotherapy clinical case study",
      "authors": [
        "A Heiderscheit",
        "A Madson"
      ],
      "year": "2015",
      "venue": "Music therapy perspectives"
    },
    {
      "citation_id": "16",
      "title": "Emotion modulation through music after sadness induction-the iso principle in a controlled experimental study",
      "authors": [
        "K Starcke",
        "J Mayr",
        "R Georgi"
      ],
      "year": "2021",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "17",
      "title": "Music therapy and other music-based interventions in pediatric health care: an overview",
      "authors": [
        "T Stegemann",
        "M Geretsegger",
        "E Phan Quoc",
        "H Riedl",
        "M Smetana"
      ],
      "year": "2019",
      "venue": "Medicines"
    },
    {
      "citation_id": "18",
      "title": "The MTG-jamendo dataset for automatic music tagging",
      "authors": [
        "D Bogdanov",
        "M Won",
        "P Tovstogan",
        "A Porter",
        "X Serra"
      ],
      "year": "2019",
      "venue": "Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "19",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "20",
      "title": "Recognizing Song Mood and Theme: Clustering-based Ensembles",
      "authors": [
        "M Mayerl",
        "M Vötter",
        "A Peintner",
        "G Specht",
        "E Zangerle"
      ],
      "year": "2021",
      "venue": "MediaEval 2021 Workshop"
    },
    {
      "citation_id": "21",
      "title": "CLAP: Learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}