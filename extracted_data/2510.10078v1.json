{
  "paper_id": "2510.10078v1",
  "title": "Improving Speech Emotion Recognition With Mutual Information Regularized Generative Model",
  "published": "2025-10-11T07:29:32Z",
  "authors": [
    "Chung-Soo Ahn",
    "Rajib Rana",
    "Sunil Sivadas",
    "Carlos Busso",
    "Jagath C. Rajapakse"
  ],
  "keywords": [
    "data augmentation",
    "generative adversarial network",
    "mutual information",
    "speech emotion recognition",
    "multimodal deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although speech emotion recognition (SER) research has been advanced, thanks to deep learning methods, it still suffers from obtaining inputs from large quality-labelled training data. Data augmentation methods have been attempted to mitigate this issue, generative models have shown success among them recently. We propose a data augmentation framework that is aided by cross-modal information transfer and mutual information regularization. Mutual information based metric can serve as an indicator for the quality. Furthermore, we expand this data augmentation scope to multimodal inputs, thanks to mutual information ensureing dependency between modalities. Our framework was tested on three benchmark datasets: IEMOCAP, MSP-IMPROV and MSP-Podcast. The implementation was designed to generate input features that are fed into last layer for emotion classification. Our framework improved the performance of emotion prediction against existing works. Also, we discovered that our framework is able to generate new inputs without any cross-modal information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "S PEECH emotion recognition (SER) had significant im- provement in recent years thanks to application of deep learning architectures  [1] ,  [2] . Training of deep learning models rely heavily on high-quality labeled datasets, while typical dataset has smaller volume than counterparts in computer vision (ex: MNIST dataset). This scarcity triggers the issue of generalizability of SER models. In this context, data augmentation is promising as it can increase the volume of the dataset  [3] ,  [4] .\n\nThe simplest scheme of data augmentation is to copy a perturbed version of an original data sample. This perturbation can come in various forms, such a simple way as signal transformation, adding gaussian noise  [5]  or sophisticated way as mix-up  [6] . In contrast, generative models have attracted researchers as they can synthesize new data samples after being trained with the original data. Generative Adversarial Networks (GANs)  [7]  have emerged among other generative models  [3] ,  [4] , thanks to their ability to generate high-quality synthetic data. The key difference between two methodology is that perturbation merely replicates data with same information with additive noise, while generative models extract information from data and synthesize new samples from extracted information. Data augmentation methods, via generative models, involve conditioned generative models. As SER problems are mostly classification problems, class conditioned generative models are most common methods  [4] ,  [8] . Thus, crucial challenge is, how to reconstruct samples based on extracted information so that synthesized samples are similar as possible to real samples. In this paper, we hypothesize that conditional generative model naively assumes deterministic mapping of emotional labels to generated sample. However, such mapping is unrealistic, for this reason, the quality of generated samples cannot be ensured.\n\nMutual information regularization can be a sound alternative against conditional generative models  [9] ,  [10] . Mutual information can provide a quantitative measurement to observe the dependency between generated samples and class labels, which is a crucial requirement for data augmentation method. We train generative model to approximate data distribution, while regularizing with mutual information  [10] .\n\nMany generative data augmentation methods focused on generating samples while conditioning on emotional class labels. Recently, researchers started to incorporate information that are not sourced from audio, which is text information  [11] ,  [12] . With cross-modal information, it is natural to expand the scope to multimodal SER. Not limited to data augmentation, many recent research adopted the framework of cross-modal information transfer  [12] -  [15] . However, most of the work focused on supplementing or recovering noisy or missing modality input respectively. Multimodal augmentation framework to explicitly improve multimodal emotion recognition has not yet been explored thoroughly.\n\nIn this paper, we propose generative data augmentation   [16]  2020 Yi et al.  [8]  2020 Latif et al.  [4]  2020 Sahu et al.  [9]  2022 Wang et al.  [17]  2022 Latif et al.  [18]  2023 Malik et al. Model  [11]  2023 Kim et al.  [19]  2024 Goncalves et al.  [13]  2022 Chen et al.  [20]  2023 Wang et al.  [21]  2023 Meng et al.  [14]  2023 Wang et al.  [12]  2023 Liu et al.  [15]  2024 Ours 2025 framework for SER that exploits text features and mutual information. Our data augmentation method adopt Info-GAN  [10]  to generate audio features while regularizing GAN via mutual information (or penalizing independence). Mutual information regularizing module of InfoGAN is de facto encoder which outputs test feature from audio feature input, thus, our method can synthesize text feature as well. Therefore, we expanded our augmentation framework to multimodal inputs. With real and generated features for both audio and text, we trained audio-text fusion emotion classification layer with every four combinations for real and generated features. Through experiments on benchmark datasets, we observed the improvement at both SER and multimodal SER, thanks to our framework. Our contribution can be summarised as follows:\n\n1) We propose generative model based data augmentation framework, that is applicable for both SER and multimodal SER. 2) We introduce combining of cross-modal transfer and mutual information for SER data augmentation methodology. 3) We discovered that mutual information regularizer can offer observables to validate the dependency of generated data with emotion and text information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "The positioning of our contribution can be summarized into a table as in Table  6 . Our novelty lies on generative model for augmentation with deliberate combination of cross-modal transfer and mutual information. Furthermore, we expand it toward multimodal augmentation to improve multimodal emotion prediction. Generative adversarial learning has been widely employed in speech emotion recognition (SER) to leverage generated samples that are expected to convey emotional characteristics. In an early study by Latif et al.  [16] , an autoencoder-based framework was proposed, where the bottleneck representation (or encoder output) served as the generator, and the discriminator was trained to distinguish between prior and generated samples. This approach, commonly referred to as an adversarial autoencoder, provided a foundation for combining generative models with SER tasks. Similarly, Yi et al.  [8]  utilized an autoencoder-based approach but introduced a separate generator in their framework.\n\nIn another study, Latif et al.  [4]  integrated a generative adversarial network (GAN) with an autoencoder, utilizing datasets augmented via the mix-up method. Sahu et al.  [9]  advanced this approach by employing an autoencoder scheme along with the mutual information maximization principle, demonstrating that InfoGAN  [10]  could enable emotion-controllable synthesis in SER. Wang et al.  [17]  also used a GAN for data augmentation but addressed the issue of imbalanced class distributions. They incorporated triplet loss guidance to learn more robust emotional features.\n\nMore recently, Latif et al.  [18]  explored text-to-speech models as a data augmentation method, capitalizing on advancements that have simplified the training of end-toend text-to-speech systems using emotional speech datasets. Malik et al.  [11]  adopted diffusion models to generate melspectrograms from text embeddings, while Kim et al.  [19]  combined diffusion models with variational autoencoders (VAEs) to enhance data generation.\n\nMeanwhile, multimodal deep learning research has gained much attention recently  [22] ,  [23] , many works intended to strengthen the representation from one modality by using the input from other modalities, referred as colearning. Fusion of representation from different modalities can be considered as implict co-learning in broad sense, but in this work we focus where one modality is explicitly augmented from other modalities. In other words, we focus on cross-modal transfer or cross-modal alignment.\n\nSeveral recent works addressed to improve the robustness in the case where some modality inputs might be missing or noisy. Goncalves et al.  [13]  used multitask learning scheme to expose the model to the situation where only one modality input is available. In addition, typical crossmodal attention layer is employed to align the representation of both modalities. Wang et al.  [21]  also handled missing modality case by doing random modality masking of input. However, these approaches are implicit, neither gaining additional information from other modalities nor reconstructing missing modality is feasible.\n\nOn the other hand, aligning representation between different modalities can be a better approach to enforce information sharing between them. As briefly mention in previous paragraph, cross-modal attention  [13]  is the most common method, but rather focused on fusion than colearning. Alternatively, optimizing metrics between representations of different modality has been explored as in the work of Chen et al.  [20] . Recently, contrastive learning has gained much attention in this context as well. Liu et al.  [15]  used contrastive learning loss between different modalities to enforce modality-invariance. Yet, alignment approach does not enable reconstruction of missing modality.\n\nReconstruction approach is intuitive co-learning scheme, Meng et al.  [14]  trains multimodal autoencoder to generate reconstructed inputs. Also, Wang et al.  [12]  specifically assumes the modality missing scheme, that missing modality input is reconstructed via conditional diffusion model. Generative models, such as GAN or diffusion models, are special case of reconstruction methodology. However, generative models are expressive and versatile, as generative models extract rich information from the data to enable generation of noisy samples or learn mutual information between modalities for alignment.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Framework",
      "text": "Our framework is intended for the situation where the one, who has a decent SER model (and also a text emotion recognition model for multimodal SER case), wishes improve the performance by increasing the training data samples. In this chapter, we demonstrate the usage of our framework with an example: we train the generative model to generate features before the classification layer, rather than generating raw audio, to reduce the complexity of the experiment (visual summary is depicted in Fig  1 .\n\nFirst, we will prepare the decent SER model, pre-trained audio transformer is fine-tuned for benchmark SER dataset, simultaneously training with text feature (or embedding), which is an output from frozen text transformer, to be aligned with audio feature (or embedding) before linear classification layer. Second, we train InfoGAN to generate audio features extracted from the training dataset with mutual information regularizer. Finally, the audio features (and text features for multimodal SER case) and generated features are prepared and fed into final linear classification and trained with emotion classification loss, with larger number of data sample to enhance performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Preparation",
      "text": "We adopted two types of pre-trained audio transformers as the example: Audio Spectrogram Transformer (AST)  [24]  and Wav2Vec2  [25] . The audio transformer will get raw audio input after necessary feature extraction and output final hidden states, which is the audio feature that we intend to generate. A linear layer with softmax activation function is added. The output of this classification layer will be referred as prediction of our baseline model.\n\nThe audio feature before the classification layer is referred to as h:\n\nwhere f a denotes transformer that consists feature encoder and x a denoting audio input after the preprocessing function. The linear classifier from the encoded feature is a fullyconnected layer predicting emotional class c:\n\nThe weights W y and biases b y of the fully connected layers which can be further fine-tuned during InfoGAN training and augmented data fine tuning. Training objective is minimizing the cross-entropy loss L SER :\n\ny denotes the emotional label corresponding audio input x a , and E denotes the expectation.\n\nIn addition, we add cross-modal alignment module to baseline model. Let us define the text feature encoding module with text transformer to obtain text feature t as:\n\nwhere f t denote pre-trained transformer model and x t denoting textual input, which is taken from the ground truth transcript, provided from the training dataset. Inspired by CLAP  [26] , we exploit contrastive learning to align audio feature h and text feature t into same representation space:\n\nwhere i and j denotes the index within the minibatch. Furthermore, we maximize the mutual information between h and t, we employ contrastive learning to maximize mutual information as in  [27] , via InfoNCE loss. InfoNCE loss shares identical structure with L CL , however, the positive pair is formulated in a way that log ratio of conditional probability of p(t|h) to marginal probability p(t). Thus, we use another linear layer to project h to t and get InfoNCE loss (L M I ) as:\n\nIn result, we fine-tune the baseline model with training dataset simultaneously with all the loss terms above.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Infogan",
      "text": "In the second stage, we will employ InfoGAN that will generate ĥ from the implicitly estimated probability distribution function of h, while regularizing generator module with mutual information between ĥ with t and emotion labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Generative Adversarial Network (Gan)",
      "text": "GAN is commonly employed as a data augmentation to improve SER performances. Generation of synthetic data from GAN has sufficient quality to mimic emotion in original data, which is also explainable in theoretic analysis. Our GAN framework consists of the generator and the discriminator. The generator emits synthetic audio features (or InfoGAN, GAN with mutual information module that predicts latent c ′ and t ′ from generated ĥ, is trained and mutual information module re-uses the same prediction layers in the first stage. Third stage (bottom): we have two parallel stream in the final stage, SER case (bottom left) and multimodal SER case (bottom right), which is training the linear classification module with all possible input combinations by switching between h or ĥ and t or t ′ . linear classifier layer is fine tuned with generated samples. embedding) that mimic the original features extracted from raw speech. Generator is a network that takes random noise vectors (and other conditioning vectors if available) as input and outputs feature representations. On the other hand. the discriminator network is a binary classifier that discriminate between real and generated data. The generator and discriminator are trained with an objective that is described as minimax game, where they are adversarially optimized. We denote h as real data sampled from the data distribution, following probability distribution function (pdf) of p data (h). Also, z is a noise vector, sampled from pdf of p z (z). And discriminator and generator is depict as functions, D(h) and G(z). Finally, training objective function of GAN ( V (D, G)) is as follows:\n\nTraining consists two alternate phase, each phase is responsible in updating either D or G.\n\n1) Discriminator updating phase: D is optimized to maximize V (D, G). 2) Generator updating phase: G is optimized in opposite direction, minimizing V (D, G), identical to minimizing log(1-D(G(z))) as the other term is not affected by updating G. However, it is implemented to maximize log D(G(z)), for the sake of efficient gradient descent (refer to original paper for more details).\n\nThe training yields optimal discriminator as follows:\n\nWe denote p g as the distribution of output of G(z) sampled from z ∼ p z . Take note that, G(z) can be replaced with h, which denotes generated sample. Thus, the random variable h can be considered as a union of original data and synthesized data. Objective function can be re-arranged with optimal discriminator as follows.\n\nAbove derivation holds true under assumption that G is a deterministic mapping from z to h. Thus, following identity\n\nis used convert integration over z to integration over h. After rewriting objective function, it turns out that discriminator updating phase resembles noise constrastive estimation, thus, discriminator updating phase results in D to implicitly parametrizing original data distribution. Yet, hidden assumption has to be taken noted: pdf of generated sample is distinctive to pdf of original data samples.\n\nFinally, it is intuitive to follow that generator updating phase is, in effect, approximating p g to p data . If generator takes conditioning vectors other that z, the GAN training will be performed under assumption that there is a determistic mapping from conditioning vector (emotional labels or text embedding) as well. For this reason, there is no observable quantity to measure dependency between conditioning vector and generate sample, which is crucial measure to ensure quality of augmented data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Information Maximizing Generative Adversarial Network (Infogan)",
      "text": "InfoGAN is an extension of GAN proposed for controllable generation via exploitation of disentangled latent vector alongside with random noise vector.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1) Gan:",
      "text": "The original training objective of GAN is defined as:\n\n2) InfoGAN: Generator of InfoGAN requires two parts: z is the noise vector as in convention and c represents the latent code (or vector) that is expected to be disentangled and interpretable (we will use two latent variables c and t as defined in baseline model during experiment, but here we simply denote as c for sake of simplicity withing this subsection only). The objective function of Info-GAN does not modifiy existing objective but simply adds a mutual information term I(c; G(z, c)) with hyperparamter λ:\n\nMutual information is defined using entropy H as follows:\n\nHere, the first entropy term, H(c), can be ignored in optimization as we can sample latent vector c from simple distribution (ex: random noise) or we use text embedding which is fixed during training. In result, mutual information maximization is achieved by minimizing the second term, which is conditional entropy. As conditional entropy is nonnegative, minimum is 0, which implies that there exists mapping from generated sample (G(z, c)) to latent vector (c). From this intuition, we can train addition network, denoted as Q, to predict c when given G(z, c) as input. While, this is informal derivation, more formal description of this technic is variational mutual information maximization, which is maximizing the lower bound of mutual information with auxiliary distribution function Q (please refer to original paper for more information).\n\nInfoGAN training can be considered as vanilla GAN training while regularizing via mutual information. During training this regularization term can be computed to infer mutual information. The loss of vanilla GAN computed from either generator or discriminator does not indicate the quality of generated sample. On the other hand, the loss value from mutual information can be a direct indicator of generated sample.\n\nIn our method, we follow conventional training procedure to train GAN as in  [7] . But for mutual information loss, we will describe our definition of loss here. In conventional InfoGAN, mutual information loss is simply cross entropy loss of predicting c from generate sample of mean squared error loss to predict c from generated sample. This is a variational approximation to maximize mutual information as c are typically chosen to be a random variable with simple pdf that is known, so that we can easily sample. Our method has two latent variable c and t, which is sampled from the dataset, to generate sample ĥ = G(z, c, t). To maximize mutual information for c, as y follows simple categorical distribution, we use the same scheme of variational approximation. Thus, minimizing following terms achieves maximum mutual information:\n\nNote that the same weights from the baseline model is used to enforce latent variable to be coded with emotional information. If not, InfoGAN training objective will code arbitrary information of variations within the dataset (similar to clustering results). Similar approach is taken for latent variable t, but use InfoNCE loss instead of mean squared error loss, as we are optimizing embedding model rather than regression model.\n\nAgain, that the same weights from the baseline model is used to enforce latent variable to be coded with textual information. Furthermore, using the weights used in baseline model allows us to maximize mutual information not only with generated samples but also with real samples.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Augmentation For Ser",
      "text": "After InfoGAN is successfully trained, we can train emotion prediction model by training data samples, together with generated data samples that resembles distribution. As final stage, all modules, except emotion classification module, are frozen. The case for SER is rather straightforward. After extracting h from training dataset, we generate same number of ĥ, and then fed them through same emotion classification layer and compute loss to further fine-tune them. In effect, the emotion predictor has been trained with dataset that has size of double from original training dataset.\n\nFor the case of multimodal SER, h, ĥ, t and t ′ has to be prepared. And new classification layer that takes concatenated feature of audio feature and text feature has to be defined. Then we can compose inputs with all possible concatentation with original and generated features. Thus, we can achieve the data sample size increase by factor of four.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iemocap (Interactive Emotional Dyadic Motion Capture) Dataset",
      "text": "The IEMOCAP dataset is a commonly used dataset for SER research, which consists of approximately 12 hours of recordings of video with audio, featuring interactions between 5 pairs of male and female actors. These pairs of actors were set in dyadic sessions to engage in dialogues to elicit emotional expressions. Multimodal raw data was collected and we used speech recording and transcript for our experiments. Each recording was segmented per utterance and manually annotated by multiple human evaluators.\n\nOriginally, there are nine categorical emotional labels but only four major emotions were chosen, which are: neutral, happiness (merged with excited), sadness and anger.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Msp-Improv (Multimodal Signal Processing -Interactive Emotion Dyadic Motion Capture) Dataset",
      "text": "The MSP-IMPROV dataset is a multimodal dataset for research in SER, which share similar structures with IEMO-CAP dataset. The MSP-IMPROV dataset consists of approximately 9 hours of recordings 12 actors (6 male and 6 female) interacting with each other. Each recording is segmented into the unit of utterances, and annotated by multiple human evaluators. Annotation for categorical emotion labels yielded four basic emotion category: hapiness, sadness and neutral.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Msp-Podcast [28]",
      "text": "This corpus is created by collecting English speech data from existing podcast recording. Comparable to IEMOCAP and MSP-IMPROV, it is not collected from lab environment, thus, it is more close to 'in-the-wild' speech dataset. The dataset comes predefined partition of training, development and test set. MSP-PODCAST data comes with more emotion category than MSP-IMPROV. But to match with our experiment setting, we only used four categories: neutral, happiness, sadness and anger.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Augmentation For Ser",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Model",
      "text": "We adopt the pre-trained AST  [24]  from 'https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593' as backbone. And add linear classifier after pooled output from AST. For text feature encoder, pretrained BERT  [29] , that is 'bert-uncased-base' version. For SER experiment, we do not consider multimodal SER accuracy, thus, BERT is frozen and we only use it to extract text features from ground-truth transcript.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Infogan Architecture",
      "text": "We train InfoGAN to generate feature encoding, which is in the dimension of 768. So we take the simplest form of architecture for Discriminator and Generator which is a linear layer without any activation function. Also the network for mutual information maximization is also linear layer only. Especially, the linear layer that maximizes mutual information with y uses the same layer from emotion classifier. In this way, we can prevent latent code vector to encode something other than emotion, as there are many other complex variations inherent in the training dataset. Emotional code vector is trained with cross entropy to maximize the mutual information. For the latent embedding given from BERT embedding we use InfoNCE loss, which is a contrastive learning loss trying attract the BERT embedding and predicted text embedding from generated sample. Lastly we adopted mix-up strategy for training generator and discriminator as proposed in  [6] , for stable training of generator.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Result",
      "text": "The experiment was conducted in two rounds. First, our data augmentation framework was tested on SER case. In first round, we observe the performance difference before and after data augmentation, and ablation study to check the contributing effects of each modules in the model. Finally, we run the multimodal SER experiments. The second round, we compare the performance of the model using proposed framework with existing works.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ser Experiment",
      "text": "We experimented our method on commonly used dataset IEMOCAP. The experiment followed leave-one-speaker-out cross validation scheme. Unweighted average recall is reported per each speaker and we statistically average and present mean and standard deviation in the Table  6  as follows.\n\nThe results depict that our augmentation method achieves the state-of-the-art performance among existing methods. Furthermore, our augmentation method shows highest improvement against its baseline after augmentation, which is 2.6%, which is same amount of improvement with Malik et al.  [11] . Sahu et al. employed typical emotional label conditional GAN and Bao et al.  [3]  CycleGAN approach to generate additional data to train SER model. Latif et al.  [31]  achieved generalization improvement by using unlabelled dataset during training. Still, the improvement of accuracy against their baseline was marginal. Recently, the margin between baseline and data augmented SER was improved when data generation model was conditioned on text input as in Malik et al.  [11] . And our model also achieved same margin and state-of-the-art performance by using mutual information between generated data and text data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We performed ablation study to examine the effect of each modules we proposed. The results are depicted in Table  3 . First, we present the results when cross-modal alignment component(text embedding component and corresponding losses) ofbaseline model is removed. This result show that the cross-modal alignment module during baseline model training improves the quality of data augmentation. We hypothesize that the loss terms corresponding to cross-modal alignment enforces strong dependence between audio embedding and text embedding, providing better initial state to train InfoGAN. Next, we observed the contribution of text embedding and mutual information maximization loss in InfoGAN training. The result depict that both component has significant contribution in quality of data augmentation. Lastly, we experiment the case where we do not use data augmentation and baseline model is without cross-modal alignment, which is the SER performance on fine-tuning on vanilla AST. The result shows similar baseline performance as in Table  3 . This implies that cross-modal alignment helps improve the quality of generated samples from InfoGAN, not directly improving SER performances.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Ser Experiment",
      "text": "Multimodal SER experiment was performed on three datasets. This experiment is an use-case-scenario, where you are provided with two emotion prediction models, that one takes speech input and text input for the other. Through this experiment, we intend to show that even with simple data augmentation method via generative model, we improve the performances to the decent amount.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Iemocap",
      "text": "The experiment result revealed the advantage of our data augmentation method, in terms of accuracy. CRA  [34] , MMIN  [35]  and CIF-MMIN  [15]  exploit the dependency between features from different modality, thus achieving decent performance among other existing works. Inspired from these results, we adopted similar methodology, not to recover missing feature (modality) but to generate new multimodal feature sets.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Msp-Improv",
      "text": "We ran same experiment with MSP-IMPROV dataset. As IEMOCAP and MSP-IMPROV are datasets with similar characteristics, similar trend with IEMOCAP experiment result is observed.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Msp-Podcast",
      "text": "Unlike, IEMOCAP and MSP-IMPROV, MSP-Podcast is more naturalistic dataset with much larger number of samples. As there weren't any existing works that ran experiment with similar protocol, we compared our method with emoDARTS  [36] , which uses neural architecture search and only audio input. Our method performed up to similar level of success to emoDARTS. Compared to emoDARTS, our model can be simpler choice to offer during implementation. Notable point is, that MSP-Podcast has severe class imbalance problem, thus, we generated data samples for the lesser count of emotional classes to make same numbers of samples per emotion. This scheme of data generation is unlike the generation in existing works, as  [12] , which requires other modality features to transfer emotional information. In this case, we generated audio feature and text feature from scratch, with only emotional label provided. Still, out data augmentation method synthesized decent sample and we were able to mitigate the class-imbalance problem.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a new framework of data augmentation for speech emotion recognition (SER) using GAN. Unlike previous approaches, we employ the combination of mutual information regularization and cross-modality transfer (from text to be specific). The advantage of mutual information regularization is: mutual information regularizing loss can serve as an observable metric to depict the quality of generated data sample, in the form of dependency measure. Furthermore, we propose multimodal data augmentation using our framework. Previous works were limited in using cross-modal information transfer or cross-modal generative model for supplementing noisy of missing modality inputs. In our work, we propose to use our cross-modal generative model to generate more multimodal inputs by preparing combination of real inputs and generated inputs. We conducted experiments to test our framework in both, SER and multimodal SER cases. The framework was implemented to generate audio features or text features that is fed into final classification layer. Our method was evaluated on IEMOCAP, MSP-IMPROV and MSP-Podcast. Thanks to our method, we observed decent amount of improvement. We conclude, that when one has two separate decent models to predict emotion from audio and text, our framework can be implemented to generate more inputs further improve the performance in multimodal emotion recognition. Additionally, we discovered that with mutual information reglarization, one can generate more decent inputs without any information from other modality.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: First, we will prepare the decent SER model, pre-trained",
      "page": 3
    },
    {
      "caption": "Figure 1: The visual summary of proposed augmentation framework to improve SER, consisted of three stages. First stage (top left): baseline model",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 6: Our novelty lies on generative",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Summary of",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Title\nYear\nGenerative Augmentation\nCross-modal transfer",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Latif et al. [16]\n2020",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Yi et al. [8]\n2020",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Latif et al. [4]\n2020",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sahu et al. [9]\n2022",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Wang et al. [17]\n2022",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Latif et al. [18]\n2023",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Malik et al. Model [11]\n2023",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Kim et al. [19]\n2024",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Goncalves et al. [13]\n2022",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Chen et al. [20]\n2023",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Wang et al. [21]\n2023",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Meng et al. [14]\n2023",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Wang et al. [12]\n2023",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Liu et al. [15]\n2024",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ours\n2025",
          "2": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 6: Our novelty lies on generative",
      "data": [
        {
          "Wang et al. [12]\n2023": "Liu et al. [15]\n2024"
        },
        {
          "Wang et al. [12]\n2023": "Ours\n2025"
        },
        {
          "Wang et al. [12]\n2023": "framework for SER that exploits text\nfeatures and mutual"
        },
        {
          "Wang et al. [12]\n2023": "information. Our data augmentation method adopt\nInfo-"
        },
        {
          "Wang et al. [12]\n2023": "GAN [10]\nto generate\naudio\nfeatures while\nregularizing"
        },
        {
          "Wang et al. [12]\n2023": "GAN via mutual information (or penalizing independence)."
        },
        {
          "Wang et al. [12]\n2023": "Mutual\ninformation regularizing module of InfoGAN is de"
        },
        {
          "Wang et al. [12]\n2023": "facto encoder which outputs test feature from audio feature"
        },
        {
          "Wang et al. [12]\n2023": "input, thus, our method can synthesize text feature as well."
        },
        {
          "Wang et al. [12]\n2023": "Therefore, we expanded our augmentation framework to"
        },
        {
          "Wang et al. [12]\n2023": "multimodal\ninputs. With real\nand generated features\nfor"
        },
        {
          "Wang et al. [12]\n2023": "both audio and text, we trained audio-text\nfusion emotion"
        },
        {
          "Wang et al. [12]\n2023": "classification layer with every four\ncombinations\nfor\nreal"
        },
        {
          "Wang et al. [12]\n2023": "and generated features. Through experiments on bench-"
        },
        {
          "Wang et al. [12]\n2023": "mark datasets, we observed the improvement at both SER"
        },
        {
          "Wang et al. [12]\n2023": "and multimodal SER, thanks to our framework."
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "Our contribution can be summarised as follows:"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "1)\nWe propose generative model based data augmenta-"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "tion framework, that is applicable for both SER and"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "multimodal SER."
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "2)\nWe\nintroduce\ncombining\nof\ncross-modal\ntransfer"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "and mutual information for SER data augmentation"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "methodology."
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "3)\nWe discovered that mutual\ninformation regularizer"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "can offer observables to validate the dependency of"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "generated data with emotion and text information."
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "2\nRELATED WORKS"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "The positioning of our\ncontribution can be\nsummarized"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "into a table as\nin Table 6. Our novelty lies on generative"
        },
        {
          "Wang et al. [12]\n2023": ""
        },
        {
          "Wang et al. [12]\n2023": "model\nfor\naugmentation with deliberate\ncombination of"
        },
        {
          "Wang et al. [12]\n2023": "cross-modal transfer and mutual information. Furthermore,"
        },
        {
          "Wang et al. [12]\n2023": "we expand it toward multimodal augmentation to improve"
        },
        {
          "Wang et al. [12]\n2023": "multimodal emotion prediction."
        },
        {
          "Wang et al. [12]\n2023": "Generative\nadversarial\nlearning has been widely em-"
        },
        {
          "Wang et al. [12]\n2023": "ployed in speech emotion recognition (SER)\nto leverage"
        },
        {
          "Wang et al. [12]\n2023": "generated samples\nthat are expected to convey emotional"
        },
        {
          "Wang et al. [12]\n2023": "characteristics.\nIn an early study by Latif\net\nal.\n[16],\nan"
        },
        {
          "Wang et al. [12]\n2023": "autoencoder-based framework was proposed, where\nthe"
        },
        {
          "Wang et al. [12]\n2023": "bottleneck representation (or encoder output) served as the"
        },
        {
          "Wang et al. [12]\n2023": "generator, and the discriminator was trained to distinguish"
        },
        {
          "Wang et al. [12]\n2023": "between prior and generated samples. This approach, com-"
        },
        {
          "Wang et al. [12]\n2023": "monly referred to as an adversarial autoencoder, provided"
        },
        {
          "Wang et al. [12]\n2023": "a foundation for\ncombining generative models with SER"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information sharing between them. As briefly mention in",
          "3": "where fa denotes transformer that consists feature encoder"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "previous paragraph, cross-modal attention [13]\nis the most",
          "3": "and xa denoting audio input after the preprocessing func-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "common method, but\nrather\nfocused on fusion than co-",
          "3": "tion. The linear classifier from the encoded feature is a fully-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning. Alternatively, optimizing metrics between repre-",
          "3": "connected layer predicting emotional class c:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentations of different modality has been explored as in the",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "(2)\ny = softmax(Wyh + by)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "work of Chen et al.\n[20]. Recently, contrastive learning has",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gained much attention in this context as well. Liu et al. [15]",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "The weights Wy and biases by of the fully connected layers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "used contrastive learning loss between different modalities",
          "3": "which can be further fine-tuned during InfoGAN training"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to\nenforce modality-invariance. Yet,\nalignment\napproach",
          "3": "and augmented data fine tuning. Training objective is mini-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "does not enable reconstruction of missing modality.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "mizing the cross-entropy loss LSER:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Reconstruction approach is intuitive co-learning scheme,",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "1(y = c)log(ˆy)}.\n(3)\nLSER = −E{"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Meng et al. [14] trains multimodal autoencoder to generate",
          "3": "(cid:88) c"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reconstructed inputs. Also, Wang\net\nal.\n[12]\nspecifically",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "assumes the modality missing scheme, that missing modal-",
          "3": "y denotes the emotional label corresponding audio input xa,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ity input\nis reconstructed via conditional diffusion model.",
          "3": "and E denotes the expectation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Generative models, such as GAN or diffusion models, are",
          "3": "In addition, we add cross-modal alignment module to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "special case of reconstruction methodology. However, gen-",
          "3": "baseline model. Let us define\nthe\ntext\nfeature\nencoding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "erative models are expressive and versatile, as generative",
          "3": "module with text transformer to obtain text feature t as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models\nextract\nrich information from the data\nto enable",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "(4)\nt = ft(xt)."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "generation of noisy samples or\nlearn mutual\ninformation",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "between modalities for alignment.",
          "3": "where\ndenote pre-trained transformer model\nft\nand xt"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "denoting textual input, which is taken from the ground truth"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "transcript, provided from the training dataset."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3\nPROPOSED FRAMEWORK",
          "3": "Inspired by CLAP [26], we exploit contrastive learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "to align audio feature h and text\nfeature t into same repre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Our\nframework\nis\nintended for\nthe\nsituation where\nthe",
          "3": "sentation space:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "one, who has a decent SER model\n(and also a text emo-",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "exp (cid:0)sim(ti, hi)/τ (cid:1)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion recognition model\nfor multimodal SER case), wishes",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": ",\n(5)\nLCL = −log"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "(cid:80)B"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "improve the performance by increasing the training data",
          "3": "j=1 exp (sim(ti, hj)/τ )"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "samples.\nIn this chapter, we demonstrate the usage of our",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "where i and j denotes the index within the minibatch."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "framework with an example: we train the generative model",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "Furthermore, we maximize the mutual\ninformation be-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to generate\nfeatures before\nthe\nclassification layer,\nrather",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tween h and t, we employ contrastive learning to maximize"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "than generating raw audio, to reduce the complexity of the",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "mutual\ninformation as in [27], via InfoNCE loss.\nInfoNCE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "experiment (visual summary is depicted in Fig 1.",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "loss shares identical structure with LCL, however, the posi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "First, we will prepare the decent SER model, pre-trained",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "tive pair is formulated in a way that log ratio of conditional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "audio transformer is fine-tuned for benchmark SER dataset,",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "probability of p(t|h) to marginal probability p(t). Thus, we"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "simultaneously training with text\nfeature (or embedding),",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "use another linear layer to project h to ˆt and get\nInfoNCE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which is\nan output\nfrom frozen text\ntransformer,\nto\nbe",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "loss (LM I ) as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "aligned with audio feature\n(or\nembedding) before\nlinear",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "classification layer. Second, we train InfoGAN to generate",
          "3": "(6)\nt = Wth + bt"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "audio\nfeatures\nextracted from the\ntraining dataset with",
          "3": "(cid:16)\n(cid:17)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "exp\nsim(ti, ˆhi)/τ"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mutual\ninformation regularizer. Finally,\nthe audio features",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "(7)\nLM I = −log\n(cid:16)\n(cid:17) ."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "(cid:80)B"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(and text\nfeatures for multimodal SER case) and generated",
          "3": "sim(ti, ˆhj)/τ"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "3": "j=1 exp"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "features are prepared and fed into final\nlinear classification",
          "3": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and trained with emotion classification loss, with larger",
          "3": "In result, we fine-tune the baseline model with training"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "number of data sample to enhance performance.",
          "3": "dataset simultaneously with all the loss terms above."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "is trained with contrastive loss and InfoNCE loss. Second stage (top right):\nInfoGAN, GAN with mutual\ninformation module that predicts latent c′"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "and t′\nfrom generated ˆh,\nis trained and mutual\ninformation module re-uses the same prediction layers in the first stage. Third stage (bottom): we"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "have two parallel stream in the final stage, SER case (bottom left) and multimodal SER case (bottom right), which is training the linear classification"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "module with all possible input combinations by switching between h or ˆh and t or t′.\nlinear classifier layer is fine tuned with generated samples."
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "discriminator and generator is depict as functions, D(h) and\nembedding) that mimic the original features extracted from"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "G(z). Finally, training objective function of GAN ( V (D, G))\nraw speech. Generator is a network that takes random noise"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "is as follows:\nvectors (and other conditioning vectors if available) as input"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "and outputs feature representations. On the other hand. the"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "discriminator network is a binary classifier\nthat discrimi-"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "min\nmax\nV (D, G) ="
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "nate between real and generated data. The generator and"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "G\nD"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "discriminator are trained with an objective that is described"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "Eh∼pdata(h)[log D(h)] + Ez∼pz(z)[log(1 − D(G(z)))]."
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "as minimax game, where they are adversarially optimized."
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "(8)"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "We denote h as real data sampled from the data distribution,"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "following probability distribution function (pdf) of pdata(h).\nTraining\nconsists\ntwo\nalternate phase,\neach phase\nis"
        },
        {
          "Fig. 1. The visual summary of proposed augmentation framework to improve SER, consisted of\nthree stages. First stage (top left): baseline model": "Also, z is a noise vector, sampled from pdf of pz(z). And\nresponsible in updating either D or G."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1)\nDiscriminator updating phase: D is optimized to\n2)\nInfoGAN: Generator\nof\nInfoGAN requires\ntwo"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "maximize V (D, G).\nparts: z\nis\nthe noise vector as\nin convention and"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2)\nGenerator updating phase: G is optimized in op-\nc represents\nthe latent code (or vector)\nthat\nis ex-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "posite direction, minimizing V (D, G),\nidentical\nto\npected to\nbe disentangled and interpretable\n(we"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "c\nminimizing log(1−D(G(z))) as the other term is not\nwill use\ntwo latent variables\nand t\nas defined"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "affected by updating G. However, it is implemented\nin baseline model during experiment, but here we"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to maximize log D(G(z)),\nfor\nthe sake of efficient\nsimply denote as c for\nsake of\nsimplicity withing"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gradient descent\n(refer\nto original paper\nfor more\nthis subsection only). The objective function of Info-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "details).\nGAN does not modifiy existing objective but simply"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "adds a mutual\ninformation term I(c; G(z, c)) with"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The training yields optimal discriminator as follows:"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hyperparamter λ:"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pdata(h)"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D∗(h) =\n.\n(9)"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pdata(h) + pg(h)\nmin\nmax\nV (D, G) − λI(c; G(z, c))."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(13)"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "G,I\nD"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "We denote\nas\nthe distribution of\noutput\nof G(z)\npg"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sampled from z ∼ pz. Take note that, G(z) can be replaced"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Mutual\ninformation is defined using entropy H as"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with h, which denotes generated sample. Thus, the random"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "follows:"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "variable h can be considered as a union of original data"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and synthesized data. Objective function can be re-arranged"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "I(c; G(z, c)) = H(c) − H(c|G(z, c)).\nwith optimal discriminator as follows.\n(14)"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Here,\nthe first entropy term, H(c), can be ignored\nV (G, D)"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(cid:90)\n(cid:90)\nc\nin optimization as we can sample latent vector"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "=\npz(z) log(1 − D(g(z)))dz\npdata(h) log(D(h))dh +"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from simple distribution (ex: random noise) or we"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "h\nz"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "use\ntext\nembedding which is fixed during train-\n(cid:90)"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "=\n(10)\npdata(h) log(D(h)) + pg(h) log(1 − D(h))dh.\ning.\nIn result, mutual\ninformation maximization is"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "h"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "achieved by minimizing the second term, which is"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Above derivation holds true under assumption that G\nconditional entropy. As conditional entropy is non-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is a deterministic mapping from z\nto h. Thus,\nfollowing\nnegative, minimum is 0, which implies\nthat\nthere"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "identity\nexists mapping\nfrom generated sample\n(G(z, c))"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(11)\npg(h|z) = δ(h − G(z)),\nto\nlatent\nvector\n(c).\nFrom this\nintuition, we\ncan"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "train addition network, denoted as Q,\nto predict c"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is used convert integration over z to integration over h."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "when given G(z, c) as input. While, this is informal"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "After rewriting objective function,\nit\nturns out\nthat dis-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "derivation, more\nformal description of\nthis\ntech-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "criminator updating phase resembles noise constrastive es-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nic is variational mutual information maximization,"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "timation,\nthus, discriminator updating phase results\nin D"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which is maximizing the lower bound of mutual"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to implicitly parametrizing original data distribution. Yet,"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information with auxiliary distribution function Q"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hidden assumption has to be taken noted: pdf of generated"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(please refer to original paper for more information)."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sample is distinctive to pdf of original data samples."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Finally,\nit\nis intuitive to follow that generator updating"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "InfoGAN training can be\nconsidered as vanilla GAN"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "phase is,\nIf generator\nin effect, approximating pg\nto pdata."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "training while regularizing via mutual\ninformation. During"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "takes\nconditioning vectors\nother\nthat\nz,\nthe GAN train-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "training this regularization term can be computed to infer"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing will be performed under\nassumption that\nthere\nis\na"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mutual\ninformation. The\nloss of vanilla GAN computed"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "determistic mapping from conditioning vector\n(emotional"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from either generator or discriminator does not indicate the"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "labels or\ntext\nembedding) as well. For\nthis\nreason,\nthere"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "quality of generated sample. On the other hand,\nthe loss"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is no observable quantity to measure dependency between"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "value from mutual\ninformation can be a direct\nindicator of"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conditioning vector and generate sample, which is crucial"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "generated sample."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "measure to ensure quality of augmented data."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In our method, we follow conventional\ntraining proce-"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3.2.2\nInformation Maximizing Generative Adversarial Net-\ndure to train GAN as in [7]. But for mutual information loss,"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "work (InfoGAN)\nwe will describe our definition of loss here. In conventional"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "InfoGAN, mutual\ninformation loss is simply cross entropy"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "InfoGAN is an extension of GAN proposed for controllable"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "loss of predicting c from generate sample of mean squared"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "generation via\nexploitation of disentangled latent vector"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "c\nerror\nloss\nto predict\nfrom generated sample. This\nis\na"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "alongside with random noise vector."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "variational approximation to maximize mutual\ninformation"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1)\nGAN: The original\ntraining objective of GAN is\nc\nas\nare\ntypically chosen to be\na\nrandom variable with"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "defined as:\nsimple pdf that is known, so that we can easily sample. Our"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "method has two latent variable c and t, which is sampled"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "min\nmax\nV (D, G) ="
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "G\nD\nfrom the dataset,\nto generate\nsample\nh = G(z, c, t). To"
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "maximize mutual information for c, as y follows simple cat-\nEh∼pdata(h)[log D(h)] + Ez∼pz(z)[log(1 − D(G(z)))]."
        },
        {
          "5\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "egorical distribution, we use the same scheme of variational\n(12)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "approximation. Thus, minimizing following terms achieves",
          "6": "Originally,\nthere are nine categorical emotional\nlabels but"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "maximum mutual information:",
          "6": "only four major emotions were chosen, which are: neutral,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "happiness (merged with excited), sadness and anger."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(15)\nyg = softmax(Wy\nh + by),",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(16)\n1(y = c)log( ˆyg)}.\nLIy = −E{",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(cid:88) c",
          "6": "4.1.2\nMSP-IMPROV (Multimodal Signal Processing - Inter-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "active Emotion Dyadic Motion Capture) dataset"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Note\nthat\nthe\nsame weights\nfrom the baseline model\nis",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "The MSP-IMPROV dataset\nis a multimodal dataset\nfor re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "used to enforce latent variable to be coded with emotional",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "search in SER, which share similar structures with IEMO-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information. If not, InfoGAN training objective will code ar-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "CAP dataset. The MSP-IMPROV dataset consists of approxi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "bitrary information of variations within the dataset (similar",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "mately 9 hours of recordings 12 actors (6 male and 6 female)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to clustering results). Similar approach is\ntaken for\nlatent",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "interacting with each other. Each recording is\nsegmented"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "variable t, but use InfoNCE loss instead of mean squared",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "into the unit of utterances, and annotated by multiple hu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "error\nloss, as we are optimizing embedding model\nrather",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "man evaluators. Annotation for categorical emotion labels"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "than regression model.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "yielded four basic emotion category: hapiness, sadness and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ",
          "6": "neutral."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(17)\ntg = Wt\nh + bt",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(cid:16)\n(cid:17)\ni",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "exp\n)/τ\nsim(ti, ˆtg",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "4.1.3\nMSP-Podcast [28]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(18)\nLIt = −log\n(cid:16)\n(cid:17) .\nj",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(cid:80)B",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ")/τ\nsim(ti, ˆtg",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "j=1 exp",
          "6": "This\ncorpus\nis\ncreated by collecting English speech data"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "from existing podcast recording. Comparable to IEMOCAP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Again,\nthat\nthe same weights\nfrom the baseline model\nis",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "and MSP-IMPROV, it is not collected from lab environment,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "used to enforce latent variable to be coded with textual\nin-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "thus,\nit\nis more close to ’in-the-wild’\nspeech dataset. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formation. Furthermore, using the weights used in baseline",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "dataset comes predefined partition of training, development"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "model allows us to maximize mutual\ninformation not only",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "and test set. MSP-PODCAST data comes with more emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "with generated samples but also with real samples.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "tion category than MSP-IMPROV. But\nto match with our"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "experiment setting, we only used four categories: neutral,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3.3\nData augmentation for SER",
          "6": "happiness, sadness and anger."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "After InfoGAN is successfully trained, we can train emotion",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "prediction model by training data samples,\ntogether with",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "4.2\nData augmentation for SER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "generated data samples\nthat\nresembles\ntheir distribution.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "As final\nstage, all modules,\nexcept\nemotion classification",
          "6": "4.2.1\nBaseline model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module, are frozen.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "We\nadopt\nthe\npre-trained\nAST\n[24]\nfrom"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The case for SER is rather straightforward. After extract-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "‘https://huggingface.co/MIT/ast-finetuned-audioset-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing h from training dataset, we generate same number of",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "10-10-0.4593’ as backbone. And add linear\nclassifier after"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ˆ",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "h, and then fed them through same emotion classification",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "pooled output\nfrom AST.\nFor\ntext\nfeature\nencoder, pre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "layer and compute loss to further fine-tune them. In effect,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "trained BERT [29],\nthat\nis ’bert-uncased-base’ version. For"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the emotion predictor has been trained with dataset that has",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "SER\nexperiment, we\ndo\nnot\nconsider multimodal\nSER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "size of double from original training dataset.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "accuracy, thus, BERT is frozen and we only use it to extract"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "For\nthe case of multimodal SER, h, ˆh,\nt and t′ has\nto",
          "6": "text features from ground-truth transcript."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "be prepared. And new classification layer\nthat\ntakes con-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "catenated feature of audio feature and text\nfeature has\nto",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "be defined. Then we can compose inputs with all possible",
          "6": "4.3\nInfoGAN architecture"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "concatentation with original and generated features. Thus,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "We train InfoGAN to generate feature encoding, which is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "we can achieve the data sample size increase by factor of",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "in the dimension of\n768.\nSo we\ntake\nthe\nsimplest\nform"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "four.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "of architecture\nfor Discriminator and Generator which is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "a\nlinear\nlayer without\nany activation function. Also\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "network for mutual information maximization is also linear"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4\nEXPERIMENT",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "layer only. Especially,\nthe linear layer that maximizes mu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4.1\nDataset",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "tual\ninformation with y uses the same layer from emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4.1.1\nIEMOCAP (Interactive Emotional Dyadic Motion Cap-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "classifier.\nIn this way, we can prevent\nlatent code vector to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ture) dataset",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "encode something other\nthan emotion, as\nthere are many"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The\nIEMOCAP dataset\nis\na\ncommonly used dataset\nfor",
          "6": "other\ncomplex variations\ninherent\nin the training dataset."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "SER research, which consists of\napproximately 12 hours",
          "6": "Emotional\ncode\nvector\nis\ntrained with cross\nentropy\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nrecordings of video with audio,\nfeaturing interactions",
          "6": "maximize the mutual information. For the latent embedding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "between 5 pairs of male and female actors. These pairs of",
          "6": "given from BERT embedding we use InfoNCE loss, which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "actors were set in dyadic sessions to engage in dialogues to",
          "6": "is a contrastive learning loss trying attract the BERT embed-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "elicit emotional expressions. Multimodal raw data was col-",
          "6": "ding and predicted text embedding from generated sample."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lected and we used speech recording and transcript for our",
          "6": "Lastly we adopted mix-up strategy for\ntraining generator"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "experiments. Each recording was segmented per utterance",
          "6": "and discriminator as proposed in [6],\nfor stable training of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and manually annotated by multiple human evaluators.",
          "6": "generator."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: as asinTable3.Thisimpliesthatcross-modalalignmenthelps",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TABLE 2",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Performance comparison with existing data augmentation methods on",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEMOCAP dataset",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Methods\nWithout augmentation\nWith augmentation",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Sahu et al. [9]\n59.42\n60.29",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Bao et al. [3]\n59.48±0.71\n60.37±0.70",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Latif et al. [31]\n60.51±0.57\n61.05±0.68",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Malik et al. [11]\n58.62±2.11\n61.22±1.85",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ours\n60.81±4.83\n63.40±2.52",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: as asinTable3.Thisimpliesthatcross-modalalignmenthelps",
      "data": [
        {
          "Malik et al. [11]": "Ours",
          "58.62±2.11": "60.81±4.83",
          "61.22±1.85": "63.40±2.52"
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "4.4",
          "58.62±2.11": "Data augmentation for multimodal SER",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "We\nadopt",
          "58.62±2.11": "the\npre-trained",
          "61.22±1.85": "Wav2Vec2\n[30]"
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "‘https://huggingface.co/audeering/wav2vec2-large-",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "robust-12-ft-emotion-msp-dim’",
          "58.62±2.11": "for",
          "61.22±1.85": "multimodal"
        },
        {
          "Malik et al. [11]": "experiment. For text feature encoder, pre-trained RoBERTA",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "[29], that is ’roberta-uncased-large’ version. Fof multimodal",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "SER experiment, RoBERTa is separately trained to predict",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "",
          "58.62±2.11": "",
          "61.22±1.85": ""
        },
        {
          "Malik et al. [11]": "emotion labels",
          "58.62±2.11": "from transcript before",
          "61.22±1.85": "the baseline"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TABLE 5",
          "8": "text\nto be specific). The advantage of mutual\ninformation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Multimodal SER experiments on MSP-IMPROV dataset",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "regularization is: mutual\ninformation regularizing loss can"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "serve as an observable metric to depict\nthe quality of gen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Unweighted Accuracy",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Model",
          "8": "erated data sample,\nin the form of dependency measure."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Audio\nText\nAudio + Text",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "Furthermore, we propose multimodal data augmentation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sc_LSTM [32]\n40.91\n32.01\n41.36",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "bc_LSTM [32]\n42.53\n57.39\n58.32",
          "8": "using our framework. Previous works were limited in using"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "AE [33]\n42.69\n27.62\n38.73",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "cross-modal\ninformation transfer or cross-modal generative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "CRA [34]\n38.96\n28.37\n37.97",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "model for supplementing noisy of missing modality inputs."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MMIN [35]\n42.71\n56.49\n60.98",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "CIF-MMIN [15]\n41.56\n58.57\n61.72",
          "8": "In our work, we propose to use our cross-modal generative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ours\n57.85\n40.29\n62.84",
          "8": "model\nto generate more multimodal\ninputs by preparing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "combination of\nreal\ninputs and generated inputs. We con-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "ducted experiments to test our framework in both, SER and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "multimodal SER cases. The framework was\nimplemented"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "to generate audio features or\ntext\nfeatures that\nis fed into"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TABLE 6",
          "8": "final\nclassification layer. Our method was\nevaluated on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Multimodal SER experiments on MSP-Podcast dataset",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "IEMOCAP, MSP-IMPROV and MSP-Podcast. Thanks to our"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "method, we observed decent amount of\nimprovement. We"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Unweighted Accuracy",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Model",
          "8": "conclude,\nthat when one has two separate decent models"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Audio\nText\nAudio + Text",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "to predict\nemotion from audio and text, our\nframework"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emoDARTS [36]\n61.15\n-\n-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Ours\n56.76\n49.01\n60.61",
          "8": "can be implemented to generate more inputs\nfurther\nim-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "prove the performance in multimodal emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "Additionally, we discovered that with mutual\ninformation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "reglarization, one can generate more decent inputs without"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "any information from other modality."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "decent performance among other existing works.\nInspired",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from these results, we adopted similar methodology, not",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "REFERENCES"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to recover missing feature (modality) but\nto generate new",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal feature sets.",
          "8": "[1]\nB. W. Schuller, “Speech emotion recognition: Two decades\nin a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "nutshell, benchmarks, and ongoing trends,” Communications of the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "ACM, vol. 61, no. 5, pp. 90–99, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "5.2.2\nMSP-IMPROV",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[2]\nS. Latif, R. Rana, S. Khalifa, R.\nJurdak,\nJ. Qadir, and B. Schuller,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "We ran same experiment with MSP-IMPROV dataset. As",
          "8": "“Survey of deep representation learning for speech emotion recog-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEMOCAP and MSP-IMPROV are datasets with\nsimilar",
          "8": "nition,” IEEE Transactions on Affective Computing, vol. 14, no. 2, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "1634–1654, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "characteristics,\nsimilar\ntrend with IEMOCAP experiment",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[3]\nF. Bao, M. Neumann, and N. T. Vu, “Cyclegan-based emotion style"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "result is observed.",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "transfer as data augmentation for speech emotion recognition.” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "Interspeech, 2019, pp. 2828–2832."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "5.2.3\nMSP-Podcast",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[4]\nS. Latif, M. Asim, R. Rana, S. Khalifa, R. Jurdak, and B. W. Schuller,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "“Augmenting generative adversarial networks for speech emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Unlike, IEMOCAP and MSP-IMPROV, MSP-Podcast is more",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "recognition,” arXiv preprint arXiv:2005.08447, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "naturalistic dataset with much larger number of samples. As",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[5]\nS. Parthasarathy and C. Busso, “Semi-supervised speech emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "there weren’t any existing works that ran experiment with",
          "8": "recognition with ladder networks,” IEEE/ACM transactions on au-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "similar protocol, we compared our method with emoDARTS",
          "8": "dio, speech, and language processing, vol. 28, pp. 2697–2709, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[6]\nH.\nZhang, M.\nCisse,\nY.\nN.\nDauphin,\nand\nD.\nLopez-Paz,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[36], which uses neural architecture search and only audio",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "arXiv\npreprint\n“mixup:\nBeyond\nempirical\nrisk minimization,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "input. Our method performed up to similar level of success",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "arXiv:1710.09412, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to emoDARTS. Compared to emoDARTS, our model can be",
          "8": "[7]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "simpler choice to offer during implementation.",
          "8": "S. Ozair, A. Courville,\nand Y. Bengio,\n“Generative\nadversarial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "in neural\ninformation processing\nnets,” Advances\nsystems, vol.\n27,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Notable point\nis,\nthat MSP-Podcast has severe class im-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "2014."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "balance problem,\nthus, we generated data samples for the",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[8]\nL. Yi and M.-W. Mak, “Improving speech emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lesser count of emotional classes to make same numbers of",
          "8": "with adversarial data augmentation network,” IEEE transactions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "on neural networks and learning systems, vol. 33, no. 1, pp. 172–184,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "samples per emotion. This scheme of data generation is un-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "like the generation in existing works, as [12], which requires",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[9]\nS. Sahu, R. Gupta, and C. Espy-Wilson, “Modeling feature repre-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "other modality features to transfer emotional\ninformation.",
          "8": "sentations\nfor affective speech using generative adversarial net-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In this\ncase, we generated audio feature and text\nfeature",
          "8": "works,” IEEE Transactions on Affective Computing, vol. 13, no. 2, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "1098–1110, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "from scratch, with only emotional\nlabel provided. Still, out",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[10] X. Chen, Y. Duan, R. Houthooft,\nJ. Schulman,\nI. Sutskever, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "data augmentation method synthesized decent sample and",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "P\n. Abbeel, “Infogan:\nInterpretable representation learning by in-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "we were able to mitigate the class-imbalance problem.",
          "8": "in\nformation maximizing generative adversarial nets,” Advances"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "neural\ninformation processing systems, vol. 29, 2016."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[11]\nI. Malik, S. Latif, R.\nJurdak, and B. W. Schuller, “A preliminary"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "6\nCONCLUSION",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "study on augmenting speech emotion recognition using a dif-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "Interspeech, Dublin,\nIreland, August,\nfusion model,” Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "We proposed a new framework of data augmentation for",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "2023, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech emotion recognition (SER) using GAN. Unlike pre-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "[12] Y. Wang, Y. Li, and Z. Cui, “Incomplete multimodality-diffused"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vious approaches, we employ the combination of mutual in-",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": "emotion recognition,” Advances"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "formation regularization and cross-modality transfer (from",
          "8": "Systems, vol. 36, 2024."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[13] L. Goncalves and C. Busso, “Robust audiovisual emotion recog-",
          "9": "[34] L. Tran, X. Liu, J. Zhou, and R. Jin, “Missing modalities imputation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nition: Aligning modalities, capturing temporal\ninformation, and",
          "9": "of\nthe\nIEEE\nvia\ncascaded residual\nautoencoder,”\nin Proceedings"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "handling missing features,” IEEE Transactions on Affective Comput-",
          "9": "conference on computer vision and pattern recognition, 2017, pp. 1405–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing, vol. 13, no. 4, pp. 2156–2170, 2022.",
          "9": "1414."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[14] T. Meng, Y. Shou, W. Ai, N. Yin, and K. Li, “Deep imbalanced",
          "9": "[35]\nJ. Zhao, R. Li, and Q. Jin, “Missing modality imagination network"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning for multimodal emotion recognition in conversations,”",
          "9": "for emotion recognition with uncertain missing modalities,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on Artificial Intelligence, 2024.",
          "9": "Proceedings of\nthe 59th Annual Meeting of\nthe Association for Com-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[15] R. Liu, H. Zuo, Z. Lian, B. W. Schuller, and H. Li, “Contrastive",
          "9": "putational Linguistics and the 11th International\nJoint Conference on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning based modality-invariant\nfeature acquisition for\nrobust",
          "9": "Natural Language Processing (Volume 1: Long Papers), 2021, pp. 2608–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal emotion recognition with missing modalities,” IEEE",
          "9": "2618."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Affective Computing, 2024.",
          "9": "[36] T. Rajapakshe, R. Rana, S. Khalifa, B. Sisman, B. W. Schuller, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "C. Busso, “emodarts: Joint optimisation of cnn & sequential neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[16]\nS. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Epps, and B. W. Schuller,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "network architectures for superior speech emotion recognition,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Multi-task semi-supervised adversarial autoencoding for speech",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "9": "IEEE Access, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions\non Affective\nemotion recognition,”\ncomputing,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 13, no. 2, pp. 992–1004, 2020.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[17]\nS. Wang, H. Hemati,\nJ. Guðnason,\nand D. Borth,\n“Generative",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "data\naugmentation guided by\ntriplet\nloss\nfor\nspeech emotion",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” in Interspeech, 2022.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[18]\nS. Latif, A. Shahid, and J. Qadir, “Generative\nemotional ai\nfor",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech emotion recognition: The\ncase\nfor\nsynthetic\nemotional",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech augmentation,” Applied Acoustics, vol. 210, p. 109425, 2023.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] Y.-J. Kim and S.-P. Lee, “A generation of enhanced data by varia-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tional autoencoders and diffusion modeling,” Electronics, vol. 13,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 7, p. 1314, 2024.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20] C. Chen, H. Hong,\nJ. Guo, and B. Song, “Inter-intra modal\nrep-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "resentation augmentation with trimodal\ncollaborative disentan-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "glement network for multimodal sentiment analysis,” IEEE/ACM",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Audio, Speech, and Language Processing, vol. 31, pp.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1476–1488, 2023.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21]\nS. Wang, Y. Ma, and Y. Ding, “Exploring complementary features",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in multi-modal\nspeech emotion recognition,”\nin ICASSP 2023-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2023 IEEE International Conference on Acoustics, Speech and Signal",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Processing (ICASSP).\nIEEE, 2023, pp. 1–5.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22] T. Baltrušaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on pattern\nlearning: A survey and taxonomy,” IEEE transactions",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "analysis and machine intelligence, vol. 41, no. 2, pp. 423–443, 2018.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23] P. P. Liang, A. Zadeh, and L.-P. Morency, “Foundations & trends",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in multimodal machine learning: Principles, challenges, and open",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "questions,” ACM Computing Surveys, vol. 56, no. 10, pp. 1–42, 2024.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] Y. Gong, Y.-A. Chung, and J. Glass, “AST: Audio Spectrogram",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transformer,” in Proc. Interspeech 2021, 2021, pp. 571–575.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A framework for\nself-supervised learning of\nspeech representa-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in neural\ntions,” Advances\ninformation processing systems, vol. 33,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 12 449–12 460, 2020.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[26] B. Elizalde, S. Deshmukh, M. Al\nIsmail,\nand H. Wang,\n“Clap",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "learning audio concepts from natural\nlanguage supervision,” in",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and Signal Processing (ICASSP).\nIEEE, 2023, pp. 1–5.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[27] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv\npreprint\ncontrastive predictive\ncoding,”\narXiv:1807.03748,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2018.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[28] R. Lotfian and C. Busso, “Building naturalistic emotionally bal-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "anced speech corpus by retrieving emotional speech from exist-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing podcast recordings,” IEEE Transactions on Affective Computing,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 10, no. 4, pp. 471–483, October-December 2019.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[29]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "training of deep bidirectional\ntransformers\nfor\nlanguage under-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "standing,” arXiv preprint arXiv:1810.04805, 2018.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[30]\nJ. Wagner, A.\nTriantafyllopoulos, H. Wierstorf, M.\nSchmitt,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F. Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of\nthe trans-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former\nera in speech emotion recognition: Closing the valence",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gap,” IEEE Transactions on Pattern Analysis and Machine Intelligence,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 45, no. 9, pp. 10 745–10 759, 2023.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[31]\nS. Latif, R. Rana, S. Khalifa, R.\nJurdak, and B. W. Schuller, “Self",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "supervised adversarial domain adaptation for\ncross-corpus and",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cross-language speech emotion recognition,” IEEE Transactions on",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Affective Computing, 2022.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[32]\nS. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, and",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "L.-P. Morency,\n“Context-dependent\nsentiment\nanalysis\nin user-",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the 55th annual meeting of\nthe\ngenerated videos,” in Proceedings of",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "association for computational linguistics (volume 1: Long papers), 2017,",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 873–883.",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[33] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, “Greedy",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "informa-\nlayer-wise training of deep networks,” Advances in neural",
          "9": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion processing systems, vol. 19, 2006.",
          "9": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Augmenting generative adversarial networks for speech emotion recognition",
      "authors": [
        "S Latif",
        "M Asim",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Augmenting generative adversarial networks for speech emotion recognition",
      "arxiv": "arXiv:2005.08447"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "6",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "7",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Improving speech emotion recognition with adversarial data augmentation network",
      "authors": [
        "L Yi",
        "M.-W Mak"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "9",
      "title": "Modeling feature representations for affective speech using generative adversarial networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "authors": [
        "X Chen",
        "Y Duan",
        "R Houthooft",
        "J Schulman",
        "I Sutskever",
        "P Abbeel"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "A preliminary study on augmenting speech emotion recognition using a diffusion model",
      "authors": [
        "I Malik",
        "S Latif",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Y Wang",
        "Y Li",
        "Z Cui"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Robust audiovisual emotion recognition: Aligning modalities, capturing temporal information, and handling missing features",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Multi-task semi-supervised adversarial autoencoding for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "17",
      "title": "Generative data augmentation guided by triplet loss for speech emotion recognition",
      "authors": [
        "S Wang",
        "H Hemati",
        "J Guðnason",
        "D Borth"
      ],
      "year": "2022",
      "venue": "Generative data augmentation guided by triplet loss for speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Generative emotional ai for speech emotion recognition: The case for synthetic emotional speech augmentation",
      "authors": [
        "S Latif",
        "A Shahid",
        "J Qadir"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "19",
      "title": "A generation of enhanced data by variational autoencoders and diffusion modeling",
      "authors": [
        "Y.-J Kim",
        "S.-P Lee"
      ],
      "year": "2024",
      "venue": "Electronics"
    },
    {
      "citation_id": "20",
      "title": "Inter-intra modal representation augmentation with trimodal collaborative disentanglement network for multimodal sentiment analysis",
      "authors": [
        "C Chen",
        "H Hong",
        "J Guo",
        "B Song"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Exploring complementary features in multi-modal speech emotion recognition",
      "authors": [
        "S Wang",
        "Y Ma",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "23",
      "title": "Foundations & trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "24",
      "title": "AST: Audio Spectrogram Transformer",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "25",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "28",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "30",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "33",
      "title": "Greedy layer-wise training of deep networks",
      "authors": [
        "Y Bengio",
        "P Lamblin",
        "D Popovici",
        "H Larochelle"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "L Tran",
        "X Liu",
        "J Zhou",
        "R Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "J Zhao",
        "R Li",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "emodarts: Joint optimisation of cnn & sequential neural network architectures for superior speech emotion recognition",
      "authors": [
        "T Rajapakshe",
        "R Rana",
        "S Khalifa",
        "B Sisman",
        "B Schuller",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    }
  ]
}