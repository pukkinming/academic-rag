{
  "paper_id": "2508.16859v1",
  "title": "Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding And Reasoning Benchmark",
  "published": "2025-08-23T01:10:29Z",
  "authors": [
    "Jinpeng Hu",
    "Hongchang Shi",
    "Chongyuan Dai",
    "Zhuo Li",
    "Peipei Song",
    "Meng Wang"
  ],
  "keywords": [
    "Emotion Reasoning",
    "Emotion Recognition",
    "Mutimodal 15.4% Fear: 6.75% Happy: 33.22% Sadness: 22.12% Surprise: 12.41%"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal large language models (MLLMs) have been widely applied across various fields due to their powerful perceptual and reasoning capabilities. In the realm of psychology, these models hold promise for a deeper understanding of human emotions and behaviors. However, recent research primarily focuses on enhancing their emotion recognition abilities, leaving the substantial potential in emotion reasoning, which is crucial for improving the naturalness and effectiveness of human-machine interactions. Therefore, in this paper, we introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR) benchmark, which encompasses 1,451 video data from real-life scenarios, along with 5,101 progressive questions. These questions cover various aspects, including emotion recognition, potential causes of emotions, future action prediction, etc. Besides, we propose a multi-agent framework, where each agent specializes in a specific aspect, such as background context, character dynamics, and event details, to improve the system's reasoning capabilities. Furthermore, we conduct experiments with existing MLLMs and our agent-based method on the proposed benchmark, revealing that most models face significant challenges with this task. \n CCS Concepts ‚Ä¢ Applied computing ‚Üí Psychology; ‚Ä¢ Information systems ‚Üí Multimedia streaming.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, multimodal large language models (MLLMs) have shown significant advancements across diverse fields due to their enhanced capabilities in multimodal understanding and reasoning  [21, 29] . One of the ultimate goal of MLLMs is to improve humanmachine interactions, enabling more seamless and intuitive communication. Central to achieving this goal is the development of multimodal emotion understanding and reasoning, as a deeper understanding of human emotions allows for responses that are more adaptive and acceptable to users. Therefore, modeling and evaluating emotional and psychological states have drawn substantial attention in recent years, and many studies have been proposed in this area  [9, 12, 33, 37, 44] . For example, the CMU-MOSI dataset  [59]  provided video segments of utterances annotated for sentiment polarity and intensity, enabling researchers to assess model performance in multimodal sentiment analysis. Furthermore, the MELD dataset  [35]  offered video clips of multi-party conversations annotated with six different emotions, enabling researchers to evaluate model performance in emotion recognition. Moreover, the Social-IQ  [58]  provided a resource for assessing social intelligence through question-answering tasks, enabling researchers to gauge model performance in understanding social and emotion cues. MEmoR  [38]  introduced a multimodal emotion recognition dataset that integrates a reasoning process, leveraging supporting evidence to better recognize emotion.\n\nAlthough these approaches have brought significant improvements, several issues cannot be appropriately considered. First, most existing studies emphasize assessing sentiment intensity, polarity, and emotion types  [10, 50] . However, they still fall short in capturing the complexity of emotion interactions and do not sufficiently explore the evolving relationships and transitions between",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mtmeur",
      "text": "The woman sits silently on the ground and holds the book tightly, with tears rolling down her cheeks.\n\nB. They could decide to withdraw from social interactions, preferring solitude as a means to deeply reflect on their emotions and thoughts.\n\nOn the skating rink, the child stands between two adults, with bright eyes and a broad smile.\n\nThe speaker is speaking on stage, while the people below are blankeyed and silent.\n\nThe woman looks straight at the ring in the red box, with her face full of surprise and excitement.  emotions. Second, most benchmarks lack detailed exploration of emotion-specific aspects, resulting in limitations in both scope and depth  [38] . For example, as shown in Figure  1 , identifying potential causes of emotions or predicting future human responses within the video context is essential for a more comprehensive understanding of emotional and psychological states, while such elements are overlooked in datasets such as Social-IQ and Social-IQ2.\n\nTherefore, in this paper, we introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR) benchmark, which consists of 1,451 carefully curated videos that cover a wide range of complex scenarios, along with 5,101 high-quality progressive questions, including emotion recognition, potential causes of emotions, future action prediction, etc. Methodologically, we propose a novel multi-agent-based approach to improve the emotion reasoning capabilities of MLLMs by efficiently extracting and integrating distinct aspects of information. This framework consists of four specialized agents, each concentrating on a unique aspect: background context, character dynamics, event details, and decision-making processes. Furthermore, we evaluate several state-of-the-art (SOTA) MLLMs and the proposed method on MTMEUR dataset, with accuracy ranging from 29.10% (Videochatgpt) to 71.19% (Qwen2-VL). The experimental results also show that the proposed method is beneficial to improving emotion reasoning ability. Our code is released at https://github.com/MindIntLab-HFUT/MTMEUR.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works 2.1 Multimodal Large Language Models",
      "text": "Natural Language Processing (NLP) methodologies have been extensively applied across diverse domains, encompassing tasks such as text summarization, information extraction, and text classification  [14-16, 25, 28] . Large Language models have revolutionized NLP tasks, achieving success across a broad spectrum of applications  [11, 13, 23, 24] . Foundational architectures like Transformer  [45]  and training paradigms like instruction tuning  [47]  have enabled models such as PaLM  [8] , and LLaMA  [43]  to demonstrate remarkable language understanding capabilities. Building upon these textual foundations, the emergence of MLLMs has extended these capabilities to visual domains. The debut of GPT-4 (Vision) has attracted significant attention due to its groundbreaking multimodal understanding capabilities. This success has sparked a wave of research into MLLMs. Early research in this area primarily focused on image-text understanding tasks  [20, 55, 61, 62] . LLaVA  [27]  and MiniGPT-4  [64] , where image encoders were utilized to extract visual features and were then aligned with LLM-based encoders for further processing. These models have paved the way for new approaches to multimodal content understanding and generation. As research has progressed, some models have begun to incorporate video modalities, exploring the vast potential of large models in video understanding. VideoChat  [22]  integrated video foundation models and LLM via a learnable neural interface. Furthermore, Video-ChatGPT  [30]  used CLIP  [36]  to extract spatial and temporal features from videos, which are then integrated into LLM to enhance understanding and generating detailed conversations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition Dataset",
      "text": "In recent years, researchers have made significant progress in the field of emotion understanding, including emotion recognition  [1] , emotional video captioning  [39] [40] [41]  and so on. Many benchmarks have been proposed to evaluate the model performance in terms of emotion recognition  [52, 54] . For instance, CMU-MOSI  [59] , CMU-MOSEAS  [57] , and Youtube  [32]  integrated text, and video modalities, and offered basic emotion annotations, which focused on predicting sentiment polarity and intensity within videos. Furthermore, MELD  [35] , MuSe-CaR  [42] , and IEMOCAP  [4]  contained videos of multi-party conversations, which further incorporated the context of interactions. CMU-MOSEI  [60]  consists of numerous YouTube video clips, each containing at least one recognized emotion, further enriching the multimodal emotion analysis. In addition, some recent studies  [49, 58]  focus on measuring emotion cognition and intelligence within social contexts through question answering. Compared to existing benchmarks, the proposed MTMEUR focuses on the reasoning of characters in a video. This task demands not only an advanced understanding of the visual and contextual elements within the video, but also necessitates the integration of sophisticated common sense knowledge. MTMEUR challenges the model to simulate human-like responses within specific emotion contexts and scenarios, offering a better evaluation of its emotion reasoning and cognitive abilities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mtmeur Dataset",
      "text": "In this section, we provide a comprehensive description of the MTMEUR dataset, including the data collection, generation, evolution, selection, and human review. Afterwards, we give the detailed dataset statistics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Collection And Preprocessing",
      "text": "We initially collect about 10,000 clips from two websites, pexels  [34]  and mixkit  [31] , respectively. Next, to ensure that the quality and emotion content of the videos meet the research requirements, we implement a rigorous screening process. Specifically, four reviewers perform an initial evaluation of the video content, aiming to select videos that not only display a single emotion change but also demonstrate emotion interactions within specific contexts. Additionally, we consider the diversity of the video backgrounds to ensure that the context provided ample information. This process also take into account the emotion expressiveness of the video, visual quality, and the effectiveness of background elements. During the screening process, each reviewer is asked to rate the videos on a scale of 1 to 5, quantifying their evaluation results. To ensure consistency in the assessments, all reviewers follow clear guidelines when assigning scores and more details can be found in supplementary materials. After completing the scoring process, we select the top 1,451 highest-rated videos.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Generation",
      "text": "Generating large-scale sentiment-annotated data manually is challenging due to its time-consuming and labor-intensive nature, demanding significant manual effort and meticulous attention to detail. To tackle this challenge, we developed a unified and automated process to generate high-quality data associated with the videos.\n\nIn the first step, we select a subset of pre-filtered seed tasks from Social-IQ  [58]  and further supplement them with 75 manually constructed examples. These high-quality examples cover diverse emotional reasoning types and serve as few-shot examples to guide GPT-4o to produce better question-answer pairs. For each video, we randomly sample six tasks from the seed pool and input both video and chosen tasks to GPT-4o to generate two more complex questions. This method allows us to efficiently generate progressive questions for each video during the initial phase. These questions cover not only common topics, ranging from fundamental areas like emotion recognition and the triggers of emotion responses, to openended inquiries that require reasoning. For instance, some questions explore the potential outcomes of a situation or ask for suggestions based on specific contexts. We generate both the correct option and several distracting options for each question, ensuring that each question has four distinct choices. These distractor options are carefully designed to resemble the correct answers in surface form while differing conceptually or even being opposites, thereby enhancing their ability to mislead.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Question And Option Evolution",
      "text": "In the first phase of data generation, the generated questions and answers may be relatively simple for MLLMs to understand and answer, and the style of the data remains quite similar to the initial seed data. Motivated by instruction evolution that enhanced instructions can empower LLM better alignment with human preference  [53] , we introduce an iterative refinement mechanism aimed at improving the complexity and diversity of the initial question set. By utilizing specially designed prompts, we gradually increase the complexity and diversity of the generated questions, making them more aligned with real-world emotion reasoning scenarios. Specifically, our iterative mechanism enhances the questions and answers from multiple dimensions, progressively elevating their sophistication to better simulate the complex challenges found in real emotion reasoning tasks. Additionally, during evolution, a filtering mechanism evaluates the evolved data, re-evolving any that fails to meet standards. Question Evolution. We evolve the questions in the following aspects. For complexity, we introduce multiple reasoning steps and add constraints based on the specific contexts within the video scenes. These constraints not only require the model to identify basic emotions but also to understand the complex relationships and causal links between emotions and their contexts. For example, while an initial question might simply ask about a single emotion, after several iterations, the question may require the model to handle the interplay of multiple emotions during the reasoning process. For diversity, we design prompts that guide the model to create coherent and diverse questions across various contexts. Through multi-turn interactions, these questions explore how emotions evolve at different stages and their impact on other characters within the scenes. In addition to complexity and diversity, we also focus on fine-grained emotion reasoning, emphasizing the subtle dynamics of emotion shifts. The emotions depicted in videos are often complex and multi-dimensional, with multiple emotions sometimes overlapping. To address this, our prompts are designed to guide the model in capturing these subtle emotion changes, ensuring that the generated questions delve deeply into the interaction between emotions.\n\nAnswer Evolution. In addition to question evolution, we further optimize the answers to encompass more contextual information and enhance the reasoning requirements of the options from two aspects. To increase the depth of the options, we first introduce more constraints by adding qualifiers and modifiers to make the description more specific. Besides, the evolution process involve integrating secondary conditions or details that are closely related to the question context, including emotion shifts in the character, subtle behaviors, and video details. For example, the original option might be \"The character feels angry due to a friend's misunderstanding. \" With increased reasoning depth, the option could be revised to \"The character feels angry due to a friend's misunderstanding but displays coldness and distrust when the friend attempts to explain. \" These steps refine the language to better match the context of the video, necessitating a deeper understanding from the model to accurately differentiate between the correct and distractor options.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Data Selection",
      "text": "After the multi-turn data generation, we further screen the generated emotion question-answer for each video. We employ a dual scoring mechanism to evaluate and select the data. In detail, we instruct ChatGPT to assign a complexity score and a quality score to each generated questions and answers. A higher complexity score indicates responses that require multi-level reasoning, involve intricate emotion relationships in the video, and capture causal links between emotions. A higher quality score reflects responses that accurately convey emotion cues while maintaining coherence and clarity in expression. We multiplied the complexity and quality scores to obtain a composite score for each question. We ranked all questions for the same video based on their composite scores and selected the highest-scoring questions as the final retained questions for that video. Afterwards, we combined the corresponding options and applied a similar scoring process to obtain composite scores, ultimately selecting the highest-scoring options as the final ones for each chosen question.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Quality Control",
      "text": "To ensure that the generated data performs well in terms of correctness, complexity, relevance, and practical application, we also incorporate a strict manual quality control process. The volunteers with relevant domain expertise carefully screened and evaluated the generated questions and corresponding answers. Reviewers assess the question-answer pair based on the following three dimensions to ensure they meet the expected standards:  We filter out any data that failed to meet the first criterion (i.e., correctness). Besides, each question is independently evaluated by at least two reviewers in terms of complexity and quality, with each dimension scored on a scale of 1 to 5. The final score for each question is the average of the two reviewers' scores. If the score difference between the two reviewers exceed a certain threshold (i.e., more than 2 points), the question is sent to a third reviewer for arbitration. After scoring, the questions are ranked based on their total scores, and only those exceeding a minimum threshold (i.e., 3) are retained in the dataset. To ensure the reliability of the manual review process, we assess the level of consistency among these reviewers. Before the formal manual review, we randomly selected 100 examples and calculate Fleiss' Kappa between reviewers, which yielded a value exceeding 0.81. This high Kappa score indicates strong agreement among reviewers, demonstrating the effectiveness and reliability of the manual review process.\n\nTo further validate the question solvability, we conducted an additional human evaluation with new volunteers who had not participated in the original annotation process. The volunteers achieved an average accuracy of 96.22% on dataset, demonstrating that questions are solvable with the corresponding videos.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Dataset Statistics",
      "text": "As shown in Figure  2 , this dataset covers seven emotions: angry, excited, fear, happy, sad, disgust, and surprise, which also displays the distribution proportions for these emotions. Each emotion category includes a variety of scenes, such as family meetings, dinners, amusement parks, graduation ceremonies, award events and so on, ensuring the diversity and representativeness of the dataset. Besides, in Table  1 , we provide a comparative analysis of MTMEUR with Question: Analyze the emotional progression and identify the likely reason for the character's reaction of throwing the phone.\n\nA cozy family breakfast scene with a variety of foods on the table.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Information Gathering Decision Making",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "R E F I N E",
      "text": "To address the lack of attention, the mother interrupts, redirecting focus away from the phone.\n\nA girl is absorbed in phone and shocked.\n\nMother interrupts her daughter. existing benchmarks, such as Social-IQ2. Each video in MTMEUR dataset includes an average of four questions, with an average question length of 19.63 words. The average word length of answers in MTMEUR is 14.27, which is longer than Social-IQ2. This suggests that options in MTMEUR are generally more detailed and offer greater specificity. We also find that the average length of correct answers is similar to that of incorrect answers, which helps to reduce bias that might arise from differences in answer length.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Method",
      "text": "Given a video ùëâ and ùëõ questions ùëÑ in natural language along with options ùëÇ, the goal is to select the correct options. Note that each ùëÇ = {ùëú 1 , ùëú 2 , . . . , ùëú ùëö } have ùëö options. In this section, we present a multi-agent-based multimodal collaboration framework for emotion recognition and reasoning, which consists of four core agents: the background agent, the character agent, the event agent, and the decision agent. The former three agents aims to better learn the distinct information from videos through dynamic interaction and collaborative reasoning and the last one is to give the final answer. An overview of our proposed method is presented in Figure  4 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Information Gathering",
      "text": "Background information is usually important for emotion understanding and reasoning. For example, in most situations, people in a park are usually happy while in a hospital tend to sad. Therefore, we first construct a background agent, which focuses on extracting and structuring environmental information from the video. The background agent employs a tailored prompt to guide the MLLM to produce detailed, structured descriptions of the scene, covering elements such as scene type, object layout, and additional contextual\n\nwhere ùê∂ is the description of character dynamics, ùëÉ ùëê is the prompt for character agent and\n\nis the history information from previous agent communication. Meanwhile, the third agent, event agent is proposed to establish causal relationships between the character's emotional states and the video event sequence.\n\nwhere ùê∏ is the description of event details, ùëÉ ùëí is the prompt for event agent. In doing so, the three agents engage in collaborative interactions to refine the quality of their individual descriptions. Sub For example, character agent take the event into consideration and provides a specific and comprehensive character description.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Decision Agent",
      "text": "We gather diverse information from the three agents, each specializing in different aspects of the video. The decision agent functions as the ultimate decision-making unit within the system, tasked with synthesizing background information, character dynamics, and event details to formulate the final answer to the questions.\n\nDuring the decision-making process, the decision agent combines contextual information ùêª = [ùêª 1 , ùêª 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùêª ùëá ] and give the correct option, where ùëá is the number of interaction turns within the multi-agent framework.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments 5.1 Baselines And Evaluation Metrics",
      "text": "To explore the performance of different models, we evaluate a range of representative open-source multimodal large models, including Video-ChatGPT  [30] , Video-LLaVA  [26] , MiniCPM  [17] , VTimeLLM  [18] , Emotion-LLaMA  [6] , ShareGPT4Video  [5] , VideoLLaMA2  [7] , Chat-UniVi  [19] , Qwen-VL-chat  [2] , and Qwen2-VL  [46] . The proposed dataset contain both single-choice and multiplechoice questions. We use accuracy to evaluate the model performance, which is defined as the ratio of completely correct answers to the total number of questions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "In our experiments, we use the Autogen  [51]  library to build our agent-based framework. Specifically, we adopt Qwen2-VL as the backbone model for our proposed approach. During inference, we set the temperature to 0 in all experiments to control randomness. Besides, our experiments are conducted on the A6000 GPUs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overall Performance",
      "text": "To explore the performance of different models in our proposed dataset, we report the results in Table  2 . We show both their overall accuracy and their respective results across different emotion categories. There are several observation drawn from the results. First, Qwen2-VL leads with an overall accuracy of 71.19%, outperforming other models, demonstrating its strong capabilities in emotion recognition and emotion reasoning. Second, our proposed model outperform its base model (i.e., Qwen2-VL), which also performs exceptionally well, achieving an accuracy of 72.93%. This illustrates that using different agents to integrate background, character, and event information is beneficial to improve emotion reasoning. Third, in terms of emotion categories, most positive emotions such as Happy and Surprise tend to be recognized more accurately. These  positive emotions are often accompanied by clear facial expressions, and semantically easy-to-interpret cues, which making it easier for models to extract emotion information. In contrast, most negative emotions are generally with relatively lower accuracy, especially Angry and Sadness, which consistently shows poorer performance due to its inherent complexity and subtle emotion cues.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To explore effect of different agent in this task, we conduct ablation study and the results are reported in Table  3 . It is observed that removing any agent from our proposed method results in a noticeable decline in performance, highlighting the contribution of each agent in enhancing the model's ability to comprehend the video content. Notably, the removal of the Background agent leads to a significant accuracy drop from 72.93% to 71.27%. This highlights the importance of background and contextual information, which serves as a foundation for understanding character actions and event development.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Question Type",
      "text": "To investigate the impact of different question categories on model performance, we evaluate the performance of different models across five question categories in the MTMEUR dataset: Current State, Direct Causality, Indirect Causality, Prediction of Future and others. The results are reported in Table  4 . It is observed that that Prediction of Future and Indirect Causality show the lowest accuracies, reflecting the challenges models face in handling complex reasoning tasks. These categories require not only recognizing emotions in the current context but also predicting how they evolve over time or how they are influenced by multiple factors. In contrast, the Current State and Direct Causality categories yield relatively higher performance, as these tasks primarily involve recognizing and reasoning based on observable emotional states and direct relationships between emotions and events.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Prompt Type Result (%) Œî",
      "text": "Qwen2-VL 71.19 -Step-Back  [63]  70.57 -0.62% AP  [56]  70.68 -0.51% Few-shot  [3]  71.84 +0.65% CoT  [48]  71.98 +0.79% Our Method 72.93 +1.74%",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Evolution",
      "text": "To explore the effect of the question and option evolution in this task, we conduct an experiment and report the results in Table  5 . We show the performance of the our dataset before and after evolution across different models. It is observed that all models experience a noticeable decline in accuracy after the evolution of the dataset. This decline can be attributed to the increased complexity and diversity of the evolved dataset, which are designed to challenge the model's emotion reasoning capabilities more effectively. Specifically, the average length of questions increases from 11.21 words to 19.63 words, and the average length of options grows from 10.94 words to 14.27 words. The increased question length introduces more contextual information and necessitates multistep reasoning, while the longer options require models to make finer distinctions between emotional states, collectively raising the difficulty of accurately interpreting emotional cues in multi-turn reasoning scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Different Prompt",
      "text": "To explore the impact of the different prompts on Qwen2-VL in our proposed dataset, we conduct experiments using four distinct prompting strategies: Few-Shot  [3] , Chain-of-Thought(CoT)  [48] , Analogical Prompting (AP)  [56] , and Step-Back  [63] . The results are shown in Table  6",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Error Analysis",
      "text": "Based on our experiments on the MTMEUR dataset, we find several typical issues, as detailed below. 1) The model exhibits significant limitations in recognizing dynamic emotion shifts, especially in the video contexts involving multiple emotion interactions and  emotion changes. As shown in Fig.  5  (a), where the girl transitions from calmness to anger, the model frequently identifies only the initial calm state, neglecting the critical shift toward anger that follows. 2) While the model occasionally identifies basic emotional states in the video, its performance declines when dealing with complex reasoning tasks. As shown in the first row of Fig.  5  (b), the model may detect a character's anger but fails to associate this emotion with subsequent behaviors or events.\n\n3) The responses of the model are often affected by background information in the video, especially in complex contexts or when the visual background contradicts emotion cues. As shown in the second row of Fig.  5  (c), the model might mistakenly select \"a lively and chaotic scene\" due to the bright, festive decorations in the video background. 4) The model sometimes generates \"hallucinations, \" producing nonexistent emotions or scenes. In the second row of Fig.  5  (d), the model relies on certain body language of the team members, mistakenly infers complex emotional dynamics, and fails to recognize that the main atmosphere of the scene is celebratory.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "In this paper, we introduce MTMEUR, a rigorously designed multimodal emotion understanding and reasoning benchmark. The dataset consists of 5,101 multiple-choice questions tailored to evaluate the emotion reasoning proficiency of MLLMs within a broad range of intricate real-world video scenarios. Moreover, we propose a multi-agent-based multimodal collaboration framework, where different agents focus on different aspects. These agents interact and refine each other's descriptions to improve information gathering so as to enhance reasoning ability. We further present an experimental evaluation of several popular MLLMs using the MTMEUR benchmark, which reveals that most existing models continue to face significant challenges in this task. Future work can leverage MTMEUR to further refine MLLMs' performance in these complex scenarios, improving their application in real-world emotion understanding and reasoning tasks. An intriguing finding from our experiments is that reasoning about negative emotions tends to be more challenging than reasoning about positive emotions. This observation highlights a key challenge in applying MLLMs.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples from MTMEUR, presenting four questions along with their answers and the reasoning processes necessary",
      "page": 2
    },
    {
      "caption": "Figure 1: , identifying potential",
      "page": 2
    },
    {
      "caption": "Figure 2: Distribution of video categories. The MTMEUR",
      "page": 3
    },
    {
      "caption": "Figure 3: Data Generation Pipeline. The pipeline for data generation comprises, which includes initial creation, iterative",
      "page": 4
    },
    {
      "caption": "Figure 2: , this dataset covers seven emotions: angry,",
      "page": 5
    },
    {
      "caption": "Figure 4: The overall architecture of our method with four",
      "page": 5
    },
    {
      "caption": "Figure 5: Examples of mistakes made by the model on the dataset. Text in red represents the model‚Äôs choice, text in green",
      "page": 8
    },
    {
      "caption": "Figure 5: (a), where the girl transitions",
      "page": 8
    },
    {
      "caption": "Figure 5: (d), the model relies",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Diverdity": "Complexity"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Correctness": "Complexity"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Filter Secondary\nDepth": "Depth",
          "Column_2": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BeforeEvolution(%)": "39.82\n71.47\n79.09\n83.47"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "M Sharmeen",
        "Abdullah Saleem",
        "Abdullah",
        "Mohammed Siddeeq Y Ameen Ameen",
        "Subhi Sadeeq",
        "Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "2",
      "title": "Qwen-vl: A frontier large visionlanguage model with versatile abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large visionlanguage model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "3",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Sharegpt4video: Improving video understanding and generation with better captions",
      "authors": [
        "Lin Chen",
        "Xilin Wei",
        "Jinsong Li",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Zehui Chen",
        "Haodong Duan",
        "Bin Lin",
        "Zhenyu Tang"
      ],
      "year": "2024",
      "venue": "Sharegpt4video: Improving video understanding and generation with better captions",
      "arxiv": "arXiv:2406.04325"
    },
    {
      "citation_id": "6",
      "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Jingdong Sun",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "7",
      "title": "Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "authors": [
        "Zesen Cheng",
        "Sicong Leng",
        "Hang Zhang",
        "Yifei Xin",
        "Xin Li",
        "Guanzheng Chen",
        "Yongxin Zhu",
        "Wenqi Zhang",
        "Ziyang Luo",
        "Deli Zhao"
      ],
      "year": "2024",
      "venue": "Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "8",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "9",
      "title": "CLPsych 2015 shared task: Depression and PTSD on Twitter",
      "authors": [
        "Glen Coppersmith",
        "Mark Dredze",
        "Craig Harman",
        "Kristy Hollingshead",
        "Margaret Mitchell"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2nd workshop on computational linguistics and clinical psychology: from linguistic signal to clinical reality"
    },
    {
      "citation_id": "10",
      "title": "FAF: A novel multimodal emotion recognition approach integrating face, body and text",
      "authors": [
        "Zhongyu Fang",
        "Aoyun He",
        "Qihui Yu",
        "Baopeng Gao",
        "Weiping Ding",
        "Tong Zhang",
        "Lei Ma"
      ],
      "year": "2022",
      "venue": "FAF: A novel multimodal emotion recognition approach integrating face, body and text",
      "arxiv": "arXiv:2211.15425"
    },
    {
      "citation_id": "11",
      "title": "Llms accelerate annotation for medical information extraction",
      "authors": [
        "Akshay Goel",
        "Almog Gueta",
        "Omry Gilon",
        "Chang Liu",
        "Sofia Erell",
        "Huong Nguyen",
        "Xiaohong Hao",
        "Bolous Jaber",
        "Shashir Reddy",
        "Rupesh Kartha"
      ],
      "year": "2023",
      "venue": "machine learning for health (ML4H)"
    },
    {
      "citation_id": "12",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "Jonathan Gratch",
        "Ron Artstein",
        "M Gale",
        "Giota Lucas",
        "Stefan Stratou",
        "Angela Scherer",
        "Rachel Nazarian",
        "Jill Wood",
        "David Boberg",
        "Stacy Devault",
        "Marsella"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "13",
      "title": "Psycollm: Enhancing llm for psychological understanding and evaluation",
      "authors": [
        "Jinpeng Hu",
        "Tengteng Dong",
        "Luo Gang",
        "Hui Ma",
        "Peng Zou",
        "Xiao Sun",
        "Dan Guo",
        "Xun Yang",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "14",
      "title": "Word Graph Guided Summarization for Radiology Findings",
      "authors": [
        "Jinpeng Hu",
        "Jianling Li",
        "Zhihong Chen",
        "Yaling Shen",
        "Yan Song",
        "Xiang Wan",
        "Tsung-Hui Chang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "15",
      "title": "Graph Enhanced Contrastive Learning for Radiology Findings Summarization",
      "authors": [
        "Jinpeng Hu",
        "Zhuo Li",
        "Zhihong Chen",
        "Zhen Li",
        "Xiang Wan",
        "Tsung-Hui Chang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Hero-Gang Neural Model For Named Entity Recognition",
      "authors": [
        "Jinpeng Hu",
        "Yaling Shen",
        "Yang Liu",
        "Xiang Wan",
        "Tsung-Hui Chang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "17",
      "title": "Minicpm: Unveiling the potential of small language models with scalable training strategies",
      "authors": [
        "Shengding Hu",
        "Yuge Tu",
        "Xu Han",
        "Chaoqun He",
        "Ganqu Cui",
        "Xiang Long",
        "Zhi Zheng",
        "Yewei Fang",
        "Yuxiang Huang",
        "Weilin Zhao"
      ],
      "year": "2024",
      "venue": "Minicpm: Unveiling the potential of small language models with scalable training strategies",
      "arxiv": "arXiv:2404.06395"
    },
    {
      "citation_id": "18",
      "title": "Vtimellm: Empower llm to grasp video moments",
      "authors": [
        "Bin Huang",
        "Xin Wang",
        "Hong Chen",
        "Zihan Song",
        "Wenwu Zhu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Wancai Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Llava-med: Training a large language-and-vision assistant for biomedicine in one day",
      "authors": [
        "Chunyuan Li",
        "Cliff Wong",
        "Sheng Zhang",
        "Naoto Usuyama",
        "Haotian Liu",
        "Jianwei Yang",
        "Tristan Naumann",
        "Hoifung Poon",
        "Jianfeng Gao"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
      "authors": [
        "Feng Li",
        "Renrui Zhang",
        "Hao Zhang",
        "Yuanhan Zhang",
        "Bo Li",
        "Wei Li",
        "Zejun Ma",
        "Chunyuan Li"
      ],
      "year": "2024",
      "venue": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
      "arxiv": "arXiv:2407.07895"
    },
    {
      "citation_id": "22",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "23",
      "title": "Selfinstructed derived prompt generation meets in-context learning: Unlocking new potential of black-box llms",
      "authors": [
        "Zhuo Li",
        "Yuhao Du",
        "Jinpeng Hu",
        "Xiang Wan",
        "Anningzhe Gao"
      ],
      "year": "2024",
      "venue": "Selfinstructed derived prompt generation meets in-context learning: Unlocking new potential of black-box llms",
      "arxiv": "arXiv:2409.01552"
    },
    {
      "citation_id": "24",
      "title": "Add-One-In: Incremental Sample Selection for Large Language Models via a Choice-Based Greedy Paradigm",
      "authors": [
        "Zhuo Li",
        "Yuhao Du",
        "Xiaoqi Jiao",
        "Yiwen Guo",
        "Yuege Feng",
        "Xiang Wan",
        "Anningzhe Gao",
        "Jinpeng Hu"
      ],
      "year": "2025",
      "venue": "Add-One-In: Incremental Sample Selection for Large Language Models via a Choice-Based Greedy Paradigm",
      "arxiv": "arXiv:2503.02359"
    },
    {
      "citation_id": "25",
      "title": "Prototype-oriented clean subset extraction for noisy long-tailed classification",
      "authors": [
        "Zhuo Li",
        "He Zhao",
        "Anningzhe Gao",
        "Dandan Guo",
        "Tsung-Hui Chang",
        "Xiang Wan"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "26",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "27",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Text Summarization with Pretrained Encoders",
      "authors": [
        "Yang Liu",
        "Mirella Lapata"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution",
      "authors": [
        "Zuyan Liu",
        "Yuhao Dong",
        "Ziwei Liu",
        "Winston Hu",
        "Jiwen Lu",
        "Yongming Rao"
      ],
      "year": "2024",
      "venue": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution",
      "arxiv": "arXiv:2409.12961"
    },
    {
      "citation_id": "30",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "31",
      "title": "Mixkit: Free Assets for Video, Music, and Sound Effects",
      "authors": [
        "Mixkit"
      ],
      "year": "2023",
      "venue": "Mixkit: Free Assets for Video, Music, and Sound Effects"
    },
    {
      "citation_id": "32",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "33",
      "title": "TILES-2018, a longitudinal physiologic and behavioral data set of hospital workers",
      "authors": [
        "Karel Mundnich",
        "Brandon Booth",
        "Michelle Hommedieu",
        "Tiantian Feng",
        "Benjamin Girault",
        "Justin 'hommedieu",
        "Mackenzie Wildman",
        "Sophia Skaaden",
        "Amrutha Nadarajan",
        "Jennifer Villatte"
      ],
      "year": "2020",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "34",
      "title": "Pexels: Free Stock Photos and Videos",
      "authors": [
        "Pexels"
      ],
      "year": "2023",
      "venue": "Pexels: Free Stock Photos and Videos"
    },
    {
      "citation_id": "35",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "37",
      "title": "Towards empathetic opendomain conversation models: A new benchmark and dataset. arXiv 2018",
      "authors": [
        "E Rashkin",
        "M Smith",
        "Li",
        "Boureau"
      ],
      "venue": "Towards empathetic opendomain conversation models: A new benchmark and dataset. arXiv 2018",
      "arxiv": "arXiv:1811.00207"
    },
    {
      "citation_id": "38",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "Guangyao Shen",
        "Xin Wang",
        "Xuguang Duan",
        "Hongzhi Li",
        "Wenwu Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "39",
      "title": "Contextual Attention Network for Emotional Video Captioning",
      "authors": [
        "Peipei Song",
        "Dan Guo",
        "Jun Cheng",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Emotional Video Captioning With Vision-Based Emotion Interpretation Network",
      "authors": [
        "Peipei Song",
        "Dan Guo",
        "Xun Yang",
        "Shengeng Tang",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "41",
      "title": "Emotion-Prior Awareness Network for Emotional Video Captioning",
      "authors": [
        "Peipei Song",
        "Dan Guo",
        "Xun Yang",
        "Shengeng Tang",
        "Erkun Yang",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lea Schumann",
        "Bj√∂rn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timoth√©e Lacroix",
        "Baptiste Rozi√®re",
        "Naman Goyal"
      ],
      "venue": ""
    },
    {
      "citation_id": "44",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "45",
      "title": "Dreaddit: A reddit dataset for stress analysis in social media",
      "authors": [
        "Elsbeth Turcan",
        "Kathleen Mckeown"
      ],
      "year": "2019",
      "venue": "Dreaddit: A reddit dataset for stress analysis in social media",
      "arxiv": "arXiv:1911.00133"
    },
    {
      "citation_id": "46",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge"
      ],
      "year": "2024",
      "venue": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "48",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "Jason Wei",
        "Maarten Bosma",
        "Y Vincent",
        "Kelvin Zhao",
        "Adams Guu",
        "Brian Yu",
        "Nan Lester",
        "Andrew Du",
        "Quoc V Dai",
        "Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "arxiv": "arXiv:2109.01652"
    },
    {
      "citation_id": "49",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "50",
      "title": "Social-IQ 2.0 Challenge: Benchmarking Multimodal Social Understanding",
      "authors": [
        "Alex Wilf",
        "Leena Mathur",
        "Sheryl Mathew",
        "Claire Ko",
        "Youssouf Kebe",
        "Paul Liang",
        "Louis-Philippe Morency"
      ],
      "year": "2023",
      "venue": "Social-IQ 2.0 Challenge: Benchmarking Multimodal Social Understanding"
    },
    {
      "citation_id": "51",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "Martin W√∂llmer",
        "Felix Weninger",
        "Tobias Knaup",
        "Bj√∂rn Schuller",
        "Congkai Sun",
        "Kenji Sagae",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "52",
      "title": "Autogen: Enabling next-gen LLM applications via multi-agent conversations",
      "authors": [
        "Qingyun Wu",
        "Gagan Bansal",
        "Jieyu Zhang",
        "Yiran Wu",
        "Beibin Li",
        "Erkang Zhu",
        "Li Jiang",
        "Xiaoyun Zhang",
        "Shaokun Zhang",
        "Jiale Liu"
      ],
      "year": "2024",
      "venue": "First Conference on Language Modeling"
    },
    {
      "citation_id": "53",
      "title": "MCPR: a Chinese product review dataset for multimodal aspect-based sentiment analysis",
      "authors": [
        "Carol Xu",
        "Xuan Luo",
        "Dan Wang"
      ],
      "year": "2022",
      "venue": "International Conference on Cognitive Computing"
    },
    {
      "citation_id": "54",
      "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
      "authors": [
        "Can Xu",
        "Qingfeng Sun",
        "Kai Zheng",
        "Xiubo Geng",
        "Pu Zhao",
        "Jiazhan Feng",
        "Chongyang Tao",
        "Daxin Jiang"
      ],
      "year": "2023",
      "venue": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
      "arxiv": "arXiv:2304.12244[cs.CL"
    },
    {
      "citation_id": "55",
      "title": "Multi-interactive memory network for aspect based multimodal sentiment analysis",
      "authors": [
        "Nan Xu",
        "Wenji Mao",
        "Guandan Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "56",
      "title": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
      "authors": [
        "An Yan",
        "Zhengyuan Yang",
        "Junda Wu",
        "Wanrong Zhu",
        "Jianwei Yang",
        "Linjie Li",
        "Kevin Lin",
        "Jianfeng Wang",
        "Julian Mcauley",
        "Jianfeng Gao"
      ],
      "year": "2024",
      "venue": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
      "arxiv": "arXiv:2404.16375"
    },
    {
      "citation_id": "57",
      "title": "Large Language Models as Analogical Reasoners",
      "authors": [
        "Michihiro Yasunaga",
        "Xinyun Chen",
        "Yujia Li",
        "Panupong Pasupat",
        "Jure Leskovec",
        "Percy Liang",
        "Ed Chi",
        "Denny Zhou"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "58",
      "title": "CMU-MOSEAS: A multimodal language dataset for Spanish, Portuguese, German and French",
      "authors": [
        "Amir Zadeh",
        "Yan Sheng Cao",
        "Simon Hessner",
        "Paul Liang",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "59",
      "title": "Social-iq: A question answering benchmark for artificial social intelligence",
      "authors": [
        "Amir Zadeh",
        "Michael Chan",
        "Paul Liang",
        "Edmund Tong",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "61",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "62",
      "title": "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
      "authors": [
        "Yanzhe Zhang",
        "Ruiyi Zhang",
        "Jiuxiang Gu",
        "Yufan Zhou",
        "Nedim Lipka",
        "Diyi Yang",
        "Tong Sun"
      ],
      "year": "2023",
      "venue": "Llavar: Enhanced visual instruction tuning for text-rich image understanding",
      "arxiv": "arXiv:2306.17107"
    },
    {
      "citation_id": "63",
      "title": "Svit: Scaling up visual instruction tuning",
      "authors": [
        "Bo Zhao",
        "Boya Wu",
        "Muyang He",
        "Tiejun Huang"
      ],
      "year": "2023",
      "venue": "Svit: Scaling up visual instruction tuning",
      "arxiv": "arXiv:2307.04087"
    },
    {
      "citation_id": "64",
      "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
      "authors": [
        "Swaroop Huaixiu Steven Zheng",
        "Xinyun Mishra",
        "Heng-Tze Chen",
        "Ed Cheng",
        "Chi",
        "Denny Quoc V Le",
        "Zhou"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "65",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}