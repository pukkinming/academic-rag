{
  "paper_id": "2508.09202v2",
  "title": "Personalized Feature Translation For Expression Recognition: An Efficient Source-Free Domain Adaptation Method",
  "published": "2025-08-08T20:13:50Z",
  "authors": [
    "Masoumeh Sharafi",
    "Soufiane Belharbi",
    "Houssem Ben Salem",
    "Ali Etemad",
    "Alessandro Lameiras Koerich",
    "Marco Pedersoli",
    "Simon Bacon",
    "Eric Granger"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression recognition (FER) models are employed in many video-based affective computing applications, such as human-computer interaction and healthcare monitoring. However, deep FER models often struggle with subtle expressions and high inter-subject variability, limiting their performance in real-world applications. To improve their performance, source-free domain adaptation (SFDA) methods have been proposed to personalize a pretrained source model using only unlabeled target domain data, thereby avoiding data privacy, storage, and transmission constraints. This paper addresses a challenging scenario where source data is unavailable for adaptation, and only unlabeled target data consisting solely of neutral expressions is available. SFDA methods are not typically designed to adapt using target data from only a single class. Further, using models to generate facial images with non-neutral expressions can be unstable and computationally intensive. In this paper, personalized feature translation (PFT) is proposed for SFDA. Unlike current image translation methods for SFDA, our lightweight method operates in the latent space. We first pre-train the translator on the source domain data to transform the subjectspecific style features from one source subject into another. Expression information is preserved by optimizing a combination of expression consistency and style-aware objectives. Then, the translator is adapted on neutral target data, without using source data or image synthesis. By translating in the latent space, PFT avoids the complexity and noise of face expression generation, producing discriminative embeddings optimized for classification. Using PFT eliminates the need for image synthesis, reduces computational overhead (using a lightweight translator), and only adapts part of the model, making the method efficient compared to imagebased translation. Extensive experiments on four challenging video FER benchmark datasets, BioVid, StressID, BAH, and Aff-Wild2, show that PFT consistently outperforms state-of-the-art SFDA methods, providing a cost-effective approach that is suitable for real-world, privacy-sensitive FER applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "FER plays an important role in video-based affective computing, enabling systems to interpret the emotional or health states of humans through non-verbal cues  (Calvo and D'Mello 2010; Ko 2018) . Its applications range from human-computer interaction  (Pu and Nie 2023) , to health monitoring  (Gaya-Morey et al. 2025) , and clinical assessment of pain, depression and stress  (Calvo and D'Mello 2010) . Despite recent advances in deep learning  (Barros, Parisi, and Wermter 2019; Sharafi et al. 2022; Sharafi, Yazdchi, and Rasti 2023)  and the availability of large annotated datasets for training  (Walter et al. 2013; Kollias and Zafeiriou 2019) , deep FER models may perform poorly when deployed on data from new users and environments. This is due to the mismatch between the training (source domain) data distribution and the testing (target domain) data due to the data collection environments. Beyond variations in capture conditions, data distributions may differ significantly across subjects. Inter-subject variability  (Zeng, Shan, and Chen 2018; Martinez 2003 ) can degrade the accuracy and robustness of deep FER models in real-world applications  (Li and Deng 2020a; Zhao, Chu, and Zhang 2016) .\n\nFor improved performance, this paper focuses on subjectbased adaptation or personalization of deep FER models to data from target subjects. Various unsupervised domain adaptation (UDA) methods have been proposed to address the distribution shifts by aligning feature distributions  (Li and Deng 2018; Zhu, Sang, and Zhao 2016; Chen et al. 2021; Li and Deng 2020b) . However, they typically require access to labeled source data during adaptation, a constraint that is often infeasible in privacy-sensitive application areas like healthcare due to concerns for data privacy, data storage, and computation costs. This has led to the emergence of SFDA, where adaptation of a pretrained source model is performed using only unlabeled target data  (Liang, Hu, and Feng 2020; Tang et al. 2024; Guichemerre et al. 2024 ). These methods  (Fang et al. 2024; Li et al. 2024 ) can be broadly categorized into (1) model-based approaches, which adapt the model parameters, using target domain statistics or pseudo-labels, and (2) data-based approaches, which instead operate at the data level by translating target images into the source domain style, enabling inference through the frozen source model without modifying its parameters.\n\nState-of-the-art SFDA methods assume access to data from all target classes, which is not practical in real-world FER applications. Indeed, person-specific data representing non-neutral expressions is typically costly or unavailable. A short neutral control video may, however, be collected for target individuals and used to personalize a model to the variability of an individual's diverse expressions. In practice, collecting and annotating neutral target data for adaptation is generally easier and less subjective than gathering non-neutral emotional data. Some recent work  (Sharafi et al. 2025)  relies on GANs to generate expressions from neutral inputs, but relies on image-level disentanglement of identity and expression, which is unstable and computationally expensive in practice. This limitation reduces the effectiveness of direct adaptation or fine-tuning methods, especially those based on pseudo-labeling. Alternative strategies translate target data into the source domain style, enabling inference without modifying the source model  (Fang et al. 2024) . This avoids adapting the parameters of the source classifier and enables direct inference with the frozen model, improving stability, efficiency, and privacy. Following this direction, several SFDA methods leverage generative models to translate target inputs into source-style images, guided by the source model  (Hou and Zheng 2021a,b) . However, these methods are not adapted for subject-specific adaptation of FER models. They consider the source as a single domain and often suppress important subject-specific cues for personalized FER. They also depend on expressive target data, which is rarely available in practice, making generative training infeasible in limited-data settings.\n\nTo address the limitations of image translation methods for SFDA, we propose a personalized feature translation (PFT) approach that explicitly models subject-specific variation within the source domain. We introduce PFT, a simple yet effective feature translation method for source-free personalization in FER. The key idea is to pre-train a translator network that maps features from one source subject to another while preserving the underlying expression. This subject-swapping objective encourages the model to capture intra-class, inter-subject variability, learning the structural relationship between expression and identity-specific features within the source domain. During adaptation, only a small subset of the translator's parameters is fine-tuned using expression-preserving from the target subject, enabling lightweight, stable, and computationally efficient personalization. As illustrated in Figure  1 , state-of-the-art image translation methods for SFDA operate by translating target inputs into the visual style of the source domain at the pixel level, which requires complex generative models and introduces additional instability and computational cost. In contrast, our proposed PFT method performs personalized feature-level translation by operating directly in the feature space. Specifically, it maps the target subject's features to those of the most similar source subject in the feature space, enabling subject alignment while preserving expression, without requiring pixel-level synthesis. Table  5  summarizes the comparison between SFDA-IT and PFT on the BioVid dataset. Despite using 100× fewer trainable parameters and 17× fewer FLOPs, PFT achieves substantially higher accuracy, demonstrating its efficiency and suitability for real-world deployment under privacy and resource constraints. Our contributions. (1) We propose a personalized feature translation (PFT) method for SFDA in FER using only target images with neutral expressions. Unlike image translation methods that require expressive target data and generative models, our approach translates features across subjects while preserving expression semantics. Adaptation is performed in the feature space with only a small subset of parameters, and significantly reducing computational complexity. (2) Style-aware and expression consistency losses are proposed to guide the translation process without requiring expressive target data. Our method requires only a few neutral samples for lightweight adaptation, introduces no additional parameters at inference time, and ensures fast, stable deployment. (3) We conduct extensive experiments on four video FER benchmarks, BioVid (pain estimation), StressID (stress recognition), BAH (ambivalencehesitancy recognition), and Aff-Wild2 (basic expression classification), and show that our PFT achieves performance that is comparable or higher than state-of-theart SFDA (pseudo-labeling and image translation) methods, with lower computational complexity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Facial Expression Recognition FER aims to identify human emotional states from facial images or video sequences. To enhance generalization, UDA methods  (Feng et al. 2023; Cao, Long, and Wang 2018; Chen et al. 2021; Ji et al. 2019; Li and Deng 2020b)  and multi-source domain adaptation (MSDA) techniques  (Zhou et al. 2024 ) align distri- butions between source and target domains using unlabeled target data. While effective, these approaches typically require access to source data during adaptation. Personalized FER methods  (Yao, Zhang, and Qian 2021; Kollias et al. 2020 ) adapt models to individual users but rely on labeled data per user. More recent subject-aware adaptation frameworks  (Zeeshan et al. 2024 (Zeeshan et al. , 2025) )  treat each subject as a domain and adapt across users, yet still depend on source data. These constraints motivate the need for SFDA, which enables model personalization without accessing source samples, offering a more practical solution for privacy-sensitive FER applications. Source-Free Domain Adaptation and Personalization SFDA addresses privacy, computational and storage concerns by adapting a pre-trained source model to an unlabeled target domain without access to source data. Common strategies include self-supervised learning  (Yang et al. 2021; Litrico, Del Bue, and Morerio 2023) , pseudolabeling  (Liang, Hu, and Feng 2020) , entropy minimization  (Liang, Hu, and Feng 2020) , and feature alignment via normalization or auxiliary modules  (Li et al. 2016; Liang et al. 2022; Kim et al. 2021) . SHOT  (Liang, Hu, and Feng 2020)  and DINE  (Liang et al. 2022 ) exemplify efficient adaptation via classifier tuning or BatchNorm statistics. However, these methods often assume confident predictions and smooth domain shifts, which are frequently violated in FER due to high inter-subject variability and subtle expression differences. FER-specific adaptations such as CluP  (Conti et al. 2022)  and FAL  (Zheng et al. 2025 ) address label noise and pseudo-label instability, yet challenges remain when only neutral target expressions are available. DSFDA  (Sharafi et al. 2025 ) tackles this by disentangling identity and expression using generative models, but its re-liance on adversarial training and multi-stage pipelines limits scalability and robustness in practical deployment. Feature Translation for SFDA Image translation is a common SFDA strategy that maps target images into the source style using generative models, allowing frozen source models to generalize without access to source data  (Hou and Zheng 2021a,b; Kurmi, Subramanian, and Namboodiri 2021; Qiu et al. 2021; Tian et al. 2021; Ding et al. 2022) . While effective in general tasks, these methods face critical limitations in FER and personalization. FER requires preserving subtle expression cues and identity-specific features, which are often distorted by image synthesis. Moreover, GAN-based approaches are computationally intensive, prone to instability, and assume access to expressive target samples, an unrealistic assumption in neutral-only personalization settings. To address these challenges, we propose translating features instead of images, using a compact, self-supervised translator that maps target features into the source-aligned space without requiring adversarial training, source data, or expressive target inputs, offering a stable and efficient solution for source-free FER personalization.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Figure  2  illustrates the overall framework of our PFT method. The source model is comprised of a feature extractor backbone and classifier head, both frozen during adaptation. To adapt this model to a new target subject with only a few neutral images extracted from a video, we introduce a translator network, a copy of the source encoder equipped with lightweight adaptation layers after the feature extractor. The translator is pretrained on source data using a subjectswapping objective: translating features between source sub-jects while maintaining expression labels. This enables the model to capture subject-specific information and preserve expression, facilitating efficient adaptation. Architecture: Let D S = {(x s , y s )} be a labeled source dataset, where x s is a source subject and y s ∈ Y its corresponding expression label. Let D T = {x t } denote the unlabeled dataset for a target subject. We denote by F the source feature extractor and by C the classifier head. The translator network is defined as the composition of F followed by a set of lightweight, subject-adaptive layers T. Thus, the translator T full = T•F takes an image as input and outputs a translated feature representation. The source classifier (F, C) is trained on D S and remains frozen during adaptation. The translator is first pretrained on D S to learn identity transformation while preserving expression, and then adapted to each target subject individually using only a few samples.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Source Pre-Training",
      "text": "The objective of source training is two-fold: (1) to train a robust expression classifier on labeled source data, and (2) to pre-train a translator network that can swap identity across subjects while preserving expression. The translator will later be adapted to a target subject using only a few samples. The source classifier consists of a feature extractor F and a classifier head C, trained jointly on the source dataset D S = {(x s , y s )} using the standard cross-entropy loss:\n\nThe translator is defined as T full = T • F, where T denotes a set of lightweight, subject-adaptive layers. To pretrain the translator, we form pairs of source images (x 1 , x 2 ) from different subjects. The first subject x 1 provides the expression to be preserved, while x 2 serves as the identity subject. We extract features from both using the frozen source encoder:\n\nThe translator is optimized with three objectives. First, to preserve the expression of the content image, we minimize the KL divergence between classifier predictions on the original and translated features:\n\nSecond, to encourage the translated feature to adopt the identity of the reference, we align their channel-wise statistics across selected early layers L. Unlike traditional style losses that rely on stored batch normalization statistics, we compute the mean and standard deviation directly from the feature maps to capture low-level identity cues such as shape and texture. This yields the following style alignment loss:\n\n(4) Finally, since the expression label y 1 for x 1 is available, we also apply a classification loss on the translated feature:\n\nThe total loss used for translator pretraining is a weighted combination of the three terms:\n\nwhere λ expr and λ style control the relative contributions of expression preservation and identity alignment.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Target Adaptation And Inference",
      "text": "Given a small number of video frames from a single target subject, we adapt the translator T full = T • F while keeping the source classifier (F, C) fixed. The adaptation is performed independently for each subject and only updates the lightweight adaptive layers T. Since all target samples come from the same identity, aligning identity features becomes unnecessary. Therefore, the adaptation objective focuses solely on preserving expression.\n\nFor each target frame x t , features are extracted using the frozen source feature extractor, and apply the current translator:\n\nTo preserve expression, we enforce consistency between the classifier outputs before and after translation using KL divergence:\n\nThis loss encourages the adapted translator to retain expression while adapting its identity representation to one of the source subjects. The process is lightweight, requires only a few neutral video frames, and enables subject-personalized FER without source data.\n\nAfter adaptation to each subject, the personalized translator T full = T • F is used for inferences. Given a new video frame x t from the same target subject, the translator maps its features to those of the most similar source subject in the feature space, preserving expression while aligning identity. The translated features are then classified using the frozen source classifier. This setup enables personalized inference without requiring target labels or access to source data during testing.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "This section presents a comprehensive evaluation of our feature translation-based approach for SFDA for personalization of deep FER models. Performance of our method is reported across multiple settings, highlighting the benefits of expression and style consistency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Methodology",
      "text": "Datasets: -BioVid Heat and Pain (Part A): This dataset  (Walter et al. 2013 ) consists of video recordings of 87 subjects experiencing thermal pain stimuli in a controlled laboratory setting. Each subject is assigned to one of five pain categories: \"no pain\" and four increasing pain levels (PA1-PA4), with PA4 representing the highest intensity. Consistent with prior work, which reports minimal facial activity at lower intensities, we focus on a binary classification between \"no pain\" and PA4. For each subject, 20 videos per class are used, each lasting 5.5 seconds. Following recommendations in  (Werner, Al-Hamadi, and Walter 2017) , we the first 2 seconds of each PA4 video is discard to eliminate frames where facial expressions are typically absent, retaining only the segments that capture stronger pain-related facial activity.\n\n-StressID: This dataset  (Chaptoukaev et al. 2023 ) focuses on assessing stress through facial expressions. It comprises facial video recordings from 54 individuals, totaling around 918 minutes of annotated visual content. In our work, we use only the visual modality. Each frame is labeled as either \"neutral\" or \"stressed,\" based on participants' selfreported stress scores. Specifically, frames corresponding to scores below 5 are labeled as neutral (label 0), while those with scores of 5 or higher are considered stressed (label 1).\n\n-BAH: The BAH dataset (González-González et al. 2025), which is designed for recognizing ambivalence and hesitancy (A/H) expressions in real-world video recordings.comprises facial recordings from 224 participants across Canada, designed to reflect a diverse demographic distribution in terms of sex, ethnicity, and province. Each participant contributes up to seven videos, with a total of 1,118 videos ( 86.2 hours). Among these, 638 videos contain at least one A/H segment, resulting in a total of 1,274 annotated A/H segments. The dataset includes 143,103 frames labeled with A/H, out of 714,005 total frames. In our setup, frames with A/H annotations are assigned a label of 1 (indicating the presence of A or H), while all other frames are considered neutral and assigned a label of 0.\n\n-Aff-Wild2 The Aff-Wild2 dataset (Kollias and Zafeiriou 2019) is a large-scale in-the-wild dataset for affect recognition, consisting of 318 videos with available annotations. In our study, we use a subset of 292 videos that each represent a single subject, which is essential for our subjectbased setting, where each individual is treated as a separate domain. We focus exclusively on basic expression categories for discrete expression classification. Specifically, we use the following seven classes: neutral (0), anger (1), disgust (2), fear (3), happiness (4), sadness (5), and surprise (6). We consider only the visual modality in our experiments.\n\nProtocol: In experiments, each subject is viewed as an independent target domain. In the BioVid, BAH, Aff-Wild2, and StressID datasets, we randomly select 10 subjects to serve as target domains, while the remaining subjects are used to construct the source domain. To ensure meaningful evaluation and generalizability, the selected target subjects represent a diverse mix of age and gender. This subjectspecific setup reflects real-world personalization scenarios and enables assessment under inter-subject variability. During adaptation, we assume access only to neutral expression data from the target subjects. No source data are available at this stage, consistent with the SFDA setting. We evaluate performance under the following four settings. Source-Only. The model is trained on labeled source-domain data and directly evaluated on target subjects without any adaptation. This serves as a lower-bound baseline, illustrating the effect of domain shift. SFDA. SFDA methods adapt a pretrained source model using only target-domain data. These approaches typically rely on fine-tuning parts of the model to reduce domain shift. SFDA w/ Translation. These methods avoid updating the source model by aligning target data to the source domain through translation. Image-level approaches perform pixel-space synthesis to generate expressive target images in the source style. Oracle. The model is fine-tuned on labeled target-domain data, including both neutral and expressive samples. While infeasible in sourcefree settings, this provides an upper-bound reference for performance.\n\nImplementation Details: We implement our model using PyTorch and conduct all experiments on a single NVIDIA A100-SXM4-40GB GPU. The source classifier is built on a ResNet-18 backbone, followed by a classifier trained for binary expression recognition. We select ResNet-18 as the feature extractor due to its widespread adoption in prior FER and domain adaptation works. During target adaptation, only the subject-adaptive layers of the translator are updated; the source backbone and classifier remain fixed. We train the model using the Adam optimizer with a learning rate of 1 × 10 -3 and a batch size of 64. We use the learning rate scheduler (ReduceLROnPlateau), which monitors the validation loss and reduces the learning rate by a factor of 0.5 if no improvement is observed for 3 consecutive epochs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "The proposed PFT is compared with several SFDA stateof-the-art methods across four target datasets: BioVid, StressID, BAH, and Aff-Wild2. The first two are labcontrolled, while the latter two capture spontaneous expressions in-the-wild. Detailed results for Aff-Wild2 are included in the supp. material due to space constraints. In Table2, PFT achieves the highest accuracy across most target subjects, outperforming all SFDA baselines and image translation methods. It reaches an average accuracy of 82.46%, which is a clear margin above DSFDA (80.24%) and SFDA-IT (76.14%). Compared to the source-only baseline, the improvement exceeds +14 points. Despite using only neutral target data, PFT approaches the oracle's performance achieved through supervised fine-tuning (91.59%).\n\nIn Table  3 , PFT again ranks first, achieving 79.49% on average. It improves over the strongest non-translational baseline (DSFDA: 78.61%) and the prior image translation method SFDA-IT (71.18%). These results highlight the effectiveness of identity alignment and subject-specific adaptation. Performance on the in-the-wild benchmark shown in Table  4  demonstrates the robustness of PFT under severe appearance and expression variability. It achieves 62.09% average accuracy, outperforming DSFDA (58.36%) and SFDA-IT (57.26%), and delivering substantial gains over the source-only model (50.03%). This shows that PFT generalizes well to uncontrolled environments.\n\nTable  5  compares image translation against GAN-based SFDA methods. SFDA-IT relies on image translation and requires a large number of trainable parameters, while DSFDA employs generative adversarial training, resulting in high  In contrast, our method achieves higher performance with significantly fewer parameters and much faster adaptation. This demonstrates that PFT offers a more efficient and scalable alternative to pixel-level translation or GAN-based approaches by operating entirely in the feature space.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "Impact of Feature Vector Size on Performance. We conducted an ablation study to investigate the impact of feature dimensionality on the performance of feature translation across four FER datasets: BioVid, StressID, BAH, and Aff-Wild2. For each dataset, we varied the dimensionality of the translated feature vector from 64 to 512 and observed consistent improvements in accuracy with increasing dimensionality. Notably, the performance gains saturated around 256 or 512 dimensions, suggesting that higherdimensional features provide richer identity and expression",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We presented PFT, an efficient SFDA method tailored for personalization FER sing only neutral expressions from the target subjects. Unlike traditional image-based approaches that depend on expressive target data and computationally expensive generative models, PFT operates entirely in the feature space. It translates features from one subject to another in the source domain by aligning subject-specific features while preserving the expression of the original subject. This allows the model to maintain the expression of the input while adapting it to the source subject, enabling effective personalization without requiring target expression data. Crucially, the adaptation process in PFT involves adapting only a few layers of the translator module on the target subject's neutral data. This makes PFT highly computationally efficient, stable during training, and well-suited for deployment in privacy-sensitive real-world scenarios such as healthcare or mobile applications. Experiments across four FER datasets demonstrate that PFT achieves better performance with lower complexity, generalizing well across both controlled and in-the-wild conditions.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of a standard image translation vs. our",
      "page": 1
    },
    {
      "caption": "Figure 1: , state-of-the-art image",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of the proposed PFT method. Top-left: During pre-training, the feature translator (T) is trained to map",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the overall framework of our PFT",
      "page": 3
    },
    {
      "caption": "Figure 3: Classification accuracy across feature dimensions",
      "page": 6
    },
    {
      "caption": "Figure 4: Comparison of target accuracy across 10 subjects",
      "page": 7
    },
    {
      "caption": "Figure 3: , highlighting the importance of se-",
      "page": 7
    },
    {
      "caption": "Figure 4: and Figure 5, both cosine-",
      "page": 7
    },
    {
      "caption": "Figure 6: , the source-only model (a) shows over-",
      "page": 7
    },
    {
      "caption": "Figure 5: Examples of source subject pairs from the",
      "page": 7
    },
    {
      "caption": "Figure 6: t-SNE visualizations of feature embeddings for tar-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source-only": "SFDA",
          "Source model (no adaptation)": "SHOT (Liang, Hu, and Feng 2020)\nNRC (Yang et al. 2021)\nTPDS (Tang et al. 2024)\nDSFDA (Sharafi et al. 2025)",
          "66.11\n55.55\n86.36\n85.11\n87.11\n59.00\n75.66\n70.44\n52.66\n48.22": "55.78\n47.76\n41.05\n52.44\n54.67\n48.89\n54.46\n49.22\n54.86\n44.44\n59.33\n39.38\n84.00\n72.89\n79.67\n42.67\n63.92\n53.95\n54.89\n42.47\n65.56\n55.98\n79.22\n68.22\n91.67\n59.11\n61.28\n69.33\n57.11\n48.22\n87.56\n77.00\n75.11\n90.89\n85.67\n88.11\n67.48\n89.22\n69.22\n72.11",
          "68.62": "50.35\n60.31\n65.57\n80.24"
        },
        {
          "Source-only": "SFDA\nw/ Translation",
          "Source model (no adaptation)": "SFIT (Hou and Zheng 2021b)\nSFDA-IT (Hou and Zheng 2021a)\nPFT (ours)",
          "66.11\n55.55\n86.36\n85.11\n87.11\n59.00\n75.66\n70.44\n52.66\n48.22": "79.94\n80.92\n68.79\n82.87\n84.70\n91.62\n57.32\n60.59\n74.56\n60.71\n75.33\n67.27\n89.00\n84.55\n90.80\n62.31\n81.77\n75.89\n57.88\n70.56\n84.93\n75.56\n95.05\n85.86\n97.59\n73.78\n88.73\n78.48\n83.49\n61.16",
          "68.62": "74.20\n75.54\n82.46"
        },
        {
          "Source-only": "Oracle",
          "Source model (no adaptation)": "Fine-Tuning",
          "66.11\n55.55\n86.36\n85.11\n87.11\n59.00\n75.66\n70.44\n52.66\n48.22": "97.11\n91.43\n96.76\n97.89\n96.11\n92.30\n90.01\n95.09\n98.22\n97.00",
          "68.62": "95.19"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source-only": "SFDA",
          "Source model (no adaptation)": "SHOT (Liang, Hu, and Feng 2020)\nNRC (Yang et al. 2021)\nTPDS (Tang et al. 2024)\nDSFDA (Sharafi et al. 2025)",
          "38.96\n41.21\n65.53\n42.04\n55.16\n65.51\n69.43\n60.78\n53.62\n55.63": "68.33\n51.95\n45.83\n39.26\n53.67\n61.38\n59.76\n45.25\n51.42\n52.05\n69.03\n52.25\n31.83\n35.29\n59.67\n42.50\n59.28\n41.25\n65.42\n54.20\n65.56\n54.98\n64.22\n58.22\n54.67\n63.11\n69.28\n59.33\n50.11\n51.98\n87.12\n75.03\n73.47\n69.39\n69.74\n79.87\n87.39\n82.80\n83.89\n77.39",
          "54.79": "52.88\n51.07\n59.17\n78.61"
        },
        {
          "Source-only": "SFDA\nw/ Translation",
          "Source model (no adaptation)": "SFIT (Hou and Zheng 2021b)\nSFDA-IT (Hou and Zheng 2021a)\nPFT (ours)",
          "38.96\n41.21\n65.53\n42.04\n55.16\n65.51\n69.43\n60.78\n53.62\n55.63": "70.41\n68.85\n69.67\n71.92\n67.48\n77.43\n70.76\n75.21\n65.98\n61.19\n73.47\n69.50\n69.90\n73.02\n66.54\n78.62\n71.30\n76.67\n65.42\n67.32\n78.33\n74.87\n73.32\n79.96\n89.00\n84.76\n84.14\n77.95\n78.17\n74.42",
          "54.79": "69.89\n71.18\n79.49"
        },
        {
          "Source-only": "Oracle",
          "Source model (no adaptation)": "Fine-Tuning",
          "38.96\n41.21\n65.53\n42.04\n55.16\n65.51\n69.43\n60.78\n53.62\n55.63": "98.89\n100\n99.53\n98.15\n99.22\n97.57\n96.02\n99.38\n99.56\n100",
          "54.79": "98.83"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , PFT again ranks first, achieving 79.49% on",
      "data": [
        {
          "Source-only": "SFDA",
          "Source model": "SHOT (Liang, Hu, and Feng 2020)\nNRC (Yang et al. 2021)\nTPDS (Tang et al. 2024)\nDSFDA (Sharafi et al. 2025)",
          "49.71\n50.00\n54.67\n47.71\n48.43\n51.51\n48.83\n50.30\n50.45\n48.65": "60.49\n49.73\n54.17\n49.55\n45.83\n51.22\n46.62\n52.76\n49.09\n47.92\n49.46\n54.05\n55.02\n49.11\n46.52\n49.91\n44.83\n52.26\n48.63\n47.24\n50.42\n52.38\n55.91\n48.75\n47.67\n51.66\n44.83\n53.20\n51.75\n55.13\n57.40\n61.24\n56.02\n60.31\n58.77\n54.19\n59.88\n53.16\n62.15\n60.49",
          "50.03": "50.74\n49.70\n51.17\n58.36"
        },
        {
          "Source-only": "SFDA\nw/ Translation",
          "Source model": "SFIT (Hou and Zheng 2021b)\nSFDA-IT (Hou and Zheng 2021a)\nPFT (Ours)",
          "49.71\n50.00\n54.67\n47.71\n48.43\n51.51\n48.83\n50.30\n50.45\n48.65": "60.15\n56.84\n60.04\n54.91\n56.15\n55.73\n56.02\n56.49\n56.22\n58.39\n60.00\n60.12\n56.48\n55.73\n56.15\n57.42\n56.80\n55.96\n56.89\n57.05\n69.46\n64.17\n60.49\n62.11\n59.83\n61.91\n57.76\n62.63\n67.92\n54.62",
          "50.03": "57.09\n57.26\n62.09"
        },
        {
          "Source-only": "Oracle",
          "Source model": "Fine-tune",
          "49.71\n50.00\n54.67\n47.71\n48.43\n51.51\n48.83\n50.30\n50.45\n48.65": "93.35\n96.61\n99.22\n95.58\n99.17\n97.89\n92.48\n96.14\n93.07\n93.38",
          "50.03": "95.69"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "OpenFace: An open source facial behavior analysis toolkit",
      "authors": [
        "B Amos",
        "B Ludwiczuk",
        "M Satyanarayanan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "2",
      "title": "A personalized affective memory model for improving emotion recognition",
      "authors": [
        "P Barros",
        "G Parisi",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised domain adaptation with distribution matching machines",
      "authors": [
        "Y Cao",
        "M Long",
        "J Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "5",
      "title": "Stressid: a multimodal dataset for stress identification",
      "authors": [
        "H Chaptoukaev",
        "V Strizhkova",
        "M Panariello",
        "B Dalpaos",
        "A Reka",
        "V Manera",
        "S Thümmler",
        "E Ismailova",
        "M Todisco",
        "M Zuluaga"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Cross-domain facial expression recognition: A unified evaluation benchmark and adversarial graph learning",
      "authors": [
        "T Chen",
        "T Pu",
        "H Wu",
        "Y Xie",
        "L Liu",
        "L Lin"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "7",
      "title": "Clusterlevel pseudo-labelling for source-free cross-domain facial expression recognition",
      "authors": [
        "A Conti",
        "P Rota",
        "Y Wang",
        "E Ricci"
      ],
      "year": "2022",
      "venue": "Proceedings in the British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "8",
      "title": "Source-free domain adaptation via distribution estimation",
      "authors": [
        "N Ding",
        "Y Xu",
        "Y Tang",
        "C Xu",
        "Y Wang",
        "D Tao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Source-free unsupervised domain adaptation: A survey",
      "authors": [
        "Y Fang",
        "P.-T Yap",
        "W Lin",
        "H Zhu",
        "M Liu"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "Unsupervised domain adaptation for medical image segmentation by selective entropy constraints and adaptive semantic alignment",
      "authors": [
        "W Feng",
        "L Ju",
        "L Wang",
        "K Song",
        "X Zhao",
        "Z Ge"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review",
      "authors": [
        "F Gaya-Morey",
        "J Buades-Rubio",
        "P Palanque",
        "R Lacuesta",
        "C Manresa-Yee"
      ],
      "year": "2025",
      "venue": "Deep Learning-Based Facial Expression Recognition for the Elderly: A Systematic Review",
      "arxiv": "arXiv:2502.02618"
    },
    {
      "citation_id": "12",
      "title": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change",
      "authors": [
        "M González-González",
        "S Belharbi",
        "M Zeeshan",
        "M Sharafi",
        "M Aslam",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2025",
      "venue": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change",
      "arxiv": "arXiv:2505.19328"
    },
    {
      "citation_id": "13",
      "title": "Source-free domain adaptation of weakly-supervised object localization models for histology",
      "authors": [
        "A Guichemerre",
        "S Belharbi",
        "T Mayet",
        "S Murtaza",
        "P Shamsolmoali",
        "L Mccaffrey",
        "E Granger"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Visualizing adapted knowledge in domain transfer",
      "authors": [
        "Y Hou",
        "L Zheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
      "arxiv": "arXiv:2008.07514"
    },
    {
      "citation_id": "15",
      "title": "Cross-domain facial expression recognition via an intracategory common feature and inter-category distinction feature fusion network",
      "authors": [
        "Y Ji",
        "Y Hu",
        "Y Yang",
        "F Shen",
        "H Shen"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "16",
      "title": "Domain adaptation without source data",
      "authors": [
        "Y Kim",
        "D Cho",
        "K Han",
        "P Panda",
        "S Hong"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Dlib-ml: A Machine Learning Toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Dlib-ml: A Machine Learning Toolkit"
    },
    {
      "citation_id": "18",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "19",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Extending the Aff-Wild Database for Affect Recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Extending the Aff-Wild Database for Affect Recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "21",
      "title": "Domain impression: A source data free domain adaptation method",
      "authors": [
        "V Kurmi",
        "V Subramanian",
        "V Namboodiri"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "22",
      "title": "A comprehensive survey on source-free domain adaptation",
      "authors": [
        "J Li",
        "Z Yu",
        "Z Du",
        "L Zhu",
        "H Shen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Deep emotion transfer network for cross-database facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "24",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "A deeper look at facial expression dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Revisiting Batch Normalization For Practical Domain Adaptation",
      "authors": [
        "Y Li",
        "N Wang",
        "J Shi",
        "J Liu",
        "X Hou"
      ],
      "year": "2016",
      "venue": "Revisiting Batch Normalization For Practical Domain Adaptation",
      "arxiv": "arXiv:1603.04779"
    },
    {
      "citation_id": "27",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "28",
      "title": "Dine: Domain adaptation from single and multiple black-box predictors",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng",
        "R He"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Guiding pseudo-labels with uncertainty estimation for sourcefree unsupervised domain adaptation",
      "authors": [
        "M Litrico",
        "A Del Bue",
        "P Morerio"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Recognizing expression variant faces from a single sample image per class",
      "authors": [
        "A Martinez"
      ],
      "year": "2003",
      "venue": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Convolutional channel attentional facial expression recognition network and its application in human-computer interaction",
      "authors": [
        "J Pu",
        "X Nie"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "32",
      "title": "Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation",
      "authors": [
        "Z Qiu",
        "Y Zhang",
        "H Lin",
        "S Niu",
        "Y Liu",
        "Q Du",
        "M Tan"
      ],
      "year": "2021",
      "venue": "Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation",
      "arxiv": "arXiv:2106.15326"
    },
    {
      "citation_id": "33",
      "title": "Disentangled Source-Free Personalization for Facial Expression Recognition With Neutral Target Data",
      "authors": [
        "M Sharafi",
        "E Ollivier",
        "M Zeeshan",
        "S Belharbi",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "E Granger"
      ],
      "year": "2025",
      "venue": "2025 IEEE 19th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "34",
      "title": "Audio-visual emotion recognition using k-means clustering and spatiotemporal cnn",
      "authors": [
        "M Sharafi",
        "M Yazdchi",
        "J Rasti"
      ],
      "year": "2023",
      "venue": "2023 6th International Conference on Pattern Recognition and Image Analysis (IPRIA)"
    },
    {
      "citation_id": "35",
      "title": "A novel spatio-temporal convolutional neural framework for multimodal emotion recognition",
      "authors": [
        "M Sharafi",
        "M Yazdchi",
        "R Rasti",
        "F Nasimi"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "36",
      "title": "Source-free domain adaptation via target prediction distribution searching",
      "authors": [
        "S Tang",
        "A Chang",
        "F Zhang",
        "X Zhu",
        "M Ye",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "37",
      "title": "VDM-DA: Virtual domain modeling for source data-free domain adaptation",
      "authors": [
        "J Tian",
        "J Zhang",
        "W Li",
        "D Xu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "38",
      "title": "The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Ehleiter",
        "J Tan",
        "H Traue",
        "P Werner",
        "A Al-Hamadi",
        "S Crawcour",
        "A Andrade",
        "G Silva"
      ],
      "year": "2013",
      "venue": "IEEE"
    },
    {
      "citation_id": "39",
      "title": "Analysis of facial expressiveness during experimentally induced heat pain",
      "authors": [
        "P Werner",
        "A Al-Hamadi",
        "S Walter"
      ],
      "year": "2017",
      "venue": "2017 Seventh international conference on affective computing and intelligent interaction workshops and demos"
    },
    {
      "citation_id": "40",
      "title": "Exploiting the intrinsic neighborhood structure for sourcefree domain adaptation",
      "authors": [
        "S Yang",
        "J Van De Weijer",
        "L Herranz",
        "S Jui"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "41",
      "title": "Few-shot learning for personalized facial expression recognition",
      "authors": [
        "A Yao",
        "S Zhang",
        "R Qian"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Subjectbased domain adaptation for facial expression recognition",
      "authors": [
        "M Zeeshan",
        "M Aslam",
        "S Belharbi",
        "A Koerich",
        "M Pedersoli",
        "S Bacon",
        "E Granger",
        "M Pedersoli",
        "A Koerich",
        "E Grange"
      ],
      "year": "2024",
      "venue": "Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition",
      "arxiv": "arXiv:2504.04252"
    },
    {
      "citation_id": "43",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "44",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "45",
      "title": "Fuzzy-aware Loss for Source-free Domain Adaptation in Visual Emotion Recognition",
      "authors": [
        "Y Zheng",
        "Y Zhang",
        "Y Wang",
        "L.-P Chau"
      ],
      "year": "2025",
      "venue": "Fuzzy-aware Loss for Source-free Domain Adaptation in Visual Emotion Recognition"
    },
    {
      "citation_id": "46",
      "title": "Cycle selfrefinement for multi-source domain adaptation",
      "authors": [
        "C Zhou",
        "Z Wang",
        "B Du",
        "Y Luo"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Discriminative feature adaptation for cross-domain facial expression recognition",
      "authors": [
        "R Zhu",
        "G Sang",
        "Q Zhao"
      ],
      "year": "2016",
      "venue": "International Conference on Biometrics (ICB)"
    }
  ]
}