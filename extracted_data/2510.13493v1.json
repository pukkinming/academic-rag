{
  "paper_id": "2510.13493v1",
  "title": "Expressnet-Moe: A Hybrid Deep Neural Network For Emotion Recognition * A Preprint",
  "published": "2025-10-15T12:42:49Z",
  "authors": [
    "Deeptimaan Banerjee",
    "Prateek Gothwal",
    "Ashis Kumer Biswas"
  ],
  "keywords": [
    "Adaptive Learning",
    "Convolution Neural Networks",
    "Facial Emotion Recognition",
    "Mixture of Experts",
    "Generalization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In many domains, including online education, healthcare, security, and human-computer interaction, facial emotion recognition (FER) is essential. Real-world FER is still difficult despite its significance because of some factors such as variable head positions, occlusions, illumination shifts, and demographic diversity. Engagement detection, which is essential for applications like virtual learning and customer services, is frequently challenging due to FER limitations by many current models . In this article, we propose ExpressNet-MoE, a novel hybrid deep learning model that blends both Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to overcome the difficulties. Our model dynamically chooses the most pertinent expert networks, thus it aids in the generalization and providing flexibility to model across a wide variety of datasets. Our model improves on the accuracy of emotion recognition by utilizing multi-scale feature extraction to collect both global and local facial features. ExpressNet-MoE includes numerous CNN-based feature extractors, a MoE module for adaptive feature selection, and finally a residual network backbone for deep feature learning. To demonstrate efficacy of our proposed model we evaluated on several datasets, and compared with current state-of-the-art methods. Our model achieves accuracies of 74.77% on AffectNet 7 , 72.55% on AffectNet 8 , 84.29% on RAF-DB, and 64.66% on FER-2013. The results show how adaptive our model is and how it may be used to develop end-to-end emotion recognition systems in practical settings. Reproducible codes and results are made publicly accessible at https://github.com/DeeptimaanB/ExpressNet-MoE.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Facial Emotion Recognition (FER) has become a major component of virtual education  [8] , security  [9] , medical industry  [5] ,  [13] , and human-computer interfaces. The ability to sense and respond to human emotions has created new opportunities for developing more flexible and intelligent systems. However, since human facial expressions are inherently complex, FER in real-world scenarios is a difficult task. Variations in head position, occlusion, lighting conditions  [8] ,  [27] ,  [36]  and demographic variability frequently cause significant performance issues in emotion detection frameworks.\n\nThe performance of FER frameworks has improved significantly over time. There have been multiple ways for FER and its classification, including Local Binary Patterns (LBP) and Histogram of Oriented Gradients (HOG), combined with traditional machine learning algorithms such as Support Vector Machines (SVM)  [31] ,  [30]  and Random Forests  [23] ,  [28] . Convolution Neural Networks (CNNs) have played a pivotal role in learning hierarchical feature representations  [17] . Moreover, transfer learning-based models have further enhanced recognition of complex emotional expressions due to the fact that they are pre-trained on multiple datasets. However, there are still many challenges in the machine learning development pipeline, such as unbalanced datasets and intra-class variations, leading to limited generalization.\n\nHence, we propose ExpressNet-MoE, a novel Deep CNN architecture. It is a hybrid deep learning model that combines CNNs with a layer of mixture of experts (MoE) to overcome the aforementioned problems. This method dynamically selects the relevant expert networks for each input, while improving its versatility and generalization over a number of datasets. ExpressNet-MoE captures simultaneously both global and fine-grained facial features and improves the accuracy of the emotion recognition task. The hybrid architecture of the model offers the most adaptive feature selection capability, and the choice of multiple CNN-based feature extractors makes it a powerful yet flexible approach.\n\nThe contributions of our research include the following.\n\n• Adaptive feature learning: Many existing solutions employ static models offering only a fixed set of features.\n\nTo overcome the limitations of static feature extraction models, ExpressNet-MoE chooses a Mixture of Experts (MoE) system that emphasizes the best expert network in the model for each individual input.\n\n• Multi-scale feature extraction: Our model extracts both global and fine-grained facial expression characteristics using CNNs with different filter sizes. The model's capacity to identify minute emotional variations across datasets is enhanced by the choice of this hybrid method.\n\n• Improved generalization across multiple datasets and real-world scenarios: Our testing results showed that the model improves the generalization capability across different datasets due to its hybrid architecture. This allows the model to handle real-world issues such as illumination, occlusion, and demographic diversity.\n\nWe have evaluated ExpressNet-MoE on three benchmark FER datasets which are AffectNet  [22] , Real-world Affective Faces Database or RAF-DB  [19; 18] , and FER-2013  [10] . Each of these datasets has their own set of unique characteristics and problems.\n\nFurthermore, ExpressNet-MoE employs deep feature representations, which offers more data-driven manner due to its dynamic MoE and transfer learning architecture to comprehend user emotions than conventional fixed CNN-based solutions.\n\nThe remainder of this article is organized as follows: Section 2 reviews related works related to current state-of-the-art emotion recognition methods. Section 3 provides the datasets utilized for both training and evaluation of our proposed model, along with comparing its performance with existing work. In Sections 4 and 5, we present our proposed methodology and ExpressNet-MoE architecture, and its associated machine learning pipeline needed for end-to-end learning. Section 6 illustrates the experimental results of the proposed method separately for each of the datasets considered in this study. A comparative analysis is described in Section 7. Section 8 provides an insight on the model performance and efficacy considerations. Finally, section 9 summarizes the study, offers our conclusion and elaborates on future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Literature Review",
      "text": "Understanding facial emotions has become a key element in emotion detection frameworks, particularly in applications such as human-computer interaction and online learning. Recent research focuses have been on the improvement of FER model accuracy, scalability, and generalization. Zhang et al.  [36]  proposed a Dual-Direction Attention Mixed Feature Network (DDAN-MFN), which integrates a Mixed Feature Network (MFN) with a Dual-Direction Attention Network (DDAN). The MFN uses convolution and bottleneck layers along with MixConv, which uses several kernel sizes.\n\nThe DDAN has an independent dual-direction attention head to capture long-range dependence, which significantly enhances the ability of the model to highlight the enlightening features of the FER undertaking.\n\nBhati et al.  [4]  proposed the Generalized Zero-Shot Convolution Neural Network (GZS-ConvNet), which aims at addressing the problem of generalization in FER systems. This architecture is designed to detect unseen facial expression using a sophisticated adaptation mechanism that exhibits high performance on a variety of datasets including FER-2013, AffectNet, and RAF-DB. GZS-ConvNet's ability to perform zero-shot categorization makes it a valuable tool for dynamic real-world applications where new expressions are likely to appear. Similarly, Bohi et al.  [5]  proposed ConvNeXt, a CNN architecture that surpasses the classical CNN model and Transformer-based systems.\n\nThe architecture uses deep convolutions, depthwise convolutions, and a modified activation function. The model demonstrates competitive performance on the AffectNet dataset. Face2Nodes, a graph-based FER system presented by Jiang et al.  [16] , uses dynamic relation-aware graph convolutions to characterize spatial and relational relationships between face regions, allowing for more structured and expressive emotion representations. Their work demonstrates strong performance on the RAF-DB dataset.\n\nUniyal et al.  [34]  in their research explored techniques to avoid overfitting and improve generalization. They performed research on deep convolution layers for emotion categorization using strategies such as max pooling, dropout, and batch normalization. Their methods underline the importance of feature extraction and regularization to achieve high accuracy on large datasets identical to AffectNet and FER-2013. Similarly, Savchenko et al.  [29]  focused on the use of ensemble models, CNNs like VGG and ResNet, for emotion and engagement in distance learning.\n\nMultimodal data and attention mechanisms have been successfully integrated to improve FER systems. In order to enhance emotion recognition, Sun et al.  [33]  suggested a multi-modal sentimental privileged information embedding (IA-MTM) that integrates audio and picture characteristics. By using both visual and aural input, their model improves FER performance by using ResNet18 for image feature extraction and an audio-decoding network to create a shared feature space. To improve FER models, Wang et al.  [35]   FER systems have advanced significantly as outlined above. However, there are still a number of research gaps that prevent their practical applications. Due to variances in demography, lighting, and face features, many current models have trouble generalizing across datasets; they perform well on certain datasets but fall short when evaluated on unseen data  [36; 35; 15] . Conventional CNN-based FER models are limited in their capacity to adapt to a variety of inputs because they rely on set feature extractors, which might not be the best for all facial expressions  [34; 15] . Furthermore, it is challenging to capture both global face structures and fine-grained expression features since the majority of architectures do not integrate multi-scale feature extraction  [34; 35; 15] . Managing occlusions, light fluctuation, and non-frontal head poses-all of which impair model performance, this is another crucial issue in real-world FER.\n\nDataset imbalances also make it difficult to classify underrepresented emotions like disgust, fear, and contempt, which restricts the model's capacity to confidently identify uncommon expressions which is evident in  [5; 36; 35] . Lowresolution grayscale images and mislabeling problems in the popular dataset FER-2013 pose additional difficulties that can have a big influence on model learning and generalization. By utilizing a Mixture of Experts (MoE) framework for adaptive feature selection, incorporating multiple CNN-based extractors for multi-scale learning, and improving generalization across datasets by combining deep feature learning and ensemble-based decision-making, ExpressNet-MoE directly addresses these limitations. ExpressNet-MoE is a very flexible and scalable solution for real-world FER applications since it enhances robustness against occlusions, illumination shifts, and dataset bias by dynamically choosing expert networks based on input data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "We have used three benchmark datasets which are AffectNet, RAF-DB, and FER-2013 to train our model. Each of the aforementioned datasets have distinct qualities that will help improve robustness and generalizability of the model. AffectNet is perfect for learning complicated emotional variations since it offers a large collection of face expressions that have been taken in real-world situations. RAF-DB's extensive collection of extremely diverse photos with well-documented annotations will help the model to test its adaptability and generalizability. Furthermore, the grayscale photographs in FER-2013 were taken in a variety of settings to help the model adapt to variations in lighting and facial expressions.\n\nIn order to balance computational efficiency, and memory constraints, we employed subsets from AffectNet and RAF-DB.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Affectnet",
      "text": "AffectNet  [22]  is one of the biggest datasets for facial expression identification. It contains over a million photos and about 440,000 samples have annotations. We have chosen as subset of this dataset with 28,175 photos from eight different emotional categories from the mentioned dataset for this study which are surprise, happy, anger, disgust, neutral, fear, sad, and contempt. These photos are collected from online sources and are taken from actual real-life situations and contain a variety of facial expressions. Because of its vast size and computational constraints, a subset of this dataset is generally used by many researchers in their works  [4; 34; 29] .\n\nFor training, we have used 22,540 images, and 5,635 were set aside for testing.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Methodology",
      "text": "The proposed methodology involves training the model on three facial expression recognition datasets: AffectNet, RAF-DB, and FER-2013 (individually, to test generalizability). Two variations of AffectNet were taken into consideration: one with seven emotion classes with the \"contempt\" category eliminated -AffectNet 7 , and another with all the eight classes kept -AffectNet 8 . There are seven emotion classes in RAF-DB as in AffectNet 7 . To maintain compatibility with other datasets for the model's input, the grayscale images in FER-2013 were transformed into three channels.\n\nTo guarantee an equitable distribution of emotion categories in both training and testing sets, a stratified train-test split is used for all datasets. This keeps the model's performance from being distorted by data imbalance as much as possible. We will be using the same test split for evaluating our model as well as comparing our proposed model with the state-of-the-art methods.\n\nThe BlazeFace model  [3]  from MediaPipe, which effectively identifies facial areas in photos, is used for face identification and alignment. It is used by many researchers for preprocessing facial images  [12; 32; 2] . The identified faces are cropped after the relative bounding-box coordinates are calculated and gently adjusted for robustness. The original image is scaled to 224 × 224 pixels if no face is found. An image generator that dynamically loads photos, does preprocessing (using BlazeFace), normalizes pixel values, and one-hot encodes emotion labels is included into the data pipeline. To maximize training efficiency and avoid overfitting, the model is trained for 15 epochs with a batch size of 32 using methods such as checkpointing, learning rate adjustments, and early stopping. This methodology ensures effective learning across diverse datasets and also improving the model's generalization for facial expression recognition. Fig.  2 . shows the complete methodology.\n\nAccuracy, precision, recall, and F 1 -score are used to evaluate facial expression recognition algorithms. Accuracy measures the overall proportion of correct predictions, giving a general sense of how well the model performs. Precision focuses on how many of the predicted expressions were actually correct. Recall, on the other hand, assesses how many of the true expressions the model was able to identify. The F1-score combines precision and recall into a single value, and provides a balanced measure which is especially useful when a dataset is imbalanced. These metrics provide a thorough evaluation of the model's performance.  To preserve spatial dimensions, the network begins with a 75 × 75 convolution layer with 8 filters, ReLU activation, and \"same\" padding. The convolution operation is defined in Equation  1 , where Y (i, j) is the output feature at position (i, j), X(i + m, j + n) is the input or previous layer's output, and K(m, n) is the filter kernel.\n\nThe ReLU formula is given in Equation  2 . ReLU (Rectified Linear Unit) is a widely used activation function that outputs the input (x) if positive, or zero otherwise. It introduces non-linearity, it is computationally efficient, and helps prevent vanishing gradients. This enables deeper networks to train effectively.\n\nA dropout layer (rate = 0.1) follows to reduce overfitting, then batch normalization is applied to stabilize and accelerate training. Given input x i in a mini-batch of size m, batch normalization proceeds as follows:\n\nEquations 3 and 4 compute the batch mean m B and variance s 2 B for input x i , where m is the batch size. Equation 5 normalizes the input using the batch statistics, with ϵ preventing division by zero. Here, γ and β are learnable scale and shift parameters used in batch normalization. A 2 × 2 max-pooling layer then reduces spatial dimensions and improves efficiency. The max-pooling operation is defined in Equation  7 .\n\nFollowing the first convolution layer, the feature extraction process continues with progressively smaller kernels and more filters: 50 × 50 (16), 25 × 25 (32), 15 × 15 (64), 9 × 9 (128), and 3 × 3 (256), each followed by dropout, batch normalization, and pooling (except the last). The output is flattened and passed to a dense layer with 512 neurons (ReLU). The dense layer (i.e., fully connected layer) calculates its outputs (y) using its definition listed in Equation  8 .\n\nW is the weight matrix of the layer and b is the bias vector for the current layer. f is the activation function used (in this case, ReLU). This is finally followed by dropout (0.5) to prevent overfitting. The model captures both low and high-level spatial features efficiently.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cnn Feature Extractor 2",
      "text": "CNN Feature Extractor 2 or CNNFE2 as shown in Fig.  4 . is also a CNN architecture designed to extract hierarchical spatial features while maintaining computational efficiency.\n\nIt starts with a 15×15 convolution layer (16 filters, ReLU, same padding), followed by dropout (0.1), batch normalization, and 2×2 max pooling. This structure repeats with 32 filters (7×7), 64 filters (5×5), 128 filters (3×3), and finally 256 filters (3×3), each layer followed by batch normalization, ReLU, and pooling. Batch normalization is used to stabilize the training process and the dropout layers are applied to reduce overfitting. Instead of flattening (like in CNNFE1), a Global Average Pooling (GAP) layer reduces each feature map to a single value, creating a compact and efficient feature vector. GAP for a given feature map X of size H × W (height × width) is computed by the equation given in Equation  9 . X(i, j) is the value of the feature map at position (i, j) and Y is the scalar output representing the average value of the entire feature map.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Resnet-50 Model",
      "text": "A pre-trained ResNet-50 model trained on the VGGFace2 dataset  [6]  originally tasked for face recognition was leveraged here to acquire the high-level face features. It ensures deep feature learning by processing 224 × 224 × 3 images through the residual blocks of ResNet50. We have used include_top=false that turns the model into a pure feature extractor by removing the classification levels. In order to avoid overfitting, a 0.5 dropout layer is used after global average pooling which condenses feature maps into a compact vector. Transfer learning is used in this architecture to provide reliable face expression recognition. This sub-model architecture is shown in Fig.  5 . The network takes a vector of size input_dim as input and starts with a dense input layer. After that, this input is run in parallel through a number of expert models (denoted by num_experts, e.g., 4 experts), each of which has a dense layer with ReLU activation to enable it to capture distinct data features. To regulate the information flow, a gating network is implemented, with num_experts logits generated by its own dense layer. A Softmax activation is used to convert these logits into a probability distribution over the experts, allowing the gating mechanism to choose the final experts. The softmax is defined Equation  10 .\n\nWhere, z i is the raw output (logit) for class i and e zi is the exponential of the logit z i . The denominator is the summation of the exponentials of all logits, this ensures that the output probabilities sum to 1.\n\nAfter stacking the expert outputs into a tensor, the top-k function from TensorFlow is used to choose the top-k experts (2 experts, default) based on the gating probabilities. Each expert's contribution is scaled by its corresponding gating",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Our Final Model -Expressnet-Moe",
      "text": "The CNNFE1, CNNFE2, ResNet-50 and MoE modules are combined in the final ExpressNet-MoE Model (Fig.  7 ) to produce a robust architecture for emotion recognition. Sequentially putting the input through these models yields their results. Following the concatenation of the CNNFE2 and ResNet50 features, a dense layer with 512 units and ReLU activation is applied to the combined features. A Dropout layer with a regularization rate of 0.5 is then applied.\n\nThe two sets of features are subjected to the MoE mechanism: a MoE layer specifically designed for CNN-based features processes the features from CNNFE1, and a second MoE layer also designed for the CNN features processes the dense layer output (from CNNFE2 and ResNet50). The high-level extracted representations of the input image are represented by a composite feature vector created by concatenating the outputs from the two MoE layers which we will call the final features.\n\nTo predict one of the emotions in input images, an output layer with softmax activation is added at the end taking the output as an input from the final features. To enhance generalization, the model is assembled using the Adam optimizer and categorical cross-entropy loss with label smoothing. The chosen loss function, categorical Cross Entropy (CCE) is calculated using Equation 11. In the equation, K is the number of classes, y i is the true label and ŷi is the predicted label. log(ŷ i ) is the natural logarithm of the predicted probability of the class i.\n\nWe ran our experiments in a Tensorflow 2.0 compute framework configured with four NVidia Titan Xp GPU coprocessors with mirrored strategy enabled. Values of the hyperparameters were chosen by heuristics and are listed in Table  1 .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Number Of Epochs 15",
      "text": "Optimizer Adam Learning rate, α = 10 -4 β 1 = 0.9, β 2 = 0.999",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Loss Function",
      "text": "Categorical Cross-entropy",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "Table  2  lists the classification report of our proposed ExpressNet-MoE model on all of the four datasets in terms of precision, recall, F 1 , and per-class accuracy (p-Acc), while Table  3  lists the overall accuracy, macro averaged precision, recall and F 1 -score of our model at the evaluation.\n\nThe model's performance varies on different datasets, which reflects issues specific to each dataset. Happiness regularly has the highest F 1 -scores, showing a good recognition of positive emotions, especially in RAF-DB (0.93) and FER-2013 (0.86). In FER-2013, anger and fear had lower F 1 -scores (0.59 and 0.40, respectively), due to class imbalance and incorrect classification. In contrast to AN-8 (0.70), disgust performs badly in FER-2013 (F 1 = 0.49), which is directly correlated to having less training and testing data. In FER-2013, neutral expressions have a high recall (0.75) but low precision (0.50), which compromises the reliability of classification. Overall, our model shows consistent performance on all of RAF-DB emotion classes (84.29%) (Table  3 ), yet it shows similar on prediction accuracies on the two versions of AffectNet datasets: AffectNet 7 (74.77%) and AffectNet 8 (72.55%).\n\nThe generalized trend noticed during the training on all datasets is that validation accuracy typically followed an increasing trend with some fluctuations, indicating possible overfitting, training accuracy demonstrated consistent improvements. Our check-points made sure only the model with the highest validation accuracy was saved to mitigate the overfitting. Adjusting the learning rate facilitated smooth convergence, while early stopping avoided needless training.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Affectnet 7 Results",
      "text": "The classification task on AffectNet 7 showed a robust learning progression. From 49.51% in the first epoch to 97.60% in the tenth (in Fig.  8  ). The training accuracy increased significantly. By the sixth epoch validation accuracy had reached 74.86%, followed by slight variations that might indicate overfitting. Despite these differences, the final stored model matches an optimally generalized version that successfully captures emotional patterns in the AffectNet 7 dataset.\n\nThe final test accuracy the model achieved in this dataset is 74.77%. Training accuracy increased significantly in the AffectNet 8 classification task, rising from 47.68% in the first epoch to 96.88% by the ninth (in Fig.  9 ).\n\nWhile training accuracy kept increasing, suggesting possible overfitting, validation accuracy peaked at 72.35% in the eighth epoch and then varied slightly. However, optimal generalization is ensured by the final stored model as it stores the model when highest validation accuracy is achieved. Even though the model was able to learn emotion representations, its resilience could be further increased by using additional regularization techniques. The final testing accuracy that the model achieved in this dataset is 72.55%.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Raf-Db Results",
      "text": "Training accuracy increased from 58.89% in the first epoch to 97.54% by the ninth (in Fig.  10 ), demonstrating a rapid improvement on the RAF-DB dataset.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Fer-2013 Results",
      "text": "A peak validation accuracy of 64.37% and a training accuracy of 93.00% are achieved on the FER-2013 training data (Fig.  11 ). However, after peaking, validation accuracy fluctuated, mostly as a result of incorrectly categorized data in the dataset and overfitting. Despite this, the model was able to acquire strong emotion representations. Better preprocessing or label correction could improve the results even though the final stored model is the most generalizable version. The final testing accuracy that the model achieved in this dataset is 64.66 In every dataset, the \"emotion\" happy is classified with the highest accuracy. This is due to the fact that all of the datasets had \"happy\" images predominantly. Similarly, our model outperforms all prior approaches in classifying AffectNet 8 dataset by achieving 72.55% as its testing accuracy on the dataset. For this dataset, D-CNN has the highest reported accuracy of 70%, followed by Norface achieving 68.69% and then QCS achieving 64.30%. The margin of 2.55% over D-CNN and 3.86% over Norface underscores our model's effectiveness in generalizing even when an additional emotion class (contempt) is included, which frequently complicates classification tasks due to its subtlety. Fine-tuned EfficientNet-B2 with an accuracy of 63.03% and EmoNeXt-XL with an accuracy of 64.13% perform significantly worse, emphasizing the advantages of our model's architecture for learning intricate facial emotions. The improvement in accuracy between AffectNet 7 and AffectNet 8 indicates that our model is both resilient to variations in the distribution of emotional classes and capable of managing large-scale datasets.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Comparison With State Of The Art Methods",
      "text": "Our model achieves 84.29% accuracy on the RAF-DB dataset which is competitive and proves the model's adaptability and generalizability but still falls short of the top-performing techniques. ResEmoteNet achieves the highest accuracy of an accuracy of 94.76%, followed closely by Norface with an accuracy of 92.97% and QCS with 92.50% accuracy.\n\nThe models outperform our approach by approximately 8.68% to 10.47%. This can be attributed to the fact that Furthermore, as we can see in Fig.  12 , the distribution of RGB values for AffectNet and RAF-DB are different with RAF-DB having higher pixel values for the red channel. This is attributed to variations in head position, occlusion, lighting conditions and skin color. The difference in the accuracies received for both RAF-DB and AffectNet can be attributed to this aspect of the datasets. We can also see in Fig.  13  that the Mediapipe library that we used for image pre-processing crops the entire face for many RAF-DB images as these images for emotions like surprise, fear, etc. have their faces covered. Therefore, the proposed architecture cannot properly classify these images for the RAF-DB dataset and underperforms for the dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "To enhance facial emotion recognition, the proposed ExpressNet-MoE presents a novel hybrid deep learning architecture that successfully combines Mixture of Experts (MoE) module with Convolution Neural Networks and provides new insights into how expert models can be dynamically selected for emotion recognition tasks. This advances our knowledge of how ensemble approaches can be used to solve visual recognition problems, especially ones that involve intricate and subtle patterns like facial expressions. The model maintains competitive performance on RAF-DB while achieving excellent accuracy on AffectNet",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Acknowledgment",
      "text": "This material is based upon work supported by the National Science Foundation under Grant No. 2329919.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the total number of images for every dataset per emotion class along with their training and",
      "page": 4
    },
    {
      "caption": "Figure 1: Training vs Test Splits for the datasets.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the complete methodology.",
      "page": 5
    },
    {
      "caption": "Figure 2: Proposed methodology",
      "page": 6
    },
    {
      "caption": "Figure 3: ) is a Deep Convolution Neural Network (D-CNN) designed to extract",
      "page": 6
    },
    {
      "caption": "Figure 3: CNN Feature Extractor 1 (CNNFE1)",
      "page": 6
    },
    {
      "caption": "Figure 4: is also a CNN architecture designed to extract hierarchical",
      "page": 7
    },
    {
      "caption": "Figure 4: CNN Feature Extractor 2 (CNNFE2)",
      "page": 8
    },
    {
      "caption": "Figure 5: Figure 5: ResNet-50 Architecture",
      "page": 8
    },
    {
      "caption": "Figure 6: The network takes a vector of size input_dim as input and starts with a dense input layer. After that, this input is run in",
      "page": 8
    },
    {
      "caption": "Figure 6: Mixture of Experts (MoE)",
      "page": 9
    },
    {
      "caption": "Figure 7: ExpressNet-MoE Architecture",
      "page": 9
    },
    {
      "caption": "Figure 8: ). The training accuracy increased significantly. By the sixth epoch validation accuracy had",
      "page": 10
    },
    {
      "caption": "Figure 8: Training Accuracy/Loss vs Validation Accuracy/Loss: AffectNet7.",
      "page": 12
    },
    {
      "caption": "Figure 9: Training Accuracy/Loss vs Validation Accuracy/Loss: AffectNet8.",
      "page": 12
    },
    {
      "caption": "Figure 10: ), demonstrating a rapid",
      "page": 12
    },
    {
      "caption": "Figure 10: Training Accuracy/Loss vs Validation Accuracy/Loss: RAF-DB.",
      "page": 12
    },
    {
      "caption": "Figure 11: ). However, after peaking, validation accuracy fluctuated, mostly as a result of incorrectly categorized data in the",
      "page": 13
    },
    {
      "caption": "Figure 11: Training Accuracy/Loss vs Validation Accuracy/Loss: FER-2013.",
      "page": 13
    },
    {
      "caption": "Figure 12: AffectNet (left) and RAF-DB (right) Normalized Color Distribution across all images",
      "page": 15
    },
    {
      "caption": "Figure 13: Mediapipe’s Detection on RAF-DB",
      "page": 15
    },
    {
      "caption": "Figure 12: , the distribution of RGB values for AffectNet and RAF-DB are different with",
      "page": 15
    },
    {
      "caption": "Figure 13: that the Mediapipe library that we used for image",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Table 1: List of Hyperparameter Values",
      "page": 10
    },
    {
      "caption": "Table 2: lists the classification report of our proposed ExpressNet-MoE model on all of the four datasets in terms of",
      "page": 10
    },
    {
      "caption": "Table 3: lists the overall accuracy, macro averaged precision,",
      "page": 10
    },
    {
      "caption": "Table 3: ), yet it shows similar on prediction accuracies on the two versions",
      "page": 10
    },
    {
      "caption": "Table 2: Classification Report of proposed ExpressNet-MoE model on all the test-sets from AffectNet7 (AN-7),",
      "page": 11
    },
    {
      "caption": "Table 3: Overall Evaluation of ExpressNet-MoE model on the test-sets in terms of accuracy and macro averaged",
      "page": 11
    },
    {
      "caption": "Table 4: presents a detailed comparison of the ExpressNET-MoE model against other existing models on the AffectNet7,",
      "page": 13
    },
    {
      "caption": "Table 4: Comparison against SOTA AffectNet7: AN-7, AffectNet8: AN-8, RAF-DB: RAF, FER-2013: FER)",
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "2",
      "title": "A novel BlazeFace based pre-processing for MobileFaceNet in face verification",
      "authors": [
        "Necmettin Bayar",
        "Kübra Güzel",
        "Deniz Kumlu"
      ],
      "year": "2022",
      "venue": "2022 45th International Conference on Telecommunications and Signal Processing (TSP)"
    },
    {
      "citation_id": "3",
      "title": "Sub-millisecond neural face detection on mobile gpus",
      "authors": [
        "Valentin Bazarevsky",
        "Yury Kartynnik",
        "Andrey Vakunov",
        "Karthik Raveendran",
        "Matthias Grundmann",
        "Blazeface"
      ],
      "year": "2019",
      "venue": "Sub-millisecond neural face detection on mobile gpus",
      "arxiv": "arXiv:1907.05047"
    },
    {
      "citation_id": "4",
      "title": "A Generalized Zero-Shot Deep Learning Classifier for Emotion Recognition Using Facial Expression Images",
      "authors": [
        "Vishal Singh Bhati",
        "Namita Tiwari",
        "Meenu Chawla"
      ],
      "year": "2025",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "A novel deep learning approach for facial emotion recognition: application to detecting emotional responses in elderly individuals with Alzheimer's disease. Neural Computing and Applications",
      "authors": [
        "Amine Bohi",
        "Yassine Boudouri",
        "Imad Sfeir"
      ],
      "year": "2024",
      "venue": "A novel deep learning approach for facial emotion recognition: application to detecting emotional responses in elderly individuals with Alzheimer's disease. Neural Computing and Applications"
    },
    {
      "citation_id": "6",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "Omkar Parkhi",
        "Andrew Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018)"
    },
    {
      "citation_id": "7",
      "title": "Engagement detection in online learning: a review",
      "authors": [
        "Mahbub Dewan",
        "Fuhua Murshed",
        "Lin"
      ],
      "year": "2019",
      "venue": "Smart Learning Environments"
    },
    {
      "citation_id": "8",
      "title": "Facial emotion recognition in smart education systems: a review",
      "authors": [
        "Admed Haleem Farman",
        "Sedik",
        "M Moustafa",
        "Maged Nasralla",
        "Esmail Abdullah"
      ],
      "venue": "2023 IEEE International Smart Cities Conference (ISC2)"
    },
    {
      "citation_id": "9",
      "title": "EmoSecure: Enhancing Smart Home Security With FisherFace Emotion Recognition and Biometric Access Control",
      "authors": [
        "Premanand Ghadekar",
        "Manas Ranjan Pradhan",
        "Debabrata Swain",
        "Biswaranjan Acharya"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "Neural information processing: 20th international conference, ICONIP 2013, daegu, korea"
    },
    {
      "citation_id": "11",
      "title": "Facial Expressions Analysis To Evaluate The Level Of Students' Understanding",
      "year": "2023",
      "venue": "2023 Intelligent Methods, Systems, and Applications (IMSA)"
    },
    {
      "citation_id": "12",
      "title": "A dataset for assessing real-time attention levels of the students during online classes",
      "authors": [
        "Muhammad Kamal",
        "Mohammad Shorif Uddin"
      ],
      "year": "2023",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in doctorpatient interactions from real-world clinical video database: Initial development of artificial empathy",
      "authors": [
        "Chih-Wei Huang",
        "C Bethany",
        "Phung Wu",
        "Anh Nguyen",
        "Hsiao-Han Wang",
        "Chih-Chung Kao",
        "Pei-Chen Lee",
        "Annisa Ristya Rahmanti",
        "Jason Hsu",
        "Hsuan-Chia Yang",
        "Yu-Chuan Jack Li"
      ],
      "year": "2023",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "14",
      "title": "Facial expression recognition with grid-wise attention and visual transformer",
      "authors": [
        "Qionghao Huang",
        "Changqin Huang",
        "Xizhe Wang",
        "Fan Jiang"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "15",
      "title": "A study on computer vision for facial emotion recognition",
      "authors": [
        "Zi-Yu Huang",
        "Chia-Chin Chiang",
        "Jian-Hao Chen",
        "Yi-Chian Chen",
        "Hsin-Lung Chung",
        "Yu-Ping Cai",
        "Hsiu-Chuan Hsu"
      ],
      "year": "2023",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "16",
      "title": "Face2nodes: learning facial expression representations with relation-aware dynamic graph convolution networks",
      "authors": [
        "Fan Jiang",
        "Qionghao Huang",
        "Xiaoyong Mei",
        "Quanlong Guan",
        "Yaxin Tu",
        "Weiqi Luo",
        "Changqin Huang"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "17",
      "title": "A shallow deep neural network for selection of migration candidate virtual machines to reduce energy consumption",
      "authors": [
        "Zeinab Khodaverdian",
        "Hossein Sadr",
        "Seyed Ahmad"
      ],
      "year": "2021",
      "venue": "2021 7th International conference on web research (ICWR)"
    },
    {
      "citation_id": "18",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "20",
      "title": "Norface: Improving facial expression analysis by identity normalization",
      "authors": [
        "Hanwei Liu",
        "Rudong An",
        "Zhimeng Zhang",
        "Bowen Ma",
        "Wei Zhang",
        "Yan Song",
        "Yujing Hu",
        "Wei Chen",
        "Yu Ding"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Real time face expression recognition along with balanced FER2013 dataset using CycleGAN",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "22",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Facial expression recognition using histogram of oriented gradients based transformed features",
      "authors": [
        "Muhammad Nazir",
        "Jan Zahoor",
        "Muhammad Sajjad"
      ],
      "year": "2018",
      "venue": "Cluster Computing"
    },
    {
      "citation_id": "24",
      "title": "Monitoring task engagement using facial expressions and body postures",
      "authors": [
        "Akilesh Rajavenkatanarayanan",
        "Ramesh Ashwin",
        "Konstantinos Babu",
        "Fillia Tsiakas",
        "Makedon"
      ],
      "year": "2018",
      "venue": "Proceedings of the 3rd International Workshop on Interactive and Spatial Computing"
    },
    {
      "citation_id": "25",
      "title": "Fine tuning vision transformer model for facial emotion recognition: Performance analysis for human-machine teaming",
      "authors": [
        "Sanjeev Roka",
        "B Danda",
        "Rawat"
      ],
      "year": "2023",
      "venue": "2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI)"
    },
    {
      "citation_id": "26",
      "title": "ResEmoteNet: bridging accuracy and loss reduction in facial emotion recognition",
      "authors": [
        "Arnab Kumar",
        "Hemant Kumar Kathania",
        "Adhitiya Sharma",
        "Abhishek Dey",
        "Md Sarfaraj",
        "Alam Ansari"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "27",
      "title": "A comprehensive survey on deep facial expression recognition: challenges, applications, and future guidelines",
      "authors": [
        "Muhammad Sajjad",
        "Fath U Min",
        "Mohib Ullah",
        "Georgia Ullah",
        "Christodoulou",
        "Alaya Faouzi",
        "Mohammad Cheikh",
        "Khan Hijji",
        "Joel Jpc Muhammad",
        "Rodrigues"
      ],
      "year": "2023",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "28",
      "title": "Fast and efficient face recognition system using random forest and histograms of oriented gradients",
      "authors": [
        "Abdel Ilah Salhi",
        "Mustapha Kardouchi",
        "Nabil Belacel"
      ],
      "year": "2012",
      "venue": "2012 BIOSIG-Proceedings of the International Conference of Biometrics Special Interest Group (BIOSIG)"
    },
    {
      "citation_id": "29",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "Andrey Savchenko",
        "Lyudmila Savchenko",
        "Ilya Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Robust facial expression recognition using local binary patterns",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2005",
      "venue": "IEEE International Conference on Image Processing 2005"
    },
    {
      "citation_id": "31",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "Caifeng Shan",
        "Shaogang Gong",
        "Peter Mcowan"
      ],
      "year": "2009",
      "venue": "Image and vision Computing"
    },
    {
      "citation_id": "32",
      "title": "Deepfake detection using convolutional vision transformers and convolutional neural networks",
      "authors": [
        "Ahmed Hatem Soudy",
        "Omnia Sayed",
        "Hala Tag-Elser",
        "Rewaa Ragab",
        "Sohaila Mohsen",
        "Tarek Mostafa",
        "Amr Abohany",
        "Salwa",
        "Slim"
      ],
      "year": "2024",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "33",
      "title": "Multimodal Sentimental Privileged Information Embedding for Improving Facial Expression Recognition",
      "authors": [
        "Ning Sun",
        "Changwei You",
        "Wenming Zheng",
        "Jixin Liu",
        "Lei Chai",
        "Haian Sun"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Analyzing Facial Emotion Patterns in AffectNet with Deep Neural Networks",
      "authors": [
        "Sagar Uniyal",
        "Ritu Agarwal"
      ],
      "year": "2024",
      "venue": "2024 1st International Conference on Advances in Computing, Communication and Networking (ICAC2N)"
    },
    {
      "citation_id": "35",
      "title": "QCS: Feature refining from quadruplet cross similarity for facial expression recognition",
      "authors": [
        "Chengpeng Wang",
        "Li Chen",
        "Lili Wang",
        "Zhaofan Li",
        "Xuebin Lv"
      ],
      "year": "2024",
      "venue": "QCS: Feature refining from quadruplet cross similarity for facial expression recognition",
      "arxiv": "arXiv:2411.01988"
    },
    {
      "citation_id": "36",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "Saining Zhang",
        "Yuhang Zhang",
        "Ye Zhang",
        "Yufei Wang",
        "Zhigang Song"
      ],
      "year": "2023",
      "venue": "Electronics"
    }
  ]
}