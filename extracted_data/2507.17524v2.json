{
  "paper_id": "2507.17524v2",
  "title": "Sdc-Net: A Domain Adaptation Framework With Semantic-Dynamic Consistency For Cross-Subject Eeg Emotion Recognition",
  "published": "2025-07-23T14:04:43Z",
  "authors": [
    "Jiahao Tang",
    "Youjun Li",
    "Xiangting Fan",
    "Yangxuan Zheng",
    "Siyuan Lu",
    "Xueping Li",
    "Peng Fang",
    "Chenxi Li",
    "Zi-Gang Huang"
  ],
  "keywords": [
    "Emotion recognition",
    "EEG",
    "transfer Learning",
    "affective brain-computer interface"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition based on electroencephalography (EEG) holds significant promise for affective brain-computer interfaces (aBCIs). However, its practical deployment faces challenges due to the variability within inter-subject and the scarcity of labeled data in target domains. To overcome these limitations, we propose SDC-Net, a novel Semantic-Dynamic Consistency domain adaptation network for fully label-free cross-subject EEG emotion recognition. First, we introduce a Same-Subject Same-Trial Mixup strategy that generates augmented samples through intra-trial interpolation, enhancing data diversity while explicitly preserving individual identity to mitigate label ambiguity. Second, we construct a dynamic distribution alignment module within the Reproducing Kernel Hilbert Space (RKHS), jointly aligning marginal and conditional distributions through multi-objective kernel mean embedding, and leveraging a confidence-aware pseudo-labeling strategy to ensure stable adaptation. Third, we propose a dual-domain similarity consistency learning mechanism that enforces cross-domain structural constraints based on latent pairwise similarities, facilitating semantic boundary learning without reliance on temporal synchronization or label priors. To validate the effectiveness and robustness of the proposed SDC-Net, extensive experiments are conducted on three widely used EEG benchmark datasets: SEED, SEED-IV, and FACED. Comparative results against existing unsupervised domain adaptation methods demonstrate that SDC-Net achieves state-of-the-art performance in emotion recognition under both cross-subject and cross-session conditions. This advancement significantly improves the accuracy and generalization capability of emotion decoding, laying a solid foundation for real-world applications of personalized aBCIs. The source code is available at: https://github.com/XuanSuTrum/SDC-Net.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Dynamic Distribution Alignment In Rkhs",
      "text": "To address the distributional discrepancy between source domain D s and target domain D t in EEG emotion recognition, we propose a unified framework named Dynamic Distribution Alignment in RKHS. This method performs joint alignment of MPD and CPD in a shared RKHS. The key idea is to unify marginal and conditional alignment into a single kernel mean embedding (KME)-based framework. The detailed derivations of Dynamic Distribution Alignment in RKHS are provided in the Supplementary Information (Section S1).\n\n1) Unified Alignment Framework: We adopt a unified kernel mean embedding (KME) framework to align both marginal and conditional distributions in the RKHS. Formally, the alignment loss is defined as\n\nwhere µ P (x|y) and µ Q(x|y) denote the conditional embeddings of the source and target domains. Detailed derivations and special cases (MMD, CMMD) are provided in the Supplementary Information (Section S1.1).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2) Alignment Of Marginal Distributions In Rkhs:",
      "text": "To reduce global distribution shift, we employ the Maximum Mean Discrepancy (MMD) between source and target domains: where Φ(•) denotes the feature mapping into RKHS. The empirical estimation and multi-kernel extension of MMD are derived in the Supplementary Information (Section S1.2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3) Alignment Of Conditional Probability Distributions In Rkhs:",
      "text": "To ensure robust conditional distribution alignment, we first introduce a dynamic confidence-based selection mechanism for pseudo-labels in the target domain. Specifically, for each target sample x t i , let ŷt i denote its predicted label distribution. If the confidence max(ŷ t i ) ≥ τ , the pseudo-label is accepted; otherwise, the sample is excluded from CMMD computation. Formally:\n\nThe threshold τ is gradually increased during training, allowing more reliable samples to be incorporated over time. Consequently, the final CMMD loss is computed only on highconfidence samples from Dt . Beyond marginal alignment, we further enforce semantic consistency by aligning class-conditional distributions across domains. The CMMD loss is defined as\n\nwhere µ P (x|y=c) and µ Q(x|y=c) denote the mean embeddings of source and target samples for class c in RKHS. Detailed derivations of CMMD are provided in the Supplementary Information (Section S1.3).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Dual-Domain Similarity Consistency Learning Strategy",
      "text": "To promote discriminative and transferable feature representations for EEG signals, we propose a DSCL strategy. This module encourages the learned feature space to maintain pairwise semantic consistency-ensuring that samples from the same class are closer, and those from different classes are further apart. In order to achieve it, pairwise similarity learning is applied in both the source and target domains, as illustrated in Fig.  2 .\n\n1) Supervised Similarity Consistency on Source Domain: In the source domain D s , ground-truth labels are available, allowing us to supervise the model's feature similarity predictions. Given two source samples x i s and x j s with labels y i s and y j s , we define their true semantic similarity using a binary indicator:\n\nTheir feature similarity is computed by cosine similarity over the normalized embeddings x i s and x j s :\n\nWe map S to the [0, 1] interval as S ′ , allowing it to be compared with the binary label using binary cross-entropy (BCE) loss. The supervised similarity loss is:\n\nwhere l(•) is the BCE loss. This objective enforces semantic structure preservation within the source domain, enhancing intra-class compactness and inter-class separation.\n\n2) Self-Inferred Similarity Consistency on Target Domain: In the target domain D t , labels are unavailable. To still enable pairwise semantic learning, we utilize pseudo-labels ŷt generated from the model itself using the confidence-based filtering strategy.\n\nThe key idea is to infer pairwise relationships based on cosine similarity. For target feature embeddings x i t and x j t , we define:\n\nHere, τ pu and τ pl represent dynamic upper and lower thresholds used to determine confident positive or negative pairs, respectively. Feature similarities within the ambiguous region [τ pl , τ pu ] are excluded to avoid noisy supervision. These thresholds are linearly adjusted during training to gradually include more pairs as the model becomes more confident (visualized in Fig.  2 ).\n\nOnly confident pairs (i, j) ∈ P are selected for learning, and the target-domain loss is:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Loss Function And Training Procedure Of Sdc-Net",
      "text": "To enable robust domain adaptation under fully unlabeled target conditions, the proposed SDC-Net is optimized with a unified objective comprising five loss components: the classification loss on the source domain (L Ds ), marginal and conditional distribution alignment losses (L mmd and L cmmd ), and pairwise similarity losses on both the source (L ps ) and target (L pt ) domains. These components are integrated into the following total loss:\n\nThe classification loss L Ds supervises the prediction of labeled source domain samples using cross-entropy, and is defined as:\n\nwhere B L is the batch size, C denotes the number of emotion classes, y l s presents the one-hot ground-truth label, and p l s means the predicted probability distribution.\n\nTo bridge the distributional gap between domains, we employ both marginal and conditional alignment losses. The marginal distribution alignment is performed via MMD, while conditional alignment is handled through CMMD, which utilizes high-confidence pseudo-labeled target samples. These two components guide the feature space toward domain invariance while preserving emotion-specific semantics.\n\nIn addition, to capture semantic structure, we impose similarity consistency constraints. The L ps loss leverages source labels to model intra-class and inter-class relations, while L pt promotes structural consistency on the target domain using high-confidence pseudo-labels. These losses enforce that samples from the same emotion class remain close in the latent space, even across domains.\n\nTo balance the influence of each loss component throughout training, we adopt a dynamic weighting strategy. The coefficient α begins with a high value to prioritize marginal alignment and is gradually reduced to emphasize semantic alignment in later stages. The coefficient β is adjusted based on the classification loss via a step function:\n\nwhere ε(•) is the Heaviside step function, and ρ 0 , ρ 1 are two predefined thresholds.\n\nTo further enhance semantic modeling on the target domain, the weight λ for the unsupervised pairwise loss L pt increases linearly with training epochs:\n\nThis progressive adjustment ensures that SDC-Net shifts from global alignment to finer semantic refinement as training evolves, ultimately improving generalization performance in fully unsupervised cross-subject EEG emotion recognition tasks. The overall learning process is provided in Algorithm S1 of the Supplementary Information.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Emotion Datasets",
      "text": "To demonstrate the effectiveness of SDC-Net for crosssubject EEG-based emotion recognition, we conduct experiments on three public benchmark datasets: SEED  [25] , SEED-IV  [26] , and FACED  [27] . SEED and SEED-IV contain EEG signals from 15 subjects recorded with a 62-channel NeuroScan system, induced by film clips designed to evoke three (positive, neutral, negative) and four (happiness, sadness, neutral, fear) emotions, respectively. FACED involves 123 participants with 32-channel recordings and supports both nine-class discrete emotion classification and a binary positive/negative task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Experiment Setting And Implementation Details",
      "text": "We evaluated SDC-Net on the SEED and SEED-IV datasets using two widely adopted cross-validation protocols: (1) Cross-subject single-session leave-one-subject-out, where one subject was used as the target domain D t and the others as the source domain D s ; (2) Cross-subject cross-session leave-one-subject-out, where one subject's entire session was treated as D t and the remaining sessions as D s . These protocols provide a rigorous assessment of generalization across both subjects and sessions. Further procedural details are provided in the Supplementary Information (Section S2.1).\n\nThe feature extractor consists of two fully connected layers (310 → 64 → 64) with ReLU activations and dropout (0.25). Training was performed for 200 epochs with SGD (momentum = 0.9), batch size = 32, and initial learning rates in {0.001, 0.01}. Additional hyperparameter specifications (e.g., weight decay, random seed, threshold schedules, and dynamic coefficients) are reported in the Supplementary Information (Section S2.2-S2.4).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Result A. Experimental Results",
      "text": "1) Cross-subject single-session leave-one-out-subject-out cross-validation Results: In TablesII and III, we conducted a comprehensive evaluation of various representations on the SEED and SEED-IV datasets using the leave-one-subject-out cross-validation method with a cross-subject single-session protocol. Our method demonstrated significant performance advantages on the SEED dataset, achieving an accuracy of 91.85%± 05.98%. Similarly, on the SEED-IV dataset, our method exhibited competitive accuracy of 74.88%±10.47%. All results are reported as mean± standard deviation over test subjects. These results strongly validate the substantial performance improvements achieved by our method on both datasets, particularly the remarkable accuracy of 91.85%± 05.98% on the SEED dataset, surpassing the industry average and demonstrating notable potential in the field of emotion recognition tasks. These findings provide compelling evidence supporting the effective application of our method in realworld scenarios. As shown in Table  S1  (Supplementary Information (Section S3.1)), the SDC-Net model was evaluated on the FACED dataset for both binary classification (FACED-2) of positive and negative emotions and nine-class emotion classification (FACED-9), achieving accuracies of 75.2%±8.46% and 42.4%±6.55%, respectively. Compared to the baseline model DE±SVM, it achieved improvements for the binary and nine-class tasks, respectively, highlighting the superior performance of our model, particularly in handling more finegrained emotion classification tasks.\n\n2) Cross-subject cross-session leave-one-out-subject-out cross-validation results: Another crucial consideration for emotion brain-computer interfaces is the substantial variability observed among different subjects across various sessions. The evaluation approach of cross-subject and cross-session represents a significant challenge for the effectiveness of models in EEG-based emotion recognition tasks. To further validate this detection approach, which aligns more closely with realworld application scenarios, we conducted experiments and obtained outstanding three-class classification performance on the SEED dataset, achieving an accuracy of 82.22%±04.68% (see TableIV). Additionally, on the SEED-IV dataset, our model achieved a four-class accuracy of 68.84%±08.05% (see TableIV). Compared to existing research, the proposed SDC-Net method demonstrated industry-leading performance with a smaller standard deviation. These results indicate that the proposed SDC-Net method exhibits excellent stability and generalization capabilities in handling subject and session differences.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Confusion Matrices",
      "text": "To further analyze classification behavior, we compare the confusion matrices of four representative models: DA-CapsNet, PLMSDANet, LGDAAN-Net, and our proposed SDC-Net. The detailed results are provided in Figure  S1  of the Supplementary Information (Section S3.2). The confusion matrices provide insight into how well each model classifies inputs into three categories: Negative, Neutral, and Positive. The diagonal elements represent correct predictions, while off-diagonal elements correspond to misclassifications. Our proposed SDC-Net model outperforms the other models, particularly in classifying the Neutral class with an accuracy of 92.45%, which is the highest among all models. Additionally, SDC-Net achieves 95.7% accuracy in the Positive class and 87.23% in the Negative class. The confusion between classes is minimal, with only 5.25% of neutral instances being misclassified as negative and 2.53% of positive instances being misclassified as negative. The low misclassification rates in SDC-Net suggest that our model effectively captures subtle differences between sentiment classes, particularly between negative and neutral sentiments, where other models faltered. It achieves the best accuracy in both the Neutral and Positive classes and exhibits significantly lower misclassification rates compared to the other models. This indicates that SDC-Net provides a more nuanced understanding of sentiment, making it particularly effective for tasks that require fine-grained sentiment analysis. In contrast, models such as LGDAAN-Net, while demonstrating strong performance in classifying positive samples, exhibit notable deficiencies in differentiating between negative and neutral sentiments. This limitation significantly undermines their effectiveness in comprehensive sentiment classification tasks.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Ablation Study",
      "text": "To evaluate the contribution of each module in the proposed SDC-Net, we performed ablation experiments by systematically removing six key components.  performance under each setting in terms of average classification accuracy. First, domain alignment mechanisms were found to be essential. Removing of the MMD component resulted in a significant drop in performance (86.82%), indicating its crucial role in reducing marginal distribution discrepancy between source and target domains. Similarly, excluding led to a moderate performance decline (89.91%), showing that conditional alignment further refines domain adaptation, though its impact is secondary to MMD. Second, the effect of DSCL was also prominent. Supervised similarity consistency on source domain contributes to extracting discriminative features from labeled source data, and its removal decreased the accuracy to 88.99%. More critically, excluding self-inferred similarity consistency leanring on target domain reduced performance to 86.99%, emphasizing its importance in modeling intraclass similarity in the unlabeled target domain, thus supporting better generalization. Third, SS-Mix based data augmentation improved generalization by synthesizing diverse EEG trials. Its removal caused a noticeable decrease in Acc (87.99%), suggesting its role in mitigating overfitting and increasing sample diversity. Finally, the pseudo-confidence mechanism, which filters unreliable pseudo-pairs, slightly improved the mean accuracy and significantly reduced variance. Without this mechanism, the model still achieved 89.66%, but with increased performance fluctuation (standard deviation 7.18), indicating its stabilizing effect on target domain predictions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Visualization Of Domain Alignment Via T-Sne",
      "text": "To qualitatively assess the effectiveness of domain alignment, we visualized the feature distributions of source and",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Negative Transfer Results",
      "text": "This study evaluates the effectiveness of the proposed SDC-Net framework in alleviating negative transfer in fully unsupervised cross-subject EEG emotion recognition across 45 experimental tasks. Negative transfer is defined as a classification accuracy lower than 33.3% on the SEED dataset or 25% on the SEED-IV dataset. As shown in TableVI, SDC-Net achieved zero instances of negative transfer, demonstrating a clear advantage over conventional transfer learning approaches.\n\nThe robustness of SDC-Net in avoiding negative transfer can be attributed to three key components:(1)SS-Mix, which augments intra-subject data while preserving individualspecific characteristics, reducing ambiguity between subject identity and emotion labels;(2) Dynamic Distribution Alignment in RKHS, which jointly aligns marginal and classconditional distributions using a unified kernel mean embed- ding framework, while adaptively filtering high-confidence pseudo-labels to ensure alignment quality;(3) DSCL, which imposes structural constraints on pairwise similarities across domains, promoting semantic consistency without relying on temporal alignment. To sum up, these innovations enable SDC-Net to dynamically adjust to distribution shifts, suppress noisy pseudo-labels, and maintain semantic integrity across subjects, thereby ensuring stable and reliable transfer performance in complex, label-free EEG emotion recognition scenarios.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "F. Topographic Analysis Of Important Eeg Patterns",
      "text": "This study utilizes an EEG topographic mapping approach grounded in Mutual Information (MI) to evaluate the contributions of different brain regions and frequency bands to emotion classification. The preprocessed EEG feature matrix X ∈ R N ×5×2 , incorporating signals from 62 electrode channels across five frequency bands, is paired with the classification probability matrix Y ∈ R N ×3 , corresponding to three emotional states: negative, neutral, and positive. By computing the MI between each EEG feature and the predicted probabilities of the emotion categories, an initial MI M I ∈ R 3×5×2 tensor is derived, capturing the nonlinear statistical dependencies. This tensor is then normalized via min-max scaling to the [0, 1] interval to enable fair comparison across dimensions. The normalized MI scores are then structured into a three-dimensional tensor (emotion × frequency band × channel) and visualized topographically using the standard 10-20 system for electrode positioning. A 3×5 grid layout illustrates MI distributions across the Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-13 Hz), Beta (13-30 Hz), and Gamma (30-50 Hz) bands for each emotional category. Color gradients denote the strength of feature importance, with electrode positions highlighted as black dots. This methodology effectively integrates information-theoretic metrics with neuroimaging visualization, offering intuitive insights into the spatial and spectral dynamics of emotion processing.\n\nThe results depicted in Fig.  4  indicate that the most informative EEG patterns for emotion recognition are predominantly concentrated in the beta and gamma frequency bands within the frontal and temporal regions. These findings are consistent with prior research, reinforcing the role of high-frequency oscillations in affective processing  [16] ,  [38] ,  [50] .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "G. Discussion",
      "text": "Addressing significant inter-subject variability in EEG data remains a central challenge in developing robust and generalizable aBCI systems. Beyond accurate classification, the ultimate goal of aBCIs is to achieve stable, interpretable, and scalable emotion measurement across users and scenarios. To this end, we propose SDC-Net, designed to improve not only recognition performance but also the consistency and robustness of emotion measurement. We conducted comprehensive evaluations on three public EEG emotion datasets-SEED, SEED-IV, and FACED-demonstrating that SDC-Net significantly outperforms state-of-the-art methods in crosssubject emotion recognition tasks, validating its effectiveness and broad applicability in affective computing and intelligent instrumentation.\n\nOne of the key innovations in SDC-Net is the SS-Mix module, which generates augmented samples through intra-trial interpolation. Compared with GAN-based data augmentation methods (e.g., GANSER  [21]  and SA-cWGAN  [22] ), which may introduce low-quality or identity-inconsistent samples, SS-Mix enhances the modeling of intra-subject emotional variability while significantly reducing semantic noise. From a measurement perspective, this approach can be regarded as a strategy for increasing reliable sample density without introducing cross-subject artifacts, which is especially important in single-trial settings or when data is limited. To address the limitations of prior methods that rely on static distribution alignment and fixed pseudo-labels-such as BiHDM  [29] , MS-MDA  [9] , MS-FRAN  [10] , DA-CapsNet  [30] , SDDA  [13] , PR-PL  [15] , and DAPLP  [12] -we propose DDA strategy in RKHS, which provides finer adaptation to inter-subject and inter-class distributional shifts. This strategy also incorporates a confidence-based pseudo-label filtering mechanism that dynamically selects target samples with well-defined semantic structures, enabling progressive alignment from easy to hard examples. Functionally, this design can be viewed as a robust calibration mechanism when labels are noisy or incomplete, significantly improving training stability and generalization. The proposed DSCL module further addresses limitations of existing contrastive learning methods (e.g., CLISA  [17] , CL-CS  [19] ) that rely on temporal synchronization and are difficult to generalize to cross-scenario measurement. DSCL infers latent structural similarity between domains and imposes corresponding consistency constraints, enabling semantic boundary learning without time alignment or pseudo-label supervision. Experimental results suggest DSCL functions as latent structural regularization, guiding the model toward learning compact and well-separated topological structures.\n\nAblation studies indicate that removing either the structural consistency module or the dynamic reweighting mechanism results in a 4%-6% drop in performance, highlighting their critical roles. Sensitivity analysis further shows that SDC-Net maintains high robustness under different kernel numbers and similarity thresholds, with performance fluctuation consistently below 2% (detailed results are provided in Supplementary information (Section S4)). The robustness of SDC-Net is also validated through visualization and negative transfer testing. As shown in Fig.  3 , emotional categories form welldefined and separable clusters across source and target domains. Additionally, SDC-Net demonstrates strong resistance to negative transfer, ensuring reliable emotion measurement in cross-subject settings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper proposes SDC-Net, a novel domain adaptation framework designed to address the challenge of individual variability in aBCIs for cross-subject EEG emotion recognition. The framework integrates three key innovations: (1) the Same-Subject Same-Trial Mixup data augmentation strategy;\n\n(2) dynamic distribution alignment in the RKHS; and (3) dual-domain similarity consistency learning strategy. Collectively, these components significantly enhance the capability of generalization and robustness for emotion recognition models across different subjects. Extensive experimental results on the SEED, SEED-IV, and FACED datasets demonstrate the superiority of SDC-Net over existing methods in both crosssubject and cross-session scenarios, achieving significant improvements in emotion recognition performance.\n\nFuture work may focus on two promising directions. First, enhancing pseudo-label quality by balancing label confidence and quantity remains crucial. Adaptive confidence thresholds or robust filtering strategies could help mitigate the trade-off between discarding noisy labels and preserving class diversity  [51] ,  [52] . Second, deploying the SDC-Net framework in realtime aBCI systems represents a crucial step toward dynamic, user-adaptive emotion measurement. Integration with online EEG acquisition platforms could enable continuous learning from streaming signals, facilitating personalized and contextaware affective computing in real-world scenarios, such as clinical monitoring or closed-loop neurofeedback instrumentation.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The flowchart of the proposed SDC-Net framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: An illustration of the proposed DSCL strategy. The left part shows supervised similarity consistency learning on the source domain using ground-truth",
      "page": 4
    },
    {
      "caption": "Figure 3: t-SNE visualization of source and target domain emotion representa-",
      "page": 7
    },
    {
      "caption": "Figure 4: Topographic analysis of the mutual information between EEG frequency-band features and model predictions across emotional states.",
      "page": 8
    },
    {
      "caption": "Figure 3: , emotional categories form well-",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "BiHDM [29]",
          "Acc(%)": "85.40±07.53"
        },
        {
          "Method": "DA-CapsNet\n[30]",
          "Acc(%)": "84.63±09.09"
        },
        {
          "Method": "WGAN-GP [31]",
          "Acc(%)": "87.10±07.10"
        },
        {
          "Method": "EPNNE [14]",
          "Acc(%)": "89.10±03.60"
        },
        {
          "Method": "MS-FRAN [10]",
          "Acc(%)": "85.61±06.55"
        },
        {
          "Method": "CU-GCN [34]",
          "Acc(%)": "87.10±05.44"
        },
        {
          "Method": "DS-AGC [35]",
          "Acc(%)": "86.38±07.25"
        },
        {
          "Method": "PLMSDANet\n[36]",
          "Acc(%)": "84.21±12.34"
        },
        {
          "Method": "CL-CS [19]",
          "Acc(%)": "88.30±08.90"
        },
        {
          "Method": "SDC-Net",
          "Acc(%)": "91.85±05.98"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MS-STM [39]",
          "Acc(%)": "61.41±09.72"
        },
        {
          "Method": "MS-MDA [9]",
          "Acc(%)": "59.34±05.48"
        },
        {
          "Method": "JDA-Net\n[11]",
          "Acc(%)": "70.83±10.25"
        },
        {
          "Method": "CU-GCN [34]",
          "Acc(%)": "74.50±07.88"
        },
        {
          "Method": "DS-AGC [35]",
          "Acc(%)": "66.00±07.93"
        },
        {
          "Method": "SDC-Net",
          "Acc(%)": "74.88±10.47"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "RF [42]",
          "SEED (Acc%)": "69.60±07.64",
          "SEED-IV (Acc%)": "50.98±09.20"
        },
        {
          "Method": "KNN [43]",
          "SEED (Acc%)": "60.66±07.93",
          "SEED-IV (Acc%)": "40.83±07.28"
        },
        {
          "Method": "SVM [44]",
          "SEED (Acc%)": "68.15±07.38",
          "SEED-IV (Acc%)": "51.78±12.85"
        },
        {
          "Method": "TCA [45]",
          "SEED (Acc%)": "64.02±07.96",
          "SEED-IV (Acc%)": "56.56±13.77"
        },
        {
          "Method": "CORAL [46]",
          "SEED (Acc%)": "68.15±07.83",
          "SEED-IV (Acc%)": "49.44±09.09"
        },
        {
          "Method": "SA [47]",
          "SEED (Acc%)": "61.41±09.75",
          "SEED-IV (Acc%)": "64.44±09.46"
        },
        {
          "Method": "GFK [48]",
          "SEED (Acc%)": "66.02±07.59",
          "SEED-IV (Acc%)": "45.89±08.27"
        },
        {
          "Method": "DANN [49]",
          "SEED (Acc%)": "81.08±05.88",
          "SEED-IV (Acc%)": "54.63±08.03"
        },
        {
          "Method": "SDC-Net",
          "SEED (Acc%)": "82.22±04.68",
          "SEED-IV (Acc%)": "68.84±08.05"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cardiogenic control of affective behavioural state",
      "authors": [
        "B Hsueh",
        "R Chen",
        "Y Jo",
        "D Tang",
        "M Raffiee",
        "Y Kim",
        "M Inoue",
        "S Randles",
        "C Ramakrishnan",
        "S Patel"
      ],
      "year": "2023",
      "venue": "Nature"
    },
    {
      "citation_id": "2",
      "title": "A framework for studying emotions across species",
      "authors": [
        "D Anderson",
        "R Adolphs"
      ],
      "year": "2014",
      "venue": "Cell"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2024",
      "venue": "Information fusion"
    },
    {
      "citation_id": "4",
      "title": "Motion semantic enhancement and autonomous information mining for static-dynamic visual emotion recognition in human-robot interaction",
      "authors": [
        "C.-S Jiang",
        "Z.-T Liu",
        "E Fukushima",
        "J She"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "5",
      "title": "Cross-subject emotion recognition brain-computer interface based on fnirs and dbjnet",
      "authors": [
        "X Si",
        "H He",
        "J Yu",
        "D Ming"
      ],
      "year": "2023",
      "venue": "Cyborg and Bionic Systems"
    },
    {
      "citation_id": "6",
      "title": "A review on transfer learning in eeg signal analysis",
      "authors": [
        "Z Wan",
        "R Yang",
        "M Huang",
        "N Zeng",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "8",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "9",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "10",
      "title": "Ms-fran: a novel multisource domain adaptation method for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "S Shao",
        "B Hou",
        "A Song"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "11",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised domain adaptation with pseudo-label propagation for cross-domain eeg emotion recognition",
      "authors": [
        "X.-C Zhong",
        "Q Wang",
        "R Li",
        "Y Liu",
        "S Duan",
        "R Yang",
        "D Liu",
        "J Sun"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "13",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin",
        "C Fan",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "14",
      "title": "Evolutionary ensemble learning for eeg-based cross-subject emotion recognition",
      "authors": [
        "H Zhang",
        "T Zuo",
        "Z Chen",
        "X Wang",
        "P Sun"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "15",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Eegmatch: Learning with incomplete labels for semisupervised eeg-based cross-subject emotion recognition",
      "authors": [
        "R Zhou",
        "W Ye",
        "Z Zhang",
        "Y Luo",
        "L Zhang",
        "L Li",
        "G Huang",
        "Y Dong",
        "Y.-T Zhang",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "17",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Contrastive learning of eeg representation of brain area for emotion recognition",
      "authors": [
        "S Dai",
        "M Li",
        "X Wu",
        "X Ju",
        "X Li",
        "J Yang",
        "D Hu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "19",
      "title": "Cross-subject emotion recognition with contrastive learning based on eeg signal correlations",
      "authors": [
        "M Hu",
        "D Xu",
        "K He",
        "K Zhao",
        "H Zhang"
      ],
      "year": "2025",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "20",
      "title": "Gc-stcl: A granger causalitybased spatial-temporal contrastive learning framework for eeg emotion recognition",
      "authors": [
        "L Wang",
        "S Wang",
        "B Jin",
        "X Wei"
      ],
      "year": "2024",
      "venue": "Entropy"
    },
    {
      "citation_id": "21",
      "title": "Ganser: A self-supervised data augmentation framework for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "Y Liu",
        "S.-H Zhong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Self-attention gan for eeg data augmentation and emotion recognition",
      "authors": [
        "J Chen",
        "Z Tang",
        "W Lin",
        "K Hu",
        "J Xie"
      ],
      "year": "2023",
      "venue": "Computer Engineering and Applications"
    },
    {
      "citation_id": "23",
      "title": "Electroencephalographic signal data augmentation based on improved generative adversarial network",
      "authors": [
        "X Du",
        "X Wang",
        "L Zhu",
        "X Ding",
        "Y Lv",
        "S Qiu",
        "Q Liu"
      ],
      "year": "2024",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "24",
      "title": "Mixmatch: A holistic approach to semi-supervised learning",
      "authors": [
        "D Berthelot",
        "N Carlini",
        "I Goodfellow",
        "N Papernot",
        "A Oliver",
        "C Raffel"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "26",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "27",
      "title": "A large finer-grained affective computing eeg dataset",
      "authors": [
        "J Chen",
        "X Wang",
        "C Huang",
        "X Hu",
        "X Shen",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "28",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "30",
      "title": "Da-capsnet: A multi-branch capsule network based on adversarial domain adaption for cross-subject eeg emotion recognition",
      "authors": [
        "S Liu",
        "Z Wang",
        "Y An",
        "B Li",
        "X Wang",
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "31",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "A domain generative graph network for eeg-based emotion recognition",
      "authors": [
        "Y Gu",
        "X Zhong",
        "C Qu",
        "C Liu",
        "B Chen"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "33",
      "title": "Dcastgcn: Eeg emotion recognition based on fusion deep convolutional and adaptive spatio-temporal graph convolutional networks",
      "authors": [
        "X Yang",
        "Z Zhu",
        "G Jiang",
        "D Wu",
        "A He",
        "J Wang"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "34",
      "title": "Graph convolutional network with connectivity uncertainty for eegbased emotion recognition",
      "authors": [
        "H Gao",
        "X Wang",
        "Z Chen",
        "M Wu",
        "Z Cai",
        "L Zhao",
        "J Li",
        "C Liu"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "35",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Semi-supervised pairwise transfer learning based on multisource domain adaptation: A case study on eeg-based emotion recognition",
      "authors": [
        "C Ren",
        "J Chen",
        "R Li",
        "W Zheng",
        "Y Chen",
        "Y Yang",
        "X Zhang",
        "B Hu"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "37",
      "title": "St-scgnn: a spatio-temporal self-constructing graph neural network for cross-subject eeg-based emotion recognition and consciousness detection",
      "authors": [
        "J Pan",
        "R Liang",
        "Z He",
        "J Li",
        "Y Liang",
        "X Zhou",
        "Y He",
        "Y Li"
      ],
      "year": "2023",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "38",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "40",
      "title": "Generalization across subjects and sessions for eeg-based emotion recognition using multisource attention-based dynamic residual transfer",
      "authors": [
        "W Jiang",
        "G Meng",
        "T Jiang",
        "N Zuo"
      ],
      "year": "2023",
      "venue": "2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "41",
      "title": "Msfr-gcn: A multi-scale feature reconstruction graph convolutional network for eeg emotion and cognition recognition",
      "authors": [
        "D Pan",
        "H Zheng",
        "F Xu",
        "Y Ouyang",
        "Z Jia",
        "C Wang",
        "H Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "42",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "43",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "44",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural processing letters"
    },
    {
      "citation_id": "45",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE transactions on neural networks"
    },
    {
      "citation_id": "46",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "47",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "48",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing: 25th International Conference"
    },
    {
      "citation_id": "49",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M March",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "50",
      "title": "The role of asymmetric frontal cortical activity in emotion-related phenomena: A review and update",
      "authors": [
        "E Harmon-Jones",
        "P Gable",
        "C Peterson"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "51",
      "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
      "authors": [
        "B Zhang",
        "Y Wang",
        "W Hou",
        "H Wu",
        "J Wang",
        "M Okumura",
        "T Shinozaki"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
      "authors": [
        "Y Wang",
        "H Chen",
        "Q Heng",
        "W Hou",
        "Y Fan",
        "Z Wu",
        "J Wang",
        "M Savvides",
        "T Shinozaki",
        "B Raj"
      ],
      "year": "2022",
      "venue": "Freematch: Self-adaptive thresholding for semi-supervised learning",
      "arxiv": "arXiv:2205.07246"
    }
  ]
}