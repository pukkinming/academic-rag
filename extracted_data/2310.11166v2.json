{
  "paper_id": "2310.11166v2",
  "title": "Visobert: A Pre-Trained Language Model For Vietnamese Social Media Text Processing",
  "published": "2023-10-17T11:34:50Z",
  "authors": [
    "Quoc-Nam Nguyen",
    "Thang Chau Phan",
    "Duc-Vu Nguyen",
    "Kiet Van Nguyen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELEC-TRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available 4 only for research purposes. Disclaimer: This paper contains actual comments on social networks that might be construed as abusive, offensive, or obscene. * Equal contribution. 4",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Language models based on transformer architecture  (Vaswani et al., 2017)  pre-trained on largescale datasets have brought about a paradigm shift in natural language processing (NLP), reshaping how we analyze, understand, and generate text. In particular, BERT  (Devlin et al., 2019)  and its variants  (Liu et al., 2019; Conneau et al., 2020)  have achieved state-of-the-art performance on a wide range of downstream NLP tasks, including but not limited to text classification, sentiment analysis, question answering, and machine translation. English is moving for the rapid development of language models across specific domains such as medical  (Lee et al., 2019; Rasmy et al., 2021) , scientific  (Beltagy et al., 2019) , legal  (Chalkidis et al., 2020) , political conflict and violence  (Hu et al., 2022) , and especially social media  (Nguyen et al., 2020; DeLucia et al., 2022; Pérez et al., 2022; Zhang et al., 2022) .\n\nVietnamese is the eighth largest language used over the internet, with around 85 million users across the world  5  . Despite a large amount of Vietnamese data available over the Internet, the advancement of NLP research in Vietnamese is still slow-moving. This can be attributed to several factors, to name a few: the scattered nature of available datasets, limited documentation, and minimal community engagement. Moreover, most existing pre-trained models for Vietnamese were primarily trained on large-scale corpora sourced from general texts  (Tran et al., 2020; Nguyen and Tuan Nguyen, 2020; Tran et al., 2023) . While these sources provide broad language coverage, they may not fully represent the sociolinguistic phenomena in Vietnamese social media texts. Social media texts often exhibit different linguistic patterns, informal language usage, non-standard vocabulary, lacking diacritics and emoticons that are not prevalent in formal written texts. The limitations of using language models pre-trained on general corpora become apparent when processing Vietnamese social media texts. The models can struggle to accurately un-derstand and interpret the informal language, using emoji, teencode, and diacritics used in social media discussions. This can lead to suboptimal performance in Vietnamese social media tasks, including emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. We present ViSoBERT, a pre-trained language model designed explicitly for Vietnamese social media texts to address these challenges. ViSoBERT is based on the transformer architecture and trained on a large-scale dataset of Vietnamese posts and comments extracted from well-known social media networks, including Facebook, Tiktok, and Youtube. Our model outperforms existing pretrained models on various downstream tasks, including emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection, demonstrating its effectiveness in capturing the unique characteristics of Vietnamese social media texts. Our contributions are summarized as follows.\n\n• We presented ViSoBERT, the first PLM based on the XLM-R architecture and pre-training procedure for Vietnamese social media text processing. ViSoBERT is available publicly for research purposes in Vietnamese social media mining. ViSoBERT can be a strong baseline for Vietnamese social media text processing tasks and their applications.\n\n• ViSoBERT produces SOTA performances on multiple Vietnamese downstream social media tasks, thus illustrating the effectiveness of our PLM on Vietnamese social media texts.\n\n• To understand our pre-trained language model deeply, we analyze experimental results on the masking rate, examining social media characteristics, including emojis, teencode, and diacritics, and implementing feature-based extraction for task-specific models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fundamental Of Pre-Trained Language Models For Social Media Texts",
      "text": "Pre-trained Language Models (PLMs) based on transformers  (Vaswani et al., 2017)  have become a crucial element in cutting-edge NLP tasks, including text classification and natural language generation. Since then, language models based on transformers related to our study have been reviewed, including PLMs for Vietnamese social media texts.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pre-Trained Language Models For Vietnamese",
      "text": "Several PLMs have recently been developed for processing Vietnamese texts. These models have varied in their architectures, training data, and evaluation metrics. PhoBERT, developed by  Nguyen and Tuan Nguyen (2020) , is the first general pretrained language model (PLM) created for the Vietnamese language. The model employs the same architecture as BERT  (Devlin et al., 2019)  and the same pre-training technique as RoBERTa  (Liu et al., 2019)  to ensure robust and reliable performance. PhoBERT was trained on a 20GB wordlevel Vietnamese Wikipedia corpus, which produces SOTA performances on a range of downstream tasks of POS tagging, dependency parsing, NER, and NLI. Following the success of  PhoBERT, viBERT (Tran et al., 2020) and vELECTRA (Tran et al., 2020) , both monolingual pre-trained language models based on the BERT and ELECTRA architectures, were introduced. They were trained on substantial datasets, with ViBERT using a 10GB corpus and vELECTRA utilizing an even larger 60GB collection of uncompressed Vietnamese text. viBERT4news\n\n6  was published by NlpHUST, a\n\nVietnamese version of BERT trained on more than 20 GB of news datasets. For Vietnamese text summarization, BARTpho  (Tran et al., 2022)  is presented as the first large-scale monolingual seq2seq models pre-trained for Vietnamese, based on the seq2seq denoising autoencoder BART. Moreover, ViT5  (Phan et al., 2022)  follows the encoderdecoder architecture proposed by  Vaswani et al. (2017)  and the T5 framework proposed by  Raffel et al. (2020) . Many language models are designed for general use, while the availability of strong baseline models for domain-specific applications remains limited. Since then,  Minh et al. (2022)  introduced ViHealthBERT, the first domain-specific PLM for Vietnamese healthcare.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pre-Trained Language Models For Social Media Texts",
      "text": "Multiple PLMs were introduced for social media for multilingual and monolinguals. BERTweet  (Nguyen et al., 2020)  was presented as the first public large-scale PLM for English Tweets. BERTweet has the same architecture as BERT Base  (Devlin et al., 2019)  and is trained using the RoBERTa pre-training procedure  (Liu et al., 2019) .  Koto et al. (2021)  proposed IndoBERTweet, the first largescale pre-trained model for Indonesian Twitter. In-doBERTweet is trained by extending a monolingually trained Indonesian BERT model with an additive domain-specific vocabulary.  RoBERTuito, presented in Pérez et al. (2022) , is a robust transformer model trained on 500 million Spanish tweets. RoBERTuito excels in various language contexts, including multilingual and codeswitching scenarios, such as Spanish and English.\n\nTWilBert  (Ángel González et al., 2021)  is proposed as a specialization of BERT architecture both for the Spanish language and the Twitter domain to address text classification tasks in Spanish Twitter. Bernice, introduced by DeLucia et al. (  2022 ), is the first multilingual pre-trained encoder designed exclusively for Twitter data. This model uses a customized tokenizer trained solely on Twitter data and incorporates a larger volume of Twitter data (2.5B tweets) than most BERT-style models.  Zhang et al. (2022)  introduced TwHIN-BERT, a multilingual language model trained on 7 billion Twitter tweets in more than 100 different languages. It is designed to handle short, noisy, user-generated text effectively. Previously,  (Barbieri et al., 2022)  extended the training of the XLM-R  (Conneau et al., 2020)  checkpoint using a data set comprising 198 million multilingual tweets. As a result, XLM-T is adapted to the Twitter domain and was not exclusively trained on data from within that domain.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Visobert",
      "text": "This section presents the architecture, pre-training data, and our custom tokenizer on Vietnamese social media texts for ViSoBERT.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training Data",
      "text": "We crawled textual data from Vietnamese public social networks such as Facebook 7  , Tiktok 8  , and YouTube 9  which are the three most well known social networks in  Vietnam, with 52.65, 49.86, and 63 .00 million users 10  , respectively, in early 2023.\n\nTo effectively gather data from these platforms, we harnessed the capabilities of specialized tools provided by each platform. Pre-processing Data: Pre-processing is vital for models consuming social media data, which is massively noisy, and has user handles (@username), hashtags, emojis, misspellings, hyperlinks, and other noncanonical texts. We perform the following steps to clean the dataset: removing noncanonical texts, removing comments including links, removing excessively repeated spam and meaningless comments, removing comments including only user handles (@username), and keeping emojis in training data.\n\nAs a result, our pretraining data after crawling and preprocessing contains 1GB of uncompressed text. Our pretraining data is available only for research purposes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "Transformers  (Vaswani et al., 2017)  have significantly advanced NLP research using trained models in recent years. Although language models  (Nguyen and Tuan Nguyen, 2020; Nguyen and Nguyen, 2021)  have also proven effective on a range of Vietnamese NLP tasks, their results on Vietnamese social media tasks  (Nguyen et al., 2022)  need to be significantly improved. To address this issue, taking into account successful hyperparameters from XLM-R  (Conneau et al., 2020) , we proposed ViSoBERT, a transformerbased model in the style of XLM-R architecture with 768 hidden units, 12 self-attention layers, and 12 attention heads, and used a masked language objective (the same as  Conneau et al. (2020) ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Vietnamese Social Media Tokenizer",
      "text": "To the best of our knowledge, ViSoBERT is the first PLM with a custom tokenizer for Vietnamese social media texts.  Bernice (DeLucia et al., 2022)   Owing to the ability to handle raw texts of Sen-tencePiece  (Kudo and Richardson, 2018)  without any loss compared to Byte-Pair Encoding  (Conneau et al., 2020) , we built a custom tokenizer on Vietnamese social media by SentencePiece on the whole training dataset. A model has better coverage of data than another when fewer subwords are needed to represent the text, and the subwords are longer  (DeLucia et al., 2022) . Figure  2  (in Appendix A) displays the mean token length for each considered model and group of tasks. ViSoBERT achieves the shortest representations for all Vietnamese social media downstream tasks compared to other PLMs.\n\nEmojis and teencode are essential to the \"language\" on Vietnamese social media platforms. Our custom tokenizer's capability to decode emojis and teencode ensure that their semantic meaning and contextual significance are accurately captured and incorporated into the language representation, thus enhancing the overall quality and comprehensiveness of text analysis and understanding.\n\nTo assess the tokenized ability of Vietnamese social media textual data, we conducted an analysis of several data samples. Table  1  shows several actual social comments and their tokenizations with the tokenizers of the two pre-trained language models, ViSoBERT and PhoBERT, the best strong baseline. The results show that our custom tokenizer performed better compared to others.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings",
      "text": "We accumulate gradients over one step to simulate a batch size of 128. When pretraining from scratch, we train the model for 1.2M steps in 12 14 https://twitter.com/ epochs. We trained our model for about three days on 2×RTX4090 GPUs (24GB). Each sentence is tokenized and masked dynamically with a probability equal to 30% (which is extensively experimented on Section 5.1 to explore the optimal value). Further details on hyperparameters and training can be found in Table  6 of Appendix B.  Downstream tasks. To evaluate ViSoBERT, we used five Vietnamese social media datasets available for research purposes, as summarized in Table  2 . The downstream tasks include emotion recognition (UIT-VSMEC)  (Ho et al., 2020) , hate speech detection (UIT-ViHSD)  (Luu et al., 2021) , sentiment analysis (SA-VLSP2016)  (Nguyen et al., 2018) , spam reviews detection (ViSpamReviews)  (Dinh et al., 2022) , and hate speech spans detection (UIT-ViHOS)  (Hoang et al., 2023) .\n\nFine-tuning. We conducted empirical finetuning for all pre-trained language models using the simpletransformers 15 . Our fine-tuning process followed standard procedures, most of which are outlined in  (Devlin et al., 2019) . For all tasks mentioned above, we use a batch size of 40, a maximum token length of 128, a learning rate of 2e-5, and AdamW optimizer  (Loshchilov and Hutter, 2019)  with an epsilon of 1e-8. We executed a 10epoch training process and evaluated downstream tasks using the best-performing model from those epochs. Furthermore, none of the pre-processing techniques is applied in all datasets to evaluate our PLM's ability to handle raw texts. Baseline models. To establish the main baseline models, we utilized several well-known PLMs, including monolingual and multilingual, to support Vietnamese NLP social media tasks. The details of each model are shown in Table  3 .\n\n• Monolingual language models: viBERT  (Tran et al., 2020)  and vELECTRA  (Tran et al., 2020)  are PLMs for Vietnamese based on BERT and ELECTRA architecture, respectively. PhoBERT, which is based on BERT architecture and RoBERTa pre-training procedure,  (Nguyen and Tuan Nguyen, 2020)  is the first large-scale monolingual language model pre-trained for Vietnamese; PhoBERT obtains state-of-the-art performances on a range of Vietnamese NLP tasks.\n\n• Multilingual language models: Additionally, we incorporated two multilingual PLMs,",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comments",
      "text": "ViSoBERT PhoBERT concặc cáilồn gìđây English: Wut is dis fuckingd1ck <s>, \"conc\", \"ặc\", \"cái\", \"l\", \"ồn\", \"gì\", \"đây\", \" \", </s> <s>, \"c o n @ @\", \"c @ @\", \"ặ c\", \"c á @ @\", \"i l @ @\", \"ồ n\", \"g @ @\",\"ì @ @\", \"đ â y\", <unk>, <unk>, <unk>, </s> e cảmơn anh English: Thankyou <s>, \"e\", \"cảm\", \"ơn\", \"anh\", \" \", \" \", </s> <s>, \"e\", \"c ả @ @\", \"m @ @\", \"ơ n\", \"a n h\", <unk>, <unk>, </s> d4y l4 vj du cko mot cau teencode English: Th1s 1s 4 teencode s3nt3nc3 <s>, \"d\", \"4\", \"y\", \"l\", \"4\", \"vj\", \"du\", \"cko\", \"mot\", \"cau\", \"teen\", \"code\", </s> <s>, \"d @ @\", \"4 @ @\", \"y\", \"l @ @\", \"4\", \"v @ @\", \"j\", \"d u\", \"c k @ @\", \"o\", \"m o @ @\", \"t\", \"c a u\"; \"t e @ @\", \"e n @ @\", \"c o d e\", </s> mBERT  (Devlin et al., 2019)  and XLM-R  (Conneau et al., 2020) , which were previously shown to have competitive performances to monolingual Vietnamese models. XLM-R, a cross-lingual PLM introduced by Conneau et al. (  2020 ), has been trained in 100 languages, among them Vietnamese, utilizing a vast 2.5TB Clean CommonCrawl dataset. XLM-R presents notable improvements in various downstream tasks, surpassing the performance of previously released multilingual models such as mBERT  (Devlin et al., 2019)  and XLM  (Lample and Conneau, 2019) .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "• Multilingual Social Media Language Models:",
      "text": "To ensure a fair comparison with our PLM, we integrated multiple multilingual social media PLMs, including XLM-T  (Barbieri et al., 2022) , TwHIN-BERT  (Zhang et al., 2022), and Bernice (DeLucia et al., 2022) .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Main Results",
      "text": "Table  4  shows ViSoBERT's scores with the previous highest reported results on other PLMs using the same experimental setup. It is clear that our Vi-SoBERT produces new SOTA performance results for multiple Vietnamese downstream social media tasks without any pre-processing technique. Emotion Recognition Task: PhoBERT and TwHIN-BERT archive the previous SOTA performances on monolingual and multilingual models, respectively. ViSoBERT obtains 68.10%, 68.37%, and 65.88% of Acc, WF1, and MF1, respectively, significantly higher than these PhoBERT and TwHIN-BERT models.\n\nHate Speech Detection Task: ViSoBERT achieves significant improvements over previous state-of-the-art models, PhoBERT and TwHIN-BERT, with scores of 88.51%, 88.31%, and 68.77% in Acc, WF1, and MF1, respectively. Notably, these achievements are made despite the presence of bias within the dataset 16 .\n\nSentiment Analysis Task: XLM-R archived SOTA performance on three evaluation metrics. However, there is no significant increase in performance on this downstream task, for 0.45%, 0.46%, and 0.46% higher on Acc, WF1, and MF1 compared to our pre-trained language model, PhoBERT Large . The SA-VLSP2016 dataset domain is technical article reviews, including Tin-hTe 17  mal language. However, ViSoBERT still surpassed other baselines by obtaining 1.31%/0.91% Acc, 1.39%/0.92% WF1, and 1.53%/0.92% MF1 compared to PhoBERT/TwHIN-BERT. Spam Reviews Detection Task: ViSoBERT performed better than the top two baseline models, PhoBERT and TwHIN-BERT. Specifically, it achieved 0.8%, 0.9%, and 2.18% higher scores in accuracy (Acc), weighted F1 (WF1), and micro F1 (MF1) compared to PhoBERT. When compared to TwHIN-BERT, ViSoBERT outperformed it with 0.52%, 0.50%, and 1.78% higher scores in Acc, WF1, and MF1, respectively.\n\nHate Speech Spans Detection Task 20 : Our pre-trained ViSoBERT boosted the results up to 91.62%, 91.57%, and 86.80% on Acc, WF1, and MF1, respectively. While the difference is insignificant, ViSoBERT indicates an outstanding ability to capture Vietnamese social media information compared to other PLMs (see Section 5.3).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multilingual Social Media Plms:",
      "text": "The results show that ViSoBERT consistently outperforms 20 For the Hate Speech Spans Detection task, we evaluate the total of spans on each comment rather than spans of each word in  Hoang et al. (2023)  to retain the context of each comment. XLM-T and Bernice in five Vietnamese social media tasks. It's worth noting that XLM-T, TwHIN-BERT, and Bernice were all exclusively trained on data from the Twitter platform. However, this approach has limitations when applied to the Vietnamese context. The training data from this source may not capture the intricate linguistic and contextual nuances prevalent in Vietnamese social media because Twitter is not widely used in Vietnam.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Result Analysis And Discussion",
      "text": "In this section, we consider the improvement of our PLM more compared to powerful others, including PhoBERT and TwHIN-BERT, in terms of different aspects. Firstly, we investigate the effects of masking rate on our pre-trained model performance (see Section 5.1). Additionally, we examine the influence of social media characteristics on the model's ability to process and understand the language used in these social contexts (see Section 5.2). Lastly, we employed feature-based extraction techniques on task-specific models to verify the potential of leveraging social media textual data to enhance word representations (see Section 5.3).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Impact Of Masking Rate On Vietnamese Social Media Plm",
      "text": "For the first time presenting the Masked Language Model,  Devlin et al. (2019)  consciously utilized a random masking rate of 15%. The authors believed masking too many tokens could lead to losing crucial contextual information required to decode them accurately. Additionally, the authors felt that masking too few tokens would harm the training process and make it less effective. However, according to  Wettig et al. (2023) , 15% is not universally optimal for model and training data.\n\nWe experiment with masking rates ranging from 10% to 50% and evaluate the model's performance on five downstream Vietnamese social media tasks. Figure  1  illustrates the results obtained from our experiments with six different masking rates. Interestingly, our pre-trained ViSoBERT achieved the highest performance when using a masking rate of 30%. This suggests a delicate balance between the amount of contextual information retained and the efficiency of the training process, and an optimal masking rate can be found within this range.\n\nHowever, the optimal masking rate also depends on the specific task. For instance, in the hate speech detection task, we found that a masking rate of 50% yielded the best results, surpassing other masking rate values. This implies that the optimal masking rate may vary depending on the nature and requirements of different tasks.\n\nConsidering the overall performance across multiple tasks, we determined that a masking rate of 30% produced the optimal balance for our pre-trained ViSoBERT model. Consequently, we adopted this masking rate for ViSoBERT, ensuring efficient and effective utilization of contextual information during training.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Impact Of Vietnamese Social Media Characteristics",
      "text": "Emojis, teencode, and diacritics are essential features of social media, especially Vietnamese social media. The ability of the tokenizer to decode emojis and the ability of the model to understand the context of teencode and diacritics are crucial. Hence, to evaluate the performance of Vi-SoBERT on social media characteristics, comprehensive experiments were conducted among several strong PLMs: PM4ViSMT, PhoBERT, and TwHIN-BERT.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of Emoji On Plms:",
      "text": "We conducted two experimental procedures to comprehensively investigate the importance of emojis, including converting emojis to general text and removing emojis.\n\nTable  5  shows our detailed setting and experimental results on downstream tasks and pre-trained models. The results indicate a moderate reduction in performance across all downstream tasks when emojis are removed or converted to text in our pre-trained ViSoBERT model. Our pre-trained decreases 0.62% Acc, 0.55% WF1, and 0.78% MF1 on Average for downstream tasks while converting emojis to text. In addition, an average reduction of 1.33% Acc, 1.32% WF1, and 1.42% MF1 can be seen in our pre-trained model while removing all emojis in each comment. This is because when emojis are converted to text, the context of the comment is preserved, while removing all emojis results in the loss of that context. This trend is also observed in the TwHIN-BERT model, specifically designed for social media processing. However, TwHIN-BERT slightly improves emotion recognition and spam reviews detection tasks compared to its competitors when operating on raw texts. Nevertheless, this improvement is marginal and insignificant, as indicated by the small increments of 0.61%, 0.13%, and 0.21% in Acc, WF1, and MF1 on the emotion recognition task, respectively, and 0.08% Acc, 0.05% WF1, and 0.04% MF1 on spam reviews detection task. One potential reason for this phenomenon is that TwHIN-BERT and ViSoBERT are PLMs trained on emojis datasets. Consequently, these models can comprehend the contextual meaning conveyed by emojis. This finding underscores the importance of emojis in social media texts.\n\nIn contrast, there is a general trend of improved performance across a range of downstream tasks when removing or converting emojis to text on  PhoBERT, the Vietnamese SOTA pre-trained language model. PhoBERT is a PLM on a general text (Vietnamese Wikipedia) dataset containing no emojis; therefore, when PhoBERT encounters an emoji, it treats it as an unknown token (see Table  1  Appendix B). Therefore, while applying emoji preprocessing techniques, including converting emoijs to text and removing emojis, PhoBERT produces better performances compared to raw text.\n\nOur pre-trained model ViSoBERT on raw texts outperformed PhoBERT and TwHIN-BERT even when applying two pre-processing emojis techniques. This claims our pre-trained model's ability to handle Vietnamese social media raw texts.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Teencode On Plms:",
      "text": "Due to informal and casual communication, social media texts often lead to common linguistic errors, such as misspellings and teencode. For example, the phrase \"ăng kơmmmmm\" should be \"ăn cơm\" (\"Eat rice\" in English), and \"ko\" should be \"không\" (\"No\" in English). To address this challenge,  Nguyen and Van Nguyen (2020)  presented several rules to standardize social media texts. Building upon the previous work, Quoc  Tran et al. (2023)  proposed a strict and efficient pre-processing technique to clean comments on Vietnamese social media.\n\nTable  7  (in Appendix C) shows the results with and without standardizing teencode on social media texts. There is an uptrend across PhoBERT, TwHIN-BERT, and ViSoBERT while applying standardized pre-processing techniques. ViSoBERT, with standardized pre-processing techniques, outperforms almost downstream tasks but spam reviews detection. The possible reason is that the ViSpamReviews dataset contains samples in which users use the word with duplicated characters to improve the comment length while standardizing teencodes leads to misunderstanding.\n\nExperimental results strongly suggest that the improvement achieved by applying complex preprocessing techniques to pre-trained models in the context of Vietnamese social media text is relatively insignificant. Despite the considerable time and effort invested in designing and implementing these techniques, the actual gains in PLMs performance are not substantial and unstable.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Vietnamese Diacritics On Plms:",
      "text": "Vietnamese words are created from 29 letters, including seven letters using four diacritics (ă, â-ê-ô, ơ-ư, and đ) and five diacritics used to designate tone (as in à, á, ả, ã, and ạ)  (Ngo, 2020) . These diacritics create meaningful words by combining syllables (Le-Hong, 2021). For instance, the syllable \"ngu\" can be combined with five different diacritic marks, resulting in five distinct syllables: \"ngú\", \"ngù\", \"ngụ\", \"ngủ\", and \"ngũ\". Each of these syllables functions as a standalone word.\n\nHowever, social media text does not always adhere to proper writing conventions. Due to various reasons, many users write text without diacritic marks when commenting on social media platforms. Consequently, effectively handling diacritics in Vietnamese social media becomes a critical challenge. To evaluate the PLMs' capability to address this challenge, we experimented by removing all diacritic marks from the datasets of five downstream tasks. This experiment aimed to assess the model's performance in processing text without diacritics and determine its ability to understand Vietnamese social media content in such cases.\n\nTable  8  (in Appendix C) presents the results of the two best baselines compared to our pre-trained diacritics experiments. The experimental results reveal that the performance of all pre-trained models, including ours, exhibited a significant decrease when dealing with social media comments lacking diacritics. This decline in performance can be attributed to the loss of contextual information caused by the removal of diacritics. The lower the percentage of diacritic removal in each comment, the more significant the performance improvement in all PLMs. However, our ViSoBERT demonstrated a relatively minor reduction in performance across all downstream tasks. This suggests that our model possesses a certain level of robustness and adaptability in comprehending and analyzing Vietnamese social media content without diacritics. We attribute this to the efficiency of the in-domain pre-training data of ViSoBERT.\n\nIn contrast, PhoBERT and TwHIN-BERT experienced a substantial drop in performance across the benchmark datasets. These PLMs struggled to cope with the absence of diacritics in Vietnamese social media comments. The main reason is that the tokenizer of PhoBERT can not encode nondiacritics comments due to not including those in pre-training data. Several tokenized examples of the three best PLMs are presented in Table  10  (in Appendix F). Thus, the significant decrease in its performance highlights the challenge of handling diacritics on Vietnamese social media. While handling diacritics remains challenging, ViSoBERT demonstrates promising performance, suggesting the potential for specialized language models tailored for Vietnamese social media analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Impact Of Feature-Based Extraction To Task-Specific Models",
      "text": "In task-specific models, the contextualized word embeddings from PLMs are typically employed as input features. We aim to assess the quality of contextualized word embeddings generated by PhoBERT, TwHIN-BERT, and ViSoBERT to verify whether social media data can enhance word representation. These contextualized word embeddings are applied as embedding features to BiLSTM, and BiGRU is randomly initialized before the classification layer. We append a linear prediction layer to the last transformer layer of each PLM regard-ing the first subword of each word token, which is similar to  Devlin et al. (2019) .\n\nOur experiment results (see Table  9  in Appendix C) demonstrate that the word embeddings generated by our pre-trained language model Vi-SoBERT outperform other pre-trained embeddings when utilized with BiLSTM and BiGRU for all downstream tasks. The experimental results indicate the significant impact of leveraging social media text data for enriching word embeddings. Furthermore, this finding underscores the effectiveness of our model in capturing the linguistic characteristics prevalent in Vietnamese social media texts.\n\nFigure  3  (in Appendix D) presents the performances of the PLMs as input features to BiLSTM and BiGRU on the dev set per epoch in terms of MF1. The results demonstrate that ViSoBERT reaches its peak MF1 score in only 1 to 3 epochs, whereas other PLMs typically require an average of 8 to 10 epochs to achieve on-par performance. This suggests that ViSoBERT has a superior capability to extract Vietnamese social media information compared to other models.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "We presented ViSoBERT, a novel large-scale monolingual pre-trained language model on Vietnamese social media texts. We illustrated that Vi-SoBERT with fewer parameters outperforms recent strong pre-trained language models such as viBERT, vELECTRA, PhoBERT, XLM-R, XLM-T, TwHIN-BERT, and Bernice and achieves stateof-the-art performances for multiple downstream Vietnamese social media tasks, including emotion recognition, hate speech detection, spam reviews detection, and hate speech spans detection. We conducted extensive analyses to demonstrate the efficiency of ViSoBERT on various Vietnamese social media characteristics, including emojis, teencodes, and diacritics. Furthermore, our pre-trained language model ViSoBERT also shows the potential of leveraging Vietnamese social media text to enhance word representations compared to other PLMs. We hope the widespread use of our opensource ViSoBERT pre-trained language model will advance current NLP social media tasks and applications for Vietnamese. Other low-resource languages can adopt how to create PLMs for enhancing their current NLP social media tasks and relevant applications.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Limitations",
      "text": "While we have demonstrated that ViSoBERT can perform state-of-the-art on a range of NLP social media tasks for Vietnamese, we think additional analyses and experiments are necessary to fully comprehend what aspects of ViSoBERT were responsible for its success and what understanding of Vietnamese social media texts ViSoBERT captures. We leave these additional investigations to future research. Moreover, future work aims to explore a broader range of Vietnamese social media downstream tasks that this paper may not cover. Also, we chose to train the base-size transformer model instead of the Large variant because base models are more accessible due to their lower computational requirements. For PhoBERT, XLM-R, and TwHIN-BERT, we implemented two versions Base and Large for all Vietnamese social media downstream tasks. However, it is not a fair comparison due to their significantly larger model configurations. Moreover, regular updates and expansions of the pre-training data are essential to keep up with the rapid evolution of social media. This allows the pre-trained model to adapt effectively to the dynamic linguistic patterns and trends in Vietnamese social media.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C Plms With Pre-Processing Techniques",
      "text": "For an in-depth understanding of the impact of social media texts on PLMs, we conducted an analysis of the test results on various processing aspects. Table  7  presents performances of the pre-trained language models on downstream Vietnamese social media tasks by applying word standardizing pre-processing techniques, while Table  8  presents performances of the pre-trained language models on downstream Vietnamese social media tasks by removing diacritics in all datasets. Table  7 : Performances of the pre-trained language models on downstream Vietnamese social media tasks by applying word standardizing pre-processing techniques.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Model",
      "text": "[♣] and [♦] denoted with and without standardizing word technique, respectively. ∆ denoted the increase (↑) and the decrease (↓) in performances of the pre-trained language models compared to its competitors without normalizing teencodes.\n\nTo emphasize the essentials of diacritics, we conducted an analysis on several data samples by removing 100%, 75%, 50%, and 25% diacritics of total words that included diacritics in each comment of five downstream tasks. Table  8  presents performances of the pre-trained language models on downstream Vietnamese social media tasks by removing diacritics in all datasets. Table  8 : Performances of the pre-trained language models on downstream Vietnamese social media tasks by removing diacritics in all datasets.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Model",
      "text": "[♣], [♠], [♥], [✠] and [♦] denoted the performances of our pre-trained on removing 100%, 75%, 50%, 25% in each comment, respectively, and not removing diacritics marks dataset, respectively. ∆ denoted the increase (↑) and the decrease (↓) in performances of the pre-trained language models compared to its competitors without removing diacritics marks.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D Plm-Based Features For Bilstm And Bigru",
      "text": "We conduct experiments with various models, including BiLSTM and BiGRU, to understand better the word embedding feature extracted from the pre-trained language models. Table  9  shows performances of the pre-trained language model as input features to BiLSTM and BiGRU on downstream Vietnamese social media tasks. Table  9 : Performances of the pre-trained language models as input features to BiLSTM and BiGRU on downstream Vietnamese social media tasks.\n\nWe implemented various PLMs when used as input features in combination with BiLSTM and BiGRU models to verify the ability to extract Vietnamese social media texts. The evaluation is conducted on the dev set, and the performance is measured per epoch for downstream tasks. Table  9  shows performances of the PLMs as input features to BiLSTM and BiGRU on the dev set per epoch.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "E Updating New Spans Of Hate Speech Span Detection Samples With Pre-Processing Techniques",
      "text": "Due to performing pre-processing techniques, the span positions on the data samples can be changed. Therefore, we present Algorithm 1, which shows how to update new span positions of samples applied with pre-processing techniques in the Hate Speech Spans Detection task (UIT-ViHOS dataset). This algorithm takes as input a comment and its spans and returns the pre-processed comment and its span along with pre-processing techniques. return new_comment, new_label",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "F Tokenizations Of The Plms On Removing Diacritics Social Comments",
      "text": "We analyze several data samples to see the tokenized ability of Vietnamese social media textual data while removing diacritics on comments. Table  10  shows several non-diacritics Vietnamese social comments and their tokenizations with the tokenizers of the three best pre-trained language models, ViSoBERT (ours), PhoBERT, and TwHIN-BERT.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the results obtained from our",
      "page": 7
    },
    {
      "caption": "Figure 1: Impact of masking rate on our pre-trained",
      "page": 7
    },
    {
      "caption": "Figure 3: (in Appendix D) presents the perfor-",
      "page": 9
    },
    {
      "caption": "Figure 2: shows the average token length by Vietnamese social media downstream tasks",
      "page": 13
    },
    {
      "caption": "Figure 2: Average token length by tasks of PLMs.",
      "page": 13
    },
    {
      "caption": "Figure 3: Performances of the PLMs as input features to BiLSTM and BiGRU on the dev set per epoch on",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comments": "concặc cáilồn gìđây\nEnglish: Wut is dis fuckingd1ck",
          "ViSoBERT": "<s>, \"conc\", \"ặc\", \"cái\", \"l\", \"ồn\", \"gì\", \"đây\",\n\", </s>\n\"",
          "PhoBERT": "<s>, \"c o n @ @\", \"c @ @\", \"ặ c\", \"c á @ @\",\n\"i l @ @\", \"ồ n\", \"g @ @\",\"ì @ @\", \"đ â y\",\n<unk>, <unk>, <unk>, </s>"
        },
        {
          "Comments": "e cảmơn anh\nEnglish: Thankyou",
          "ViSoBERT": "<s>, \"e\", \"cảm\", \"ơn\", \"anh\", \"\n\", \"\n\", </s>",
          "PhoBERT": "<s>, \"e\", \"c ả @ @\", \"m @ @\", \"ơ n\", \"a n h\",\n<unk>, <unk>, </s>"
        },
        {
          "Comments": "d4y l4 vj du cko mot cau teencode\nEnglish: Th1s 1s 4 teencode s3nt3nc3",
          "ViSoBERT": "<s>, \"d\", \"4\", \"y\", \"l\", \"4\", \"vj\", \"du\", \"cko\", \"mot\",\n\"cau\", \"teen\", \"code\", </s>",
          "PhoBERT": "<s>, \"d @ @\", \"4 @ @\", \"y\", \"l @ @\", \"4\",\n\"v @ @\", \"j\", \"d u\", \"c k @ @\", \"o\", \"m o @ @\",\n\"t\", \"c a u\"; \"t e @ @\", \"e n @ @\", \"c o d e\", </s>"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "UIT-VSMEC\nUIT-HSD\nSA-VLSP2016\nViSpamReviews\nViHOS",
          "Train\nDev\nTest": "5,548\n686\n693\n24,048\n2,672\n6,680\n5,100\n-\n1,050\n14,306\n1,590\n3,974\n8,844\n1,106\n1,106",
          "Task": "Emotion Recognition (ER)\nHate Speech Detection (HSD)\nSentiment Analysis (SA)\nSpam Reviews Detection (SRD)\nHate Speech Spans Detection (HSSD)",
          "Evaluation Metrics": "Acc, WF1, MF1 (%)",
          "Classes": "7\n3\n3\n4\n3"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Avg": "",
          "Emotion Regconition": "Acc\nWF1\nMF1",
          "Hate Speech Detection": "Acc\nWF1\nMF1",
          "Sentiment Analysis": "Acc\nWF1\nMF1",
          "Spam Reviews Detection": "Acc\nWF1\nMF1",
          "Hate Speech Spans Detection": "Acc\nWF1\nMF1"
        },
        {
          "Model": "viBERT\nvELECTRA\nPhoBERTBase\nPhoBERTLarge",
          "Avg": "71.57\n72.43\n72.81\n73.47",
          "Emotion Regconition": "61.91\n61.98\n59.70\n64.79\n64.71\n61.95\n63.49\n63.36\n61.41\n64.71\n64.66\n62.55",
          "Hate Speech Detection": "85.34\n85.01\n62.07\n86.96\n86.37\n63.95\n87.12\n86.81\n65.01\n87.32\n86.98\n65.14",
          "Sentiment Analysis": "74.85\n74.73\n74.73\n74.95\n74.88\n74.88\n75.72\n75.52\n75.52\n76.52\n76.36\n76.22",
          "Spam Reviews Detection": "89.93\n89.79\n76.80\n89.83\n89.68\n76.23\n89.83\n89.75\n76.18\n90.12\n90.03\n76.88",
          "Hate Speech Spans Detection": "90.42\n90.45\n84.55\n90.59\n90.58\n85.12\n91.32\n91.38\n85.92\n91.44\n91.46\n86.56"
        },
        {
          "Model": "mBERT (cased)\nmBERT (uncased)\nXLM-RBase\nXLM-RLarge\nXLM-T\nTwHIN-BERTBase\nTwHIN-BERTLarge\nBernice\nViSoBERT",
          "Avg": "68.07\n67.66\n72.08\n73.40\n72.23\n71.60\n73.42\n72.49\n75.65",
          "Emotion Regconition": "56.27\n56.17\n53.48\n56.23\n56.11\n53.32\n60.92\n61.02\n58.67\n62.44\n61.37\n60.25\n64.64\n64.37\n59.86\n61.49\n60.88\n57.97\n64.21\n64.29\n61.12\n64.21\n64.27\n60.68\n68.10\n68.37\n65.88‡",
          "Hate Speech Detection": "83.55\n83.99\n60.62\n83.38\n81.27\n58.92\n86.36\n86.08\n63.39\n87.15\n86.86\n65.13\n86.22\n86.12\n63.48\n86.63\n86.23\n63.67\n87.23\n86.78\n65.23\n86.12\n86.48\n64.32\n88.51\n88.31\n68.77‡",
          "Sentiment Analysis": "67.14\n67.16\n67.16\n67.25\n67.22\n67.22\n76.38\n76.38\n76.38\n78.28\n78.21\n78.21\n75.66\n75.60\n75.60\n73.76\n73.72\n73.72\n76.92\n76.83\n76.83\n74.57\n74.90\n74.90\n77.83\n77.75\n77.75",
          "Spam Reviews Detection": "89.05\n88.89\n74.52\n88.92\n88.72\n74.32\n90.16\n89.96\n76.55\n90.36\n90.31\n76.75\n90.07\n90.11\n76.66\n90.25\n90.35\n76.98\n90.47\n90.42\n77.28\n90.22\n90.21\n76.89\n90.99\n90.92\n79.06‡",
          "Hate Speech Spans Detection": "89.88\n89.87\n84.57\n89.84\n89.82\n84.51\n90.74\n90.72\n85.42\n91.52\n91.50\n86.66\n90.88\n90.88\n85.53\n90.99\n90.90\n85.67\n91.45\n91.47\n86.65\n90.48\n90.06\n85.67\n91.62\n91.57\n86.80"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 7: (in Appendix C) shows the results critic marks when commenting on social media",
      "data": [
        {
          "PhoBERTLarge\n∆\nTwHIN-BERTLarge\n∆\nViSoBERT [♦]\n∆": "ViSoBERT [♠]",
          "65.21\n65.14\n62.81\n↑ 0.50\n↑ 0.48\n↑ 0.26\n62.03\n62.14\n59.25\n↓ 2.18\n↓ 1.15\n↓ 1.87\n66.52\n67.02\n64.55\n↓ 1.58\n↓ 1.35\n↓ 1.33": "68.10\n68.37\n65.88",
          "87.25\n86.72\n64.85\n↓ 0.07\n↓ 0.26\n↓ 0.29\n86.98\n86.32\n64.22\n↓ 0.25\n↓ 0.46\n↓ 1.01\n87.32\n87.12\n66.98\n↓ 1.19\n↓ 1.19\n↓ 1.79": "88.51\n88.31\n68.77",
          "76.72\n76.48\n76.48\n↑ 0.20\n↑ 0.12\n↑ 0.12\n75.00\n75.11\n75.11\n↓ 1.92\n↓ 1.72\n↓ 1.72\n76.25\n75.98\n75.98\n↓ 1.58\n↓ 1.77\n↓ 1.77": "77.83\n77.75\n77.75",
          "90.21\n90.09\n77.02\n↑ 0.09\n↑ 0.06\n↑ 0.10\n89.83\n89.75\n76.85\n↓ 0.64\n↓ 0.67\n↓ 0.43\n89.72\n89.69\n77.95\n↓ 1.27\n↓ 1.23\n↓ 1.11": "90.99\n90.92\n79.06",
          "91.53\n91.51\n86.62\n↑ 0.09\n↑ 0.05\n↑ 0.09\n91.32\n91.33\n86.42\n↓ 0.13\n↓ 0.14\n↓ 0.23\n91.58\n91.53\n86.72\n↓ 0.04\n↓ 0.04\n↓ 0.08": "91.62\n91.57\n86.80"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 8: presents performances of the pre-trained language models on downstream",
      "data": [
        {
          "Model": "",
          "Emotion Recognition": "Acc\nWF1\nMF1",
          "Hate Speech Detection": "Acc\nWF1\nMF1",
          "Sentiment Analysis": "Acc\nWF1\nMF1",
          "Spam Reviews Detection": "Acc\nWF1\nMF1",
          "Hate Speech Spans Detection": "Acc\nWF1\nMF1"
        },
        {
          "Model": "PhoBERTLarge\n∆\nTwHIN-BERTLarge\n∆\nViSoBERT [♣]\n∆",
          "Emotion Recognition": "64.94\n64.85\n62.71\n↑ 0.23\n↑ 0.19\n↑ 0.16\n64.42\n64.46\n61.28\n↑ 0.21\n↑ 0.17\n↑ 0.16\n68.25\n68.52\n65.94\n↑ 0.15\n↑ 0.15\n↑ 0.06",
          "Hate Speech Detection": "87.68\n87.25\n65.41\n↑ 0.36\n↑ 0.27\n↑ 0.27\n87.82\n87.28\n65.68\n↑ 0.59\n↑ 0.50\n↑ 0.45\n88.53\n88.33\n68.82\n↑ 0.02\n↑ 0.02\n↑ 0.08",
          "Sentiment Analysis": "76.80\n76.61\n76.61\n↑ 0.28\n↑ 0.25\n↑ 0.25\n77.17\n76.94\n76.94\n↑ 0.25\n↑ 0.11\n↑ 0.11\n78.01\n77.88\n77.88\n↑ 0.18\n↑ 0.13\n↑ 0.13",
          "Spam Reviews Detection": "89.47\n89.41\n76.12\n↓ 0.65\n↓ 0.62\n↓ 0.76\n89.49\n89.43\n76.35\n↓ 0.98\n↓ 0.99\n↓ 0.93\n90.83\n90.75\n78.77\n↓ 0.16\n↓ 0.17\n↓ 0.29",
          "Hate Speech Spans Detection": "91.73\n91.62\n86.59\n↑ 0.29\n↑ 0.16\n↑ 0.03\n91.74\n91.64\n86.67\n↑ 0.29\n↑ 0.17\n↑ 0.02\n91.89\n91.82\n86.93\n↑ 0.27\n↑ 0.25\n↑ 0.13"
        },
        {
          "Model": "ViSoBERT [♦]",
          "Emotion Recognition": "68.10\n68.37\n65.88",
          "Hate Speech Detection": "88.51\n88.31\n68.74",
          "Sentiment Analysis": "77.83\n77.75\n77.75",
          "Spam Reviews Detection": "90.99\n90.92\n79.06",
          "Hate Speech Spans Detection": "91.62\n91.57\n86.80"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 8: presents performances of the pre-trained language models on downstream",
      "data": [
        {
          "PhoBERTLarge\n∆\nTwHIN-BERTLarge\n∆\nViSoBERT [✠]\n∆": "ViSoBERT [♦]",
          "61.03\n60.80\n57.87\n↓ 3.68\n↓ 3.86\n↓ 4.68\n61.18\n60.98\n57.42\n↓ 3.03\n↓ 3.31\n↓ 3.70\n64.64\n64.53\n61.29\n↓ 3.43\n↓ 3.84\n↓ 4.59": "68.10\n68.37\n65.88",
          "85.97\n85.51\n61.96\n↓ 1.35\n↓ 1.47\n↓ 3.18\n86.85\n86.13\n63.14\n↓ 0.38\n↓ 0.65\n↓ 2.09\n87.85\n87.56\n66.54\n↓ 0.66\n↓ 0.75\n↓ 2.23": "88.51\n88.31\n68.77",
          "73.42\n73.28\n73.28\n↓ 3.10\n↓ 3.08\n↓ 2.94\n73.21\n73.11\n73.11\n↓ 3.71\n↓ 3.72\n↓ 3.72\n75.42\n75.44\n75.44\n↓ 2.41\n↓ 2.31\n↓ 2.31": "77.83\n77.75\n77.75",
          "89.80\n89.59\n75.53\n↓ 0.32\n↓ 0.44\n↓ 1.35\n89.91\n89.43\n76.32\n↓ 0.56\n↓ 0.99\n↓ 0.96\n90.76\n90.64\n78.15\n↓ 0.23\n↓ 0.28\n↓ 0.91": "90.99\n90.92\n79.06",
          "90.63\n90.69\n85.76\n↓ 0.81\n↓ 0.77\n↓ 0.80\n91.09\n90.72\n86.02\n↓ 0.36\n↓ 0.75\n↓ 0.63\n91.22\n91.24\n86.47\n↓ 0.40\n↓ 0.33\n↓ 0.33": "91.62\n91.57\n86.80"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comment": "PhoBERT",
          "cai con do choi do mua o dau nhi . cuoi deo nhat duoc\nmom .": "<s>, \"c a i\", \"c o n\", \"d o\", \"c h o @ @\", \"i\", \"d o\", \"m u a\",\n\"o\", \"d @ @\", \"a u\", \"n h i\", \".\", \"c u @ @\", \"o i\",\n\"d @ @\", \"e o\", \"n h @ @\", \"a t\", \"d u @ @\", \"o c\",\n\"m o m\", \".\", <unk>, <unk>, <unk>, </s>",
          "Oi bo cai lu thanh nien ham lol. Dep mat qua": "<s>, \"O @ @\", \"i\", \"b o\", \"c a i\", \"l u\", \"t h a n h\", \"n i @ @\",\n\"e n\", \"h a m\", \"l o @ @\", \"l\", \".\", \"D e @ @\", \"p\", \"m a t\",\n\"q u a\", <unk>, <unk>, </s>"
        },
        {
          "Comment": "TwHIN-BERT",
          "cai con do choi do mua o dau nhi . cuoi deo nhat duoc\nmom .": "<s>, \"cai\", \"con\", \"do\", \"cho\", \"i\", \"do\", \"mua\", \"o\", \"dau\",\n\"nhi\", \"\", \".\", \"cu\", \"oi\", \"de\", \"o\", \"nha\", \"t\", \"du\", \"oc\",\n\"mom\", \"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>",
          "Oi bo cai lu thanh nien ham lol. Dep mat qua": "<s>, \"Oi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"nie\", \"n\", \"ham\", \"lol\",\n\".\", \"De\", \"p\", \"mat\", \"qua\", \"\", \"\n\", \"\n\", </s>"
        },
        {
          "Comment": "ViSoBERT",
          "cai con do choi do mua o dau nhi . cuoi deo nhat duoc\nmom .": "<s>, \"cai\", \"con\", \"do\", \"choi\", \"do\", \"mua\", \"o\", \"dau\", \"nhi\",\n\".\",\"cu\", \"oi\", \"d\", \"eo\", \"nhat\", \"duoc\", \"m\", \"om\", \".\",\n\"\n\", </s>",
          "Oi bo cai lu thanh nien ham lol. Dep mat qua": "<s>, \"O\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"ni\", \"en\", \"h\", \"am\",\n\"lol\", \".\", \"D\", \"ep\", \"mat\", \"qua\", \"\n\", </s>"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comment": "PhoBERT",
          "cai con do chơi do mua o đâu nhi . cười deo nhat duoc\nmom .": "<s>, \"c a i\", \"c o n\", \"d o\", \"c h ơ i\", \"d o\", \"m u a\", \"o\",\n\"đ â u\", \"n h i\", \".\", \"c ư ờ i\", \"d @ @\", \"e o\", \"n h @ @\",\n\"a t\", \"d u @ @\", \"o c\", \"m o m\", \".\",<unk>, <unk>,\n<unk>, </s>",
          "Ôi bo cai lu thanh niên hãm lol. Dep mat qua": "<s>, \"Ô i\", \"b o\", \"c a i\", \"l u\", \"t h a n h _ n i ê n\", \"h ã m\",\n\"l o @ @\", \"l\", \".\", \"D e @ @\", \"p\", \"m a t\", \"q u a\", <unk>,\n<unk>, </s>"
        },
        {
          "Comment": "TwHIN-BERT",
          "cai con do chơi do mua o đâu nhi . cười deo nhat duoc\nmom .": "<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhi\", \"\", \".\", \"cười\", \"de\", \"o\", \"nha\", \"t\", \"du\", \"oc\",\n\"mom\", \"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>",
          "Ôi bo cai lu thanh niên hãm lol. Dep mat qua": "<s>, \"Ô\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"niên\", \"\", \"hã\", \"m\",\n\"lol\", \".\", \"De\", \"p\", \"mat\", \"qua\", \"\", \"\n\", \"\n\", </s>"
        },
        {
          "Comment": "ViSoBERT",
          "cai con do chơi do mua o đâu nhi . cười deo nhat duoc\nmom .": "<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhi\", \".\", \"cười\", \"d\", \"eo\", \"nhat\", \"duoc\", \"m\", \"om\",\n\".\", \"\n\", </s>",
          "Ôi bo cai lu thanh niên hãm lol. Dep mat qua": "<s>, \"Ôi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"n\", \"iên\", \"hã\", \"m\",\n\"lol\", \".\", \"D\", \"ep\", \"mat\", \"qua\", \"\n\", </s>"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comment": "PhoBERT",
          "cai con do chơi do mua o đâu nhỉ . cười đéo nhặt duoc\nmom .": "<s>, \"c a i\", \"c o n\", \"d o\", \"c h ơ i\", \"d o\", \"m u a\", \"o\",\n\"đ â u\", \"n h ỉ\", \".\", \"c ư ờ i\", \"đ @ @\", \"é o\", \"n h ặ t\",\n\"d u @ @\", \"o c\", \"m o m\", \".\", <unk>, <unk>, <unk>, </s>",
          "Ôi bo cai lu thanh niên hãm lol. Dep mặt quá": "<s>, \"Ô i\", \"b o\", \"c a i\", \"l u\", \"t h a n h _ n i ê n\", \"h ã m\",\n\"l o @ @\", \"l\", \".\", \"D e @ @\", \"p\", \"m ặ t\", \"q u á\", <unk>,\n<unk>, </s>"
        },
        {
          "Comment": "TwHIN-BERT",
          "cai con do chơi do mua o đâu nhỉ . cười đéo nhặt duoc\nmom .": "<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\", \"nhỉ\",\n\"\", \".\", \"cười\", \"đ\", \"é\", \"o\", \"nh\", \"ặt\", \"du\", \"oc\", \"mom\",\n\"\", \".\", \"\", \"\n\", \"\n\", \"\n\", </s>",
          "Ôi bo cai lu thanh niên hãm lol. Dep mặt quá": "<s>, \"Ô\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"niên\", \"\", \"hã\", \"m\",\n\", \"\n\", </s>\n\"lol\", \".\", \"De\", \"p\", \"mặt\", \"quá\", \"\", \""
        },
        {
          "Comment": "ViSoBERT",
          "cai con do chơi do mua o đâu nhỉ . cười đéo nhặt duoc\nmom .": "<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhỉ\", \".\", \"cười\", \"đéo\", \"nh\", \"ặt\", \"duoc\", \"m\", \"om\",\n\".\", \"\n\", </s>",
          "Ôi bo cai lu thanh niên hãm lol. Dep mặt quá": "<s>, \"Ôi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"n\", \"iên\", \"hã\", \"m\",\n\"lol\", \".\", \"D\", \"ep\", \"mặt\", \"quá\", \"\n\", </s>"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comment": "PhoBERT",
          "cai con do chơi đó mua ở đâu nhỉ . cười đéo nhặt duoc\nmồm .": "<s>, \"c a i\", \"c o n\", \"d o\", \"c h ơ i\", \"đ ó\", \"m u a\", \"ở\",\n\"đ â u\", \"n h ỉ\", \".\", \"c ư ờ i\", \"đ @ @\", \"é o\", \"n h ặ t\",\n\"d u @ @\", \"o c\", \"m ồ m\", \".\", <unk>, <unk>, <unk>, </s>",
          "Ôi bo cai lu thanh niên hãm lol. Đep mặt quá": "<s>, \"Ô i\", \"b o\", \"c a i\", \"l u\", \"t h a n h _ n i ê n\", \"h ã m\",\n\"l o @ @\", \"l\", \".\", \"Đ e p _ @ @\", \"m ặ t\", \"q u á\", <unk>,\n<unk>, </s>"
        },
        {
          "Comment": "TwHIN-BERT",
          "cai con do chơi đó mua ở đâu nhỉ . cười đéo nhặt duoc\nmồm .": "<s>, \"cai\", \"con\", \"do\", \"chơi\", \"do\", \"mua\", \"o\", \"đâu\",\n\"nhỉ\", \"\", \".\", \"cười\", \"đ\", \"é\", \"o\", \"nh\", \"ặt\", \"du\", \"oc\",\n\", \"\n\", \"\n\", </s>\n\"mom\", \"\", \".\", \"\", \"",
          "Ôi bo cai lu thanh niên hãm lol. Đep mặt quá": "<s>, \"Ô\", \"i\", \"bo\", \"cai\", \"lu\", \"thanh\", \"niên\", \"\", \"hã\", \"m\",\n\"lol\", \".\", \"Đep\", \"mặt\", \"quá\", \"\", \"\n\", \"\n\", </s>"
        },
        {
          "Comment": "ViSoBERT",
          "cai con do chơi đó mua ở đâu nhỉ . cười đéo nhặt duoc\nmồm .": "<s>, \"cai\", \"con\", \"do\", \"chơi\", \"đó\", \"mua\", \"ở\", \"đâu\",\n\"nhỉ\", \".\", \"cười\", \"đéo\", \"nh\", \"ặt\", \"duoc\", \"mồm\", \".\",\n\"\n\", </s>",
          "Ôi bo cai lu thanh niên hãm lol. Đep mặt quá": "<s>, \"Ôi\", \"bo\", \"cai\", \"lu\", \"thanh\", \"n\", \"iên\", \"hã\", \"m\",\n\"lol\", \".\", \"Đep\", \"mặt\", \"quá\", \"\n\", </s>"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Uit-Hsd"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "556 of CLEAN, OFFENSIVE, and HATE class",
      "venue": "556 of CLEAN, OFFENSIVE, and HATE class"
    },
    {
      "citation_id": "3",
      "title": "XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond",
      "authors": [
        "Francesco Barbieri",
        "Luis Espinosa Anke",
        "Jose Camacho-Collados"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "4",
      "title": "SciB-ERT: A Pretrained Language Model for Scientific Text",
      "authors": [
        "Iz Beltagy",
        "Kyle Lo",
        "Arman Cohan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "5",
      "title": "Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos",
      "authors": [
        "Ilias Chalkidis",
        "Manos Fergadiotis"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.261"
    },
    {
      "citation_id": "6",
      "title": "Unsupervised Cross-lingual Representation Learning at Scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.747"
    },
    {
      "citation_id": "7",
      "title": "Bernice: A Multilingual Pre-trained Encoder for Twitter",
      "authors": [
        "Alexandra Delucia",
        "Shijie Wu",
        "Aaron Mueller",
        "Carlos Aguirre",
        "Philip Resnik",
        "Mark Dredze"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "9",
      "title": "Detecting Spam Reviews on Vietnamese E-Commerce Websites",
      "authors": [
        "Co Van Dinh",
        "Son Luu",
        "Anh Gia-Tuan Nguyen"
      ],
      "year": "2022",
      "venue": "Intelligent Information and Database Systems",
      "doi": "10.1007/978-3-031-21743-2_48"
    },
    {
      "citation_id": "10",
      "title": "Emotion Recognition for Vietnamese Social Media Text",
      "authors": [
        "Anh Vong",
        "Duong Ho",
        "Danh Huynh-Cong Nguyen",
        "Linh Hoang Nguyen",
        "Duc-Vu Thi-Van Pham",
        "Kiet Nguyen",
        "Ngan Van Nguyen",
        "-Thuy Luu",
        "Nguyen"
      ],
      "year": "2019",
      "venue": "Computational Linguistics: 16th International Conference of the Pacific Association for Computational Linguistics",
      "doi": "10.1007/978-981-15-6168-9_27"
    },
    {
      "citation_id": "11",
      "title": "ViHOS: Hate Speech Spans Detection for Vietnamese",
      "authors": [
        "Gia Phu",
        "Canh Hoang",
        "Khanh Luu",
        "Kiet Tran",
        "Ngan Nguyen",
        "Nguyen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence",
      "authors": [
        "Yibo Hu",
        "Mohammadsaleh Hosseini",
        "Erick Skorupa Parolin",
        "Javier Osorio",
        "Latifur Khan",
        "Patrick Brandt",
        "Vito D' Orazio"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.400"
    },
    {
      "citation_id": "13",
      "title": "IndoBERTweet: A pretrained language model for Indonesian Twitter with effective domain-specific vocabulary initialization",
      "authors": [
        "Fajri Koto",
        "Jey Han Lau",
        "Timothy Baldwin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.833"
    },
    {
      "citation_id": "14",
      "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
      "authors": [
        "Taku Kudo",
        "John Richardson"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/D18-2012"
    },
    {
      "citation_id": "15",
      "title": "Crosslingual Language Model Pretraining",
      "authors": [
        "Guillaume Lample",
        "Alexis Conneau"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "16",
      "title": "Diacritics generation and application in hate speech detection on Vietnamese social networks",
      "authors": [
        "Phuong Le-Hong"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2021.107504"
    },
    {
      "citation_id": "17",
      "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho",
        "Jaewoo Kang"
      ],
      "year": "2019",
      "venue": "Bioinformatics",
      "doi": "10.1093/bioinformatics/btz682"
    },
    {
      "citation_id": "18",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "citation_id": "19",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "20",
      "title": "A Large-Scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts",
      "authors": [
        "Son Luu",
        "Kiet Van Nguyen",
        "Ngan Luu",
        "-Thuy Nguyen"
      ],
      "year": "2021",
      "venue": "Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices",
      "doi": "10.1007/978-3-030-79457-6_35"
    },
    {
      "citation_id": "21",
      "title": "ViHealthBERT: Pre-trained language models for Vietnamese in health text mining",
      "authors": [
        "Minh Nguyen",
        "Vu Vu Hoang Tran",
        "Hoang",
        "Duc Huy",
        "Trung Huu Ta",
        "Steven Bui",
        "Hung Quoc",
        "Truong"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "22",
      "title": "Vietnamese: An Essential Grammar. Essential grammar",
      "authors": [
        "B Ngo"
      ],
      "year": "2020",
      "venue": "Vietnamese: An Essential Grammar. Essential grammar"
    },
    {
      "citation_id": "23",
      "title": "PhoBERT: Pre-trained language models for Vietnamese",
      "authors": [
        "Quoc Dat",
        "Anh Nguyen",
        "Nguyen"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
      "doi": "10.18653/v1/2020.findings-emnlp.92"
    },
    {
      "citation_id": "24",
      "title": "BERTweet: A pre-trained language model for English tweets",
      "authors": [
        "Thanh Dat Quoc Nguyen",
        "Anh Vu",
        "Nguyen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.2"
    },
    {
      "citation_id": "25",
      "title": "VLSP Shared Task: Sentiment Analysis",
      "authors": [
        "Hung Huyen Tm Nguyen",
        "Quyen Nguyen",
        "Ngo",
        "X Luong",
        "Vu Vu",
        "Mai Tran",
        "X Bach",
        "Cuong A Ngo",
        "Le"
      ],
      "year": "2018",
      "venue": "Journal of Computer Science and Cybernetics"
    },
    {
      "citation_id": "26",
      "title": "Exploiting Vietnamese social media characteristics for textual emotion recognition in Vietnamese",
      "authors": [
        "Khang Phuoc-Quy Nguyen",
        "Kiet Van Nguyen"
      ],
      "year": "2020",
      "venue": "2020 International Conference on Asian Language Processing (IALP)"
    },
    {
      "citation_id": "27",
      "title": "PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing",
      "authors": [
        "Linh The Nguyen",
        "Dat Quoc Nguyen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations"
    },
    {
      "citation_id": "28",
      "title": "SMTCE: A social media text classification evaluation benchmark and BERTology models for Vietnamese",
      "authors": [
        "Luan Nguyen",
        "Kiet Nguyen",
        "Ngan Nguyen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th Pacific Asia Conference on Language, Information and Computation"
    },
    {
      "citation_id": "29",
      "title": "RoBERTuito: a pre-trained language model for social media text in Spanish",
      "authors": [
        "Juan Manuel Pérez",
        "Damián Ariel Furman",
        "Laura Alonso Alemany",
        "Franco Luque"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "30",
      "title": "ViT5: Pretrained text-to-text transformer for Vietnamese language generation",
      "authors": [
        "Long Phan",
        "Hieu Tran",
        "Hieu Nguyen",
        "Trieu Trinh"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop"
    },
    {
      "citation_id": "31",
      "title": "Vietnamese hate and offensive detection using PhoBERT-CNN and social media streaming data",
      "authors": [
        "An Khanh Quoc Tran",
        "Phu Nguyen",
        "Gia Hoang",
        "Canh Duc Luu",
        "Trong-Hop Do",
        "Kiet Van Nguyen"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-022-07745-w"
    },
    {
      "citation_id": "32",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research",
      "doi": "10.5555/3455716.3455856"
    },
    {
      "citation_id": "33",
      "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
      "authors": [
        "Laila Rasmy",
        "Yang Xiang",
        "Ziqian Xie",
        "Cui Tao",
        "Degui Zhi"
      ],
      "year": "2021",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "34",
      "title": "ViDeBERTa: A powerful pre-trained language model for Vietnamese",
      "authors": [
        "Cong Dao Tran",
        "Nhut Huy Pham",
        "Anh-Tuan Nguyen",
        "Son Hy",
        "Tu Vu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EACL 2023"
    },
    {
      "citation_id": "35",
      "title": "BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese",
      "authors": [
        "Duong Nguyen Luong Tran",
        "Minh Le",
        "Dat Quoc Nguyen"
      ],
      "year": "2022",
      "venue": "Proceedings of the 23rd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "36",
      "title": "Improving sequence tagging for Vietnamese text using transformer-based neural models",
      "authors": [
        "Thi Oanh Tran",
        "Phuong Hong"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th Pacific Asia conference on language, information and computation"
    },
    {
      "citation_id": "37",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Should You Mask 15% in Masked Language Modeling?",
      "authors": [
        "Alexander Wettig",
        "Tianyu Gao",
        "Zexuan Zhong",
        "Danqi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations",
      "authors": [
        "Xinyang Zhang",
        "Yury Malkov",
        "Omar Florez",
        "Serim Park",
        "Brian Mcwilliams",
        "Jiawei Han",
        "Ahmed El-Kishky"
      ],
      "year": "2022",
      "venue": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations",
      "arxiv": "arXiv:2209.07562"
    },
    {
      "citation_id": "40",
      "title": "TWilBert: Pre-trained deep bidirectional transformers for Spanish Twitter",
      "authors": [
        "José Ángel González",
        "Lluís-F Hurtado",
        "Ferran Pla"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.09.078"
    }
  ]
}