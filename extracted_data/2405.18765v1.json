{
  "paper_id": "2405.18765v1",
  "title": "Large Brain Model For Learning Generic Rep-Resentations With Tremendous Eeg Data In Bci",
  "published": "2024-05-29T05:08:16Z",
  "authors": [
    "Wei-Bang Jiang",
    "Li-Ming Zhao",
    "Bao-Liang Lu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Electroencephalography (EEG) is a method to record an electrogram of the spontaneous electrical activity of the brain. It is typically non-invasive, with the EEG electrodes placed along the scalp using the international 10-20 system. EEG signals can be formulated as a matrix of real numbers X ∈ R C×T , where C is the number of EEG electrodes (channels) that may vary depending on the acquisition equipment used, and T represents the total number of samples, which is related to the collection time and sampling rate. As highly objective physiological signals, EEG has demonstrated remarkable potential in seizure epilepsy classification  (Boonyakitanont et al., 2020) , acute stress detection  (Sharma et al., 2022) , sleep stage classification  (Aboalayon et al., 2016) , motor imagery recognition  (Amin et al., 2019) , abnormal identification  (Roy et al., 2019) , emotion analysis  (Suhaimi et al., 2020) , and auditory attention detection  (Biesmans et al., 2016) .\n\nNumerous deep learning models have been proposed to address the aforementioned tasks in their respective fields. Some works apply convolutional neural networks (CNN) across and within raw EEG channels to encode spatial and temporal features  (Lawhern et al., 2018) , while others preprocess the data using short-time Fourier transform (STFT) and employ Graph Neural Network  (GNN)  on the resulting spectrograms to obtain semantic features of brain area links  (Song et al., 2018) . Researchers also segment the signal and use a CNN segment encoder with a downstream sequence model such as recurrent neural networks (RNN) to capture temporal dynamics  (Xu et al., 2020) . These models primarily focus on EEG samples that adhere to specific task formats, mainly because the equipment used to collect EEG differs between datasets, which introduces mismatched channels and variable lengths. Meanwhile, EEG data collection is quite expensive, which makes it challenging to build large EEG datasets specifically designed for a particular task. To prevent overfitting, the parameters of these models need to be regulated, which in turn hampers the model's ability to learn EEG expressions and limits its generalizability. Consequently, we discovered that current EEG models are typically proprietary and lack the capacity to perform cross-task learning.\n\nRecently, we have been impressed by the capabilities of LLMs  (Ouyang et al., 2022; Wei et al., 2022) . Specifically, Transformer-based models have demonstrated promising results in natural language processing tasks, which highlights the potential of self-supervised pre-training as a means for harnessing large-scale data. These masked language modeling tasks involve randomly masking some proportion of tokens within a text and then recovering the masked tokens based on the Transformer encoding results of the corrupted text. Motivated by these methods, we propose to apply reconstruction ideas to pre-train neural Transformers. However, it is a daunting task to directly apply LLM-style pre-training to EEG data. The challenges are summarized as follows:\n\n1) Lack of sufficient EEG data. The acquisition of EEG data is significantly challenging compared to natural language and image data. Moreover, the annotation of EEG data usually requires a lot of effort on the part of experts in the corresponding field, thus leading to the fact that only small labeled datasets exist for specific tasks in BCI, where EEG signals are often collected from a small number of participants, typically less than tens of hours in duration. As a result, there is currently no single EEG dataset that is large enough to support the training of LEMs. It remains problems Q1: how to utilize large-scale unlabeled EEG data? and Q2: how much data is needed to train LEMs?.\n\n2) Diverse configurations of EEG collection. Despite the availability of the international 10-20 system to ensure standardization in EEG testing, users may choose to collect data using EEG caps with different electrode numbers or patch electrodes based on their practical application needs. Thus, how to handle the diverse formats of EEG data in order to match the input units of neural Transformers remains a significant research endeavor.\n\n3) Lack of effective EEG representation learning paradigm. Low signal-to-noise ratio (SNR) and different types of noise are the greatest challenges. Additionally, balancing temporal and spatial characteristics is crucial for effective EEG representation learning. Despite the availability of various deep learning-based EEG representation learning paradigms, such as CNN, RNN, and GNN, for raw EEG data, many researchers still prefer to design artificial EEG features due to these challenges.\n\nIn this paper, our objective is to devise a versatile large EEG model that can efficiently handle diverse EEG datasets with varying channels and lengths. By utilizing unsupervised training on a substantial amount of EEG data, we envision the model to possess universal EEG data comprehension capabilities, enabling it to quickly adapt to various EEG downstream tasks. We collected over 2,500 hours of diverse EEG data across various tasks and formats from about 20 datasets. These datasets were primarily obtained from publicly available EEG datasets, as well as our own collected EEG data. Raw EEG signals were first segmented into EEG channel patches to deal with the issues of variant electrodes and time length. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer to generate neural vocabulary. Specifically, the tokenizer was trained by predicting the Fourier spectrum of the original signal. During pre-training, part of EEG patches are masked while the objective of the neural Transformer is to predict masked tokens from visible patches. We pre-trained three models with varying parameter sizes, ranging from 5.8M to 369M, which are the largest models in BCI ever, and fine-tuned them on four distinct types of downstream tasks encompassing both classification and regression. The contributions of this work are summarized as follows:\n\n• Large-scale EEG pre-training. We collected and pre-trained a large-scale neural Transformer model on more than 2,500 hours of diverse EEG data. As far as we know, this is the first time such extensive and varied datasets have been utilized for EEG pre-training.  spatial and temporal embeddings. Hence, one pre-trained LaBraM can adapt to any downstream dataset with different configurations.\n\n• Effective EEG representation learning. The utilization of the neural Transformer allows the model to effectively capture both temporal and spatial features of EEG signals with varying channels and lengths, making it suitable for a wide range of downstream tasks in EEG analysis. We further define a neural codebook that offers a compact, versatile, and meaningful representation of EEG signals. We resolve Q1 by leveraging this codebook to pre-train LaBraM by masked EEG modeling. The empirical performance demonstrates the effectiveness of our proposed method and paves the way for further development in aligning this codebook with natural language. • Comprehensive experiments on downstream datasets. We evaluate our LaBraMs on four representative downstream tasks in BCI, where they surpass all SOTA methods by a large margin. Additionally, we conduct experiments to answer Q2 by scaling the pre-training data size and conclude the amount of pre-training data required for models of different sizes in Section 3.6.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this section, we detail the whole framework of LaBraM. We first formulate the multi-channel EEG signals as X ∈ R C×T , where C is the number of EEG electrodes (channels) and T is the total timestamps. The electrode set of X is formulated as C X = {c i1 , c i2 , ..., c i C }, where C X ⊆ C = {c 1 , c 2 , ..., c |C| } and C is the universal set of channels in the international 10-20 system.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "We introduce the neural Transformer, a general architecture for decoding EEG signals that can deal with any input EEG signals with arbitrary number of channels and time length, as illustrated in Figure  1 . The key operation for achieving this is segmenting the EEG signals into patches, inspired by patch embeddings in images  (Dosovitskiy et al., 2021) . Assume that the timestamp for each sample is t and the stride is s. X can be segmented into ⌊ T -t s ⌋ + 1 samples, and each sample x ∈ R C×t . We use a w-length window without overlap to segment each EEG channel into patches, obtaining x = {x ci j ,k ∈ R w |j = 1, 2, ..., C, k = 1, 2, ..., ⌊ t w ⌋}. The total number of the patches x is |x| = C⌊ t w ⌋. Temporal Encoder. As EEG is of high resolution in the temporal domain, it is vital to extract temporal features before patch-wise interaction by self-attention. We employ a temporal encoder which consists of several temporal convolution blocks to encode each EEG patch into a patch embedding. The temporal convolution block is composed of a 1-D convolution layer, a group normalization layer  (Wu & He, 2018) , and a GELU activation function  (Hendrycks & Gimpel, 2016) . We denote the output patch embeddings from the temporal encoder as where d is the dimension of the embeddings.\n\nTemporal & Spatial Embedding. In order to enable the model to be aware of the temporal and spatial information of patch embeddings, we initialize a temporal embedding list T E = {te 1 , te 2 , ..., te tmax } and a spatial embedding list SE = {se 1 , se 2 , ..., se |C| }, both of which are ddimension and are set learnable during training. Note that tmax is the hyperparameter determining the maximum number of time patches and ⌊ t w ⌋ ≤ tmax. Meanwhile, for each channel c i , we can find its corresponding spatial embedding se i in the spatial embedding list SE. Thus, given one arbitrary output embedding e ci j ,k in Equation 1 from the temporal encoder, we add the corresponding temporal and spatial embeddings to it:\n\nwhere temporal and spatial embeddings act as absolute position encoding.\n\nTransformer Encoder. Finally, the sequence of embeddings will be directly fed into the Transformer encoder  (Vaswani et al., 2017) . To make the training of Transformer more stable and efficient, we incorporate some modifications  (Dehghani et al., 2023) . First, we add layer normalization to the queries and keys before the dot-product attention mechanism, which avoids over-large values in attention logits:\n\nwhere d head is the dimension of one head in the multi-head attention and LN denotes the layerNorm  (Ba et al., 2016) . Next, we omit the bias term in QKV computations, which accelerates the training without performance degradation. For downstream tasks, we use average pooling on the output embeddings followed by task-specific prediction heads.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Neural Tokenizer Training",
      "text": "Prior to pre-training LaBraM through masking and prediction, we need to tokenize the EEG into discrete tokens. We propose the vector-quantized neural spectrum prediction, which is trained by predicting the Fourier spectrum, as shown in Figure  2 . The key components are the neural tokenizer which encodes EEG samples into patch representations and the neural decoder which decodes the Fourier spectrum from neural embeddings. The idea is basically inspired by VQ-VAE  (Van Den Oord et al., 2017)  which encodes images into discrete latent representations.\n\nNeural Tokenizer. We define a neural codebook V = {v i |i = 1, ..., K} ∈ R K×D , where K is the number of the discrete neural embeddings and D is the dimensionality of each embedding. Given an EEG signal sample x, the neural tokenizer whose backbone is just described in Section 2.1 first encode it to patch representations p = {p i |i = 1, ..., N }, where N = C⌊ t w ⌋. After that, we utilize a quantizer to quantize all the patch representations into the neural codebook embeddings. The codebook looks up the nearest neighbor of each patch p i in the neural codebook V. This procedure can be formulated as\n\nwhere ℓ 2 represents ℓ 2 normalization and z i is the quantized vector after the quantizer. This is equivalent to finding the closest neural embedding by cosine similarity and such ℓ 2 normalization improves the codebook utilization  (Peng et al., 2022) .\n\nFourier Spectrum Prediction. Unlike images that are of high signal-to-noise ratio, EEG signals are of low signal-to-noise ratio and have characteristics of apparent stochasticity, nonstationarity, and nonlinearity nature, which make it hard to reconstruct the original signals well  (Moss et al., 2004) . In our previous experiments, the loss fails to converge while directly reconstructing raw EEG signals. Instead, the frequency and phase distribution from the Fourier spectrum of EEG signals reveals the underlying neurophysiological activities of the brain  (Wu et al., 2022) . Therefore, we propose to reconstruct the amplitude and phase from discrete neural tokens for training the neural tokenizer and neural decoder. For an EEG patch\n\nof channel c and time k in a sample x, we apply the Discrete Fourier Transform (DFT) as follows\n\nwhere m ∈ [1, N ] and j is the imaginary unit. We rewrite Equation 5 using Euler's formula as\n\nNote that xm c,k indicates the spectrum of the sequence at frequency ω m = 2πm N . Consequently, the amplitude and phase can be calculated as\n\nwhere Re and Im stand for the real and imaginary parts of a complex number. It is worthwhile to mention that we adopt z-score normalization to normalize A m and ϕ m within a sample for stable convergence.\n\nAfter being tokenized by the quantizer, the normalized discrete neural embeddings {ℓ 2 (v zi )|i = 1, ..., N } are passed into the neural decoder that comprises several Transformer blocks. The output representations are aggregated by average pooling followed by two specific prediction heads to regress the spectrum amplitude o A and phase o ϕ , respectively. The mean squared error (MSE) loss is utilized to guide the prediction. Ultimately, the total loss for training the vector-quantized neural spectrum prediction is defined as\n\nwhere D is all EEG data and sg represents the stop-gradient operation that is defined as an identity at the forward pass and has zero gradients. To make the codebook update more stable, we employ the exponential moving average strategy  (Van Den Oord et al., 2017) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pre-Training Labram",
      "text": "Masked EEG Modeling. To enforce LaBraM learning generic representations with tremendous EEG data, we propose masked EEG modeling. The whole procedure is presented in Figure  2 . As formulated in Section 2.1, given an EEG sample x, the temporal encoder first transforms it to patch embeddings e = {e i |i = 1, ..., N }. We randomly generate a mask M = {m i |i = 1, ..., N } where m i ∈ {0, 1} with r proportion of m is 1. After that, we replace the masked patches of x with the learnable mask token e M ∈ R d . The corrupted EEG patches can be denoted as e M = {e i :\n\n., N }, which will be added by temporal and spatial embeddings, and then fed into Transformer encoder. We denote the output hidden vectors as h = {h i |i = 1, ..., N }, which are used to predict the corresponding neural tokens through a linear classifier:\n\n(10) Our objective training loss is\n\nSymmetric Masking. We further propose a symmetric masking strategy to improve training efficiency. We calculate the inverse of the generated mask M, obtaining M = {∼ m i |i = 1, ..., N }.\n\nSimilarly, we use the new mask M to perform the masked EEG modeling, obtaining the masked EEG prediction loss L sym M . The motivation is from two aspects: 1) Since we introduce the neural tokenizer, there will be an extra computation overhead, i.e., one forward pass for each EEG sample. Thus, the symmetric masking reuses the same discrete representations, thus improving training efficiency. 2) The symmetric masking provides more masking perspectives in one batch, increasing the data divergency. This simple strategy boosts downstream performance as demonstrated in Appendix I.\n\nFinally, the overall training objective for pre-training LaBraM is\n\n3 EXPERIMENTS",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Evaluation Datasets",
      "text": "We systematically evaluate our LaBraM on the following downstream datasets:\n\n• TUAB (abnormal detection)  (Obeid & Picone, 2016 ): A corpus of EEGs which are 23-channel and sampled at 256 Hz. All data have been annotated as normal or abnormal. There are total 409,455 10-second samples that we use for binary classification to predict normal/abnormal. • TUEV (event type classification)  (Obeid & Picone, 2016 ): This corpus is a subset of TUEG that contains annotations of EEG segments as one of six classes: (1) spike and sharp wave (SPSW), (2) generalized periodic epileptiform discharges (GPED), (3) periodic lateralized epileptiform discharges (PLED), (4) eye movement (EYEM), (  5 ) artifact (ARTF) and (  6 ) background (BCKG).\n\nThe EEG signals contain 23 channels at 256 Hz and are segmented into 112,491 5-second samples.\n\nMore experimental results on other BCI tasks can be found in Appendix F.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Setup",
      "text": "Model Variants. We devise three different configurations of LaBraM: LaBraM-Base, LaBraM-Large, and LaBraM-Huge. The number of parameters is 5.8M for LaBraM-Base, 46M for LaBraM-Large, and 369M for LaBraM-Huge, respectively, which is increased by enlarging the depth of the Transformer encoder and hidden sizes. More details of the architecture settings are listed in Appendix C. Unless otherwise specified, the results are from LaBraM-Base in this paper.\n\nThe time window w of a patch is set to 200 (1 second). To ensure stable computing resource usage, the number of patches (sequence length) is limited to 256. That means, for example, the time length of EEG with 64 (32) channels is set to 4 (8) seconds. As for the window stride (data stride), it is set to 4 seconds in order to cover all training data as well as boost the training speed.\n\nPre-training & Fine-tuning. For pre-training LaBraM and the vector-quantized neural spectrum prediction, we collect a total time of over 2,500 hours from public datasets and our self-collected data as described in Appendix D. Note that the four downstream datasets are excluded from the pretraining datasets. For the data splitting of TUAB and TUEV, we strictly follow the same strategy as BIOT  (Yang et al., 2023a)   Preprocessing. We only employ very little of the necessary preprocessing. We first filter the EEG signals between 0.1 Hz and 75 Hz to remove low-frequency noise. Then, a notch filter of 50 Hz is applied to avoid power-line interference. Finally, all EEG signals are resampled to 200 Hz. As the range of EEG value is typically between -0.1 mV to 0.1 mV, we normalize it by setting the unit to 0.1 mV to guarantee the value mainly between -1 to 1.\n\nBaselines & Metrics. The baselines are from  Yang et al. (2023a) , where we choose the best results to compare with. We use the following metrics for comparison: 1) Balanced Accuracy: the average of recall on each class, which is utilized for both binary and multi-class classification. 2) AUC-PR: area under the precision-recall curve for binary classification. 3) AUROC: area under the receiver operating characteristic curve, which is used for binary classification as well. 4) Cohen's Kappa: a measure of agreement between categorical variables X and Y , which is calculated from the observed and expected frequencies on the diagonal of a square contingency table. It is used for multi-class classification. 5) Weighted F1: A harmonic mean of the precision and recall, where the relative contribution of precision and recall to the F1 score are equal. We use it to evaluate multi-class classification. We set AUROC as the monitor score for binary classification and Cohen's Kappa as the monitor score for multi-class classification. Figure  3  compares the convergence curves of the total pre-training loss and masked EEG modeling accuracy between the base, large, and huge models. We observe that a larger model with more parameters can converge to a smaller loss and higher accuracy. Notably, the loss of the huge model seems to have an obvious downward trend while the accuracy tends to increase if we train it longer. This observation suggests scaling up the model size has the potential to obtain better performance.\n\nPublished as a conference paper at ICLR 2024",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Comparison With State-Of-The-Art",
      "text": "Table  1  and Table  2  present the results of state-of-the-art baselines as well as LaBraM from TUAB and TUEV. The results demonstrate that our LaBraM-Base model outperformed all baselines on various evaluation metrics for both tasks. Particularly in the more challenging multi-class classification task of TUEV, our model achieved a significant improvement in performance. In our own model, we observed that as the number of model parameters increased, the LaBraM-Huge model performed the best, followed by the LaBraM-Large model and then the LaBraM-Base model. We attribute this good performance to the increase in pre-training data volume and model parameters. We believe that with sufficient data volume, large-scale EEG models can learn more generalizable EEG patterns, leading to improved performance on a wide range of downstream tasks in EEG analysis.  to eliminate the influence of the pretraining data on downstream tasks, we compared the results with or without incorporating the downstream task dataset into the pre-training process or not. It is noted that the recordings of TUAB and TUEV are disjoint from recordings of pre-training datasets.\n\nAs Figure  4  illustrates, the performance of the model on the downstream task was not significantly affected by whether or not to incorporate the downstream task datasets into the model's pre-training process. This demonstrates that our model has the capability to learn universal EEG representations, and provides guidance for the collection of more EEG data in the future. In other words, we do not need to expend a significant amount of effort on labeling EEG data during the pre-training process.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Scaling Data Size",
      "text": "Although we have collected approximately 2,500 hours of EEG data, it is still relatively small compared to the sample size in natural language processing and image processing. We answer Q2 about the demand for data size to train LaBraMs with different sizes by scaling the pre-training data size. As illustrated in Figure  5 , the performance of the Base model with 500 hours of training exceeds that of the 2500-hour model on TUAB, while approaching over 90% of the 2500-hour performance on TUEV. For the Large model, performance generally improves with increased data volume, though the growth rate slows after 1000 hours. In contrast, the Huge model exhibits a noticeable upward trend in performance as data size continues to expand. Therefore, we believe that with further expansion of the dataset, our model can achieve better performance. The question of how much EEG data is required for pre-training a large EEG model is undoubtedly an important issue worth exploring in this field. Nevertheless, 2,500 hours is not the answer to this question at least. Our observation basically follows the scaling law  (Kaplan et al., 2020) , from which we deduce that the Huge model would continue to perform better with the data size on the order of at least ten thousand hours.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes a Large Brain Model (LaBraM) that learns universal embeddings through unsupervised pre-training on over 2,500 hours of diverse EEG data. The LaBraM is capable of handling diverse EEG datasets due to the segmentation of raw EEG signals into channel patches and the use of vector-quantized neural spectrum prediction to generate a rich semantic tokenizer during pretraining. Additionally, the neural Transformer architecture enables effective representation learning of both temporal and spatial features of EEG signals, making it suitable for a wide range of downstream tasks in EEG analysis. The LaBraM was validated on multiple downstream tasks, including abnormal detection, event type classification, emotion recognition, and gait prediction. Our experiments show that the LaBraM outperforms all SOTA methods in their respective fields. In the end, we hope our work can have implications for future developments in EEG-based deep learning models with improved perceptual capabilities and generalizability.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Related Work",
      "text": "Self-supervised Pre-training. In recent years, self-supervised pre-training has made significant progress in natural language processing and computer vision. BERT  (Devlin et al., 2018)  innovatively proposed the idea of masking part of the input sentences and then reconstructing them. The GPT series  (Radford et al., 2018; 2019; Brown et al., 2020)  proposed to pre-train large language models by a large corpus of data in an autoregressive way. Both studies improved the fine-tuning performance significantly in various downstream tasks. In computer vision, iGPT  (Chen et al., 2020)  firstly brought the idea from GPT to pre-train a vision model. BEiT  (Bao et al., 2022)  pioneerly trained a vision tokenizer and leveraged BERT-like pre-training for training a vision Transformer. MAE  (He et al., 2022)  and SimMIM  (Xie et al., 2022)  practiced masked image modeling by simply reconstructing the raw pixels and achieved appreciable improvement.\n\nLearning with Heterogeneous Datasets. MMM introduced a pre-training framework built on the unified topology and obtained topology-agnostic representations  (Yi et al., 2023) .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B Labram Pre-Training Analysis",
      "text": "The pre-training of LaBraM can be interpreted as the training of a variational autoencoder  (Kingma & Welling, 2014; Bao et al., 2022) . We denote the original EEG sample as x, the corrupted EEG by masking as x M , and its Fourier spectrum (amplitude and phase) as x. The focus is on the evidence lower bound (ELBO) of the log-likelihood p(x|x M ), which involves recovering the Fourier spectrum of the original EEG signals from the masked perspective:\n\n) where q ϕ (z|x) represents the neural tokenizer that encodes the EEG sample into discrete neural tokens, p ψ (x|z) denotes the neural decoder predicting the Fourier spectrum from given neural tokens, and p θ (z|x M ) is the LaBraM pre-training for masked EEG modeling, where the LaBraM encoder reconstructs neural tokens from the corrupted EEG input.\n\nThe whole framework is optimized through a two-stage procedure as  (Van Den Oord et al., 2017) . For the first stage, we train the neural tokenizer as a discrete variational autoencoder by minimizing the reconstruction loss -E zi∼q ϕ (z|xi) (log p ψ (x i |z i ) with a uniform prior. For the second stage, we set q ϕ as well as p ψ fixed and learn the prior p θ by minimizing the loss D KL . For simplicity, q ϕ (z|x i ) is defined as a one-point distribution with the most likely neural tokens ẑi = arg max z q ϕ (z|x i ). Consequently, we can rewrite Equation  13 as\n\nwhere the first term is the objective for vector-quantized neural spectrum prediction and the second term is the objective for LaBraM pre-training. • SEED Series  (Zheng & Lu, 2015; Zheng et al., 2018; Liu et al., 2022)    (von Weltin et al., 2017) : This is another subset of TUEG that contains annotations of slowing events (23 channels, 256 Hz). This corpus has been used to study common error modalities in automated seizure detection. (total time: 20.59 hours) • Self-collected EEG Data  (Jiang et al., 2023; 2021; Luo et al., 2022; Li et al., 2021; Tao & Lu, 2020)  We further visualize how the amplitude and phase in the Fourier domain are reconstructed. As depicted in Figure  7 , although some details are missing, the overall trend of the amplitude is reconstructed well. In contrast, the reconstruction of the phase is not as good as the amplitude. Nevertheless, it can be seen from Figure  6  that there is still a stable decrease in the reconstruction loss during training, which indicates the discrete codebook does learn high-level information from the Fourier domain.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "C Hyperparameter Settings",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "2Uljlqdo((*Vljqdov 2Uljlqdo$Psolwxgh 5Hfrqvwuxfwhg$Psolwxgh 2Uljlqdo3Kdvh 5Hfrqvwuxfwhg3Kdvh",
      "text": "Figure  7 : Visualization of reconstructed Fourier spectrum. Note that we only visualize half of the results since DFT is conjugate symmetric.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "F More Experiments On Other Bci Tasks",
      "text": "We conduct two additional BCI tasks on the following datasets:\n\n• SEED-V (emotion recognition)  (Liu et al., 2021) : An emotion EEG dataset containing five emotion categories (happy, sad, neutral, disgust, and fear). The experiment collected EEG data (62 channels, 1000 Hz) from 20 subjects, including 10 males and 10 females. Each subject participated in the experiments three times and each session included fifteen video clips corresponding to the five emotions, where each video clip lasted for several minutes. The EEG signals are segmented into 148,080 1-second samples. • MoBI (gait prediction)  (He et al., 2018) : A mobile brain-body imaging dataset acquired during treadmill walking in a BCI task, which is a lower limb motor imagery dataset. Six goniometers were employed to record bilateral joint angles on the legs (hip, knee, and ankle). The objective is to regress the angles for 12 targets (left leg and right leg). The data were collected from 8 healthy subjects, each of whom had three identical trials. The EEG signals (60 channels, 100 Hz) were recorded by the ActiCap system. Setting the stride to 50 ms, the dataset involves 575,830 2-second samples.\n\nFor SEED-V, as there are fifteen trials for one session, we separate the fifteen trials into three parts with an equal number of trials, i.e., 5:5:5. We merge each part from all sessions of subjects and derive the training, validation, and test set. As SEED-V is overall balanced, we consider accuracy instead of balanced accuracy as a metric to compare performance. Note that some implementation details are a bit different from default settings on this dataset due to different characteristics (peak learning rate: 5e-4 (L) 5e-3 (H); total epochs: 50 (L/H), warmup epochs: 4 (L) 5 (H)).\n\nFor MoBI, each trial consisted of a 15-minute treadmill walking session (training session), followed by a 5-minute treadmill walking session (test session) with a closed-loop BCI. To validate the model, we split the training session into two parts: the first 10 minutes of EEG and its corresponding joint data were used as training data, while the last 5 minutes of data were used as validation data. Meanwhile, we combined all the training data, validation data, and testing data of the eight subjects to form corresponding larger training datasets, validation datasets, and testing datasets. Since most angles are typically lower than 90 • , the target angles are divided by 90 for normalization. We report the average value of 12 targets for each metric.\n\nAs the task of MoBI is regression, we choose the following metrics to evaluate the performance of different methods: 1) Pearson's correlation: Pearson's correlation coefficient which is used to quantify the models' regression effect. It measures the linear correlation between two variables X and Y. 2) R2 score: R 2 (coefficient of determination) regression score function, which measures how well a statistical model predicts an outcome. 3) RMSE: Root Mean Square Error is the standard deviation of the residuals (prediction errors). R2 score is utilized as the monitor to select the best model. MSE loss is the objective to optimize the models.\n\nThe experimental results are presented in Figure  6 . On SEED-V, LaBraMs outperform all baseline methods on all metrics. The phenomenon that the performance increases when the model gets larger is also observed. For MoBI, our Base model archives competitive results compared to the best baseline method. Whereas, the Large and Huge models obtain better performance among all methods.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "G Effectiveness Of Vector-Quantized Neural Spectrum Prediction",
      "text": "To verify the effectiveness of vector-quantized neural spectrum prediction, we elaborate on three types of experimental settings as illustrated in Table  7 . The comparison between LaBraM and Setting 1 demonstrates that the codebook is effective for masked EEG modeling. LaBraM obtains the best performance on TUEV and the lowest standard deviations on TUAB. There is an interesting observation that masked EEG modeling with the assistance of training an auxiliary neural tokenizer (LaBraM and Setting 1) performs greatly better on TUEV while the naive masked EEG modeling (Setting 2 and Setting 3) performs slightly better on TUAB. One explanation for this phenomenon is that learning semantic representations from the neural tokenizer and codebook significantly benefits high-level downstream tasks like TUEV which classifies different types of events. Whereas, TUAB is a low-level downstream task where the clinically normal/abnormal EEG segments can be easily distinguished visually. Hence, simply reconstructing origin signals or their Fourier spectrum is able to perform well on these low-level tasks but fails to obtain satisfying performance on high-level tasks. Setting 1: We directly predict output embeddings of the neural tokenizer by maximizing cosine similarity instead of predicting the discrete neural tokens from the codebook. Setting 2: We discard the neural tokenizer and directly reconstruct raw EEG patches by minimizing MSE loss.\n\nSetting 3: We discard the neural tokenizer and reconstruct the Fourier spectrum (amplitude and phase) of raw EEG patches by minimizing MSE loss.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "H Ablation On Mask Ratio",
      "text": "In this experiment, we conduct different settings of the mask ratio to explore its impact. It is noted that we introduce the symmetric masking strategy, so we only need to validate half of the mask ratios. As the mask ratio is set to r, the symmetric masking will mask 1 -r proportion of EEG patches. The ablation results are provided in Table  8 , where experiments are conducted on TUAB and TUEV. It can be induced that the best mask ratio is 0.4 (0.6) for TUAB and 0.5 (0.5) for TUEV. Moreover, 0.5 (0.5) is the second-best mask ratio for TUAB while the remaining mask ratios are incredibly close. The performance for mask ratios except 0.5 (0.5) is also similar to each other. Notably, the mask ratio of 0.5 (0.5) achieves smaller standard deviations on both TUAB and TUEV. Therefore, we conclude that 0.5 (0.5) is a relatively good mask ratio for the masked EEG modeling of LaBraM pre-training.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of LaBraM, i.e., neural Transformer. All input EEG signals",
      "page": 3
    },
    {
      "caption": "Figure 1: The key operation for achieving this is segmenting the EEG signals into patches, inspired",
      "page": 3
    },
    {
      "caption": "Figure 2: Overview of neural tokenizer training and LaBraM pre-training. Up: We train a neural",
      "page": 4
    },
    {
      "caption": "Figure 2: The key components are the neural tok-",
      "page": 5
    },
    {
      "caption": "Figure 3: The pre-training loss curve and masked EEG modeling accuracy curve.",
      "page": 7
    },
    {
      "caption": "Figure 3: compares the convergence curves of the total pre-training loss and masked EEG modeling",
      "page": 7
    },
    {
      "caption": "Figure 4: A comparison of the model’s performance on the TUAB and TUEV datasets when incor-",
      "page": 8
    },
    {
      "caption": "Figure 4: illustrates, the performance of the model on the downstream task was not significantly",
      "page": 9
    },
    {
      "caption": "Figure 5: , the performance of the Base model with 500 hours of training exceeds that",
      "page": 9
    },
    {
      "caption": "Figure 5: A comparison of the performance of the Base model, Large model, and Huge model on",
      "page": 9
    },
    {
      "caption": "Figure 6: The reconstruction loss curve of amplitude and",
      "page": 18
    },
    {
      "caption": "Figure 6: that there is still",
      "page": 18
    },
    {
      "caption": "Figure 7: Visualization of reconstructed Fourier spectrum. Note that we only visualize half of the",
      "page": 18
    },
    {
      "caption": "Figure 6: On SEED-V, LaBraMs outperform all baseline",
      "page": 19
    },
    {
      "caption": "Figure 8: Balanced Accuracy",
      "page": 21
    },
    {
      "caption": "Figure 8: Comparison with model without pre-training.",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "748": "114"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "98": "603"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "460": "139"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "69": "114"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "98"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "139"
        },
        {
          "4": "53"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "M",
          "M": "",
          "Column_3": "M"
        },
        {
          "Column_1": "",
          "M": "",
          "Column_3": "M"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "M",
          "M": "",
          "Column_3": "M"
        },
        {
          "Column_1": "",
          "M": "M",
          "Column_3": ""
        },
        {
          "Column_1": "M",
          "M": "M",
          "Column_3": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "748"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "460"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ModelSize": "0.79M\n1.6M\n3.2M\n2.4M\n3.5M\n3.2M"
        },
        {
          "ModelSize": "5.8M\n46M\n369M"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ModelSize": "0.79M\n1.6M\n3.2M\n2.4M\n3.5M\n3.2M"
        },
        {
          "ModelSize": "5.8M\n46M\n369M"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sleep stage classification using EEG signal analysis: a comprehensive survey and new investigation",
      "authors": [
        "Khald Ali I Aboalayon",
        "Miad Faezipour",
        "Saeid Wafaa S Almuhammadi",
        "Moslehpour"
      ],
      "year": "2016",
      "venue": "Entropy"
    },
    {
      "citation_id": "2",
      "title": "Deep Learning for EEG motor imagery classification based on multi-layer CNNs feature fusion",
      "authors": [
        "Mansour Syed Umar Amin",
        "Ghulam Alsulaiman",
        "Mohamed Muhammad",
        "M Amine Mekhtiche",
        "Shamim Hossain"
      ],
      "year": "2019",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Uncovering the structure of clinical EEG signals with self-supervised learning",
      "authors": [
        "Hubert Banville",
        "Omar Chehab",
        "Aapo Hyvärinen",
        "Denis-Alexander Engemann",
        "Alexandre Gramfort"
      ],
      "year": "2021",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "6",
      "title": "BEit: BERT pre-training of image transformers",
      "authors": [
        "Hangbo Bao",
        "Li Dong",
        "Songhao Piao",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "Auditory-inspired speech envelope extraction methods for improved EEG-based auditory attention detection in a cocktail party scenario",
      "authors": [
        "Wouter Biesmans",
        "Neetha Das",
        "Tom Francart",
        "Alexander Bertrand"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "8",
      "title": "The non-invasive berlin brain-computer interface: fast acquisition of effective performance in untrained subjects",
      "authors": [
        "Benjamin Blankertz",
        "Guido Dornhege",
        "Matthias Krauledat",
        "Klaus-Robert Müller",
        "Gabriel Curio"
      ],
      "year": "2007",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "9",
      "title": "A review of feature extraction and performance evaluation in epileptic seizure detection using EEG",
      "authors": [
        "Poomipat Boonyakitanont",
        "Apiwat Lek-Uthai",
        "Krisnachai Chomtho",
        "Jitkomut Songsiri"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Recent advances in the TUH EEG corpus: improving the interrater agreement for artifacts and epileptiform events",
      "authors": [
        "Buckwalter",
        "S Chhin",
        "I Rahman",
        "Obeid",
        "Picone"
      ],
      "year": "2021",
      "venue": "2021 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)"
    },
    {
      "citation_id": "12",
      "title": "Generative pretraining from pixels",
      "authors": [
        "Mark Chen",
        "Alec Radford",
        "Rewon Child",
        "Jeffrey Wu",
        "Heewoo Jun",
        "David Luan",
        "Ilya Sutskever"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "13",
      "title": "Scaling vision transformers to 22 billion parameters",
      "authors": [
        "Mostafa Dehghani",
        "Josip Djolonga",
        "Basil Mustafa",
        "Piotr Padlewski",
        "Jonathan Heek",
        "Justin Gilmer",
        "Andreas Steiner",
        "Mathilde Caron",
        "Robert Geirhos",
        "Ibrahim Alabdulmohsin",
        "Rodolphe Jenatton",
        "Lucas Beyer",
        "Michael Tschannen",
        "Anurag Arnab",
        "Xiao Wang",
        "Carlos Ruiz",
        "Matthias Minderer",
        "Joan Puigcerver",
        "Utku Evci",
        "Manoj Kumar",
        "Sjoerd Van Steenkiste",
        "Gamaleldin Fathy Elsayed",
        "Aravindh Mahendran",
        "Fisher Yu",
        "Avital Oliver",
        "Fantine Huot",
        "Jasmijn Bastings",
        "Mark Collier",
        "Alexey Gritsenko",
        "Vighnesh Birodkar",
        "Cristina Vasconcelos",
        "Yi Tay",
        "Thomas Mensink",
        "Alexander Kolesnikov",
        "Filip Pavetic",
        "Dustin Tran",
        "Thomas Kipf",
        "Mario Lucic",
        "Xiaohua Zhai",
        "Daniel Keysers",
        "Jeremiah Harmsen",
        "Neil Houlsby"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning"
    },
    {
      "citation_id": "14",
      "title": "Eeg synchronization analysis for seizure prediction: A study on data of noninvasive recordings",
      "authors": [
        "Paolo Detti",
        "Giampaolo Vatti",
        "Garazi Zabalo",
        "Manrique De"
      ],
      "year": "2020",
      "venue": "Processes"
    },
    {
      "citation_id": "15",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "16",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "17",
      "title": "Generalizable Movement Intention Recognition with Multiple Heterogeneous EEG Datasets",
      "authors": [
        "Xiao Gu",
        "Jinpei Han",
        "Guang-Zhong Yang",
        "Benny Lo"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
      "doi": "10.1109/ICRA48891.2023.10160462"
    },
    {
      "citation_id": "18",
      "title": "EEG Decoding for Datasets with Heterogenous Electrode Configurations using Transfer Learning Graph Neural Networks",
      "authors": [
        "Jinpei Han",
        "Xiaoxi Wei",
        "Aldo Faisal"
      ],
      "year": "2023",
      "venue": "EEG Decoding for Datasets with Heterogenous Electrode Configurations using Transfer Learning Graph Neural Networks",
      "arxiv": "arXiv:2306.13109"
    },
    {
      "citation_id": "19",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "A mobile brain-body imaging dataset recorded during treadmill walking with a brain-computer interface",
      "authors": [
        "Yongtian He",
        "Phat Trieu",
        "Kevin Luu",
        "Sho Nathan",
        "Jose L Contreras- Nakagome",
        "Vidal"
      ],
      "year": "2018",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "21",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "22",
      "title": "Discriminating Surprise and Anger from EEG and Eye Movements with a Graph Network",
      "authors": [
        "Wei-Bang Jiang",
        "Li-Ming Zhao",
        "Ping Guo",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": "10.1109/BIBM52615.2021.9669637"
    },
    {
      "citation_id": "23",
      "title": "Multimodal Adaptive Emotion Transformer with Flexible Modality Inputs on A Novel Dataset with Continuous Labels",
      "authors": [
        "Wei-Bang Jiang",
        "Xuan-Hao Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM '23",
      "doi": "10.1145/3581783.3613797"
    },
    {
      "citation_id": "24",
      "title": "Development of expert-level classification of seizures and rhythmic and periodic patterns during eeg interpretation",
      "authors": [
        "Jin Jing",
        "Wendong Ge",
        "Shenda Hong",
        "Marta Fernandes",
        "Zhen Lin",
        "Chaoqi Yang",
        "Sungtae An",
        "Aaron Struck",
        "Aline Herlopian",
        "Ioannis Karakis"
      ],
      "year": "2023",
      "venue": "Neurology"
    },
    {
      "citation_id": "25",
      "title": "Scaling laws for neural language models",
      "authors": [
        "Jared Kaplan",
        "Sam Mccandlish",
        "Tom Henighan",
        "Tom Brown",
        "Benjamin Chess",
        "Rewon Child",
        "Scott Gray",
        "Alec Radford",
        "Jeffrey Wu",
        "Dario Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "26",
      "title": "Auto-Encoding Variational Bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations, ICLR 2014"
    },
    {
      "citation_id": "27",
      "title": "Brain Invaders calibration-less P300-based BCI with modulation of flash duration Dataset (bi2015a)",
      "authors": [
        "Louis Korczowski",
        "Martine Cederhout",
        "Anton Andreev",
        "Grégoire Cattan",
        "Pedro Luiz",
        "Coelho Rodrigues",
        "Violette Gautheret",
        "Marco Congedo"
      ],
      "year": "2019",
      "venue": "Research report"
    },
    {
      "citation_id": "28",
      "title": "BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data",
      "authors": [
        "Demetres Kostas",
        "Stephane Aroca-Ouellette",
        "Frank Rudzicz"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "29",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "30",
      "title": "Motor imagery EEG classification algorithm based on CNN-LSTM feature fusion network",
      "authors": [
        "Hongli Li",
        "Man Ding",
        "Ronghua Zhang",
        "Chunbo Xiu"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "31",
      "title": "Discrimination of Decision Confidence Levels from EEG Signals",
      "authors": [
        "Rui Li",
        "Le-Dian Liu",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)",
      "doi": "10.1109/NER49283.2021.9441086"
    },
    {
      "citation_id": "32",
      "title": "Aggregating intrinsic information to enhance BCI performance through federated learning",
      "authors": [
        "Rui Liu",
        "Yuanyuan Chen",
        "Anran Li",
        "Yi Ding",
        "Han Yu",
        "Cuntai Guan"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "33",
      "title": "Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "34",
      "title": "Identifying similarities and differences in emotion recognition with EEG and eye movements among Chinese, German, and French People",
      "authors": [
        "Wei Liu",
        "Wei-Long Zheng",
        "Ziyi Li",
        "Si-Yuan Wu",
        "Lu Gan",
        "Bao-Liang Lu"
      ],
      "year": "2022",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "35",
      "title": "Multi-channel EEG recordings during 3,936 grasp and lift trials with varying weight and friction",
      "authors": [
        "Ewa Matthew D Luciw",
        "Jarocka",
        "Benoni B Edin"
      ],
      "year": "2014",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "36",
      "title": "Multimodal emotion recognition in response to oil paintings",
      "authors": [
        "Shuai Luo",
        "Yu-Ting Lan",
        "Dan Peng",
        "Ziyi Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2022",
      "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)",
      "doi": "10.1109/EMBC48229.2022.9871630"
    },
    {
      "citation_id": "37",
      "title": "Objective and subjective evaluation of online error correction during p300-based spelling",
      "authors": [
        "Perrin Margaux",
        "Maby Emmanuel",
        "Daligault Sébastien",
        "Bertrand Olivier",
        "Mattout Jérémie"
      ],
      "year": "2012",
      "venue": "Advances in Human-Computer Interaction"
    },
    {
      "citation_id": "38",
      "title": "Stochastic resonance and sensory information processing: a tutorial and review of application",
      "authors": [
        "Frank Moss",
        "Lawrence Ward",
        "Walter Sannita"
      ],
      "year": "2004",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "39",
      "title": "The temple university hospital EEG data corpus",
      "authors": [
        "Iyad Obeid",
        "Joseph Picone"
      ],
      "year": "2016",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "40",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "41",
      "title": "Transformer convolutional neural networks for automated artifact detection in scalp EEG",
      "authors": [
        "Wei Yan Peh",
        "Yuanyuan Yao",
        "Justin Dauwels"
      ],
      "year": "2022",
      "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "42",
      "title": "Beit v2: Masked image modeling with vector-quantized visual tokenizers",
      "authors": [
        "Zhiliang Peng",
        "Li Dong",
        "Hangbo Bao",
        "Qixiang Ye",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "Beit v2: Masked image modeling with vector-quantized visual tokenizers",
      "arxiv": "arXiv:2208.06366"
    },
    {
      "citation_id": "43",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan",
        "Tim Salimans",
        "Ilya Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "44",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "45",
      "title": "ChronoNet: A deep recurrent neural network for abnormal EEG identification",
      "authors": [
        "Subhrajit Roy",
        "Isabell Kiral-Kornek",
        "Stefan Harrer"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence in Medicine: 17th Conference on Artificial Intelligence in Medicine"
    },
    {
      "citation_id": "46",
      "title": "Emotion detection in the loop from brain signals and facial images",
      "authors": [
        "Arman Savran",
        "Koray Ciftci",
        "Guillame Chanel",
        "Javier Mota",
        "Hong Viet",
        "Bülent Sankur",
        "Lale Akarun",
        "Alice Caplier",
        "Michele Rombaut"
      ],
      "year": "2006",
      "venue": "eINTERFACE'06-SIMILAR NoE Summer Workshop on Multimodal Interfaces"
    },
    {
      "citation_id": "47",
      "title": "BCI2000: a general-purpose brain-computer interface (BCI) system",
      "authors": [
        "Gerwin Schalk",
        "Dennis Mcfarland",
        "Thilo Hinterberger",
        "Niels Birbaumer",
        "Jonathan Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "48",
      "title": "The temple university hospital seizure detection corpus",
      "authors": [
        "Vinit Shah",
        "Eva Von Weltin",
        "Silvia Lopez",
        "James Mchugh",
        "Lillian Veloso",
        "Meysam Golmohammadi",
        "Iyad Obeid",
        "Joseph Picone"
      ],
      "year": "2018",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "49",
      "title": "Evolutionary inspired approach for mental stress detection using EEG signal",
      "authors": [
        "Lakhan Dev Sharma",
        "Vijay Bohat",
        "Maria Habib",
        "Al-Zoubi Ala",
        "Hossam Faris",
        "Ibrahim Aljarah"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "50",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Transformer-based spatial-temporal feature learning for EEG decoding",
      "authors": [
        "Yonghao Song",
        "Xueyu Jia",
        "Lie Yang",
        "Longhan Xie"
      ],
      "year": "2021",
      "venue": "Transformer-based spatial-temporal feature learning for EEG decoding",
      "arxiv": "arXiv:2106.11170"
    },
    {
      "citation_id": "52",
      "title": "EEG-based emotion recognition: A state-of-the-art review of current trends and opportunities",
      "authors": [
        "James Nazmi Sofian Suhaimi",
        "Jason Mountstephens",
        "Teo"
      ],
      "year": "2020",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "53",
      "title": "Emotion Recognition under Sleep Deprivation Using a Multimodal Residual LSTM Network",
      "authors": [
        "Le-Yan Tao",
        "Bao-Liang Lu"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN48605.2020.9206957"
    },
    {
      "citation_id": "54",
      "title": "Prediction of reaction time and vigilance variability from spatio-spectral features of resting-state EEG in a long sustained attention task",
      "authors": [
        "Mastaneh Torkamani-Azar",
        "Sumeyra Demir Kanik",
        "Serap Aydin",
        "Mujdat Cetin"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "55",
      "title": "Raw EEG Data",
      "authors": [
        "Logan Trujillo"
      ],
      "year": "2020",
      "venue": "Raw EEG Data",
      "doi": "10.18738/T8/SS2NHB"
    },
    {
      "citation_id": "56",
      "title": "The effect of electroencephalogram (EEG) reference choice on information-theoretic measures of the complexity and integration of EEG signals",
      "authors": [
        "Candice Logan T Trujillo",
        "Ruben Stanfield",
        "Vela"
      ],
      "year": "2017",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "57",
      "title": "Neural discrete representation learning",
      "authors": [
        "Aaron Van Den",
        "Oriol Oord",
        "Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "59",
      "title": "Big data resources for EEGs: Enabling deep learning research",
      "authors": [
        "Veloso",
        "E Mchugh",
        "S Von Weltin",
        "I Lopez",
        "Obeid",
        "Picone"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing in Medicine and Biology Symposium"
    },
    {
      "citation_id": "60",
      "title": "Electroencephalographic slowing: A primary source of error in automatic seizure detection",
      "authors": [
        "Tameem Eva Von Weltin",
        "Vinit Ahsan",
        "Dawer Shah",
        "Meysam Jamshed",
        "Iyad Golmohammadi",
        "Joseph Obeid",
        "Picone"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing in Medicine and Biology Symposium"
    },
    {
      "citation_id": "61",
      "title": "BrainBERT: Self-supervised representation learning for intracranial recordings",
      "authors": [
        "Christopher Wang",
        "Vighnesh Subramaniam",
        "Uri Adam",
        "Gabriel Yaari",
        "Boris Kreiman",
        "Ignacio Katz",
        "Andrei Cases",
        "Barbu"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations"
    },
    {
      "citation_id": "62",
      "title": "Emergent abilities of large language models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler"
      ],
      "year": "2022",
      "venue": "Emergent abilities of large language models",
      "arxiv": "arXiv:2206.07682"
    },
    {
      "citation_id": "63",
      "title": "neuro2vec: Masked fourier spectrum prediction for neurophysiological representation learning",
      "authors": [
        "Di Wu",
        "Siyuan Li",
        "Jie Yang",
        "Mohamad Sawan"
      ],
      "year": "2022",
      "venue": "neuro2vec: Masked fourier spectrum prediction for neurophysiological representation learning",
      "arxiv": "arXiv:2204.12440"
    },
    {
      "citation_id": "64",
      "title": "Group normalization",
      "authors": [
        "Yuxin Wu",
        "Kaiming He"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "65",
      "title": "Simmim: A simple framework for masked image modeling",
      "authors": [
        "Zhenda Xie",
        "Zheng Zhang",
        "Yue Cao",
        "Yutong Lin",
        "Jianmin Bao",
        "Zhuliang Yao",
        "Qi Dai",
        "Han Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "66",
      "title": "A one-dimensional cnn-lstm model for epileptic seizure recognition using eeg signal analysis",
      "authors": [
        "Gaowei Xu",
        "Tianhe Ren",
        "Yu Chen",
        "Wenliang Che"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "67",
      "title": "BIOT: Biosignal transformer for cross-data learning in the wild",
      "authors": [
        "Chaoqi Yang",
        ", M Brandon Westover",
        "Jimeng Sun"
      ],
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems, 2023a"
    },
    {
      "citation_id": "68",
      "title": "Self-Supervised Electroencephalogram Representation Learning for Automatic Sleep Staging: Model Development and Evaluation Study",
      "authors": [
        "Chaoqi Yang",
        "Cao Xiao",
        "M Brandon Westover",
        "Jimeng Sun"
      ],
      "year": "2023",
      "venue": "JMIR AI"
    },
    {
      "citation_id": "69",
      "title": "Learning Topology-Agnostic EEG Representations with Geometry-Aware Modeling",
      "authors": [
        "Ke Yi",
        "Yansen Wang",
        "Kan Ren",
        "Dongsheng Li"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "70",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics",
      "doi": "10.1109/TCYB.2018.2797176"
    },
    {
      "citation_id": "71",
      "title": "Investigating critical frequency bands and channels for EEGbased emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development",
      "doi": "10.1109/TAMD.2015.2431497"
    }
  ]
}