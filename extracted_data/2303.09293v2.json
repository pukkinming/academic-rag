{
  "paper_id": "2303.09293v2",
  "title": "A Transformer-Based Approach To Video Frame-Level Prediction In Affective Behaviour Analysis In-The-Wild",
  "published": "2023-03-16T13:13:13Z",
  "authors": [
    "Dang-Khanh Nguyen",
    "Ngoc-Huynh Ho",
    "Sudarshan Pant",
    "Hyung-Jeong Yang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, transformer architecture has been a dominating paradigm in many applications, including affective computing. In this report, we propose our transformer-based model to handle Emotion Classification Task in the 5 th Affective Behavior Analysis In-the-wild Competition. By leveraging the attentive model and the synthetic dataset, we attain a score of 0.4775 on the validation set of Aff-Wild2, the dataset provided by the organizer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Representing human emotion is a fundamental topic in behavior analysis  [3, 6] . A naïve approach is using a discrete classification of 6 or 7 basic emotions. Besides, emotion could be also represented in a continuous 2dimensional space (i.e., valence-arousal). Other proposals use the presentation of the facial action units as the emotional representation. Analyzing human interaction from various perspectives  [11, 14]  can help researchers deeply understand their feeling and behavior.\n\nThe 5 th Competition on Affective Behavior Analysis Inthe-wild (ABAW5) aims to achieve that target by conducting three challenges corresponding to three methods representing the human emotion: Valence-Arousal (VA) estimation  [9, 10] , Expression (EX) recognition, and Action Unit (AU) detection  [8] . Hume AI also collaborates with the organizer on the Emotional Reaction Intensity (ERI) estimation challenge, which is a brand-new task compared to the three above traditional ones  [2, 4, 5] . This is also a regression problem measuring the level of each emotion label.\n\nThis paper introduces our solution for the three conventional challenges of the competition. We adopt a pre-trained EfficientNet  [15]  as a feature extractor and manipulate a transformer encoder  [12]  to learn the relationship among a sequence of facial frames. Noticeably, in Expression classification task, to resolve the imbalance limitation, we utilize the generated data from ABAW4's Learning from Synthetic Data (LSD) challenge  [1]  to increase the scale of the training dataset. It considerably improves the performance of our discriminative model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "We utilize the transformer paradigm for our proposed method. To create a sequence of embeddings from a series of facial frames, we exploit the pre-trained EfficientNet  [13]  well trained with multiple facial analysis tasks. This pretrained network extracts a feature vector for each image. Then, the features are projected to the embedding space by a linear layer before being fed to a transformer encoder. The encoder is designed similarly to the conventional transformer encoder  [12]  with multi-head attention and feed-forward networks. The output from the encoder is sent to the emotion detection head, which is simply a fullyconnected neural network with one hidden layer.\n\nIn the classification task, to overcome the imbalance issue of the dataset, we apply weighted scores for each emotion in the cross-entropy loss. On the other hand, due to the quadratic complexity of the input length of the attention mechanism, we cannot put all frames of a video into the transformer. The frame sequence is fragmented into multiple segments with a common length. The detail of our proposed method is illustrated in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset And Experiment Dataset",
      "text": "We used Aff-Wild2  [7]  and the synthetic facial dataset  [1]  for our training process. The Aff-Wild2 for expression classification is a collection of 546 videos. More than 2 million frames are extracted from the videos and annotated as one among 8 emotional classes. The classification comprises 6 basic emotions: \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\" and \"surprise\". Two additional classes are included, \"neural\" and \"other\". They are also the two classes having the largest number of samples in the dataset.\n\nTo enlarge the scale of the training dataset, we leverage the synthetic facial dataset from the 4 th ABAW competition, Learning from Synthetic Data challenge. The corpus includes 277,251 synthesized images classified into 6 basic classes. We merge this dataset with the training split of the Aff-Wild2 dataset. It does not only increase the size of training samples but also decreases the influence of the imbalance issue.\n\nThe Aff-Wild2 is provided in three versions: raw videos, cropped images, and cropped-aligned images. We used the cropped-aligned images in our experiments. These images have the same size of 112x112 pixels. Regarding the synthetic data, the image size is 128x128 pixels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setting",
      "text": "The sequence of frames in each video is split into segments comprising 64 frames. All images are normalized and resized to 224x224 pixels. Regarding the transformer, we use a hidden size and feed-forward size of 512, dropout ratio is 0.1. The emotion detection head is a neural network with one hidden layer with a size of 256. We train our models in 20 epochs with a batch size of 64 and a learning rate of 0.001. The Adam optimizer is used with the weight decay of .\n\nTo boost the performance, we apply the ensemble of 3 models with different configurations of the transformer encoder. The number of heads and layers in 3 settings are  (4, 4) ,  (8, 4) , and (4, 6), respectively. Soft average voting is used to obtain the final prediction from 3 logit vectors of the models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "We compare our models in various settings. Particularly, the performance on the validation set of each approach in Expression recognition challenge is shown in table  1 . As the result, using a pre-trained EfficientNet followed by fully connected layers obtains a score of 0.3327, which is higher than the baseline  [16] . Adding a transformer encoder to the model can improve the F1-score. Moreover, if we exploit the synthetic data to train the transformer encoder, the results are boosted significantly to more than 0.44. Then, we try to increase the scale of the transformer, particularly, the number of heads and encoder layers. The performance is slightly enhanced but not noticeable. Afterward, we use an ensemble to combine the logit output of each model settings. Consequently, we attain better output compared to a single model. The combinations of two transformer-based models attain the F1-score from 0.4663 to 0.4729. The ensemble of 3 best configurations accomplishes a score of 0.4775. This is also our best result on the validation set of Aff-Wild2. Regarding the two remained challenges, we apply the same methodology and configuration except that the synthetic dataset is not used. Again, the metric scores are enhanced after we fuse the prediction of multiple models by soft average voting. The detailed results of AU detection and VA estimation tasks are denoted in Table  3  and Table  4 , respectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusion",
      "text": "In this report, we provide a straightforward and effective method for frame-level video classification. The transformer is employed to learn the correlation among the frames in a sequence. In the emotion classification task of the ABAW5 competition, our best achievement on the validation set of Aff-Wild2 is 0.4775, which outperforms the baseline provided by the organizer.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The block diagram of our proposed method.  The",
      "page": 1
    },
    {
      "caption": "Figure 1: 3. Dataset and Experiment",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The performance on the validation set of various",
      "data": [
        {
          "Configuration \nF1-score": "Effnet+FCs"
        },
        {
          "Configuration \nF1-score": "Effnet+ Encoder (N=4, h=4) +FCs"
        },
        {
          "Configuration \nF1-score": "(1) Effnet+ Encoder (N=4, h=4)+FCs+Syn"
        },
        {
          "Configuration \nF1-score": "(2) Effnet+Encoder (N=4, h=8)+FCs+Syn"
        },
        {
          "Configuration \nF1-score": "(3) ffnet+Encoder (N=6, h=4)+FCs+Syn"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: The performance on the validation set of various",
      "data": [
        {
          "Ensemble model \nF1-score": "Soft average voting (1) (2) \n0.4663"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1) (3) \n0.4672"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (3) (2) \n0.4729"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1), (2) and (3) \n0.4775"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: and Table",
      "data": [
        {
          "Ensemble model \nF1-score": "(1) Effnet+ Encoder (N=4, h=4)+FCs"
        },
        {
          "Ensemble model \nF1-score": "(2) Effnet+Encoder (N=4, h=8)+FCs"
        },
        {
          "Ensemble model \nF1-score": "(3) Effnet+Encoder (N=6, h=4)+FCs"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1) (2)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1) (3)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (3) (2)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1), (2) and (3)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: and Table",
      "data": [
        {
          "Ensemble model \nF1-score": "Effnet+ Encoder (N=4, h=4)+FCs (1)"
        },
        {
          "Ensemble model \nF1-score": "Effnet+Encoder (N=4, h=8)+FCs (2)"
        },
        {
          "Ensemble model \nF1-score": "Effnet+Encoder (N=6, h=4)+FCs (3)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1) (2)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1) (3)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (3) (2)"
        },
        {
          "Ensemble model \nF1-score": "Soft average voting (1), (2) and (3)"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "2",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia",
        "Sharmanska",
        "Zafeiriou Stefanos"
      ],
      "year": "2021",
      "venue": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "4",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Zafeiriou Stefanos"
      ],
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework",
      "authors": [
        "Dimitrios Kollias",
        "Zafeiriou Stefanos"
      ],
      "year": "2021",
      "venue": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "6",
      "title": "Analysing Affective Behavior in the First ABAW 2020 Competition",
      "authors": [
        "D Kollias",
        "E Schulc",
        "S Hajiyev"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "7",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "Dimitrios Kollias",
        "Zafeiriou Stefanos"
      ],
      "year": "2019",
      "venue": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "8",
      "title": "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia",
        "Sharmanska",
        "Zafeiriou Stefanos"
      ],
      "year": "2019",
      "venue": "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis",
        "Tzirakis",
        "A Mihalis",
        "Nicolaou",
        "Athanasios",
        "Papaioannou",
        "Guoying",
        "Bjorn Zhao",
        "Irene Schuller",
        "Kotsia",
        "Zafeiriou Stefanos"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Aff-wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Nicolaou",
        "Athanasios",
        "Papaioannou",
        "Guoying",
        "Zhao",
        "Kotsia Irene"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "11",
      "title": "Affective Behavior Analysis Using Action Unit Relation Graph and Multi-task Cross Attention",
      "authors": [
        "Nguyen",
        "Sudarshan Dang-Khanh",
        "Ngoc-Huynh Pant",
        "Guee-Sang Ho",
        "Soo-Hyung Lee",
        "Hyung-Jeong Kim",
        "Yang"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "arxiv": "arXiv:2203.13436"
    },
    {
      "citation_id": "14",
      "title": "Multi-Task Learning Framework for Emotion Recognition In-the-Wild",
      "authors": [
        "Tenggan Zhang",
        "Chuanhe Liu",
        "Xiaolong Liu",
        "Yuchen Liu",
        "Liyu Meng",
        "Lei Sun",
        "Wenqiang Jiang",
        "Fengyuan Zhang",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le",
        "Efficientnet"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "16",
      "title": "ABAW: Valence-Arousal Estimation",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis",
        "Alice Tzirakis",
        "Alan Baird",
        "Cowen",
        "Zafeiriou Stefanos"
      ],
      "year": "2023",
      "venue": "Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges",
      "arxiv": "arXiv:2303.01498"
    }
  ]
}