{
  "paper_id": "2503.10688v3",
  "title": "Culemo: Cultural Lenses On Emotion -Benchmarking Llms For Cross-Cultural Emotion Understanding",
  "published": "2025-03-12T01:01:30Z",
  "authors": [
    "Tadesse Destaw Belay",
    "Ahmed Haj Ahmed",
    "Alvin Grissom II",
    "Iqra Ameer",
    "Grigori Sidorov",
    "Olga Kolesnikova",
    "Seid Muhie Yimam"
  ],
  "keywords": [
    "-based emotion recognition",
    "overlooking crucial cultural dimensions required for deeper emotion understanding",
    "and (2) many are created by translating English-annotated data into other languages",
    "leading to potentially unreliable evaluation. To address these issues",
    "we introduce Cultural Lenses on Emotion (CuLEmo)",
    "Arabic",
    "English",
    "German",
    "Hindi",
    "and Spanish. CuLEmo comprises 400 crafted ques"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "NLP research has increasingly focused on subjective tasks such as emotion analysis. However, existing emotion benchmarks suffer from two major shortcomings: (1) they largely rely on keyword-based emotion recognition, overlooking crucial cultural dimensions required for deeper emotion understanding, and (2) many are created by translating English-annotated data into other languages, leading to potentially unreliable evaluation. To address these issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first benchmark designed to evaluate culture-aware emotion prediction across six languages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo comprises 400 crafted questions per language, each requiring nuanced cultural reasoning and understanding. We use this benchmark to evaluate several state-of-the-art LLMs on cultureaware emotion prediction and sentiment analysis tasks. Our findings reveal that (1) emotion conceptualizations vary significantly across languages and cultures, (2) LLMs performance likewise varies by language and cultural context, and (3) prompting in English with explicit country context often outperforms in-language prompts for culture-aware emotion and sentiment understanding. The dataset 1 and evaluation code 2 is available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Despite progress in bridging language barriers  (Ahuja et al., 2023) , large language models (LLMs) still struggle to capture cultural nuances and adapt to specific cultural contexts  (Shen et al., 2024) . Ideally, multilingual LLMs can not only facilitate cross-lingual communication but also incorporate an awareness of cultural sensitivities (i.e., what is deemed acceptable, normal, or inappropriate in a given culture), integrating such knowledge to foster deeper global connections  (Liu et al., 2024a) . LLMs and agent systems are employed to interact extensively with humans across applications such as customer service, healthcare, and education  (Wang et al., 2024) . To facilitate effective interaction, incorporating aspects of cognitive and emotional-social intelligence, the ability to recognize and interpret human emotions  (Mathur et al., 2024) , can facilitate better interactions with more people. Factors such as age, cultural background, and personal experiences influence how individuals perceive and process information, particularly within subjective NLP tasks. Among these, emotion recognition (ER) and sentiment analysis (SA) are particularly sensitive to language-and culturespecific nuances  (Plaza-del-Arco et al., 2024) .\n\nNatural language frequently encodes emotional information  (Jim et al., 2024) . For example, con-sider tipping customs in restaurants: in some cultures (e.g., North America), tipping is widely practiced, whereas in China, it is rare, and in Japan, it may even be considered offensive  (Givi and Galak, 2017) . Such differences underscore the importance of culturally-aware language technologies.\n\nAlthough prior work has attempted crosslingual emotion evaluation by translating emotionannotated data in English into other languages  (Tahir et al., 2023; De Bruyne, 2023) , relying solely on translations from English can introduce incomplete or misleading insights. A more fair comparison requires the same underlying scenarios, each annotated natively across different languages and cultures. While emotion is language-and culture-dependent  (Plaza-del-Arco et al., 2024) , comprehensive cross-cultural evaluations remain largely unexplored.\n\nTo bridge this gap, we propose Cultural Emotion (CuLEmo), a novel dataset that captures events and annotates them across multiple cultures and languages from scratch. CuLEmo enables the evaluation of multilingual LLM performance in analogous scenarios across different cultural contexts. Figure  1  shows the CuLEmo dataset creation and evaluation pipeline.\n\nCulture can manifest in 1) the language of the data itself and 2) the annotation labels (i.e., multiculturally informed annotations)  (Liu et al., 2024b) . CuLEmo satisfies both conditions: it is multilingual and includes culturally grounded annotations. Indeed, the same event may evoke distinct emotional reactions in different cultures. In light of this, we pose the following research questions (RQs):\n\n• RQ1. Do LLMs provide culturally awareemotional responses? • RQ2. Which cultures are more effectively represented in LLMs? • RQ3. Can LLMs identify a country's culture based on the text describing an event in the prompt? • RQ3. Does the language of the prompt affect the ability of LLMs for culture-aware emotion understanding?\n\nTo that end, this paper makes three key contributions. First, we introduce CuLEmo, a highquality, multicultural, and multilingual benchmark dataset. Second, we leverage CuLEmo to investigate whether widely used multilingual LLMs can capture variations in emotional expression across cultures and languages in emotion and sentiment tasks. Finally, we highlight the variation in performance on culture-aware emotion understanding when LLMs are prompted in different languages.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "We now review related work in culturally-aware NLP, culture-oriented benchmarks, and crosslingual study of linguistic emotional expression. Although culture is a complex concept, most definitions of culture encompass people, groups of people, and interactions between individuals and groups  (Liu et al., 2024c) . Understanding culture is important for the safety and fairness of LLMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Culture-Oriented Benchmarks",
      "text": "Given the significance of culture in language model evaluation  (Adilazuarda et al., 2024) , researchers have proposed various culture-oriented benchmarks to explore its effects on language understanding and generation. These efforts typically involve collecting and annotating multilingual and multicultural corpora to study culturaldriven phenomena in downstream NLP tasks. For instance, prior work has examined cross-cultural user statements  (Liu et al., 2021; Nayak et al., 2024) , detected cultural differences and user attributes  (Sweed and Shahaf, 2021; Qian et al., 2021) , studied multilingual moral understanding  (Guan et al., 2022; Mohamed et al., 2022) , and addressed culture-specific time expression grounding  (Shwartz, 2022; Fung et al., 2023) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Across Languages",
      "text": "Several studies have examined multilingual emotion datasets. Some findings suggest that emotion categories can be preserved through machine translation, for example, by exploring how Englishannotated emotion data translate into Finnish, French, German, Hindi, and Italian  (Kajava et al., 2020; Tahir et al., 2023; Bianchi et al., 2022) . These works suggest that the changes in emotion labels often stem from the inherent difficulty of annotation rather than from linguistic differences.\n\nConversely, other works emphasize that emotions may not be consistently preserved across different languages.  De Bruyne et al. (2022)  showed that typologically dissimilar languages pose challenges for cross-lingual learning with mBERTbased models. De Bruyne (2023) argued that translation could fail to capture language-specific verbalizations and connotations-especially if certain emotion keywords do not exist in a given language (e.g., there is no direct word for \"sadness\" in Tahitian, and Amharic does not have an exact term for \"surprise\").  Qian et al. (2023)  found that roughly half of the machine-translated outputs from English to Chinese fail to adequately preserve the original emotion, attributing these discrepancies to emotionspecific words and complex linguistic phenomena.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotions And Cultures",
      "text": "Recent work examines how cultural contexts shape emotional expression across languages.  Havaldar et al. (2023a)  analyze embeddings of 271 emotion keywords in English, Spanish, Chinese, and Japanese by projecting into a Valence-Arousal plane using XLM-RoBERTa  (Reimers and Gurevych, 2020)  embeddings, finding that multilingual models embed non-English emotion words differently.  Havaldar et al. (2023b)  analyzes Pride/Shame as a known cultural difference by prompting GPT-3.5 and GPT-4 to explore how these models handle pride and shame in the USA vs. Japan. They find that GPT-3.5 displays limited knowledge of culturally specific norms.  Ahmad et al. (2024)  expands the 19 cultural questions of the work  (Havaldar et al., 2023b)  to 37 questions and evaluates ChatGPT for the low-resource Hausa language for sentiment analysis, but these evaluations are limited to 19/37 culturally relevant questions (not diverse and representative data), limited classes (Pride/Shame or positive/negative/mixed), and limited cultures (USA vs. Japan or Hausa).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Culemo Dataset Preparation",
      "text": "We now describe the precise steps in creating the CuLEmo dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Collecting Cultural Events",
      "text": "We manually craft scenarios, search on the web, and prompt LLMs to gather traditions, events, norms, and actions that elicit culturally different emotions across six target countries (UAE, USA, Germany, Ethiopia, India, and Mexico). We draw inspiration from the work of  Havaldar et al. (2023b)  to enhance topic diversity. Language representatives are asked to propose events distinct from those of other countries in the form of emotion-oriented questions. Importantly, these events do not contain explicit emotional keywords (as typically seen in traditional emotion datasets). We also refer to the International Survey on Emotion Antecedents and Reactions (ISEAR)  (Scherer and Wallbott, 1994)  data format, \"When I ... situations that cause a specific emotion\", a well-known English dataset for emotion analysis consisting of self-reported events from around 3,000 respondents across 37 countries and five continents.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Human-Adapted Translation",
      "text": "After collecting the events in English, we translate them into five target languages-Arabic, Amharic, German, Hindi, and Spanish-using Google Translate, followed by native-speaker approvals. Because the questions are simple \"How do you feel when . . . ?\" questions and lack explicit emotion keywords, translation quality did not affect their cultural content. While we acknowledge that existing works often depend on translating emotionannotated data from English into other languages with their labels  (Kajava et al., 2020; Tahir et al., 2023; Bianchi et al., 2022) , our translation process is done before any annotation. To ensure correctness, native speakers evaluate the translations. While most translations were acceptable, a few adjustments were made, e.g., to fix gender references and timing expressions for Amharic.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Language And Cultures Covered",
      "text": "Several factors guide our choice of languages, ensuring a broad range of cultural norms and concepts: (1) typological variety (five languages with four scripts), (2) geographical diversity (eastern vs. western contexts), (3) resource availability (low-vs. high-resource languages), and (4) the availability of native speakers for translation reviews.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Culemo Annotation",
      "text": "We use Amazon Mechanical Turk (MTurk) for most of our annotations, ensuring at least five native-speaker annotators per instance from the respective targeted country. We use a customized POTATO  (Pei et al., 2022)  annotation tool for languages lacking sufficient MTurk annotators (e.g., Amharic) and recruit local native speakers who met our criteria. Annotators are fairly compensated $12/hr (better than the Prolific 3 's minimum annotation wage, which is $9/hr). Where no majority vote emerges among five annotators, we assign two additional annotations and use that majority vote. Figure  2  illustrates the distribution of emotion labels. We use six categories: joy, fear, sadness, anger, guilt, and neutral (no specific emotion). These labels were adapted  from De Bruyne et al. (2019) , where five categories were clustered, plus a neutral class. Our annotation guidelines group related labels as \"helper\" categories-for instance, \"love\" and \"happy\" under joy, and \"shame\" under guilt-to assist annotators in selecting the most appropriate coarse-grained label. Examples from the dataset are provided in Appendix A.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pairwise Label Agreements Across Countries",
      "text": "We further examine label differences across countries by computing pairwise agreements after majority voting (Figure  3 ). Ethiopia and the United Arab Emirates exhibit the highest agreement at 55%, along with Germany and the United Arab Emirates; both exhibit a high neutral class (Figure  2 ), while Germany and India show the lowest agreement, at 29.0%. Labels from India thus diverge from those of other countries.  4 Experimental Setup",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Task Formulation",
      "text": "To investigate the emotional comprehension capabilities of LLMs, we examine how they associate different cultures of a country with their corresponding languages. Specifically, we explore culture-aware emotion understanding via two main tasks: (1) emotion prediction and (2) sentiment analysis. All tested models are instruction-finetuned, except for the Aya-expanse model. We also experiment with prompts that do and do not include explicit country context, using the phrase \"You live in «country name»,\" (where «country name» is one of the six targeted countries: UAE, USA, Ethiopia, Germany, India, and Mexico). Each task is framed as a text-generation problem, and the models are evaluated in a zero-shot setting.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Selection",
      "text": "We evaluate a variety of recent LLMs known for strong performance on standard benchmarks. We aim to include both smaller and medium-sized models, as well as open-source and proprietary models: 1. Open Source: LLaMA-3 (3.2-3B, 3.1-8B)  (Dubey et al., 2024; Meta AI, 2024) , Gemma (2B, 9B)  (Team Gemma et al., 2024) , Aya (expanse-8b, 101-13B)  (Üstün et al., 2024; Dang et al., 2024) , Ministral (3B, 8B) (Mistral AI, 2024) 2. Proprietary: GPT (3.5, 4)  (Achiam et al., 2024) , Gemini-1.5  (Team et al., 2024) , Claude (3.5-sonnet, 3-opus) (Anthropic AI, 2024)",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multilingual Prompt Construction",
      "text": "To examine the impact of prompt language on model performance and to assess each model's cultural awareness, we design both English and inlanguage prompts. The instruction, input text, and expected answer are in English when using English prompts. For in-language prompts, all elements are in one of the five target languages-Arabic (AR), Amharic (AM), German (DE), Hindi (HI), or Spanish  (ES) . Complete examples of our multilingual prompts for both emotion prediction and sentiment analysis are provided in Appendix C. We extract each model's answer from its generated text using the PEDANTS tool  (Li et al., 2024) .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Result And Analysis",
      "text": "5.1 Culture-Aware Emotion Prediction Do LLMs provide culturally aware emotional responses? A culture-aware model should accurately answer questions related to any culture, demonstrating uniformly high accuracy. To test this, we assess each LLM's emotional understanding using English and in-language prompts with the context \"You live in «country name»,\".\n\nResults: Table  2  presents the accuracy of each LLM for culture-aware emotion prediction. The choice of prompt language significantly influences performance. Generally, proprietary models are less affected by in-language prompts compared to open-source models, especially for Spanish and German. Certain cultures appear better represented in the models-Ministral-8B scores highly on German (72%), and GPT-4 performs best on Mexican (65%). In contrast, performance in Indian culture (Hindi) lags, particularly those using Hindi, Amharic, or Arabic scripts. Larger models do not always outperform smaller ones; Gemma-2-2B and Ministral-8B show competitive or superior accuracy relative to some proprietary models. When prompted in English, all models achieve a reasonable accuracy. Ministral-8B can exceed proprietary performance in English and German. GPT-4 answers the same emotion while using the corresponding country name in the prompting; see predicted examples in Table  3 . Overall, results suggest that culture-aware emotion understanding remains challenging for the tested LLMs, especially for low-resource languages and cultures.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Culture Representation In Llms",
      "text": "Which culture is more represented in LLMs?\n\nHere, we evaluate LLM performance using English prompts without any explicit country context. We then measure how models respond to events from each target country.\n\nResults: According to Table  4 , English prompt column category, the USA, Mexico, and Germany consistently achieve higher accuracy scores, while the UAE, Ethiopia, and India remain less accurately represented. This suggests that certain cultures may be more prevalent in the underlying training data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Does Language Represent Country?",
      "text": "Can LLMs identify one country's culture based solely on the prompt language? In this experiment, we remove explicit country context (\"You live in «country name»\") and test whether LLMs can infer cultural cues only from the language used in the prompt.\n\nResults: Table  4 , in-language prompt column category, shows that accuracy drops significantly when country context is omitted. Comparing these scores with Table  2  (where country context is included), we see consistent performance boosts (e.g. +1% in Spanish with GPT-4, +5% in Arabic with Gemini1.5, +6% in German with Ministral, +9% in Amharic with Claude-3.5-sonnet, and +21% in Hindi with GPT-4) when the prompt explicitly names the country. This indicates that language alone does not reliably convey cultural context. Notably, models like Claude-3.5-sonnet and GPT-4 show improvements in Indian and Ethiopian data when country context is specified, while English prompts (without \"USA\" context) are less affected. Overall, providing the country name remains crucial for accurate culture-aware emotion understanding, especially for less-resourced languages.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Culture-Aware Sentiment Analysis",
      "text": "For the sentiment analysis experiment, we follow      (Spanish) with Claude-3-opus and GPT-4, each at 75%. By contrast, Aya-expanse-8b struggles more, as it is not instruction fine tuned. Smaller models like Gemma-2-2B are competitive with proprietary models. Still, performance drops persist for Amharic and Hindi, reflecting the challenges of culture-aware tasks in lower-resource contexts.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "Our analyses provide several insights into the current state of LLMs with respect to cultural emotion understanding. We highlight three main lessons learned and propose potential steps to enhance the cultural awareness of LLMs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Variance Across Languages And Cultures",
      "text": "Data analysis from the CuLEmo dataset in Figure  2  shows notable differences in how annotators from different countries perceive the same event differently. For instance, the events annotated from Germany have the highest proportion of neutral (no emotion) labels. Figure  5  further illustrates the distribution of positive, negative, and neutral sentiments across 400 instances in each country.\n\nHow are emotions distributed across languages and cultures? Based on the dataset analysis of emotion distribution across languages, shown in Figure  2 : German (87%), Arabic (58%), and English (50.5%) data have the most neutral (no emotion). Amharic (29.5%), Arabic (22%), and Spanish (21.5%) languages have the most anger emotion. These findings confirm that a single event can evoke distinct emotional reactions depending on the cultural background and language.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Prompt Language Strongly Affects Cultural Emotion Understanding",
      "text": "As illustrated in the Table  4  and summarized results in Figure  4 , prompt language plays a major role in LLM performance for emotion prediction and sentiment analysis. For less-resourced languages like Amharic and Hindi, prompting in English consistently yields better results-sometimes by as much as a 20% improvement. Conversely, in-language prompts with explicit country context tend to work best for high-resource languages such as German and Spanish. These discrepancies stem from differences in both linguistic coverage and instruction-following abilities learned during pre-training. One practical solution is to leverage English prompts while specifying the target country (e.g., \"You live in «country name»,\").",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Performance Gaps Reflect Under-Represented Cultures",
      "text": "We observe notably lower accuracy for Ethiopia and India in both emotion and sentiment tasks, suggesting that models may be less exposed to cultural practices and norms for these under-represented contexts. Ensuring greater diversity in training corpora is key to improving model performance for such cultures. Providing explicit country context can partially offset these gaps by nudging models to incorporate relevant cultural knowledge. Overall, our findings underscore the importance of cultural context in developing and deploying LLMs. Beyond balanced data collection, researchers may explore culture-specific tuning or reinforcement learning from human feedback to further refine the abilities of models to interpret and respect cultural nuances.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we evaluate a diverse set of state-ofthe-art LLMs for their ability to predict culturally aware emotion prediction and sentiment analysis tasks. We investigate the influence of including explicit country references \"You live in «country name»\" and varying the query language. Our results indicate that LLMs tend to excel at culturally driven emotions that are well-represented in their training data and underperform for less represented cultures. Specifically, we find that 1) emotion is culture-dependent and can vary notably across languages and regions; 2) LLMs exhibit sizable performance gaps when tested on culture-specific emotions from under-represented locales; 3) providing explicit country context in prompts improves both emotion and sentiment prediction; and 4) sentiment analysis is better for the models, likely because it involved fewer class (positive, negative, neutral) than fine-grained emotion categories.\n\nMoving forward, we suggest training LLMs in approaches such as 1) enriching the training data with diverse cultural information from various sources like literature, news, and cultural databases and 2) implementing fine-tuning techniques that specifically train the LLM on prompts and datasets focused on different cultural contexts so that they can be culturally aware and able to generate responses that are sensitive to cultural nuances. We also encourage more extensive evaluation of multilingual models using benchmarks designed to measure cultural awareness alongside standard accu-racy metrics. Future research could also explore the influence of annotator demographics-such as age, gender, education level, political stance, and religion-on culture-specific emotion annotation. Finally, we hope that releasing the CuLEmo dataset will foster further exploration into culturally nuanced NLP tasks and lead to more inclusive language models.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "Subjectivity of emotion Emotional subjectivity remains a central challenge in emotion analysis tasks. Although annotating data via crowdsourcing such as Amazon Mechanical Turk (MTurk) is common in NLP dataset creation  (Mohammad et al., 2018) , and despite applying strict qualification criteria for annotators, maintaining consistent annotations is difficult given the inherently subjective nature of emotions.\n\nLimited number of events Our test comprises only 400 questions for each language, which is certainly not sufficient to capture the full cultural differences in emotional expression.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Drawback Of Majority Vote",
      "text": "We decide the final label of the annotations using majority vote, such as an emotion label greater than or equal to three votes from a total of five annotators per instance will pass as a final emotion label. As a general drawback of the majority vote, this will exclude the perspectives of minority votes. Modeling annotator-level data without applying the majority vote can address this.\n\nLimited emotion label space Additional constraints arise from our decision to limit the emotion label space to six classes; including more emotion categories (e.g. surprise or disgust) during the annotation could yield more fine-grained insights  (Niu et al., 2024) . Our dataset also covers only six languages/countries and comprises 400 events, which may restrict generalizability.\n\nAnnotation bias Emotion annotation is subjective in nature and can vary widely depending on personal background; it likely still has consistency issues, affecting the reliability of the evaluations.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Limited Model Evaluations",
      "text": "Regarding opensource LLMs, we opted to evaluate only small-(2B,3B) and medium-sized (8B, 9B, 13B) models due to resource constraints and for experimental reproducibility. While larger LLMs might achieve higher accuracies across target languages, they remain beyond the scope of our current setup. Finally, although 400 events allow for controlled experiments, evaluating models on more extensive and varied data would provide a clearer picture of their culture-aware performance.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ethics Statement",
      "text": "We conducted this work with careful attention to ethical considerations involving data creation, annotation, and potential downstream impacts.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Data Collection And Annotation",
      "text": "Cultural Respect The CuLEmo dataset was curated with input from native speakers and cultural representatives. We designed questions to capture diverse cultural norms and emotional responses without perpetuating stereotypes.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Consent And Compensation",
      "text": "We used Amazon Mechanical Turk (MTurk) and an in-house annotation platform for data labeling. Workers were informed of the task's nature and compensated fairly at a rate of $12/hour, which exceeds minimumwage standards in the majority of the annotators' countries of residence.\n\nPrivacy and Confidentiality All scenario-based questions were artificially created or adapted from publicly available cultural information. No personally identifiable information was collected, and no real names or private details were used.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Fair Representation And Potential Biases",
      "text": "Under-Representation While we included six languages (English, Arabic, Amharic, German, Hindi, and Spanish) to broaden cultural coverage, bias in representation is inevitable in such datasets and evaluations. Clearly, many global cultures and languages remain unrepresented in our work, including minority language speakers of the languages we studied and speakers of those languages in less dominant regions. Additionally, cultures are complex and not subject to clean delineation. We therefore make no contention that this is a complete or fully representative dataset.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Subjectivity Of Emotions",
      "text": "Emotions are inherently subjective and influenced by personal and cultural backgrounds. Crowd-sourced annotations may inadvertently amplify majority cultural norms or obscure minority perspectives. We minimized these risks by providing clear guidelines, but acknowledge that subjective variation is inevitable.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Responsible Use Of The Dataset And Models",
      "text": "Cultural Sensitivity The dataset includes prompts and scenarios potentially sensitive to specific cultural contexts (e.g., religious practices, social norms). We urge researchers and practitioners to exercise cultural sensitivity and caution when using the dataset or resulting models in applications that could impact cultural or ethnic groups.\n\nDownstream Applications Models trained or evaluated on CuLEmo could be applied in contexts such as mental health or social support, potentially affecting vulnerable populations. We encourage developers to consider safety, fairness and informed consent when deploying such systems. We caution against deployment in high-stakes settings, particularly without appropriate safeguards, user testing, and especially ethical oversight.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Transparency And Future Work",
      "text": "Open Access We release the CuLEmo dataset publicly to facilitate reproducibility and encourage further research in culturally aware NLP.\n\nOngoing Improvement Future efforts should expand cultural and linguistic diversity, refine annotation protocols, and include more nuanced emotional labels. We welcome community feedback to improve both the dataset and modeling approaches.\n\nWe aim to advance culturally aware NLP through responsible data practices, fair representation, and transparent sharing, and hope this work fosters a more inclusive understanding of emotion across languages and cultures.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "D English Prompt Results",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "E Mturk Annotation Qualification Settings",
      "text": "To target suitable workers on MTurk, we set the following qualifications:\n\n1. Location must be in the target country for each language by assuming annotators that live in the specified country are native or adopted the culture. 2. Number of HITs approved must exceed 1,000 to ensure experienced workers. 3. HIT approval rate must be at least 99%, favoring high-quality, consistent annotators",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: CuLEmo dataset creation pipeline and evalua-",
      "page": 1
    },
    {
      "caption": "Figure 1: shows the CuLEmo dataset creation",
      "page": 2
    },
    {
      "caption": "Figure 2: Emotion label distribution across coun-",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the distribution of emotion la-",
      "page": 4
    },
    {
      "caption": "Figure 3: ). Ethiopia and the United Arab",
      "page": 4
    },
    {
      "caption": "Figure 3: Pairwise emotion label agreements across",
      "page": 4
    },
    {
      "caption": "Figure 4: Emotion prediction accuracy in radar chart across countries in English and in-language prompts. For",
      "page": 6
    },
    {
      "caption": "Figure 5: Sentiment (positive, negative, and neutral) distribution across countries in the CuLEmo dataset.",
      "page": 7
    },
    {
      "caption": "Figure 5: further illustrates",
      "page": 8
    },
    {
      "caption": "Figure 2: German (87%), Arabic (58%), and En-",
      "page": 8
    },
    {
      "caption": "Figure 4: , prompt language plays a major",
      "page": 8
    },
    {
      "caption": "Figure 6: English prompting results with You live in «country name» context for the across languages and LLMs.",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 2: LLMs’ accuracy for the emotion prediction task. Columns labeled EN/AR/DE/AM/HI/ES show the",
      "data": [
        {
          "LLMs": "",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "EN\nEN\nAR\nEN\nDE\nEN\nAM\nEN\nHI\nEN\nES"
        },
        {
          "LLMs": "Llama-3.2-3B\nLlama-3.1-8B",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.44\n0.20\n0.12\n0.20\n0.20\n0.18\n0.30\n0.26\n0.26\n0.28\n0.37\n0.58\n0.52\n0.47\n0.48\n0.39\n0.48\n0.14\n0.37\n0.34\n0.56\n0.57"
        },
        {
          "LLMs": "Gemma-2-2B\nGemma-2-9B",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.62\n0.59\n0.45\n0.64\n0.54\n0.48\n0.17\n0.38\n0.29\n0.58\n0.56\n0.57\n0.51\n0.56\n0.47\n0.53\n0.47\n0.29\n0.40\n0.34\n0.53\n0.60"
        },
        {
          "LLMs": "Aya-expanse-8b*\nAya-101-13B\nMinistral-8B",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.38\n0.28\n0.20\n0.30\n0.24\n0.30\n0.29\n0.32\n0.29\n0.37\n0.47\n0.60\n0.60\n0.43\n0.68\n0.43\n0.52\n0.34\n0.39\n0.34\n0.56\n0.48\n0.65\n0.61\n0.72\n0.06\n0.58\n0.49\n0.23\n0.39\n0.19\n0.57\n0.32"
        },
        {
          "LLMs": "Claude-3.5-sonnet\nClaude-3-opus\nGemini1.5-flash\nGPT-4",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.57\n0.48\n0.54\n0.46\n0.42\n0.51\n0.49\n0.40\n0.36\n0.58\n0.61\n0.54\n0.48\n0.47\n0.43\n0.32\n0.53\n0.43\n0.37\n0.36\n0.61\n0.61\n0.41\n0.41\n0.56\n0.56\n0.56\n0.46\n0.48\n0.51\n0.51\n0.62\n0.64\n0.54\n0.65\n0.60\n0.55\n0.48\n0.51\n0.50\n0.29\n0.40\n0.40\n0.64"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LLMs": "",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "EN\nEN\nAR\nEN\nDE\nEN\nAM\nEN\nHI\nEN\nES"
        },
        {
          "LLMs": "Llama-3.2-3B\nLlama-3.1-8B",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.57\n0.41\n0.15\n0.20\n0.24\n0.48\n0.51\n0.60\n0.56\n0.55\n0.60\n0.65\n0.58\n0.50\n0.49\n0.43\n0.59\n0.43\n0.57\n0.55\n0.64\n0.68"
        },
        {
          "LLMs": "Gemma-2-2B\nGemma-2-9B",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.68\n0.65\n0.58\n0.66\n0.59\n0.56\n0.46\n0.50\n0.59\n0.66\n0.66\n0.62\n0.55\n0.57\n0.63\n0.49\n0.56\n0.50\n0.57\n0.61\n0.63\n0.68"
        },
        {
          "LLMs": "Aya-expanse-8b*\nAya-101-13B\nMinistral-8B",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.45\n0.33\n0.26\n0.31\n0.27\n0.39\n0.50\n0.45\n0.58\n0.45\n0.57\n0.65\n0.64\n0.55\n0.69\n0.45\n0.55\n0.53\n0.52\n0.56\n0.61\n0.63\n0.66\n0.73\n0.70\n0.06\n0.60\n0.59\n0.46\n0.54\n0.24\n0.65\n0.53"
        },
        {
          "LLMs": "Claude-3.5-sonnet\nClaude-3-opus\nGemini1.5-flash\nGPT-4",
          "USA\nUAE\nGermany\nEthiopia\nIndia\nMexico": "0.65\n0.58\n0.61\n0.49\n0.46\n0.66\n0.62\n0.60\n0.61\n0.66\n0.72\n0.67\n0.75\n0.63\n0.60\n0.56\n0.46\n0.35\n0.58\n0.59\n0.61\n0.72\n0.63\n0.63\n0.63\n0.48\n0.50\n0.63\n0.60\n0.60\n0.53\n0.69\n0.71\n0.67\n0.62\n0.75\n0.61\n0.54\n0.48\n0.51\n0.62\n0.49\n0.58\n0.72"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "English Examples from the CuLEmo dataset": "1. How would you feel when you did not tip the waiter at the restaurant?\n2. How would you feel when someone insults someone’s religion?\n3. How would you feel if someone wear black to a wedding?\n4. How would you feel when you see a female wearing small pants on the street\n5. How would you feel when your attendee joined the meeting after 10 minutes started\n6. How would you feel when you saw a young dog running to you?\n7. How would you feel when someone accepted your gift in one hand?\n8. How would you feel when you crossed an older man in the street?\n9. How would you feel when someone calls a religious leader by their name\n10. You are a doctor. How would you feel when someone calls you by your name without the\ndoctor prefix\n11. How would you feel if your parents arranged a marriage for you without your input?\n12. How would you feel\nif someone touched or shook hands with you while you were of the\nopposite sex?\n13. How would you feel when a guest leaves food on their plate at your home?\n14. How would you feel if you saw a woman not cover her hair while entering a religious place?\n15. How would you feel when you were expected to wash other legs\n16. How would you feel if someone refused your gift before accepting it?\n17. How would you feel when you kissed your girlfriend/boyfriend and saw your father?\n18. How would you feel when your parent caught you with a pornographic film?\n19.\nIf someone is passing near to you, how would you feel when they didn’t give you a greeting?\n20. How would you feel when you see a female eating on the street?\n21. How would you feel when you see a Sheikh drinking alcohol?\n22. How would you feel when you see a priest drinking alcohol?\n23. How would you feel when you see homosexual friends?\n24. How would you feel when someone shows you his middle finger gesture?\n25. How would you feel when someone gave the money in their left hand?\n26. How would you feel if your guests chose to keep their shoes on when entering your home?\n27. How would you feel if a friend offered to pay for your meal in a restaurant?\n28. How would you feel\nif your child expressed a desire to marry someone from a different\nreligion?\n29. How would you feel if someone bargained for your prices?\n30. When you are at a meal, how would you feel about being told to wait for the host to say start\nbefore starting to eat?\n31. How would you feel when you expected to wait for the eldest person to start eating before\nyou begin?\n32. How would you feel if someone were scrolling their phone during a meal?\n33. How would you feel when you saw a young dog running toward you?\n34. How would you feel when someone cheek-to-cheek kisses your wife/husband?\n35. How would you feel if someone bargained for your prices?\n36. How do you feel when a guest arrives late to your lunch invitation?\n37. How do you feel seeing someone eating in hand without using utensils?\n38. How would you feel if someone called an elder by their first name without a title?\n39. How would you feel if a guest left your home without eating anything?\n40. How would you feel if your friend didn’t stand up when an elder entered the room?"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 6: Prompts used for probing emotions from LLMs. In English prompt, «Country name» will change",
      "data": [
        {
          "Prompts": "You live in <<Country name>>, choose the best emotion you feel for the given question. Emotions can be only \neither 'anger', 'fear', 'sadness', 'joy', 'guilt', or 'neutral'. Return only a single emotion word from the list of \nemotions without further explanation.",
          "Language": "English"
        },
        {
          "Prompts": "امإ\n \nفطاوعلا\n \nنوكت\n \n نأ\nنكمي\n .\nحورطملا\n \nلاؤسلا\n \nهاجت\n \nاهب\n \nرعشت\n \nيتلا\n \nلضفلأا\n \nةيفطاعلا\n \nةلاحلا\n \nرتخا\n \n،ةدحتملا\n \nةيبرعلا\n \nتاراملإا\n \nةلود\n \n يف\nشيعت\n \nتنأ\n \n\"\nيفاضإ\n \nريسفت\n \n يأ\nنود\n \nفطاوعلا\n \nةمئاق\n \n نم\nةدحاو\n \nةفطاع\n \nةملك\n \n طقف\nدعأ\n .\"\nدياحم\n \"\nوأ\n \"\n  \"بنذ\nوأ\n \"\nحرف\n \"\nوأ\n \"\nنزح\n  \"\nوأ\n \"\nفوخ\n  \"\nوأ\n \"\nبضغ\n.",
          "Language": "Arabic"
        },
        {
          "Prompts": "Sie leben in Deutschland. Wählen Sie die beste Emotion, die Sie für die gestellte Frage empfinden. Emotionen \nkönnen nur 'wut', 'freude', 'traurigkeit', 'schuld', 'furcht' oder 'neutral' sein. Geben Sie nur ein einziges \nEmotionswort aus der Liste der Emotionen ohne weitere Erklärung zurück.",
          "Language": "German"
        },
        {
          "Prompts": "የምትኖረው ኢትዮጵያ ውስጥ ነው፣ ለተሰጠው ጥያቄ የሚሰማህን ስሜት ምረጥ። ስሜቶች 'ቁጣ', 'ጥፋተኛ', 'ሀዘን', 'ደስታ', 'ፍርሀት' ወይም \n'መደበኛ' ብቻ ሊሆኑ ይችላሉ:: ያለተጨማሪ ማብራሪያ ከስሜቶች ዝርዝር ውስጥ አንዱን ስሜት ብቻ ይመልሱ።",
          "Language": "Amharic"
        },
        {
          "Prompts": "आप भारत में रहते हैं, दिए गए प्रश्न के\n दिए अपनी सबसे अच्छी भावना चुनें। भावनाएँ  के वि 'उिासी', 'आनंि', 'अपराध', \n'गुस्सा', 'डर', या 'सामान्य' हो सकती हैं। दबना दकसी अदतररक्त स्पष्टीकरण के\n भावनाओं की सूची से के वि एक ही भावना \nशब्द िौटाएँ ।",
          "Language": "Hindi"
        },
        {
          "Prompts": "Vives en México. Elige la emoción que sientes más a menudo en la pregunta. Las emociones pueden ser \n'enojo','tristeza','culpa','alegría','miedo' o 'neutral'. Solo responde con una palabra de la lista de emociones sin \nmás explicaciones.",
          "Language": "Spanish"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. 2024. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "venue": "Shyamal Anadkat, et al. 2024. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Towards measuring and modeling \"culture\" in LLMs: A survey",
      "authors": [
        "Muhammad Farid Adilazuarda",
        "Sagnik Mukherjee",
        "Pradhyumna Lavania",
        "Shivdutt Siddhant",
        "Alham Singh",
        "Jacki O' Fikri Aji",
        "Ashutosh Neill",
        "Monojit Modi",
        "Choudhury"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.882"
    },
    {
      "citation_id": "3",
      "title": "Are generative language models multicultural? a study on Hausa culture and emotions using ChatGPT",
      "authors": [
        "Ibrahim Ahmad",
        "Shiran Dudy",
        "Resmi Ramachandranpillai",
        "Kenneth Church"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP",
      "doi": "10.18653/v1/2024.c3nlp-1.8"
    },
    {
      "citation_id": "4",
      "title": "MEGA: Multilingual evaluation of generative AI",
      "authors": [
        "Kabir Ahuja",
        "Harshita Diddee",
        "Rishav Hada",
        "Millicent Ochieng",
        "Krithika Ramesh",
        "Prachi Jain",
        "Akshay Nambi",
        "Tanuja Ganu",
        "Sameer Segal",
        "Mohamed Ahmed",
        "Kalika Bali",
        "Sunayana Sitaram"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.258"
    },
    {
      "citation_id": "5",
      "title": "Introducing the next generation of claude",
      "authors": [
        "A Anthropic"
      ],
      "year": "2024",
      "venue": "Introducing the next generation of claude"
    },
    {
      "citation_id": "6",
      "title": "XLM-EMO: Multilingual emotion prediction in social media text",
      "authors": [
        "Federico Bianchi",
        "Debora Nozza",
        "Dirk Hovy"
      ],
      "year": "2022",
      "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis",
      "doi": "10.18653/v1/2022.wassa-1.18"
    },
    {
      "citation_id": "7",
      "title": "Aya expanse: Combining research breakthroughs for a new multilingual frontier",
      "authors": [
        "John Dang",
        "Shivalika Singh",
        "D' Daniel",
        "Arash Souza",
        "Alejandro Ahmadian",
        "Madeline Salamanca",
        "Aidan Smith",
        "Sungjin Peppin",
        "Manoj Hong",
        "Terrence Govindassamy",
        "Sandra Zhao",
        "Meor Kublik",
        "Viraat Amer",
        "Jon Aryabumi",
        "Yi-Chern Campos",
        "Tom Tan",
        "Florian Kocmi",
        "Nathan Strub",
        "Yannis Grinsztajn",
        "Acyr Flet-Berliac",
        "Hangyu Locatelli",
        "Dwarak Lin",
        "Bharat Talupuru",
        "David Venkitesh",
        "Bowen Cairuz",
        "Tim Yang",
        "Wei-Yin Chung",
        "Sylvie Ko",
        "Amir Shi",
        "Sammie Shukayev",
        "Bae"
      ],
      "year": "2024",
      "venue": "Aya expanse: Combining research breakthroughs for a new multilingual frontier",
      "arxiv": "arXiv:2412.04261"
    },
    {
      "citation_id": "8",
      "title": "The paradox of multilingual emotion detection",
      "authors": [
        "Luna De"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
      "doi": "10.18653/v1/2023.wassa-1.40"
    },
    {
      "citation_id": "9",
      "title": "Towards an empirically grounded framework for emotion analysis",
      "authors": [
        "Luna De Bruyne",
        "Orphée De Clercq",
        "Véronique Hoste"
      ],
      "year": "2019",
      "venue": "HUSO 2019: The Fifth International Conference on Human and Social Analytics"
    },
    {
      "citation_id": "10",
      "title": "How language-dependent is emotion detection? evidence from multilingual BERT",
      "authors": [
        "Luna De Bruyne",
        "Pranaydeep Singh",
        "Orphee De Clercq",
        "Els Lefever",
        "Veronique Hoste"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Workshop on Multi-lingual Representation Learning (MRL)",
      "doi": "10.18653/v1/2022.mrl-1.7"
    },
    {
      "citation_id": "11",
      "title": "The llama 3 herd of models",
      "authors": [
        "Abhimanyu Dubey",
        "Abhinav Jauhri",
        "Abhinav Pandey",
        "Abhishek Kadian",
        "Ahmad Al-Dahle",
        "Aiesha Letman",
        "Akhil Mathur",
        "Alan Schelten",
        "Amy Yang",
        "Angela Fan",
        "Anirudh Goyal",
        "Zhenyu Yang",
        "Zhiwei Zhao"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "12",
      "title": "NORM-SAGE: Multi-lingual multi-cultural norm discovery from conversations on-the-fly",
      "authors": [
        "Yi Fung",
        "Tuhin Chakrabarty",
        "Hao Guo",
        "Owen Rambow",
        "Smaranda Muresan",
        "Heng Ji"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.941"
    },
    {
      "citation_id": "13",
      "title": "Sentimental value and gift giving: Givers' fears of getting it wrong prevents them from getting it right",
      "authors": [
        "Julian Givi",
        "Jeff Galak"
      ],
      "year": "2017",
      "venue": "Journal of Consumer Psychology",
      "doi": "10.1016/j.jcps.2017.06.002"
    },
    {
      "citation_id": "14",
      "title": "A corpus for understanding and generating moral stories",
      "authors": [
        "Jian Guan",
        "Ziqi Liu",
        "Minlie Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.374"
    },
    {
      "citation_id": "15",
      "title": "Sharath Chandra Guntuku, and Lyle Ungar. 2023a. Multilingual language models are not multicultural: A case study in emotion",
      "authors": [
        "Shreya Havaldar",
        "Bhumika Singhal",
        "Sunny Rai",
        "Langchen Liu"
      ],
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
      "doi": "10.18653/v1/2023.wassa-1.19"
    },
    {
      "citation_id": "16",
      "title": "Sharath Chandra Guntuku, and Lyle Ungar. 2023b. Multilingual language models are not multicultural: A case study in emotion",
      "authors": [
        "Shreya Havaldar",
        "Bhumika Singhal",
        "Sunny Rai",
        "Langchen Liu"
      ],
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
      "doi": "10.18653/v1/2023.wassa-1.19"
    },
    {
      "citation_id": "17",
      "title": "Recent advancements and challenges of nlp-based sentiment analysis: A state-ofthe-art review",
      "authors": [
        "Jamin Rahman",
        "Md Apon",
        "Riaz Talukder",
        "Partha Malakar",
        "Md Mohsin Kabir",
        "Kamruddin Nur",
        "M Mridha"
      ],
      "year": "2024",
      "venue": "Natural Language Processing Journal",
      "doi": "10.1016/j.nlp.2024.100059"
    },
    {
      "citation_id": "18",
      "title": "Emotion preservation in translation: Evaluating datasets for annotation projection",
      "authors": [
        "Kaisla Kajava",
        "Emily Öhman",
        "Piao Hui",
        "Jörg Tiedemann"
      ],
      "year": "2020",
      "venue": "Digital Humanities in the Nordic Countries"
    },
    {
      "citation_id": "19",
      "title": "PEDANTS: Cheap but effective and interpretable answer equivalence",
      "authors": [
        "Zongxia Li",
        "Ishani Mondal",
        "Huy Nghiem",
        "Yijun Liang",
        "Jordan Boyd-Graber"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2024",
      "doi": "10.18653/v1/2024.findings-emnlp.548"
    },
    {
      "citation_id": "20",
      "title": "2024a. Are multilingual LLMs culturallydiverse reasoners? an investigation into multicultural proverbs and sayings",
      "authors": [
        "Chen Liu",
        "Fajri Koto",
        "Timothy Baldwin",
        "Iryna Gurevych"
      ],
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2024.naacl-long.112"
    },
    {
      "citation_id": "21",
      "title": "Iryna Gurevych, and Anna Korhonen. 2024b. Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art",
      "authors": [
        "Cecilia Chen",
        "Liu"
      ],
      "venue": "Iryna Gurevych, and Anna Korhonen. 2024b. Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art"
    },
    {
      "citation_id": "22",
      "title": "2024c. Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art",
      "authors": [
        "Cecilia Chen",
        "Iryna Liu",
        "Anna Gurevych",
        "Korhonen"
      ],
      "venue": "2024c. Culturally aware and adapted nlp: A taxonomy and a survey of the state of the art",
      "arxiv": "arXiv:2406.03930"
    },
    {
      "citation_id": "23",
      "title": "Visually grounded reasoning across languages and cultures",
      "authors": [
        "Fangyu Liu",
        "Emanuele Bugliarello",
        "Maria Edoardo",
        "Siva Ponti",
        "Nigel Reddy",
        "Desmond Collier",
        "Elliott"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.818"
    },
    {
      "citation_id": "24",
      "title": "Advancing social intelligence in ai agents: Technical challenges and open questions",
      "authors": [
        "Leena Mathur",
        "Paul Liang",
        "Louis-Philippe Morency"
      ],
      "year": "2024",
      "venue": "Advancing social intelligence in ai agents: Technical challenges and open questions",
      "arxiv": "arXiv:2404.11023"
    },
    {
      "citation_id": "25",
      "title": "Llama 3.2: Revolutionizing edge ai and vision with open, customizable models",
      "authors": [
        "A Meta"
      ],
      "year": "2024",
      "venue": "Llama 3.2: Revolutionizing edge ai and vision with open, customizable models"
    },
    {
      "citation_id": "26",
      "title": "Introducing the world's best edge models",
      "authors": [
        "A Mistral"
      ],
      "year": "2024",
      "venue": "Introducing the world's best edge models"
    },
    {
      "citation_id": "27",
      "title": "ArtELingo: A million emotion annotations of WikiArt with emphasis on diversity over language and culture",
      "authors": [
        "Youssef Mohamed",
        "Mohamed Abdelfattah",
        "Shyma Alhuwaider",
        "Feifan Li",
        "Xiangliang Zhang",
        "Kenneth Church",
        "Mohamed Elhoseiny"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2022.emnlp-main.600"
    },
    {
      "citation_id": "28",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez",
        "Mohammad Salameh",
        "Svetlana Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1001"
    },
    {
      "citation_id": "29",
      "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
      "authors": [
        "Aida Mostafazadeh Davani",
        "Mark Díaz",
        "Vinodku"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00449"
    },
    {
      "citation_id": "30",
      "title": "Benchmarking vision language models for cultural understanding",
      "authors": [
        "Shravan Nayak",
        "Kanishk Jain",
        "Rabiul Awal",
        "Siva Reddy",
        "Sjoerd Van Steenkiste",
        "Lisa Hendricks",
        "Karolina Stanczak",
        "Aishwarya Agrawal"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.329"
    },
    {
      "citation_id": "31",
      "title": "Rethinking emotion annotations in the era of large language models",
      "authors": [
        "Minxue Niu",
        "Yara El-Tawil",
        "Amrit Romana",
        "Emily Provost"
      ],
      "year": "2024",
      "venue": "Rethinking emotion annotations in the era of large language models",
      "arxiv": "arXiv:2412.07906"
    },
    {
      "citation_id": "32",
      "title": "POTATO: The portable text annotation tool",
      "authors": [
        "Jiaxin Pei",
        "Aparna Ananthasubramaniam",
        "Xingyao Wang",
        "Naitian Zhou",
        "Apostolos Dedeloudis",
        "Jackson Sargent",
        "David Jurgens"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2022.emnlp-demos.33"
    },
    {
      "citation_id": "33",
      "title": "Emotion analysis in NLP: Trends, gaps and roadmap for future directions",
      "authors": [
        "Flor Miriam",
        "Plaza- Del-Arco",
        "Alba Curry",
        "Amanda Curry",
        "Dirk Hovy"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "34",
      "title": "Pchatbot: A largescale dataset for personalized chatbot",
      "authors": [
        "Hongjin Qian",
        "Xiaohe Li",
        "Hanxun Zhong",
        "Yu Guo",
        "Yueyuan Ma",
        "Yutao Zhu",
        "Zhanliang Liu",
        "Zhicheng Dou",
        "Ji-Rong Wen"
      ],
      "year": "2021",
      "venue": "Pchatbot: A largescale dataset for personalized chatbot",
      "arxiv": "arXiv:2009.13284"
    },
    {
      "citation_id": "35",
      "title": "Evaluation of Chinese-English machine translation of emotionloaded microblog texts: A human annotated dataset for the quality assessment of emotion translation",
      "authors": [
        "Shenbin Qian",
        "Constantin Orasan",
        "Felix Carmo",
        "Qiuliang Li",
        "Diptesh Kanojia"
      ],
      "year": "2023",
      "venue": "Proceedings of the 24th Annual Conference of the European Association for Machine Translation"
    },
    {
      "citation_id": "36",
      "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.18653/v1/2020.emnlp-main.365"
    },
    {
      "citation_id": "37",
      "title": "Evidence for universality and cultural variation of differential emotion response patterning",
      "authors": [
        "R Klaus",
        "Harald Scherer",
        "Wallbott"
      ],
      "year": "1994",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "38",
      "title": "Understanding the capabilities and limitations of large language models for cultural commonsense",
      "authors": [
        "Siqi Shen",
        "Lajanugen Logeswaran",
        "Moontae Lee",
        "Honglak Lee",
        "Soujanya Poria",
        "Rada Mihalcea"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2024.naacl-long.316"
    },
    {
      "citation_id": "39",
      "title": "Good night at 4 pm?! time expressions in different cultures",
      "authors": [
        "Vered Shwartz"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022",
      "doi": "10.18653/v1/2022.findings-acl.224"
    },
    {
      "citation_id": "40",
      "title": "Catchphrase: Automatic detection of cultural references",
      "authors": [
        "Nir Sweed",
        "Dafna Shahaf"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-short.1"
    },
    {
      "citation_id": "41",
      "title": "On the effect of emotion identification from limited translated text samples using computational intelligence",
      "authors": [
        "Madiha Tahir",
        "Zahid Halim",
        "Muhmmad Waqas",
        "Shanshan Tu"
      ],
      "year": "2023",
      "venue": "International Journal of Computational Intelligence Systems",
      "doi": "10.1007/s44196-023-00234-5"
    },
    {
      "citation_id": "42",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "Gemini Team",
        "Petko Georgiev",
        "Ian Ving",
        "Ryan Lei",
        "Libin Burnell",
        "Anmol Bai",
        "Garrett Gulati",
        "Damien Tanzer",
        "Zhufeng Vincent",
        "Shibo Pan",
        "Wang"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "43",
      "title": "Gemma: Open models based on gemini research and technology",
      "authors": [
        "Team Gemma",
        "Thomas Mesnard",
        "Cassidy Hardin",
        "Robert Dadashi",
        "Surya Bhupatiraju",
        "Shreya Pathak",
        "Laurent Sifre",
        "Morgane Rivière",
        "Mihir Sanjay Kale",
        "Juliette Love",
        "Others"
      ],
      "year": "2024",
      "venue": "Gemma: Open models based on gemini research and technology",
      "arxiv": "arXiv:2403.08295"
    },
    {
      "citation_id": "44",
      "title": "",
      "authors": [
        "Lei Wang",
        "Chen Ma",
        "Xueyang Feng",
        "Zeyu Zhang",
        "Hao Yang",
        "Jingsen Zhang",
        "Zhiyuan Chen",
        "Jiakai Tang",
        "Xu Chen",
        "Yankai Lin",
        "Wayne Zhao",
        "Zhewei Wei"
      ],
      "venue": ""
    }
  ]
}