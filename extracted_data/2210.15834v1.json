{
  "paper_id": "2210.15834v1",
  "title": "Gm-Tcnet: Gated Multi-Scale Temporal Convolutional Network Using Emotion Causality For Speech Emotion Recognition",
  "published": "2022-10-28T02:00:40Z",
  "authors": [
    "Jia-Xin Ye",
    "Xin-Cheng Wen",
    "Xuan-Ze Wang",
    "Yong Xu",
    "Yan Luo",
    "Chang-Li Wu",
    "Li-Yan Chen",
    "Kun-Hong Liu"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Temporal Convolution Network",
    "Emotion Causality",
    "Multi-Scale",
    "Gating Mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In human-computer interaction, Speech Emotion Recognition (SER) plays an essential role in understanding the user's intent and improving the interactive experience. While similar sentimental speeches own diverse speaker characteristics but share common antecedents and consequences, an essential challenge for SER is how to produce robust and discriminative representations through causality between speech emotions. In this paper, we propose a Gated Multi-scale Temporal Convolutional Network (GM-TCNet) to construct a novel emotional causality representation learning component with a multi-scale receptive field. GM-TCNet deploys a novel emotional causality representation learning component to capture the dynamics of emotion across the time domain, constructed with dilated causal convolution layer and gating mechanism. Besides, it utilizes skip connection fusing high-level features from different gated convolution blocks to capture abundant and subtle emotion changes in human speech. GM-TCNet first uses a single type of feature, mel-frequency cepstral coefficients, as inputs and then passes them through the gated temporal convolutional module to generate the high-level features. Finally, the features are fed to the emotion classifier to accomplish the SER task. The experimental results show that our model maintains the highest performance in most cases compared to state-of-the-art techniques. The source code is available at: https://github.com/Jiaxin-Ye/GM-TCNet.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human-computer interaction (HCI) involves the study of the design and usage of computer technologies. It focuses not only on creating a natural and effective environment for interaction between humans and computers but also on providing a friendly interactive experience for users. As human speech signals are abundant of information, they convey the intent of messages through factors such as the speaker's identity, emotion, and intonation  [1] .\n\nFor HCI applications, the identification of these factors behind the speech signals, especially the emotion, can enhance the understanding of the user's intent and improve the experience during the interaction. Therefore, the speech emotion recognition (SER) task that empowers machines to perceive emotion in human speech is becoming increasingly prevalent in the HCI field  [2] . SER develops rapidly and has been applied to many fields recently.\n\nFor example, it had been widely used in the HCI  [3, 4] , and Chen et al.  [5]  had embedded the SER technique in robots, so that robots could track a variety of emotions instantly. Moreover, it was also used to determine the suicidal tendency of patients  [6]  and to prevent drivers from traffic accidents  [7] . The typical SER system mainly includes two parts: feature extraction and emotion classification. For speech emotion feature extraction, it is mainly divided into qualitative features, spectral features, continuous features, and manually extracted features  [8, 9] . Nowadays, spectral features have been extensively used, such as the Mel-Frequency Cepstral Coefficients (MFCC)  [10] , Linear Predictor Coefficients (LPC)  [11] . Besides, some studies  [12]  tried to combine MFCC, Linear Predictive Cepstral Coefficient (LPCC)  [13]  and other spectral features to tackle the SER task. At the same time, more and more manually extracted features are also introduced. For example, Tuncer et al.  [14]  selected the features by the shuffle box for feature generation and Mustaqeem et al.  [15]  used Radial Based Function Network (RBFN) similarity measurement to select a key sequence segment.\n\nThese methods achieved high performance on multiple datasets by generating informative features in diversified ways.\n\nRecently, researchers have proposed many different Deep Learning (DL) methods for the SER task, which can be divided into three categories: Generative Deep Learning (GDL), Discriminative Deep Learning  (DDL)  and Hybrid Deep Learning (HDL). Some typical models of the GDLs include the Generative Adversarial Nets (GAN), Deep Restricted Boltzmann Machine (DRBM) and Deep Auto-Encoders (DAE). Fang et al.  [16]  proposed a CycleGAN-based method to transfer features extracted from a large unlabeled speech corpus to synthetic features to represent the given target emotions. Zhang et al.  [17]  used the DRBM to learn relations between highdimensional features and Fei et al.  [18]  utilized DAE to extract the features from raw speech. However, GDLs suffer from the vanishing gradient problem due to the sigmoid cross-entropy loss function used for training  [19] .\n\nDDL has been more extensively used in SER compared to GDL, such as Convolutional Neural Network (CNN)  [20, 21, 22]  and Recurrent Neural Network (RNN)  [23] . These techniques combined different layers in networks to provide high discriminative ability and eliminate the dependence on expert-driven handcrafted features. For example, to obtain a simpler and more general classification model, Issa et al.  [24]  adopted the 1-D CNN and stacked different speech features as input. Kwon et al.  [25]  proposed a model based on a 1-D dilated CNN (DCNN) with a multi-learning strategy to learn spatial and temporal features parallelly. Zhang et al.  [26]  designed a model based on multiple deep CNNs, which comprised 1-D, 2-D, and 3-D CNN to integrate different utterance-level results. Owing to the strong extensibility of CNN, researchers began to inject other networks into the CNN structure  [27] . However, CNN cannot effectively model temporal dependencies in the series data  [28] . Therefore, other DL frameworks with significant capability in handling sequential data, such as RNN, had been adopted in SER to preserve the temporality of speech signals  [29] . For instance, Xie et al.  [30]  proposed the Long Short-Term Memory (LSTM) based on attention and gating mechanisms to control information flow by point-wise multiplication to capture dependencies from sequences and regulate the information at each frame. Su et al.  [31]  proposed a Graph Attention mechanism on the Gated Recurrent Unit network (GA-GRU) to handle the SER task, and Lin et al.  [32]  combined gated network and LSTM networks in a flexible way to preserve the temporal information of the sentence.   [36] , a modification based on CNN, was likewise used to maintain the temporal information of speech signals.\n\nIt offers the capability of large-scale parallel processing with low training costs because it does not process the sequence data sequentially like RNN  [36] , so as to avoid high training costs  [37] .\n\nNevertheless, there are still some limitations in the proposed methods, including:\n\n1) The emotion causality in speech is not sufficiently explored. The causality is a prerequisite for the perception of human  [38] , which has a temporal priority in the time domain (cause precedes effect)  [39] .\n\nBesides, the speech signal always carries rich contextual sentimental information and the emotion with the same valence shares common antecedents and consequences  [40] . Therefore, the emotion causality is significant for addressing the SER task. Recently, various approaches based on emotion causality have been introduced to analyze emotions in diverse media. For instance, Mittal et al.  [41]  analyzed the affective of movies with the multimodal method based on the ideas from emotion causation theories, which introduces Granger causality to model the temporal causality. These works illustrated the importance of causality in emotion analysis. However, to the best of our knowledge, most of works in the SER ignored the importance of emotion causality.\n\n2) The problem of long-term dependency persists. The existing SER methods did not well utilize highlevel features. The low-level convolution layers are not as capable of building long-term dependencies as the high-level layers, which have a larger receptive field to maintain the long-term dependencies. Without the sufficient high-level features, most of the approaches failed to effectively build reliable long-term dependencies in the existing SER methods.\n\n3) The existing single-scale architectures are inadequate for modelling speech emotion. The human speech is not a type of single-scale signals. Instead, such signals contain ample information in nature, and should be treated as the multi-scale data. For instance, prosody has a multi-scale expression across the time domain, which results in abundant and subtle emotion changes in human speech  [42] . However, most of the existing approaches had been directed at modelling speech emotion on a global scale, suffering from the absence of local-scale modelling.\n\nTo address these challenges, we propose a new approach based on TCNN, called Gated Multi-scale Temporal Convolutional Network (GM-TCNet). It aims to construct a novel emotional causality representation learning component with multi-scale receptive fields.\n\nTo the best of our knowledge, this is the first try to mine temporal causality among speech emotions in SER.\n\nThe causality of different neurons is well maintained by introducing the causal convolution to the structure of GM-TCNet. Consequently, GM-TCNet can simulate the human perception of speech emotions by the causal convolution, and infer the emotions at the frame level. Specifically, we use the term \"emotion causality\" or \"temporal causality\" to refer to the constraint that speech should be processed in a forward manner. It is different from causal learning in  [43] . Furthermore, by employing dilated convolution and gating mechanism, the ability of the dilated convolution is strengthened in building long-term sentimental dependency across the time domain. The gating mechanism can enhance the ability of low-level convolution layers to build a reliable long-term dependency.\n\nGM-TCNet utilizes skip connection to fuse high-level features with diverse receptive fields to capture abundant and subtle emotion changes in human speech. The proposed approach is evaluated on four commonly used datasets compared with the state-of-the-art (SOTA) approaches. The experimental results demonstrate the superior performance of the GM-TCNet, achieving great improvements on weighted average recall (WAR) and unweighted average recall (UAR) scores on four widely used datasets. The main contributions of this paper are summarized as follows.\n\n1) A novel emotional causality representation learning component. Constructed with dilated causal convolution layer and gating mechanism, the proposed GM-TCNet can capture the changes of emotion across time domain and better model the speech emotions at frame level. It also has a strong ability to build a reliable long-term sentimental dependency. To the best of our knowledge, this is the first attempt at applying the causality learning method to SER.\n\n2) High-level features with multi-scale receptive fields. GM-TCNet extracts high-level features from different Gated Convolution Blocks (GCB) with multi-scale receptive fields. It uses skip connection combining features to capture abundant and subtle emotion changes in human speech.\n\n3) Superior-performance in speech emotion recognition. The experimental results demonstrated that the proposed GM-TCNet can effectively produce the features with emotional causality from speech and significantly outperform the SOTA approaches.\n\nThe remainder of this paper is organized as follows. Section 2 gives a brief overview of TCNN, and the details of our approach are presented in Section 3. Section 4 reports experimental results along with discussions. Finally, Section 5 concludes this paper and points out some future research directions.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Overview Of Temporal Convolutional Neural Networks",
      "text": "The original architecture of TCNN  [36]  is a sequential model to process data across the time domain. Formally speaking, the sequence model network is a function F that produces a mapping: X T +1 → Y T +1 . F receives an input sequence x 0 , x 1 , . . . , x T and produces a corresponding output sequence y 0 , y 1 , . . . , y T . As Eq.(  1 ) shows, the target of training in F is to evaluate ŷ0 , ŷ1 , . . . , ŷT by minimizing some loss functions between the corresponding output sequence y 0 , y 1 , . . . , y T and the estimated sequence ŷ0 , ŷ1 , . . . , ŷT . In SER, the input sequences are the low-level feature sequences, and the output sequences are the high-level feature sequences extracted from the time domain. ŷ0 , ŷ1 , . . . , ŷT = F(x 0 , x 1 , . . . , x T )\n\nTCNN deploys the dilated causal convolution layers to learn long-term dependencies and achieve causal constraints. The dilated convolution increases the receptive field exponentially. Formally, for a 1-D sequence input\n\nx ∈ R n and a filter f : {0, . . . , k -1} → R, the dilated convolution operation O on element s of the sequence x is defined by Eq.(2). d is the dilated rate, * d represents the calculation symbol for dilated convolution, k is the filter size, and (sd • i) accounts for the direction of the past. In Figure  1 , TCNN increases dilated rate d exponentially with the depth of the network (i.e., d = 2 j at level j of the network). Moreover, the causal constraint indicates the prediction of ŷt is only related to x 0 , x 1 , . . . , x t and unrelated to the future inputs x t+1 , x t+2 , . . . , x T , which ensures that future information is not leaked to the past.\n\nAnother superiority of the TCNN is the usage of residual blocks  [44] , as shown in Figure  2 . The residual block is composed of two branches. The right one has an optional 1 × 1 convolution to ensure that the input and output have the same shape, and the left one is composed of two sets of identical blocks. Moreover, each block includes a dilated causal convolution layer, a weight regularization layer, a Rectified Linear Unit (ReLU) layer, and a spatial dropout layer.\n\nDue to the excellent sequence modeling ability of TCNN, it has abundant applications in speech recognition. Unlike the existing methods, to the best of our knowledge, our GM-TCNet is the first TCNN-based work for the SER task. The dilated causal convolutional layers are deployed to mine the causal relationship of different emotions, which are more oriented towards addressing long-term sentimental dependencies. In addition, the multiscale architecture can capture abundant and subtle emotion changes in human speech and the novel gated residual block is deployed to change the dilated rate distribution. The details are given in the next section.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "The Proposed Approach",
      "text": "This paper proposed a new TCNN-based approach called Gated Multi-scale Temporal Convolutional Neural Network (GM-TCNet). Compared to TCNN, GM-TCNet is designed by merging the gating mechanism and skip connection to control information flow and capture multi-scale temporal features. It first accepts the 39-D MFCC features as inputs to further extract high-level features through the Gated Temporal Convolutional Module (GTCM). And then the corresponding outputs are fed to the Global Average 1-D Pooling (GAP) and Fully Connected (FC) layer to produce the final decision. the related modulus and their square are calculated based on the speech signal spectrum to generate the power spectrum, which passes through a set of Mel-scale triangular filter banks. Finally, the logarithmic energy output is processed by the discrete cosine transformation to obtain the MFCC features. These coefficients are spliced together and transformed to a set of 39-D features, which serve as the inputs to GM-TCNet.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Feature Extraction",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Gm-Tcnet",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Gated Temporal Convolutional Module",
      "text": "Gated Temporal Convolutional Module(GTCM) is the core of GM-TCNet. It is built by stacking a 1-D causal convolutional layer and seven Gated Convolutional Blocks (GCB) with the exponentially increasing dilated rates.\n\nThere are two levels in a GCB, each of which has three branches with the same architecture called Gated Sub Convolution Block (GSCB). As shown in Figure  4 , for the i th GCB, the dilated rate d i.1 in the first level is 2 i-1 and the dilated rate d i.2 of the three sub-blocks ensembled in the second level is 2 i . For example, the dilated rates of the first level and the second level GSCBs in the first GCB are 1 and 2 respectively, and those of the first level and the second level GSCBs in the seventh GCB are 64 and 128 respectively. GTCM accepts a 2-D feature with the size of T ×39 as inputs, where T represents the number of frames and 39 represents the dimension of the MFCC features. The first layer of GTCM performs 1-D causal convolution with the kernel size of 1 and filter number of 39. Then the output of the first layer is fed to seven GCBs sequentially.\n\nThe skip connection sums the outputs F i (x) from seven GCBs (i = 1, 2, • • • , 7) and the summed output is fed to the LeakyReLU layer (alpha=0.05). In particular, the first six GCBs use residual connections to effectively learn modifications to the identity mapping  [44] . The output of GTCM is fed to the emotion classifier at last.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Gated Convolution Block",
      "text": "Based on the residual block in the TCNN structure, each GCB in GTCM introduces the gating mechanism and average pooling strategy. The gating mechanism provides more effective control on information transmission and the capability of learning long-term sentimental dependencies  [48] . The average pooling strategy aids in improving each single model's performance. Specifically, as Eq.(  3 ) shows, the input G i (x) of the i th GCB is equal to H i-1 (x) when i > 1. When i = 1, the input G 1 (x) of the first GCB comes from the output of the first 1-D convolutional layer, represented by x. Moreover, as Eq.(  8 ) shows, H i (x) is the sum of the G i (x) and the output F i (x) of the i th GCB.\n\nFigure  4  shows that GCB is divided into two levels. The first level is called \"Input Gate\", used to identify the importance of the current inputs to the second level. It also helps to determine how much the input of the current GCB is preserved to the unit state C i . The second level controls how much C i is captured in the output F i (x) of the i th GCB, named \"Output Gate\".\n\nEach level has three GSCBs, which share the same structure. As shown in Figure  4 , the input feature is fed to two different branches respectively, which first performs a 1-D dilated causal convolution with a dilated rate d and wise by the production of two branches, it can provide the overall importance of the current input. Specifically, as Eq.(  4 ) and Eq.(  6 ) show, W 2 denotes the weight of the dilated causal convolution and the kernel size is 2, * d is the dilated causal convolution operation with the dilated rate d, σ(•) is the sigmoid function, ReLU(•) is the ReLU activation function, I j i (x) and O j i (x) denote j th GSCB of input gate and output gate in the i th GCB. Furthermore, each GCB employs the average pooling strategy to improve the model's performance. In the i th GCB, the dilated rate is 2 i-1 for the input gate, and their outputs I 1 i (x), I 2 i (x), I 3 i (x) are averaged as C i (x) by Eq.(  5 ). C i (x) is sent to the output gate, whose dilated rate d is 2 i . After that, the same steps are processed by Eq.(  6 ) to get the output results\n\n) and then the output F i (x) is averaged by Eq.(  7 ). To effectively learn modifications to the identity mapping rather than the entire transformation, the residual connection is introduced in Eq.(  8 ). H i (x) is the sum of the input G i (x) and the output F i (x) of the i th GCB.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Emotion Classifier",
      "text": "The process of the emotion classifier is shown in Figure  4 , which includes the GAP layer and FC layer. After fusing the output of GTCM, the network uses the GAP layer to reduce the number of learnable parameters and drives CNN to fit the inputs at modified size  [49] . As depicted in Figure  5 , GAP generates a feature map for each related object with the high-level features generated by seven GCBs. Next, it takes the average of each feature map in the time domain to avoid overfitting. Finally, the generated vector is sent to the FC layer.\n\nThe FC layer uses the Softmax function as the activation function, which handles the high-level features extracted by GM-TCNet for the classification task. It maps the outputs of multiple neurons to [0,1], and normalizes them in Eq.(  9 ). Among them, e i represents the exponential function of the i th element of the high-level features input by GM-TCNet, and s(z) i denotes the softmax value. Academy of Sciences (CASIA)  [50] , Berlin Emotional dataset (EMODB)  [51] , Ryerson Audio-Visual dataset of Emotional Speech and Song (RAVDESS)  [52] , and Surrey Audio-Visual Expressed Emotion dataset (SAVEE)  [53] . The language used in both RAVDESS and SAVEE is English, while the speeches in EMODB and CASIA datasets are in German and Chinese respectively. The details of these datasets are given in Table  1    Features. In the experiments, the 39-D MFCC features are extracted by the Librosa toolbox  [54]  with the default settings. That is, the frame length is 0.05 s, the frame shift is 0.0125 s, the sample rate is 22050 Hz and the window function added for the speech data is Hamming window.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Implementation And Training.",
      "text": "The proposed approach is implemented in the TensorFlow framework  [55] . The batch size is set to 64 and the training process is optimized by Adam algorithm  [56]  with an initial learning rate α = 1.0 × 10 -3 , exponential decay rates β 1 = 0.93, β 2 = 0.98, and weight decay = 1.0 × 10 -8 . Moreover, the cross-entropy loss is employed as the loss function. For better comparison with the SOTA approaches, the holdout validation (80% data for training and 20% for testing), 5-fold cross-validation (CV), and 10-fold CV schemes are all used. To compare with other methods fairly, we used all data in all comparison experiments with random divisions. In each type of partitioning, we performed multiple partitions and verified the experimental results.\n\nEvaluation Metrics. The Weighted Average Recall (WAR) is the weighted average recall with weights equal to the class probabilities, and Unweighted Average Recall (UAR) is the average recall of different sentiment classes.\n\nThey are employed for performance comparison, as defined by:\n\nHere, K, M and N represent the number of sentiment classes, the number of speech signals of class α and the number of all speech signals respectively. T P β α , T N β α , FP β α and FN β α represent the true positive, true negative, false positive, and false negative values of class α for speech signal β respectively.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Experimental Results",
      "text": "In the experiments, four speech emotion datasets CASIA, EMODB, RAVDESS, and SAVEE are used to verify the effectiveness of the GM-TCNet. Table  3  shows the results of our proposed model. Moreover, we list the highest accuracy obtained by different studies in recent publications in Tables  4 5 6 7 . We can see from these tables that GM-TCNet performs better than other approaches in most cases and obtains the highest accuracy on all four datasets. These results confirm the high performance of GM-TCNet across different datasets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Casia",
      "text": "GM-TCNet obtains an 89.50% WAR score on the CASIA dataset, which is 1.60% higher than the highest accuracy ever reported in 5-fold CV. Hong et al.  [57]  only used the MFCC feature for SER and achieved 83.65% WAR score. With the same type of MFCC feature, our model gains a 8.85% higher WAR score on the CASIA dataset. Overall, GM-TCNet improves the performance to 92.50%, 89.50%, 90.17% on the hold-out, 5-fold CV and 10-fold CV, respectively. Furthermore, as shown in Figure  6a , GM-TCNet achieves 93.00% and 91.00% accuracy on the Neutral and Angry emotions, respectively, getting the best scores ever reported.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emodb",
      "text": "On the EMODB dataset, Table  5  shows that GM-TCNet achieves 89.35% and 91.40% WAR scores in the 5-fold and 10-fold CV. As Figure  6b  shows that our model achieved 96.06% and 95.16 % accuracy on the Angry and Neutral emotions, higher than the results of other emotions. Ozer  [61]  used CNN based method to obtain 91.32% accuracy index on the 10-fold CV, which was the highest accuracy result reported on the EMODB dataset.\n\nNevertheless, our approach is slightly higher than theirs in terms of performance. At the same time, our accuracy and UAR scores are 1.31% and 0.98% higher than the methods proposed by Tuncer et al.  [14] . It shows that our approach can achieve the best overall performance and better balanced results. Compared with the approach of generic recurrent architectures (BiLSTM, etc.), our approach can trace long-term dependencies to some extent. For instance, our approach obtains a 3.90% improvement on UAR scores compared with BiLSTM approach  [15] . It indicates that our method can build a long-term dependency across the time domain, which maintains the temporal information of the speech as well.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ravdess",
      "text": "Table  6  shows that GM-TCNet achieves the highest WAR and UAR scores on the RAVDESS dataset. In detail, GM-TCNet obtains the highest WAR scores in all three data split schemes. As shown in Figure  6c , the accuracies of the Angry and Calm emotions are higher than 90%. At the same time, the WAR and UAR scores of our proposed method are close to those of the method proposed by Tuncer et al.  [14] . Compared with the method using the MFCC feature  [24] , our method promotes the results by 15.47% on the WAR scores. It shows that our method is able to make full use of the temporal information in the MFCC feature, which positively improves its performance of the SER.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Savee",
      "text": "Table  7  shows that the proposed method achieves the highest accuracy score of 90.63%, 84.79% and 86.01% on the hold-out, 5-fold CV and 10-fold CV respectively on the SAVEE dataset. Although the method proposed by Tuncer et al.  [14]  achieved the best-reported accuracy score, 84.79%, on the 10-fold CV, it is 1.22% lower than that of GM-TCNet. On the other hand, both Mekruksavanich et al.  [70]  and our work use the MFCC feature and 1-D convolutional layers, but GM-TCNet beats the former by achieving 20.18% higher accuracy.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Experiments",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "The Gating Level",
      "text": "This ablation experiment is performed at each GCB gating level, aiming to explore the influence of different gating levels on the performance of GM-TCNet. As shown in Figure  7 , the WAR scores are the highest when the gating level is 2 and the other parameters are kept unchanged. When the gating level is larger than 2, the WAR scores decrease. The results show that the two-level gating strategy leads to the best performances on the four datasets. Therefore, the two-level gating can capture key features in the speech signals better than the one-level gating. The reason lies in that the second level gating adds an output gate to control the units' states, providing the network structure with more robust capabilities to filter out irrelevant features. When the gating level is higher than 2, the sigmoid function will attenuate the signals to the degree that the intensity of the necessary information is faded, which results in losing part of the critical feature information in the neural network and hence deteriorating the performance.\n\nMeanwhile, the higher gating level requires more training data to optimize the corresponding parameters. Due to the small sample size in the training data, there should not be too many gating levels in the neural network.\n\nOtherwise, the under-fitting problem would occur.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "The Number Of Gscb",
      "text": "This experiment aims to discover the impact of the number of GSCBs in each gating level on the performance of GM-TCNet. The results in Figure  8  show that the best GSCB number in each layer is 3, which promises the highest performance on the four datasets. It can effectively enhance the stability and performance of GM-TCNet. The performance deteriorates when the number is too small because of its high variance and low stability.\n\nWhen the number of GSCBs is larger than 3, the variances of models on different datasets increase, and the WAR scores decrease. The larger number of GSCBs would require more parameters to learn, making the model hard to converge. In summary, three GSCBs are the optimal structure for our model, resulting in lower computational cost and better fitting for the real-time application requirement.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "The Multi-Scale Temporal Receptive Field",
      "text": "To figure out the contribution of multi-scale receptive fields, this experiment compares the impact of the maxscale and multi-scale receptive field methods on the performance of GM-TCNet. The max-scale receptive field method only utilizes the output F 7 (x) of the last GCB as the input to the LeakyReLU layer, indicating that the receptive field of GTCM is the same as the seventh GCB. The multi-scale receptive field method utilizes the skip  connection to sum the outputs F i (x) from seven GCBs as the input to the LeakyReLU layer. The multi-scale method makes GTCM to obtain multi-scale receptive fields from different GCBs.\n\nThe results in Figure  9  show that the multi-scale method can gain +8.64% and +9.84% relative improvement on WAR and UAR compared to the max-scale method. Since the receptive field of a single scale cannot adapt to the changes of different emotions on a time scale, GM-TCNet uses skip connection to combine features from different receptive fields to capture richer multi-scale details. It can effectively enhance the capability of dynamic perception emotion of GM-TCNet.",
      "page_start": 16,
      "page_end": 19
    },
    {
      "section_name": "The Distribution Of Dilated Rates",
      "text": "The dilated causal convolutional layer is one of the widely-used basic structures in GSCB. Notably, the dilated rate setting can strongly affect the size of the receptive field and the extraction of high-level features. Therefore, we set diverse values according to the corresponding levels of GCBs. This subsection conducts experiments to explore the influence of different Dilated Rate Distribution (DRD) on performance. Since the feature lengths of the four datasets range within  [128, 256]  after feature extraction, we set the receptive field sizes in the range of  [128, 256]  in this experiment. In Figure  10 , \"Raw-128/256\" means to use TCNN's original DRD on GM-TCNet, that   Table  8  shows that our DRD leads to the best results on four datasets compared with the original DRD. When the number of layers is the same, our DRD obtains a larger receptive field than the original DRD. It is beneficial to capture the global features in the time domain. The experiment results show that the best WAR scores of the proposed method is +2.92% higher than the original DRD on the four datasets on average, indicating that the adjusted receptive field offers sentimental information on more temporal scales and higher capability in utilizing the temporal information.\n\nWhen the size of the receptive field is unchanged, the original DRD requires one more layer than the proposed method. Table  8  shows that a more complex network structure leads to higher training cost with worse performance. When the receptive field is set to 256, our DRD beats the original one with +1.94% higher in WAR on the four datasets on average. Therefore, the proposed DRD fits the SER applications better with higher generalization ability, which reveals that different speech corpus might need much longer memory.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Interpretability Analysis Of Gm-Tcnet And High-Level Features",
      "text": "The input to GM-TCNet is the MFCC feature that is composed of 39-D cepstral coefficients. We can obtain the coefficients after the discrete cosine transform processes the speech frequency spectrum. The frequency spectrum of the speech signal can be regarded as the superposition of the low-frequency envelope and the high-frequency details. Specifically, the low-frequency component of the cepstrum is the envelope of the spectrum. The envelope  In this section we compare the MFCC features with the high-level features extracted by GTCM in the time domain and the frequency domain. Figure  12  shows that the initial MFCC features contain a large amount of redundant information. In contrast, the high-level features are the outputs of the GTCM, which only retain a small quantity of critical information in the speech signals. In the MFCC features, except for the logarithmic energy coefficient of the first dimension, the diversity of the cepstral coefficients in other dimensions is low. However, the high-level feature of GM-TCNet is significantly different in each dimension to produce more discriminative representation for the SER task. Moreover, Figure  13  confirms that GM-TCNet can capture characteristic information in the time domain and frequency domain simultaneously.",
      "page_start": 17,
      "page_end": 19
    },
    {
      "section_name": "Analysis Of High-Level Features Among Different Emotions",
      "text": "Arousal and valence are two independent dimensions of the continuous emotional model  [73] . Arousal represents how excited or indifferent the emotion is, and valence represents how positive or negative the emotion is.\n\nAlmost all human emotions can be represented by the 2-D space formed by arousal and valence. As shown in Figure  14 , for the high-arousal speech signals, such as Angry, Happy, and Surprise  [74] , the extracted high-level In order to get the insights into the contributions of the high-level features, the information entropy is introduced to evaluate the contributions of the high-level features in various emotions. The information entropy is a quantitative assessment of the information expressed by the image, which reflects how much information there is in the image  [77] . We calculate entropy values for the normalized high-level feature maps from GTCM.\n\nSpecifically, it is assumed that the size of the high-level feature map M is W × H, and the size of M after zero padding is (W + 2) × (H + 2). The average value of the 3 × 3 neighborhood G(x, y) corresponding to the pixel point (x, y) is defined in Eq.(  12 ), where 1 ≤ x ≤ W, 1 ≤ y ≤ H. In particular, K represents the number of non-padding elements in the region of 3 × 3 neighborhood. For every point, a 2-tuple (m, n) is introduced to show that the pixel point (x, y) has the property of M(x, y) = m and G(x, y) = n. In addition, r mn represents the frequency of the 2-tuple (m, n). Then the joint probability density P mn is obtained by Eq.(  13 ), and the 2-D entropy E is calculated by Eq.  (14) .\n\nAs shown in Table  9 , the entropy values of Angry, Happy, and Surprise emotions are higher than those of Sad, Calm, and Boredom emotions. The former three can be classified as excited in arousal, and the last three can be classified as indifferent. This is in accordance with the mapping of the diverse emotion groups onto the arousal in  Furthermore, an autoencoder (AE) is deployed to project the high-level features into low dimensional feature space to explore the difference in high-level features across diverse emotion classes. As shown in Figure  15 , AE is a particular type of neural network composed of an encoder and decoder. And it is trained on the encoded data, and outputs a recreation of that data. In this experiment, the encoder consists of four FC layers with 64, 16, 8 and 2 neurons. The encoder can mine the low dimension representation of high-level features. The decoder consists of four FC layers, and the first three layers contain 8, 16 and 128 neurons. While the last layer includes the same number as the compressed length of the high-level feature extracted by GTCM. In this way, the decoder ensures a small deviation between the reconstructed features and high-level features.  Similar conclusions can be drawn from Figure  17  and Figure  18 . In the EMODB dataset, the Happy emotion forms a cluster that largely overlaps with the Angry emotion, which matches the results in Figure  6b . A similar observation is found in Figure  18 (a) on the RAVDESS dataset. Additionally, Figure  17  and Figure  18  also show that the clusters of arousal emotion are of higher purity than those of the valence emotions, confirming that GM-TCNet provides a higher discriminative ability for the arousal emotions than for the valence emotions.\n\nIt is interesting to find that the patterns shown in Figure  19",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "Conclusions",
      "text": "This paper discusses the SER task by proposing a novel GM-TCNet approach based on the dilated causal convolution and gating mechanism. GM-TCNet is designed to explore causal relationships and long-term dependencies among different emotions. A novel emotional causality representation learning component is designed to capture the dynamics of emotion across the time domain. It also has a strong ability to build a reliable long-term sentimental dependency. It is the first attempt at applying the causality learning method to SER to the best of our knowledge. The experimental results confirm that mining emotional causality in speech is of great significance  for the SER task.\n\nIn the consideration that the human speech expression is not single-scale but multi-scale in nature, GM-TCNet uses the skip connection among all Gated Convolution Blocks. It provides our network structure with a multi-scale temporal receptive field, enhancing the model's speech emotion perception. Moreover, a new dilated rate distribution of blocks is designed to obtain a larger receptive field, so as to better fit the SER applications with higher generalization ability. Compared with the widely deployed methods for SER that used multi-modal features, we believe that the information embedded in a single type of feature can support high discriminative ability given an effective mining scheme. Therefore, this study only deploys the standard MFCC feature to extract high-level features by our GM-TCNet.\n\nExperiment results verify that GM-TCNet successfully captures the high-level features of speech in the time domain. Compared with other studies, it obtains the highest accuracies on the four commonly used datasets in most cases compared to SOTA techniques.\n\nHowever, since the speech datasets used in this study are audio files with short duration, the performance of GM-TCNet in the real-world applications still needs to be further tested. Therefore, our future work will focus on enhancing the generalization ability of the long-duration audio data. At the same time, more efforts will be made to explore other types of features, especially those in the time domain.",
      "page_start": 22,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A dilated causal convolution with dilated rates = 1,2,4 and kernel size k = 2 in the TCNN.",
      "page": 5
    },
    {
      "caption": "Figure 1: , TCNN increases dilated rate d exponentially",
      "page": 5
    },
    {
      "caption": "Figure 2: The residual block",
      "page": 5
    },
    {
      "caption": "Figure 2: TCNN residual block. A 1 × 1 convolution layer is added when block input and output have a diﬀerent shape.",
      "page": 6
    },
    {
      "caption": "Figure 3: shows the MFCC feature extraction process. The framing and windowing operations are ﬁrst applied",
      "page": 6
    },
    {
      "caption": "Figure 3: The workﬂow chart of MFCC feature extraction",
      "page": 7
    },
    {
      "caption": "Figure 4: , for the ith GCB, the dilated rate di.1 in the ﬁrst level is 2i−1 and",
      "page": 7
    },
    {
      "caption": "Figure 4: The structure of the GM-TCNet is composed of the GTCM module (including a Dilated Causal Convolutional layer and seven",
      "page": 8
    },
    {
      "caption": "Figure 4: shows that GCB is divided into two levels. The ﬁrst level is called ”Input Gate”, used to identify the",
      "page": 8
    },
    {
      "caption": "Figure 4: , the input feature is fed to",
      "page": 8
    },
    {
      "caption": "Figure 5: The process of data transfer in Global Average 1-D Pooling Layer.",
      "page": 9
    },
    {
      "caption": "Figure 4: , which includes the GAP layer and FC layer. After",
      "page": 9
    },
    {
      "caption": "Figure 5: , GAP generates a feature map for each",
      "page": 9
    },
    {
      "caption": "Figure 6: a, GM-TCNet achieves 93.00% and 91.00%",
      "page": 11
    },
    {
      "caption": "Figure 6: The 10-fold CV confusion matrix obtained using GM-TCNet on various datasets.",
      "page": 12
    },
    {
      "caption": "Figure 6: b shows that our model achieved 96.06% and 95.16 % accuracy on the Angry",
      "page": 12
    },
    {
      "caption": "Figure 7: The curves reﬂect the inﬂuence of diﬀerent gating levels on the accuracy of the model based on 4 datasets.",
      "page": 15
    },
    {
      "caption": "Figure 7: , the WAR scores are the highest when the gating level is 2 and the other parameters",
      "page": 15
    },
    {
      "caption": "Figure 8: show that the best GSCB number in each layer is 3, which promises",
      "page": 15
    },
    {
      "caption": "Figure 8: The histograms and the curves respectively reﬂect the inﬂuence of diﬀerent numbers of GSCB on the accuracy and variance of the",
      "page": 16
    },
    {
      "caption": "Figure 9: The histograms of WAR and UAR scores obtained by the max-scale and multi-scale receptive ﬁeld methods on 4 datasets.",
      "page": 16
    },
    {
      "caption": "Figure 9: show that the multi-scale method can gain +8.64% and +9.84% relative improvement",
      "page": 16
    },
    {
      "caption": "Figure 10: , ”Raw-128/256” means to use TCNN’s original DRD on GM-TCNet, that",
      "page": 16
    },
    {
      "caption": "Figure 10: The WAR score histograms of diﬀerent Dilated Rate Distributions on 4 datasets.",
      "page": 17
    },
    {
      "caption": "Figure 11: The visualized results of the MFCC feature. From left to right, the nine bars are: input, the outputs of seven GCBs, and the output",
      "page": 18
    },
    {
      "caption": "Figure 11: , GM-TCNet oﬀers similar feature extraction capabilities on the four datasets. In the",
      "page": 18
    },
    {
      "caption": "Figure 11: It can be observed",
      "page": 18
    },
    {
      "caption": "Figure 12: MFCC feature (top) and high-level feature (bottom) visualizations without global 1-D average pooling",
      "page": 19
    },
    {
      "caption": "Figure 13: MFCC feature (left) and high-level feature (right) visualization with global 1-D average pooling",
      "page": 19
    },
    {
      "caption": "Figure 12: shows that the initial MFCC features contain a large amount of",
      "page": 19
    },
    {
      "caption": "Figure 13: conﬁrms that GM-TCNet can capture characteristic information",
      "page": 19
    },
    {
      "caption": "Figure 14: , for the high-arousal speech signals, such as Angry, Happy, and Surprise [74], the extracted high-level",
      "page": 19
    },
    {
      "caption": "Figure 14: The comparison of high-level feature visualization between Angry and Sad on four datasets",
      "page": 20
    },
    {
      "caption": "Figure 15: The network structure of autoencoder.",
      "page": 21
    },
    {
      "caption": "Figure 16: -19 show the 2D projections generated by AE. These ﬁgures depict that the samples in each cluster",
      "page": 21
    },
    {
      "caption": "Figure 16: (a) shows that the features can well split the Angry, Happy, Neutral, and Surprise emotions since the corre-",
      "page": 21
    },
    {
      "caption": "Figure 16: (b) and the valence scores in Figure 16(c) show that the GM-TCNet is",
      "page": 21
    },
    {
      "caption": "Figure 16: High-level features visualizations of the AE representations for CASIA.",
      "page": 22
    },
    {
      "caption": "Figure 17: High-level features visualizations of the AE representations for EMODB.",
      "page": 22
    },
    {
      "caption": "Figure 16: (b) show the gradual change trend. In contrast, the distributions of valence scores present heavy",
      "page": 22
    },
    {
      "caption": "Figure 17: and Figure 18. In the EMODB dataset, the Happy emotion",
      "page": 22
    },
    {
      "caption": "Figure 6: b. A similar",
      "page": 22
    },
    {
      "caption": "Figure 18: (a) on the RAVDESS dataset. Additionally, Figure 17 and Figure 18 also show",
      "page": 22
    },
    {
      "caption": "Figure 19: are pretty diﬀerent from those on other datasets. The",
      "page": 22
    },
    {
      "caption": "Figure 19: (b) does not show the continuous distribution from",
      "page": 22
    },
    {
      "caption": "Figure 19: (b) and Figure 19(c) still exhibit that the three arousal levels",
      "page": 22
    },
    {
      "caption": "Figure 18: High-level features visualizations of the AE representations for RAVDESS.",
      "page": 23
    },
    {
      "caption": "Figure 19: High-level features visualizations of the AE representations for SAVEE.",
      "page": 23
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "...",
          "...": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "...",
          "Column_4": "",
          "...": ""
        },
        {
          "Column_1": "...",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "...": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "..."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)Class labels (b)Arousal scores (c)Valence scores\nFigure16:High-levelfeaturesvisualizationsoftheAErepresentationsforCASIA.\nCASIA": "Figure16:High-levelfeaturesvisualizationsoftheAErepresentationsforCASIA.\nCASIA"
        },
        {
          "(a)Class labels (b)Arousal scores (c)Valence scores\nFigure16:High-levelfeaturesvisualizationsoftheAErepresentationsforCASIA.\nCASIA": "(a)Class labels (b)Arousal scores (c)Valence scores\nFigure17:High-levelfeaturesvisualizationsoftheAErepresentationsforEMODB.\ninFigure16(b)showthegradualchangetrend.Incontrast,thedistributionsofvalencescorespresent\nEMO-DB\nppingscausedbythehighvarianceintheclusteroflowvalencescores.Thesampleswithlowvalence\ngherrorrates. Theseobservationsalignwiththefindingsin[78].\nmilarconclusionscanbedrawnfromFigure17andFigure18. IntheEMODBdataset,theHappye"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)Class labels (b)Arousal scores (c)Valence scores": "Figure18:High-levelfeaturesvisualizationsoftheAErepresentationsforRAVDESS.\nRAVDESS"
        },
        {
          "(a)Class labels (b)Arousal scores (c)Valence scores": "(a)Class labels (b)Arousal scores (c)Valence scores\nFigure19:High-levelfeaturesvisualizSaAtioVnEsEoftheAErepresentationsforSAVEE.\neSERtask.\ntheconsiderationthatthehumanspeechexpressionisnotsingle-scalebutmulti-scaleinnature,GM-TC\nheskipconnectionamongallGatedConvolutionBlocks.Itprovidesournetworkstructurewithamulti-s\nralreceptivefield, enhancingthemodel’sspeechemotionperception. Moreover, anewdilatedratedi\nofblocksisdesignedtoobtainalargerreceptivefield,soastobetterfittheSERapplicationswithhi"
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "3pro -an unsupervised method for the automatic detection of sentence prominence in speech",
      "authors": [
        "S Kakouros",
        "O Räsänen"
      ],
      "year": "2016",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc ¸ay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition in emotional feedback for human-robot interaction",
      "authors": [
        "J Rázuri",
        "D Sundgren",
        "R Rahmani",
        "A Moran",
        "I Bonet",
        "A Larsson"
      ],
      "year": "2015",
      "venue": "International Journal of Advanced Research in Artificial Intelligence (IJARAI)"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition research: an analysis of research focus",
      "authors": [
        "M Mustafa",
        "M Yusoof",
        "Z Don",
        "M Malekzadeh"
      ],
      "year": "2018",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "5",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Inf. Sci"
    },
    {
      "citation_id": "6",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi",
        "S Silverman",
        "M Silverman",
        "D Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "9",
      "title": "Perceptual audio features for emotion detection",
      "authors": [
        "M Sezgin",
        "B Günsel",
        "G Kurt"
      ],
      "year": "2012",
      "venue": "EURASIP J. Audio Speech Music. Process"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from chinese speech for smart affective services using a combination of SVM and DBN",
      "authors": [
        "L Zhu",
        "L Chen",
        "D Zhao",
        "J Zhou",
        "W Zhang"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "11",
      "title": "The state of the art of feature extraction techniques in speech recognition, Speech and language processing for human-machine communications",
      "authors": [
        "D Gupta",
        "P Bansal",
        "K Choudhary"
      ],
      "year": "2018",
      "venue": "The state of the art of feature extraction techniques in speech recognition, Speech and language processing for human-machine communications"
    },
    {
      "citation_id": "12",
      "title": "An appraisal on speech and emotion recognition technologies based on machine learning, language",
      "authors": [
        "C Jason",
        "S Kumar"
      ],
      "year": "2020",
      "venue": "An appraisal on speech and emotion recognition technologies based on machine learning, language"
    },
    {
      "citation_id": "13",
      "title": "Eeg signal classification using linear predictive cepstral coefficient features",
      "authors": [
        "S Pazhanirajan",
        "P Dhanalakshmi"
      ],
      "year": "2013",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "14",
      "title": "Automated accurate speech emotion recognition system using twine shuffle pattern and iterative neighborhood component analysis techniques",
      "authors": [
        "T Tuncer",
        "S Dogan",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "15",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Mustaqeem",
        "S Sajjad",
        "Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "Interspeech 2019, 20th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "17",
      "title": "Deep learning and svm-based emotion recognition from chinese speech for smart affective services",
      "authors": [
        "W Zhang",
        "D Zhao",
        "Z Chai",
        "L Yang",
        "X Liu",
        "F Gong",
        "S Yang"
      ],
      "year": "2017",
      "venue": "Softw. Pract. Exp"
    },
    {
      "citation_id": "18",
      "title": "Research on speech emotion recognition based on deep auto-encoder",
      "authors": [
        "W Fei",
        "X Ye",
        "Z Sun",
        "Y Huang",
        "X Zhang",
        "S Shang"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems"
    },
    {
      "citation_id": "19",
      "title": "SEGAN: speech enhancement generative adversarial network",
      "authors": [
        "S Pascual",
        "A Bonafonte",
        "J Serrà"
      ],
      "year": "2017",
      "venue": "SEGAN: speech enhancement generative adversarial network"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service"
    },
    {
      "citation_id": "21",
      "title": "3d cnn-based speech emotion recognition using k-means clustering and spectrograms",
      "authors": [
        "N Hajarolasvadi",
        "H Demirel"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "22",
      "title": "Deep and shallow features fusion based on deep convolutional neural network for speech emotion recognition",
      "authors": [
        "L Sun",
        "J Chen",
        "K Xie",
        "T Gu"
      ],
      "year": "2018",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "23",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "25",
      "title": "Mlt-dnet: Speech emotion recognition using 1d dilated CNN based on multi-learning trick approach",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2021",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "26",
      "title": "Learning deep multimodal affective features for spontaneous speech emotion recognition",
      "authors": [
        "S Zhang",
        "X Tao",
        "Y Chuang",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition using fusion of three multi-task learningbased classifiers: Hsf-dnn, MS-CNN and LLD-RNN",
      "authors": [
        "Z Yao",
        "Z Wang",
        "W Liu",
        "Y Liu",
        "J Pan"
      ],
      "year": "2020",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "28",
      "title": "Deep learning approaches for speech emotion recognition: state of the art and research challenges",
      "authors": [
        "R Jahangir",
        "Y Teh",
        "F Hanif",
        "G Mujtaba"
      ],
      "year": "2021",
      "venue": "Multim. Tools Appl"
    },
    {
      "citation_id": "29",
      "title": "Learning the sequential temporal information with recurrent neural networks",
      "authors": [
        "P Murugan"
      ],
      "venue": "Learning the sequential temporal information with recurrent neural networks"
    },
    {
      "citation_id": "30",
      "title": "Attention-based dense LSTM for speech emotion recognition",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEICE Trans. Inf. Syst"
    },
    {
      "citation_id": "31",
      "title": "Improving speech emotion recognition using graph attentive bi-directional gated recurrent unit network",
      "authors": [
        "B Su",
        "C Chang",
        "Y Lin",
        "C Lee"
      ],
      "year": "2020",
      "venue": "Interspeech 2020, Virtual Event"
    },
    {
      "citation_id": "32",
      "title": "An efficient temporal modeling approach for speech emotion recognition by mapping varied duration sentences into fixed number of chunks",
      "authors": [
        "W Lin",
        "C Busso"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition using deep 1d & 2d CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "Clstm: Deep feature-based speech emotion recognition using the hierarchical convlstm network",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2020",
      "venue": "Mathematics"
    },
    {
      "citation_id": "36",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
    },
    {
      "citation_id": "37",
      "title": "Recent advances in recurrent neural networks",
      "authors": [
        "H Salehinejad",
        "J Baarbe",
        "S Sankar",
        "J Barfett",
        "E Colak",
        "S Valaee"
      ],
      "venue": "Recent advances in recurrent neural networks"
    },
    {
      "citation_id": "38",
      "title": "The causal theory of perception",
      "authors": [
        "J Hyman"
      ],
      "year": "1950",
      "venue": "The Philosophical Quarterly"
    },
    {
      "citation_id": "39",
      "title": "Causal inference in time series analysis",
      "authors": [
        "M Eichler"
      ],
      "year": "2012",
      "venue": "Causality: Statistical Perspectives and Applications"
    },
    {
      "citation_id": "40",
      "title": "The organisation of emotional experience: Creating links among emotion, thinking, language, and intentional action",
      "authors": [
        "N Stein",
        "T Trabasso"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "41",
      "title": "Affect2mm: Affective analysis of multimedia content using emotion causality",
      "authors": [
        "T Mittal",
        "P Mathur",
        "A Bera",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual"
    },
    {
      "citation_id": "42",
      "title": "Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "X Wang",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "43",
      "title": "On the opportunity of causal learning in recommendation systems: Foundation, estimation, prediction and challenges",
      "authors": [
        "P Wu",
        "H Li",
        "Y Deng",
        "W Hu",
        "Q Dai",
        "Z Dong",
        "J Sun",
        "R Zhang",
        "X Zhou"
      ],
      "year": "2022",
      "venue": "IJCAI"
    },
    {
      "citation_id": "44",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "TCNN: temporal convolutional neural network for real-time speech enhancement in the time domain",
      "authors": [
        "A Pandey",
        "D Wang"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Furcanext: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks",
      "authors": [
        "L Zhang",
        "Z Shi",
        "J Han",
        "A Shi",
        "D Ma"
      ],
      "year": "2020",
      "venue": "MultiMedia Modeling -26th International Conference, MMM 2020"
    },
    {
      "citation_id": "47",
      "title": "End-to-end speech emotion recognition using a novel context-stacking dilated convolution neural network",
      "authors": [
        "D Tang",
        "P Kuppens",
        "L Geurts",
        "T Van Waterschoot"
      ],
      "year": "2021",
      "venue": "EURASIP J. Audio Speech Music. Process"
    },
    {
      "citation_id": "48",
      "title": "Gatenet: Gating-enhanced deep network for click-through rate prediction",
      "authors": [
        "T Huang",
        "Q She",
        "Z Wang",
        "J Zhang"
      ],
      "venue": "Gatenet: Gating-enhanced deep network for click-through rate prediction"
    },
    {
      "citation_id": "49",
      "title": "Convolutional neural networks: an overview and application in radiology, Insights into imaging",
      "authors": [
        "R Yamashita",
        "M Nishio",
        "R Do",
        "K Togashi"
      ],
      "year": "2018",
      "venue": "Convolutional neural networks: an overview and application in radiology, Insights into imaging"
    },
    {
      "citation_id": "50",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "J Tao",
        "F Liu",
        "M Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge 2008 workshop"
    },
    {
      "citation_id": "51",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "INTERSPEECH 2005 -Eurospeech, 9th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "52",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "53",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "54",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "55",
      "title": "12th USENIX Symposium on Operating Systems Design and Implementation",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard",
        "M Kudlur",
        "J Levenberg",
        "R Monga",
        "S Moore",
        "D Murray",
        "B Steiner",
        "P Tucker",
        "V Vasudevan",
        "P Warden",
        "M Wicke",
        "Y Yu",
        "X Zheng"
      ],
      "year": "2016",
      "venue": "12th USENIX Symposium on Operating Systems Design and Implementation"
    },
    {
      "citation_id": "56",
      "title": "A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "57",
      "title": "Research on psychological counseling and personality analysis algorithm based on speech emotion",
      "authors": [
        "Z Hong",
        "C Wei",
        "Y Zhuang",
        "Y Wang",
        "Y Wang",
        "L Zhao"
      ],
      "year": "2020",
      "venue": "Artificial Intelligence and Security -6th International Conference"
    },
    {
      "citation_id": "58",
      "title": "Decision tree SVM model with fisher feature selection for speech emotion recognition",
      "authors": [
        "L Sun",
        "S Fu",
        "F Wang"
      ],
      "year": "2019",
      "venue": "EURASIP J. Audio Speech Music. Process"
    },
    {
      "citation_id": "59",
      "title": "End-to-end speech emotion recognition based on onedimensional convolutional neural network",
      "authors": [
        "M Gao",
        "J Dong",
        "D Zhou",
        "Q Zhang",
        "D Yang"
      ],
      "year": "2019",
      "venue": "ICIAI 2019: The 3rd International Conference on Innovation in Artificial Intelligence"
    },
    {
      "citation_id": "60",
      "title": "Speech emotion recognition using xgboost and CNN BLSTM with attention",
      "authors": [
        "J He",
        "L Ren"
      ],
      "year": "2021",
      "venue": "2021 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Internet of People and Smart City Innovation"
    },
    {
      "citation_id": "61",
      "title": "Pseudo-colored rate map representation for speech emotion recognition",
      "authors": [
        "I Ozer"
      ],
      "year": "2021",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "62",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T Özseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "63",
      "title": "Speaker awareness for speech emotion recognition",
      "authors": [
        "G Assunc ¸ão",
        "P Menezes",
        "F Perdigão"
      ],
      "year": "2020",
      "venue": "Int. J. Online Biomed. Eng"
    },
    {
      "citation_id": "64",
      "title": "Speech emotion recognition using discriminative dimension reduction by employing a modified quantum-behaved particle swarm optimization algorithm",
      "authors": [
        "F Daneshfar",
        "S Kabudian"
      ],
      "year": "2020",
      "venue": "Multim. Tools Appl"
    },
    {
      "citation_id": "65",
      "title": "Automatic speech emotion recognition using an optimal combination of features based on EMD-TKEO",
      "authors": [
        "L Kerkeni",
        "Y Serrestou",
        "K Raoof",
        "M Mbarki",
        "M Mahjoub",
        "C Cléder"
      ],
      "year": "2019",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "66",
      "title": "A modified feature selection method based on metaheuristic algorithms for speech emotion recognition",
      "authors": [
        "S Yildirim",
        "Y Kaya",
        "F Kılıc"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "67",
      "title": "Cross corpus multi-lingual speech emotion recognition using ensemble learning",
      "authors": [
        "W Zehra",
        "A Javed",
        "Z Jalil",
        "H Khan",
        "T Gadekallu"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "68",
      "title": "Speech emotion recognition using 1d cnn with no attention, in: 2019 23rd international computer science and engineering conference (ICSEC)",
      "authors": [
        "Y Li",
        "C Baidoo",
        "T Cai",
        "G Kusi"
      ],
      "year": "2019",
      "venue": "Speech emotion recognition using 1d cnn with no attention, in: 2019 23rd international computer science and engineering conference (ICSEC)"
    },
    {
      "citation_id": "69",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "70",
      "title": "Negative emotion recognition using deep learning for thai language",
      "authors": [
        "S Mekruksavanich",
        "A Jitpattanakul",
        "N Hnoohom"
      ],
      "year": "2020",
      "venue": "2020 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering"
    },
    {
      "citation_id": "71",
      "title": "Speech emotion recognition using clustering based ga-optimized feature set",
      "authors": [
        "S Kanwal",
        "S Asghar"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "72",
      "title": "Grouped echo state network with late fusion for speech emotion recognition",
      "authors": [
        "H Ibrahim",
        "C Loo",
        "F Alnajjar"
      ],
      "year": "2021",
      "venue": "Neural Information Processing -28th International Conference, ICONIP 2021"
    },
    {
      "citation_id": "73",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "74",
      "title": "Emotion classification in arousal valence model using mahnob-hci database",
      "authors": [
        "M Wiem",
        "Z Lachiri"
      ],
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "75",
      "title": "A hierarchical static-dynamic framework for emotion classification",
      "authors": [
        "E Mower",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "76",
      "title": "Evaluation of influence of arousal-valence primitives on speech emotion recognition",
      "authors": [
        "I Trabelsi",
        "D Ayed",
        "N Ellouze"
      ],
      "year": "2018",
      "venue": "Int. Arab J. Inf. Technol"
    },
    {
      "citation_id": "77",
      "title": "Information entropy measure for evaluation of image quality",
      "authors": [
        "D Tsai",
        "Y Lee",
        "E Matsuyama"
      ],
      "year": "2008",
      "venue": "J. Digit. Imaging"
    },
    {
      "citation_id": "78",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "79",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "G Rigoll",
        "A Wendemuth"
      ],
      "year": "2009",
      "venue": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
    }
  ]
}