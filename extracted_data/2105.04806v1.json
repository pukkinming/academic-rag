{
  "paper_id": "2105.04806v1",
  "title": "Deep Scattering Network For Speech Emotion Recognition",
  "published": "2021-05-11T06:37:41Z",
  "authors": [
    "Premjeet Singh",
    "Goutam Saha",
    "Md Sahidullah"
  ],
  "keywords": [
    "Deep convolutional networks",
    "Deep scattering transform",
    "EmoDB",
    "IEMOCAP",
    "RAVDESS",
    "Shift invariance",
    "Speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper introduces scattering transform for speech emotion recognition (SER). Scattering transform generates feature representations which remain stable to deformations and shifting in time and frequency without much loss of information. In speech, the emotion cues are spread across time and localised in frequency. The time and frequency invariance characteristic of scattering coefficients provides a representation robust against emotion irrelevant variations e.g., different speakers, language, gender etc. while preserving the variations caused by emotion cues. Hence, such a representation captures the emotion information more efficiently from speech. We perform experiments to compare scattering coefficients with standard melfrequency cepstral coefficients (MFCCs) over different databases. It is observed that frequency scattering performs better than time-domain scattering and MFCCs. We also investigate layerwise scattering coefficients to analyse the importance of time shift and deformation stable scalogram and modulation spectrum coefficients for SER. We observe that layer-wise coefficients taken independently also perform better than MFCCs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The introduction of deep learning has caused rapid development in different speech signal processing tasks. One such domain is speech emotion recognition (SER) which finds applications in health-care systems, sentiment analysis, and many other human-computer interaction applications  [1] -  [3] . Even after two decades of research, SER is still considered as a challenging task  [2] . One of the major challenges is the interpersonal and intrapersonal variability in emotion expression, e.g., different speaking styles, cultural background, language, context, speaker's mood etc  [1] ,  [3] . This causes SER systems to face difficulty in generalization across different emotion speech samples, especially when the train and test data samples belong to different domains  [4] . This downside motivates the search for a robust feature that can extract emotion-specific information irrespective of different variabilities.\n\nThe deep learning based SER approaches generally addresses the issue of variabilities by using deep convolutional neural networks (CNN), which are considered efficient feature extractors in terms of invariances learned against signal shifts  [5] ,  [6] . However, the complexity and lack of explainability of deep CNNs come as an added disadvantage to automatic feature extraction ability  [7] .\n\nTo obtain an end-to-end framework, some SER studies use 1D CNNs to learn the relevant features directly from raw speech  [8] . Such studies observe that deep networks learn representations similar to handcrafted features  [9] . However, due to inherent instability and requirement of large data samples for training, suitability of deep neural networks becomes uncertain  [10] .\n\nFollowing similar lines, we propose the use of scattering transform for SER. Scattering transform was introduced in  [11] ,  [12]  as a deep convolutional network involving convolution with predefined kernel instead of automatic kernel learning. It also introduces stability to both temporal shift and deformations in the feature representation of signals. As emotion cues spread temporally in speech, such characteristics of scattering transform should provide a representation robust to both time-shifting of cues and emotion irrelevant temporal variations. In  [13] , authors compute scattering coefficients across the log-frequency domain for frequency transposition invariance to obtain speaker-independent representation for speech recognition. Such characteristics may also help in learning speaker-independent emotion cues.\n\nScattering transform is used in both 1-D and 2-D data processing. Regarding 1-D signals, authors in  [14]  introduce joint time-frequency scattering that incorporates multiscale frequency energy distribution over time-invariant representation for various audio classification frameworks. Authors in  [15]  use two-layer scattering coefficients with CNN layers to obtain a stable descriptor of speaker information from raw speech. In  [16] , authors compute different moments of scattering coefficients layers and found that such moments contain enough information for decent voice synthesis quality. Authors in  [17]  used scattering transform for urban sound classification and report a marginal performance improvement but with reduced training data. They attribute their finding to better background information learning ability of temporal modulations. Along similar lines,  [18]  also applies to scatter coefficients for environmental sound classification. In  [19]  a joint time-frequency based scattering representation is used to analyze the timbral similarity between different acoustic instruments. Our work is the first to apply scattering transform for SER. We conduct experiments on three different SER datasets using time and frequency scattering coefficients and achieve noticeable improvement over standard mel-frequency cepstral coefficients (MFCCs). The main contributions of this work are summarized below: 1) Optimization of scattering transform parameters and analysing its performance for SER, 2) Analysis of time-domain and frequency-domain scattering coefficients over different databases, and 3) Layer-wise analysis of scattering coefficients for SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Scattering Transform",
      "text": "For 1D signals, the idea behind scattering transform is to obtain a robust feature that remains invariant to the location of information cues (translation invariance) and time warping (diffeomorphism) of information across different instances of the signal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Notion Of Lipschitz Continuity And Information Loss In Mfcc",
      "text": "Let the signal x(t) be translated by c. Then we have, x c (t) = x(t -c). Taking its Fourier transform, we obtain, X c (ω) = e -jωc X(ω). Taking modulus on both sides, we obtain, |X c (ω)| = |X(ω)|. In short time Fourier transform (STFT), translation c is localized to the time frame duration T . If |c| ≪ T , the STFT representation is already invariant to such translation. However, a robust feature representation also requires stability to time deformations appearing in the signal. Let x(t) is warped (deformed) in time by a factor τ , i.e., x τ (t) = x(t -τ (t)). A transformation φ of x(t) is said to be stable to deformation τ if, ||φ(x) -φ(x τ )|| ≤ Csup\n\nHence, the frequency components at ω are shifted to 1  1-ω . This effect is more prominent at high frequencies. Hence, modulus of Fourier transform is not Lipschitz continuous. In MFCC, filters with non-linearly varying bandwidths are applied over the STFT. As output of every mel-filter is averaged to get a single coefficient, filter application can be viewed as averaging performed over frequency domain which counters the effect of instability. The mel-filters applied around high frequency regions have higher bandwidth which reduces the effect of deformation instability. However, this averaging in frequency also leads to information loss, especially at higher frequencies. Now, consider x t (u) to be the signal frame at time t. Then x t (u) = x(u)φ(u-t) where φ is a window of length T . The application of mel-filters over Fourier transform of x t (u), can be written in frequency as,\n\nwhere x t (ω) is the frequency response of x t (u) and ψ λ (ω) is the mel-filter with support λ. Since multiplication in frequency becomes convolution in time, Eq. 1 becomes,\n\nIf T is much greater than the filter support λ in time, φ(t) is constant over λ. Hence we can write, φ(u\n\nEq. 3 shows that averaging in frequency domain is equivalent to time-domain averaging of |x * ψ λ | 2 with window duration T and becomes the basis for scattering transform. Hence, M x (t, λ) remains invariant to time shifts smaller than T . In MFCC, T generally equals 20ms which limits the capturing of long time scale information. If T is increased, loss of high frequency information takes place due to averaging  [13] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Layer-Wise Scattering Coefficients",
      "text": "To prevent the information loss appearing in MFCC while, at the same time, capturing high frequency information, scattering transform computes second layer coefficients or the modulation spectrogram of signal. This includes computing the frequency response of the time-series of scalogram frequency bins. This is followed by averaging of both scalogram and modulation spectrum coefficients for stability against deformation. Scattering transform uses constant quality factor wavelet filters over complete frequency range to compute scalogram and modulation spectrogram. If Q defines the number of wavelets per octave, the center frequencies of wavelets becomes λ = 2 k Q , where k is an integer and the bandwidth is proportional to 1/Q. Every wavelet has a support of bandwidth λ/Q around center frequency λ. The same wavelet in time provide a spread of 2πQ λ . To make sure that this spread is less than T (because T is the time window over which time averaging will be computed for stable feature representation) wavelets are only defined for λ ≥ 2πQ T . For frequencies less than 2πQ T the frequency interval is covered with wavelets of equal bandwidth given by 2π T , hence covering complete frequency range. The mel-filter operation and filtering due to time window (T ) in MFCC is replaced with wavelet-based filters in scattering transform.\n\nAn issue with using wavelets is that they \"commute with translation\"  [20] . However, invariance to translation can generally be introduced by averaging. Following this, x(t) * ψ λ (t)dt should be translation invariant. But due to symmetric nature of wavelet, x(t) * ψ λ (t)dt = 0. Hence, a modulus nonlinear operator is applied to prevent this vanishing of integral, i.e. |x(t) * ψ λ (t)|dt. The reason behind choosing modulus non-linearity is because modulus operation is contractive and preserves the norm of the vector to which it is applied  [14] . The modulus of Fourier transform coefficients are then lowpass filtered with filter φ to obtain feature representation robust against time-warping induced deformation instability. These features are then invariant to translations smaller than 2 J , where J is the scale of the low-pass wavelet filter (φ). Therefore, any mth order scattering coefficients are given as,\n\nwhere,\n\nTo obtain invariance to frequency transpositions, wavelet transform is first computed over the log-frequency axis (e.g., log(λ 1 )) of time-domain scattering coefficients followed by wavelet averaging. This is similar to the DCT operation in MFCC  [13] . Such averaging introduces invariance to shifts in frequency defined by the scale of the averaging wavelet. The frequency scattering coefficients are cascaded with time scattering coefficients to generate a feature representation of the signal.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experimental Details",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset Description",
      "text": "Three different speech corpora used in the experiments are described in Table  I . The speech samples are downsampled at 16 kHz sampling frequency when required. EmoDB database is a German-language database whereas, both IEMOCAP and RAVDESS contain speech samples in the English language. For IEMOCAP, we use only four emotions (Happy, Angry, Sad, and Neutral) following other works with this dataset  [4] ,  [21] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Evaluation & Methodology",
      "text": "We first optimize the scattering transform parameters over the EmoDB database. The optimized parameter set is then used for SER evaluation over other databases. We perform only twolevel decomposition as higher level coefficients contain a very small fraction of signal energy  [14] . We optimize the number of wavelets per octave (Q) and maximal wavelet length or  averaging scale (T ) while using an empirically chosen fixed value of input signal duration (N ). For frequency scattering, we select the maximal wavelet length as 32. We choose this to obtain invariance over five octaves, found optimum for EmoDB database. Also, following  [13] , for the first layer timescattering coefficients, we compute only the frequency-domain wavelet decomposition. The required optimum frequency averaging is computed and applied by the classifier itself  [13] . We use Morlet wavelet for both time and frequency scattering coefficients computation  [13] . For implementation, we use the ScatNet toolkit 1 .\n\nFor classification purposes, we use a radial basis function (RBF) kernel-based support vector machine (SVM), as the feature representations obtained are utterance-level representations. We use the leave-one-speaker-out (LOSO) crossvalidation strategy and report average performance obtained over different train-test pairs. For every experiment, we keep speech samples of one speaker in validation, one in test and remaining in training set. We optimize the hyperparameters of the SVM classifier on the validation set.\n\nFor comparison, we also compute MFCCs for speech utterances of duration N and evaluate the performance with the same LOSO cross-validation strategy. We choose the MFCC feature because of its characteristic of mimicking human sound perception and its versatility in various speech processing domains. For performance analysis, we use both accuracy and unweighted average recall (UAR) performance metrics. The UAR is defined as,\n\nHere A is the contingency matrix, A ij is the number of samples in class i classified into class j and K is the number of classes. UAR is better suited as a performance metric in class imbalance situations as compared to standard accuracy  [25] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Parameter Optimization Over Emodb",
      "text": "Figure  2  shows the results obtained by varying Q and T parameters of scattering transform over EmoDB database.  The optimum value of signal duration (N ) is empirically at 51000 samples (or 3.18 seconds at 16 kHz sampling rate). We start with the lowest value of T to be 4096 samples, which correspond to 256 ms at 16KHz, a duration considered optimum to convey sufficient emotion cues  [26] . T is then increased by a factor of 2 until its value is below N . Interestingly, T = 4096 shows poor performance, whereas, T = 32768 shows better performance over different values of Q. In terms of parameter Q, we observed a general increase in performance for lower values of wavelets per octave. This indicates the importance of coarse-level frequency domain information for SER. This result is also comparable to the experiments done using constant-Q transform (CQT) for SER  [27] . We select T = 16384 and Q = 5 as the optimum value of parameters for further experiments. Figure  3  shows the confusion matrices for scattering coefficients and MFCCs over EmoDB database. Scattering coefficients can better classify speech samples of different emotion classes in EmoDB as compared to MFCC.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Evaluation On Other Datasets",
      "text": "Table  II  shows the performance obtained with the optimised scattering parameters (ScatNet) over different databases and its comparison with standard MFCC. We compute 13 MFCCs over window length of 20 ms, 10 ms hop and 512 frequency bins over same utterance duration N (3.18 seconds). Mean and standard deviation of MFCCs is computed over all the frames to obtain a vector representation for every utterance. The scattering coefficients are observed to outperform MFCCs over every database. We also compare performance with frequency domain scattering transform (F-ScatNet) with similar optimized parameters (Q, T and N ). The frequency transposition invariance obtained from frequency domain scattering introduces speaker invariance hence improving the performance. However, such improvement is not very prominent in the EmoDB database. One probable reason for such observation could be increased redundancy in frequency scattering coefficients because of small database size and low number of speakers.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Layer-Wise Analysis",
      "text": "To analyse emotion relevance of different layers of scattering transform, we compare SER performance by using the first and second layer coefficients separately in Table  III . The first layer coefficients are averaged scalogram coefficients similar to MFCC. However, the averaging provided by filter φ introduces time shift invariance which results in better performance than MFCC. The second layer coefficients further perform better than first layer coefficients showing the relevance of modulation spectrum coefficients from SER perspective. We observe that the scattering transform coefficients, with optimized parameters, provide feature representations that are better suited for SER. The optimized wavelet averaging scale provides sufficient invariance and stability to irrelevant temporal variations to capture the emotion cues in the time domain. The low value of wavelets per octave generates coarse-level frequency domain information improving SER performance. Invariance to frequency transposition in frequency scattering further reduces the speaker-dependent variations enhancing the system performance. The performed layer-wise analysis shows the importance of time-domain averaging over the typical scalogram/mel-spectrogram coefficients. Results obtained with second layer scattering coefficients indicate the relevance of amplitude modulation of time series of different scalogram frequency bins.\n\nWe conclude that deep convolutional scattering coefficients capture more relevant emotion-related information than the standard mel-filterbank based feature. However, the performance with optimized parameter set on EmoDB varies noticeably across databases, indicating the lack of generalization. We also observe that the system faces difficulty in classifying emotions which have similar arousal characteristics (e.g., Happy & Angry). This hints towards the requirement of further analysis on improving the scattering coefficient based feature representation for SER. Our work is a preliminary study that introduces scattering transform for SER. In future, we will explore other back-end classifiers like multi-layer perceptron, gaussian mixture models etc. with scattering coefficients. To further evaluate the generalisation across samples from different domains, cross-corpus analysis for SER can also be evaluated.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Graphical representation of scattering transform. Here, x is the input",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the results obtained by varying Q and",
      "page": 3
    },
    {
      "caption": "Figure 2: Comparison of various time-domain scattering transform parameters over EmoDB database. Q is number of wavelets per octaves and T is the",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion Matrices of time-domain scattering coefﬁcients and MFCC features over EmoDB database. Although scattering coefﬁcients show higher",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the confusion matrices",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "37": "5",
          "2": "20",
          "7": "5",
          "6": "1",
          "3": "5",
          "8": "8"
        },
        {
          "37": "8",
          "2": "3",
          "7": "27",
          "6": "2",
          "3": "0",
          "8": "29"
        },
        {
          "37": "12",
          "2": "4",
          "7": "0",
          "6": "24",
          "3": "8",
          "8": "2"
        },
        {
          "37": "9",
          "2": "0",
          "7": "2",
          "6": "48",
          "3": "0",
          "8": "3"
        },
        {
          "37": "4",
          "2": "0",
          "7": "0",
          "6": "4",
          "3": "45",
          "8": "0"
        },
        {
          "37": "6",
          "2": "0",
          "7": "12",
          "6": "4",
          "3": "1",
          "8": "104"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "",
          "F-ScatNet": "Acc.\nUAR",
          "ScatNet": "Acc.\nUAR",
          "MFCC": "Acc.\nUAR"
        },
        {
          "Database": "EmoDB",
          "F-ScatNet": "74.59\n70.27",
          "ScatNet": "74.40\n71.30",
          "MFCC": "58.39\n54.03"
        },
        {
          "Database": "RAVDESS",
          "F-ScatNet": "51.81\n50.52",
          "ScatNet": "50.00\n48.50",
          "MFCC": "36.74\n34.77"
        },
        {
          "Database": "IEMOCAP",
          "F-ScatNet": "61.55\n51.00",
          "ScatNet": "60.41\n50.40",
          "MFCC": "55.54\n47.19"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "",
          "First\nlayer": "Accuracy\nUAR",
          "Second layer": "Accuracy\nUAR"
        },
        {
          "Database": "EmoDB",
          "First\nlayer": "62.18\n57.94",
          "Second layer": "68.64\n61.22"
        },
        {
          "Database": "RAVDESS",
          "First\nlayer": "38.06\n36.72",
          "Second layer": "48.61\n47.20"
        },
        {
          "Database": "IEMOCAP",
          "First\nlayer": "56.48\n47.30",
          "Second layer": "58.93\n48.80"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of speech emotion recognition in natural environment",
      "authors": [
        "M Shah Fahad",
        "A Ranjan",
        "J Yadav",
        "A Deepak"
      ],
      "year": "2021",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Analysis of deep learning architectures for cross-corpus speech emotion recognition",
      "authors": [
        "J Parry",
        "D Palaz",
        "G Clarke",
        "P Lecomte",
        "R Mead",
        "M Berger",
        "G Hofer"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "5",
      "title": "Convolutional neural networks for speech recognition",
      "authors": [
        "O Abdel-Hamid",
        "A Mohamed",
        "H Jiang",
        "L Deng",
        "G Penn",
        "D Yu"
      ],
      "year": "2014",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "The mythos of model interpretability",
      "authors": [
        "Z Lipton"
      ],
      "year": "2018",
      "venue": "Queue"
    },
    {
      "citation_id": "8",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Understanding and visualizing raw waveform-based CNNs",
      "authors": [
        "H Muckenhirn",
        "V Abrol",
        "M Magimai-Doss",
        "S Marcel"
      ],
      "year": "2019",
      "venue": "Proc. INTER-SPEECH"
    },
    {
      "citation_id": "10",
      "title": "Deep learning is robust to massive label noise",
      "authors": [
        "D Rolnick",
        "A Veit",
        "S Belongie",
        "N Shavit"
      ],
      "year": "2017",
      "venue": "Deep learning is robust to massive label noise",
      "arxiv": "arXiv:1705.10694"
    },
    {
      "citation_id": "11",
      "title": "Recursive interferometric representation",
      "authors": [
        "S Mallat"
      ],
      "year": "2010",
      "venue": "Proc. of EU-SICO conference"
    },
    {
      "citation_id": "12",
      "title": "Group invariant scattering",
      "year": "2012",
      "venue": "Communications on Pure and Applied Mathematics"
    },
    {
      "citation_id": "13",
      "title": "Deep scattering spectrum",
      "authors": [
        "J Andén",
        "S Mallat"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Joint time-frequency scattering",
      "authors": [
        "J Andén",
        "V Lostanlen",
        "S Mallat"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Hybrid network for end-to-end text-independent speaker identification",
      "authors": [
        "W Ghezaiel",
        "L Brun",
        "O Lézoray"
      ],
      "year": "2021",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Audio texture synthesis with scattering moments",
      "authors": [
        "J Bruna",
        "S Mallat"
      ],
      "year": "2013",
      "venue": "Audio texture synthesis with scattering moments",
      "arxiv": "arXiv:1311.0407"
    },
    {
      "citation_id": "17",
      "title": "Feature learning with deep scattering for urban sound analysis",
      "authors": [
        "J Salamon",
        "J Bello"
      ],
      "year": "2015",
      "venue": "Proc. EUSIPCO"
    },
    {
      "citation_id": "18",
      "title": "Representing environmental sounds using the separable scattering transform",
      "authors": [
        "C Baugé",
        "M Lagrange",
        "J Andén",
        "S Mallat"
      ],
      "year": "2013",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Time-frequency scattering accurately models auditory similarities between instrumental playing techniques",
      "authors": [
        "V Lostanlen",
        "C El-Hajj",
        "M Rossignol",
        "G Lafay",
        "J Andén",
        "M Lagrange"
      ],
      "year": "2021",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "20",
      "title": "Invariant scattering convolution networks",
      "authors": [
        "J Bruna",
        "S Mallat"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "22",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "23",
      "title": "The Ryerson Audio-Visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS One"
    },
    {
      "citation_id": "24",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "25",
      "title": "Classifying skewed data: Importance weighting to optimize average recall",
      "authors": [
        "A Rosenberg"
      ],
      "year": "2012",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Non-linear frequency warping using constant-q transformation for speech emotion recognition",
      "authors": [
        "P Singh",
        "G Saha",
        "M Sahidullah"
      ],
      "venue": "2021 International Conference on Computer Communication and Informatics"
    }
  ]
}