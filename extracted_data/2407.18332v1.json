{
  "paper_id": "2407.18332v1",
  "title": "Analyzing Speech Unit Selection For Textless Speech-To-Speech Translation",
  "published": "2024-07-08T08:53:26Z",
  "authors": [
    "Jarod Duret",
    "Yannick Estève",
    "Titouan Parcollet"
  ],
  "keywords": [
    "speech translation",
    "discrete audio token",
    "selfsupervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advancements in textless speech-to-speech translation systems have been driven by the adoption of selfsupervised learning techniques. Although most state-of-the-art systems adopt a similar architecture to transform source language speech into sequences of discrete representations in the target language, the criteria for selecting these target speech units remains an open question. This work explores the selection process through a study of downstream tasks such as automatic speech recognition, speech synthesis, speaker recognition, and emotion recognition. Interestingly, our findings reveal a discrepancy in the optimization of discrete speech units: units that perform well in resynthesis performance do not necessarily correlate with those that enhance translation efficacy. This discrepancy underscores the nuanced complexity of target feature selection and its impact on the overall performance of speech-to-speech translation systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech-to-speech translation (S2ST) provides a powerful means of overcoming the communication gap between people speaking different languages by enabling effective communication across diverse languages and cultures. Several approaches have been proposed in the literature, including cascaded approaches  [1, 2]  that combine automatic speech recognition (ASR), machine translation (MT) and text-to-speech (TTS). More recently, textless approach which leverages discrete speech units extracted from self-supervised representation has been introduced  [3, 4] . This technique is specifically designed to capture the linguistic content of the target speech effectively while minimizing the influence of the speaker's prosodic features. Previous studies demonstrated that the use of discrete speech units effectively separates linguistic content from prosodic characteristics and speaker identity. However, an open question remains regarding the construction and selection of these discrete speech units. In  [3, 4] , the authors opted to utilize HuBERT  [5] , as this model has demonstrated superior performance in automatic speech recognition (ASR), spoken language modeling, and speech synthesis compared to other unsupervised representations, as shown in  [6] . Although this superiority has been established for continuous representations, there is a notable lack of analysis concerning discrete ones, especially on a layer-wise basis.\n\nFurthermore, available speech translation corpora, such as Fisher and CALLHOME  [7]  or CoVoST 2  [8] , do not contain parallel speech, the target language speech must be synthesized from text translations. A few datasets already provide TTS-synthesized speech, such as CVSS  [9] , and more recently, SpeechMatrix  [10] , a large-scale multilingual corpus containing real speech. The common issue with all these datasets is the mismatch in speaker identity and emotion, they are not consistent across the source and target speech. This inconsistency underscores the need to take these elements into account when selecting the target speech representation.\n\nIn this study, we explore the challenge of choosing effective discrete units for textless speech-to-speech translation. Additionally, we evaluate discrete self-supervised representations from various encoders reported in the literature across four downstream tasks: automatic speech recognition, speech synthesis, speaker recognition, and emotion recognition. Then, we investigate the potential of using semantically aligned (speechtext) speech representations to improve the ability of discrete speech units to preserve semantic information. This approach aims to enhance robustness against acoustic variations that could otherwise lead to diminished translation performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "For this study, we considered all existing models in the literature for speech-to-speech translation (S2ST) to the best of our knowledge. Consequently, two self-supervised encoders have been selected: Wav2vec 2.0  [11]  and HuBERT  [5] , each available in both monolingual and multilingual versions. Additionally, SAMU-XLSR  [12] , a distilled version of Wav2Vec XLS-R  [13]  fine-tuned to predict text embeddings from a LaBSE  [14]  text encoder, is also included in our analysis. All considered models generate output at the same frequency, producing a representation of size D every 20 ms of the audio signal. For the Large versions, D = 1,024, and for the Base versions, D = 768. The models are based on very similar Transformer-based architectures, yet they differ in their pretraining pretext tasks. The training of Wav2vec 2.0 is based on the contrastive predictive coding  [15]  (CPC) objective, which aims to maximize the mutual information between a set of context features and predicted future samples. Meanwhile, HuBERT's approach involves mapping unlabeled audio to sequences of pseudo-labels obtained through the clustering of previous representations. To extract the sequence of speech units, we employ k-means clustering on the raw speech features, using the learned centroids of the K clusters to convert audio into a sequence of cluster indices for every 20ms segment of the input audio signal. For the base model, we extract representations from every second layer, and for the large model, from every fourth layer, to maintain manageable experiment scales. Another parameter is the choice of k. In line with prior research, we explore three values of k: 128, 512, and 1024. This approach allows us to assess the impact of cluster granularity on the performance of our downstream tasks. arXiv:2407.18332v1 [eess.AS] 8 Jul 2024",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Downstream Tasks And Datasets",
      "text": "To align with previous self-supervised learning (SSL) studies, we evaluate discrete speech representations across various tasks assessing different aspects of the speech signal We present four tasks designed to analyze aspects related to phonetics, speaker identity, emotions, and semantics.\n\nEmotion Recognition (ER): ESD  [16] , a multilingual emotional database, consists of 350 parallel utterances recorded by 10 native English and 10 native Chinese speakers (10 females, 10 males), containing five emotional states (neutral, happy, angry, sad, and surprise). In this study, we focus exclusively on the English subset. The official training, development, and testing splits are utilized for evaluation, with accuracy serving as the evaluation metric.\n\nAutomatic Speech Recognition (ASR): LibriSpeech  [17] , a corpus of approximately 1000 hours of 16kHz read English speech derived from read audiobooks. In this study, we concentrate on the train-clean-100 subset for training. The dev-clean subset is used for validation, while the test-clean and test-other subsets are employed for testing. Character Error Rate (CER) serves as the error metric.\n\nAutomatic Speaker Verification (ASV): VoxCeleb1  [18] , a large-scale speaker identification dataset, contains over 100,000 utterances from 1,251 celebrities, extracted from videos uploaded to YouTube. Official training, development, and testing splits are utilized for evaluation, ensuring no overlap between speakers in the training and testing sets. The evaluation metric is the Equal Error Rate (EER).\n\nSpeech Synthesis: LJSpeech  [19] , a dataset comprising 13,100 short audio clips from a single speaker reading passages from 7 non-fiction books, totaling approximately 24 hours. We randomly split the dataset into training, development, and testing sets with a ratio of 80:10:10%. The evaluation metric is the Mean Opinion Score (MOS). Given the number of models, we opted for the UTokyo-SaruLab MOS prediction system  [20]  to automatically assess the quality of the trained models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Systems Description",
      "text": "This section provides a brief description of the downstream probes employed in our study. For all downstream tasks, the discrete tokens are initially passed through an embedding layer that is randomly initialized. The code for all experiments, training logs, and hyperparameters will be accessible once the review process has been completed.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Er & Asv:",
      "text": "For the classification tasks, we follow previous benchmarks  [21, 6]  and utilize ECAPA-TDNN, which combines convolutional and residual blocks. This system is trained using negative log-likelihood loss for ER and Additive Margin Softmax Loss for ASV.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Asr:",
      "text": "In the speech recognition task, we replicate a previously established benchmark  [21, 6] , utilizing a vanilla 2-layer BiL-STM with 1,024 units each, followed by a linear layer that maps audio to characters. The system is trained using the Connectionist Temporal Classification (CTC) loss at the character level.\n\nSpeech Synthesis: Following  [22] , we use the HiFi-GAN neural vocoder  [23]  to synthesize speech. HiFiGAN is a generative adversarial network (GAN) consisting of one generator and a set of discriminators. We adapted the generator architecture to take as input a sequence of discrete-unit.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech To Speech Translation",
      "text": "The following section describes the dataset and the speechto-unit translation (S2UT) model used to assess the choice of discrete speech units. We use the CVSS corpora to train and evaluate our speech-to-unit translation model. CVSS is a massively multilingual-to-English speech-to-speech translation corpus, covering pairs from 21 languages to English. However, only the French-to-English translation is considered in this study. The dataset includes two versions of spoken translation: CVSS-C and CVSS-T. While both versions can be utilized to train our system, we use CVSS-C because of its superior speech quality. Official training, development and testing splits are utilized for evaluation. We build the S2UT model by adapting the transformer encoder-decoder framework presented in  [24] . The encoder is composed of a Wav2Vec 2.0 base pre-trained on 3K hours of French speech 1  . As a decoder, we use 6 transformer layers with a random weight initialization.\n\nWe combined the Wav2Vec 2.0 encoder along with the transformer decoder and we finetune the whole model endto-end. During inference, the S2UT model's predictions are fed into a vocoder trained on discrete speech units for speech synthesis Recent research in speech-to-speech translation advocates for using BLEU scores to evaluate translation quality. First, we use a speech recognition model 2  to compute the transcriptions of the generated speech. Then, we compute the BLEU score for the ASR-decoded text in comparison to the reference translations. We acknowledge that the ASR BLEU score may be influenced by ASR model performance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "In the following section, we first discuss the results of the four downstream tasks independently. Next, we evaluate the translation quality of the retained encoders and k-means (k=number of discrete speech units) against a baseline setup reproduced from the literature. Finally, we discuss the correlation between downstream task and speech-to-speech translation performance. In the following tables, for Base model, we report scores from every second layer, and for Large model, from every fourth layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "From Table  1 , we can observe similar performance for HuBERT and Wav2Vec2. Both encoders start with relatively high accuracy in initial layers, indicating their capability to capture emotional content effectively at these stages. The best performance is observed with the HuBERT Base model using k=1024 and layer 2, achieving the highest accuracy of 66.1%. We denote a progressive decrease in performance as layers progress, this decrease is especially pronounced in the Wav2Vec2 Base model, where accuracy drops significantly from initial to subsequent layers, highlighting a potential issue in maintaining emotional content representation in deeper layers. For Multilingual encoders, Wav2Vec2 XLS-R generally achieves better performance across most layers. The decline in SAMU-XLSR performance across consecutive layers likely stems from its spe-cialization in encoding semantic information more effectively in the upper layers, albeit at the cost of less efficient encoding of emotional information. In addition, due to the presence of identical linguistic in the utterances for all emotional states in the dataset, the model struggles to effectively utilize semantic information for accurate label prediction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Speech Recognition",
      "text": "As expected for the Automatic Speech Recognition (ASR) task, Table  2  illustrates that discrete speech units generated by the high middle layers generally yield superior results in Wav2Vec 2.0 models. Regarding continuous speech representation,  [25]  has demonstrated that linguistic word-level information is better encoded in the high middle layers of these self-supervised learning (SSL) models. HuBERT exhibits a distinct behavior, as the discrete speech units generated in its deepest layers (excluding the last one) achieve optimal performance, consistent with observations for continuous speech representations presented in  [26] . Wav2Vec2 Base consistently outperforms HuBERT Base across all cluster sizes, achieving the lowest Character Error Rate (CER) of 1.38 at layer 10 with k = 1024. The multilingual HuBERT model demonstrates a decrease in performance compared to its monolingual counterpart, indicating potential challenges in handling diverse languages for ASR tasks. Notably, Wav2Vec2 XLS-R surpasses SAMU-XLSR across all layers, despite SAMU-XLSR being fine-tuned for text embedding prediction. Furthermore, we observe that the impact of cluster size is more pronounced in the larger models than in the base models, suggesting that larger models benefit from a higher number of clusters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Speaker Verification",
      "text": "From Table  3 , we denote that HuBERT Base model systematically outperforms the Wav2Vec2 Base across different layers and cluster sizes. The table also illustrates a notable increase in the EER for SAMU-XLSR at higher layers across all cluster sizes, significantly underperforming compared to Wav2Vec2 XLS-R. This trend suggests a loss of speaker-specific informa- tion in SAMU-XLSR's higher layers, which might be due to its focus on retaining semantic information. We observe a consistent pattern from the data, showing that models generally achieve better performance at lower to mid layers than at the highest layers for speaker verification tasks. The impact of cluster size on EER varies across models, but the general improvement in performance with increasing cluster size suggests that more granular speech unit representations can enhance ASV performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Synthesis",
      "text": "In Table  4 , we can see that the vocoder fed by the discrete speech units computed from HuBERT Base model outperforms the one fed by the discrete speech units generated by Wav2Vec2 Base model, particularly on layer 6 with a cluster size of 512, achieving a MOS score of 3.45. The highest score at layer 6 aligns with previous work's on speech-to-speech translation, highlighting the significance of this configuration for achieving high-quality speech synthesis. Among the multilingual models, Wav2Vec2-XLS-R outperforms both mHuBERT Base and SAMU XLSR Large with the highest MOS score of 3.80 on layer 20 with a cluster size of 512. This indicates a potential benefit of larger model and extensive training data, meanwhile mHuBERT base shows competitive performance, especially with a cluster size of 512. Finally, SAMU-XLSR Large demonstrates a significant decline in MOS scores at higher layers and for all cluster sizes. This drop can be attributed to the model's focus on semantic over acoustic information which is critical for a vocoder.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech To Speech Translation",
      "text": "In the following section, we evaluate a subset of previous configurations on the speech-to-speech translation task. To ensure comparable results with previous studies, we chose to include the HuBERT Base at layer 6 with a cluster size of 128 as the baseline. Additionally, to assess the effect of the number of clusters on the translation task, we include results at layer 6 with cluster sizes of 512 and 1024. To understand the impact between semantic and acoustic on the S2ST task, we selected both Wav2Vec2 XLS-R and SAMU-XLSR, choosing configurations that maximize performance on the ASR task (layer 16) and configurations that maximize performance on the speech synthesis task (layer 8).\n\nFrom Figure  1 , we can observe that BLEU scores improve as the cluster count increases from 128 to 1024, with a minor reduction when transitioning from 128 to 512 clusters. It is interesting considering that the configuration Hubert Base with layer 6 and 512 clusters achieved the highest Mean Opinion Score (MOS). This pattern indicates that a higher number of clusters generally enhances model performance. The most effective configuration tested is the Hubert Base model with 1024 clusters, which achieved the highest BLEU score of 20.14.\n\nLooking at the BLEU scores presented in Table  5 , the Wav2Vec2 XLS-R Large model consistently outperforms the SAMU-XLSR Large model across both layer configurations. This suggests that the fine-tuning of SAMU-XLSR Large on text embeddings may not effectively contribute to speech-tospeech translation tasks. Furthermore, the results on Wav2Vec2 XLS-R Large underscore the complexity in selecting the optimal layer for extracting discrete speech units. Relying solely on the Mean Opinion Score (MOS) for token selection does not appear to yield the best results, compared to adopting a balanced approach that considers both the Character Error Rate (CER) and MOS scores.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we have described various experiments to evaluate the robustness of discrete speech units in downstream tasks. The results obtained in textless speech-to-speech translation underscore the complexity in selecting encoders and the number of clusters. We hope this analysis will help the community in better understanding the extraction of discrete tokens. Looking forward, future research will explore the combination of multiple encoders and layers within the same task and extend the approach to additional language pairs, particularly those that are unwritten.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we can observe that BLEU scores improve",
      "page": 4
    },
    {
      "caption": "Figure 1: Effect of number of clusters BLEU scores.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Benchmarking results for the emotion recognition",
      "data": [
        {
          "Setting": "k=128\nk=512\nk=1024"
        },
        {
          "Setting": "k=128\nk=512\nk=1024"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: illustrates that discrete speech units generated by the",
      "data": [
        {
          "Setting": "k=128\nk=512\nk=1024"
        },
        {
          "Setting": "k=128\nk=512\nk=1024"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: , we can see that the vocoder fed by the discrete",
      "data": [
        {
          "k=128\nk=512\nk=1024": "k=128\nk=512\nk=1024"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Benchmarking results for the emotion recognition",
      "data": [
        {
          "Setting": "k=128\nk=512\nk=1024"
        },
        {
          "Setting": "k=128\nk=512\nk=1024"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Benchmarking results for the emotion recognition",
      "data": [
        {
          "k=128\nk=512\nk=1024": "k=128\nk=512\nk=1024"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: Benchmarking results for the speech synthesis task",
      "data": [
        {
          "Setting": "k=128\nk=512\nk=1024"
        },
        {
          "Setting": "k=128\nk=512\nk=1024"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: This table compares the BLEU scores of Wav2Vec2",
      "data": [
        {
          "8": "16",
          "1024": "1024"
        },
        {
          "8": "8",
          "1024": "1024"
        },
        {
          "8": "16",
          "1024": "1024"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Janus-iii: speech-to-speech translation in multiple languages",
      "authors": [
        "A Lavie",
        "A Waibel",
        "L Levin",
        "M Finke",
        "D Gates",
        "M Gavalda",
        "T Zeppenfeld",
        "Z Puming"
      ],
      "year": "1997",
      "venue": "1997 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "The atr multilingual speech-to-speech translation system",
      "authors": [
        "S Nakamura",
        "K Markov",
        "H Nakaiwa",
        "G Kikui",
        "H Kawai",
        "T Jitsuhiro",
        "J.-S Zhang",
        "H Yamamoto",
        "E Sumita",
        "S Yamamoto"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Direct speech-tospeech translation with discrete units",
      "authors": [
        "A Lee",
        "P Chen",
        "C Wang",
        "J Gu",
        "S Popuri",
        "X Ma",
        "A Polyak",
        "Y Adi",
        "Q He",
        "Y Tang",
        "J Pino",
        "W Hsu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Enhancing expressivity transfer in textless speech-to-speech translation",
      "authors": [
        "J Duret",
        "B O'brien",
        "Y Estève"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "6",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2021.3122291"
    },
    {
      "citation_id": "7",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S Yang",
        "P Chi",
        "Y Chuang"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "8",
      "title": "Fisher and callhome spanish-english speech translation",
      "authors": [
        "M Post",
        "G Kumar",
        "A Lopez"
      ],
      "year": "2014",
      "venue": "LDC2014T23. Web Download. Philadelphia: Linguistic Data Consortium"
    },
    {
      "citation_id": "9",
      "title": "Covost 2 and massively multilingual speech translation",
      "authors": [
        "C Wang",
        "A Wu",
        "J Gu",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "10",
      "title": "Cvss corpus and massively multilingual speech-to-speech translation",
      "authors": [
        "Y Jia",
        "M Ramanovich",
        "Q Wang",
        "H Zen"
      ],
      "year": "2022",
      "venue": "Cvss corpus and massively multilingual speech-to-speech translation",
      "arxiv": "arXiv:2201.03713"
    },
    {
      "citation_id": "11",
      "title": "Speechmatrix: A largescale mined corpus of multilingual speech-to-speech translations",
      "authors": [
        "P Duquenne",
        "H Gong",
        "N Dong"
      ],
      "year": "2022",
      "venue": "Speechmatrix: A largescale mined corpus of multilingual speech-to-speech translations",
      "arxiv": "arXiv:2211.04508"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition"
    },
    {
      "citation_id": "13",
      "title": "Samu-xlsr: Semanticallyaligned multimodal utterance-level cross-lingual speech representation",
      "authors": [
        "S Khurana",
        "A Laurent",
        "J Glass"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing",
      "doi": "10.1109/JSTSP.2022.3192714"
    },
    {
      "citation_id": "14",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale"
    },
    {
      "citation_id": "15",
      "title": "Language-agnostic bert sentence embedding",
      "authors": [
        "F Feng",
        "Y Yang",
        "D Cer",
        "N Arivazhagan",
        "W Wang"
      ],
      "year": "2022",
      "venue": "Language-agnostic bert sentence embedding"
    },
    {
      "citation_id": "16",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "17",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "Emotional voice conversion: Theory, databases and esd"
    },
    {
      "citation_id": "18",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: a large-scale speaker identification dataset",
      "arxiv": "arXiv:1706.08612"
    },
    {
      "citation_id": "20",
      "title": "The lj speech dataset",
      "authors": [
        "I Keith",
        "J Linda"
      ],
      "year": "2017",
      "venue": "The lj speech dataset"
    },
    {
      "citation_id": "21",
      "title": "Utmos: Utokyosarulab system for voicemos challenge 2022",
      "authors": [
        "T Saeki",
        "D Xin",
        "W Nakata",
        "T Koriyama"
      ],
      "year": "2022",
      "venue": "Utmos: Utokyosarulab system for voicemos challenge 2022",
      "arxiv": "arXiv:2204.02152"
    },
    {
      "citation_id": "22",
      "title": "Speech self-supervised representation benchmarking: Are we doing it right?",
      "authors": [
        "S Zaiem",
        "Y Kemiche",
        "T Parcollet"
      ],
      "year": "2023",
      "venue": "Speech self-supervised representation benchmarking: Are we doing it right?",
      "arxiv": "arXiv:2306.00452"
    },
    {
      "citation_id": "23",
      "title": "Speech resynthesis from discrete disentangled self-supervised representations",
      "authors": [
        "A Polyak",
        "Y Adi",
        "J Copet",
        "E Kharitonov",
        "K Lakhotia",
        "W Hsu",
        "A Mohamed",
        "E Dupoux"
      ],
      "venue": "Speech resynthesis from discrete disentangled self-supervised representations"
    },
    {
      "citation_id": "24",
      "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong",
        "J Kim",
        "J Bae"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Enhanced direct speech-tospeech translation using self-supervised pre-training and data augmentation",
      "authors": [
        "S Popuri",
        "P Chen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Enhanced direct speech-tospeech translation using self-supervised pre-training and data augmentation",
      "arxiv": "arXiv:2204.02967"
    },
    {
      "citation_id": "26",
      "title": "Layer-wise analysis of a self-supervised speech representation model",
      "authors": [
        "A Pasad",
        "J.-C Chou",
        "K Livescu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "27",
      "title": "Comparative layer-wise analysis of self-supervised speech models",
      "authors": [
        "A Pasad",
        "B Shi",
        "K Livescu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}