{
  "paper_id": "2401.08166v1",
  "title": "Ed-Tts: Multi-Scale Emotion Modeling Using Cross-Domain Emotion Diarization For Emotional Speech Synthesis",
  "published": "2024-01-16T07:13:16Z",
  "authors": [
    "Haobin Tang",
    "Xulong Zhang",
    "Ning Cheng",
    "Jing Xiao",
    "Jianzong Wang"
  ],
  "keywords": [
    "emotional speech synthesis",
    "speech emotion diarization",
    "diffusion denoising probabilistic model ‚Ä† Equal Contribution"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing emotional speech synthesis methods often utilize an utterance-level style embedding extracted from reference audio, neglecting the inherent multi-scale property of speech prosody. We introduce ED-TTS, a multi-scale emotional speech synthesis model that leverages Speech Emotion Diarization (SED) and Speech Emotion Recognition (SER) to model emotions at different levels. Specifically, our proposed approach integrates the utterance-level emotion embedding extracted by SER with fine-grained frame-level emotion embedding obtained from SED. These embeddings are used to condition the reverse process of the denoising diffusion probabilistic model (DDPM). Additionally, we employ crossdomain SED to accurately predict soft labels, addressing the challenge of a scarcity of fine-grained emotion-annotated datasets for supervising emotional TTS training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent researches have shown significant progress in emotional text-to-speech (TTS) thanks to the denoising diffusion probabilistic models (DDPM)  [1, 2] . EmoDiff  [3]  uses DDPM with classifier guidance  [4]  to synthesize controllable and mixed emotion. However, such label guidance will lead to low diversity without manual control and is hard to extend to unseen emotions. EmoMix  [5]  uses a pre-trained speech emotion recognition (SER) model that extracts high dimensional emotion embedding from reference audio to condition the reverse process of DDPM. Such reference-based emotional TTS methods can generate more diverse emotion expression compared to label-based approaches. But the widely used utterance level style embedding fail to capture the multi-scale features of speech style, which span from coarse to fine. Some fine-grained prosodic expressions, like intonation, are studied. QI-TTS  [6]  uses a multi-style extractor where the final syllable level style indicates intonation, while the sentence level depicts emotion. But sentence level emotion representation can not accurately locating emotion boundaries and fine-grained variations of emotions in the synthesized speech. It's observed that occasionally, a speaker will stress certain segments of their speech, which makes the emotion more apparent. In that case, the rest of the sentence may sound very neutral. So, we should view emotions expressed in speech as varying speech events that have clear temporal boundaries, instead of the characteristics of the whole speech.\n\nSpeech emotion diarization (SED)  [7]  is a fine-grained speech emotion recognition task that aims to simultaneously identify the correct emotions and their corresponding boundaries following \"Which emotion appears when?\". To effectively capture the nuances of speech emotion and their boundary, we introduce ED-TTS. This multi-scale approach allows for the modeling of emotions at various levels. ED-TTS is a sequence-to-sequence architecture based on DDPM, with pretrained SER and SED models that can extract utterance-level and frame-level emotional features. Furthermore, we employ SED to address the challenge posed by the scarcity of finely annotated datasets for emotions in emotional TTS. We use the fine-grained soft emotion label predicted by SED on unlabeled TTS dataset to supervise TTS model training. Inspired by Cai et al.  [8] , we use cross-domain training to improve the soft label accuracy on TTS datasets by reducing the distribution shift of SED and TTS datasets. The main advantages of ED-TTS are:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "ùíÅ C",
      "text": "Fig.  1 : The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level emotion labels by SED (e.g. red for non-neutral and blue for neutral). Extracter denotes CNN-based feature encoder of SED.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "ED-TTS is based on the design of GradTTS  [9] , while the multi-scale style encoder use SER as utterance-level extracter and an additional pre-trained SED model for accurately modeling fine-grained emotion feature and their boundaries. We use the extracted multi-scale style embedding to condition the reverse process of DDPM. Furthermore, we employ frame-level soft emotion labels predicted by pre-trained cross-domain SED model on TTS dataset to supervise the TTS model training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preliminary On Score-Based Diffusion Model",
      "text": "ED-TTS follows GradTTS  [9]  to apply score-based diffusion model  [2]  which uses stochastic differential equation (SDE) to TTS. Specifically, it defines a diffusion process which converts any data distribution X 0 to terminal distribution X T :\n\nwhere Œ≤ t denotes pre-defined noise schedule and W t is the Wiener process. Above SDE has a corresponding reverse SDE that follows the diffusion process's reverse trajectory. By solving a discretized version of the reverse time SDE, an ordinary differential equation, GradTTS can generate data X 0 from terminal distribution X T as follows:\n\nwhere t ‚àà 1 N , 2 N , . . . , 1 and N denotes the number of discretized reverse process steps. z t is sampled from standard Gaussian noise. But there is an intractable score ‚àá Xt log p t (X t ). We can get X t given X 0 from the distribution derived from Eq. (1) as\n\nwhere œÅ (X 0 , t) and Œª(t) have closed form. So that the score\n\nwhere œµ t is the Gaussian noise. To estimate the score a neural network œµ Œ∏ (X t , ¬µ, t, Z s ) is trained using\n\nwhere ¬µ is style and text related Gaussian mean.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Scale Style Encoder",
      "text": "As shown in the yellow part of Fig.  1 , we extend the emotion encoder of EmoMix  [5]  which contains single SER to multi-scale, to extract the emotion category, emotion variation and emotion boundary information. This module contains the pre-trained SER for utterance-level style features with an additional SED model for frame-level style features. We follow the SER  [10]  to extract a fixed size embedding from reference speech's mel-spectrogram and its delta, delta-delta coefficients. The SED  [7]  employs pre-trained WavLM  [11] , a modern self-supervised model, followed by a linear classifier. WavLM includes a CNN-based feature encoder followed by transformer blocks and is fine-tuned on the downstream frame-wise SED task. We use the transformer output as our frame-level style embedding. For speaker conditioning we use resemblyzer  [12]  as our speaker encoder.\n\nA challenge in fine-grained style conditioning is the alignment of variable-length frame-level prosodic features with input text representations  [13] . The traditional approach in emotional speech synthesis directly adds style embeddings to text embedding. To align the style representations with the phonetic representations Z c , we adopt the multi-head attention block which aims to reweight content according to the given style learning the alignment between the two modalities. The phoneme representations Z c processed by text encoder is used as query while frame-level style representation is key and value. After content-style alignment, the aligned representation is added with utterance-level style embedding and speaker embedding to form multi-scale style embedding Z s . Then Z s is fed to duration predictor and denoiser to condition the duration modeling and reverse DDPM process.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Domain Training Of Sed",
      "text": "To minimize the emotion style gap and boundary offsets between reference and synthesized speech. We use SED to predict frame-level soft emotion labels of the unlabelled TTS dataset to supervise ED-TTS training. We train ED-TTS with an additional cross entropy loss which force the synthesized sample have the same frame-level emotion as reference speech. Since SED is pre-trained on a curated SED dataset which significantly differs from the TTS dataset, we employ domain adaptation techniques to minimize the distribution shift of different datasets.\n\nKernel-based metric, maximum mean miscrepancy [14] (MMD), is used to determine the equivalence of two distributions. It has been widely used in domain adaptation tasks and has been validated useful for cross-domain SER within the context of emotional TTS  [8] . Specifically, with the source data S = {S 1 , S 2 , . . . , S ns } and target data\n\nwhere œï(‚Ä¢) denote a map from data to reproducing kernel hilbert space (RKHS) and k means the Gaussian kernel function. We divided the source domain and the target domain into different subdomains according to emotion categories to adopt local MMD (LMMD)  [15]  for each subdomains. Moreover, we extend LMMD to multi-layer LMMD (MLMMD) which is adopted not only to bottleneck layer but also other CNN layers of the feature encoder part to achieve a more suitable shared feature space. In that case, MLMMD can be expressed as:\n\nwhere L denotes the count of CNN layers in the SED feature encoder. C is the number of emotion categories. The emotion categories in the source and target domain is mixed or unknown. So we use classification probabilities W C Si and W C Tj obtained from the pre-trained SER to represent unknown or mixed emotion categories  [5]  in the source domain and the target domain respectively. The training of cross-domain SED can be regard as fine-tuning a WavLM model on the down stream SED task and the total loss function for training is:\n\nwhere Œª is the weight of MLMMD loss.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "The SER model is pre-trained on a subset of IEMOCAP  [16]  which contains happy, sad, angry, and neutral emotions.\n\nThe SED model is pre-trained on a curated data  [7]  which contain randomly concatenated audio samples from IEMO-CAP  [16] , RAVDESS  [17] , Emov-DB  [18] , ESD  [19] , and JL CORPUS  [20] . We test the cross-domain performance of MLMMD on cross-domain SED tasks on another dataset: Zaion Emotion Dataset (ZED)  [7]  which has 180 utterances and 73 speakers derived from emotional YouTube videos. ZED provides discrete emotion labels and emotional segment boundaries for each sample. We use a segmented part of BC2013-English audiobook dataset  [21] , which has about 70 hours and 93k utterances, to train and evaluate ED-TTS. This dataset is read by a single female speaker with expressive style, but without annotations, which fits our task.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments Setting",
      "text": "We train the cross-domain SED model with Adam optimizer under 64 batch size and 10 -5 learning rate setting. The score estimation network œµ Œ∏ composed of U-Net and linear attention modules, mirroring those found in GradTTS. The training of ED-TTS is conducted with 32 batch size and Adam optimizer under a 10 -4 learning rate for a total of 1 million steps. To train the duration predictor, we extract speech-text alignment using Montreal Forced Aligner (MFA)  [22] . For the subsequent experiments, Hifi-GAN  [23]  is utilized as the vocoder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Domain Sed Results",
      "text": "To assess the performance of MLMMD in cross-domain SED tasks, we perform experiments on the ZED dataset, measuring the Emotion Diarization Error Rate (EDER) as defined in  [7] . EDER metric is specifically designed to accurately assess the temporal alignment between predicted emotion intervals and the actual emotion intervals. We train five models which share the same model structure but use different domain adaptation loss. The weight Œª in Eq.(  6 ) is set to 0.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Speech Evaluation",
      "text": "To assess the quality of synthesized speech samples, we compare them with the baseline models:\n\n1. GT and GT (voc.): speech samples from test set and reconstructed speeches usinig vocoder and ground truth mel-spectrogram.\n\n2. FG-TTS  [13] : The fine-grained style modeling method based on Transformer TTS.\n\n3. EmoMix  [5] : A controllable emotional TTS using emotion embedding extracted by a pre-trained SER to condition DDPM.\n\nIn subjective evaluation, 25 assessors are tasked with rating 20 speech samples per emotion. They judge the speech quality using the mean opinion score (MOS) and the emotion similarity using the similarity MOS (SMOS), on a scale of 1 to 5. In objective evaluation, Emotion Reclassification Accuracy (ERA) is used to measure how the synthesized speech fit the frame-level emotion labels of reference speech predicted by SED. Specifically,we reused our pre-trained SED to reclassify the synthesized audio clips and calculated the reclassification accuracy. Table  2  demonstrates that the vocoder's impact is minimal. ED-TTS outperforms the baseline models in terms of SMOS and ERA by a considerable margin while maintaining MOS scores. These findings highlight ED-TTS's advantage over the baseline models, attributed to its use of multi-scale emotion modeling and cross-domain training techniques.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We propose ED-TTS, a text-to-speech model towards multiscale style transfer for emotional TTS. We design several techniques to learn the fine-grained emotion variations in speech: 1) ED-TTS employs a multi-scale style encoder to capture and transfer diverse style attributes, encompassing speaker and utterance-level emotional characteristics, as well as nuanced frame-level prosodic representations; 2) ED-TTS uses SED to predict frame-level labels as an auxiliary supervision for TTS model training. Cross-domain training is adopted to SED model for improving the soft emotion label accuracy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgement",
      "text": "Supported by the Key Research and Development Program of Guangdong Province (grant No. 2021B0101400003) and corresponding author is Ning Cheng.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level",
      "page": 2
    },
    {
      "caption": "Figure 1: , we extend the emo-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "2University of Science and Technology of China"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "emotion representation can not accurately locating emotion"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "boundaries\nand fine-grained variations of\nemotions\nin the"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "synthesized speech. It‚Äôs observed that occasionally, a speaker"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "will stress certain segments of their speech, which makes the"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "emotion more apparent.\nIn that case,\nthe rest of the sentence"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "may sound very neutral.\nSo, we should view emotions ex-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "pressed in speech as varying speech events\nthat have clear"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "temporal\nboundaries,\ninstead\nof\nthe\ncharacteristics\nof\nthe"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "whole speech."
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "Speech emotion diarization (SED)\n[7]\nis a fine-grained"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "speech emotion recognition task that aims to simultaneously"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "identify the correct emotions and their corresponding bound-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "aries following ‚ÄúWhich emotion appears when?‚Äù. To effec-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "tively capture the nuances of speech emotion and their bound-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "ary, we introduce ED-TTS. This multi-scale approach allows"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "for the modeling of emotions at various levels. ED-TTS is a"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "sequence-to-sequence architecture based on DDPM, with pre-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "trained SER and SED models that can extract utterance-level"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "and frame-level emotional features. Furthermore, we employ"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "SED to address the challenge posed by the scarcity of finely"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "annotated datasets\nfor emotions\nin emotional TTS. We use"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "the fine-grained soft emotion label predicted by SED on unla-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "beled TTS dataset to supervise TTS model training.\nInspired"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "by Cai et al. [8], we use cross-domain training to improve the"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "soft label accuracy on TTS datasets by reducing the distribu-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "tion shift of SED and TTS datasets. The main advantages of"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "ED-TTS are:"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "1. ED-TTS is a multi-scale emotional\nspeech synthesis"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "model built on DDPM. It includes two pre-trained com-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "ponents:\nthe utterance-level SER and the frame-level"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "SED. These are designed to identify the category of"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "emotion at\nthe utterance level,\nand the variation and"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "boundaries of emotion at the frame level, respectively."
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "2. ED-TTS further\nutilizes\nthe SED model\nto\npredict"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "frame-level soft emotion labels to supervise TTS model"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "training. Cross-domain training is adopted for improv-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "ing the performance of SED on TTS dataset."
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "3. The results from both subjective and objective evalua-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "tion indicate that ED-TTS outshines the baseline mod-"
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": ""
        },
        {
          "1Ping An Technology (Shenzhen) Co., Ltd., China": "els in terms of audio quality and expressiveness."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "where œÅ (X0, t) and Œª(t) have closed form. So that the score"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "| X0) = ‚àíŒª(t)‚àí1œµt, where œµt is the Gaussian\n‚àáXt log pt (Xt"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "noise. To estimate the score a neural network œµŒ∏(Xt, ¬µ, t, Zs)"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "is trained using"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "(3)\nLdif f = Ex0,t,Zs,œµt[||œµŒ∏(Xt, ¬µ, t, Zs) + Œª(t)‚àí1œµt||2\n2]"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "where ¬µ is style and text related Gaussian mean."
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "2.2. Multi-Scale Style Encoder"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "As shown in the yellow part of Fig. 1, we extend the emo-"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "tion encoder of EmoMix [5] which contains single SER to"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "multi-scale, to extract the emotion category, emotion variation"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "and emotion boundary information. This module contains the"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "pre-trained SER for utterance-level style features with an ad-"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "ditional SED model for frame-level style features. We follow"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "the SER [10]\nto extract a fixed size embedding from refer-"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "ence speech‚Äôs mel-spectrogram and its delta, delta-delta co-"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "efficients.\nThe SED [7] employs pre-trained WavLM [11],"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "a modern self-supervised model, followed by a linear classi-"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "fier. WavLM includes a CNN-based feature encoder followed"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "by transformer blocks and is fine-tuned on the downstream"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "frame-wise SED task. We use the transformer output as our"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "frame-level\nstyle embedding.\nFor\nspeaker conditioning we"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "use resemblyzer [12] as our speaker encoder."
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "A challenge in fine-grained style conditioning is the align-"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "ment of variable-length frame-level prosodic features with"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "input\ntext\nrepresentations [13].\nThe traditional approach in"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "emotional\nspeech synthesis directly adds\nstyle embeddings"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": ""
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "to text embedding.\nTo align the style representations with"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "the phonetic\nadopt\nthe multi-head\nrepresentations Zc, we"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "attention block which aims\nto reweight\ncontent\naccording"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "to the given style learning the alignment between the two"
        },
        {
          "Fig. 1: The overview of ED-TTS and cross-domain training for SED. The color in waveforms denotes the predicted frame-level": "modalities.\nThe phoneme representations Zc processed by"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "text encoder\nis used as query while frame-level style repre-": "sentation is key and value.\nAfter content-style alignment,",
          "where L denotes the count of CNN layers in the SED fea-": "ture encoder. C is the number of emotion categories.\nThe"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "the aligned representation is added with utterance-level style",
          "where L denotes the count of CNN layers in the SED fea-": "emotion categories in the source and target domain is mixed"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "embedding and speaker embedding to form multi-scale style",
          "where L denotes the count of CNN layers in the SED fea-": "or unknown. So we use classification probabilities W C\nand"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "Si"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "embedding Zs. Then Zs is fed to duration predictor and de-",
          "where L denotes the count of CNN layers in the SED fea-": "W C\nobtained from the pre-trained SER to represent unknown"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "Tj"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "noiser to condition the duration modeling and reverse DDPM",
          "where L denotes the count of CNN layers in the SED fea-": "or mixed emotion categories [5] in the source domain and the"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "process.",
          "where L denotes the count of CNN layers in the SED fea-": "target domain respectively. The training of cross-domain SED"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "can be regard as fine-tuning a WavLM model on the down"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "stream SED task and the total loss function for training is:"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "2.3. Cross-domain Training of SED",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "(6)\nL = LCE + ŒªLM LMM D"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "To minimize the emotion style gap and boundary offsets be-",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "tween reference and synthesized speech. We use SED to",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "where Œª is the weight of MLMMD loss."
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "predict frame-level soft emotion labels of the unlabelled TTS",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "dataset\nto supervise ED-TTS training.\nWe\ntrain ED-TTS",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "3. EXPERIMENTS"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "with an additional cross entropy loss which force the synthe-",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "sized sample have the same frame-level emotion as reference",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "3.1. Dataset"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "speech.\nSince SED is pre-trained on a curated SED dataset",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "which significantly differs from the TTS dataset, we employ",
          "where L denotes the count of CNN layers in the SED fea-": "The SER model is pre-trained on a subset of IEMOCAP [16]"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "domain adaptation techniques\nto minimize the distribution",
          "where L denotes the count of CNN layers in the SED fea-": "which\ncontains\nhappy,\nsad,\nangry,\nand\nneutral\nemotions."
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "shift of different datasets.",
          "where L denotes the count of CNN layers in the SED fea-": "The SED model\nis pre-trained on a curated data [7] which"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "Kernel-based metric, maximum mean miscrepancy [14]",
          "where L denotes the count of CNN layers in the SED fea-": "contain randomly concatenated audio samples from IEMO-"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "(MMD),\nis used to determine\nthe\nequivalence of\ntwo dis-",
          "where L denotes the count of CNN layers in the SED fea-": "CAP [16], RAVDESS [17], Emov-DB [18], ESD [19], and"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "tributions.\nIt has been widely used in domain adaptation",
          "where L denotes the count of CNN layers in the SED fea-": "JL CORPUS [20]. We test\nthe cross-domain performance"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "tasks and has been validated useful\nfor cross-domain SER",
          "where L denotes the count of CNN layers in the SED fea-": "of MLMMD on cross-domain SED tasks on another dataset:"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "within the context of emotional TTS [8].\nSpecifically, with",
          "where L denotes the count of CNN layers in the SED fea-": "Zaion Emotion Dataset (ZED) [7] which has 180 utterances"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "=\nthe\nsource\ndata S\nand\ntarget\ndata\n{S1, S2, . . . , Sns }",
          "where L denotes the count of CNN layers in the SED fea-": "and 73 speakers derived from emotional YouTube videos."
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "T = {T1, T2, . . . , Tnt} the definition of MMD is",
          "where L denotes the count of CNN layers in the SED fea-": "ZED provides discrete emotion labels and emotional segment"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "boundaries\nfor each sample. We use a segmented part of"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "2",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "BC2013-English audiobook dataset [21], which has about 70"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "ns(cid:88)\nnt(cid:88)",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\n1 n\n1 n\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\nMMD2 (S, T ) =\nœï (Si) ‚àí\nœï (Tj)",
          "where L denotes the count of CNN layers in the SED fea-": "hours and 93k utterances, to train and evaluate ED-TTS. This"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "s\nt",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "i=1\nj=1",
          "where L denotes the count of CNN layers in the SED fea-": "dataset\nis\nread by a single female speaker with expressive"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "H",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "style, but without annotations, which fits our task."
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "nt(cid:88)\nnt(cid:88)\nns(cid:88)\nns(cid:88)",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "1 n\n1 n\n2t\n=\nk (Ti, Tj)\nk (Si, Sj) +",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "2s",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "i=1\nj=1\ni=1\nj=1",
          "where L denotes the count of CNN layers in the SED fea-": "3.2. Experiments Setting"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "2\nns(cid:88)\nnt(cid:88)",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "We train the cross-domain SED model with Adam optimizer"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "‚àí\n(4)\nk (Si, Tj)",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "nsnt",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "i=1\nj=1",
          "where L denotes the count of CNN layers in the SED fea-": "under 64 batch size and 10‚àí5 learning rate setting. The score"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "estimation network œµŒ∏ composed of U-Net and linear attention"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "where œï(¬∑) denote a map from data to reproducing kernel",
          "where L denotes the count of CNN layers in the SED fea-": "modules, mirroring those found in GradTTS. The training of"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "hilbert space (RKHS) and k means the Gaussian kernel func-",
          "where L denotes the count of CNN layers in the SED fea-": "ED-TTS is conducted with 32 batch size and Adam optimizer"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "tion. We divided the source domain and the target domain",
          "where L denotes the count of CNN layers in the SED fea-": "under a 10‚àí4 learning rate for a total of 1 million steps. To"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "into different subdomains according to emotion categories to",
          "where L denotes the count of CNN layers in the SED fea-": "train the duration predictor, we extract speech-text alignment"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "adopt local MMD (LMMD) [15] for each subdomains. More-",
          "where L denotes the count of CNN layers in the SED fea-": "using Montreal Forced Aligner\n(MFA)\n[22].\nFor\nthe subse-"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "over, we extend LMMD to multi-layer LMMD (MLMMD)",
          "where L denotes the count of CNN layers in the SED fea-": "quent experiments, Hifi-GAN [23] is utilized as the vocoder."
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "which is adopted not only to bottleneck layer but also other",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "CNN layers of\nthe feature encoder part\nto achieve a more",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "3.3. Cross-domain SED Results"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "suitable shared feature space.\nIn that case, MLMMD can be",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "To assess the performance of MLMMD in cross-domain SED"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "expressed as:",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "tasks, we perform experiments on the ZED dataset, measuring"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "MLMMD2 (S, T )",
          "where L denotes the count of CNN layers in the SED fea-": "the Emotion Diarization Error Rate (EDER) as defined in [7]."
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "2",
          "where L denotes the count of CNN layers in the SED fea-": "EDER metric is specifically designed to accurately assess the"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "1\n(cid:88)\n(cid:88)",
          "where L denotes the count of CNN layers in the SED fea-": "temporal alignment between predicted emotion intervals and"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "L(cid:88) L\nC(cid:88) C\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)\nW C\nW C\n=\nœï (Tj)\nœï (Si) ‚àí",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "Tj\nSi",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "L ¬∑ C",
          "where L denotes the count of CNN layers in the SED fea-": "the actual emotion intervals. We train five models which share"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "=1\n=1\nTj ‚ààDt\nSi‚ààDs",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "H",
          "where L denotes the count of CNN layers in the SED fea-": "the same model\nstructure but use different domain adapta-"
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "(5)",
          "where L denotes the count of CNN layers in the SED fea-": ""
        },
        {
          "text encoder\nis used as query while frame-level style repre-": "",
          "where L denotes the count of CNN layers in the SED fea-": "tion loss.\nThe weight Œª in Eq.(6)\nis set\nto 0.5. According"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Comparative MOS (CMOS)",
      "data": [
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": ""
        },
        {
          "To evaluate the impact of": "cluding SED utilization,",
          "techniques used in ED-TTS,\nin-": "frame-level\nsoft\nlabel\nsupervision,"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "and cross-domain training, we conduct ablation studies and"
        },
        {
          "To evaluate the impact of": "present",
          "techniques used in ED-TTS,\nin-": "the findings in Table 3. Comparative MOS (CMOS)"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": ""
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "and comparative ERA (CERA) are utilized to assess the qual-"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": ""
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "ity and expressiveness of the generated speech. ED-TTS (w/o"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": ""
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "SED) denotes the ED-TTS model conditioned on single-scale"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "style embedding extracted by SER. The decrease in both qual-"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "ity and reclassification scores shows the significance of mod-"
        },
        {
          "To evaluate the impact of": "",
          "techniques used in ED-TTS,\nin-": "eling emotion style representation at a fine-grained level. Fur-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Comparative MOS (CMOS)",
      "data": [
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "mance enhancement of 3.5% over\nthe SED-base model and",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "Model\nMOS ‚Üë\nSMOS ‚Üë\nERA ‚Üë"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "1.7% over the SED-MMD model. This indicates that reduc-",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "4.47 ¬± 0.08\n4.43 ¬± 0.08\n‚àí\nGT"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "ing the distributional gap between two domains enhances the",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "4.40 ¬± 0.10\n4.38 ¬± 0.08\n0.931\nGT (voc.)"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "cross-domain SED performance. Extending MMD to Multi-",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "3.94 ¬± 0.12\n3.92 ¬± 0.10\n0.679\nFG-TTS [13]"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "layer Local MMD (MLMMD) contributes to creating a more",
          "Table 2: Evaluation results for emotion synthesis.": "4.10 ¬± 0.10\n4.02 ¬± 0.08\n0.623\nEmoMix [5]"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "suitable shared feature space for both the source and target",
          "Table 2: Evaluation results for emotion synthesis.": "4.12 ¬± 0.08\n4.10 ¬± 0.12\n0.749\nED-TTS"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "datasets. Moreover, we present\nthe EDER results of Multi-",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "Layer MMD (MMMD) and Local MMD (LMMD) as abla-",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "tion study for cross-domain SED model.",
          "Table 2: Evaluation results for emotion synthesis.": "3.5. Ablation Study"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "To evaluate the impact of\ntechniques used in ED-TTS,\nin-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "Table 1: Emotion Diarization Error Rate (EDER) results for",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "cluding SED utilization,\nframe-level\nsoft\nlabel\nsupervision,"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "cross-domain SED.",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "and cross-domain training, we conduct ablation studies and"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "present\nthe findings in Table 3. Comparative MOS (CMOS)"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "Model\nEDER ‚Üì",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "and comparative ERA (CERA) are utilized to assess the qual-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "31.3\nSED",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "ity and expressiveness of the generated speech. ED-TTS (w/o"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "29.5\nSED-MMD",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "28.2\nSED-MMMD",
          "Table 2: Evaluation results for emotion synthesis.": "SED) denotes the ED-TTS model conditioned on single-scale"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "28.6\nSED-LMMD",
          "Table 2: Evaluation results for emotion synthesis.": "style embedding extracted by SER. The decrease in both qual-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "27.8\nSED-MLMMD",
          "Table 2: Evaluation results for emotion synthesis.": "ity and reclassification scores shows the significance of mod-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "eling emotion style representation at a fine-grained level. Fur-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "thermore,\nthe absence of\nsoft\nlabel\nsupervision and cross-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "domain training results in a noticeable decline in CERA, indi-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "3.4. Emotional Speech Evaluation",
          "Table 2: Evaluation results for emotion synthesis.": "cating that accurate soft\nlabel supervision play a crucial role"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "in guiding ED-TTS to synthesize the correct fine-grained tar-"
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "To assess the quality of synthesized speech samples, we com-",
          "Table 2: Evaluation results for emotion synthesis.": ""
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "",
          "Table 2: Evaluation results for emotion synthesis.": "get emotion."
        },
        {
          "to Table 1,\nthe SED-MLMMD model demonstrates a perfor-": "pare them with the baseline models:",
          "Table 2: Evaluation results for emotion synthesis.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Comparative MOS (CMOS)",
      "data": [
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": "pare them with the baseline models:"
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": "1. GT and GT (voc.):"
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": "mel-spectrogram."
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": "based on Transformer TTS."
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        },
        {
          "To assess the quality of synthesized speech samples, we com-": "dition DDPM."
        },
        {
          "To assess the quality of synthesized speech samples, we com-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "Moreno, ‚ÄúGeneralized end-to-end loss for speaker veri-"
        },
        {
          "6. REFERENCES": "[1]\nJonathan Ho, Ajay Jain, and Pieter Abbeel,\n‚ÄúDenois-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "fication,‚Äù in ICASSP, 2018, pp. 4879‚Äì4883."
        },
        {
          "6. REFERENCES": "ing diffusion probabilistic models,‚Äù\nin NeurIPS, 2020,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[13] Li-Wei Chen and Alexander Rudnicky,\n‚ÄúFine-grained"
        },
        {
          "6. REFERENCES": "vol. 33, pp. 6840‚Äì6851.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "style control in transformer-based text-to-speech synthe-"
        },
        {
          "6. REFERENCES": "[2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "sis,‚Äù in ICASSP, 2022, pp. 7907‚Äì7911."
        },
        {
          "6. REFERENCES": "Abhishek Kumar,\nStefano\nErmon,\nand Ben\nPoole,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[14] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,"
        },
        {
          "6. REFERENCES": "‚ÄúScore-based\ngenerative modeling\nthrough\nstochastic",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "Bernhard Sch¬®olkopf, and Alexander J. Smola, ‚ÄúA kernel"
        },
        {
          "6. REFERENCES": "differential equations,‚Äù in ICLR, 2021.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "two-sample test,‚Äù J. Mach. Learn. Res., vol. 13, pp. 723‚Äì"
        },
        {
          "6. REFERENCES": "[3] Yiwei Guo, Chenpeng Du, Xie Chen,\nand Kai Yu,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "773, 2012."
        },
        {
          "6. REFERENCES": "‚ÄúEmodiff:\nIntensity\ncontrollable\nemotional\ntext-to-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[15] Zhao Huijuan, YE Ning,\nand Wang Ruchuan,\n‚ÄúIm-"
        },
        {
          "6. REFERENCES": "speech with soft-label guidance,‚Äù in ICASSP, 2023, pp.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "proved cross-corpus speech emotion recognition using"
        },
        {
          "6. REFERENCES": "1‚Äì5.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "deep local domain adaptation,‚Äù Chinese Journal of Elec-"
        },
        {
          "6. REFERENCES": "[4] Prafulla Dhariwal and Alexander Quinn Nichol, ‚ÄúDiffu-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "tronics, vol. 32, no. 3, pp. 1‚Äì7, 2023."
        },
        {
          "6. REFERENCES": "sion models beat gans on image synthesis,‚Äù in NeurIPS,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[16] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe"
        },
        {
          "6. REFERENCES": "2021, vol. 34, pp. 8780‚Äì8794.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N."
        },
        {
          "6. REFERENCES": "[5] Haobin Tang, Xulong Zhang,\nJianzong Wang, Ning",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "Chang,\nSungbok Lee,\nand Shrikanth S. Narayanan,"
        },
        {
          "6. REFERENCES": "Cheng, and Jing Xiao,\n‚ÄúEmomix: Emotion mixing via",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "‚ÄúIEMOCAP:\ninteractive emotional dyadic motion cap-"
        },
        {
          "6. REFERENCES": "diffusion models for emotional speech synthesis,‚Äù in In-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "ture database,‚Äù Lang. Resour. Evaluation, vol. 42, no. 4,"
        },
        {
          "6. REFERENCES": "terspeech, 2023, pp. 12‚Äì16.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "pp. 335‚Äì359, 2008."
        },
        {
          "6. REFERENCES": "[6] Haobin Tang, Xulong Zhang,\nJianzong Wang, Ning",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[17] Steven R Livingstone and Frank A Russo,\n‚ÄúThe ryer-"
        },
        {
          "6. REFERENCES": "Cheng,\nand Jing Xiao,\n‚ÄúQi-tts: Questioning intona-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "son audio-visual database of emotional speech and song"
        },
        {
          "6. REFERENCES": "tion control for emotional speech synthesis,‚Äù in ICASSP,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "(ravdess): A dynamic, multimodal set of facial and vo-"
        },
        {
          "6. REFERENCES": "2023, pp. 1‚Äì5.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "cal expressions in north american english,‚Äù\nPloS one,"
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "vol. 13, no. 5, pp. e0196391, 2018."
        },
        {
          "6. REFERENCES": "[7] Yingzhi Wang, Mirco Ravanelli, Alaa Nfissi, and Alya",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "Yacoubi,\n‚ÄúSpeech emotion diarization: Which emotion",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[18] Adaeze Adigwe, No¬¥e Tits, Kevin El Haddad, Sarah"
        },
        {
          "6. REFERENCES": "appears when?,‚Äù CoRR, vol. abs/2306.12991, 2023.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "Ostadabbas,\nand\nThierry Dutoit,\n‚ÄúThe\nemotional"
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "voices database:\nTowards controlling the emotion di-"
        },
        {
          "6. REFERENCES": "[8] Xiong Cai, Dongyang Dai, Zhiyong Wu, Xiang Li, Jing-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "arXiv preprint\nmension in voice generation systems,‚Äù"
        },
        {
          "6. REFERENCES": "bei Li, and Helen Meng,\n‚ÄúEmotion controllable speech",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "arXiv:1806.09514, 2018."
        },
        {
          "6. REFERENCES": "synthesis using emotion-unlabeled dataset with the as-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "sistance of cross-domain speech emotion recognition,‚Äù",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[19] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li,"
        },
        {
          "6. REFERENCES": "in ICASSP, 2021, pp. 5734‚Äì5738.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "‚ÄúSeen and unseen emotional\nstyle\ntransfer\nfor voice"
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "conversion with a new emotional\nspeech dataset,‚Äù\nin"
        },
        {
          "6. REFERENCES": "[9] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "ICASSP, 2021, pp. 920‚Äì924."
        },
        {
          "6. REFERENCES": "Sadekova, and Mikhail Kudinov, ‚ÄúGrad-tts: A diffusion",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "probabilistic model for text-to-speech,‚Äù in ICML, 2021,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[20]\nJesin James, Li Tian, and Catherine Inez Watson,\n‚ÄúAn"
        },
        {
          "6. REFERENCES": "pp. 8599‚Äì8608.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "open source emotional speech corpus for human robot"
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "interaction\napplications,‚Äù\nin\nInterspeech,\n2018,\npp."
        },
        {
          "6. REFERENCES": "[10] Mingyi Chen, Xuanji He, Jing Yang, and Han Zhang,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "2768‚Äì2772."
        },
        {
          "6. REFERENCES": "‚Äú3-d convolutional\nrecurrent neural networks with at-",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "IEEE\ntention model\nfor speech emotion recognition,‚Äù",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[21] S. King and Vasilis Karaiskos,\n‚ÄúThe blizzard challenge"
        },
        {
          "6. REFERENCES": "Signal Process. Lett., vol. 25, no. 10, pp. 1440‚Äì1444,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "2013,‚Äù 2013."
        },
        {
          "6. REFERENCES": "2018.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[22] Michael McAuliffe, Michaela Socolof, Sarah Mihuc,"
        },
        {
          "6. REFERENCES": "[11] Sanyuan\nChen,\nChengyi Wang,\nZhengyang\nChen,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "Michael Wagner, and Morgan Sonderegger,\n‚ÄúMontreal"
        },
        {
          "6. REFERENCES": "Yu Wu, Shujie Liu, Zhuo Chen,\nJinyu Li, Naoyuki",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "forced aligner: Trainable text-speech alignment using"
        },
        {
          "6. REFERENCES": "Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "kaldi.,‚Äù in Interspeech, 2017, pp. 498‚Äì502."
        },
        {
          "6. REFERENCES": "Zhou, Shuo Ren, Yanmin Qian, Yao Qian,\nJian Wu,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "[23]\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae, ‚ÄúHifi-"
        },
        {
          "6. REFERENCES": "Michael Zeng, Xiangzhan Yu, and Furu Wei,\n‚ÄúWavlm:",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "gan: Generative adversarial networks for efficient and"
        },
        {
          "6. REFERENCES": "Large-scale\nself-supervised pre-training for\nfull\nstack",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "high fidelity\nspeech\nsynthesis,‚Äù\nin NeurIPS,\n2020,"
        },
        {
          "6. REFERENCES": "speech processing,‚Äù\nIEEE J. Sel. Top. Signal Process.,",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": "vol. 33, pp. 17022‚Äì17033."
        },
        {
          "6. REFERENCES": "vol. 16, no. 6, pp. 1505‚Äì1518, 2022.",
          "[12] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "year": "2020",
      "venue": "Denoising diffusion probabilistic models"
    },
    {
      "citation_id": "3",
      "title": "Score-based generative modeling through stochastic differential equations",
      "authors": [
        "Yang Song",
        "Jascha Sohl-Dickstein",
        "P Diederik",
        "Abhishek Kingma",
        "Stefano Kumar",
        "Ben Ermon",
        "Poole"
      ],
      "year": "2021",
      "venue": "Score-based generative modeling through stochastic differential equations"
    },
    {
      "citation_id": "4",
      "title": "Emodiff: Intensity controllable emotional text-tospeech with soft-label guidance",
      "authors": [
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "5",
      "title": "Diffusion models beat gans on image synthesis",
      "authors": [
        "Prafulla Dhariwal",
        "Alexander Quinn"
      ],
      "year": "2021",
      "venue": "Diffusion models beat gans on image synthesis"
    },
    {
      "citation_id": "6",
      "title": "Emomix: Emotion mixing via diffusion models for emotional speech synthesis",
      "authors": [
        "Haobin Tang",
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "year": "2023",
      "venue": "Emomix: Emotion mixing via diffusion models for emotional speech synthesis"
    },
    {
      "citation_id": "7",
      "title": "Qi-tts: Questioning intonation control for emotional speech synthesis",
      "authors": [
        "Haobin Tang",
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "year": "2023",
      "venue": "ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion diarization: Which emotion appears when?",
      "authors": [
        "Yingzhi Wang",
        "Mirco Ravanelli",
        "Alaa Nfissi",
        "Alya Yacoubi"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "Xiong Cai",
        "Dongyang Dai",
        "Zhiyong Wu",
        "Xiang Li",
        "Jingbei Li",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Grad-tts: A diffusion probabilistic model for text-to-speech",
      "authors": [
        "Vadim Popov",
        "Ivan Vovk",
        "Vladimir Gogoryan",
        "Tasnima Sadekova",
        "Mikhail Kudinov"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "11",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "12",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao",
        "Jian Wu",
        "Long Zhou",
        "Shuo Ren",
        "Yanmin Qian",
        "Jian Yao Qian",
        "Michael Wu",
        "Xiangzhan Zeng",
        "Furu Yu",
        "Wei"
      ],
      "year": "2022",
      "venue": "IEEE J. Sel. Top. Signal Process"
    },
    {
      "citation_id": "13",
      "title": "Generalized end-to-end loss for speaker verification",
      "authors": [
        "Li Wan",
        "Quan Wang",
        "Alan Papir",
        "Ignacio Lopez-Moreno"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Fine-grained style control in transformer-based text-to-speech synthesis",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "15",
      "title": "A kernel two-sample test",
      "authors": [
        "Arthur Gretton",
        "Karsten Borgwardt",
        "Malte Rasch",
        "Bernhard Sch√∂lkopf",
        "Alexander Smola"
      ],
      "year": "2012",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "16",
      "title": "Improved cross-corpus speech emotion recognition using deep local domain adaptation",
      "authors": [
        "Zhao Huijuan",
        "Wang Ning",
        "Ruchuan"
      ],
      "year": "2023",
      "venue": "Chinese Journal of Electronics"
    },
    {
      "citation_id": "17",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "18",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "19",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "Adaeze Adigwe",
        "No√© Tits",
        "Kevin Haddad",
        "Sarah Ostadabbas",
        "Thierry Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "arxiv": "arXiv:1806.09514"
    },
    {
      "citation_id": "20",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "21",
      "title": "An open source emotional speech corpus for human robot interaction applications",
      "authors": [
        "Jesin James",
        "Li Tian",
        "Catherine Watson"
      ],
      "year": "2018",
      "venue": "An open source emotional speech corpus for human robot interaction applications"
    },
    {
      "citation_id": "22",
      "title": "The blizzard challenge 2013",
      "authors": [
        "S King",
        "Vasilis Karaiskos"
      ],
      "year": "2013",
      "venue": "The blizzard challenge 2013"
    },
    {
      "citation_id": "23",
      "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
      "authors": [
        "Michael Mcauliffe",
        "Michaela Socolof",
        "Sarah Mihuc",
        "Michael Wagner",
        "Morgan Sonderegger"
      ],
      "year": "2017",
      "venue": "Montreal forced aligner: Trainable text-speech alignment using kaldi"
    },
    {
      "citation_id": "24",
      "title": "Hifigan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "Jungil Kong",
        "Jaehyeon Kim",
        "Jaekyoung Bae"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    }
  ]
}