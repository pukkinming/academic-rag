{
  "paper_id": "2208.00883v1",
  "title": "A Two-Stage Efficient 3-D Cnn Framework For Eeg Based Emotion Recognition",
  "published": "2022-07-26T05:33:08Z",
  "authors": [
    "Ye Qiao",
    "Mohammed Alnemari",
    "Nader Bagherzadeh"
  ],
  "keywords": [
    "Emotion recognition",
    "Electroencephalogram",
    "3D-CNN",
    "ResNet",
    "Quantization",
    "Deep learning",
    "Binary CNN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a novel two-stage framework for emotion recognition using EEG data that outperforms stateof-the-art models while keeping the model size small and computationally efficient. The framework consists of two stages; the first stage involves constructing efficient models named EEGNet, which is inspired by the state-of-the-art efficient architecture and employs inverted-residual blocks that contain depthwise separable convolutional layers. The EEGNet models on both valence and arousal labels achieve the average classification accuracy of 90%, 96.6%, and 99.5% with only 6.4k, 14k, and 25k parameters, respectively. In terms of accuracy and storage cost, these models outperform the previous state-of-the-art result by up to 9%. In the second stage, we binarize these models to further compress them and deploy them easily on edge devices. Binary Neural Networks (BNNs) typically degrade model accuracy. We improve the EEGNet binarized models in this paper by introducing three novel methods and achieving a 20% improvement over the baseline binary models. The proposed binarized EEGNet models achieve accuracies of 81%, 95%, and 99% with storage costs of 0.11Mbits, 0.28Mbits, and 0.46Mbits, respectively. Those models help deploy a precise human emotion recognition system on the edge environment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The Deep Neural Network achieves incredible results in computer vision, speech recognition, and natural language processing  [1] . Deep neural network models also perform exceptionally well for Brain-computer Interaction (BCI) and Human-computer Interaction (HCI) fields. Electroencephalogram (EEG) signals are used in a variety of BCI applications such as control prosthetics, neurofeedback, and emotion Recgnition  [2] . In real-time, a BCI system records EEG signals in a non-invasive manner and produces a message or computational command from the recorded signals. A BCI system is made up of three different components: sensors (mostly electrodes mounted on the scalp to record EEG signals); translation and communication (mostly translating EEG signals to command or computational language); real-time actions (actions based on EEG signals). Convolutional Neural Networks (CNNs) produce promising results in computer vision applications such as image recognition, object detection, and semantic * Equal Contributions segmentation. There are various types of CNNs, but in terms of dimensionality, 1-D CNNs are the most commonly used for time series applications such as human activity identification  [3]  and physiological signals as shown in  [4] . 2-D CNNs are the most commonly used for image data and computer vision applications such as image classification and segmentation. 3-D CNNs, which are mostly useful for volumetric data, have been adopted successfully in video analysis and object recognition tasks  [5] ,  [6] . 3-D CNNs take 3-dimensional inputs and apply 3-dimensional filters to them. The filters will move along three axes to form 3D shape outputs. Long sequence data such as video, audio, electrocardiogram (ECG), and electroencephalogram (EEG) signals can benefit from the extra convolutional dimension due to the spatiotemporal correlations between data segments in the long sequence.\n\nEmotions play an important role in human reasoning, and are linked to rational decision making, perception, human interaction, and even human intelligence itself  [7] . There are various methods existing for modeling human emotions, and one of the most effective approaches is using multiple dimensions or scales for emotion categorization. In such a model, emotions are defined by two major perception dimensions: valence and arousal. Arousal ranges from low to high, whereas valence ranges from positive to negative. For example, fear has a negative valence and high arousal, whereas excitement has a positive valence and high arousal. Electroencephalography (EEG) signals are brain waves that measure eclectic field behaviors from the human scalp. They naturally can be applied to human emotion recognition due to their reflection of human response and linkage to the cortical activities  [8] . Several works of literature studied EEG-based emotion recognition by extracting EEG features using deep neural network algorithms. These methods demonstrated high recognition accuracies compared to classical machine learning models but still required feature extraction and selection prior to the classifier  [9]    [10] . Using CNN for emotion recognition is not novel. A 2-D CNN used for emotion recognition of power spectrum density features (PSD) from the original EEG signals as input to the neural network, demonstrated good results  [11]  and  [12] . However, CNN typically requires a large number of computational resources, and computing power spectrum density is not efficient. As a result, deploying these types of models on low resource constrain devices is not possible. In this paper, we proposed a novel two-stage 3-D convolutional neural networks framework for emotion recognition without special signal transformations on EEG data. Our method significantly outperformed the state-of-the-art result while keeping the model size small and computationally efficient. Thus allowing model deployment on the edge environments where devices have limited resources.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "A variety of approaches, including traditional machine learning and deep learning algorithms, were introduced to identify and classify emotions. Traditional machine learning algorithms require feature extraction and selection before applying the classifier. The Support Vector Machine (SVM) has been used on the publicly available dataset DEAP to identify the valence and arousal perceptions using feature vectors based on statistical measurements of the frequency bands in the EEG signal, and achieved 67% by using all features  [13] . A deep belief network with glia chain (DBN-GC) was also adopted in this area. They extracted the intermediate representation of raw EEG signals from each domain separately, and then used the glia chain to mine correlation information. Lastly, they fused all information together using the Restricted Boltzmann Machine (RBM) to implement emotion recognition that achieved 75.92% and 76.83% on arousal and valence, respectively  [14] . On the other hand, the deep learning method proposed by  [15]  with a 2-D CNN achieved comparable performance as the SVM. However, there are two significant drawbacks of applying 2-D CNNs to raw EEG signals: covariance shift and unreliability of emotional ground truth. The covariance shift is the difference in statistical distribution between training and testing data, which is severe in EEG signals due to its nonstationary nature of the signal  [16] .\n\nUsually, raw EEG signals are segmented into several input sequences to augment the data. Emotion EEG trials should correspond to their ground truths, which are self-reported. The difference between the average of the segmented signals and ground truths causes the unreliability of each epoch which influences the model training  [15] . To address those two issues, the 3-D CNN structure was introduced because of its ability to simultaneously extract spatial and temporal features. Salama et al.  [8]  proposed a 3-D CNN strategy for EEGbased emotion recognition with a data augmentation method by adding normalized random Gaussian noise and achieved a better result compared to the previous methods. Yang et al.  [17]  developed a different multi-column CNN structure whose prediction is produced by a weighted sum of the decisions from all individual recognizing modules and obtained around 90% on both valence and arousal labels. Zhao et al.  [18]  constructed another 3-D CNN model with reshaping channel matrices that achieved the state-of-the-art result at 96.43% and 96.61% on valence and arousal respectively. However, the large parameters counts of the redundant model prevented its usage in practice.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Eeg Signal",
      "text": "EEG signals can be classified into different ranges based on their frequency. Delta waves are the slowest and most common in deep sleep and relaxation. Theta wave is commonly associated with Rapid Eye Movement (REM) sleep, as well as learning and memory retention. Alpha wave is linked to sensory-motor and memory function. The most common wave is the beta wave, which is associated with alertness and awareness as well as day-to-day problem-solving tasks. Gamma is the fastest wave and is associated with alertness and high virtues such as altruism and compassion  [19] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Emotions Recognition",
      "text": "Emotions vary from one person to another. Several methods are presented in the literature for modeling human emotions. One model depicts simple emotions such as happiness and sadness  [20] , while the other depicts fear, anger, depression, and satisfaction  [21] . Another modeling method employs several measurements or scales to categorize human emotions  [22] . The most prevalent human emotion model uses two key dimensions to model the emotions which are valence and arousal. Valence varies from positive to negative, while arousal ranges from low to high. Fear, for example, has negative valence and high arousal, while excitement has positive valence and high arousal  [23] .\n\nIn the frequency domain, power features are often used in researches. The alpha band power spectral density (PSD) of EEG correlates with valence  [24] . Furthermore, delta and theta bands of power spectral density (PSD) of EEG when extracted from three central channels, contain information that is associated with valence and arousal  [25] . In this paper, we focused on the arousal and valence scale.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Proposed Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Efficient 3-D Cnn Models With Inverted Residual Block",
      "text": "The 3-D convolutional neural network expands on the traditional 2-D CNNs by adding an additional dimension. 3-D CNN is written as follows:\n\nWhere x l-1 is the output from the previous layer after activation applied, ω is the 3-D convolutional kernel with size m * n * p, and y l is the convolution output. To the best of our knowledge, there are only a few researches that have been conducted with the use of 3-D CNNs, particularly for EEG signals.\n\nTo test the benefits of using 3-D CNNs, we modified the most widely used architectures in literature, ResNet-18  [26]  and MobileNetV2  [27] , by replacing 2-D CNNs with 3-D CNNs. The use of these models on our EEG dataset resulted in excellent accuracy at the expense of very high storage and computational costs as shown in the section V. However, deploying these models on edge BCI devices is not a viable option.  We created three extremely efficient network models named EEGNet V1, V2, and V3, in which we modified the block structure introduced by  [27]  by replacing the inverted residual blocks and depthwise separable convolutions to 3-D operations instead of 2-D as shown in Fig.  1 . Depthwise separable convolution decoupled the standard convolution into a 3 × 3 depthwise convolution and a 1 × 1 pointwise convolution that reduce the learning parameters and computational costs of the networks  [28] . In a standard residual block, inputs are followed by multiple bottleneck layers, which are then followed by expansions. As shown with 3-D inverted residual blocks in Fig.  1 . We reversed the bottleneck and expansion layers, and then applied shortcut connections directly between the two bottlenecks when both the input and output tensor of the block have the same shape. The blocks also have their 3-D batch normalization layer and ReLU activation layer inside. Our models consisted of three inverted residual blocks and then were followed by another point-wise convolution layer to expand the feature maps for classification. The last two layers are a dropout layer which is included for regularization to prevent an overfitting and a fully connected linear layer for generating classification logits in which then go through a softmax function to produce final classification probabilities with two classes (positive and negative).\n\nAs shown in Table  I , we tuned three hyperparameters to create three versions of EEGNet models: the expansion factor t, the width multiplier factor (WF), and the number of output neurons of the last convolution layer. The Expansion factor t determines the number of times to expand the channels of the input tensor to the hidden expansion layers of the second and third inverted residual blocks. WF determines the overall model channel width. The number of output neurons controls the number of output neurons of the last point-wise convolution layer. The V1, V2, and V3 are in ascending order of model complexity, but increasing test accuracy as well. They are targeting edge platforms with different resource constraints and latency requirements.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Model Binarization",
      "text": "To further compress the models, we binarized the weights and activation within the inverted residual blocks as the second stage of our framework. Binarization is an extreme case of quantization in which only one bit is used to represent the number in the weights and activation, therefore greatly reducing the computation and memory footprint. The XNOR-Net is a common binarization method that achieves up to 58x of inference speed and 32x of memory saves  [29] . The issue with binary neural networks is that model accuracy often degrades significantly. Hence, we introduced three techniques to improve the accuracy of the binarized models with little to no additional cost in terms of storage and computational resources.\n\n1) Add real value residual connections to each block: Residual connections provide the possibility of constructing very deep models without performance degradation. Normally feature maps produced by convolution operation are directly calculated, but  [26]  suggested that the convolution layers have difficulty learning the identity mapping assuming the solution is already optimal from the previous layer. With residual connection between layers, the feature map H(x) now constructed by two-part: H(x) = x + F (x). Where x is identity mapping and F (x) is the residual that is learned by the current layer, so the performance of a deep model will at worst be equal to its shallow counterpart. Our baseline models applied residual connection only when the given input channel is equal to output channel and stride is (1,1,1) as shown in Fig.  1, i .e. only the first block.\n\nThe previous residual setting is sufficient for real value baseline models, since our network architecture is not very deep. However, the binarized models will suffer significantly due to information loss when applying nonlinear activation even with a shallow network structure. Hence, for the binarized network, we preserved the real value feature map from the previous layer, and add it to the output activation. Then applied the real value residual connections to all the three blocks in the proposed models. If the convolution stride is not (1,1,1) or channels of input/output are different, we first downsampled the real value feature map to its desired shape and then added as shown in Fig.  2 . Those dense real value connections increased the representation capability significantly due to the limited knowledge that binarized activation maps contained. The only additional cost of adding more real value shortcuts is a small amount of element-wise addition operations and no extra memory cost because the addition operations are computed on the fly during the inference stage.\n\n2) Apply channel-wise scaling factor: Binarize a neural network by applying the sign function X b = sign(X r ) to both the weights and the activation, resulting in crucial information loss. To address the issue, Rastegari et al.  [29]  suggested the scaling factors α and K to approximate floating-point weights and activation after binarization, as follows:\n\nWhere A and W are weights and activation, * is the real value convolution operator, represent binary convolution with XNOR and bits shift operations, α is a weight scaling factor such that W r ≈ αW b , and K is the scalar factor matrix of the corresponding activation. K was removed in our case due to its high computational cost and negligible impact on performance, as suggested by  [29] . However, We found that analytically calculated α = ||W || n , n = c × w × h is not optimal. It averaged out all of the weighting channels but ignored their significance and overall magnitude levels. As a result, we proposed a channel-wise scaling factor, and added the third dimension to empathize model representation ability with\n\n, where c is weight channels and k × w × h is data dimensions. The usage of channel-wise scaling factor improved the performance of binarized models significantly, as section V demonstrated.\n\n3) Architecture modification for additional compression: In the proposed models, we binarized the layers inside the inverted residual blocks for better performance. The first and last full-precision convolution layers, which require a high computation and storage cost, become the most expensive parts of the model during training and inference. Therefore, to further compress the binarized models, we reduced the number of filters in the first channel from 32×WF to 16×WF. This avoided performing millions of multiplication and accumulation operations with negligible performance loss due to the redundancy nature of CNNs. Furthermore, the final full-precision convolution layer, which usually requires a high computation and storage due to its property of generating a large number of feature maps for final prediction, has been modified by adding the average pooling layer ahead. This resulted in the computations being done at 1 × 1 × 1 spatialtemporal resolution in the final layer rather than 3 × 8 × 32. Additionally, the last stage tuning not only reduced the computation but increased the accuracy as well because the early average pooling helped to avoid location sensitivity of input features in the activation.\n\nCombining these three methods vastly improved the performance of binarized models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Experiment And Results Analysis",
      "text": "A. DEAP Dataset DEAP  [30]  is a well-known public EEG dataset for the analysis of human affective states. It consists of 32 participants with recordings as each watched 40 one-minute-long excerpts of music videos. Then participants rated each video to the levels of valence, arousal, liking, and dominance  [30] . Resulted in 63-second multi-channel EEG signals with 8064 sample points per channel per trial. The first 3-second of each trial is the baseline prior to the actual experiment and will be removed in our data process step. There are 40 channels that have been recorded per trial with 32 EEG channels and 8 side channels. The EEG channels are recorded using a standard international 10-20 system with 32 active AgCl electrodes  [30] . The rated score of each video clip ranges from 1 to 9 per label.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Data Preprocess And 3D Representation",
      "text": "We first adopted a preprocessing method that is widely used in the literature  [31] , in which we downsampled the data from 512 Hz to 128 Hz and removed Electrooculography (EOG) artifacts. Then, to eliminate unwanted frequency components, we introduced a bandpass filter, which only preserved the 4-45Hz frequency range covering Theta, Alpha, and Beta waves. The 3 seconds pre-trial baseline and the eight side channels are then removed as well. We then normalized the data for each channel of each trial to be between 0 and 1. Lastly, we divided each trial into 32 1-second data frames with a window size of 128 points and an overlapping ratio of 50% as shown in Fig.  1 . In a short period of time, this method preserves the temporal stationery of EEG signals.\n\nTo prepare the datasets for 3D-CNNs, we stacked up multiple 32-channel by 1-second-long consecutive data frames to form 3-D data chunks  [8] . After experimenting with various frame sizes, we selected six frames to continue as they yielded the best accuracy, as shown in Table  II . Then, we assigned each chunk the labels the same as the ground-truth labels of its corresponding trial. In total, we constructed 25600 chunks of data with a size of 6*32*128 as the method presented. We experimented with valence and arousal perception in this paper with their corresponding labels. A threshold value of 5 was applied to assign positive and negative labels from the provided 1 to 9 scores, as this is common in the area of research literature. Finally, our proposed method is shown in Fig.  1 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Training Setting",
      "text": "All models were trained on an NVIDIA Tesla V100 GPU and implemented with the PyTorch framework  [32] . We split the training and validation datasets with 80% and 20% respectively. The dropout layer was set to a 0.2 dropout rate. We trained the models for 100 epochs with a batch size of 256 and adopted Adam  [33]  optimizer. The initial learning rate was set to 0.001 and The multi-step learning rate scheduler was applied with a milestone set to 75, and gamma equal to 0.5. We used the cross entropy loss function across all models and applied additional label smoothing  [34]  with = 0.1 specifically to the binarized models. Label smoothing provides additional regularization by constructing soft labels as\n\nWhere K is the number of label classes and was set to 2 in our cases.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Results Of Baseline Models",
      "text": "Our proposed method significantly outperformed previous studies without special feature extraction or signal transformation while still maintaining a compact size. As Table  III  shows, our EEGNet V1 achieves better results compared to  [17]  and  [8]  but with over 10x fewer learning parameters, which is only 6.4K. Our proposed EEGNet V2 achieves comparable performance with the state-of-the-art 3-D CNN approach  [18]  and 3-D ResNet18 in the experiment but with only 14.6K learning parameters, which is less than 1% of the parameter counts compared to  [18] . Ultimately, our largest V3 variant achieved an average classification accuracy of 99.5% with only 24.8K parameters and significantly outperformed the previous studies. To the best of our knowledge, these three variants    [16] , and dense prediction solved the unreliability issue  [15] . Inverted residual blocks maintained the manifolds of interest in neural networks and reduced the information loss when doing non-linear transformation  [27] . Finally, applying depthwise separable convolution reduced the parameters and the computation complexity of the models. These advantages make efficient model deployment become possible for low power and resource-constrained device on edge.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Results Of Binarized Models",
      "text": "Our model binarization algorithm adopted the piece-wise polynomial function proposed by  [37]  to estimate the derivative of the sign function to successfully propagation the binary models. Table  IV  shows the effectiveness of our three proposed techniques, and results in model performance improvements of 5%, 17%, and 2% respectively on arousal label with EEGNet V2 setting. The valence label showed a similar performance gain in our experiments. Combining these we achieved over 20% improvement over the plain binary model while still maintaining similar storage and computational cost. As shown in Table  III , the binarized models with v2 and v3 settings achieved comparable performance to their full precision counterparts while further compressing the size over 40%. The memory usage (i.e. model size) was calculated by summing up the 32-bit multiples the number of real-valued parameters and 1-bit multiples the number of binary parameters in each model. Hence, our binarized models can take advantage of the proposed method and speed up significantly when deploy to potential edge systems while still maintaining state-of-the-art performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we proposed a two-stage 3-D CNN framework that specialized in emotion recognition tasks using timedomain EEG signals. The framework extracted spatiotemporal feature representations automatically. The public DEAP dataset was used to conduct experiments with data processing techniques to form the 3-D inputs. Our models showed superior performance on classifying both valence and arousal perceptions, which can be easily processed to actual human emotions afterward. The introduced baseline EEGNet V1, V2, and V3 in the first stage achieved average classification accuracies of 90%, 96.6%, and 99.5% respectively with a small number of learning parameters and compact model size. In the second stage, we binarized these models with three novel techniques to perform further compression and help to take advantage of efficient bitwise operation. Model binarization saved over 40% of storage costs and computational resources compared to the baseline EEGnet, while still keeping comparable performance to their full precision counterparts. This paper mainly focused on the algorithmic and efficient model design. In future work, we will provide more comprehensive experiments on real edge environments with detailed latency and energy consumption analysis. A neural architecture search (NAS) strategy based on Multi-Objective Bayesian Optimization (MOBO) will also be introduced to further improve the existing model design to balance various device constraints and needs. Finally, the efficient models we presented helped the real-time deployment of a precise human emotion recognition system in a resourceconstrained environment become a viable option.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Data Process Steps and Proposed EEGNet Architecture",
      "page": 3
    },
    {
      "caption": "Figure 1: Depthwise separable",
      "page": 3
    },
    {
      "caption": "Figure 1: We reversed the bottleneck and expansion layers, and",
      "page": 3
    },
    {
      "caption": "Figure 2: Proposed Binary EEGNet Architecture",
      "page": 4
    },
    {
      "caption": "Figure 1: , i.e. only the ﬁrst block.",
      "page": 4
    },
    {
      "caption": "Figure 2: Those dense real value connections",
      "page": 4
    },
    {
      "caption": "Figure 1: In a short period of time, this method preserves the",
      "page": 5
    },
    {
      "caption": "Figure 1: C. Training Setting",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model\nValence (%)\nArousal\n(%)\nParameters\nMemory Usage": "Samara et al.\n[13]\n66.90\n66.69\n-\n-"
        },
        {
          "Model\nValence (%)\nArousal\n(%)\nParameters\nMemory Usage": "Chao et al.\n[14]\n76.83\n75.92\n-\n-"
        },
        {
          "Model\nValence (%)\nArousal\n(%)\nParameters\nMemory Usage": "Wang et al.\n[35]\n72.10\n73.30\n-\n-"
        },
        {
          "Model\nValence (%)\nArousal\n(%)\nParameters\nMemory Usage": "Yanagimoto et al.\n[36]\n81.16\n-\n-\n-\nSalama et al.\n[8]\n87.44\n88.49\n2.5M\n75.13 Mbits\nYang et al.\n[17]\n90.01\n90.65\n314K\n9.58 Mbits\nZhao et al.\n[18]\n96.43\n96.61\n170M\n5435 Mbits\nResnet18-3D\n92.77\n97.53\n33.2M\n1012 Mbits\nMobileNetV2-3D\n99.68\n99.70\n2.4M\n71.87 Mbits"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "2",
      "title": "Brain-computer interface (bci) applications in mapping of epileptic brain networks based on intracranial-eeg: An update",
      "authors": [
        "R Alkawadri"
      ],
      "year": "2019",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "3",
      "title": "Understanding 1d convolutional neural networks using multiclass time-varying signals",
      "authors": [
        "R Srinivasamurthy"
      ],
      "year": "2018",
      "venue": "Understanding 1d convolutional neural networks using multiclass time-varying signals"
    },
    {
      "citation_id": "4",
      "title": "1d convolutional neural networks and applications: A survey",
      "authors": [
        "S Kiranyaz",
        "O Avci",
        "O Abdeljaber",
        "T Ince",
        "M Gabbouj",
        "D Inman"
      ],
      "year": "2021",
      "venue": "Mechanical systems and signal processing"
    },
    {
      "citation_id": "5",
      "title": "3d convolutional neural networks for human action recognition",
      "authors": [
        "S Ji",
        "W Xu",
        "M Yang",
        "K Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "6",
      "title": "Large-scale video classification with convolutional neural networks",
      "authors": [
        "A Karpathy",
        "G Toderici",
        "S Shetty",
        "T Leung",
        "R Sukthankar",
        "L Fei-Fei"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities",
      "authors": [
        "N Suhaimi",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "8",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Shalaby"
      ],
      "year": "2018",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "9",
      "title": "Revealing critical channels and frequency bands for emotion recognition from eeg with deep belief network",
      "authors": [
        "W.-L Zheng",
        "H.-T Guo",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "2015 7th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "10",
      "title": "Integration of a low cost eeg headset with the internet of thing framework",
      "authors": [
        "M Alnemari"
      ],
      "year": "2017",
      "venue": "Integration of a low cost eeg headset with the internet of thing framework"
    },
    {
      "citation_id": "11",
      "title": "A novel deeplearning based framework for multi-subject emotion recognition",
      "authors": [
        "R Qiao",
        "C Qing",
        "T Zhang",
        "X Xing",
        "X Xu"
      ],
      "year": "2017",
      "venue": "2017 4th International Conference on Information, Cybernetics and Computational Social Systems (ICCSS)"
    },
    {
      "citation_id": "12",
      "title": "Convolutional neural network approach for eeg-based emotion recognition using brain connectivity and its spatial information",
      "authors": [
        "S.-E Moon",
        "S Jang",
        "J.-S Lee"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Feature extraction for emotion recognition and modelling using neurophysiological data",
      "authors": [
        "A Samara",
        "M Menezes",
        "L Galway"
      ],
      "year": "2016",
      "venue": "2016 15th international conference on ubiquitous computing and communications and 2016 international symposium on cyberspace and security"
    },
    {
      "citation_id": "14",
      "title": "Recognition of emotions using multichannel eeg data and dbn-gc-based ensemble deep learning framework",
      "authors": [
        "H Chao",
        "H Zhi",
        "L Dong",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "15",
      "title": "Categorical and dimensional affect analysis in continuous input: Current trends and future directions",
      "authors": [
        "H Gunes",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "16",
      "title": "Eeg-based emotion recognition using deep learning network with principal component based covariate shift adaptation",
      "authors": [
        "S Jirayucharoensak",
        "S Pan-Ngum",
        "P Israsena"
      ],
      "year": "2014",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "17",
      "title": "A multi-column cnn model for emotion recognition from eeg signals",
      "authors": [
        "H Yang",
        "J Han",
        "K Min"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "A 3d convolutional neural network for emotion recognition based on eeg signals",
      "authors": [
        "Y Zhao",
        "J Yang",
        "J Lin",
        "D Yu",
        "X Cao"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "19",
      "title": "Basics of brain computer interface",
      "authors": [
        "R Ramadan",
        "S Refat",
        "M Elshahed",
        "R Ali"
      ],
      "year": "2015",
      "venue": "Brain-Computer Interfaces"
    },
    {
      "citation_id": "20",
      "title": "Attribution, emotion, and action",
      "authors": [
        "B Weiner"
      ],
      "year": "1986",
      "venue": "Attribution, emotion, and action"
    },
    {
      "citation_id": "21",
      "title": "Emotions, cognition, and behavior. CUP Archive",
      "authors": [
        "C Izard",
        "J Kagan",
        "R Zajonc"
      ],
      "year": "1984",
      "venue": "Emotions, cognition, and behavior. CUP Archive"
    },
    {
      "citation_id": "22",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "23",
      "title": "Valence, arousal and dominance in the eeg during game play",
      "authors": [
        "B Reuderink",
        "C Mühl",
        "M Poel"
      ],
      "year": "2013",
      "venue": "International journal of autonomous and adaptive communications systems"
    },
    {
      "citation_id": "24",
      "title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals",
      "authors": [
        "G Verma",
        "U Tiwary"
      ],
      "year": "2014",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "25",
      "title": "Toward emotion aware computing: an integrated approach using multichannel neurophysiological recordings and affective visual stimuli",
      "authors": [
        "C Frantzidis",
        "C Bratsas",
        "C Papadelis",
        "E Konstantinidis",
        "C Pappas",
        "P Bamidis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "26",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "28",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "29",
      "title": "Xnor-net: Imagenet classification using binary convolutional neural networks",
      "authors": [
        "M Rastegari",
        "V Ordonez",
        "J Redmon",
        "A Farhadi"
      ],
      "year": "2016",
      "venue": "Xnor-net: Imagenet classification using binary convolutional neural networks"
    },
    {
      "citation_id": "30",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "31",
      "title": "Classification of eeg signals using the wavelet transform",
      "authors": [
        "N Hazarika",
        "J Chen",
        "A Tsoi",
        "A Sergejew"
      ],
      "year": "1997",
      "venue": "Signal processing"
    },
    {
      "citation_id": "32",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "34",
      "title": "When does label smoothing help?",
      "authors": [
        "R Müller",
        "S Kornblith",
        "G Hinton"
      ],
      "year": "2019",
      "venue": "When does label smoothing help?",
      "arxiv": "arXiv:1906.02629"
    },
    {
      "citation_id": "35",
      "title": "Emotionet: A 3-d convolutional neural network for eeg-based emotion recognition",
      "authors": [
        "Y Wang",
        "Z Huang",
        "B Mccane",
        "P Neo"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "36",
      "title": "Recognition of persisting emotional valence from eeg using convolutional neural networks",
      "authors": [
        "M Yanagimoto",
        "C Sugimoto"
      ],
      "year": "2016",
      "venue": "2016 IEEE 9th International Workshop on Computational Intelligence and Applications (IWCIA)"
    },
    {
      "citation_id": "37",
      "title": "Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm",
      "authors": [
        "Z Liu",
        "B Wu",
        "W Luo",
        "X Yang",
        "W Liu",
        "K.-T Cheng"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    }
  ]
}