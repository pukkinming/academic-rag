{
  "paper_id": "2111.10776v3",
  "title": "A Case Study On The Independence Of Speech Emotion Recognition In Bangla And English Languages Using Language-Independent Prosodic Features",
  "published": "2021-11-21T09:28:49Z",
  "authors": [
    "Fardin Saad",
    "Hasan Mahmud",
    "Mohammad Ridwan Kabir",
    "Md. Alamin Shaheen",
    "Paresha Farastu",
    "Md. Kamrul Hasan"
  ],
  "keywords": [
    "Emotional Speech Set",
    "Language Independent Features",
    "Native/Non-native Speakers",
    "Prosodic Features",
    "Speech Emotion Recognition",
    "Support Vector Machines"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A language agnostic approach to recognizing emotions from speech remains an incomplete and challenging task. In this paper, we performed a step-by-step comparative analysis of Speech Emotion Recognition (SER) using Bangla and English languages to assess whether distinguishing emotions from speech is independent of language. Six emotions were categorized for this study, such as -happy, angry, neutral, sad, disgust, and fear. We employed three Emotional Speech Sets (ESS), of which the first two were developed by native Bengali speakers in Bangla and English languages separately. The third was a subset of the Toronto Emotional Speech Set (TESS), which was developed by native English speakers from Canada. We carefully selected language-independent prosodic features, adopted a Support Vector Machine (SVM) model, and conducted three experiments to carry out our proposition. In the first experiment, we measured the performance of the three speech sets individually, followed by the second experiment, where different ESS pairs were integrated to analyze the impact on SER. Finally, we measured the recognition rate by training and testing the model with different speech sets in the third experiment. Although this study reveals that SER in Bangla and English languages is mostly language-independent, some disparities were observed while recognizing emotional states like disgust and fear in these two languages. Moreover, our investigations revealed that non-native speakers convey emotions through speech, much like expressing themselves in their native tongue.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Humans have been intuitively using speech as the most natural and preferred means of communication. However, speech signals are mostly non-stationary processes, containing multiple components that vary in time and frequency. Since these signals occur naturally, they are erratic in nature  [1] . Furthermore, these signals carry a lot of information and, at the same time, convey an individual's emotional status. An emotional speech expresses the patterns of rhythm and intonation, often referred to as prosody in a language  [2] ,  [3] . The prosodic cues in speech signals are also known as para-linguistic features since they are linked with speech segments properties such as syllables, words, and sentences  [3] . Zeng et al.  [4]  demonstrated prosodic features to contain the most critical and exclusive emotional information. Consequently, numerous research in Speech Emotion Recognition (SER) have used prosodic features to identify emotional states  [3] .\n\nSER directs the identification of emotional states of a person from their speech  [5] . It facilitates the measurement of acoustic cues from speech as a standard for emotion recognition  [6] , which led the path for researching the most comprehensive features that contribute greatly to this cause. In line with this, Pell et al.  [7]  emphasized the significance of vocal features for emotion recognition from speech. Several studies were conducted to investigate the salient vocal features. Amongst them, prosodic features were found to give better if not the same emotion recognition accuracy than human judges  [3] ,  [8] ,  [9] . Apart from this, SER systems are useful in several applications such as e-learning, where the emotional attributes of the pupils can be identified to regulate the teaching style of the tutors  [10] ,  [11] . Rapid commercialization of speech emotion recognition can be seen in employee mood identification  [12] , interactive games  [13] , call centers  [14]  to decipher customer queries and complaints, psychiatric aids, among others.\n\nFeature extraction and selection from speech impart great importance in successfully identifying emotional states. However, selecting a large number of features breeds various complexities, which eventually results in classification error  [3] ,  [15] . Furthermore, languages can differ in many ways with regards to their grammatical and morphological properties  [15] , making SER ambiguous. The variations in dialects of a language can occur due to the usage of words, accents, or how people arrange their speech. These variations can be credited to certain social factors, culture, or geographical distance  [16] . Moreover, factors such asgender, age, background, vocal features, etc., of a speaker highly influence the expression of different emotions  [2] ,  [17] ,  [18] . Due to these multifarious factors, emotion recognition from speech remains an arduous task  [19] . Hence, there may be particularly pivotal factors that influence speech emotion recognition  [20] . So, it becomes essential for us to work with vocal features which are independent of the nuances of language.\n\nFrom previous research, we found that prosodic features  [3] ,  [4] ,  [21]  such as fundamental frequency or pitch-related features containing pitch mean, pitch median, pitch standard deviation, and energy-related features like intensity carry a lot of emotional information  [22] ,  [23] . In contrast, the spectral features such as MFCC related features depend on phonemes, and thus the style of an utterance  [23] . Therefore, it can be inferred that spectral features are language-dependent  [23]  features, whereas prosodic features are language-independent  [21] ,  [22] .\n\nSchull et al.  [24]  enforced wrapper-based search along with the dynamic base contour to develop a feature set from intensity, pitch, formants, and MFCCs. The best emotion recognition rate has been accomplished using MFCC features with SVM. Rajoo et al.  [20]  used MFCCs, formants, energy, and fundamental frequency or pitch as acoustics cues for speech emotion recognition. Borchert et al.  [3] ,  [25]  used prosodic vocal features and quality features such as formants, spectral energy distribution, jitter, and shimmer, to classify emotions. For deducing the salient set of vocal features, Kostoulas et al.  [26]  and Anagnostopoulos et al.  [27]  employed a subset of correlated features. Despite improving the emotion classification rate, these features are not language-independent since most used spectral features on a single emotional corpus.\n\nThus, to construct a Speech Emotion Recognition system, it is vital to select prosodic features such as fundamental frequency or pitch and energy-related features and select a classifier independent of these features. Noroozi et al.  [15]  presented a system for achieving a set of language-independent features. They observed that pitch and energy-related features were language-independent when they adopted a Support Vector Machine (SVM) classifier. While exploring language-independent features via feature selection, Shaukat et al.  [22]  discerned that fundamental frequency or pitch, formants, intensity or energy, harmonicity, loudness, duration, etc., were language-independent features. Subsequently, they achieved a higher performance rate when using SVM for their classification method. Lieke et al.  [28]  used 3 prosodic cues onto speech by Spanish learners of Dutch to judge whether it improves native Dutch speakers' perception of accentedness and comprehensibility. Luengo et al.  [8]  illustrated that six prosodic features incorporated with an SVM classifier could achieve an emotion recognition rate almost equal to GMM models with 512 spectral MFCC features emphasizing the significance of prosodic features in distinguishing emotions. Rao et al.  [29]  extracted and employed local and global prosodic features with SVM at different speech segments to recognize emotions from the Telegu emotional speech corpus. Bhatti et al.  [21]  used 17 prosodic features, which included pitch mean, pitch median, pitch standard deviation, etc., as language-independent features for identifying emotional states from speech.\n\nIn this study, we aim to juxtapose and compare Bangla and English languages to measure the language independency of SER using language-independent prosodic features and native/non-native speakers. We used 6 emotional states such as -happy, angry, neutral, sad, disgust, and fear  [1] ,  [2] ,  [20] ,  [23] , in English and Bangla languages across three Emotional Speech Sets (ESS), such as -1) the English TESS (E-TESS), a subset of the Toronto ESS (TESS)  [30] , for the native English language, 2) the Bangla ESS (BESS) for the native Bangla language, developed by native Bangla speakers in accordance with the development process of TESS  [31] , and 3) similar to BESS, the English ESS (EESS) for the nonnative English language, developed by the same native Bangla speakers, or in other words non-native English speakers. We constructed a feature set through careful selection of language-independent features such as pitch mean, pitch median, pitch standard deviation and intensity  [1] ,  [15] ,  [21] -  [23]  and analyzed its influence on SER. These 4 features are the most commonly used and salient prosodic features in almost all the SER systems that used prosodic features with SVM classifier  [1] ,  [3] ,  [8] ,  [9] ,  [15] ,  [21] -  [23] . Furthermore, we avoided choosing too many features to preclude misclassification, correlated features, and overlap. Since our objective is not to increase the performance of our SER system instead to analyze the nuances in Bangla and English languages, we adopted SVM  [32]  for classifying the emotions mentioned earlier. However, we also employed traditional classifiers for SER, such as -Hidden Markov Model (HMM), Gaussian Mixture Model (GMM), Artificial Neural Networks (ANN), and k-Nearest Neighbor (kNN)  [3]  only to compare the emotion recognition rate of these classifiers with SVM.\n\nIn Section 2, we thoroughly discussed about our proposed approach, followed by feature extraction and selection, in Sections 3. In the subsequent sections, we have elaborated on our classification approach (Section 4), investigated the language independence of Speech Emotion Recognition for Bangla and English languages (Section 5), and discussed its scope and future possibilities (Section 6).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotional Speech Set Acquisition And Development",
      "text": "Due to the lack of any significant emotional speech corpora for Bangla language, we have developed a Bangla ESS (BESS), in accordance with the Toronto ESS (TESS)  [30] , as shown in Fig.  1 , with assistance from 11 native Bangla speakers (64% Male, 36% Female) who are undergraduate students of the affiliated institution. Similarly, the same native speakers, or in other words, the same non-native English speakers, participated in the development of the English ESS (EESS). The 11 native Bangla speakers were carefully selected for this study based on their fluency and proficiency in both Bangla and English languages. We identified 6 of the most common emotional states used in SER, such ashappy, angry, neutral, sad, disgust, and fear for our speech corpora  [2] ,  [15] ,  [20] ,  [23] ,  [31] . The 11 speakers were asked to elicit emotional speech, simulating these emotions. Approximately 30-40 hours were spent for recording each emotional state, among which 5-6 hours were allotted for rehearsal and calibration of the speaker's portrayal of each emotion. At least one male and one female speaker were involved in recording the audio samples of each emotion.\n\nEach audio sample, containing the phrase, \"Say the word…\", followed by a monosyllabic noun  [30] ,  [31] , was recorded within 1-2 seconds. For instance, it can be observed from Fig.  1  that a sentence, used in the dataset was, \"Say the word Read.\" The monosyllabic nouns were selected based on the existing speech intelligibility such as the Northwestern University Auditory Test-Number 6 (NU-6)  [33] . While developing the Bangla dataset, the same sentence, \"Say the word Read\" in English, was translated to Bangla, which read as, \"Poro shobdo ti bolo\", as shown in Fig.  1 . It can be discerned from Fig.  2  that the verbatim translation of Bangla words such as -\"Poro\", \"shobdo ti\", and \"bolo\" to its corresponding English words are \"Read\", \"the word\", and \"Say\", respectively. For comprehensive understanding, Bangla words used in the Bangla speech sample were mapped to their corresponding English words, and in this manner, BESS and EESS were developed.  Although the Auditory Test-Number 6 (NU-6) housed 200 words  [31] , we carefully selected 50 words whose Bangla translation would adhere to the lexical and semantic properties of speech intelligibility. Therefore, for the 2 datasets (BESS and EESS) and for the 6 emotions (happy, angry, neutral, sad, disgust, and fear) with 50 audio speech samples each, we have a total of 2 datasets × 6 emotions × 50 speech samples = 600 audio samples, 300 samples per dataset (BESS and EESS).\n\nOn the other hand, TESS was developed by 2 Canadian actors, of whom one was younger (26 years of age), and the other was older (64 years of age), containing a total of 2800 audio samples (200 NU-6 words x 7 emotions x 2 actors)  [31] . However, as established earlier, to ensure preservation of the speech intelligibility for Bangla and English languages, audio samples containing the same 50 words as BESS or EESS, were retrieved from TESS for the 6 emotions to create the dataset, English TESS (E-TESS) as a subset of TESS, containing 300 samples. The workflow of generating E-TESS is depicted in Fig.  3 . In this manner, 3 datasets were assembled for our experiments, which will be addressed as Bangla Emotional Speech Set (BESS), English Emotional Speech Set (EESS), and English TESS (E-TESS), as shown in Fig.  1  and Fig.  3 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction And Selection",
      "text": "Features dictate the performance of a Speech Emotion Recognition system, and identifying emotional states require various features  [20] . For the expression of emotions, speech features contribute in particular ways  [34] . However, the most salient features can improve the performance of a model exponentially. Misclassification and overlaps of emotional states by an SER system are mostly due to inadequate and ineffective choices of features  [35] . In our work, the vocal features for both BESS and EESS were extracted using signal processing techniques, implemented in the open-source software package, \"Praat\", developed by the University of Amsterdam  [36] . This process is briefly elaborated on in the following section. For this study, we used language-independent prosodic features, such aspitch median, pitch mean, pitch standard deviation, and intensity.\n\n• Pitch or fundamental frequency  [37] ,  [38]  is frequently deployed in Speech Emotion Recognition Systems  [3] ,  [15] ,  [20] ,  [39] ,  [40] . It represents the vibration frequency of the vocal cords during sound production. • Pitch-related features such as pitch mean and pitch median are widely used in SER systems  [1] ,  [21] ,  [23]  and are regarded as language-independent features  [15] ,  [21] -  [23] . • Pitch standard deviation is another prosodic feature which carries vital emotional information  [21] ,  [23]  and is used as one of the acoustic feature  [21] ,  [41] . • Energy or intensity refers to the loudness of sound. This feature is also used quite frequently in SER systems  [16] ,  [42] . It is deemed to be a language-independent prosodic feature  [15] ,  [22] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Speech Normalization And Feature Analysis Using Praat Software",
      "text": "The audio samples were scrupulously recorded with a high-quality microphone at a sampling rate of 44 kHz, or in other words 44100 samples per second per channel  [36]  were recorded for maximum audio quality. A quiet room was selected for the recording sessions to reduce background noise. No filters were applied on the audio recordings to avoid distortion and to ensure preservation of the original intensity of the speech signals.\n\nThe sample audio recordings of BESS and EESS for the phrases \"Poro shobdo ti bolo\" and \"Say the word Read\", eliciting the emotional state, happy, are illustrated in Fig.  4a  and Fig.  4b , respectively. As demonstrated in Fig.  2 , these phrases are translations of each other.\n\nIn each of Fig.  4a  and Fig.  4b , the audio signals are divided into 2 parts, such as -1) the time-domain audio signal, depicted in the upper portion of each figure, with the x-axis representing time and the y-axis representing the amplitude of the signal, and 2) a frequency vs time plot of the corresponding audio signal, depicted in the lower portion of each figure  [36] . If observed carefully, the amplitude of the Bangla audio signal in Fig.  4a , varies compared to that of the English audio signal in Fig.  4b . The peaks of amplitude for the Bangla audio sample can be segregated into individual words such as \"Poro\", \"shobdo\", \"ti\", and \"bolo\". Similarly, the English audio sample in Fig.  4b , can be separated into individual words such as \"Say\", \"the\", \"word\", and \"Read\". Upon further inspection and keeping in mind that both audio samples were recorded for the happy emotion, it can be discerned that there are intermittent signal gaps for the Bangla audio sample (Fig.  4a ). However, such manifestation cannot be inferred for the English audio sample (Fig.  4b ).\n\nEssentially, in the English language, the sentence pattern is set asthe subject, then the verb, and finally the object  [43]  whereas, in Bangla, it follows the patternthe subject, then the object, and finally the verb  [44] . Furthermore, from the verbatim translation of Bangla to English words in Fig.  2 , it is evident that both languages adhere to a different sentence pattern. The variation in the speech signals for English and Bangla languages is mainly due to these nuances. However, as mentioned earlier, since we are only using prosodic features, which are language-independent  [21] ,  [22] , the style of utterance does not matter, and these nuances do not hamper the classification of emotions from varying speech signals.\n\nConsidering the two prosodic features, pitch standard deviation and intensity for the emotional states, happy and disgust, 2 feature distribution plots, combining these features for all the 3 datasets (BESS, EESS, and E-TESS), are depicted in Fig.  5a  and Fig.  5b , respectively. In the feature distribution plot for the happy emotion (Fig.  5a ), the features for the 3 datasets appear to be clustered together. This observation further clarifies that the prosodic features are independent of the nuances of language, and therefore, it may be stated, \"for Bangla and English languages, the emotional state 'happy' is language and speaker (native/non-native) independent\". On the other hand, from the feature distribution plot of the \"disgust\" emotion (Fig.  5b ), the two prosodic features (pitch standard deviation and intensity) for the E-TESS dataset do not overlap with those of the BESS and the EESS datasets whereas, they appear to be overlapping for latter two datasets. Therefore, from this illustration, it may be surmised, \"for Bangla and English languages, the emotional state 'disgust' is language and speaker (native/non-native) dependent\". The validity of these two statements will be further investigated in Section 5.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Classifier",
      "text": "Support Vector Machine (SVM)  [32]  is one of the conventional classifiers deployed in SER Systems  [3] ,  [6] ,  [18] ,  [34] ,  [39] ,  [45] -  [48] . The models that deploy SVM classifier, designate new training examples to any one category, making this binary linear classifier, a non-probabilistic one, in the process. Using a phenomenon known as kernel trick, SVMs can effectively execute non-linear classifications alongside linear ones. As mentioned earlier, the language-independent vocal features  [21]  that are selected for SER, have a better recognition rate when SVM is used  [15] ,  [22] . Conventionally known as a separating hyperplane, a Support Vector Machine may be denoted as a discriminative classifier. Nevertheless, we used various traditional SER classifiers, such as -HMM, GMM, ANN, and kNN  [3]  for evaluating their performance on the 3 datasets (BESS, EESS, E-TESS). As depicted in Table  1 , the average emotion classification rate using an SVM classifier supersedes the other traditional classifiers (HMM, GMM, kNN, and ANN) of SER in all the datasets. Therefore, in this study, we adopted a generic SVM kernel analogous to the classifiers deployed in SER to generate a non-linear hyperplane for recognizing the 6 emotional states (happy, angry, neutral, sad, disgust, and fear). Additionally, we avoided using any deep learning-based classifiers due to their overfitting complexities and preliminary processing of the ESSs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Result Analysis",
      "text": "As mentioned earlier, each of the 3 datasets, namely, BESS, EESS, and E-TESS had a total of 300 audio speech samples, i.e., 50 audio samples for each of the 6 emotions. Considering these datasets, 3 types of experiments were conducted for investigating the language independent nature of SER, and eventually unearthing other disparities corresponding to emotions and native/non-native speakers, such as -1) on individual speech sets, 2) on integrated speech sets, and 3) by training the model using one speech set and testing it with a different speech set. We elaborate on these 3 experiments in the following sub-sections.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment 1: Individual Speech Set",
      "text": "In this experiment, the SVM classifier was trained and tested on each of the 3 datasets individually, with an 80-20, train to test split of the 300 audio samples (240 training and 60 testing samples). The overall emotion recognition rate across the datasets BESS, EESS, and E-TESS were recorded to be 88.3%, 85%, and 93.3%, respectively. The average performance of the 6 emotional states happy, angry, neutral, sad, disgust, and fear were 83.3%, 90%, 96.7%, 93.3%. 86.7%, and 83.3%, respectively. From this result, it can be inferred that SER works well only when one language is involved. However, for EESS, the model has a lower recognition rate compared to E-TESS. The recognition rates of the SVM model for individual ESS and different emotions are summarized in Table  2 . It is evident from the literature  [20]  that when using a second language, speakers are prone to feel less strongly due to fewer recollections and deep-rooted memories. Therefore, a probable reason for a lower recognition rate of the model for EESS may be attributed to the fact that this dataset was developed by the 11 Bangle native speakers, who despite being fluent in English, could not express their emotions in English as effectively as they could have in Bangla. For IESS-1, with BESS and EESS, having individual recognition rate of 88.3% and 81.7%, respectively, an overall accuracy of 85% was observed. On the other hand, IESS-2 had an overall classification rate of 83.3% while each of the datasets, BESS and E-TESS, had a performance rate of 75% and 91.7%, respectively. However, the classification rate of this experiment decreased with respect to the first one. The recognition rates of the SVM classifier for the IESS-1 and IESS-2, are summarized in Table  3  and Table 4 , respectively.\n\nSince both datasets, BESS and EESS, were developed by native Bangla speakers, the higher overall emotion recognition rate for IESS-1 compared to that for IESS-2, perhaps suggests that non-native speakers tend to express their emotions in English, likewise their native tongue. Furthermore, the higher recognition rate for BESS compared to that for EESS may be attributed to the native speaker's inability to naturally express their emotions in English  [20] . Again, since the datasets, BESS and E-TESS, were developed by Bangla and English native speakers, respectively, higher recognition rate for E-TESS compared to that for BESS leads us to believe that there may be certain differences in language, governing how different emotions are expressed.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiment 3: Distinct Speech Set For Training And Testing",
      "text": "Considering the 3 datasets (BESS, EESS, E-TESS), the final experiment involved training the SVM classifier by one speech set and separately testing it with the remaining two. For instance, as illustrated in Table  5 , the model was firstly trained using BESS, followed by separately testing it with the remaining datasets (EESS and E-TESS). Similarly, other combinations of the datasets were considered, and emotion recognition rates were recorded.\n\nWith BESS as the training dataset, the model achieved recognition rates of 76.7% and 55%, for EESS and E-TESS datasets, respectively. However, with E-TESS as the training dataset, the recognition rate of the model for both BESS and EESS was 45%.\n\nAs observed from Table  5 , the classification rate of this experiment decreased even further compared to the previous two, which may be credited to the failure of recognizing the emotional states, \"disgust\" and \"fear\" at all, for the following combinations -1) BESS as training, E-TESS as testing, 2) E-TESS as training, EESS as testing, and 3) E-TESS as training, BESS as testing. These findings lead us to believe that the prosodic cues for the emotions, \"disgust\" and \"fear\", may be different in Bangla and English when expressed by their respective native speakers. Furthermore, there might be a possibility that these emotions are influenced by the culture or background of the speaker  [2] ,  [15] . However, the remaining emotional states were moderately recognized for this pair of training and testing datasets. On the other hand, the recognition rate of the model for EESS was recorded to be comparatively higher with BESS as the training set compared to E-TESS (Table  5 ), fairly recognizing all the six emotions, even though the emotions, \"disgust\" and \"fear\" are expressed differently in English and Bangla languages when expressed by their native speakers. This outcome is conclusive because, to reiterate, both BESS and EESS were developed by native Bangla speakers, and from the prior experiment, we deduced that non-native speakers might convey emotions as if they were expressing themselves in their native language. Hence, it is intuitive that this combination of datasets (BESS as training, EESS as testing) will result in a reasonable recognition rate. The result of this experiment also reinforces our claim that the non-native speakers tend to express their emotions in English, likewise their native tongue. These claims are further corroborated in the following sections.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Binary Classification Of Emotions",
      "text": "This experiment includes substantiating the claims formulated for the emotions, \"disgust\" and \"fear\", which were found to have different expressions in Bangla and English languages when spoken by their native speakers. The third experiment is replicated in this instance, which involved training the SVM classifier by one speech set and separately testing it with the remaining two. However, instead of multi-class classification, binary classification for each of the six emotions is carried out. For example, considering the emotion, happy, any instance of any dataset, labelled as either disgust or fear, or any of the remaining emotions, can be equivalently labelled as, not happy. In this manner, for each dataset, there will be only two emotions, for example, \"happy\" -\"not happy\", \"angry\" -\"not angry\", \"neutral\" -\"not neutral\", \"sad\" -\"not sad\", \"disgust\" -\"not disgust\", and \"fear\" -\"not fear\".\n\nEach of the training sets contains an equal number of labelled training samples for the binary emotions. For instance, the emotion, happy, has 50 audio samples and the emotion, not happy, also includes 50 audio samples (5 remaining emotions × 10 audio samples per emotion). This procedure is replicated for the rest of the emotions for each dataset. After this, the SVM model is trained on the binary labelled speech set and tested with a different binary labelled speech set for the same emotion. Testing was conducted with subsets of 30 audio samples from the testing dataset, and the overall recognition rate was recorded. As delineated in Table  6  and Table  7 , the results from this binary classification further substantiate and validate the claims purported in experiment3. The emotions, disgust and fear were completely un-recognized with BESS as the training and EESS as the testing datasets. Furthermore, with E-TESS as the training and either of the two remaining speech sets (BESS and EESS) as the testing datasets, the emotions disgust and fear, were left unrecognized as well. The confusion matrix for the \"disgust\" emotion (with E-TESS as the training and EESS as the testing datasets), as shown in Table  8 , demonstrates that no true positives were recorded for this emotion, in which case the precision and the recall of the model becomes 0. Consequently, the F1-score, a weighted average of precision and recall, will be 0 as well. Therefore, it may be concluded that the emotions disgust and fear are expressed differently in Bangla and English languages. In contrast, these emotions were fairly recognized with BESS and EESS as the training and the testing datasets, respectively, which corroborates the claim that non-native speakers of a language tend to express their emotions, likewise their native tongue. Across all the experiments, the emotional states: happy, angry, neutral, and sad were identified regardless of language and native/non-native speakers. However, the emotions: disgust and fear brought about some discrepancies. Considering the two hypotheses formed in section 3.1, from these suites of experiments, it can be inferred that for Bangla and English languages, the emotional state happy is perhaps language and speaker (native/non-native) independent while the emotional states, disgust and fear are perhaps language and speaker (native/non-native) dependent. Additionally, non-native speakers are found to convey emotions analogous to their expressions in their native language.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we developed 3 datasets namely, Bangla Emotional Speech Set (BESS), English Emotional Speech Set (EESS), and English TESS (E-TESS) to evaluate the language independence of Speech Emotion Recognition (SER), featuring 6 emotions, such as -happy, angry, neutral, sad, disgust, and fear  [1] ,  [2] ,  [15] ,  [20] ,  [23] ,  [31] , in English and Bangla languages through language-independent vocal feature selection. The datasets BESS and EESS were developed by 11 native Bangla speakers and E-TESS was developed as a subset of the Toronto Emotional Speech Set (TESS) while preserving the speech intelligibility functions across the datasets.\n\nWe coordinated 3 experiments, where we deployed SVM for classifying emotions, as it is one of the most widely used classifiers for SER systems. Although the performance of the model varied across the 3 experiments, the reliability of recognizing different emotions was convincing. The results from the first experiment revealed that native speakers tend to express their emotions better in their native language than non-native speakers of that language. From the second and the third experiments, it was observed that non-native speakers of a language have a keen proclivity of expressing their emotions, likewise their native language. Nonetheless, these experiments further point out that there may be certain differences in languages that govern the expression of different emotions.\n\nThis claim was eventually solidified by the end of the third experiment when the emotional states, disgust and fear were revealed to be language and speaker dependent. The factors behind this contrast may be credited to cultural, background, environmental or communal differences. However, this study also demonstrated that the emotional states such as happy, angry, neutral, and sad were moderately recognized irrespective of language and native/non-native speakers. Therefore, we can deduce that there may be language-specific differences for certain, if not all emotions.\n\nThe findings direct us towards the conclusion that SER in Bangla and English is mostly language independent. However, some disparity exists in the emotional states owing to cultural, environmental, or communal factors. Hence, we hope that the community will find the results of this study applicable towards achieving comprehensive control over SER regardless of language. In the future, we intend to extend our analysis of language independence of SER by including more languages, speakers, and language independent features.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , with assistance from 11 native Bangla speakers (64%",
      "page": 2
    },
    {
      "caption": "Figure 1: Construction of Bangla Emotional Speech Set (BESS) and English Emotional Speech Set (EESS) datasets by",
      "page": 3
    },
    {
      "caption": "Figure 1: that a sentence, used in the dataset was, “Say the word",
      "page": 3
    },
    {
      "caption": "Figure 1: It can be discerned",
      "page": 3
    },
    {
      "caption": "Figure 2: that the verbatim translation of Bangla words such as – “Poro”, “shobdo ti”, and “bolo” to its corresponding",
      "page": 3
    },
    {
      "caption": "Figure 2: Verbatim translation of Bangla words to the corresponding English words.",
      "page": 3
    },
    {
      "caption": "Figure 3: Formation of the English TESS (E-TESS) dataset as a subset of the existing Toronto Emotional Speech Set",
      "page": 4
    },
    {
      "caption": "Figure 3: In this manner, 3 datasets were assembled for our experiments, which will be addressed as Bangla Emotional Speech",
      "page": 4
    },
    {
      "caption": "Figure 1: and Fig. 3.",
      "page": 4
    },
    {
      "caption": "Figure 4: a and Fig.4b, respectively. As demonstrated in Fig. 2, these",
      "page": 4
    },
    {
      "caption": "Figure 4: a and Fig. 4b, the audio signals are divided into 2 parts, such as – 1) the time-domain audio signal,",
      "page": 4
    },
    {
      "caption": "Figure 4: a, varies compared to that of the English audio",
      "page": 4
    },
    {
      "caption": "Figure 4: b. The peaks of amplitude for the Bangla audio sample can be segregated into individual words such as",
      "page": 4
    },
    {
      "caption": "Figure 4: b, can be separated into individual words",
      "page": 4
    },
    {
      "caption": "Figure 4: a). However, such manifestation cannot be inferred for the English audio sample (Fig. 4b).",
      "page": 4
    },
    {
      "caption": "Figure 2: , it is evident that both languages adhere to a different sentence",
      "page": 5
    },
    {
      "caption": "Figure 5: a and Fig. 5b, respectively. In the feature distribution plot for the happy emotion (Fig. 5a), the features for the 3 datasets",
      "page": 5
    },
    {
      "caption": "Figure 5: b), the two prosodic features (pitch standard deviation and intensity) for the E-TESS dataset do not overlap",
      "page": 5
    },
    {
      "caption": "Figure 4: Audio samples for “happy” Emotion for – (a) the phrase “Poro shobdo ti bolo” of the Bangla Emotional Speech",
      "page": 5
    },
    {
      "caption": "Figure 5: Feature (Pitch Standard deviation vs Intensity) distribution plot of the datasets BESS, EESS, and E-TESS for – (a)",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , the average emotion classification rate",
      "page": 6
    },
    {
      "caption": "Table 1: Recognition Rate of different SER classifiers for Bangla (BESS), English (EESS) and English TESS (E-TESS)",
      "page": 6
    },
    {
      "caption": "Table 2: It is evident from the literature [20] that when using a second",
      "page": 7
    },
    {
      "caption": "Table 2: Experiment 1 – Recognition rate of SVM for the individual Emotional Speech Set (ESS).",
      "page": 7
    },
    {
      "caption": "Table 3: and Table 4, respectively.",
      "page": 7
    },
    {
      "caption": "Table 3: Recognition Rate of SVM for the Integrated Emotional Speech Set – 1 (IESS-1), consisting of BESS and EESS.",
      "page": 7
    },
    {
      "caption": "Table 4: Recognition rate of SVM for the Integrated Emotional Speech Set – 2 (IESS-2), consisting of BESS and E-",
      "page": 7
    },
    {
      "caption": "Table 5: , the model was firstly trained",
      "page": 8
    },
    {
      "caption": "Table 5: , the classification rate of this experiment decreased even further compared to the previous",
      "page": 8
    },
    {
      "caption": "Table 5: Experiment 3 – Recognition rate of SVM when using distinct Emotional Speech Set for training and testing.",
      "page": 8
    },
    {
      "caption": "Table 5: ), fairly recognizing all the six emotions, even though the emotions, “disgust”",
      "page": 8
    },
    {
      "caption": "Table 6: and Table 7, the results from this binary classification further substantiate and validate the",
      "page": 9
    },
    {
      "caption": "Table 6: Experiment 3.1 – Recognition rate of SVM in Binary Classification of the emotions – happy, angry, and neutral.",
      "page": 9
    },
    {
      "caption": "Table 7: Experiment 3.1 – Recognition rate of SVM in Binary Classification of the emotions – sad, disgust, and fear.",
      "page": 9
    },
    {
      "caption": "Table 8: Confusion Matrix for Disgust Emotion (Training Dataset: English TESS, Testing Dataset: English).",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Analysis of speech features for emotion detection: A review",
      "authors": [
        "R Sudhakar",
        "M Anil"
      ],
      "year": "2015",
      "venue": "Proceedings -1st International Conference on Computing, Communication, Control and Automation",
      "doi": "10.1109/ICCUBEA.2015.135"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from Assamese speeches using MFCC features and GMM classifier",
      "authors": [
        "A Kandali",
        "A Routray",
        "T Basu"
      ],
      "year": "2008",
      "venue": "Emotion recognition from Assamese speeches using MFCC features and GMM classifier",
      "doi": "10.1109/TENCON.2008.4766487"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oğuz"
      ],
      "year": "2020",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2019.12.001"
    },
    {
      "citation_id": "4",
      "title": "A survey of affect recognition methods: Audio, visual and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2007",
      "venue": "Proceedings of the 9th International Conference on Multimodal Interfaces, ICMI'07",
      "doi": "10.1145/1322192.1322216"
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "R Picard",
        "Affective Computing"
      ],
      "year": "2000",
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "Machine learning approach for emotion recognition in speech",
      "authors": [
        "M Gjoreski",
        "H Gjoreski",
        "A Kulakov"
      ],
      "year": "2014",
      "venue": "Inform",
      "doi": "10.31449/inf.v38i4.719"
    },
    {
      "citation_id": "7",
      "title": "Factors in the recognition of vocally expressed emotions: A comparison of four languages",
      "authors": [
        "M Pell",
        "S Paulmann",
        "C Dara",
        "A Alasseri",
        "S Kotz"
      ],
      "year": "2009",
      "venue": "J. Phon",
      "doi": "10.1016/j.wocn.2009.07.005"
    },
    {
      "citation_id": "8",
      "title": "Automatic emotion recognition using prosodic parameters",
      "authors": [
        "I Luengo",
        "E Navas",
        "I Hernáez",
        "J Sánchez"
      ],
      "year": "2005",
      "venue": "9th European Conference on Speech Communication and Technology",
      "doi": "10.21437/interspeech.2005-324"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "A Nogueiras",
        "A Moreno",
        "A Bonafonte",
        "J Mariño"
      ],
      "year": "2001",
      "venue": "EUROSPEECH 2001 -SCANDINAVIA -7th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "10",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "11",
      "title": "The relationship between task difficulty and emotion in online computer programming tutoring",
      "authors": [
        "J Wiggins",
        "J Grafsgaard",
        "K Boyer",
        "E Wiebe",
        "J Lester"
      ],
      "year": "2014",
      "venue": "The relationship between task difficulty and emotion in online computer programming tutoring",
      "doi": "10.1145/2538862.2544298"
    },
    {
      "citation_id": "12",
      "title": "Determinants and consequences of employee displayed positive emotions",
      "authors": [
        "W.-C Tsai"
      ],
      "year": "2001",
      "venue": "J. Manage",
      "doi": "10.1177/014920630102700406"
    },
    {
      "citation_id": "13",
      "title": "EmoVoice -A framework for online recognition of emotions from voice",
      "authors": [
        "T Vogt",
        "E André",
        "N Bee"
      ],
      "year": "2008",
      "venue": "LNCS",
      "doi": "10.1007/978-3-540-69369-7_21"
    },
    {
      "citation_id": "14",
      "title": "Recognition of emotions in interactive voice response systems",
      "authors": [
        "S Yacoub",
        "S Simske",
        "X Lin",
        "J Burns"
      ],
      "year": "2003",
      "venue": "EUROSPEECH 2003 -8th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "15",
      "title": "A Study of Language and Classifierindependent Feature Analysis for Vocal Emotion Recognition",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "A Study of Language and Classifierindependent Feature Analysis for Vocal Emotion Recognition"
    },
    {
      "citation_id": "16",
      "title": "Evidence for cultural dialects in vocal emotion expression: Acoustic classification within and across five nations",
      "authors": [
        "P Laukka",
        "D Neiberg",
        "H Elfenbein"
      ],
      "year": "2014",
      "venue": "Emotion",
      "doi": "10.1037/a0036048"
    },
    {
      "citation_id": "17",
      "title": "Recognizing emotion from speech based on age and gender using hierarchical models",
      "authors": [
        "F Shaqra",
        "R Duwairi",
        "M Al-Ayyoub"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2019.04.009"
    },
    {
      "citation_id": "18",
      "title": "How aging affects the recognition of emotional speech",
      "authors": [
        "S Paulmann",
        "M Pell",
        "S Kotz"
      ],
      "year": "2008",
      "venue": "Brain Lang",
      "doi": "10.1016/j.bandl.2007.03.002"
    },
    {
      "citation_id": "19",
      "title": "Handbook of human-computer interaction",
      "year": "2014",
      "venue": "Handbook of human-computer interaction"
    },
    {
      "citation_id": "20",
      "title": "Influences of languages in speech emotion recognition: A comparative study using Malay, English and Mandarin languages",
      "authors": [
        "R Rajoo",
        "C Aun"
      ],
      "year": "2016",
      "venue": "ISCAIE 2016 -2016 IEEE Symposium on Computer Applications and Industrial Electronics",
      "doi": "10.1109/ISCAIE.2016.7575033"
    },
    {
      "citation_id": "21",
      "title": "A neural network approach for human emotion recognition in speech",
      "authors": [
        "M Bhatti",
        "Y Wang",
        "L Guan"
      ],
      "year": "2004",
      "venue": "Proceedings -IEEE International Symposium on Circuits and Systems",
      "doi": "10.1109/iscas.2004.1329238"
    },
    {
      "citation_id": "22",
      "title": "Exploring Language-Independent Emotional Acoustic Features via Feature Selection",
      "authors": [
        "A Shaukat",
        "K Chen"
      ],
      "year": "2010",
      "venue": "Exploring Language-Independent Emotional Acoustic Features via Feature Selection"
    },
    {
      "citation_id": "23",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "Proceedings -IEEE International Conference on Multimedia and Expo",
      "doi": "10.1109/ICME.2003.1220939"
    },
    {
      "citation_id": "24",
      "title": "Evolutionary feature generation in speech emotion recognition",
      "authors": [
        "B Schuller",
        "S Reiter",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "2006 IEEE International Conference on Multimedia and Expo, ICME 2006 -Proceedings",
      "doi": "10.1109/ICME.2006.262500"
    },
    {
      "citation_id": "25",
      "title": "Emotions in speech -Experiments with prosody and quality features in speech for use in categorical and dimensional emotion recognition environments",
      "authors": [
        "M Borchert",
        "A Düsterhöft"
      ],
      "year": "2005",
      "venue": "Proceedings of 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering, IEEE NLP-KE'05",
      "doi": "10.1109/NLPKE.2005.1598724"
    },
    {
      "citation_id": "26",
      "title": "Enhancing emotion recognition from speech through feature selection",
      "authors": [
        "T Kostoulas",
        "T Ganchev",
        "A Lazaridis",
        "N Fakotakis"
      ],
      "year": "2010",
      "venue": "Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)",
      "doi": "10.1007/978-3-642-15760-8_43"
    },
    {
      "citation_id": "27",
      "title": "Sound processing features for speaker-dependent and phrase-independent emotion recognition in Berlin database",
      "authors": [
        "C Anagnostopoulos",
        "E Vovoli"
      ],
      "year": "2009",
      "venue": "Inf. Syst. Dev. Towar. a Serv. Provis. Soc",
      "doi": "10.1007/B137171_43"
    },
    {
      "citation_id": "28",
      "title": "The interplay of prosodic cues in the L2: How intonation, rhythm, and speech rate in speech by Spanish learners of Dutch contribute to L1 Dutch perceptions of accentedness and comprehensibility",
      "authors": [
        "L Van Maastricht",
        "T Zee",
        "E Krahmer",
        "M Swerts"
      ],
      "year": "2021",
      "venue": "Speech Commun",
      "doi": "10.1016/j.specom.2020.04.003"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2013",
      "venue": "Int. J. Speech Technol",
      "doi": "10.1007/S10772-012-9172-2"
    },
    {
      "citation_id": "30",
      "title": "Toronto emotional speech set (TESS) Collection",
      "authors": [
        "K Dupuis"
      ],
      "year": "2015",
      "venue": "Toronto emotional speech set (TESS) Collection"
    },
    {
      "citation_id": "31",
      "title": "Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set",
      "authors": [
        "K Dupuis",
        "Kathleen Pichora-Fuller"
      ],
      "year": "2011",
      "venue": "Canadian Acoustics -Acoustique Canadienne"
    },
    {
      "citation_id": "32",
      "title": "Support-vector networks",
      "authors": [
        "C Cortes",
        "V Vapnik"
      ],
      "year": "1995",
      "venue": "Mach. Learn",
      "doi": "10.1007/bf00994018"
    },
    {
      "citation_id": "33",
      "title": "An expanded test for speech discrimination utilizing CNC monosyllabic words",
      "authors": [
        "T Tillman",
        "R Carhart"
      ],
      "year": "1966",
      "venue": "Tech. Rep. SAM-TR"
    },
    {
      "citation_id": "34",
      "title": "Recognition of emotions from speech using excitation source features",
      "authors": [
        "S Koolagudi",
        "S Devliyal",
        "B Chawla",
        "A Barthwaf",
        "K Rao"
      ],
      "year": "2012",
      "venue": "Procedia Engineering",
      "doi": "10.1016/j.proeng.2012.06.394"
    },
    {
      "citation_id": "35",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif. Intell. Rev",
      "doi": "10.1007/s10462-012-9368-5"
    },
    {
      "citation_id": "36",
      "title": "Praat: Doing Phonetics by Computer",
      "year": "2011",
      "venue": "Ear Hear",
      "doi": "10.1097/aud.0b013e31821473f7"
    },
    {
      "citation_id": "37",
      "title": "Algorithms and devices for pitch determination of speech signals",
      "authors": [
        "W Hess"
      ],
      "year": "1982",
      "venue": "Phonetica",
      "doi": "10.1159/000261664"
    },
    {
      "citation_id": "38",
      "title": "Audio-vocal responses of vocal fundamental frequency and formant during sustained vowel vocalizations in different noises",
      "authors": [
        "S Lee",
        "T Hsiao",
        "G Lee"
      ],
      "year": "2015",
      "venue": "Hear. Res",
      "doi": "10.1016/j.heares.2015.02.005"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "Y Pan",
        "P Shen",
        "L Shen"
      ],
      "year": "2012",
      "venue": "Int. J. Smart Home",
      "doi": "10.30534/ijeter/2020/43842020"
    },
    {
      "citation_id": "40",
      "title": "Psychoacoustic abilities as predictors of vocal emotion recognition",
      "authors": [
        "E Globerson",
        "N Amir",
        "O Golan",
        "L Kishon-Rabin",
        "M Lavidor"
      ],
      "year": "2013",
      "venue": "Attention, Perception, Psychophys",
      "doi": "10.3758/s13414-013-0518-x"
    },
    {
      "citation_id": "41",
      "title": "Developmental change and cross-domain links in vocal and musical emotion recognition performance in childhood",
      "authors": [
        "R Allgood",
        "P Heaton"
      ],
      "year": "2015",
      "venue": "Br. J. Dev. Psychol",
      "doi": "10.1111/bjdp.12097"
    },
    {
      "citation_id": "42",
      "title": "The development of emotion recognition from facial expressions and non-linguistic vocalizations during childhood",
      "authors": [
        "G Chronaki",
        "J Hadwin",
        "M Garner",
        "P Maurage",
        "E Sonuga-Barke"
      ],
      "year": "2015",
      "venue": "Br. J. Dev. Psychol",
      "doi": "10.1111/bjdp.12075"
    },
    {
      "citation_id": "43",
      "title": "Subject-verb-object -Wikipedia",
      "year": "2022",
      "venue": "Subject-verb-object -Wikipedia"
    },
    {
      "citation_id": "44",
      "title": "Subject-object-verb -Wikipedia",
      "year": "2022",
      "venue": "Subject-object-verb -Wikipedia"
    },
    {
      "citation_id": "45",
      "title": "Dimensionality reduction for emotional speech recognition",
      "authors": [
        "P Fewzee",
        "F Karray"
      ],
      "year": "2012",
      "venue": "Proceedings -2012 ASE/IEEE International Conference on Privacy, Security, Risk and Trust and 2012 ASE/IEEE International Conference on Social Computing",
      "doi": "10.1109/SocialCom-PASSAT.2012.83"
    },
    {
      "citation_id": "46",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech Commun",
      "doi": "10.1016/j.specom.2011.01.011"
    },
    {
      "citation_id": "47",
      "title": "On the impact of non-speech sounds on speaker recognition",
      "authors": [
        "A Janicki"
      ],
      "year": "2012",
      "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics",
      "doi": "10.1007/978-3-642-32790-2_69"
    },
    {
      "citation_id": "48",
      "title": "Recognition of human emotion from a speech signal based on plutchik's model",
      "authors": [
        "D Kamińska",
        "A Pelikant"
      ],
      "year": "2012",
      "venue": "Int. J. Electron. Telecommun",
      "doi": "10.2478/v10177-012-0024-4"
    }
  ]
}