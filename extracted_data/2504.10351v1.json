{
  "paper_id": "2504.10351v1",
  "title": "Multimodal Representation Learning Techniques For Comprehensive Facial State Analysis",
  "published": "2025-04-14T16:00:57Z",
  "authors": [
    "Kaiwen Zheng",
    "Xuri Ge",
    "Junchen Fu",
    "Jun Peng",
    "Joemon M. Jose"
  ],
  "keywords": [
    "Facial Representation Learning",
    "MFA Dataset",
    "Face Foundation Model",
    "Efficient Fine tuning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal foundation models have significantly improved feature representation by integrating information from multiple modalities, making them highly suitable for a broader set of applications. However, the exploration of multimodal facial representation for understanding perception has been limited. Understanding and analyzing facial states, such as Action Units (AUs) and emotions, require a comprehensive and robust framework that bridges visual and linguistic modalities. In this paper, we present a comprehensive pipeline for multimodal facial state analysis. First, we compile a new Multimodal Face Dataset (MFA) by generating detailed multilevel language descriptions of face, incorporating Action Unit (AU) and emotion descriptions, by leveraging GPT-4o. Second, we introduce a novel Multilevel Multimodal Face Foundation model (MF 2 ) tailored for Action Unit (AU) and emotion recognition. Our model incorporates comprehensive visual feature modeling at both local and global levels of face image, enhancing its ability to represent detailed facial appearances. This design aligns visual representations with structured AU and emotion descriptions, ensuring effective crossmodal integration. Third, we develop a Decoupled Fine-Tuning Network (DFN) that efficiently adapts MF 2 across various tasks and datasets. This approach not only reduces computational overhead but also broadens the applicability of the foundation model to diverse scenarios. Experimentation show superior performance for AU and emotion detection tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Face representation learning plays an important role in automatic facial state analysis, such as expression recognition  [1]  and medical diagnosis  [2] , and has received extensive attention in recent decades. Its main goal is to extract facial appearance representations for face perception and recognition. However, face representation learning is very challenging due to the complex and diverse appearance details of facial texture and muscle states.\n\nEarlier studies  [3]  extracted facial representations from global images using convolutional neural networks (CNNs) to predict facial states such as emotions. For example, Burkert et al.  [4]  designed a deep CNN for facial expression recognition that uses convolutional layers to capture hierarchical features. While such global representations effectively encode coarsegrained texture and muscle combinations, they often lack * Corresponding author.\n\nthe fine-grained localization needed for many downstream tasks. Other works  [5]  have focused on facial muscle analysis through Action Unit (AU) recognition, with methods such as  [6] ,  [7]  proposing local-global relational networks that accurately locate AU-specific regions via landmark detection. Although both global and local face representations have been successfully applied in tasks like AU recognition  [8]  and emotion recognition  [9] , they still do not provide explicit facial feature explanations-for instance, linguistic descriptions-that could further enhance interpretability.\n\nRecently, multimodal joint representation learning has achieved notable success in various applications such as health assessment  [10]  and driver fatigue detection  [11] . However, its impact on facial state analysis remains limited due to the complexity of facial appearance features and privacy concerns. On one hand, generating high-quality multimodal face annotations is challenging. Although pre-trained Multimodal Large Language Models (MLLMs) like CoCa  [12]  and Blip  [13]  can produce image descriptions in diverse scenarios, no unified approach exists for generating optimal facial state descriptions. Methods such as Exp-BLIP  [14]  and VL-FAU  [15]  use LLMs to generate general face descriptions; however, they either lack sufficiently detailed AU annotations or omit nuanced emotion reasoning. On the other hand, effectively aligning multi-level multimodal face representations-integrating both local and global visual features with corresponding AU and emotion language representations-remains underexplored. For instance, Exp-BLIP  [14]  employs coarse-grained image-text pairs for expression captioning, while VL-FAU  [15]  relies on fixedform AU descriptions that limit further improvement in visual representation.\n\nIn this paper, we address two key challenges in multimodal face representation learning: (i) developing robust, multilevel face annotation methods that provide language-image pairs at various granularities (e.g., detailed AU and emotion context descriptions), and (ii) effectively aligning these multimodal representations to enhance feature extraction.\n\nTo this end, we propose a comprehensive pipeline consisting of a novel Multilevel Multimodal Facial Foundation model (MF 2 ) and an efficient Decoupled Fine-Tuning Network (DFN) for downstream tasks. Specifically, we first leverage the pre-trained MLLM GPT-4o  [16]  to generate fine-grained AU descriptions and emotion reasoning for face images. Next, the MF 2 model integrates local and global visual features with detailed language annotations to yield explicit and comprehensive facial representations, serving as a foundation for tasks such as FAU and emotion recognition. Finally, the DFN enables efficient adaptation of MF 2 , significantly reducing training overhead while maintaining performance.\n\nThe contributions of this paper are as follows:\n\n• To enable comprehensive face representation learning, we compile a new multimodal face dataset with high-quality, multilevel language annotations, including descriptions for various AU and emotion reasoning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Multimodel Facial Annotation",
      "text": "To address the limitations of existing facial datasets, we constructed a new Multimodal Facial dataset (MFA). Figure  1  illustrates the specific steps we followed to reconfigure the dataset, utilizing ground truth labels (emotion and AU annotation) and carefully designed prompts to generate reasonable, high-quality, multilevel facial language descriptions through GPT-4o  [16] . In this section, we introduce the collection process of the dataset, the prompt strategies, and an overview of the MFA dataset.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset Construction",
      "text": "Creating a new dataset from scratch was deemed impractical due to the significant costs and complexities involved. Instead, we opted to use an existing dataset as our foundation. To identify a suitable dataset, we defined two key criteria:\n\n• The dataset must include both Action Unit (AU) and Emotion annotations.\n\n• Each image should have an individual emotion label. After a comprehensive comparison of available datasets, as summarized in Table  I , we found that only the Aff-Wild2 dataset satisfied these requirements  [17] . Consequently, we selected Aff-Wild2 as the base for our work. Data Filtering: To construct a balanced dataset, we began by filtering the Aff-Wild2 dataset to include only images with both Action Units (AUs) and Emotion annotations. This filtering step also ensured emotion class balance across the",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Gpt-4O",
      "text": "Emotion label list: Ground Truth",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Gpt4O Prompt",
      "text": "The person in the image appears to be displaying an expression of surprise.\n\nThe wide-open eyes and slightly parted lips suggest a reaction to something unexpected or astonishing. The raised eyebrows also contribute to the look of surprise, indicating that the person is reacting to something unforeseen or shocking.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Gpt-4O",
      "text": "AU label list: Ground Truth",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Gpt4O Prompt",
      "text": "The image shows a person with a facial expression that can be broken down into several Facial Action Coding System (FACS) Action Units (AUs). Here are the Aus that appear to be present:  dataset. Following this process, the refined dataset was split into training and validation sets. Given the video-based nature of the Aff-Wild2 dataset, we maintained a balance in both the number of videos and individual images when dividing the data into these subsets.\n\nGPT-4o Prompt Strategy: Our objective is to linguistically annotate each image for Action Units (AUs) and emotion, leveraging the existing annotations effectively. Textual descriptions are incorporated to bridge the gap between annotations and model understanding, guiding emotion and AU detection models by highlighting the nuanced differences in these units. This approach helps the models capture subtle variations, improving overall classification accuracy. To ensure optimal output quality, we experimented with various generation methods and prompt designs. Ultimately, GPT-4o was selected for its nuanced understanding and adaptability. Our structured prompt framework, designed for generating highquality captions, consists of three key components: task setup, output formatting, and signal specification. This structured approach enables the model to fully comprehend the task, ensuring consistent and detailed outputs across diverse captioning scenarios. Supplementary materials show more prompt design details.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Dataset Consolidation And Summarisation",
      "text": "The dataset comprising a total of 34,696 images extracted from 151 videos. These images have been split into a training",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multilevel Multimodal Face Foundation Model -Mf 2",
      "text": "Overview. MF 2 consists of two main Q-former-based visual-language branches, i.e. a global-level visual encoder with reasoning-based emotion language alignment (Emo-VL) and a local-level visual encoder with fine-grained AU language alignment (AU-VL). The former uses global contexts and situational reasoning in emotional language to assist and improve the ability and discriminability of global face visual representation. The latter further uses each AU language description to accurately improve the visual representation of each muscle area, and improves the fine-grained face representation. During training, we leverage linguistic descriptions to guide the model in identifying situational cues.\n\nEmo-VL. Following BLIP-2  [31] , we model the global face visual representation and emotion description language representation in a unified Q-former, as shown in Figure  2  (a). Emo-VL employs a pre-trained ViT model  [32]  to extract the global face feature V g and then it is input into a Q-formerbased multimodal alignment module to align with the emotion language representation S e from a pre-trained BERT  [33]  for the final recognition tasks. Specifically, the Q-former-based global alignment module contains a visual encoder and a language encoder. The visual encoder consists of N transformerbased blocks, each containing a Self-Attention layer (SA), a Cross-Attention layer (CA), and a Feedforward Network (F F N ). The language encoder also consists of N blocks, where each contains a self-attention layer and an FFN. Due to the characteristics of Q-former  [31] , an additional crossattention layer with the learned queries (Q g ) is contained in the visual encoder. Similar with BLIP-2  [31] , we utilize the Image-Text Contrastive Learning loss (L ITC ), Image-grounded Text Generation loss (L ITG ) and Image-Text Matching loss (L ITM ) to optimise the visual-language alignment and recognition of face states, such as FAU activation state and emotion category, by corresponding task classifiers. The overall working flow of Emo-VL is formulated as:\n\n(1)\n\nThe object functions are followed as:\n\nwhere M is the size of image-text pairs. w i is the target word to predict in text generation or masked language modeling tasks. τ is a temperature parameter. AU-VL. Emo-VL improves the ability to represent global faces by aligning the global face feature with the emotion language, which explicitly contains global emotion reasoning information. To further compensate for the lack of fine-grained face representation, we propose local face representation enhancement based on the positioning accuracy advantage of Action Units (AUs), as shown in Figure  2  (b), named AU-VL. Similarly, we use the AU language description to align with the local AU visual representation in a Q-former-based module to improve its multimodal representation capability. The local AU visual representations are extracted based on the detected face landmarks from a pre-trained landmark detector  [34] . The structure of the local Q-former-based alignment module is the same as Emo-VL. Specifically, to extract the precise AU features in a face image, we use a pre-trained landmark detector  [34]  to localise the AU positions and extract the corresponding representations V AU = {V a 1 , ..., V a n } from ViT-based visual features. All AU captions are embedded by the BERT  [33]  as S AU ={S a 1 , ..., S a n }. After that, we also employ the Q-former-based AU alignment module to align the local AU visual features and fine-grained AU language features by the same objective functions in Emo-VL. Note that, the visual encoder and language encoder in Q-former alignment are shared for different AUs to save parameters. Finally, we obtain the local AU representations V a and their corresponding detailed language representations Ŝa .\n\nDuring the multilevel visual-language joint learning, we use the cross entropy loss function  [35]  to optimize an AU recognizer and an emotion recognizer respectively for the final facial state analysis. Thus, we obtain a face foundation model for FAU recognition and emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Efficient Decoupled Fine-Tuning Network -Dfn",
      "text": "As the foundational backbone of MF 2 , the Q-former faces two primary limitations: (1) its transformer-based architecture is computationally expensive, and (2) to mitigate this cost, it employs shared Self-Attention and FFN modules for multimodal contrastive learning. While this design may enhance cross-modal interaction, it compromises the unique representation capability of individual modalities. To address these challenges and improve the generalization of the proposed foundation model MF 2 , we propose a simple yet effective Decoupled Fine-Tuning Network (DFN) for pre-trained MF 2 built entirely with lightweight adapters. The detailed framework is shown in Figure  2 (c ). Inspired by the advanced Side Adapter paradigm  [37] ,  [38] , which outperforms traditional adapters and LoRA in efficiency  [39] ,  [40] , DFN decouples the shared modules into distinct side adapter pathways. By incorporating unique modality-specific adjustments through two independent side adapters, DFN effectively mitigates interference between modalities while significantly reducing computational overhead. Specifically, DFN is parallel to each modality branch in MF 2 and performs decoupling fine-tuning. Therefore, there are 4N DFN cells in total, each of which consists of a downsampling and upsampling layer composed of a fully connected layer, and is connected using an activation function. When fine-tuning the DFN, we freeze the MF 2 backbone and only update the parameters of DFN for the new task, under the optimization of new task objective functions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Experimental Settings",
      "text": "Implemental Details. All details are shown in supplementary materials. Evaluation Metrics. The evaluation metrics include the F1 score for facial Action Unit (AU) detection and the classification accuracy for face emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Experimental Results",
      "text": "Compared Methods. We compare the proposed MF 2 and its DFN-based fine-tuning model with three baselines for AU recognition in Table II and two baselines for emotion recognition in Table  III . For AU recognition, it contains ME-GraphAU  [36] , Exp-BLIP  [14] , VL-FAU  [15] . For Emotion recognition, HSEomtion  [41]  and Exp-BLIP  [14]  are compared with our models. More baseline model details are shown in supplementary materials. Performance of FAU Recognition. Table  II  highlights the performance of various models on the MFA dataset for FAU recognition. Among the baseline models, VL-FAU achieves the highest average performance with an F1 score of 48.19%. However, both versions of our proposed MF 2 model significantly outperform these baselines. Specifically, the MF 2 (Pre-Train) model achieves an average F1 score of 50.77%, while MF 2 (Fine-Tuning) further improves to 53.35%, representing a substantial margin of +5.16% over the best-performing baseline (VL-FAU). Performance of Emotion Recognition. Table  III  presents the performance of various models on the MFA dataset for emotion recognition. Our MF 2 (Pre-Train) model achieves an average accuracy of 83.48%, and the MF 2 (Fine-Tuning) model further boosts performance to 84.40%, demonstrating a notable margin of +2.26% over the best-performing baseline Exp-BLIP  [14] . These results, combined with the recognition",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Ablation Study",
      "text": "To demonstrate the effectiveness of the proposed modules, we conducted extensive ablation studies. We show how each component influences the overall performance of the MF 2 model. Table  IV   The ablation studies confirm the effectiveness of the MF 2 model's design, highlighting the critical role of each component in achieving state-of-the-art performance while ensuring computational and parameter efficiency.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper presented a novel multimodal facial representation learning pipeline, integrating image and text modalities to enhance AU and emotion recognition. We compiled the MFA dataset with high-quality detailed AU and emotional description linguistically. The proposed foundation model MF 2 effectively combines global (Emo-VL) and local (AU-VL) visual-language representations with emotion and AU language alignment learning, ensuring comprehensive and detailed facial feature enhancement. Additionally, our Decoupled Fine-Tuning Network (DFN) enables efficient task-specific fine-tuning, reducing computational cost and achieving superior performance. Experimental results validated the effectiveness of our multimodal MF 2 model and its efficient fine-tuning strategy (DFN), outperforming state-of-the-art methods while demonstrating a reduction in training time. Future work will focus on exploring advanced multimodal representations and improving relational reasoning in face analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Gpt-4O Prompt Design",
      "text": "We designed a three-stage GPT-4o prompt (Initial Setup, Output Formpt and Output Signal) to generate the three highquality descriptive captions we needed: the AU caption, the Emo caption and the Key AU caption. Below, we discuss the rationale and considerations behind the prompt structures used.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Note:",
      "text": "The prompt examples used in the introductory architecture section are all emotion prompt examples.\n\nYou are currently acting as an emotion description expert and your ability is to recognize a person's expression, and their possible elicited hidden emotions based on an image of their face, please answer the questions according to this example:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Initial Setup",
      "text": "Fig.  3 : Emotion Initial Setup Prompt Initial Setup. In this step Figure  3 , the Prompt model is assigned a specific role relevant to the task. For example, the model can be instructed to take on the role of an \"emotion description expert\" or an \"action unit recognition expert.\" This helps ChatGPT better understand the task's context, clarify the desired goal, and focus on a particular task, such as recognizing Action Units in emoticons. By doing so, the model reduces ambiguity and applies relevant knowledge more accurately, enhancing the response's relevance and the quality of the generated results. This step ensures that the model performs optimally when addressing specific problems, thereby effectively improving the accuracy and consistency of the generated content.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Output Format",
      "text": "Question: \"What is the emotion of this face?\" Answer: \"The person in the image appears to have a serious or intense expression. The eyebrows are slightly furrowed, and the mouth is closed in a neutral position, which could suggest concentration, concern, or deep thought. There is no clear smile or frown, so it's not a definitive indication of happiness or sadness, but rather a more contemplative or focused demeanor.\"",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Fig. 4: Emotion Output Format Prompt",
      "text": "Output Format. In this step Figure  4 , we provide the model with an example question-answer format that serves as a guide for structuring its responses. This example helps the model understand the desired level of detail, tone, and format, ensuring standardized outputs across different inputs. By referencing the example, the model learns to include all necessary components in its responses, such as specific facial features, their emotional implications, and the relationships between facial action units. This consistency is especially crucial for complex tasks like emotion and AU classification, where responses must be informative, contextually relevant, and coherent. The example acts as a template, helping the model generate responses that are accurate, well-organized, and easy to interpret. Additionally, it sets a standard for depth and clarity, ensuring that the model consistently delivers context-aware, detailed, and relevant outputs.\n\nOutput Signal. In this step Figure  5 , we provide the model with an example question-answer format that serves as a guide for structuring its responses. This example helps",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Output Signal",
      "text": "Question : What is the emotion of this face?. <Emotion label list: Surprise> Answer: Fig.  5 : Emotion Output Signal Prompt the model understand the desired level of detail, tone, and format, ensuring standardized outputs across different inputs. By referencing the example, the model learns to include all necessary components in its responses, such as specific facial features, their emotional implications, and the relationships between facial action units. This consistency is especially crucial for complex tasks like emotion and AU classification, where responses must be informative, contextually relevant, and coherent. The example acts as a template, helping the model generate responses that are accurate, well-organized, and easy to interpret. Additionally, it sets a standard for depth and clarity, ensuring that the model consistently delivers context-aware, detailed, and relevant outputs.\n\nSummary. We employ a novel prompt-based method using GPT-4o  [16]  to generate detailed captions for both emotion and action unit (AU) analysis, offering deeper insights into facial expressions and their emotional implications. For emotion captioning, the model, guided by a prompt that positions it as an \"emotion description expert,\" interprets subtle facial cues such as eyebrow or lip movements to produce rich, contextaware descriptions beyond simple emotion labels. For AU captioning, the model acts as an \"AU description expert,\" breaking down facial expressions into specific AUs (e.g., AU4 for Brow Lowerer, AU24 for Lip Pressor) with detailed explanations of their contributions to overall expressions. Furthermore, the key AU caption approach focuses on identifying the most influential AUs for a given emotion, highlighting their decisive roles in conveying emotional states. This integrated approach provides a comprehensive understanding of how facial muscle movements define emotions, offering precise interpretations of complex expressions where multiple AUs interact.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Gpt-4O Prompt Example",
      "text": "Due to space limitations in the main text, we cannot present complete examples of the three prompt types and their generated captions (AU caption, emotion caption, and key AU caption). To clarify the differences among these three prompts, we provide basic examples of each in Figure  6 .\n\nAs shown in the figure, the differences between AU captions and emotion captions are minimal. The key distinction lies in the initial role setting (emotion expert or AU expert), which ensures GPT focuses on the required domain knowledge while mitigating the influence of unrelated factors. Another difference is the Output Format, which controls the content of the required response and indirectly guides GPT's reasoning process. In contrast, the key AU caption differs significantly from the other two. It emphasizes the interaction between AUs and emotions and incorporates more detailed prompt settings to achieve this.\n\nYou are currently acting as an emotion description expert and your ability is to recognize a person's expression, and their possible elicited hidden emotions based on an image of their face, please answer the questions according to this example: Question: \"What is the emotion of this face?\" Answer: \"The person in the image appears to have a serious or intense expression. The eyebrows are slightly furrowed, and the mouth is closed in a neutral position, which could suggest concentration, concern, or deep thought. There is no clear smile or frown, so it's not a definitive indication of happiness or sadness, but rather a more contemplative or focused demeanor.\"\n\nQuestion : What is the emotion of this face?. <Emotion label list: Surprise> Answer:",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Initial Setup Output Format",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Output Signal",
      "text": "One shot emotion prompt with ChatGPT-4o You are currently acting as a best AU description expert and your ability is to recognize a person's AU(action units) based on an image of their face and accurately identify the meaning it expresses, please answer the questions according to this example: Question: \"What is the action units of this face?\" Answer: \"The image shows a person with a facial expression that can be broken down into several Facial Action Coding System (FACS) Action Units (AUs). Here are the AUs that appear to be present: 1. AU4 (Brow Lowerer): There is a slight downward pull of the brows, which could indicate a frown or a concentration. 2. AU7 (Lid Tightener):\n\nThe eyelids appear to be tightened, which can be associated with a squint or a focused gaze. 3. AU24 (Lip Pressor):\n\nThe lips appear to be pressed together, which can be a sign of tension or determination. The overall expression seems to convey a sense of seriousness or intensity, but without additional context, it's difficult to determine the exact emotional state or intent behind the expression\" Question : What is the action units of this face?. <AU label list: AU1, AU2, AU4…> Answer:",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Initial Setup Output Format",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Output Signal",
      "text": "One shot au prompt with ChatGPT-4o You are currently acting as an emotion and AU description expert and your ability is to recognize a person's expression and their possible elicited hidden emotions based on an <emotion label list>'s label and an image of their face. At the same time, You also can recognize a person's AU and accurately identify the meaning it expresses based on an image of their face. Once organized, you can select from the <au active list> the most influential key AUs that can determine the sentiment category (in most cases, not all activated au's are the ones that ultimately make a difference, don't directly divide the entire au active list into key au. As an au expert, you have to choose the most influential AU as the key AU.).\n\nQuestion: \"What is the action units of this face?\" Answer: \"The image shows a person with a facial expression that can be broken down into several Facial Action Coding System (FACS) Action Units (AUs). Here are the AUs that appear to be present: 1. AU4 (Brow Lowerer): There is a slight downward pull of the brows, which could indicate a frown or a concentration. 2. AU7 (Lid Tightener): The eyelids appear to be tightened, which can be associated with a squint or a focused gaze. 3. AU24 (Lip Pressor): The lips appear to be pressed together, which can be a sign of tension or determination. The overall expression seems to convey a sense of seriousness or intensity, but without additional context, it's difficult to determine the exact emotional state or intent behind the expression\" Question : What is the action units of this face?. <AU label list: AU1, AU2, AU4…> Answer:",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Initial Setup Output Format",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Output Signal",
      "text": "One shot emotion with key au prompt with ChatGPT-4o Fig.  6 : Three Types of Prompt Design Details (AU prompt, emotion prompt and key AU prompt). When you finally type the prompt we will personalize the output format according to the format you want to get, this format is not fixed, it just depends on what information you need to get. In summary, this design ensures GPT-generated captions are accurate, contextually rich, and tailored to support downstream tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Example Of Caption",
      "text": "From the two sets of examples in Figure  7  we can clearly see the characteristics of the three different captions. Each type of caption serves a distinct purpose:\n\n• Emotion Caption: Provides an overview of the emotional state expressed by the face, utilizing the full spectrum of emotions present in the dataset.\n\n• AU Captiont: Describes the specific facial action units, breaking down the muscle movements involved in the expression.\n\n• Key AU Caption: Highlights the most influential action units that determine the emotional state, based on the ground truth emotion and AU labels. This novel caption type helps identify the critical facial movements responsible for conveying specific emotions.\n\nCompared to conventional single captions manually generated solely based on Ground Truth Labels, our approach",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Gt: Happiness",
      "text": "The person in the image appears to be experiencing happiness. The expression is characterized by a genuine smile, with the corners of the mouth turned upwards. The eyes are also slightly squinted, which is often associated with a true, joyful smile. The overall facial expression suggests a positive, pleasant emotion.\n\nGT: AU6, 7, 10, 12, 25 1.AU6 (Cheek Raiser):The cheeks are raised, which is commonly associated with a smile.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "2.Au7 (Lid Tightener):",
      "text": "The lower eyelids appear to be tightened, which is often seen in genuine smiles or expressions of happiness. The person in the image appears to be displaying an expression of surprise. The wide-open eyes and slightly parted lips suggest a reaction to something unexpected or astonishing. The raised eyebrows also contribute to the look of surprise, indicating that the person is reacting to something unforeseen or shocking.  All captions are entered into the GPT using a combination of the designed prompt and the corresponding ground true, and it is worth noting that the prompt is accompanied by the corresponding image, which allows the GPT to generate a personalized caption for the image. uses refined prompts, images, and Ground Truth Labels as inputs to generate captions through ChatGPT-4o  [16] . This method produces more descriptive captions that incorporate the intrinsic information of the images, resulting in captions that are more accurate, unique, and diverse.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "1.Au1 (Inner Brow",
      "text": "To this end, we designed a key AU prompt that provides a unique approach to generating captions with large language models. When generating key AU captions, only the image, prompt, and the Ground Truth Labels for AU and Emotion are provided, without including any information about the key AU itself. The carefully crafted prompt ensures that ChatGPT-4o  [16]  fully analyzes the image, going beyond simplistic descriptions of individual AUs and emotions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Transcition Experiments",
      "text": "Finally, we pre-train AU on the MF 2 model and then fine-tune emotion on the MF 2 (Fine-tuning) model after pretraining, and we name this final model MF 2 (Intern-VL).We tested the performance of this model on emotion and against the baseline model, and according to the results in table V, we can find that our model is 1.16% better than Exp-BLIP  [14] .",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the specific steps we followed to reconfigure the",
      "page": 2
    },
    {
      "caption": "Figure 1: Multimodal facial annotation for detailed AU descrip-",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework: (a) Emo-VL combines global image features with sentiment text; (b) AU-VL integrates local image features with AU",
      "page": 3
    },
    {
      "caption": "Figure 2: Our proposed Multilevel Mul-",
      "page": 3
    },
    {
      "caption": "Figure 2: (a). Emo-VL employs a pre-trained ViT model [32] to extract",
      "page": 3
    },
    {
      "caption": "Figure 2: (b), named AU-",
      "page": 4
    },
    {
      "caption": "Figure 2: (c). Inspired by the advanced Side",
      "page": 4
    },
    {
      "caption": "Figure 3: Emotion Initial Setup Prompt",
      "page": 7
    },
    {
      "caption": "Figure 3: , the Prompt model is",
      "page": 7
    },
    {
      "caption": "Figure 4: Emotion Output Format Prompt",
      "page": 7
    },
    {
      "caption": "Figure 4: , we provide the",
      "page": 7
    },
    {
      "caption": "Figure 5: , we provide the",
      "page": 7
    },
    {
      "caption": "Figure 5: Emotion Output Signal Prompt",
      "page": 7
    },
    {
      "caption": "Figure 6: As shown in the figure, the differences between AU captions",
      "page": 7
    },
    {
      "caption": "Figure 6: Three Types of Prompt Design Details (AU prompt, emotion prompt and key AU prompt). When you finally type the",
      "page": 8
    },
    {
      "caption": "Figure 7: we can clearly",
      "page": 8
    },
    {
      "caption": "Figure 7: Three Types of Caption Example (AU caption, emotion caption and key AU caption).All captions are entered into the",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Name": "AffectNet\n[18]\nRAF-DB [19]\nDFEW [20]\nDISFA [21]\nFERV39K [22]\nSFEW [23]\nAFEW [24]\nGFT [25]\nRAF-AU [26]\nCK+ [27]\nEmotioNet\n[28]\nCASME-II\n[29]\nBP4D [30]\nAffWild2 [17]",
          "AU": "✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "Emotion": "✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "Requirements": "✓\n✓\n✓\n✓\n✓\n✓\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✓",
          "Caption": "✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗"
        },
        {
          "Name": "MFA (Ours)",
          "AU": "✓",
          "Emotion": "✓",
          "Requirements": "✓",
          "Caption": "✓"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Exp-BLIP [14]\nME-GraphAU [36]\nVL-FAU [15]",
          "AU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU15\nAU23\nAU24\nAU25\nAU26": "60.18\n40.25\n12.63\n63.41\n53.28\n69.43\n71.76\n46.85\n27.60\n10.27\n86.43\n25.61\n61.42\n41.94\n13.72\n55.91\n41.92\n76.57\n70.48\n53.68\n20.13\n03.88\n85.53\n30.47\n73.51\n43.09\n15.86\n55.59\n49.35\n77.57\n54.81\n60.00\n29.50\n03.72\n84.25\n31.08",
          "Avg.": "47.31\n46.30\n48.19"
        },
        {
          "Models": "MF2 (Pre-Train)\nMF2 (Fine-Tuning)",
          "AU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU15\nAU23\nAU24\nAU25\nAU26": "50.17\n18.75\n73.18\n54.83\n34.80\n76.58\n70.00\n52.57\n48.92\n29.06\n11.72\n88.68\n76.70\n33.49\n43.02\n89.26\n44.76\n15.64\n66.90\n50.42\n73.17\n57.80\n54.51\n34.55",
          "Avg.": "50.77\n53.35"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Exp-BLIP [14]\nHSEmotion [41]",
          "Neutral\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurprise\nOther": "86.79\n90.73\n82.17\n92.74\n86.58\n88.30\n79.56\n50.24\n86.82\n80.95\n85.99\n86.73\n85.31\n77.84\n69.05\n76.30",
          "Avg.": "82.14\n81.12"
        },
        {
          "Model": "MF2 (Pre-Train)\nMF2 (Fine-Tuning)",
          "Neutral\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurprise\nOther": "87.70\n95.50\n86.37\n86.64\n86.05\n88.09\n82.08\n55.39\n89.51\n84.53\n92.57\n79.95\n82.41\n83.92\n87.14\n75.21",
          "Avg.": "83.48\n84.40"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "MF2 (Fine-Tuning)",
          "Neutral\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurprise\nOther": "89.51\n87.14\n84.53\n92.57\n79.95\n82.41\n83.92\n75.21",
          "Avg.": "84.40",
          "TT": "12.6",
          "IT": "6.4",
          "TP": "52.88M"
        },
        {
          "Model": "w/o DFN\nw/o Emo-VL\nw/o AU-VL",
          "Neutral\nAnger\nDisgust\nFear\nHappiness\nSadness\nSurprise\nOther": "95.50\n86.05\n87.70\n86.37\n86.64\n88.09\n82.08\n55.39\n78.44\n88.57\n94.55\n86.82\n86.76\n84.66\n84.86\n50.71\n90.52\n86.85\n87.17\n93.39\n71.39\n88.92\n71.18\n61.56",
          "Avg.": "83.48\n82.42\n81.87",
          "TT": "62.3\n8.4\n4.1",
          "IT": "5.1\n4.5\n2.1",
          "TP": "373.4M\n186.7M\n186.7M"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Micro-expression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "Ling Lei",
        "Tong Chen",
        "Shigang Li",
        "Jianfeng Li"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "2",
      "title": "Deep facial diagnosis: Deep transfer learning from face recognition to facial diagnosis",
      "authors": [
        "Bo Jin",
        "Leandro Cruz",
        "Nuno Gonc ¸alves"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "General facial representation learning in a visual-linguistic manner",
      "authors": [
        "Yinglin Zheng",
        "Hao Yang",
        "Ting Zhang",
        "Jianmin Bao",
        "Dongdong Chen",
        "Yangyu Huang",
        "Lu Yuan",
        "Dong Chen",
        "Ming Zeng",
        "Fang Wen"
      ],
      "year": "2022",
      "venue": "General facial representation learning in a visual-linguistic manner"
    },
    {
      "citation_id": "4",
      "title": "Facial expression recognition via deep learning",
      "authors": [
        "Abir Fathallah",
        "Lotfi Abdi",
        "Ali Douik"
      ],
      "year": "2017",
      "venue": "AICCSA"
    },
    {
      "citation_id": "5",
      "title": "A comprehensive survey on automatic facial action unit analysis",
      "authors": [
        "Ruicong Zhi",
        "Mengyi Liu",
        "Dezheng Zhang"
      ],
      "year": "2020",
      "venue": "Vis. Comput"
    },
    {
      "citation_id": "6",
      "title": "Local global relational network for facial action units recognition",
      "authors": [
        "Xuri Ge",
        "Pengcheng Wan",
        "Hu Han",
        "M Joemon",
        "Zhilong Jose",
        "Zhongqin Ji",
        "Xiao Wu",
        "Liu"
      ],
      "year": "2021",
      "venue": "FG"
    },
    {
      "citation_id": "7",
      "title": "Algrnet: Multi-relational adaptive facial action unit modelling for face representation and relevant recognitions",
      "authors": [
        "Xuri Ge",
        "M Joemon",
        "Pengcheng Jose",
        "Arunachalam Wang",
        "Xiao Iyer",
        "Hu Liu",
        "Han"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition based on facial action unit",
      "authors": [
        "Jiannan Yang",
        "Fan Zhang",
        "Bike Chen",
        "Samee Khan"
      ],
      "year": "2019",
      "venue": "IGSC"
    },
    {
      "citation_id": "9",
      "title": "Facial emotion recognition: A survey and real-world user experiences in mixed reality",
      "authors": [
        "Dhwani Mehta",
        "Mohammad Faridul Haque",
        "Ahmad Siddiqui",
        "Javaid"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "10",
      "title": "Latent representation learning for alzheimer's disease diagnosis with incomplete multi-modality neuroimaging and genetic data",
      "authors": [
        "Tao Zhou",
        "Mingxia Liu",
        "Kim-Han Thung",
        "Dinggang Shen"
      ],
      "year": "2019",
      "venue": "ITMT"
    },
    {
      "citation_id": "11",
      "title": "Fatigue driving detection method based on time-space-frequency features of multimodal signals",
      "authors": [
        "Jinxuan Shi",
        "Kun Wang"
      ],
      "year": "2023",
      "venue": "BSPC"
    },
    {
      "citation_id": "12",
      "title": "Coca: Contrastive captioners are image-text foundation models",
      "authors": [
        "Jiahui Yu",
        "Zirui Wang",
        "Vijay Vasudevan",
        "Legg Yeung",
        "Mojtaba Seyedhosseini",
        "Yonghui Wu"
      ],
      "year": "2022",
      "venue": "Coca: Contrastive captioners are image-text foundation models"
    },
    {
      "citation_id": "13",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Caiming Xiong",
        "Steven Hoi"
      ],
      "year": "2022",
      "venue": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
    },
    {
      "citation_id": "14",
      "title": "Describe your facial expressions by linking image encoders and large language models",
      "authors": [
        "Yujian Yuan",
        "; Jiabei Zeng",
        "Shiguang Shan"
      ],
      "venue": "Describe your facial expressions by linking image encoders and large language models"
    },
    {
      "citation_id": "15",
      "title": "Towards end-to-end explainable facial action unit recognition via vision-language joint learning",
      "authors": [
        "Xuri Ge",
        "Junchen Fu",
        "Fuhai Chen",
        "Shan An",
        "Nicu Sebe",
        "Joemon M Jose"
      ],
      "year": "2024",
      "venue": "ACM MM"
    },
    {
      "citation_id": "16",
      "title": "Chatgpt: a comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
      "authors": [
        "Partha Pratim"
      ],
      "venue": "Internet of Things and Cyber-Physical Systems"
    },
    {
      "citation_id": "17",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges"
    },
    {
      "citation_id": "18",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "19",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for unconstrained facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "20",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild"
    },
    {
      "citation_id": "21",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "Mohammad Mahoor",
        "Kevin Bartlett",
        "Philip Trinh",
        "Jeffrey Cohn"
      ],
      "year": "2013",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "22",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos"
    },
    {
      "citation_id": "23",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE ICCV Workshops"
    },
    {
      "citation_id": "24",
      "title": "Afew-va database for valence and arousal estimation in-thewild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image Vision Comput"
    },
    {
      "citation_id": "25",
      "title": "Sayette group formation task (GFT) spontaneous facial expression database",
      "authors": [
        "Wen-Sheng Jeffrey M Girard",
        "L Chu",
        "Jeffrey Jeni",
        "Fernando Cohn",
        "La Torre",
        "Michael Sayette"
      ],
      "year": "2017",
      "venue": "Sayette group formation task (GFT) spontaneous facial expression database"
    },
    {
      "citation_id": "26",
      "title": "Raf-au database: In-the-wild facial expressions with subjective emotion judgement and objective au annotations",
      "authors": [
        "Wen-Jing Yan",
        "Shan Li",
        "Chengtao Que",
        "Jiquan Pei",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "ACCV"
    },
    {
      "citation_id": "27",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "CVPR Workshops"
    },
    {
      "citation_id": "28",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "Ramprakash Srinivasan",
        "Aleix Martinez"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Cas(me)2: A database of spontaneous macro-expressions and micro-expressions",
      "authors": [
        "Fangbing Qu",
        "Sujing Wang",
        "Wen-Jing Yan",
        "Xiaolan Fu"
      ],
      "year": "2016",
      "venue": "Cas(me)2: A database of spontaneous macro-expressions and micro-expressions"
    },
    {
      "citation_id": "30",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Jeffrey Cohn",
        "Shaun Canavan",
        "Michael Reale",
        "Andy Horowitz",
        "Peng Liu",
        "Jeffrey Girard"
      ],
      "year": "2013",
      "venue": "IVC"
    },
    {
      "citation_id": "31",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
    },
    {
      "citation_id": "32",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "33",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "34",
      "title": "How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)",
      "authors": [
        "Adrian Bulat",
        "Georgios Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)"
    },
    {
      "citation_id": "35",
      "title": "Cross-entropy loss functions: Theoretical analysis and applications",
      "authors": [
        "Anqi Mao",
        "Mehryar Mohri",
        "Yutao Zhong"
      ],
      "year": "2023",
      "venue": "Cross-entropy loss functions: Theoretical analysis and applications"
    },
    {
      "citation_id": "36",
      "title": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "authors": [
        "Cheng Luo",
        "Siyang Song",
        "Weicheng Xie",
        "Linlin Shen",
        "Hatice Gunes"
      ],
      "year": "2022",
      "venue": "IJCAI"
    },
    {
      "citation_id": "37",
      "title": "Efficient and effective adaptation of multimodal foundation models in sequential recommendation",
      "authors": [
        "Junchen Fu",
        "Xuri Ge",
        "Xin Xin",
        "Alexandros Karatzoglou",
        "Ioannis Arapakis",
        "Kaiwen Zheng",
        "Yongxin Ni",
        "Joemon M Jose"
      ],
      "year": "2024",
      "venue": "Efficient and effective adaptation of multimodal foundation models in sequential recommendation",
      "arxiv": "arXiv:2411.02992"
    },
    {
      "citation_id": "38",
      "title": "Iisan: Efficiently adapting multimodal representation for sequential recommendation with decoupled peft",
      "authors": [
        "Junchen Fu",
        "Xuri Ge",
        "Xin Xin",
        "Alexandros Karatzoglou",
        "Ioannis Arapakis",
        "Jie Wang",
        "Joemon M Jose"
      ],
      "year": "2024",
      "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "39",
      "title": "Exploring adapter-based transfer learning for recommender systems: Empirical studies and practical insights",
      "authors": [
        "Junchen Fu",
        "Fajie Yuan",
        "Yu Song",
        "Zheng Yuan",
        "Mingyue Cheng",
        "Shenghui Cheng",
        "Jiaqi Zhang",
        "Jie Wang",
        "Yunzhu Pan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 17th ACM international conference on web search and data mining"
    },
    {
      "citation_id": "40",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "Neil Houlsby",
        "Andrei Giurgiu",
        "Stanislaw Jastrzebski",
        "Bruna Morrone",
        "Quentin De Laroussilhe",
        "Andrea Gesmundo",
        "Mona Attariyan",
        "Sylvain Gelly"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "41",
      "title": "Hsemotion: High-speed emotion recognition library",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2022",
      "venue": "Software Impacts"
    },
    {
      "citation_id": "42",
      "title": "Towards end-to-end explainable facial action unit recognition via vision-language joint learning",
      "authors": [
        "Xuri Ge",
        "Junchen Fu",
        "Fuhai Chen",
        "Shan An",
        "Nicu Sebe",
        "Joemon Jose"
      ],
      "venue": "MM. Oct. 2024, MM '24"
    }
  ]
}