{
  "paper_id": "2309.10294v1",
  "title": "Leveraging Speech Ptm, Text Llm, And Emotional Tts For Speech Emotion Recognition",
  "published": "2023-09-19T03:52:01Z",
  "authors": [
    "Ziyang Ma",
    "Wen Wu",
    "Zhisheng Zheng",
    "Yiwei Guo",
    "Qian Chen",
    "Shiliang Zhang",
    "Xie Chen"
  ],
  "keywords": [
    "speech emotion recognition",
    "text generation",
    "speech synthesis",
    "data augmentation",
    "self-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we explored how to boost speech emotion recognition (SER) with the state-of-the-art speech pre-trained model (PTM), data2vec, text generation technique, GPT-4, and speech synthesis technique, Azure TTS. First, we investigated the representation ability of different speech self-supervised pre-trained models, and we found that data2vec has a good representation ability on the SER task. Second, we employed a powerful large language model (LLM), GPT-4, and emotional text-to-speech (TTS) model, Azure TTS, to generate emotionally congruent text and speech. We carefully designed the text prompt and dataset construction, to obtain the synthetic emotional speech data with high quality. Third, we studied different ways of data augmentation to promote the SER task with synthetic speech, including random mixing, adversarial training, transfer learning, and curriculum learning. Experiments and ablation studies on the IEMOCAP dataset demonstrate the effectiveness of our method, compared with other data augmentation methods, and data augmentation with other synthetic data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is an active research field of intelligent speech processing, both in academia and industry. With the development of deep learning methods, the performance of SER has been significantly improved  [1] . Nonetheless, data scarcity is one of the most important reasons that hinders the further performance improvement of SER. Since annotating emotions in speech is timeconsuming and subjective, large-scale high-quality labeled data is challenging to obtain. Therefore, alleviating the data scarcity problem becomes essential.\n\nUsing self-supervised learning (SSL) features is an effective way to make up for the lack of labeled data. SSL does not require laborintensive manual annotation, while learning universal representations from a large amount of unlabeled data. There are prominent speech self-supervised pre-trained models (PTMs)  [6, 7, 8, 9, 10, 11, 12, 13] , which have been proven to work well across different downstream tasks  [2] , such as speech recognition  [14] , speaker verification  [15] , as well as emotion recognition  [16] .\n\nIn the field of SER, the most commonly used upstream pretrained models are wav2vec 2.0  [6]  and HuBERT  [7] . Some works Table  1 . Performance of different SSL pre-trained models on SER task. The setting of the downstream model follows SUPERB  [2]  to test the representation ability of different upstream models on the IEMOCAP  [3]  dataset. \"LS 960 hr\" means LibriSpeech 960 hours, and \"Mix 94k hr\" means 94k hours of data including LibriLight, VoxPopuli, and GigaSpeech.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Model",
      "text": "# Params Pre-training Corpus WA(%) ↑ wav2vec  [4]  32.54M LS 960 hr 59.79 vq-wav2vec  [5]  34.15M LS 960 hr 58.24 wav2vec 2.0  [6]  95.04M LS 960 hr 63.43 HuBERT  [7]  94.68M LS 960 hr 64.92 WavLM  [8]  94.70M LS 960 hr 65.94 WavLM+  [8]  94.70M Mix 94k hr 67.98 data2vec  [9]  93.75M LS 960 hr 68.02 freeze the upstream model and then use the extracted features to build downstream models  [17, 18] . Other works finetune the upstream model on emotion recognition corpus and achieve better results  [19, 20, 21] . A series of subsequent works comprehensively investigated the SER performance of wav2vec 2.0 and HuBERT model, in the case of no fine-tuning, partial fine-tuning, and entire fine-tuning, either on automatic speech recognition (ASR) corpus or SER corpus  [22, 23] . Some recent works utilize WavLM  [8]  as the feature extractor, leading to better performance  [24, 25] . However, one of the most recent speech PTMs, data2vec  [9] , has little presence in the field of SER. Following the setting of the SUPERB  [2]  benchmark, we freeze the data2vec model and conduct a weighted sum of features from different Transformer layers to generate the final representations. For the downstream model, only linear layers and a pooling layer are employed. Table  1  compare different speech PTMs for the SER task with weighted accuracy (WA). data2vec achieves better accuracy despite using less pre-training data than other widely used models. Therefore, in our experiments, we employ data2vec as the feature extractor, so as to construct a strong baseline.\n\nStarting with a strong baseline, we attempt to perform data augmentation to further improve the performance. Early works conduct data augmentation by modifying or combining the original speech. Speed perturbation  [26] , SpecAugment  [27]  and mixup  [28]  are widely adopted techniques in ASR to enhance the robustness and performance of the systems. Research has shown that these tricks can improve the robustness of SER systems  [29, 30, 31, 32]  while the performance gain is limited, since little new emotional information is introduced. Adding noise  [30]  and applying impulse response  [33]  can also help improve the generalization ability of SER systems, while their impact is diminished when SSL models are incorporated for feature extraction  [34] . Recent works leverage generative models to create additional training data, so as to improve the performance of SER. Methods of adding synthetic data are not limited by front-end features, which cooperate well with SSL features. Popular methods include using GAN-based models  [35, 36]  and diffusion-based models  [37] . These methods use the original training data to train generative models, thereby generating synthetic data consistent with the original distribution. Little has been investigated on how to incorporate generated speech with the original speech during training in cases of cross-corpus and cross-domain.\n\nRecent work  [38]  investigates the capabilities of generative LLM in the field of affective computing. However, their work lies in textual emotions and sentiments, and there is still a blank in the field of speech emotion recognition.\n\nOur goal is to leverage state-of-the-art text generation and speech synthesis techniques to synthesize high-quality speech, and effectively fuse it with the original speech to enhance SER performance. For text generation, we use GPT-4  [39] , one of the best-performing large language mode (LLM) to synthesize text with emotional expressions. Careful prompt design and delicate data engineering are performed to ensure the quality of text obtained. We utilize the generated text to synthesize speech using Azure textto-speech (TTS) 1 with emotionally congruent expressions, which guarantees acoustic and semantic consistency. For example, a person would almost never say a sad text in a happy tone. We explored different ways of data augmentation, including random mixing, adversarial training, transfer learning, and curriculum learning, to make synthetic data helpful. We also test our method with EmoDiff  [40] , a state-of-the-art diffusion-based specialist model for emotional TTS. Experiments and ablation studies on the IEMOCAP  [3]  dataset demonstrate the effectiveness of the proposed method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Approach",
      "text": "In this section, we first introduce how to synthesize high-quality emotional speech data, and then present how to use the synthetic data to improve the performance of speech emotion recognition. 1 https://learn.microsoft.com/en-us/azure/ai-services/speech-service/textto-speech",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Synthesis Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Configuration",
      "text": "There are two steps to synthesize semantically consistent emotional speech: 1) Generate emotional text data. 2) Synthesize emotional speech data rich in corresponding emotions using the generated text. In order to accomplish the above goals, we design a setting to construct the synthetic dataset, as shown in Table  2 , where we list the elements involved in text generation and speech synthesis.\n\nWhen configuring text generation, we mainly consider 4 kinds of information:\n\n1. Narrative Styles. In the real world, human speech comes either from a conversation scene (dialogue) or from reading some text (narrative).\n\n2. Scenarios. Referring to the classification method of Gi-gaSpeech  [41] , we classify the scenarios into 24 classes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Styles (Emotions).",
      "text": "According to the predefined styles of Azure Emotional TTS, We choose 11 of them, along with 1 additional \"neutral\" emotion. The emotions can be taken as a subset of the styles.\n\n4. Max Tokens. In order to make the subsequent synthesized speech diverse in length distribution, we make the generated text with different ranges of token numbers. We use max token lengths to represent short, medium, and long sentences.\n\nWhen configuring speech synthesis, we mainly consider 2 kinds of information:\n\n1. Speakers. To train a SER model, different speakers are usually required to increase the robustness of the model. Here we employ 5 female timbres and 4 male timbres.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Styles (Emotions).",
      "text": "We generate emotionally congruent text and speech, therefore the number of styles (emotions) remains consistent with the text, which is 12 classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Generation",
      "text": "To synthesize high-quality text with emotional expressions, we use one of the most powerful LLM available, GPT-4  [39] , for this task. A key point to the successful use of GPT-4 is to design a good prompt. Algorithm 1 presents our prompt for emotion text generation. Given lists of narrative styles, scenarios, emotions, and max tokens, we group tuples from them. For each tuple, we choose the corresponding prompt according to the narrative style and fill in the rest content Map max token to text description that length2str:{10: short, 30: middle, 50: long} 5:\n\nif narrative style = \" dialogue \" then 6: prompt = \" In the context of scenario, say something in first-person or second-person that expresses your feeling, or using the speaking style of emotion, as if you are talking to somebody. Do not write any explanations and just answer the question. What you say should be length2str(max token) length with no more than max token words. \"",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "7:",
      "text": "else if narrative style = \" narrative \" then 8:\n\nprompt = \" In the context of scenario, describe a third-person scene that conveys the emotion, or using the speaking style of emotion. Do not write any explanations and just answer the question. What you say should be length2str(max token) length with no more than max token words. \" Sample a batch of generated text u with prompt and add u to U 11: end for 12: Perform data engineering on the generated text U to ensure the quality of generation 13: return U into the prompt. We then sample a batch of generated text from GPT-4 and add them to the output pool. Finally, we perform data cleaning on the generated text to guarantee the generated text is exactly what we need for the subsequent speech synthesis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Synthesis",
      "text": "Once the generated text is obtained, we synthesize speech that is semantically consistent with the text. We implement this step by using Azure Emotional TTS with Speech Synthesis Markup Language (SSML)  [42] . The total length of synthesis speech is more than 500 hours, which is far more than the size of the IEMOCAP dataset. We select a subset to conduct the experiments, which will be detailed in Section 3.1.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Configuration And Training",
      "text": "For the upstream model, we use a data2vec model with parameters frozen. In other words, we use a pre-trained data2vec model as the feature extractor to extract speech representations of 768 dimensions. Representations using the last layer or multi-layer weighted with learnable parameters are both considered. The learnable parameters are initialized with an average of layer numbers, and optimized during the training process of the downstream model.\n\nFor the downstream model, following the common practice of SUPERB  [2] , we use a linear layer to map 768 dimensions to 128 dimensions, followed by a ReLU activation function and an average pooling layer, and finally use a linear classification layer to obtain the probability distribution of each emotion. The cross-entropy loss is employed for optimizing the downstream model.\n\nFor the training procedure, we use the AdamW optimizer and set the learning rate to 1e-3, and the weight decay to 2e-3. We train the model on a single Nvidia RTX 3090 GPU with a batch size of 128 for 50 epochs.\n\nFor the inference procedure, we apply widely employed evaluation metrics, weighted accuracy (WA) and unweighted accuracy (UA), to evaluate the performance of speech emotion recognition.\n\nWA corresponds to the overall accuracy while UA corresponds to the average class-wise accuracy.\n\nFor data augmentation with synthetic speech, simply mixing the synthetic with the original does not improve the performance. Here we explore different data augmentation methods, say, adversarial training, transfer learning, and curriculum learning. The details and results of different ways of data augmentation are clarified in Section 3.2.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets Details",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Real Datasets",
      "text": "The benchmark dataset IEMOCAP  [3]  is used for SER. It consists of 5 dyadic conversational sessions performed by 10 professional actors with a session being a conversation between two exclusive speakers. Each utterance was annotated by three human annotators and the ground-truth labels were determined by majority vote. Following prior work  [2] , four emotion classes are used: happy (merged with excited), sad, angry, and neutral, which give 5531 utterances. Leaveone-session-out 5-fold cross-validation setup is used. In each fold, one session is held out for testing, while the remaining four sessions are utilized for training and validation, split in an 8:2 ratio. The average results cross folds are reported.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Synthetic Datasets",
      "text": "By observing the IEMOCAP dataset, we found that almost all of the speech in IEMOCAP is short speech. We choose the speech synthesized by sentences with a max token of 10. Besides, emotions in synthetic speech include happy sad, angry, and neutral, and the narrative style is dialogue, both of which keep consistent with the IEMOCAP dataset. This results in a subset of 6473 sentences, which outcomes about 8.8 hours of speech. Due to the difference in distribution between the augmented data and the original data, when training with synthetic speech, we select the checkpoint of the last epoch instead of the best checkpoint on the validation set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results With Different Augmentation Methods",
      "text": "Due to the difference in the distribution of synthetic data and original data, mixing directly does not improve the performance of the model. As shown in Table  3 , we explore several different data augmentation methods, where experiments are based on the features extracted from the last layer of the data2vec model.\n\nRandom mixing. We just add the synthetic speech to the original speech and shuffle them. Compared with the baseline, the performance of the SER task does not improve due to the difference with respect to the distribution.\n\nAdversarial training. In the case of adversarial training, the model contains three sub-networks, namely feature fuser, SER classifier, and domain classifier  [43] . The feature fuser conducts dimension transformation on multi-layer or single-layer features and pooling operation. The SER classifier employs cross-entropy loss for emotion classification, while the domain classifier employs binary cross-entropy loss for domain classification. Each batch is optimized in three steps. The first step is to calculate the loss of emotion classification, and then optimize the feature fuser and the SER classifier simultaneously. The second step is to calculate the loss of domain classification, and then only optimize the domain classifier, with the gradient of the feature fuser detached. The third step is to pass the domain classification loss back to the feature fuser, perform gradient reversal, and optimize the feature fuser. Through adversarial training, data with different distributions will be adjusted to a similar distribution within the feature fuser. However, through this training criterion, the performance of SER is not greatly improved. The reason may be that this method is designed to adapt the new domain to the original domain, while the effect on improving the performance of the original domain is limited.\n\nTransfer learning. Transfer learning is a training method that transfers learned knowledge to target data. Under this paradigm, we first train the SER model on synthetic speech, then reduce the learning rate, and then migrate to the IEMOCAP dataset for training. Through transfer learning, the performance of the model is effectively improved.\n\nCurriculum learning. Curriculum learning is a training strategy that organizes the learning process by gradually increasing the complexity of training samples. In this case, we sort the augmented data from short to long, and gradually add it to the original data every fixed number of epochs during the training phase. The model has great performance gain on the IEMOCAP dataset in this way. We found that adding more augmented data is not always more profitable. Based on the optimal configuration from Section 3.2, we control the total amount of synthetic data added. As shown in Figure  1 , both weighted accuracy and unweighted accuracy become better as the amount of synthetic data increases. The model achieves best performance when the amount of synthetic data added is half of the original data. When the synthetic data continues to be added, the performance of the model begins to decline. One possible reason is that adding too much synthetic data will allow the distribution of synthetic data to dominate the training progress.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results On The Ser Task",
      "text": "With a suitable augmentation method and a suitable amount of data, we train the downstream model with our method on the SER task. More specifically, we use the training strategy of curriculum learning and set the data ratio to 1:2 between synthetic data to real data. Extracted features employing the last layer or multi-layer weighted from the pre-trained data2vec model are both considered. We compare our method with the baseline without synthetic data. We also compare with models augmented with synthetic data generated from EmoDiff  [40] , the most recent emotional TTS utilizing diffusion technique, with the same emotional text. As shown in Table  4 , our method improves the SER performance on both weighted accuracy and unweighted accuracy. Experiments with EmoDiff also improve compared to the baseline, indicating that our method can be transferred to other emotional TTS.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we aim to leverage state-of-the-art techniques for augmenting speech emotion recognition. We utilize an awesome large language model contemporarily, GPT-4, to generate text with emotional expressions, and Azure Emotional TTS, to synthesize speech with emotional consistency. We also leverage a powerful speechbased pre-trained model, data2vec, as the feature extractor, to extract features of synthetic data and real data. Besides, effective augmentation strategies and suitable augmented data volume are explored. Experiments on the IEMOCAP dataset demonstrate the effectiveness of our method. In our experiments, only a small subset of our synthetic data is used. In the future, we will explore how to use large-scale high-quality synthetic data to enhance cross-corpus, cross-domain, and cross-language situations.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Results with different amounts of augmentation data. Both",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "technique, Azure TTS. First, we investigated the representation abil-",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "and “Mix 94k hr” means 94k hours of data including LibriLight,"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "ity of different speech self-supervised pre-trained models, and we",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": ""
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": ""
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "found that data2vec has a good representation ability on the SER",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": ""
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "task. Second, we employed a powerful large language model (LLM),",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "# Params"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "GPT-4, and emotional\ntext-to-speech (TTS) model, Azure TTS,\nto",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "32.54M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "generate emotionally congruent\ntext and speech. We carefully de-",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "34.15M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "signed the text prompt and dataset construction,\nto obtain the syn-",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "95.04M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "thetic emotional\nspeech data with high quality.\nThird, we stud-",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "94.68M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "ied different ways of data augmentation to promote the SER task",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "94.70M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "with synthetic speech,\nincluding random mixing, adversarial\ntrain-",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "94.70M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "ing,\ntransfer\nlearning, and curriculum learning.\nExperiments and",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": "93.75M"
        },
        {
          "data2vec,\ntext generation technique, GPT-4, and speech synthesis": "ablation studies on the IEMOCAP dataset demonstrate the effective-",
          "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "Table 1. Performance of different SSL pre-trained models on SER"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "task. The setting of the downstream model follows SUPERB [2] to"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "test\nthe representation ability of different upstream models on the"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "IEMOCAP [3] dataset. “LS 960 hr” means LibriSpeech 960 hours,"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "and “Mix 94k hr” means 94k hours of data including LibriLight,"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "VoxPopuli, and GigaSpeech."
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "Model\n# Params\nPre-training Corpus\nWA(%) ↑"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "wav2vec [4]\n32.54M\nLS 960 hr\n59.79"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "vq-wav2vec [5]\n34.15M\nLS 960 hr\n58.24"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "wav2vec 2.0 [6]\n95.04M\nLS 960 hr\n63.43"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "HuBERT [7]\n94.68M\nLS 960 hr\n64.92"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "WavLM [8]\n94.70M\nLS 960 hr\n65.94"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "WavLM+ [8]\n94.70M\nMix 94k hr\n67.98"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "data2vec [9]\n93.75M\nLS 960 hr\n68.02"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "freeze the upstream model and then use the extracted features\nto"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "build downstream models [17, 18]. Other works finetune the up-"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "stream model on emotion recognition corpus\nand achieve better"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "results [19, 20, 21]. A series of subsequent works comprehensively"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "investigated the SER performance of wav2vec 2.0 and HuBERT"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "model,\nin the case of no fine-tuning, partial fine-tuning, and entire"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "fine-tuning, either on automatic speech recognition (ASR) corpus or"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "SER corpus [22, 23]. Some recent works utilize WavLM [8] as the"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "feature extractor,\nleading to better performance [24, 25]. However,"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "one of the most recent speech PTMs, data2vec [9], has little presence"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "in the field of SER. Following the setting of the SUPERB [2] bench-"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "mark, we freeze the data2vec model and conduct a weighted sum"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "of\nfeatures from different Transformer\nlayers to generate the final"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "representations. For the downstream model, only linear layers and a"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "pooling layer are employed. Table 1 compare different speech PTMs"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "for the SER task with weighted accuracy (WA). data2vec achieves"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "better accuracy despite using less pre-training data than other widely"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "used models. Therefore, in our experiments, we employ data2vec as"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "the feature extractor, so as to construct a strong baseline."
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "Starting with a\nstrong baseline, we\nattempt\nto perform data"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "augmentation to further\nimprove\nthe performance.\nEarly works"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "conduct data augmentation by modifying or combining the original"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "speech. Speed perturbation [26], SpecAugment [27] and mixup [28]"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "are widely adopted techniques\nin ASR to enhance the robustness"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "and performance of\nthe systems.\nResearch has\nshown that\nthese"
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": ""
        },
        {
          "3 Speech Lab of DAMO Academy, Alibaba Group, China": "tricks can improve the robustness of SER systems [29, 30, 31, 32]"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "Descriptions"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": ""
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "dialogue, narrative"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "arts, autos and vehicles, business comedy, crime, education, entertainment, film and animation,"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "gaming, health and fitness, history, howto and style, kids and family, leisure, music,"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": ""
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "news and politics, nonprofits and activism, people and blogs, pets and animals,"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "religion and spirituality, science and technology, society and culture, sports, travel and events"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "angry, cheerful, excited, friendly, hopeful, sad, shouting, terrified, unfriendly, whispering, terrified, neutral"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "10, 30, 50"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": ""
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "5 females, 4 males"
        },
        {
          "Table 2. Dataset descriptions and statistics at a glance.": "angry, cheerful, excited, friendly, hopeful, sad, shouting, terrified, unfriendly, whispering, terrified, neutral"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speakers\n5 females, 4 males": "Styles (Emotions)",
          "9": "angry, cheerful, excited, friendly, hopeful, sad, shouting, terrified, unfriendly, whispering, terrified, neutral\n12"
        },
        {
          "Speakers\n5 females, 4 males": "information is introduced. Adding noise [30] and applying impulse",
          "9": "2.1. Data Synthesis Methods"
        },
        {
          "Speakers\n5 females, 4 males": "response [33] can also help improve the generalization ability of",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "2.1.1. Dataset Configuration"
        },
        {
          "Speakers\n5 females, 4 males": "SER systems, while their\nimpact\nis diminished when SSL models",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "are incorporated for feature extraction [34]. Recent works leverage",
          "9": "There are two steps to synthesize semantically consistent emotional"
        },
        {
          "Speakers\n5 females, 4 males": "generative models to create additional training data, so as to improve",
          "9": "speech:\n1) Generate emotional\ntext data.\n2) Synthesize emotional"
        },
        {
          "Speakers\n5 females, 4 males": "the performance of SER. Methods of adding synthetic data are not",
          "9": "speech data rich in corresponding emotions using the generated text."
        },
        {
          "Speakers\n5 females, 4 males": "limited by front-end features, which cooperate well with SSL fea-",
          "9": "In order to accomplish the above goals, we design a setting to con-"
        },
        {
          "Speakers\n5 females, 4 males": "tures. Popular methods include using GAN-based models [35, 36]",
          "9": "struct\nthe synthetic dataset, as shown in Table 2, where we list\nthe"
        },
        {
          "Speakers\n5 females, 4 males": "and diffusion-based models [37].\nThese methods use the original",
          "9": "elements involved in text generation and speech synthesis."
        },
        {
          "Speakers\n5 females, 4 males": "training data to train generative models, thereby generating synthetic",
          "9": "When configuring text generation, we mainly consider 4 kinds"
        },
        {
          "Speakers\n5 females, 4 males": "data consistent with the original distribution. Little has been inves-",
          "9": "of information:"
        },
        {
          "Speakers\n5 females, 4 males": "tigated on how to incorporate generated speech with the original",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "1. Narrative Styles.\nIn the real world, human speech comes"
        },
        {
          "Speakers\n5 females, 4 males": "speech during training in cases of cross-corpus and cross-domain.",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "either from a conversation scene (dialogue) or from reading"
        },
        {
          "Speakers\n5 females, 4 males": "Recent work [38] investigates the capabilities of generative LLM in",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "some text (narrative)."
        },
        {
          "Speakers\n5 females, 4 males": "the field of affective computing. However, their work lies in textual",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "emotions and sentiments, and there is still a blank in the field of",
          "9": "2. Scenarios.\nReferring to the\nclassification method of Gi-"
        },
        {
          "Speakers\n5 females, 4 males": "speech emotion recognition.",
          "9": "gaSpeech [41], we classify the scenarios into 24 classes."
        },
        {
          "Speakers\n5 females, 4 males": "Our\ngoal\nis\nto\nleverage\nstate-of-the-art\ntext\ngeneration\nand",
          "9": "(Emotions).\n3. Styles\nAccording to the predefined styles of"
        },
        {
          "Speakers\n5 females, 4 males": "speech synthesis techniques to synthesize high-quality speech, and",
          "9": "Azure Emotional TTS, We choose 11 of them, along with 1"
        },
        {
          "Speakers\n5 females, 4 males": "effectively fuse\nit with the original\nspeech to enhance SER per-",
          "9": "additional “neutral” emotion. The emotions can be taken as a"
        },
        {
          "Speakers\n5 females, 4 males": "formance.\nFor\ntext generation, we use GPT-4 [39],\none of\nthe",
          "9": "subset of the styles."
        },
        {
          "Speakers\n5 females, 4 males": "best-performing large language mode (LLM) to synthesize text with",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "4. Max Tokens.\nIn order\nto make the subsequent synthesized"
        },
        {
          "Speakers\n5 females, 4 males": "emotional\nexpressions.\nCareful prompt design and delicate data",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "speech diverse in length distribution, we make the generated"
        },
        {
          "Speakers\n5 females, 4 males": "engineering are performed to ensure the quality of\ntext obtained.",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "text with different ranges of token numbers. We use max to-"
        },
        {
          "Speakers\n5 females, 4 males": "We utilize the generated text to synthesize speech using Azure text-",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "ken lengths to represent short, medium, and long sentences."
        },
        {
          "Speakers\n5 females, 4 males": "to-speech (TTS)\n1 with emotionally congruent expressions, which",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "When configuring speech synthesis, we mainly consider 2 kinds"
        },
        {
          "Speakers\n5 females, 4 males": "guarantees acoustic and semantic consistency. For example, a per-",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "of information:"
        },
        {
          "Speakers\n5 females, 4 males": "son would almost never say a sad text in a happy tone. We explored",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "different ways\nof\ndata\naugmentation,\nincluding\nrandom mixing,",
          "9": "1. Speakers. To train a SER model, different speakers are usu-"
        },
        {
          "Speakers\n5 females, 4 males": "adversarial\ntraining,\ntransfer\nlearning, and curriculum learning,\nto",
          "9": "ally required to increase the robustness of\nthe model. Here"
        },
        {
          "Speakers\n5 females, 4 males": "make synthetic data helpful. We also test our method with EmoD-",
          "9": "we employ 5 female timbres and 4 male timbres."
        },
        {
          "Speakers\n5 females, 4 males": "iff [40], a state-of-the-art diffusion-based specialist model for emo-",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "2. Styles (Emotions). We generate emotionally congruent\ntext"
        },
        {
          "Speakers\n5 females, 4 males": "tional TTS. Experiments and ablation studies on the IEMOCAP [3]",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "and speech,\ntherefore the number of\nstyles\n(emotions)\nre-"
        },
        {
          "Speakers\n5 females, 4 males": "dataset demonstrate the effectiveness of the proposed method.",
          "9": ""
        },
        {
          "Speakers\n5 females, 4 males": "",
          "9": "mains consistent with the text, which is 12 classes."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "Require:\nInput:\nlists of Narrative Styles N, Scenarios S, Emotions E, Max Tokens M"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "Ensure: Output:\nlists of generated text U from GPT-4"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "1: Set the system role: “You are a helpful assistant with human emotions and talking styles.”"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "2: Group tuples T which meets\nthe requirements\nthat narrative style ∈ N,\nscenario ∈ S,\nemotion ∈ E, max token ∈ M,\nand"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "(narrative style, scenario, emotion, max token) ∈ T"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "3:\nfor each tuple t ∈ T do"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "4:\nMap max token to text description that length2str:{10: short, 30: middle, 50:\nlong}"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "5:\nif narrative style = “ dialogue ” then"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "6:\nprompt = “ In the context of scenario, say something in first-person or second-person that expresses your\nfeeling, or using the"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "speaking style of emotion, as if you are talking to somebody. Do not write any explanations and just answer the question. What you"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "say should be length2str(max token) length with no more than max token words. ”"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "7:\nelse if narrative style = “ narrative ” then"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "8:\nprompt = “ In the context of scenario, describe a third-person scene that conveys the emotion, or using the speaking style of"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "emotion. Do not write any explanations and just answer the question. What you say should be length2str(max token) length with"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "no more than max token words. ”"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "9:\nend if"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "10:\nSample a batch of generated text u with prompt and add u to U"
        },
        {
          "Algorithm 1 Prompt Engineering for Emotional Text Generation": "11:\nend for"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "13:\nreturn U"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "into the prompt. We then sample a batch of generated text from GPT-"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "4 and add them to the output pool. Finally, we perform data cleaning"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "on the generated text to guarantee the generated text is exactly what"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "we need for the subsequent speech synthesis."
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "2.1.3.\nSpeech Synthesis"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "Once the generated text\nis obtained, we synthesize speech that\nis"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "semantically consistent with the text. We implement this step by us-"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "ing Azure Emotional TTS with Speech Synthesis Markup Language"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "(SSML) [42]. The total length of synthesis speech is more than 500"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "hours, which is far more than the size of the IEMOCAP dataset. We"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "select a subset to conduct the experiments, which will be detailed in"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "Section 3.1.2."
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "2.2. Model Configuration and Training"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "For\nthe upstream model, we use a data2vec model with parame-"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "ters frozen.\nIn other words, we use a pre-trained data2vec model as"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "the feature extractor to extract speech representations of 768 dimen-"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "sions. Representations using the last\nlayer or multi-layer weighted"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "with learnable parameters are both considered. The learnable param-"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "eters are initialized with an average of layer numbers, and optimized"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "during the training process of the downstream model."
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "For\nthe downstream model,\nfollowing the common practice of"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "SUPERB [2], we use a linear layer to map 768 dimensions to 128"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": ""
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "dimensions, followed by a ReLU activation function and an average"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "pooling layer, and finally use a linear classification layer\nto obtain"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "the probability distribution of each emotion. The cross-entropy loss"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "is employed for optimizing the downstream model."
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "For\nthe training procedure, we use the AdamW optimizer and"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "set the learning rate to 1e-3, and the weight decay to 2e-3. We train"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "the model on a single Nvidia RTX 3090 GPU with a batch size of"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "128 for 50 epochs."
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "For\nthe inference procedure, we apply widely employed eval-"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "uation metrics, weighted accuracy (WA) and unweighted accuracy"
        },
        {
          "12: Perform data engineering on the generated text U to ensure the quality of generation": "(UA),\nto evaluate the performance of speech emotion recognition."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Results with different augmentation methods compared",
      "data": [
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "is that adding too much synthetic data will allow the distribution of"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Due to the difference in the distribution of synthetic data and orig-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "synthetic data to dominate the training progress."
        },
        {
          "3.2. Results with Different Augmentation Methods": "inal data, mixing directly does not\nimprove the performance of the",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "model. As shown in Table 3, we explore several different data aug-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "mentation methods, where experiments are based on the features ex-",
          "performance of\nthe model begins to decline. One possible reason": "71\nWA"
        },
        {
          "3.2. Results with Different Augmentation Methods": "tracted from the last layer of the data2vec model.",
          "performance of\nthe model begins to decline. One possible reason": "UA\n70"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Random mixing. We just add the synthetic speech to the origi-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "69"
        },
        {
          "3.2. Results with Different Augmentation Methods": "nal speech and shuffle them. Compared with the baseline, the perfor-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "68"
        },
        {
          "3.2. Results with Different Augmentation Methods": "mance of the SER task does not\nimprove due to the difference with",
          "performance of\nthe model begins to decline. One possible reason": "Accuracy(%)"
        },
        {
          "3.2. Results with Different Augmentation Methods": "respect to the distribution.",
          "performance of\nthe model begins to decline. One possible reason": "67"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Adversarial training.\nIn the case of adversarial\ntraining,\nthe",
          "performance of\nthe model begins to decline. One possible reason": "66"
        },
        {
          "3.2. Results with Different Augmentation Methods": "model contains three sub-networks, namely feature fuser, SER clas-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "65"
        },
        {
          "3.2. Results with Different Augmentation Methods": "sifier, and domain classifier [43]. The feature fuser conducts dimen-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "sion transformation on multi-layer or single-layer features and pool-",
          "performance of\nthe model begins to decline. One possible reason": "0\n25%\n50%\n75%\n100%"
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "Data Ratio between Augmented Data and IEMOCAP Data"
        },
        {
          "3.2. Results with Different Augmentation Methods": "ing operation.\nThe SER classifier employs cross-entropy loss for",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "emotion classification, while the domain classifier employs binary",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "Fig. 1. Results with different amounts of augmentation data. Both"
        },
        {
          "3.2. Results with Different Augmentation Methods": "cross-entropy loss for domain classification. Each batch is optimized",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "WA and UA are drawn out here."
        },
        {
          "3.2. Results with Different Augmentation Methods": "in three steps. The first step is to calculate the loss of emotion clas-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "sification, and then optimize the feature fuser and the SER classifier",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "3.4. Results on the SER Task"
        },
        {
          "3.2. Results with Different Augmentation Methods": "simultaneously. The second step is to calculate the loss of domain",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "classification, and then only optimize the domain classifier, with the",
          "performance of\nthe model begins to decline. One possible reason": "With a suitable augmentation method and a suitable amount of data,"
        },
        {
          "3.2. Results with Different Augmentation Methods": "gradient of the feature fuser detached. The third step is to pass the",
          "performance of\nthe model begins to decline. One possible reason": "we train the downstream model with our method on the SER task."
        },
        {
          "3.2. Results with Different Augmentation Methods": "domain classification loss back to the feature fuser, perform gradient",
          "performance of\nthe model begins to decline. One possible reason": "More specifically, we use the training strategy of curriculum learn-"
        },
        {
          "3.2. Results with Different Augmentation Methods": "reversal, and optimize the feature fuser. Through adversarial\ntrain-",
          "performance of\nthe model begins to decline. One possible reason": "ing and set\nthe data ratio to 1:2 between synthetic data to real data."
        },
        {
          "3.2. Results with Different Augmentation Methods": "ing, data with different distributions will be adjusted to a similar",
          "performance of\nthe model begins to decline. One possible reason": "Extracted features employing the last\nlayer or multi-layer weighted"
        },
        {
          "3.2. Results with Different Augmentation Methods": "distribution within the feature fuser. However,\nthrough this training",
          "performance of\nthe model begins to decline. One possible reason": "from the pre-trained data2vec model are both considered. We com-"
        },
        {
          "3.2. Results with Different Augmentation Methods": "criterion, the performance of SER is not greatly improved. The rea-",
          "performance of\nthe model begins to decline. One possible reason": "pare our method with the baseline without synthetic data. We also"
        },
        {
          "3.2. Results with Different Augmentation Methods": "son may be that this method is designed to adapt the new domain to",
          "performance of\nthe model begins to decline. One possible reason": "compare with models augmented with synthetic data generated from"
        },
        {
          "3.2. Results with Different Augmentation Methods": "the original domain, while the effect on improving the performance",
          "performance of\nthe model begins to decline. One possible reason": "EmoDiff\n[40],\nthe most\nrecent emotional TTS utilizing diffusion"
        },
        {
          "3.2. Results with Different Augmentation Methods": "of the original domain is limited.",
          "performance of\nthe model begins to decline. One possible reason": "technique, with the same emotional\ntext. As shown in Table 4, our"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Transfer learning.\nTransfer learning is a training method that",
          "performance of\nthe model begins to decline. One possible reason": "method improves the SER performance on both weighted accuracy"
        },
        {
          "3.2. Results with Different Augmentation Methods": "transfers\nlearned knowledge to target data.\nUnder\nthis paradigm,",
          "performance of\nthe model begins to decline. One possible reason": "and unweighted accuracy. Experiments with EmoDiff also improve"
        },
        {
          "3.2. Results with Different Augmentation Methods": "we first\ntrain the SER model on synthetic speech,\nthen reduce the",
          "performance of\nthe model begins to decline. One possible reason": "compared to the baseline,\nindicating that our method can be trans-"
        },
        {
          "3.2. Results with Different Augmentation Methods": "learning rate, and then migrate to the IEMOCAP dataset for train-",
          "performance of\nthe model begins to decline. One possible reason": "ferred to other emotional TTS."
        },
        {
          "3.2. Results with Different Augmentation Methods": "ing. Through transfer learning,\nthe performance of the model\nis ef-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "Table 4. Results on SER task compared with baselines. Represen-"
        },
        {
          "3.2. Results with Different Augmentation Methods": "fectively improved.",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "tations using the last\nlayer or multi-layer weighted with learnable"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Curriculum learning.\nCurriculum learning is a training strat-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "parameters are both considered. WA and UA are reported here."
        },
        {
          "3.2. Results with Different Augmentation Methods": "egy that organizes the learning process by gradually increasing the",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "complexity of training samples.\nIn this case, we sort the augmented",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "Representations\nTraining Data\nWA ↑\nUA ↑"
        },
        {
          "3.2. Results with Different Augmentation Methods": "data from short to long, and gradually add it to the original data ev-",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "IEMOCAP\n64.52\n68.22"
        },
        {
          "3.2. Results with Different Augmentation Methods": "ery fixed number of epochs during the training phase. The model",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "last layer\nIEMOCAP + EmoDiff\n68.02\n69.67"
        },
        {
          "3.2. Results with Different Augmentation Methods": "has great performance gain on the IEMOCAP dataset in this way.",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "IEMOCAP + ours\n68.57\n70.86"
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "IEMOCAP\n68.02\n69.95"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Table 3.\nResults with different augmentation methods compared",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "multiple layers\nIEMOCAP + EmoDiff\n68.39\n71.06"
        },
        {
          "3.2. Results with Different Augmentation Methods": "with the baseline. WA and UA are reported here.",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "IEMOCAP + ours\n68.85\n71.89"
        },
        {
          "3.2. Results with Different Augmentation Methods": "Method\nWA(%) ↑\nUA (%)↑",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "baseline\n64.52\n68.22",
          "performance of\nthe model begins to decline. One possible reason": "4. CONCLUSION"
        },
        {
          "3.2. Results with Different Augmentation Methods": "random mixing\n64.24\n68.06",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "In this paper, we aim to leverage state-of-the-art techniques for aug-"
        },
        {
          "3.2. Results with Different Augmentation Methods": "adversarial training\n64.88\n68.85",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "menting speech emotion recognition. We utilize an awesome large"
        },
        {
          "3.2. Results with Different Augmentation Methods": "transfer learning\n68.20\n70.72",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "language model contemporarily, GPT-4,\nto generate text with emo-"
        },
        {
          "3.2. Results with Different Augmentation Methods": "curriculum learning\n68.57\n70.86",
          "performance of\nthe model begins to decline. One possible reason": ""
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "tional expressions, and Azure Emotional TTS,\nto synthesize speech"
        },
        {
          "3.2. Results with Different Augmentation Methods": "",
          "performance of\nthe model begins to decline. One possible reason": "with emotional consistency. We also leverage a powerful speech-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "“A fine-tuned wav2vec\n2.0/hubert\nbenchmark\nfor\nspeech\nemotion"
        },
        {
          "5. REFERENCES": "[1]\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, Junaid Qadir,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "recognition, speaker verification and spoken language understanding,”"
        },
        {
          "5. REFERENCES": "and Bjoern W Schuller,\n“Survey of deep representation learning for",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "arXiv preprint arXiv:2111.02735, 2021."
        },
        {
          "5. REFERENCES": "speech emotion recognition,” Trans. of TAC, 2021.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[23]\nJohannes Wagner, Andreas Triantafyllopoulos, Hagen Wierstorf, Maxi-"
        },
        {
          "5. REFERENCES": "[2]\nShu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I\nJeff Lai,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "milian Schmitt, Felix Burkhardt, Florian Eyben, and Bj¨orn W Schuller,"
        },
        {
          "5. REFERENCES": "Kushal Lakhotia, Yist Y Lin, Andy T Liu, Jiatong Shi, Xuankai Chang,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "“Dawn of the Transformer era in speech emotion recognition: closing"
        },
        {
          "5. REFERENCES": "Guan-Ting Lin, et al.,\n“SUPPERB: Speech processing universal per-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "the valence gap,” Trans. of TPAMI, 2023."
        },
        {
          "5. REFERENCES": "formance benchmark,” Proc. of Interspeech, 2021.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[24] George Ioannides, Michael Owen, Andrew Fletcher, Viktor Rozgic,"
        },
        {
          "5. REFERENCES": "[3] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe Kazemzadeh,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "and Chao Wang,\n“Towards paralinguistic-only speech representations"
        },
        {
          "5. REFERENCES": "Emily Mower, Samuel Kim,\nJeannette N Chang, Sungbok Lee, and",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Proc. of\nfor\nend-to-end speech emotion recognition,”\nInterspeech,"
        },
        {
          "5. REFERENCES": "Shrikanth S Narayanan, “IEMOCAP: Interactive emotional dyadic mo-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "2023."
        },
        {
          "5. REFERENCES": "tion capture database,” J. of LRE, 2008.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[25] Weidong Chen, Xiaofen Xing, Peihao Chen, and Xiangmin Xu,\n“Ves-"
        },
        {
          "5. REFERENCES": "[4]\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "per: A compact and effective pretrained model\nfor\nspeech emotion"
        },
        {
          "5. REFERENCES": "Proc.\n“wav2vec: Unsupervised pre-training for speech recognition,”",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "recognition,” arXiv preprint arXiv:2307.10757, 2023."
        },
        {
          "5. REFERENCES": "Interspeech, 2019.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[26]\nTom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur,"
        },
        {
          "5. REFERENCES": "[5] Alexei Baevski, Steffen Schneider, and Michael Auli,\n“vq-wav2vec:",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "“Audio augmentation for speech recognition,” in Proc. of Interspeech,"
        },
        {
          "5. REFERENCES": "Self-supervised learning of discrete speech representations,”\nin Proc.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "2015."
        },
        {
          "5. REFERENCES": "of ICLR, 2019.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[27] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret"
        },
        {
          "5. REFERENCES": "[6] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Zoph, Ekin D Cubuk, and Quoc V Le,\n“Specaugment: A simple data"
        },
        {
          "5. REFERENCES": "Auli,\n“wav2vec 2.0: A framework for\nself-supervised learning of",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "augmentation method for automatic speech recognition,” Proc. of In-"
        },
        {
          "5. REFERENCES": "speech representations,” Proc. of NeurIPS, 2020.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "terspeech, 2019."
        },
        {
          "5. REFERENCES": "[7] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakho-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[28] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-"
        },
        {
          "5. REFERENCES": "tia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “Hubert: Self-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Proc. of\nPaz,\n“mixup: Beyond empirical\nrisk minimization,”\nICLR,"
        },
        {
          "5. REFERENCES": "supervised speech representation learning by masked prediction of hid-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "2018."
        },
        {
          "5. REFERENCES": "den units,” Trans. of TASLP, 2021.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[29]\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, and Julien Epps,"
        },
        {
          "5. REFERENCES": "[8]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "“Direct modelling of speech emotion from raw speech,” Proc. of Inter-"
        },
        {
          "5. REFERENCES": "Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "speech, 2019."
        },
        {
          "5. REFERENCES": "et al.,\n“Wavlm: Large-scale self-supervised pre-training for full stack",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "speech processing,” J. of JSTSP, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[30]\nEgor Lakomkin, Mohammad Ali Zamani, Cornelius Weber,\nSven"
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Magg, and Stefan Wermter,\n“On the robustness of\nspeech emotion"
        },
        {
          "5. REFERENCES": "[9] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "recognition for human-robot\ninteraction with deep neural networks,”"
        },
        {
          "5. REFERENCES": "and Michael Auli, “Data2vec: A general framework for self-supervised",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "in Proc. of IROS, 2018."
        },
        {
          "5. REFERENCES": "learning in speech, vision and language,” Proc. of ICML, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[31]\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, and Bj¨orn W"
        },
        {
          "5. REFERENCES": "[10]\nZiyang Ma, Zhisheng Zheng, Changli Tang, Yujin Wang, and Xie Chen,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Schuller,\n“Multitask learning from augmented auxiliary data for\nim-"
        },
        {
          "5. REFERENCES": "“MT4SSL: Boosting self-supervised speech representation learning by",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "proving speech emotion recognition,” Trans. of TAC, 2022."
        },
        {
          "5. REFERENCES": "integrating multiple targets,” Proc. of Interspeech, 2023.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[32]\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, and Bj¨orn W"
        },
        {
          "5. REFERENCES": "[11]\nZhuoyuan Yao, Shuo Ren, Sanyuan Chen, Ziyang Ma, Pengcheng",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Schuller, “Deep architecture enhancing robustness to noise, adversarial"
        },
        {
          "5. REFERENCES": "Guo, and Lei Xie,\n“Tessp:\ntext-enhanced self-supervised speech pre-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "attacks, and cross-corpus setting for speech emotion recognition,”\nin"
        },
        {
          "5. REFERENCES": "training,” arXiv preprint arXiv:2211.13443, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Proc. of Interspeech, 2020."
        },
        {
          "5. REFERENCES": "[12] Xiaohuan Zhou, Jiaming Wang, Zeyu Cui, Shiliang Zhang, Zhijie Yan,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[33] Raghavendra Pappagari, Tianzi Wang,\nJesus Villalba, Nanxin Chen,"
        },
        {
          "5. REFERENCES": "Jingren Zhou, and Chang Zhou,\n“MMspeech: Multi-modal multi-task",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "and Najim Dehak,\n“x-vectors meet emotions: A study on dependen-"
        },
        {
          "5. REFERENCES": "Proc. of\nInter-\nencoder-decoder pre-training for speech recognition,”",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "cies between emotion and speaker recognition,”\nin Proc. of ICASSP,"
        },
        {
          "5. REFERENCES": "speech, 2023.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "2020."
        },
        {
          "5. REFERENCES": "[13]\nZiyang Ma, Zhisheng Zheng, Guanrou Yang, Yu Wang, Chao Zhang,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[34] Bagus Tris Atmaja and Akira Sasou, “Effects of data augmentations on"
        },
        {
          "5. REFERENCES": "and Xie Chen,\n“Pushing the limits of unsupervised unit discovery for",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "speech emotion recognition,” J. of Sensors, 2022."
        },
        {
          "5. REFERENCES": "SSL speech representation,” Proc. of Interspeech, 2023.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[35]\nFang Bao, Michael Neumann, and Ngoc Thang Vu,\n“Cyclegan-based"
        },
        {
          "5. REFERENCES": "[14] William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "emotion style transfer as data augmentation for speech emotion recog-"
        },
        {
          "5. REFERENCES": "Maiti,\nand Shinji Watanabe,\n“Reducing barriers\nto self-supervised",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "nition.,” in Proc. of Interspeech, 2019."
        },
        {
          "5. REFERENCES": "learning: Hubert pre-training with academic compute,”\nin Proc. of In-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "terspeech, 2023.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[36] Georgios Rizos, Alice Baird, Max Elliott, and Bj¨orn Schuller, “Stargan"
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "for emotional speech conversion: Validated by data augmentation of"
        },
        {
          "5. REFERENCES": "[15]\nZhengyang Chen, Sanyuan Chen, Yu Wu, Yao Qian, Chengyi Wang,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "end-to-end emotion recognition,” in Proc. of ICASSP, 2020."
        },
        {
          "5. REFERENCES": "Shujie Liu, Yanmin Qian,\nand Michael Zeng,\n“Large-scale\nself-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "supervised speech representation learning for automatic speaker veri-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[37]\nIbrahim Malik, Siddique Latif, Raja Jurdak, and Bj¨orn Schuller,\n“A"
        },
        {
          "5. REFERENCES": "fication,” in Proc. of ICASSP, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "preliminary study on augmenting speech emotion recognition using a"
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "diffusion model,” in Proc. of Interspeech, 2023."
        },
        {
          "5. REFERENCES": "[16] Wen Wu, Chao Zhang, and Philip C. Woodland,\n“Integrating emotion",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "recognition with speech recognition and speaker diarisation for conver-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[38]\nZixing Zhang, Liyizhe Peng, Tao Pang,\nJing Han, Huan Zhao,\nand"
        },
        {
          "5. REFERENCES": "sations,” in Proc. of Interspeech, 2023.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Bjorn W Schuller, “Refashioning emotion recognition modelling: The"
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "advent of generalised large models,” arXiv preprint arXiv:2308.11578,"
        },
        {
          "5. REFERENCES": "[17]\nLeonardo Pepino, Pablo Riera, and Luciana Ferrer, “Emotion recogni-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "2023."
        },
        {
          "5. REFERENCES": "tion from speech using wav2vec 2.0 embeddings,” Proc. of Interspeech,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "2021.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[39] OpenAI, “GPT-4 technical report,” 2023."
        },
        {
          "5. REFERENCES": "[18] Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai, “Ex-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[40] Yiwei Guo, Chenpeng Du, Xie Chen, and Kai Yu,\n“Emodiff:\nInten-"
        },
        {
          "5. REFERENCES": "ploration of a self-supervised speech model: A study on emotional cor-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "sity controllable emotional text-to-speech with soft-label guidance,” in"
        },
        {
          "5. REFERENCES": "pora,” in Proc. of SLT, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Proc. of ICASSP, 2023."
        },
        {
          "5. REFERENCES": "[19]\nEdmilson Morais, Ron Hoory, Weizhong Zhu, Itai Gat, Matheus Dam-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[41] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, et al.,\n“Gi-"
        },
        {
          "5. REFERENCES": "asceno, and Hagai Aronowitz, “Speech emotion recognition using self-",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "gaspeech: An evolving, multi-domain asr corpus with 10,000 hours of"
        },
        {
          "5. REFERENCES": "supervised features,” in Proc. of ICASSP, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "transcribed audio,” arXiv preprint arXiv:2106.06909, 2021."
        },
        {
          "5. REFERENCES": "[20]\nLi-Wei Chen and Alexander Rudnicky,\n“Exploring wav2vec 2.0 fine",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[42]\nPaul Taylor and Amy Isard,\n“SSML: A speech synthesis markup lan-"
        },
        {
          "5. REFERENCES": "tuning for improved speech emotion recognition,” in Proc. of ICASSP,",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "guage,” J. of Speech Communication, 1997."
        },
        {
          "5. REFERENCES": "2023.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "[43] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain,"
        },
        {
          "5. REFERENCES": "[21]\nItai Gat, Hagai Aronowitz, Weizhong Zhu, Edmilson Morais, and Ron",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand,\nand Victor"
        },
        {
          "5. REFERENCES": "Hoory,\n“Speaker normalization for\nself-supervised speech emotion",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "J. of\nLempitsky,\n“Domain-adversarial\ntraining of neural networks,”"
        },
        {
          "5. REFERENCES": "recognition,” in Proc. of ICASSP, 2022.",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": ""
        },
        {
          "5. REFERENCES": "",
          "[22] Yingzhi Wang, Abdelmoumene Boumadane, and Abdelwahab Heba,": "JMLR, 2016."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Junaid Qadir",
        "Bjoern Schuller"
      ],
      "year": "2021",
      "venue": "Survey of deep representation learning for speech emotion recognition"
    },
    {
      "citation_id": "3",
      "title": "SUPPERB: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "J. of LRE"
    },
    {
      "citation_id": "5",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "8",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "Trans. of TASLP"
    },
    {
      "citation_id": "9",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "J. of JSTSP"
    },
    {
      "citation_id": "10",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "11",
      "title": "MT4SSL: Boosting self-supervised speech representation learning by integrating multiple targets",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Changli Tang",
        "Yujin Wang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Tessp: text-enhanced self-supervised speech pretraining",
      "authors": [
        "Zhuoyuan Yao",
        "Shuo Ren",
        "Sanyuan Chen",
        "Ziyang Ma",
        "Pengcheng Guo",
        "Lei Xie"
      ],
      "year": "2022",
      "venue": "Tessp: text-enhanced self-supervised speech pretraining",
      "arxiv": "arXiv:2211.13443"
    },
    {
      "citation_id": "13",
      "title": "MMspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition",
      "authors": [
        "Xiaohuan Zhou",
        "Jiaming Wang",
        "Zeyu Cui",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou",
        "Chang Zhou"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Pushing the limits of unsupervised unit discovery for SSL speech representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Reducing barriers to self-supervised learning: Hubert pre-training with academic compute",
      "authors": [
        "William Chen",
        "Xuankai Chang",
        "Yifan Peng",
        "Zhaoheng Ni",
        "Soumi Maiti",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Large-scale selfsupervised speech representation learning for automatic speaker verification",
      "authors": [
        "Zhengyang Chen",
        "Sanyuan Chen",
        "Yu Wu",
        "Yao Qian",
        "Chengyi Wang",
        "Shujie Liu",
        "Yanmin Qian",
        "Michael Zeng"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Integrating emotion recognition with speech recognition and speaker diarisation for conversations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "Proc. of SLT"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using selfsupervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "21",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Speaker normalization for self-supervised speech emotion recognition",
      "authors": [
        "Itai Gat",
        "Hagai Aronowitz",
        "Weizhong Zhu",
        "Edmilson Morais",
        "Ron Hoory"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "23",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "24",
      "title": "Dawn of the Transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "Trans. of TPAMI"
    },
    {
      "citation_id": "25",
      "title": "Towards paralinguistic-only speech representations for end-to-end speech emotion recognition",
      "authors": [
        "George Ioannides",
        "Michael Owen",
        "Andrew Fletcher",
        "Viktor Rozgic",
        "Chao Wang"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Peihao Chen",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "arxiv": "arXiv:2307.10757"
    },
    {
      "citation_id": "27",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "William Daniel S Park",
        "Yu Chan",
        "Chung-Cheng Zhang",
        "Barret Chiu",
        "Ekin Zoph",
        "Quoc V Cubuk",
        "Le"
      ],
      "year": "2019",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "29",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "David Yann N Dauphin",
        "Lopez-Paz"
      ],
      "year": "2018",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "30",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Julien Epps"
      ],
      "year": "2019",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "31",
      "title": "On the robustness of speech emotion recognition for human-robot interaction with deep neural networks",
      "authors": [
        "Egor Lakomkin",
        "Mohammad Ali Zamani",
        "Cornelius Weber",
        "Sven Magg",
        "Stefan Wermter"
      ],
      "year": "2018",
      "venue": "Proc. of IROS"
    },
    {
      "citation_id": "32",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Multitask learning from augmented auxiliary data for improving speech emotion recognition"
    },
    {
      "citation_id": "33",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "34",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "Raghavendra Pappagari",
        "Tianzi Wang",
        "Jesus Villalba",
        "Nanxin Chen",
        "Najim Dehak"
      ],
      "year": "2020",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "35",
      "title": "Effects of data augmentations on speech emotion recognition",
      "authors": [
        "Bagus Tris",
        "Akira Sasou"
      ],
      "year": "2022",
      "venue": "J. of Sensors"
    },
    {
      "citation_id": "36",
      "title": "Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition",
      "authors": [
        "Fang Bao",
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "37",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of end-to-end emotion recognition",
      "authors": [
        "Georgios Rizos",
        "Alice Baird",
        "Max Elliott",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "38",
      "title": "A preliminary study on augmenting speech emotion recognition using a diffusion model",
      "authors": [
        "Ibrahim Malik",
        "Siddique Latif",
        "Raja Jurdak",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "39",
      "title": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "authors": [
        "Zixing Zhang",
        "Liyizhe Peng",
        "Tao Pang",
        "Jing Han",
        "Huan Zhao",
        "Bjorn Schuller"
      ],
      "year": "2023",
      "venue": "Refashioning emotion recognition modelling: The advent of generalised large models",
      "arxiv": "arXiv:2308.11578"
    },
    {
      "citation_id": "40",
      "title": "GPT-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report"
    },
    {
      "citation_id": "41",
      "title": "Emodiff: Intensity controllable emotional text-to-speech with soft-label guidance",
      "authors": [
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "42",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "Guoguo Chen",
        "Shuzhou Chai",
        "Guanbo Wang",
        "Jiayu Du"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "arxiv": "arXiv:2106.06909"
    },
    {
      "citation_id": "43",
      "title": "SSML: A speech synthesis markup language",
      "authors": [
        "Paul Taylor",
        "Amy Isard"
      ],
      "year": "1997",
      "venue": "J. of Speech Communication"
    },
    {
      "citation_id": "44",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Mario Franc ¸ois Laviolette",
        "Victor Marchand",
        "Lempitsky"
      ],
      "year": "2016",
      "venue": "J. of JMLR"
    }
  ]
}