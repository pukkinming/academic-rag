{
  "paper_id": "2410.11861v1",
  "title": "Investigating Role Of Big Five Personality Traits In Audio-Visual Rapport Estimation",
  "published": "2024-10-07T08:52:33Z",
  "authors": [
    "Takato Hayashi",
    "Ryusei Kimura",
    "Ryo Ishii",
    "Shogo Okada"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic rapport estimation in social interactions is a central component of affective computing. Recent reports have shown that the estimation performance of rapport in initial interactions can be improved by using the participant's personality traits as the model's input. In this study, we investigate whether this findings applies to interactions between friends by developing rapport estimation models that utilize nonverbal cues (audio and facial expressions) as inputs. Our experimental results show that adding Big Five features (BFFs) to nonverbal features can improve the estimation performance of self-reported rapport in dyadic interactions between friends. Next, we demystify how BFFs improve the estimation performance of rapport through a comparative analysis between models with and without BFFs. We decompose rapport ratings into perceiver effects (people's tendency to rate other people), target effects (people's tendency to be rated by other people), and relationship effects (people's unique ratings for a specific person) using the social relations model. We then analyze the extent to which BFFs contribute to capturing each effect. Our analysis demonstrates that the perceiver's and the target's BFFs lead estimation models to capture the perceiver and the target effects, respectively. Furthermore, our experimental results indicate that the combinations of facial expression features and BFFs achieve best estimation performances not only in estimating rapport ratings, but also in estimating three effects. Our study is the first step toward understanding why personality-aware estimation models of interpersonal perception accomplish high estimation performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The term rapport can be defined as the feeling of being \"in sync\" with a conversational partner  [14] . If a machine learning model can estimate the rapport of dyads, the system will be able to provide various types of support according to the estimates. For example, the rapport between students leads to learning gains in peer tutoring  [27] , so if teachers have access to the estimates, they can support dyads (pairs of students) more selectively. In these cases, dyads are already in a close relationship to some extent. Previous research  [3, 9, 13, 20, 24, 26, 32]  has addressed rapport estimation in initial interactions, but despite its importance, only a few studies  [20, 32]  have addressed it in interactions between intimate participants. Thus, we focus here on rapport estimation in dyadic interactions between friends.\n\nOur final goal is to clarify how to maximize the potential of rapport estimation models by using personality traits in interactions between friends. In general rapport estimation, the input of rapport estimation models is the participant's behavioral features in an interaction, and the output is a value or class indicating the degree of rapport. Among the previous studies on automatic rapport estimation  [3, 24] , a key finding is that the use of personality trait features based on the five-factor model can improve the estimation performances of rapport in initial interactions. In addition to the empirical evidence, various theoretical findings regarding personality traits and interpersonal perceptions (e.g., rapport) have been reported in the field of social psychology  [4, 7, 11, 31] . For example, Cuperman et al.  [4]  showed that participants with high agreeableness tend to rate rapport toward their conversational partners highly.\n\nOur first research objective (RO-1) is to clarify whether Big Five features (BFFs) can improve the estimation performance of rapport in interactions between friends. As stated above, it has already been established that adding personality trait features to behavioral features improves the estimation performance of rapport  [3, 24] . However, it is unclear whether this finding applies to interactions between friends since the association between personality traits and rapport changes depending on the relationship between participants. For example, according to Tickle-Degnen and Rosenthal, positivity is important for building rapport in initial interactions, but its importance decreases as intimacy among participants increases  [28] . Thus, even though extraversion (the Big Five dimension related to positivity) is a strong cue for estimating rapport in initial interactions, it may be a weak cue in interactions between friends.\n\nTo achieve RO-1, we develop personality-aware rapport estimation models (see Fig.  1 ). Furthermore, we investigate how to use BFFs to achieve higher performance improvement. To this end, we combine the BFFs with audio, facial expressions, and bimodal features, and then compare their performance. We also compare the performance under three conditions: 1) only the BFFs of a perceiver (person who rates rapport) are accessible, 2) only the BFFs of a target (person whose rapport is rated) are accessible, and 3) both the perceiver's and the target's BFFs are accessible.\n\nThe second research objective (RO-2) is to reveal the role of BFFs in the performance improvement through comparisons between the model with and without BFFs. Our experimental results related to RO-1 show that BFFs substantially improve the rapport estimation performance. The ratings of interpersonal perception consist of three components: (a) people's overall tendencies to rate other people (perceiver effect), (b) people's overall tendencies to be rated by other people (target effect), and (c) people's unique ratings for a specific partner beyond the perceiver and target effects (relationship effect)  [15, 16] . These effects represent the different aspects of interpersonal perceptions. Thus, if the estimation performance of rapport ratings improves, the model becomes able to capture any or all of these effects more accurately. At the same time, better capturing each effect leads to better estimation performances of rapport ratings. Hence, we reveal the extent to which performance gains by BFFs can be attributed to capturing each effect.\n\nTo calculate three effects, we introduce an analysis approach based on a social relations model (SRM)  [16, 17] , which is a tool for understanding human perception between two individuals. First, we decompose true (human) ratings and the estimates of ratings into the three effects using SRM. Second, to measure how a model can capture these effects, we calculate the Pearson's product-moment correlation coefficient (PCC) and concordance correlation coefficient (CCC) between the true effects and the estimates of effects. Finally, we compare the PCC and CCC for each effect between the model with and without BFFs.\n\nFor RO-2, we come up with three hypotheses and verify them: 1) the perceiver's BFFs lead models to capture perceiver effects, 2) the target's BFFs lead models to capture target effects, and 3) the combinations of both BFFs lead models to capture relationship effects. The first and second hypotheses are based on evidence that the perceiver's Big Five personality traits influence the perceiver's tendency to rate rapport with other people and evidence that the target's Big Five personality traits influence the target's tendency to be rated by other people, respectively  [4] . The third hypothesis is based on evidence that the specific combinations of the Big Five personality traits (e.g., both have high agreeableness) lead to uniquely high or low rapport  [4] .\n\nThe motivation behind the two research objectives is to reveal findings that help us develop a better rapport estimation model among intimate dyads. RO-1 provides readers with information to judge whether performance gain is worth the cost of collecting Big Five personality traits. As for achieving RO-2, it provides interpretability about the performance gain by BFFs and clarifies which types of BFFs are suitable for obtaining the desired auxiliary information. If the models can capture any or all three effects, an estimate of the rapport ratings supplies the auxiliary information. For example, estimates of target effects are useful for detecting exceptionally undesirable people (i.e., low target effect). Furthermore, beyond the limit of rapport and BFFs, our analysis approach is helpful in analyzing the extent to which model improvements or data extensions have contributed to capturing different aspects of interpersonal perception.\n\nIn summation, our contributions are delineated as follows:\n\nâ€¢ Our experimental results show that BFFs can improve the estimation performance of self-reported rapport in dyadic interactions between friends. â€¢ We introduce an analysis approach using SRM, which reveals the extent to which the model can capture perceiver, target, and relationship effects. â€¢ We present evidence that the perceiver's and target's BFFs lead estimation models to accurately capture the perceiver and target effects, respectively. â€¢ We demonstrate that the combinations of facial expression and BFFs achieve best estimation performances not only in estimating rapport ratings, but also in estimating three effects. Section 2 provides an overview of related works. Section 3 introduces our dataset and analysis results, and Section 4 explains the methods of feature extraction, model, and analysis approach using SRM. In Section 5, we describe the experimental settings. Section 6 shows our experimental results, and in Section 7, we discuss their implications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works 2.1 Rapport And Machine Learning",
      "text": "As rapport plays an essential role in building good relationships with others, it is an extensively researched topic in social psychology. Bernieri et al.  [2]  outlined nonverbal cues indicating rapport in various contexts. Tickle-Degnen and Rosenthal  [28]  demonstrated that the key nonverbal cues indicating rapport can change depending on the relationship between participants. Harrigan et al.  [10]  found significant differences of eye movement and gesture between high-and low-rapport doctors. Furthermore, Miles et al.  [23]  showed that synchrony between two participants is associated with high rapport in both visual and audio cues. Grahe and Bernieri  [8]  clarified that visual cues are the most useful for the observer to perceive rapport accurately. Overall, previous studies provide evidence that visual cues are more strongly associated with rapport than audio cues.\n\nInspired by findings on the relationships between nonverbal cues and rapport, recent researchers in machine learning have addressed automatic rapport estimation. Hayashi et al.  [12, 13]  proposed a ranking model to rank conversation partners based on the degree of self-reported rapport. Other studies  [20, 32]  have estimated the rapport between two students in a peer tutoring scenario. As with studies in social psychology, visual features are found to be more useful cues for rapport estimation than audio features  [3, 9, 24, 30] .\n\nAmong the previous studies on rapport estimation, a key finding is that the use of personality trait features based on the five-factor model can improve the estimation performances in initial interactions  [3, 24] . Cerekovic et al.  [3]  showed that a model using features combining social cues and personality traits achieves the best estimation performance of rapport for virtual agents. Similarly, in the task of detecting low rapport in the early stages of group interaction, Muller et al.  [24]  found that adding personality trait features to facial expression features improves the estimation performance. These two studies formulated rapport estimation as classifying the high/low rapport class. However, in these cases, the estimation results have only a little information about the degree of rapport. Furthermore, there is a risk of adding bias due to the class splitting criteria  [21] . Therefore, in this study, we formulate rapport estimation as regression. In addition, in contrast to previous studies investigating initial interactions, we focus on the usefulness of personality traits in friend interactions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Interpersonal Perception And Social Relations Model",
      "text": "The ratings of interpersonal perception consist of three components: perceiver, target, and relationship effects  [16] . These effects represent the different aspects of interpersonal perceptions. As an example, take the perception of rapport that John feels for Tom. The perception can be decomposed into three components: a) John's tendencies to rate with other people (John's perceiver effect), b) Tom's tendencies to be rated by other people (Tom's target effect), and c) John's unique ratings for Tom beyond perceiver and target effect (John's relationship effect to Tom). These effects are typically calculated using a social relations model (SRM)  [16, 17] , which is a tool for analyzing human perception between two individuals. In this section, we explain how to calculate these effects with the settings of a round-robin design, in which the same participant serves as both a perceiver and a target (see table in Fig.  4 ). According to SRM  [16, 17] , the rapport score by perceiver ğ‘– to target ğ‘— in group ğ‘˜ consists of the perceiver effect, the target effect, the relationship effect, and the group average:\n\nwhere ğ‘ ğ‘– is the perceiver effect for person ğ‘–; ğ‘¡ ğ‘— is the target effect for person ğ‘—; ğ‘Ÿ ğ‘– ğ‘— is the relationship effect for ğ‘– with ğ‘—; and ğ‘€ ..ğ‘˜ is the mean of the ratings given by all participants in group ğ‘˜. The perceiver effect represents how the perceiver views other people on average, and the target effect represents how the target is viewed on average by others. The relationship effect represents the perceiver's unique view toward the target. The perceiver effect (ğ‘ ğ‘– ) is\n\nand the target effect (ğ‘¡ ğ‘– ) is calcurated as\n\nwhere ğ‘€ ğ‘–.ğ‘˜ is the mean of the ratings given by participant ğ‘– in group ğ‘˜. ğ‘€ .ğ‘–ğ‘˜ is the mean of the ratings given to participant ğ‘– in group ğ‘˜, and ğ‘› is the group size. The relationship effect for perceiver ğ‘– with target ğ‘— (ğ‘” ğ‘– ğ‘— ) is\n\nPrevious studies have reported theoretical findings regarding the relationship between personality traits and interpersonal perceptions  [4, 7, 11, 31] . Cuperman et al.  [4]  showed that participants with high agreeableness tend to rate rapport toward their conversational partners highly and to be rated highly by their conversational partners. Wood et al.  [31]  also reported that participants with high agreeableness tend to rate their friend positively. Furthermore, participants' extraversion and agreeableness are positively correlated with their ratings of friendship satisfaction  [11] . Overall, the Big Five dimensions most closely related to interpersonal perception are extraversion and agreeableness, but other dimensions can influence it as well. Inspired by these studies, we hypothesize that personality trait features promote estimation models to capture people's rating tendencies. We introduce SRM to confirm this hypothesis. Our study is the first attempt to use SRM to analyze estimation models.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Data",
      "text": "To achieve our research objectives, we collected online dyadic interactions between friends. In Section 3.1, we present an overview of our dataset, and in Section 3.2, we analyze the relationship between the Big Five and the three effects of rapport ratings. All recordings were reviewed and approved by our institution's research ethics committee.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We recruited eight groups consisting of four participants each who were friends with each other through a recruitment agency. There were a total of 32 Japanese participants (16 men, 16 women; 26 in their 20s, 6 in their 30s), and each participant was paired with other participants in the same group (48 dyads). All participants in the same dyads were the same gender. Each dyad conducted three online interactions based on different conversation topics. Each interaction lasted 20 minutes. We used the first interactions for our analysis and experiments because the first are the least restrictive and the most natural interactions. In the first interaction, the dyad introduces themselves (e.g., their favorite foods and artists). For more information about the recording settings and topics, see Section III of  [13] , as we strictly followed their procedure for collecting initial interactions. The dataset includes participants' rapport ratings for their conversational partners, which was reported after each interaction. We utilized a questionnaire developed by Bernieri et al.  [2]  to measure self-reported rapport. A previous study translated the questionnaire from English into Japanese, and its internal consistency among items was sufficient (ğ›¼ = 0.92)  [18] . The questionnaire comprises 18 items, each of which is rated on an 8-point Likert scale (1 = strongly disagree, 8 = strongly agree). We define the rapport score as the sum of the responses after reversing the values of the negative questions. In addition, at the end of all recordings, participants completed the Japanese Big Five questionnaire  [29] , which consists of 60 items on a 7-point Likert scale. After reversing the values of the negative questions, we summed the responses along each Big Five dimension; Fig.  3  shows box plots of the rapport score, the three effects, and the Big Five dimensions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Data Analysis",
      "text": "3.2.1 Perceiver effect and target effect. We calculated the Pearson's product-moment correlation coefficient to estimate the association between the perceiver/target effects and Big Five dimensions (see Table  1 ). In a previous study conducting large-scale research on initial interactions (87 dyads), extraversion and agreeableness were significantly associated with perceiver and target effects of rapport perception  [4] . Although we found no significant correlations in our data (significance level ğ›¼ = .05), this result may be due to the insufficient sample size (48 dyads). A small positive correlation was observed between the perceiver effects and neuroticism (.25). Extraversion and openness had small positive correlations with the target effects (.21), and conscientiousness had a small negative correlation with the target effects (-.21). Our different results here compared to previous studies  [4]  may be due to differences in cultural backgrounds or relationships among participants, but we cannot say this for certain. Overall, the findings suggest that the perceiver's and target's Big Five features (BFFs) lead models to capture the perceiver and target effects, respectively.\n\n3.2.2 Relationship effect. We present bar plots in Fig.  3  to visualize the relationship effects according to dyads that have specific combinations of Big Five dimensions. For each Big Five dimension, we categorized the perceiver and target into two types (High or Low) using a threshold. The threshold is defined as ğ‘€ Â±ğ‘†ğ· Ã—0.5, where ğ‘€ is the mean total responses corresponding to the Big Five dimension and ğ‘†ğ· is the standard deviation. We focus here on extraversion and agreeableness, as combinations of type in these dimensions lead to unique high or low rapport  [4] . In our dataset, dyads with different extraversion types tended to yield negative relationship effects. Furthermore, dyads between a disagreeable perceiver and an agreeable target yielded negative relationship effects. These results suggest that the combination of the perceiver's and target's BFFs leads models to accurately capture the relationship effects.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method 4.1 Features Extraction",
      "text": "4.1.1 Audio features. We used OpenSMILE  [6]  to extract audio features from each utterance. Audio features were based on eGeMAPS  [5] , which is recognized as the default setting in speech emotion recognition. Audio features were 88 dimensions, including pitch and volume. We standardized them across the utterance pool of each participant.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Facial Expression Features.",
      "text": "We utilized OpenFace  [1]  to extract the intensity of 17 action units (AUs) from each frame. Facial expression features included 14 statistics calculated for each AU; thus, they totaled 238 (17 Ã— 14) dimensions. The 14 statistics were the mean, median, standard deviation, skewness, kurtosis, maximum and minimum, mean of the first and second differences, range, slope, intercept of the linear approximation, and the 25th and 75th percentile values. We standardized the facial expression features across the utterance pool of each participant.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Big Five Features (Bffs).",
      "text": "We utilized responses on the Big Five questionnaire as BFFs. The Big Five questionnaire quantifies the Big Five dimensions of Extraversion, Agreeableness, Conscientiousness, Neuroticism, and Openness. The BFFs had 60 dimensions, which we normalized across the participant pool using min-max normalization.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model Architecture And Training.",
      "text": "We developed a mapping function ğ‘“ inspired by Poria et al.  [25] . Our mapping function is composed of unidirectional long short-term memory networks (LSTM) and fully connected neural networks (FCNN). The perceiver's sequence of utterances {ğ‘¼ 1 , â€¢ â€¢ â€¢ , ğ‘¼ ğ‘» } is input to the LSTM, and the output vector corresponding to the last utterance ğ’‰ ğ‘‡ is extracted. We then concatenate ğ’‰ ğ‘‡ and BFFs ğ‘©, and map this vector to the estimate of rapport score Å·,\n\nThe model is trained by minimizing the mean square error.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analytical Approach Using Social Relations Model (Srm)",
      "text": "Related to RO-2, we explain how to investigate the role of BFFs in performance improvement using SRM. As shown in Eq. (  1 ), the rapport score consist of the perceiver, target, relationship effect, and group average. Thus, when the estimation performance of rapport score, the model becomes able to capture any or all of these effects more accurately. Our approach is designed to demystify the performance improvement of the rapport score based on the variation in the estimation performance of each effect. This approach consists of three steps (see Fig.  4 ).\n\nIn the first step, we decompose true (human) rapport scores and estimates of rapport scores into perceiver, target, and relationship effects using SRM. We compute the three effects from the true rapport score according to Eqs. (  2 )-(  4 ). We define these effects as true effects. We also compute the three effects of the estimated rapport score. We define these effects as the estimates of effects.\n\nIn the second step, we calculate the Pearson's correlation coefficient (PCC) and concordance correlation coefficient (CCC) between the true effects and the estimates of effects.\n\nIn the third step, we compare the PCC and CCC for each effect between models with and without BFFs. By comparing the two models, it becomes quantitatively clear how useful the BFFs are in accurately capturing each effect.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Settings 5.1 Evaluation Methods",
      "text": "We evaluate estimation performances using an eight-fold crossvalidation, where each fold corresponds to one group consisting of four participants. The cross-validation ensured that the same participant was not duplicated across the training and test sets. In our data, 96 samples were created from 48 dyads since the rapport rating is bidirectional.\n\nThe Pearson's correlation coefficient (PCC) and concordance correlation coefficient (CCC)  [19]  were calculated based on true rapport scores and estimated rapport scores. PCC (ğœŒ) is defined as .01 Â±.09 A = Audio features; F = Facial expression features; BFF = Big Five features; P = Perceiver; T = Target. Bold and underlined values represent the best performances within each modality and across modalities, respectively. The asterisk denotes that the performance is significantly better than that of the model without BFF for each modality.\n\nwhere ğœ ğ‘¦ and ğœ Å· are standard deviations of true and estimated rapport scores, respectively. ğœ ğ‘¦ Å· is their covariance. CCC (ğœŒ ğ‘ ) is defined as\n\nwhere C is a bias correction factor:\n\nThe ğ¶ is between 0 and 1; the maximum value is realized when two variables have an identical mean and standard deviation. Thus, the CCC is a correlation coefficient that takes into account the similarity of distribution between two variables. Each experiment was performed 30 times based on different random seed values, and we reported the average performance and standard deviation to reduce the influence of the initial model parameters.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Model Settings",
      "text": "For training on the estimation models, we set the mini-batch size to 32 and the number of epochs to 50. Dropout was applied to the hidden layer of FCNN with a drop rate of 30%. The learning rate was 1.5e-4 for the models using audio features, 1.0e-4 for the models using facial expression features and bimodal features, and 1.0e-3 for the models using only BFFs to ensure that all models were sufficiently fitted to the training data with 50 epochs. All models were implemented in Pytorch 2.0.1, and all experiments were conducted on NVIDIA GeForce RTX 3090.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Table  2  shows the estimation performance of rapport scores and the perceiver, target, and relationship effects. Bold and underlined values represent the best performances within each modality and across modalities, respectively. The standard deviation of the rapport score and relationship effect represents the amount of variation per rapport score; the standard deviation of the perceiver and target effect represents the amount of variation per person. As a naive baseline, we used a model that predicts values randomly within the range of true rapport scores. To test if the performances of each estimation model are significantly better than those of the baseline, we conducted a Mann-Whitney U test (ğ‘ =30, ğ›¼=.05, onesided test), where ğ‘ corresponds to the number of random seed values. We also conducted the same statistical test to determine if the performance with Big Five features (BFFs) was significantly better than that without BFFs for each modality; the asterisk denotes this significant differences. In this section, we first report the performance of rapport scores. Then, we review the performance of the three effects. We mainly focus on unimodal models, as these models are the minimal units of bimodal models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Estimation Performance Of Rapport Scores",
      "text": "Rows 2 and 3 in Table  2  list the estimation performances of the rapport scores. Overall, models with facial expressions achieved high performances. Conversely, those with audio were low. In both modalities, the statistical test showed that the performances of all models were significantly better than that of the random baseline. Therefore, models were able to sufficiently predict rapport scores.\n\nThe results also indicate that adding BFFs improves performances. In facial expressions, the model (F + BFF[P+T]) achieved the highest performances across modalities with PCC (.38) and CCC (.21). Furthermore, both of the BFF[P] alone and the BFF[T] alone yielded performance gains. In audio, BFFs also led to better performances. In short, almost all models with BFFs achieved significantly better performances than models without BFFs in both modalities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Implicit Estimation Performance Of Perceiver, Target, And Relationship Effects",
      "text": "Next, we examine how BFFs contribute to capturing the three effects. Rows 4 to 9 in Table  2  list the implicit estimation performances of the three effects. The term implicit here denotes that models do not estimate the three effects directly; rather, the estimates are calculated using a post-hoc approach. In both modalities, the results of the statistical test showed that models achieved significantly better performances than the random baseline except in these situations: PCC and CCC for perceiver effects by all models using audio; PCC for target effects by the model (F alone).\n\nRegarding the perceiver effect, experimental results indicated that adding the perceiver's BFFs improves the performances of perceiver effects. The overall performance of models using facial expression was better than that of the other models. Specifically, model (F + BFF[P+T]) achieved the best performances across all modalities (PCC: .36, CCC: 0.28). Models (A + BFF[P+T]) and (A + BFF[P]) also accomplished the best PCC and CCC within the audio, respectively. Although BFF[T] improved performances in both modalities, the performance gain by BFF[P] was substantially better than that by BFF  [T] .\n\nRegarding the target effect, the results indicated that adding the target's BFFs improves the performances of target effects. The overall performance of the audio model was better than that of the other models. The model (A+BFF[P+T]) achieved the highest PCC across modalities (.23), and Model (F+BFF[P+T]) achieved the highest CCC across modalities (.16). In audio, BFF[T] improved performances, although BFF[P] did not. In facial expressions, both BFF[P] and BFF[T] yielded performance gains. However, the performance gain by BFF[T] (PCC: +.16, CCC: +.12) was substantially better than that by BFF[P] (PCC: +.06, CCC: +.07).\n\nRegarding the relationship effect, the results indicated that adding BFFs improves the performances of relationship effects. The overall performance of models using facial expressions was higher than that of the other models. Model (A+BFF[P]) achieved the highest performances across all modalities (PCC: .45, CCC: .40). BFF[P+T] yielded the best performances within audio; however, it did not yield the best performance within facial expression.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "We first discuss whether Big Five features (BFFs) can improve the estimation performance of rapport in interactions between friends (RO-1). We then examine the role of BFFs in performance improvement (RO-2). Next, we investigate which Big Five dimensions (BFDs) are effective in rapport estimation. Finally, we touch on the limitations of our study and the versatility of our proposed analytical approach.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Big Five Features Improve Rapport Estimation Performance Between Friends (Ro-1).",
      "text": "The experimental results showed that facial expressions contain rich information for rapport estimation. This finding is in line with a previous study  [24]  revealing that facial expression features yielded better estimation performances of rapport than other nonverbal features (e.g., audio features). The results also highlighted the importance of the BFFs for rapport estimation not only in initial interactions  [3, 24]  but also in interactions between friends. Specifically, combining the perceiver's and the target's BFFs yields the best performance. This result makes sense because rapport is built among two participants, and their Big Five personality traits influence rapport building  [4] . The implication of these findings is that collecting the target's Big Five personality traits is useful for estimation models with facial expression to substantially improve their performance. However, practitioners do not always have access to it in practical situations. Therefore, further studies is required to determined actual personality trait can be substituted with personality trait estimated by estimation models (e.g.,  [22] ) for the performance improvement.\n\n7.2 BFFs promote models to capture three effects (RO-2).\n\nWe verify three hypotheses: 1) the perceiver's BFFs lead models to capture perceiver effects, 2) the target's BFFs lead models to capture target effects, and 3) combinations of the two lead models to capture relationship effects. First, the experimental results showed that BFF[P] lead models to capture perceiver effects, which substantiates our first hypothesis. The performance gain by BFF[P] was better than that by BFF  [T] . The results can be explained by evidence that the perceiver's Big Five personality traits are more strongly associated with the perceiver's tendency to rate other people  [4, 7] . Furthermore, adding BFF[T] also yielded performance gains. Although it is not clear how the target's BFFs contributed to the performance gain, one possibility is that the perceiver's nonverbal behavior toward a conversation partner with specific Big Five personality traits contains rich information about perceiver effects. The performance gain by adding BFFs[P+T] (e.g., PCC: .16, CCC: .10 in facial expression) is greater than that of models using BFFs[P+T] alone. Thus, nonverbal features and BFFs may have a synergistic effect for capturing perceiver effects.\n\nSecond, we confirmed that BFF[T] leads models to capture target effects, which supports our second hypothesis. The performance gain by BFF[T] was better than that by BFF[P]. This result is in line with findings showing that the target's Big Five more strongly influences the target's tendency to be rated by other people  [4, 7] . Furthermore, BFF[P] also yielded better performances in facial expression. Although we cannot say for certain how the perceiver's BFFs contributed to the performance gain, one possible explanation is that the facial expression by the perceiver with specific Big Five personality traits may be an indicator of target effects.\n\nFinally, BFF[P+T] leads models to capture relationship effects in audio, which partially confirms our third hypothesis. In audio, BFF[P+T] led to the best PCC and CCC. However, in facial expression, BFF[P+T] did not yielded the best performances. The result imply that the perceiver's audio features and both BFFs are complementary relationship for capture the unique rapport of dyads.\n\nNext, we focus on relationship between modality and the estimation performances of three effects. On the whole, facial expression lead models better estimation performances of the perceiver and the relationship effects than audio; in contrast, audio bring models better estimation performances of the target effects. One possible explanation for the results is that specific facial expression (e.g., smile) is associated with the tendency to feel rapport for other people and unique rapport. On the other hand, specific prosody may be useful cues of the tendency to be felt rapport by other people.\n\nThe implication of these findings is that BFFs yield better performances of rapport because the perceiver's and the target's BFFs help models to capture the corresponding effects, respectively. Furthermore, the findings clarify which types of BFF are effective for obtaining the desired auxiliary information for rapport. If practitioners need to detect people who are exceptionally misanthropic (i.e., low perceiver effect) or who are undesirable (i.e., low target effect)  [15] , the utilization of the perceiver's BFFs or the target's BFFs, respectively, is a suitable choice.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis Of Effective Big Five Dimensions",
      "text": "Fig.  5  shows the relationship between the PCC and BFDs. This analysis focuses only on facial expression features due to their high estimation performances. This analysis also focuses on PCC, as PCC and CCC have a similar ordinal relationship according to the type of features. We investigate the effectiveness of the perceiver's and the target's BFDs for improving the PCC of rapport scores and the corresponding effects. The red dotted line represents the PCC of models with all BFDs. The error bar denotes the standard deviations between 30 random seed values.\n\nIn (a), we examine the effectiveness of the perceiver's BFDs for enhancing the PCC. The perceiver's agreeableness and neuroticism improved the PCC of rapport scores; the effective perceiver's BFDs for improving perceiver effects were extraversion, agreeableness, and neuroticism. Agreeableness and neuroticism corresponded to BFDs that highly correlated with perceiver effects (see Table  1 ).\n\nIn (b), we explore the effectiveness of the target's BFDs for enhancing the PCC. The target's extraversion, agreeableness, and neuroticism improved the PCC of rapport scores. Furthermore, the effective target's BFDs for estimating target effects were extraversion, agreeableness, and conscientiousness. Extraversion and conscientiousness corresponded to BFDs that highly correlated with target effects (see Table  1 ). However, the target's openness yielded performance loss despite its high correlations with target effects.\n\nThe BFDs which strongly effects to interpersonal perception are extraversion and agreeableness  [4, 11] . The results revealed that these BFDs are also important cues for implicitly effect estimation. Furthermore, specific BFDs did not reach the estimation performances with all BFDs (red dotted line) on perceiver and target effects. These findings indicate that implicit capturing of perceiver and target effects is based not on specific BFDs but rather on multiple BFDs.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations",
      "text": "As with most studies related to interpersonal perception, our findings have limitations in their generality. Specifically, our study imposed limitations on the generality of the findings with respect to the participants' restricted range of age and cultural backgrounds. In addition, we relied on limited conversation settings, such as online conversations. Therefore, further research is needed to determine whether our findings can be generalized to various participants and conversation settings. Despite these limitations, our study represents an important first step to understanding the role of personality traits in interpersonal perception.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Versatility Of Our Analytical Approach",
      "text": "Beyond the limit of rapport and BFFs, our analysis approach based on the social relations model (SRM) is helpful in analyzing the extent to which model improvements and data extensions contribute to capturing different aspects of interpersonal perception. Our approach can be applied to interpersonal perception other than rapport (e.g., romantic desire) and features other than BFFs (e.g., attachment style). In summary, our approach is an effective tool for understanding personality-aware estimation models of interpersonal perception and obtaining insight to improve them.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We (1) investigated whether Big Five features (BFFs) can improve the estimation performance of rapport in interactions between friends and (2) examined the role of BFFs in improving the estimation performance of rapport. Our experimental results showed that BFFs substantially improve the estimation performance of rapport in interactions between friends. Furthermore, we demonstrated that BFFs yield better estimation performances of rapport, as the perceiver's and target's BFFs lead models to accurately capture the corresponding effects. Our study is the first step toward understanding why personality-aware estimation models achieve high estimation performances of interpersonal perception.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of rapport estimation.",
      "page": 2
    },
    {
      "caption": "Figure 1: ). Furthermore, we investigate how to use",
      "page": 2
    },
    {
      "caption": "Figure 4: ). According to SRM [16, 17], the rapport score",
      "page": 3
    },
    {
      "caption": "Figure 2: Box plots of rapport scores and the relationship",
      "page": 3
    },
    {
      "caption": "Figure 3: Relationship effects according to combinations of",
      "page": 4
    },
    {
      "caption": "Figure 3: shows box plots of the rapport score, the",
      "page": 4
    },
    {
      "caption": "Figure 3: to visualize",
      "page": 4
    },
    {
      "caption": "Figure 4: Our Proposed Analytical approach based on SRM",
      "page": 5
    },
    {
      "caption": "Figure 5: Relationship between PCC and Big Five dimensions (BFDs). In (a), we investigate the effectiveness of the perceiverâ€™s",
      "page": 8
    },
    {
      "caption": "Figure 5: shows the relationship between the PCC and BFDs. This",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A\n+BFF [P]\n+BFF [T]\n+BFF [P+T]": "F\n+BFF [P]\n+BFF [T]\n+BFF [P+T]",
          ".05 Â±.04\n.03 Â±.03\nÂ±.06\n.10âˆ—\n.04 Â±.04\nÂ±.02\n.07 Â±.04\n.05âˆ—\nÂ±.04\nÂ±.03\n.12âˆ—\n.06âˆ—": ".31 Â±.05\n.18 Â±.03\n.32 Â±.05\n.18 Â±.03\nÂ±.04\nÂ±.02\n.35âˆ—\n.20âˆ—\nÂ±.04\nÂ±.03\n.38âˆ—\n.21âˆ—",
          "â€“.29 Â±.07\nâ€“.26 Â±.05\nÂ±.10\nÂ±.08\nâˆ’.06âˆ—\nâˆ’.04âˆ—\nâ€“.28 Â±.09\nâ€“.25 Â±.06\nÂ±.07\nÂ±.06\nâˆ’.02âˆ—\nâˆ’.07âˆ—": ".20 Â±.12\n.18 Â±.10\nÂ±.11\nÂ±.08\n.30âˆ—\n.25âˆ—\nÂ±.10\nÂ±.08\n.27âˆ—\n.22âˆ—\nÂ±.09\nÂ±.07\n.36âˆ—\n.28âˆ—",
          ".17 Â±.09\n.09 Â±.08\n.12 Â±.08\n.04 Â±.08\nÂ±.08\n.12âˆ—\n.21 Â±.11\nÂ±.08\n.23âˆ—\n.11 Â±.07": "-.01 Â±.07\n.02 Â±.05\nÂ±.08\nÂ±.05\n.05âˆ—\n.09âˆ—\nÂ±.10\nÂ±.07\n.15âˆ—\n.14âˆ—\nÂ±.08\nÂ±.06\n.17âˆ—\n.16âˆ—",
          ".19 Â±.08\n.17 Â±.07\nÂ±.07\nÂ±.07\n.25âˆ—\n.24âˆ—\nÂ±.05\n.23 Â±.06\n.21âˆ—\nÂ±.08\nÂ±.07\n.29âˆ—\n.27âˆ—": ".41 Â±.05\n.38 Â±.05\nÂ±.07\n.45âˆ—\n.40 Â±.07\n.41 Â±.06\n.38 Â±.06\n.43 Â±.06\n.39 Â±.06"
        },
        {
          "A\n+BFF [P]\n+BFF [T]\n+BFF [P+T]": "A+F\n+BFF [P]\n+BFF [T]\n+BFF [P+T]",
          ".05 Â±.04\n.03 Â±.03\nÂ±.06\n.10âˆ—\n.04 Â±.04\nÂ±.02\n.07 Â±.04\n.05âˆ—\nÂ±.04\nÂ±.03\n.12âˆ—\n.06âˆ—": ".24 Â±.05\n.12 Â±.03\n.21 Â±.05\n.10 Â±.03\nÂ±.03\n.26âˆ—\n.13 Â±.02\n.25 Â±.06\n.13 Â±.04",
          "â€“.29 Â±.07\nâ€“.26 Â±.05\nÂ±.10\nÂ±.08\nâˆ’.06âˆ—\nâˆ’.04âˆ—\nâ€“.28 Â±.09\nâ€“.25 Â±.06\nÂ±.07\nÂ±.06\nâˆ’.02âˆ—\nâˆ’.07âˆ—": ".06 Â±.11\n.04 Â±.10\nÂ±.12\nÂ±.09\n.11âˆ—\n.09âˆ—\n.07 Â±.07\n.05 Â±.06\nÂ±.10\nÂ±.09\n.15âˆ—\n.10âˆ—",
          ".17 Â±.09\n.09 Â±.08\n.12 Â±.08\n.04 Â±.08\nÂ±.08\n.12âˆ—\n.21 Â±.11\nÂ±.08\n.23âˆ—\n.11 Â±.07": ".02 Â±.11\n.06 Â±.08\n.02 Â±.09\n.08 Â±.07\nÂ±.10\nÂ±.06\n.18âˆ—\n.15âˆ—\nÂ±.13\nÂ±.09\n.14âˆ—\n.13âˆ—",
          ".19 Â±.08\n.17 Â±.07\nÂ±.07\nÂ±.07\n.25âˆ—\n.24âˆ—\nÂ±.05\n.23 Â±.06\n.21âˆ—\nÂ±.08\nÂ±.07\n.29âˆ—\n.27âˆ—": ".40 Â±.06\n.36 Â±.06\n.36 Â±.07\n.32 Â±.05\n.34 Â±.07\n.31 Â±.06\n.37 Â±.07\n.32 Â±.06"
        },
        {
          "A\n+BFF [P]\n+BFF [T]\n+BFF [P+T]": "BFF [P]\nBFF [T]\nBFF [P+T]",
          ".05 Â±.04\n.03 Â±.03\nÂ±.06\n.10âˆ—\n.04 Â±.04\nÂ±.02\n.07 Â±.04\n.05âˆ—\nÂ±.04\nÂ±.03\n.12âˆ—\n.06âˆ—": ".01 Â±.09\n.00 Â±.04\n.06 Â±.04\n.03 Â±.02\n.09 Â±.06\n.04 Â±.04",
          "â€“.29 Â±.07\nâ€“.26 Â±.05\nÂ±.10\nÂ±.08\nâˆ’.06âˆ—\nâˆ’.04âˆ—\nâ€“.28 Â±.09\nâ€“.25 Â±.06\nÂ±.07\nÂ±.06\nâˆ’.02âˆ—\nâˆ’.07âˆ—": ".07 Â±.11\n.02 Â±.09\nâ€”\nâ€”\n.12 Â±.08\n.06 Â±.06",
          ".17 Â±.09\n.09 Â±.08\n.12 Â±.08\n.04 Â±.08\nÂ±.08\n.12âˆ—\n.21 Â±.11\nÂ±.08\n.23âˆ—\n.11 Â±.07": "â€”\nâ€”\n.17 Â±.07\n.12 Â±.06\n.17 Â±.11\n.10 Â±.09",
          ".19 Â±.08\n.17 Â±.07\nÂ±.07\nÂ±.07\n.25âˆ—\n.24âˆ—\nÂ±.05\n.23 Â±.06\n.21âˆ—\nÂ±.08\nÂ±.07\n.29âˆ—\n.27âˆ—": "â€”\nâ€”\nâ€”\nâ€”\n.14 Â±.16\n.10 Â±.08"
        },
        {
          "A\n+BFF [P]\n+BFF [T]\n+BFF [P+T]": "Random",
          ".05 Â±.04\n.03 Â±.03\nÂ±.06\n.10âˆ—\n.04 Â±.04\nÂ±.02\n.07 Â±.04\n.05âˆ—\nÂ±.04\nÂ±.03\n.12âˆ—\n.06âˆ—": ".00 Â±.12\n-.00 Â±.07",
          "â€“.29 Â±.07\nâ€“.26 Â±.05\nÂ±.10\nÂ±.08\nâˆ’.06âˆ—\nâˆ’.04âˆ—\nâ€“.28 Â±.09\nâ€“.25 Â±.06\nÂ±.07\nÂ±.06\nâˆ’.02âˆ—\nâˆ’.07âˆ—": "-.02 Â±.19\n-.02 Â±.16",
          ".17 Â±.09\n.09 Â±.08\n.12 Â±.08\n.04 Â±.08\nÂ±.08\n.12âˆ—\n.21 Â±.11\nÂ±.08\n.23âˆ—\n.11 Â±.07": "-.03 Â±.18\n-.01 Â±.15",
          ".19 Â±.08\n.17 Â±.07\nÂ±.07\nÂ±.07\n.25âˆ—\n.24âˆ—\nÂ±.05\n.23 Â±.06\n.21âˆ—\nÂ±.08\nÂ±.07\n.29âˆ—\n.27âˆ—": ".03 Â±.14\n.01 Â±.09"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "OpenFace 2.0: Facial Behavior Analysis Toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "2",
      "title": "Dyad rapport and the accuracy of its judgment across situations: A lens model analysis",
      "authors": [
        "J Frank",
        "John Bernieri",
        "Janet Gillis",
        "Jon Davis",
        "Grahe"
      ],
      "year": "1996",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "3",
      "title": "Rapport with Virtual Agents: What Do Human Social Cues and Personality Explain?",
      "authors": [
        "Aleksandra Cerekovic",
        "Oya Aran",
        "Daniel Gatica-Perez"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Big Five predictors of behavior and perceptions in initial dyadic interactions: personality similarity helps extraverts and introverts, but hurts \"disagreeables",
      "authors": [
        "Ronen Cuperman",
        "William Ickes"
      ],
      "year": "2009",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "5",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "BjÃ¶rn Schuller",
        "Johan Sundberg",
        "Elisabeth AndrÃ©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Khiet P Shrikanth S Narayanan",
        "Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "openSMILE -The Munich Versatile and Fast Open-Source Audio Feature Extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 9th ACM International Conference on Multimedia, MM 2010. unknown"
    },
    {
      "citation_id": "7",
      "title": "Behavioral manifestations of personality: an ecological approach to judgmental accuracy",
      "authors": [
        "D C Funder",
        "C D Sneed"
      ],
      "year": "1993",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "8",
      "title": "The Importance of Nonverbal Cues in Judging Rapport",
      "authors": [
        "Jon Grahe",
        "Frank Bernieri"
      ],
      "year": "1999",
      "venue": "J. Nonverbal Behav"
    },
    {
      "citation_id": "9",
      "title": "Predicting Levels of Rapport in Dyadic Interactions through Automatic Detection of Posture and Posture Congruence",
      "authors": [
        "Juan Lorenzo Hagad",
        "Roberto Legaspi",
        "Masayuki Numao",
        "Merlin Suarez"
      ],
      "year": "2011",
      "venue": "2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing"
    },
    {
      "citation_id": "10",
      "title": "Rapport expressed through nonverbal behavior",
      "authors": [
        "Thomas Jinni A Harrigan",
        "Robert Oxman",
        "Rosenthal"
      ],
      "year": "1985",
      "venue": "J. Nonverbal Behav"
    },
    {
      "citation_id": "11",
      "title": "On friendship development and the Big Five personality traits",
      "authors": [
        "Kelci Harris",
        "Simine Vazire"
      ],
      "year": "2016",
      "venue": "Soc. Personal. Psychol. Compass"
    },
    {
      "citation_id": "12",
      "title": "Rapport Prediction Using Pairwise Learning in Dyadic Conversations Among Strangers and Among Friends",
      "authors": [
        "Takato Hayashi",
        "Ryusei Kimura",
        "Ryo Ishii",
        "Fumio Nihei",
        "Atsushi Fukayama",
        "Shogo Okada"
      ],
      "year": "2024",
      "venue": "Social Computing and Social Media"
    },
    {
      "citation_id": "13",
      "title": "A Ranking Model for Evaluation of Conversation Partners Based on Rapport Levels",
      "authors": [
        "Takato Hayashi",
        "Candy Olivia Mawalim",
        "Ryo Ishii",
        "Akira Morikawa",
        "Atsushi Fukayama",
        "Takao Nakamura",
        "Shogo Okada"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Virtual Rapport 2.0. In Intelligent Virtual Agents",
      "authors": [
        "Lixing Huang",
        "Louis-Philippe Morency",
        "Jonathan Gratch"
      ],
      "year": "2011",
      "venue": "Virtual Rapport 2.0. In Intelligent Virtual Agents"
    },
    {
      "citation_id": "15",
      "title": "Is Romantic Desire Predictable?",
      "authors": [
        "Samantha Joel",
        "Paul Eastwick",
        "Eli Finkel"
      ],
      "year": "2017",
      "venue": "Machine Learning Applied to Initial Romantic Attraction. Psychol. Sci"
    },
    {
      "citation_id": "16",
      "title": "Interpersonal Perception: The Foundation of Social Relationships",
      "authors": [
        "A David",
        "Kenny"
      ],
      "year": "2019",
      "venue": "Interpersonal Perception: The Foundation of Social Relationships"
    },
    {
      "citation_id": "17",
      "title": "The Social Relations Model",
      "authors": [
        "A David",
        "Lawrence Kenny",
        "Voie"
      ],
      "year": "1984",
      "venue": "Advances in Experimental Social Psychology"
    },
    {
      "citation_id": "18",
      "title": "Expressivity halo effect in the conversation about emotional episodes",
      "authors": [
        "Masanori Kimura",
        "Masao Yogo",
        "Ikuo Daibo"
      ],
      "year": "2005",
      "venue": "THE JAPANESE JOURNAL OF RESEARCH ON EMOTIONS"
    },
    {
      "citation_id": "19",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I-Kuei Lawrence",
        "Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "20",
      "title": "Using Temporal Association Rule Mining to Predict Dyadic Rapport in Peer Tutoring",
      "authors": [
        "Michael Madaio",
        "Rae Lasko",
        "Amy Ogan",
        "Justine Cassell"
      ],
      "year": "2017",
      "venue": "International Conference of Educational Data Mining"
    },
    {
      "citation_id": "21",
      "title": "Don't Classify Ratings of Affect; Rank Them",
      "authors": [
        "P HÃ©ctor",
        "Georgios MartÃ­nez",
        "John Yannakakis",
        "Hallam"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2352268"
    },
    {
      "citation_id": "22",
      "title": "Personality trait estimation in group discussions using multimodal analysis and speaker embedding",
      "authors": [
        "Candy Olivia Mawalim",
        "Shogo Okada",
        "Yukiko Nakano",
        "Masashi Unoki"
      ],
      "year": "2023",
      "venue": "J. Multimodal User Interfaces"
    },
    {
      "citation_id": "23",
      "title": "The rhythm of rapport: Interpersonal synchrony and social perception",
      "authors": [
        "K Lynden",
        "Louise Miles",
        "C Nind",
        "Macrae"
      ],
      "year": "2009",
      "venue": "Journal of Experimental Social Psychology",
      "doi": "10.1016/j.jesp.2009.02.002"
    },
    {
      "citation_id": "24",
      "title": "Detecting Low Rapport During Natural Interactions in Small Groups from Non-Verbal Behaviour",
      "authors": [
        "Philipp MÃ¼ller",
        "Michael Huang",
        "Andreas Bulling"
      ],
      "year": "2018",
      "venue": "Proceedings of the 23rd International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "25",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Coupled Systems for Modeling Rapport Between Interlocutors",
      "authors": [
        "Srijan Sharma",
        "Girish Kantha",
        "Fei Gangadhara",
        "Anne Xu",
        "Mark Solbu Slowe",
        "Ifeoma Frank",
        "Nwogu"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "27",
      "title": "We Click, We Align, We Learn: Impact of Influence and Convergence Processes on Student Learning and Rapport Building",
      "authors": [
        "Tanmay Sinha",
        "Justine Cassell"
      ],
      "year": "2015",
      "venue": "Proceedings of the 1st Workshop on Modeling INTERPERsonal SynchrONy And infLuence"
    },
    {
      "citation_id": "28",
      "title": "The Nature of Rapport and Its Nonverbal Correlates",
      "authors": [
        "Linda Tickle-Degnen",
        "Robert Rosenthal"
      ],
      "year": "1990",
      "venue": "Psychol. Inq"
    },
    {
      "citation_id": "29",
      "title": "Construction of the Big Five Scales of personality trait terms and concurrent validity with NPI",
      "authors": [
        "Sayuri Wada"
      ],
      "year": "1996",
      "venue": "Shinrigaku Kenkyu"
    },
    {
      "citation_id": "30",
      "title": "Rapport and facial expression",
      "authors": [
        "Ning Wang",
        "Jonathan Gratch"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "31",
      "title": "Perceiver effects as projective tests: what your perceptions of others say about you",
      "authors": [
        "Dustin Wood",
        "Peter Harms",
        "Simine Vazire"
      ],
      "year": "2010",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "32",
      "title": "Socially-aware virtual agents: Automatically assessing dyadic rapport from temporal patterns of behavior",
      "authors": [
        "Ran Zhao",
        "Tanmay Sinha",
        "Alan Black",
        "Justine Cassell"
      ],
      "year": "2016",
      "venue": "Intelligent Virtual Agents"
    }
  ]
}