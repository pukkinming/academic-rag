{
  "paper_id": "2509.15774v1",
  "title": "Affective Air Quality Dataset: Personal Chemical Emissions From Emotional Videos",
  "published": "2025-09-19T09:01:54Z",
  "authors": [
    "Jas Brooks",
    "Javier Hernandez",
    "Mary Czerwinski",
    "Judith Amores"
  ],
  "keywords": [
    "Dataset",
    "affect",
    "gas sensors",
    "emotional chemosignals",
    "air quality",
    "electronic nose",
    "breath analysis",
    "body emissions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Inspired by the role of chemosignals in conveying emotional states, this paper introduces the Affective Air Quality (AAQ) dataset, a novel dataset collected to explore the potential of volatile odor compound and gas sensor data for non-contact emotion detection. This dataset bridges the gap between the realms of breath & body odor emission (personal chemical emissions) analysis and established practices in affective computing. Comprising 4-channel gas sensor data from 23 participants at two distances from the body (wearable and desktop), alongside emotional ratings elicited by targeted movie clips, the dataset encapsulates initial groundwork to analyze the correlation between personal chemical emissions and varied emotional responses. The AAQ dataset also provides insights drawn from exit interviews, thereby painting a holistic picture of perceptions regarding air quality monitoring and its implications for privacy. By offering this dataset alongside preliminary attempts at emotion recognition models based on it to the broader research community, we seek to advance the development of odor-based affect recognition models that prioritize user privacy and comfort.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recent studies have illuminated the critical role of chemosignals in conveying emotional states through alterations in bodily emissions  [1] -  [4] . This phenomenon underpins the potential of using gas sensing technologies to detect emotional states as a potential new modality for existing affective computing systems. Understanding and accurately detecting human emotions presents a challenge compounded by privacy concerns with audiovisual methods  [5]  and user discomfort with contact-based sensors. Motivated to addressing these issues, this paper introduces the Affective Air Quality (AAQ) Dataset, which endeavors to explore the potential of gas sensors as a novel, non-contact modality for emotion detection in a personal manner. By analyzing changes in breath and body emission composition, the AAQ Dataset seeks to offer an unobtrusive alternative to traditional affective computing approaches, mitigating privacy and comfort concerns associated with methods that rely on audio-visual input or direct physical contact. We describe the first-of-its-kind, open-access dataset that captures real-time volatile odor compound (VOC) and gas sensor data (personal chemical emissions) while participants experience different emotional responses elicited by movie clips in a controlled setting. Sensor data are obtained by two custom-made devices (a wearable and a standalone), that use off-the-shelf metal oxide gas sensors. This initiative not only fills a gap in existing research, which has largely relied on labgrade equipment for chemical analysis, but also democratizes access to data for building gas/odor-based emotion recognition models applicable in everyday settings.\n\nOur paper offers the following contributions:  (1)  The AAQ dataset: the first open-access collection of personal chemical emission data from gas sensors (wearable and desktop), capturing participant responses to 12 videos across four emotions (Amusement, Relaxation, Sadness, Disgust) during a ∼47minute controlled study. (2) A simple, off-the-shelf design for a wearable gas sensing headset to roughly detect -in real-time -a variety of gases, besides carbon monoxide (CO), nitrogen dioxide (NO 2 ), ethyl alcohol (C 2 H 5 CH), Volatile Organic Compounds (VOC), etc. In addition, we provide qualitative results that point to the design's usability and robustness for sensing. (3) A preliminary analysis of automatic affect detection based on the data, which did not demonstrate full feasibility but held promising results pointing to a need for further investigation. (4) An initial investigation of participant's perceptions on air quality monitoring, potential risks involving privacy and security, as well as ethical considerations when designing AAQ systems. More specifically, we also provide and analyze 3 hours and 50 minutes of transcribed exit interviews on these topics.\n\nThrough a review of related work, design of a simple gas sensing wearable, detailed dataset construction, and preliminary analysis of findings, this paper lays the groundwork for further investigation into using gas sensors as a potential privacy-preserving, comfortable, and effective tool for emotion detection. By bridging the gap between breath & body emission (emotional chemosignal) analysis and affective computing, the AAQ dataset aims to pave the way for advancements in alternative, unobtrusive emotion recognition technologies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "To contextualize the contribution of this work, we provide an integrated overview of human emotional chemosignals, gas sensing technologies, and existing gas and air quality datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotional Chemosignaling In Humans",
      "text": "Chemical signals, or chemosignals, significantly influence behavioral responses within species. The communicative function of chemosignals in humans, historically a topic of debate  [6] ,  [7] , has gained clarity over recent decades. Emerging research underscores chemosignals' critical role in intra-and interspecific interactions  [4] , particularly emphasizing the conveyance of emotional states through bodily emissions  [8] . Central to this discourse is the notion that human emotions materialize physically, notably altering body odor and breath composition. Far from mere physiological byproducts, these changes act as nuanced communicative cues, shaping social perceptions and interactions. Intriguingly, common social behaviors, such as handshaking, may facilitate the unconscious sampling of another's chemosignals, evidenced by increased hand sniffing post-handshake  [9] .\n\nHistorically, research into emotional chemosignaling has predominantly explored the transmission of \"negative emotions,\" such as fear, stress, or anxiety  [2] . For instance, fear-induced sweat can bias others toward perceiving fear in ambiguous facial expressions  [10] , a phenomenon observed regardless of the fear-associated chemosignal concentration and not observed with sweat induced by physical exercise  [11] . Similarly, male recipients exposed to women's tears reported diminished sexual arousal, an effect absent when exposed to odorless, negative-emotion tears  [12] .\n\nConversely, further research has recorded analogous outcomes with \"positive emotions,\" such as happiness or increased sexual arousal  [3] ,  [13] -  [15] , indicating a broad spectrum of emotions communicable via human chemosignals.\n\nWhile the primary focus has been on body odors (i.e., sweat), breath composition may also contain emotional chemosignals, as composition changes after stressful or relaxing stimuli  [1]  and even repeatably throughout a film when analyzing an audience's aggregated breath composition  [16] .\n\nGiven the complex chemical composition of these emissions, behavioral scientists have called for machine learning techniques to correlate chemical structures with psychological effects  [17] . This work aims to provide the affective computing field with the data to explore the link between bodily emission and emotional state to further the investigation of data analysis and machine learning approaches for affective air quality.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Gas Sensing Technologies",
      "text": "Metal-oxide (MOX) gas sensors, or chemiresistors, are central to smell-sensing applications due to their costeffectiveness and compactness. They detect changes in electrical resistance when a metal oxide surface absorbs oxygen in the presence of gases  [18] , making them a practical choice for integrating into interactive systems. By adjusting the surface coating, thickness, or shape, MOX sensors can be fine-tuned to detect specific gases, though they generally react to a wide range of gases non-selectively.\n\nGas sensors have been employed to identify everyday activities  [19]  such as eating, restroom use, and smoking, and have monitored cooking stages and household scents, including cooking odors and candle lighting.  [20] -  [22] . Projects like Bin-ary have used gas sensors for environmental applications, such as masking organic waste malodors  [23] , and AirSense monitored indoor air quality  [24] , showcasing the versatility of gas sensing in enhancing environmental and quality of life.\n\nOne of the earliest HCI research example of gas sensing for interaction is Back to Mouth, an interactive game using breath odors to repulse monsters  [25] . Recent initiatives propose electronic noses for assessing body odor intensity  [26] and incorporate gas sensors into wearables for health, well-being, and environmental monitoring, like VOCNEA for sleep apnea detection  [27]  and AirSpec smart glasses to track the impact of air quality on comfort  [28] . Multiple projects have also integrated gas sensors to measure personal exposure to air pollution both indoor and outdoor  [29] -  [33] .\n\nWhile existing research in gas sensing within HCI has primarily concentrated on detecting odors or monitoring our environment, we sought to produce a dataset exploring the connection between a user's breath or bodily emission composition and their emotional state.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Breath And Body Odor Analysis",
      "text": "Detecting emotions via body odor or a breath's chemical composition is not new. Using mass spectrometry, researchers applied atmospheric chemistry concepts to sensitively measure the changes in bodily emanations from humans in isolation or groups  [34] . Others have explored the use of quartz crystal resonators with plasma-polymer films tuned for specific chemical compounds to differentiate between a breath from a stressed or relaxed participant with an accuracy of 70%  [35] ,  [36] . However, the relaxed condition had participants drinking tea, which may have impacted the breath composition for this condition. Still, others measured breath composition changes after long stressful or relaxing stimuli using a gas chromatographic-ion mobility spectrometer  [1] .\n\nIn contrast to the measurement of individuals, others have proposed analyzing the collective breath of crowds to assess group behavior, activity, or emotions (\"crowd breath\")  [16] , building off their prior work showing repeatable changes in measured VOC in audience breath in a cinema while watching movies  [37] ,  [38] . They used a mass spectrometer to detect and identify precise volatile compound concentrations. Most recently, a study demonstrated that a crowd's collective experience directly correlates to the biomarkers in the crowd's breath  [39] , suggesting visual and auditory experiences have predictable effects on human volatile compounds.\n\nHowever, the use of mass spectrometers for such analyses presents challenges, including their size, cost, and the need for specialized operation. For crowd breath studies, these devices require a building-wide ventilation system to collect samples, introducing a five-minute delay for volatile compounds to reach the spectrometer.\n\nUnlike prior work, we present a dataset of real-time emotional responses to video clips using an individual's breathing via cheap, off-the-shelf MOX gas sensors.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Gas Sensing Datasets",
      "text": "Research in environmental monitoring and electronic nose applications has generated datasets that, despite often utilizing similar technology-arrays of low-cost commercial metal oxide (MOX) gas sensors-differ significantly in their focus and methodology. Environmental monitoring datasets typically aim to quantify air pollutants likecarbon monoxide (CO), carbon dioxide (CO 2 ), sulfur dioxide (SO 2 ), nitrogen dioxide (NO 2 ), and ozone (O), focusing on air quality assessments  [40] -  [43] . Conversely, electronic nose datasets, prioritizing a broader array of MOX sensors, target creating \"digital fingerprints\" of gas mixtures for applications in food safety, health, and gas discrimination, offering insights into the sensor behavior across various stages of food spoilage, quality, or adulteration  [44] -  [48] , as well as identifying chemical compounds and studying sensor drift  [49] ,  [50] . A unique dataset also explores electronic nose responses to breath samples from healthy individuals and those with Chronic Obstructive Pulmonary Disease (COPD)  [51] . However, there appears to be no publicly available dataset yet that captures gas sensor data correlated with diverse emotional experiences. However, as far as we have found, there is currently no publicly accessible dataset collecting gas sensor data during emotional experiences.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. The Affective Air Quality Dataset",
      "text": "The Affective Air Quality (AAQ) Dataset consists of emotional ratings and 18 hours & 11 minutes of gas data collected from 23 participants while watching 12 movie clips that targeted five emotional responses (amusement, disgust, relaxation, sadness, and neutral), with a total of 3 clips per valance-arousal quadrant. The data was collected using a wearable device we developed using an off-the-shelf air quality monitor with four gas sensors (described in Section III-B1). We also developed an annotation platform to present the movie clips in a randomized order, which allowed the participant to rate their emotional response after each clip using 9-item Likert scales for valence (from very unpleasant to very pleasant), arousal (low to high energy), and familiarity with the content. Section III-C summarizes the stimuli to elicit targeted emotional responses. In addition to the gas sensor data and emotional ratings, we provide transcripts from all exit interviews (≈4h of data), as well as (  1 ) the timing and nature of their last consumed beverage and meal, (2) sleep quality via the Richards-Campbell Sleep Questionnaire, (3) personality traits through the short Big Five personality test, (4) current and general anxiety levels measured by the short State-Trait Anxiety Inventory (STAI), (  5 ) mood states assessed by the short form of the International Positive and Negative Affect Schedule (I-PANAS-SF) and (  6 ) emotional regulation skills via the ERQ Questionnaire.\n\nDetailed information about participant's demographics, experimental setup, and data collection can be found in the following sections and appendix. All data collected has been made anonymous to prevent potential privacy breaches. The study was approved by the local Institutional Review Board and conducted in a controlled office room from August 15, 2023, to September 12, 2023.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Participants And Data Exclusion",
      "text": "We recruited 34 healthy participants aged 18-64 through email and advertising, but due to recording software issues, data from 11 sessions were lost, though their exit interviews were preserved. This left us with data from 22 complete sessions and 1 partial session due to the sensing apparatus accidentally getting disconnected. Participant ages were: 18-24 (n = 7), 25-34 (n = 14), 45-54 (n = 1), and 55-64 (n = 1), with 16 identifying as women, 6 as men, and 1 as non-binary/gender diverse.\n\nThe exclusion criteria included a known history of smoking or lung and respiratory problems. Participants were required to have at least least 5 hours of sleep prior, no alcohol for 24 hours, no scented products, and no eating or drinking 2 hours before the study. The study was conducted in a controlled office room, and all participants provided written informed consent and were compensated with a 70 USD gift card.\n\nAt the start of the study, participants completed an intake form and questionnaires (RCSQ, BFI, STAI, I-PANAS-SF) to document recent food or drink intake that could impact their bodily emissions and establish an emotional baseline.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experimental Procedure",
      "text": "To ensure the integrity of our data, we scheduled adequate intervals between participants. This strategy allowed the building's ventilation system sufficient time to return the gas sensor readings to a stable baseline, ensuring no residual influences from prior participants lingered within the space. The study setup was initiated 30 minutes before each session to allow the air quality monitors to stabilize and establish a baseline reading of the room air composition. Upon arrival and after providing written consent, participants completed an intake form and were then led to wear the air quality headset (as shown in Figure  2 ).\n\nWe summarize the procedure in Figure  1 . Participants were familiarized with the interface through a neutral video, during which they could adjust audio levels to their comfort. This introduction also covered the concepts of valence, arousal, and familiarity to prepare them for the self-assessments. Participants then watched 12 emotion-eliciting video clips (in a fully randomized order to mitigate potential ordering effects) and rated their valence, arousal, and familiarity after each. Neutral washout clips were randomly selected from two options to maintain participant engagement. An additional (and final) relaxing video aimed to help mitigate any residual negative emotions induced by previous stimuli, ensuring participants ended the experiment in a positive or neutral emotional state. Before this final clip, half of the participants received a mintflavored breath strip in order to potentially examine if breath alteration could negatively impact emotion recognition using these sensors. (This portion was inspired by the peppermint benchmarking protocol for GC-MS-based breath analysis  [52] ,  [53] .) After this portion, the researcher engaged the participant   4 ) completing another self-assessment. In addition, at the end of certain emotional clips and first neutral video, participants were guided to hold their breath and exhale for three seconds. Before the final clip, half of the participants took a Listerine Cool Mint breath strip and let it dissolve on their tongue. Then, all participants watched a relaxing clip followed by a final self-assessment. in a semi-structured interview. The interview focused on feedback on the equipment, air quality monitoring experiences, and privacy concerns related to air quality data collection.\n\nThe total study duration was about 1 hour and 15 minutes. 1) Hardware: Chemical signals were captured using the Grove Multichannel Gas Sensor (v2), which includes four MOX gas sensors each optimized for different gases but generally non-selective towards oxidizing gases. These sensors are specifically responsive to nitrogen dioxide (Winsen GM-102B), ethyl alcohol (Winsen GM-302B), volatile organic compounds (Winsen GM-502B), and carbon monoxide (Winsen GM-702B), yet can detect a similar range of gases. With ethanol, 2-propranol and 1-propanol identified as stresssensitive compounds  [1] , MOX sensors hold potential for stress detection. Fig.  2 . Prototype wearable gas sensor system using headphones, with sensors mounted on the microphone goose-neck near the nose and mouth. The sensors are optimized for response to nitrogen dioxide (NO 2 ), ethyl alcohol (EtOH), volatile organic compounds (VOC), and carbon monoxide (CO).\n\nTo ensure thorough coverage of the testing area, we placed gas sensors in two strategic locations: directly in front of the participant's nose and mouth, and on the study desk about 0.75 m from the participant. The sensor near the nose and mouth was attached to a microphone goose-neck on standard headphones (detailed in Figure  2 ). Participants were guided on aligning the microphone for optimal capture of their breath, though most managed this without help.\n\n2) Software: We utilized Qualtrics for intake form questionnaires and employed two scripts for data collection during emotional video clips. A Python script managed the recording and timestamping from both air quality monitors at a sampling rate of 91 Hz, except for P11's session, recorded at 58 Hz due to software differences. Additionally, we developed an annotation platform with PsychoPy to oversee the experimen-tal sequence and let participants' annotate their emotional responses after each video.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Emotional Movie Clips",
      "text": "In order to elicit targeted emotional responses, we utilized short video clips sourced from databases with established reliability and validity  [54] -  [56] . Recognizing the prior literature's emphasis on negative valence emotions such as Disgust, Fear, and Sadness, we incorporated clips known to induce positive valence such as Relaxation  [57]  and Amusement as shown in Figure  3 . For Amusement we sourced high quality funny animal video clips from YouTube based on their similarity to what others have used in the past  [58] , prioritizing clips with over six million views by August 2023, and search terms including \"funny video,\" \"babies laughing,\" and \"hilarious videos.\"\n\nTo balance stimuli across the valence-arousal space, we selected enough footage in each quadrant to reach 399 seconds of stimuli across three videos. To balance the Disgust category, we appended additional surgery footage to the surgery video originally included in  [54] . Each quadrant of the valencearousal space was limited to three video clips, standardizing the number of washout neutral videos across the experiment. All selected videos met a minimum resolution criterion of 1280×720 px. Ultimately, 12 emotion-eliciting clips were chosen, with lengths ranging from 59 to 232 seconds long (M = 133 s, SD = 45.6 s). Detailed information about the video clip sources and characteristics is provided in our repository.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Data Records And Contents",
      "text": "The AAQ dataset is accessible on GitHub.  1  The PsychoPy experimental setup code is also accessible as a Supplementary Material. Due to the lack of standard preprocessing pipelines for continuous gas sensing of human breath, we include both raw data and a preliminary quantitative analysis with details on preprocessing and feature extraction steps. The dataset contents are summarized in Appendix Table  II .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Gas Sensor Analysis",
      "text": "To set a baseline for the community, we conducted a preliminary investigation into the possibility of automatic emotion detection with the AAQ dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Preprocessing And Feature Extraction",
      "text": "To prepare our gas sensor data for feature extraction, we implemented several pre-processing steps. We used a fourthorder band-pass Butterworth filter with a 30 Hz cutoff to eliminate high and low-frequency noise, followed by an Exponential Weighted Moving Average (EWMA) with α = 0.08. Finally, min-max normalization was applied to standardize the data range across channels, per video clip.\n\nGiven the novelty of the set-up, it is unclear which features would be most relevant to detecting emotion from gas sensor signals. While we do not track participants' respiration, we propose that the averaged signal across the four headset gas sensors could be used to estimate respiration cycles, inspired by VOCNEA  [27] . Alternatively, gas sensor signals during a single exhalation or full respiration cycle could also be used.\n\nAs a first step, we included the gas sensor signals from both the headset and desktop AQ monitors during emotioneliciting clips. Features extracted included statistical measures (mean, standard deviation, area under curve, skewness, kurtosis), conductance changes, relative abundance (separated by device), mean & standard deviation of each DWT coefficient, and Rényi entropy for each gas sensor. Additionally, we sensor response metrics (peak-to-peak time, trough-to-peak signal, and minimum and maximum slope) for both the unnormalized and normalized gas sensor signals, generating in total a 248 D feature vector. Inspired by  [59] , we share the Linear Discriminant Analysis (LDA) plots to present the separability of arousal, valence, and familiarity scores (see Figure  4 ).\n\nFor emotion prediction, we employed SVM (with RBF kernel), RF (100 trees), and XGBoost models as well as an LSTM network, using a 5-fold cross-validation approach. After initial testing with LDA, we opted to not conduct dimensionality reduction for the SVM, RF, and XGBOOST model. We treated the problem as a regression task, for which the models needed to predict the valence or arousal of a sequence (we trained separate models for each). Our LSTM model, designed to address the data's temporal aspect, included two 100-unit layers, processed the last 30 seconds of sensor signals, and featured Dropout layers to reduce overfitting. The network used the GlorotUniform initializer and LeakyReLU activation, concluding with a Dense layer for valence or arousal predictions. Training involved the Adam optimizer and mean squared error loss, with early stopping to curtail overfitting after 50 epochs or 10 epochs without loss improvement. For the crossvalidation, all samples were randomly divided into five folds, and in each validation, four folds are used for training and the remaining fold for testing. (Note, the data from the same participant may be divided across the training and testing sets, which is a current limitation of our preliminary analysis.)",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Results",
      "text": "For evaluating our models, we prioritized Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) as our metrics, given their direct interpretability in the context of numerical valence and arousal prediction. MAE provides a clear measure of average prediction accuracy in the same units as the target variable, making it particularly relevant for applications in affective computing where precision in emotional state prediction is paramount. The RMSE complements the MAE metric by providing a relatively high weight to large errors, making it useful to evaluate a model when large errors are particularly undesirable. Finally, we included the R 2 , which represents the proportion of the variance in the dependent variable that is predictable from the independent variable.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Interviews",
      "text": "We collected approximately 3 hours and 50 minutes of transcribed interviews across all participants 2  (including those whose quantitative sessions were lost). Using Burnard's method  [60] , we identified key topics such as the privacy and sensitivity of AAQ data, focusing on social intimacy concerns of bodily scents and the perception of air quality data as nonprivate. We also explored the connection between emotion and health information, sensitivity towards air quality data, usability feedback, strategies in monitored environments, and the use of AQ-related technologies.\n\nAll participants found the headset comfortable or unobtrusive. P15 stated, \"It's comfortable. I like it,\" while P31 found the prototype familiar and comfortable, \"it's very familiar to the headset I usually wear.\" Three participants mentioned slight discomfort from the headset pressing against their glasses, several suggested improving the prototype by using wireless headphones that cover the ear, and P27 suggested integrating a longer microphone goose-neck to better adjust the sensors' position.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Social Intimacy Of Bodily Emissions",
      "text": "In our exploration of user perceptions regarding monitoring bodily emissions and odors, a theme of intimacy emerged, intertwined with notions of social privacy and personal identity. Our findings suggest that while there is curiosity about the potential insights derived from monitoring bodily emissions, there remains a significant concern for the intimacy and social implications of such data collection.Our analysis reveals a key misunderstanding about air quality (AQ) monitoring technology capabilities. Participants often conflate the quantitative collection of data regarding bodily emissions with the qualitative recording of the actual scent, fearing that such technology could enable judgments about the odor itself (and -by extension -themselves). As P24 intimates, the worry is not just about data collection but about the potential for this data to reflect on them through a lens of social judgment. This concern highlights a critical gap in understanding; current technology does not allow for the social assessment of odors in the way participants fear (poor AQ readings do not inherently mean bad smells), yet the anxiety about being subject to such scrutiny remains palpable.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Privacy And Sensitivity Of Air Quality Data",
      "text": "Despite concerns about the social implications of monitoring bodily emissions, a majority participants (n = 16) did not perceive AQ data related to their bodies as private or sensitive. This sentiment was echoed across responses, suggesting a comfort with the collection and use of AQ data for purposes such as public health and personal wellness.\n\nMany participants said they do not consider AQ data related to their body as personally identifiable or sensitive. For instance, participants likened sharing AQ data to sharing nonsensitive, publicly observable information, such as body odor or breath quality, or to everyday experiences where personal scents are naturally detected by others in close proximity. These comparisons imply a belief that AQ data does not constitute a serious privacy concern.\n\nSeveral participants explicitly dismissed concerns over the privacy of AQ data. For example, P15 found value in accessing any internal information for health purposes (\"Not really... it's always good to have access\"), while P9 expressed no particular concern, and P16 likened it to \"just the air.\" P22 did not consider AQ data as personally identifiable or a privacy issue, assuming responsible data management.\n\nA subset of participants differentiated between types of data, deeming some as more privacy-sensitive than others. AQ data was often seen as less sensitive than personal data such as health records. This perspective was grounded in the belief that AQ, a component of the surrounding environment, inherently carries less personal identification potential.\n\nThese reflections collectively illustrate a predominant sentiment among participants that AQ data, particularly when related to bodily emissions, does not significantly intrude upon personal privacy. This consensus suggests a potential broader acceptance of the collection and use of such data.\n\nHowever, privacy concerns were conditional for several participants (n = 11), depending on the context of data use and the extent of anonymization. There was a recognition of the utility of AQ data for public health or environmental monitoring, with several participants indicating acceptance of collective data use for broader societal benefits. This acceptance was, however, contingent on assurances of anonymity and appropriate use, highlighting a nuanced understanding of privacy concerns where individual consent is balanced against the potential for communal benefit.\n\n15 participants evaluated the perceived sensitivity of not only AQ data but also emotional and health data derived from the AQ data. Through our coding process, we assigned these perceptions into four broad categories: Not Sensitive/Private (0), Slightly Sensitive/Private (1), Conditionally Sensitive/Private (2), and Sensitive/Private (3). Our analysis revealed a positive correlation between these sensitivities, indicating that concerns about privacy tend to increase together. Specifically, we found a moderate positive correlation (0.37) between AQ and Emotional sensitivity, and a stronger correlation (0.54) with Health sensitivity.\n\nAmong participants who considered AQ data not sensitive or private (n = 8), about half also viewed derived Emotion information as not sensitive. Roughly a third felt the same about Health information derived from AQ.\n\nThese results suggest a trend: individuals who already regard AQ data as sensitive and private are more inclined to view Emotional and Health data through a similar lens of sensitivity and privacy, with an especially pronounced correlation observed in the realm of Health data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Strategies For Privacy Protection",
      "text": "In discussions on privacy in AQ-monitored environments, seventeen participants devised strategies to safeguard their privacy, reflecting a range of concerns and pragmatism. Three participants considered using perfumes, breath mints, or hygiene products to mask bodily emissions, with one viewing it as introducing \"artifacts\" into the data. Four others thought about altering their breathing or using masks to affect data collection. Some even considered tampering with the equipment directly, such as covering or disabling sensors.\n\nHowever, responses to these privacy measures varied. Two participants deliberated the effort versus the privacy benefit, showing a conditional approach. Meanwhile, a few displayed indifference towards privacy in these scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Personal Use Of Aq-Related Devices",
      "text": "After consolidating responses to account for variations in terminology, we identified the following as the most frequently cited AQ-related systems used by participants: smoke detectors (30), carbon monoxide detectors  (17) , humidifiers or dehumidifiers  (11) , and air purifiers  (9) . Air purifiers and humidifiers, often situated in bedrooms, are regularly interacted with directly. In contrast, smoke alarms and carbon monoxide alarms are more passive fixtures, spanning many rooms and generally only drawing attention when triggered.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vii. Discussion",
      "text": "While our models' R 2 were largely small (except for the RF model's R 2 for valence), the MAE and RMSE's coarse accuracy suggest an interesting potential for the modality's use in affective computing applications. The models may be suitable to roughly estimate levels in valence and arousal (i.e., low, neutral, and high), but not fine-grained prediction. A future direction could involve training an LSTM network on the features we used for the RF models, but with a sliding window capturing 2-3 breathing cycles.\n\nThe Linear Discriminant Analysis (LDA) plots (Figure  4 ) suggest that there is space for further enhancing separation between emotional states with improved feature engineering. Notably, LDA showed improved separation when combining data from both headset and desktop sensors and including trough/peak features, suggesting these could be fruitful areas for further research. Ultimately, our preliminary analysis leaves room for many improvements.\n\nOur qualitative analysis revealed two interesting lines of thought. First, while participants may find bodily emissions intimate or socially concerning, most participants paradoxically did not view AQ data as sensitive. Our interviews also identified a network of AQ sensors in domestic and professional environments, potentially laying the groundwork for future affective computing research utilizing bodily emissions. Despite their lack of proximity to the body compared to devices in our dataset, their closeness during prolonged activities like sleep could allow for ongoing emission monitoring, complementing wearable technologies.\n\nHowever, this study has limitations. Our participant pool was predominantly young women, indicating a need for greater diversity. We lacked specific breath monitoring and environmental data like temperature and humidity, though we propose a method for estimating breathing patterns for future exploration. The emotional response overlap in our videos, particularly between disgust and sadness, underscores the need for clearer differentiation in stimuli. Our crossvalidation method, which might include data from the same participants in both training and testing sets, raises concerns about the models learning individual characteristics rather than generalizable features. Additionally, the timing of our exit interviews may have influenced participants' responses to privacy concerns.\n\nLastly, we introduced a relaxation video with breath mint masking but have yet to assess its impact on model performance in detecting valence or arousal. This aspect, along with the aforementioned limitations and observations, offers rich avenues for further investigation by the research community.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "In this work, we introduced the Affective Air Quality dataset to explore affect recognition through the novel lens of emotional chemosignaling using off-the-shelf air quality monitors. This dataset comprises 4-channel gas sensor data collected from two positions (head-mounted and desktop) across 23 individuals experiencing varying emotions elicited by video clips. Additionally, it includes valence, arousal, and familiarity assessments for each clip, and is complemented by exit interview transcripts that shed light on participants' views on air quality monitoring and its implications for privacy.\n\nOur work not only introduces a novel dataset but also lays the groundwork for future explorations in affect recognition utilizing this non-visual, non-contact method. The findings suggest that gas sensor data could be used to correlate changes in breath or body odor with emotional states, an area of research currently needing sophisticated and costly laboratory equipment like mass spectrometers, or methods requiring structured breathing protocols. We encourage the research community to engage with our dataset to develop affect recognition models and engineer even better features for extraction, opening new avenues for understanding and interpreting human emotions through the lens of human chemosignaling.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "Emotion recognition technologies inherently carry a significant risk of misuse, given the sensitivity of emotional information. Our protocol incorporated semi-structured exit interviews to gauge participant perceptions regarding the privacy and sensitivity of air quality data, including how it relates to the inference of emotional states or personal health information. As evidenced by our qualitative findings, participants paradoxically have concerns about their body odor or breath being monitored (from fear of social judgment about the smells' hedonic qualities), yet are generally not concerned about air quality data's sensitivity or privacy unless it specifically discloses extracting emotional or personal health information. This paradox underscores a critical ethical consideration: the need for clear communication and stringent privacy safeguards when developing and deploying technologies that intersect closely with emotion, including when using non-contact and non-visual methods like AAQ.\n\nGiven the sensitive video clips often included in emotioneliciting video studies (e.g., disgust-or anger-eliciting videos), we took much care to make the protocol non-distressing. Firstly, we ensured participants knew they could stop or exit the videos at any time, though no one opted to. Secondly, to protect participant well-being, we deliberately omitted videos that induce anger through depictions of racist or sexist violence, focusing instead on disgust-eliciting clips for lowvalence, high-arousal emotions. We balanced the emotional range by including relaxing videos for high-valence, lowarousal emotions. Finally, we presented a relaxing video at the end of the protocol to help mitigate any residual negative emotions induced by previous stimuli, ensuring participants ended the experiment in a positive or neutral emotional state. We suggest future researchers employing emotion-eliciting videos to carefully select their content, as it may profoundly distress participants, especially those with personal connections to the material presented.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Participants were",
      "page": 3
    },
    {
      "caption": "Figure 1: The core of the experiment consisted of twelve iterations, each including: (1) viewing an emotion-eliciting video clip, (2) completing a self-assessment,",
      "page": 4
    },
    {
      "caption": "Figure 2: Prototype wearable gas sensor system using headphones, with sensors",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Participants were guided on",
      "page": 4
    },
    {
      "caption": "Figure 3: For Amusement we sourced high quality funny ani-",
      "page": 4
    },
    {
      "caption": "Figure 3: Average valence and arousal (and their standard deviations) for",
      "page": 5
    },
    {
      "caption": "Figure 4: 2D Linear Discriminant Analysis (LDA) projections based on features",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Real Time Mental Stress Detection Through Breath Analysis",
      "authors": [
        "P Santos",
        "P Roth",
        "J Fernandes",
        "V Fetter",
        "V Vassilenko"
      ],
      "year": "2020",
      "venue": "Technological Innovation for Life Improvement"
    },
    {
      "citation_id": "2",
      "title": "Human Fear Chemosignaling: Evidence from a Meta-Analysis",
      "authors": [
        "J De Groot",
        "M Smeets"
      ],
      "year": "2017",
      "venue": "Chemical Senses"
    },
    {
      "citation_id": "3",
      "title": "A Sniff of Happiness",
      "authors": [
        "J De Groot",
        "M Smeets",
        "M Rowson",
        "P Bulsing",
        "C Blonk",
        "J Wilkinson",
        "G Semin"
      ],
      "year": "2015",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "4",
      "title": "The scent of emotions: A systematic review of human intra-and interspecific chemical communication of emotions",
      "authors": [
        "E Calvi",
        "U Quassolo",
        "M Massaia",
        "A Scandurra",
        "B D'aniello",
        "P D'amelio"
      ],
      "year": "2020",
      "venue": "Brain and Behavior"
    },
    {
      "citation_id": "5",
      "title": "To Mask or Not to Mask?: Balancing Privacy with Visual Confirmation Utility in Activity-Oriented Wearable Cameras",
      "authors": [
        "R Alharbi",
        "M Tolba",
        "L Petito",
        "J Hester",
        "N Alshurafa"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "6",
      "title": "Facts, fallacies, fears, and frustrations with human pheromones",
      "authors": [
        "C Wysocki",
        "G Preti"
      ],
      "year": "2004",
      "venue": "The Anatomical Record Part A: Discoveries in Molecular, Cellular, and Evolutionary Biology"
    },
    {
      "citation_id": "7",
      "title": "The Great Pheromone Myth",
      "authors": [
        "R Doty"
      ],
      "year": "2010",
      "venue": "The Great Pheromone Myth"
    },
    {
      "citation_id": "8",
      "title": "Chemosignals Communicate Human Emotions",
      "authors": [
        "J De Groot",
        "M Smeets",
        "A Kaldewaij",
        "M Duijndam",
        "G Semin"
      ],
      "year": "2012",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "9",
      "title": "A social chemosignaling function for human handshaking",
      "authors": [
        "I Frumin",
        "O Perl",
        "Y Endevelt-Shapira",
        "A Eisen",
        "N Eshel",
        "I Heller",
        "M Shemesh",
        "A Ravia",
        "L Sela",
        "A Arzi",
        "N Sobel"
      ],
      "year": "2015",
      "venue": "eLife"
    },
    {
      "citation_id": "10",
      "title": "Fear-Related Chemosignals Modulate Recognition of Fear in Ambiguous Facial Expressions",
      "authors": [
        "W Zhou",
        "D Chen"
      ],
      "year": "2009",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "11",
      "title": "Titrating the Smell of Fear: Initial Evidence for Dose-Invariant Behavioral, Physiological, and Neural Responses",
      "authors": [
        "J De Groot",
        "P Kirk",
        "J Gottfried"
      ],
      "year": "2021",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "12",
      "title": "Human Tears Contain a Chemosignal",
      "authors": [
        "S Gelstein",
        "Y Yeshurun",
        "L Rozenkrantz",
        "S Shushan",
        "I Frumin",
        "Y Roth",
        "N Sobel"
      ],
      "year": "2011",
      "venue": "Science"
    },
    {
      "citation_id": "13",
      "title": "Enhanced Chemosensory Detection of Negative Emotions in Congenital Blindness",
      "authors": [
        "K Iversen",
        "M Ptito",
        "P Møller",
        "R Kupers"
      ],
      "year": "2015",
      "venue": "Neural Plasticity"
    },
    {
      "citation_id": "14",
      "title": "Entangled chemosensory emotion and identity: Familiarity enhances detection of chemosensorily encoded emotion",
      "authors": [
        "W Zhou",
        "D Chen"
      ],
      "year": "2011",
      "venue": "Social Neuroscience"
    },
    {
      "citation_id": "15",
      "title": "Reduced recruitment of orbitofrontal cortex to human social chemosensory cues in social anxiety",
      "authors": [
        "W Zhou",
        "P Hou",
        "Y Zhou",
        "D Chen"
      ],
      "year": "2011",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "16",
      "title": "Crowd-based breath analysis: Assessing behavior, activity, exposures, and emotional response of people in groups",
      "authors": [
        "J Williams",
        "J Pleil"
      ],
      "year": "2016",
      "venue": "Journal of Breath Research"
    },
    {
      "citation_id": "17",
      "title": "More Data, Please: Machine Learning to Advance the Multidisciplinary Science of Human Sociochemistry",
      "authors": [
        "J De Groot",
        "I Croijmans",
        "M Smeets"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "18",
      "title": "Metal Oxide Gas Sensors: Sensitivity and Influencing Factors",
      "authors": [
        "C Wang",
        "L Yin",
        "L Zhang",
        "D Xiang",
        "R Gao"
      ],
      "year": "2010",
      "venue": "Sensors"
    },
    {
      "citation_id": "19",
      "title": "A Context Aware System Based on Scent",
      "authors": [
        "Y Kobayashi",
        "T Terada",
        "M Tsukamoto"
      ],
      "year": "2011",
      "venue": "2011 15th Annual International Symposium on Wearable Computers"
    },
    {
      "citation_id": "20",
      "title": "Detecting cooking state with gas sensors during dry cooking",
      "authors": [
        "S Hirano",
        "J Brubaker",
        "D Patterson",
        "G Hayes"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "21",
      "title": "NOSE: A Novel Odor Sensing Engine for Ambient Monitoring of the Frying Cooking Method in Kitchen Environments",
      "authors": [
        "P Khaloo",
        "B Oubre",
        "J Yang",
        "T Rahman",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "22",
      "title": "What's Cooking? Olfactory Sensing Using Off-the-Shelf Components",
      "authors": [
        "S Kratz",
        "A Monroy-Hernández",
        "R Vaish"
      ],
      "year": "2022",
      "venue": "Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology"
    },
    {
      "citation_id": "23",
      "title": "Bin-ary: Detecting the state of organic trash to prevent insalubrity",
      "authors": [
        "J Amores",
        "P Maes",
        "J Paradiso"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers -UbiComp '15"
    },
    {
      "citation_id": "24",
      "title": "AirSense: An intelligent homebased sensing system for indoor air quality analytics",
      "authors": [
        "B Fang",
        "Q Xu",
        "T Park",
        "M Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "25",
      "title": "Back to mouth",
      "authors": [
        "T Iwamoto",
        "Y Sasayama",
        "M Motoki",
        "T Kosaka"
      ],
      "year": "2009",
      "venue": "ACM SIGGRAPH 2009 Emerging Technologies"
    },
    {
      "citation_id": "26",
      "title": "Development of Electronic Nose for Evaluation of Fragrance and Human Body Odor in the Cosmetic Industry",
      "authors": [
        "T Eamsa-Ard",
        "M Swe",
        "T Seesaard",
        "T Kerdcharoen"
      ],
      "year": "2018",
      "venue": "2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)"
    },
    {
      "citation_id": "27",
      "title": "VOCNEA: Sleep apnea and hypopnea detection using a novel tiny gas sensor",
      "authors": [
        "T Röddiger",
        "M Beigl",
        "M Köpke",
        "M Budde"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "28",
      "title": "AirSpec: A Smart Glasses Platform, Tailored for Research in the Built Environment",
      "authors": [
        "P Chwalek",
        "S Zhong",
        "D Ramsay",
        "N Perry",
        "J Paradiso"
      ],
      "year": "2023",
      "venue": "Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing & the 2023 ACM International Symposium on Wearable Computing"
    },
    {
      "citation_id": "29",
      "title": "Wearable sensors for analyzing personal exposure to air pollution",
      "authors": [
        "N Dam",
        "A Ricketts",
        "B Catlett",
        "J Henriques"
      ],
      "year": "2017",
      "venue": "2017 Systems and Information Engineering Design Symposium (SIEDS)"
    },
    {
      "citation_id": "30",
      "title": "W-Air: Enabling Personal Air Pollution Monitoring on Wearables",
      "authors": [
        "B Maag",
        "Z Zhou",
        "L Thiele"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "31",
      "title": "Wearable Smart Prototype for Personal Air Quality Monitoring",
      "authors": [
        "A Geczy",
        "L Kuglics",
        "L Jakab",
        "G Harsanyi"
      ],
      "year": "2020",
      "venue": "2020 IEEE 26th International Symposium for Design and Technology in Electronic Packaging (SIITME)"
    },
    {
      "citation_id": "32",
      "title": "Hilo-wear: Exploring Wearable Interaction with Indoor Air Quality Forecast",
      "authors": [
        "S Zhong",
        "H Alavi",
        "D Lalanne"
      ],
      "year": "2020",
      "venue": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "33",
      "title": "Wearable system for outdoor air quality monitoring in a WSN with cloud computing: Design, validation and deployment",
      "authors": [
        "S Palomeque-Mangut",
        "F Meléndez",
        "J Gómez-Suárez",
        "S Frutos-Puerto",
        "P Arroyo",
        "E Pinilla-Gil",
        "J Lozano"
      ],
      "year": "2022",
      "venue": "Chemosphere"
    },
    {
      "citation_id": "34",
      "title": "Decoding the social volatilome by tracking rapid context-dependent odour change",
      "authors": [
        "S Roberts",
        "P Misztal",
        "B Langford"
      ],
      "year": "2020",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "35",
      "title": "Feasibility of emotion recognition from breath gas information",
      "authors": [
        "K Takahashi",
        "I Sugimoto"
      ],
      "year": "2008",
      "venue": "2008 IEEE/ASME International Conference on Advanced Intelligent Mechatronics"
    },
    {
      "citation_id": "36",
      "title": "Remarks on emotion recognition from breath gas information",
      "year": "2009",
      "venue": "2009 IEEE International Conference on Robotics and Biomimetics (ROBIO)"
    },
    {
      "citation_id": "37",
      "title": "Cinema Data Mining: The Smell of Fear",
      "authors": [
        "J Wicker",
        "N Krauter",
        "B Derstorff",
        "C Stönner",
        "E Bourtsoukidis",
        "T Klüpfel",
        "J Williams",
        "S Kramer"
      ],
      "year": "2015",
      "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "38",
      "title": "Cinema audiences reproducibly vary the chemical composition of air during films, by broadcasting scene specific emissions on breath",
      "authors": [
        "J Williams",
        "C Stönner",
        "J Wicker",
        "N Krauter",
        "B Derstroff",
        "E Bourtsoukidis",
        "T Klüpfel",
        "S Kramer"
      ],
      "year": "2016",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "39",
      "title": "From What You See to What We Smell: Linking Human Emotions to Bio-markers in Breath",
      "authors": [
        "J Bensemann",
        "H Cheena",
        "D Huang",
        "E Broadbent",
        "J Williams",
        "J Wicker"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "One-year dataset of hourly air quality parameters from 100 air purifiers used in China residential buildings",
      "authors": [
        "J Wei",
        "Y Wang",
        "J Mo",
        "C Fan"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "41",
      "title": "AQ-Bench: A benchmark dataset for machine learning on global air quality metrics",
      "authors": [
        "C Betancourt",
        "T Stomberg",
        "R Roscher",
        "M Schultz",
        "S Stadtler"
      ],
      "year": "2021",
      "venue": "Earth System Science Data"
    },
    {
      "citation_id": "42",
      "title": "H2020 project CAPTOR dataset: Raw data collected by low-cost MOX ozone sensors in a real air pollution monitoring network",
      "authors": [
        "J Barcelo-Ordinas",
        "P Ferrer-Cid",
        "J Garcia-Vidal",
        "M Viana",
        "A Ripoll"
      ],
      "year": "2021",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "43",
      "title": "Raw data collected from NO2 , O3 and NO air pollution electrochemical low-cost sensors",
      "authors": [
        "P Ferrer-Cid",
        "J Barcelo-Ordinas",
        "J Garcia-Vidal"
      ],
      "year": "2022",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "44",
      "title": "Electronic nose dataset for detection of wine spoilage thresholds",
      "authors": [
        "J Rodriguez Gamboa",
        "E Albarracin",
        "A Da Silva",
        "T Ferreira"
      ],
      "year": "2019",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "45",
      "title": "Electronic nose dataset for beef quality monitoring in uncontrolled ambient conditions",
      "authors": [
        "D Wijaya",
        "R Sarno",
        "E Zulaika"
      ],
      "year": "2018",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "46",
      "title": "Electronic Nose Dataset for Classifying Rice Quality using Neural Network",
      "authors": [
        "F Erlangga",
        "D Wijaya",
        "W Wikusna"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Information and Communication Technology (ICoICT)"
    },
    {
      "citation_id": "47",
      "title": "Electronic nose homogeneous data sets for beef quality classification and microbial population prediction",
      "authors": [
        "D Wijaya",
        "R Sarno",
        "E Zulaika",
        "F Afianti"
      ],
      "year": "2022",
      "venue": "BMC Research Notes"
    },
    {
      "citation_id": "48",
      "title": "Electronic nose dataset for pork adulteration in beef",
      "authors": [
        "R Sarno",
        "S Sabilla",
        "D Wijaya",
        "D Sunaryono",
        "C Fatichah"
      ],
      "year": "2020",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "49",
      "title": "Open-set gas recognition: A case-study based on an electronic nose dataset",
      "authors": [
        "C Qu",
        "C Liu",
        "Y Gu",
        "S Chai",
        "C Feng",
        "B Chen"
      ],
      "year": "2022",
      "venue": "Sensors and Actuators B: Chemical"
    },
    {
      "citation_id": "50",
      "title": "Chemical gas sensor array dataset",
      "authors": [
        "J Fonollosa",
        "I Rodríguez-Luján",
        "R Huerta"
      ],
      "year": "2015",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "51",
      "title": "Electronic nose dataset for COPD detection from smokers and healthy people through exhaled breath analysis",
      "authors": [
        "C Durán Acevedo",
        "C Vasquez",
        "J Carrillo Gómez"
      ],
      "year": "2021",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "52",
      "title": "A benchmarking protocol for breath analysis: The peppermint experiment",
      "authors": [
        "B Henderson",
        "D Ruszkiewicz",
        "M Wilkinson",
        "J Beauchamp",
        "S Cristescu",
        "S Fowler",
        "D Salman",
        "F Francesco",
        "G Koppen",
        "J Langejürgen",
        "O Holz",
        "A Hadjithekli",
        "S Moreno",
        "M Pedrotti",
        "P Sinues",
        "G Slingers",
        "M Wilde",
        "T Lomonaco",
        "D Zanella",
        "R Zenobi",
        "J.-F Focant",
        "S Grassin-Delyle",
        "F Franchina",
        "M Malásková",
        "P.-H Stefanuto",
        "G Pugliese",
        "C Mayhew",
        "C Thomas"
      ],
      "year": "2020",
      "venue": "Journal of Breath Research"
    },
    {
      "citation_id": "53",
      "title": "The peppermint breath test: A benchmarking protocol for breath sampling and analysis using GC-MS",
      "authors": [
        "M Wilkinson",
        "I White",
        "K Hamshere",
        "O Holz",
        "S Schuchardt",
        "F Bellagambi",
        "T Lomonaco",
        "D Biagini",
        "F Di",
        "S Fowler"
      ],
      "year": "2021",
      "venue": "Journal of Breath Research"
    },
    {
      "citation_id": "54",
      "title": "Emotion elicitation using films",
      "authors": [
        "J Rottenberg",
        "R Ray",
        "J Gross"
      ],
      "year": "2007",
      "venue": "The Handbook of Emotion Elicitation and Assessment"
    },
    {
      "citation_id": "55",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "A Schaefer",
        "F Nils",
        "X Sanchez",
        "P Philippot"
      ],
      "year": "2010",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "56",
      "title": "Emognition dataset: Emotion recognition with self-reports, facial expressions, and physiology using wearables",
      "authors": [
        "S Saganowski",
        "J Komoszyńska",
        "M Behnke",
        "B Perz",
        "D Kunc",
        "B Klich",
        "Ł Kaczmarek",
        "P Kazienko"
      ],
      "year": "2022",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "57",
      "title": "Nature-Based Relaxation Videos and Their Effect on Heart Rate Variability",
      "authors": [
        "A Benz",
        "R Gaertner",
        "M Meier",
        "E Unternaehrer",
        "S Scharndke",
        "C Jupe",
        "M Wenzel",
        "U Bentele",
        "S Dimitroff",
        "B Denk",
        "J Pruessner"
      ],
      "year": "2022",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "58",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Recognizing lung cancer using a homemade e-nose: A comprehensive study",
      "authors": [
        "W Li",
        "Z Jia",
        "D Xie",
        "K Chen",
        "J Cui",
        "H Liu"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "60",
      "title": "A method of analysing interview transcripts in qualitative research",
      "authors": [
        "P Burnard"
      ],
      "year": "1991",
      "venue": "Nurse Education Today"
    },
    {
      "citation_id": "61",
      "title": "Optimization of Metal Oxide Nanosensors and Development of a Feature Extraction Algorithm to Analyze VOC Profiles in Exhaled Breath",
      "authors": [
        "M Maciel",
        "S Sankari",
        "M Woollam",
        "M Agarwal"
      ],
      "year": "2023",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "62",
      "title": "Exhaled breath analysis using electronic nose and gas chromatographymass spectrometry for non-invasive diagnosis of chronic kidney disease, diabetes mellitus and healthy subjects",
      "authors": [
        "T Saidi",
        "O Zaim",
        "M Moufid",
        "N Bari",
        "R Ionescu",
        "B Bouchikhi"
      ],
      "year": "2018",
      "venue": "Sensors and Actuators B: Chemical"
    },
    {
      "citation_id": "63",
      "title": "Detection of Liver Dysfunction Using a Wearable Electronic Nose System Based on Semiconductor Metal Oxide Sensors",
      "authors": [
        "A Voss",
        "R Schroeder",
        "S Schulz",
        "J Haueisen",
        "S Vogler",
        "P Horn",
        "A Stallmach",
        "P Reuken"
      ],
      "year": "2022",
      "venue": "Biosensors"
    },
    {
      "citation_id": "64",
      "title": "A feature extraction algorithm for multi-peak signals in electronic noses",
      "authors": [
        "R Haddad",
        "L Carmel",
        "D Harel"
      ],
      "year": "2007",
      "venue": "Sensors and Actuators B: Chemical"
    },
    {
      "citation_id": "65",
      "title": "High-Dimensional Time Series Feature Extraction for Low-Cost Machine Olfaction",
      "authors": [
        "P Shakya",
        "E Kennedy",
        "C Rose",
        "J Rosenstein"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "66",
      "title": "Detecting of Coal Gas Weak Signals Using Lyapunov Exponent under Strong Noise Background",
      "authors": [
        "M Xian-Min"
      ],
      "year": "2013",
      "venue": "2013 Third International Conference on Intelligent System Design and Engineering Applications"
    },
    {
      "citation_id": "67",
      "title": "Development of wavelet transforms to predict methane in chili using the electronic nose",
      "authors": [
        "S Sabilla",
        "R Sarno"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Advanced Mechatronics, Intelligent Manufacture, and Industrial Automation (ICAMIMIA)"
    },
    {
      "citation_id": "68",
      "title": "Exploiting plume structure to decode gas source distance using metal-oxide gas sensors",
      "authors": [
        "M Schmuker",
        "V Bahr",
        "R Huerta"
      ],
      "year": "2016",
      "venue": "Sensors and Actuators B: Chemical"
    },
    {
      "citation_id": "69",
      "title": "Smelling Nano Aerial Vehicle for Gas Source Localization and Mapping",
      "authors": [
        "J Burgués",
        "V Hernández",
        "A Lilienthal",
        "S Marco"
      ],
      "year": "2019",
      "venue": "Sensors"
    }
  ]
}