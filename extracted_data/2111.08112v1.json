{
  "paper_id": "2111.08112v1",
  "title": "Biologically Inspired Speech Emotion Recognition",
  "published": "2021-11-15T22:28:05Z",
  "authors": [
    "Reza Lotfidereshgi",
    "Philippe Gournay"
  ],
  "keywords": [
    "speech emotion recognition",
    "source-filter model",
    "liquid state machine",
    "reservoir computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conventional feature-based classification methods do not apply well to automatic recognition of speech emotions, mostly because the precise set of spectral and prosodic features that is required to identify the emotional state of a speaker has not been determined yet. This paper presents a method that operates directly on the speech signal, thus avoiding the problematic step of feature extraction. Furthermore, this method combines the strengths of the classical source-filter model of human speech production with those of the recently introduced liquid state machine (LSM), a biologically-inspired spiking neural network (SNN). The source and vocal tract components of the speech signal are first separated and converted into perceptually relevant spectral representations. These representations are then processed separately by two reservoirs of neurons. The output of each reservoir is reduced in dimensionality and fed to a final classifier. This method is shown to provide very good classification performance on the Berlin Database of Emotional Speech (Emo-DB). This seems a very promising framework for solving efficiently many other problems in speech processing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech is a fundamental means of communicating not only words, but also a vast range of human emotions. Consequently, speech processing applications, such as humanmachine interfacing and speech recognition, could benefit from the introduction of a reliable method for automatic recognition of human emotions through speech.\n\nConventional speech emotion recognition methods consist of a feature extraction step followed by a classifier. Various spectral and prosodic features can be used  [1] . Finding the \"best\" set of features, namely, one that is both complete and compact, is a critical step which has a considerable impact on the performance of the system. State of the art conventional methods mostly differ in their choice of features and of classifier type  [2, 3] .\n\nIn recent years, there has been an increasing trend toward developing speech processing methods that operate directly on the speech signal in order to avoid the problematic feature extraction step. For example, Convolutional Neural Networks (CNNs)  [4]  and Deep Neural Networks (DNNs)  [5] , have been successfully used for recognizing emotions directly from raw temporal or spectral data.\n\nThe Liquid State Machine (LSM) is another recently proposed method that operates directly on raw data. The LSM relies on a network of spiking neurons that are much closer to biological neurons than the rate-based model used in CNNs and DNNs. Despite its theoretical appeal, the LSM is slow in finding practical applications. The main problem when implementing an LSM is to create a specific reservoir design that is best adapted to the task at hand  [6] . In this paper, this problem is solved by introducing prior knowledge about the human speech production system into the LSM. Without any loss in terms of information, the speech signal is divided into two components: the source and the vocal tract. Individually, each component is easier to process by a reservoir of spiking neurons. Furthermore, the inclusion of a production model in the recognition system is justified by the motor theory of speech perception, that states that people perceive speech by identifying the vocal tract gestures that produced it  [7] .\n\nThe outline of the paper is as follows. The source-filter model for human speech production and the principles underlying the liquid state machine are reviewed in section 2. The proposed biologically inspired method is presented in section 3. Some experimental results are given and discussed in section 4, and conclusions are drawn in section 5.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Relation To Prior Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Source-Filter Speech Production Model",
      "text": "According to the source-filter model of human speech production, a speech signal is produced by passing a source of air pressure through an acoustic filter  [8] . The source is a combination of a noise-like turbulent excitation produced by constrictions along the vocal tract (for unvoiced speech) and a quasi-periodic excitation produced by vibrating vocal folds (for voiced speech). The filter represents the variable ©2017 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses. arXiv:2111.08112v1 [eess.AS] 15 Nov 2021 response of the vocal tract. In practice, the most commonly used method to separate the contributions of the source and the filter is the Linear Predictive (LP) analysis. The LP analysis is a frame-based process which results in: (1) a set of LP coefficients which represent the filter for the frame; and (2) a residual error signal which represents the source. Equation  1 shows the calculation of a predicted speech sample x(n) from past speech samples x(n -i) and the calculation of a residual sample e(n). The Levinson-Durbin algorithm is usually used to find the a i coefficients that minimize the quadratic error E as shown in equation 2.\n\nThe LPC analysis has long proven to be a very efficient tool in speech processing and is now used for example in every speech coder.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Liquid State Machine",
      "text": "A reservoir computing system consists of a Recurrent Neural Network (RNN) followed by an output layer of neurons that performs the final recognition/classification task  [6] . In a reservoir computing system, the RNN is randomly created and does not need to be trained using supervised methods such as the gradient descent. The output layer, in contrast, is trained using a supervised method. Reservoir computing is successful for complex nonlinear classification tasks for two reasons. First, because training an RNN using a gradientdescent algorithm would be time consuming and prone to convergence issues. Secondly, because reservoir computing has been shown to outperform most other nonlinear identification, prediction and classification methods on various problems.\n\nThe Liquid State Machine (LSM) is a special type of reservoir computing method where the reservoir is a Spiking Neural Network (SNN)  [9] . SNNs use temporal coding and therefore process information in very much the same way as a biological neural structure does. Fig.  1  shows a typical LSM structure. First, the LSM uses a function L M to map the input u(t) to the \"liquid state\" x(t), where x(t) is an arbitrary nonlinear function of the input u(t) and of the past input values. Secondly, a memoryless function f M maps x(t) to the output y(t). This \"readout function\" is trained for the task to accomplish. The SNN performs a nonlinear mapping from the input space to the high dimensional \"liquid state\" space. As a result of this projection, the separation of different classes by the readout function is much easier. Several methods including Support Vector Machine (SVM), Multi Layer Perceptron (MLP) and ridge regression have been tried as reservoir readouts  [6] . Fig.  1 . A typical LSM structure. Only the output layer is trained using a supervised methods.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Fig.  2  shows the flowchart of the proposed speech emotion recognition method. The recognition process is divided into two steps: the preprocessing and the liquid state machine.\n\nIn the preprocessing step, the input speech signal is divided into two orthogonal and complementary components that are transformed and perceptually shaped according to the properties of the human cochlea. Specifically, an LP analysis is performed on a frame base. The prediction residual is calculated according to equation 1 and decomposed using a 77channel gammatone filterbank with ERB scaling. This constitutes the input of the first reservoir. In parallel, the frequency response of each all-pole LP filter is computed to reveal the formant structure of the speech signal. This frequency response is also shaped using the exact same ERB scaling and constitutes the input of the second reservoir.\n\nAs in the lower auditory nuclei, even auditory cortex has the tonotopic structure  [10] . Such structure suggests that closer frequency channels are processed by closer groups of neurons. In the design of the reservoirs, the neurons are therefore arranged in 3D structures, each reservoir containing 77 layers of 3*3 neurons. Each layer of neurons is excited by only one of the 77 input channels, in order of increasing frequency. Connections between closer neurons are favored, with a probability of connection between neuron n1 and n2 that depends on the distance D(n1, n2) according to equation 3. Parameters C and λ are responsible for controlling the reach and density of the connections, and are set respectively to 1 and 3.4. These values were determined after some experiments and could probably be further optimized.\n\nA standard implementation of the integrate-and-fire neuron by Troyer is used  [11] . The Asymmetric Spike Time-Dependent Plasticity (STDP) is then used as the learning rule to adapt the conductance of the synapses throughout the speech sample. This learning rule is known to result in stable networks that are very effective at extracting the correlations present in the input  [12] . The exact learning rule is given in equation 4.\n\nMore details about this learning rule can be found in  [12] . ∆ is the time difference between pre-and post-synaptic spikes. A + and A -are maximum amount of synaptic modifications. Two key parameters to be set are the time constants τ + and τ -because they condition the memory of the reservoir. Following  [12] , τ + was set to 20 ms and τ -was tuned to maximize performance (see section 4). The simulation of neural activity is done using the Brian2 simulator  [13] .\n\nTo reduce dimensionality, Principal Component Analysis (PCA) is applied to the average activity of the neurons from each reservoir. Compared to the widely used ridge regression, PCA presents the advantage of being able to shrink the output of the two reservoirs separately. The outputs of the two PCAs are simply combined. For final recognition, Linear Discriminant Analysis (LDA) is used.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Berlin Database Of Emotional Speech",
      "text": "The proposed method was tested on the Berlin database of emotional speech (Emo-DB,  [14] ). This is a well recorded and now widely used emotional speech database. It is easily accessible and well documented. It contains 535 utterances produced by ten professional actors pronouncing ten different texts and covers seven different emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing",
      "text": "The preprocessing step first consists in an LP analysis of the input speech signal. The autocorrelation method is used to estimate LP filters of order 16. A 30 ms Hamming window is used so that the formant structure is adequately captured. The LP coefficients are updated every 5 ms in order to closely track the changes in the vocal tract. The source and vocal tract components of the speech signal are then separated. First, the LP residual is computed and fed to a 77-channel gammatone filterbank. For each channel of the filterbank, the energy of 5 ms segments is computed and a logarithm is applied to reduce the dynamic range of this representation of the source component. Secondly, the frequency response of each LP filters is computed and shaped using an ERB frequency scaling.\n\nA logarithm is also applied to reduce the dynamic range of this representation. An example of emotional speech signal is represented in Fig.  3(a) . The corresponding source and vocal tract representations are presented in Fig.  3 (b) and Fig.  3 (c), respectively. These two spectro-temporal representations of the speech signal are used as inputs for two reservoirs of spiking neurons.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Lsm Tuning",
      "text": "Fig.  4  shows the recognition rate of the proposed method for different numbers of principal components for each of the two reservoirs. The reservoir for the vocal tract component was tuned for τ- τ+ = 5 and the reservoir for the source component was tuned for τ- τ+ = 3. The results are obtained using 50-fold cross validation where 90% of the database is used for training and 10% for testing. Results below 60% of recognition rate are not shown. The vertical and horizontal axes are the number of principal component selected from the vocal tract and source reservoirs, respectively. The borders of the figure shows the performance when only one reservoir is used (no component from the other reservoir is selected). It is quite clear that both reservoirs contribute highly to the final recognition rate. The performance of the proposed method is not very sensitive to the choice of numbers of principal components, since the recognition rate stays above 80% for a wide range of numbers of components.\n\nThe highest recognition rate of 82.35% is achieved for 29 and 44 principal components for the vocal tract and the source reservoirs, respectively, and the 95% confidence interval is ±1.36%. Table  1  shows the corresponding confusion matrix.\n\nTable  2  compares the recognition rate obtained with the proposed method to those obtained with other methods that have been tested on the same emotional speech database. Using feature selection and fusion, the method presented by Jin in  [15]  achieved 83.10% of correct recognition. It should be noted however that this method was tested on a subset of only 494 speech samples out of 535, which artificially increases the performance. Using an enhanced kernel isomap, the method presented by Zhang in  [16]  achieved a recognition rate of 80.85%. Finally, using rhyme and temporal features, the one presented by Bhargava in  [17]  achieved 80.60%. With a recognition rate of 82.35%, the method proposed in this paper compares favorably to these state of the art methods.\n\nIn another experiment, we used an LSM with one single reservoir to recognize emotion directly from the speech signal, without separating the source from the vocal tract. The same preprocessing as for the source component was used.   The design of the rest of the system was not changed. After tuning the reservoir, a recognition rate of 75.73% was obtained. The 6.62% difference in recognition rate clearly indicates that including a source-filter model in the recognition system significantly improves performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "5.",
      "text": "This paper proposed a new method for automatic recognition of speech emotions based on the Liquide State Machine (LSM), an emerging and very promising tool. This method operates directly on the speech signal and thus requires no feature extraction. It is based on several biological elements. First, the LSM includes a reservoir of spiking neurons which are very close to biological cortical neurons. Then, its original LSM design with two separate reservoirs (one for the source signal and the other for the vocal tract) builds upon the motor theory of human speech perception. This design is more flexible and tunable, as for example the size and memory of the two reservoirs can be tuned separately. One could imagine decomposing the signal even further, using for example rapidly evolving and slowly evolving waveform decomposition of the source signal  [18]  Finally, the source and vocal tract components of the speech signal are both analyzed on an Equivalent Rectangular Bandwidth (ERB) scale which is a good model for the human peripheral auditory system.\n\nThe experimental results showed that this method provides a very good classification performance for an emotion recognition task. It is, however, a very general framework that should also perform well for many other speech processing tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A typical LSM structure. Only the output layer is",
      "page": 2
    },
    {
      "caption": "Figure 2: Flowchart of proposed emotion recognition method.",
      "page": 3
    },
    {
      "caption": "Figure 3: Preprocessing of speech signal. The scales of the",
      "page": 3
    },
    {
      "caption": "Figure 4: Performance (in percent) of the proposed method for",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: compares the recognition rate obtained with the",
      "data": [
        {
          "Ourmethod": "82.35%",
          "Jin": "*83.10%",
          "Zhang": "80.85%",
          "Bhargava": "80.60%"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Robust emotion recognition using spectral and prosodic features",
      "authors": [
        "Sreenivasa Rao",
        "Shashidhar Koolagudi"
      ],
      "year": "2013",
      "venue": "Robust emotion recognition using spectral and prosodic features"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "Christos-Nikolaos Anagnostopoulos",
        "Theodoros Iliou",
        "Ioannis Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "5",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Stefanos Nicolaou",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Towards realtime speech emotion recognition using deep neural networks",
      "authors": [
        "M Hm Fayek",
        "L Lech",
        "Cavedon"
      ],
      "year": "2015",
      "venue": "Signal Processing and Communication Systems (ICSPCS), 2015 9th International Conference on"
    },
    {
      "citation_id": "7",
      "title": "Reservoir computing approaches to recurrent neural network training",
      "authors": [
        "Mantas Lukoševičius",
        "Herbert Jaeger"
      ],
      "year": "2009",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "8",
      "title": "Perception of the speech code",
      "authors": [
        "Franklin S Alvin M Liberman",
        "Donald Cooper",
        "Michael Shankweiler",
        "Studdert-Kennedy"
      ],
      "year": "1967",
      "venue": "Psychological review"
    },
    {
      "citation_id": "9",
      "title": "Acoustic theory of speech production: with calculations based on X-ray studies of Russian articulations",
      "authors": [
        "Gunnar Fant"
      ],
      "year": "1971",
      "venue": "Acoustic theory of speech production: with calculations based on X-ray studies of Russian articulations"
    },
    {
      "citation_id": "10",
      "title": "A model for real-time computation in generic neural microcircuits",
      "authors": [
        "Wolfgang Maass",
        "Thomas Natschläger",
        "Henry Markram"
      ],
      "year": "2002",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "Auditory perception and cognition",
      "authors": [
        "Rungsun Munkong",
        "Biing-Hwang Juang"
      ],
      "year": "2008",
      "venue": "IEEE signal processing magazine"
    },
    {
      "citation_id": "12",
      "title": "Integrate-andfire neurons matched to physiological fi curves yield high input sensitivity and wide dynamic range",
      "authors": [
        "W Todd",
        "Kenneth Troyer",
        "Miller"
      ],
      "year": "1997",
      "venue": "Computational Neuroscience"
    },
    {
      "citation_id": "13",
      "title": "Cortical development and remapping through spike timing-dependent plasticity",
      "authors": [
        "Sen Song",
        "Larry Abbott"
      ],
      "year": "2001",
      "venue": "Neuron"
    },
    {
      "citation_id": "14",
      "title": "Equation-oriented specification of neural models for simulations",
      "authors": [
        "Marcel Stimberg",
        "Dan Goodman",
        "Victor Benichoux",
        "Romain Brette"
      ],
      "year": "2014",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "15",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "16",
      "title": "A feature selection and feature fusion combination method for speaker-independent speech emotion recognition",
      "authors": [
        "Yun Jin",
        "Peng Song",
        "Wenming Zheng",
        "Li Zhao"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition using an enhanced kernel isomap for human-robot interaction",
      "authors": [
        "Shiqing Zhang",
        "Xiaoming Zhao",
        "Bicheng Lei"
      ],
      "year": "2013",
      "venue": "International Journal of Advanced Robotic Systems"
    },
    {
      "citation_id": "18",
      "title": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "authors": [
        "Mayank Bhargava",
        "Tim Polzehl"
      ],
      "year": "2013",
      "venue": "Improving automatic emotion recognition from speech using rhythm and temporal feature",
      "arxiv": "arXiv:1303.1761"
    },
    {
      "citation_id": "19",
      "title": "Waveform interpolation speech coder at 4 kb/s",
      "authors": [
        "Eddie Lt Choy"
      ],
      "year": "1998",
      "venue": "Waveform interpolation speech coder at 4 kb/s"
    }
  ]
}