{
  "paper_id": "2304.11117v1",
  "title": "A Vector Quantized Masked Autoencoder For Speech Emotion Recognition",
  "published": "2023-04-21T16:37:57Z",
  "authors": [
    "Samir Sadok",
    "Simon Leglaive",
    "Renaud Séguier"
  ],
  "keywords": [
    "Self-supervised learning",
    "masked autoencoder",
    "vector-quantized variational autoencoder",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER)  [1]  is a research area focused on automatically identifying emotions from speech signals. With the growth of technology and the increasing use of speech-based interfaces, there is a growing demand for systems that can accurately recognize emotions in speech. In recent years, deep learning methods have played a major role in improving SER performance  [2] . However, the scarcity and high cost of obtaining labeled speech data for emotion recognition pose a major difficulty. To address this challenge, researchers have shifted their focus towards self-supervised learning approaches  [3, 4] . In these approaches, models are pretrained on a self-supervised task, such as predicting masked tokens in speech signals, and then fine-tuned on a smaller set of labeled data for the SER task  [5, 6] . This method has the advantage of scalability, as the self-supervised task can be trained on large amounts of unlabeled speech data, reducing the need for labeled data  [7, 8] . Self-supervised training has been successfully applied in the field of SER, and has demonstrated promising results by allowing the model to learn useful representations of speech signals for emotion recognition, even in scenarios where labeled data is limited  [6, 9] .\n\nThis article focuses on self-supervised SER with the masked autoencoder (MAE) approach  [10] . The MAE is an asymmetrical encoder-decoder architecture that relies on input masking  [10] . Originally developed in natural language processing (NLP)  [11] , the MAE approach has also been applied to image analysis using vision transformers (ViT)  [12] . The MAE process involves dividing the input into non-overlapping patches, each represented by a token embedding. A large proportion of tokens are masked (usually 75% for image modeling and 15% for text modeling), and only the visible tokens are fed to the encoder. A lightweight decoder then reconstructs the image/text by combining the visible tokens from the encoder and learnable mask tokens. The cost function is applied only to the masked tokens. Recently, the MAE has been adapted for audio using 2D time-frequency representations such as the melspectrogram  [5, 13, 14] . However, using L1 or L2 losses for reconstruction can result in a blurred image or a noisy audio signal. As He et al. suggest  [10] , improving the quality of MAE predictions can potentially lead to better representations for downstream tasks. This paper introduces the vector quantized MAE for speech (VQ-MAE-S), a self-supervised model designed for emotion detection in speech signals. VQ-MAE-S is an adapted version of the Audio-MAE model proposed in  [5, 13, 14] . Unlike Audio-MAE, VQ-MAE-S operates on the discrete latent representation of a vector-quantized variational autoencoder (VQ-VAE)  [15]  instead of the spectrogram representation. The pre-training of the model is performed on the VoxCeleb2 dataset  [16] , and fine-tuning is carried out on several standard emotional speech datasets. We conduct several experiments to study the impact of different model design choices (e.g., masking ratio, masking strategy, patch size, etc.). The experimental results demonstrate that the proposed VQ-MAE-S model yields improvements in SER performance compared to an MAE using raw spectrogram data and other state-of-the-art methods. The code and qualitative reconstruction results are available at https://samsad35.github.io/VQ-MAE-Speech/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vector Quantized Masked Autoencoder For Speech",
      "text": "This section presents the proposed self-supervised VQ-MAE-S model, which is represented in Figure  1 . The model takes as input the power spectrogram of a speech signal, denoted by x ∈ R T ×D + , where T and D correspond to the time and frequency dimensions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vector Quantized Variational Autoencoder",
      "text": "The proposed self-supervised approach builds upon the discrete latent representation of a VQ-VAE  [15]  assumed to be pre-trained and frozen (more details on the pre-training are given in Section 3.1.1). The VQ-VAE encoder is used to obtain a compressed and quantized representation xq ∈ Z T ×D of the input speech power spectrogram x ∈ R T ×D + . Each entry of xq corresponds to the index of a vector in the VQ-VAE codebook. The quantized representation xq keeps the aspect of a time-frequency representation, as the VQ-VAE is designed to be fully convolutional on the frequency axis and to process spectrogram frames independently. xq can thus be seen as a discrete and compressed representation of the speech power spectrogram x, where compression occurs along the frequency axis (D D). As illustrated in Figure  1  and discussed in the next subsections, the proposed self-supervised learning approach operates on this discrete and  compressed representation before reconstruction with the VQ-VAE decoder.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Tokens Creation, Masking, And Embedding",
      "text": "Discrete index tokens As illustrated in Figure  1 , we build discrete index tokens from the quantized representation xq by dividing it into non-overlapping patches of size (t × d). This leads to nt = T t and n d = D d patches horizontally and vertically, respectively. The rep-\n\nand used as input to the masking process. Masking We apply masking to the (nt × n d ) time-frequency grid of discrete index tokens, each of which has a dimension of t • d. The masked tokens are then replaced with a trainable vector. We explore three different types of patch-based masking, where tokens are masked randomly along the time-frequency, time, or frequency dimensions. Additionally, we test the frame masking strategy that involves masking the entire discrete frames of xq (columns) instead of the patches. The effectiveness of these strategies is experimentally evaluated in the context of SER, including varying the ratio of masked tokens from 50% to 90%. Continuous embedding tokens The discrete index tokens correspond to the indices obtained through the quantization step of the pretrained VQ-VAE encoder. Before being input to the VQ-MAE-S encoder, these discrete tokens are replaced with trainable continuous embedding vectors taken from a codebook of dimension R k×e , where k is the number of codes in the codebook, and e is the dimension of each code. This is simply achieved by replacing the t • d indices of a discrete token by the corresponding t • d vectors of dimension e in the codebook. The codebook is initialized by the pretrained VQ-VAE codebook and it can be held frozen or fine-tuned. After this embedding process (represented by the orange block in Figure1), the discrete index tokens in Z (n t •n d )×(t•d) are transformed into continuous embedding tokens in R (n t •n d )×(t•d•e) . In the experiments, we will investigate the effect of varying the embedding size e on the SER performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vq-Mae-S Encoder, Decoder, And Loss Function",
      "text": "Encoder The VQ-MAE-S encoder, similar to the ViT architecture  [12] , consists of a single Transformer encoder  [17] . This encoder is a stack of L residual blocks that includes a self-attention layer, a normalization layer, and a Multi-layer Perceptron (MLP) block. Trained position embeddings are added for each token  [11] . As in  [10] , the encoder inputs are only the visible tokens; this is to learn a representation that relies on the context. In addition, we add a global trainable token [CLS] as in  [11] , which will be used for SER tasks. Since most of the tokens are masked and only the visible tokens are fed to the encoder, this resolves the quadratic complexity issue inherent in transformer models with respect to the sequence length  [10] . As a result, the number of attention blocks in the encoder can be increased without experiencing computational inefficiencies. Decoder The VQ-MAE-S decoder takes in both visible and masked tokens along with an additional position embedding. It consists of attention blocks similar to the encoder, but with fewer consecutive blocks (L ) compared to the encoder (L > L ). The decoder also includes a linear layer at the end that maps to the size of the VQ-VAE codebook. The output of this linear layer corresponds to the logits of the discrete index tokens. After applying a max operation, we obtain a reconstruction x rec q of the indices xq that were provided by the VQ-VAE encoder. Loss function To train the VQ-MAE-S model, we minimize the cross-entropy loss applied only to the masked discrete index tokens:\n\nwhere xq(ΩM) and x rec q (ΩM) represent the masked tokens in xq and their reconstructions, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "3.1. Pre-training, fine-tuning, and evaluation",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pre-Training Of Vq-Mae-Speech",
      "text": "To pre-train VQ-MAE-S, we used the VoxCeleb2 dataset  [16] , which provides an extensive collection of audio speech data from opensource media, including speech segments corrupted by various realworld noises. We restricted our use of the dataset to a subset of around 1000 hours of audio-visual speech, including 2170 speakers.\n\nThe VQ-VAE model was trained on the same portion of the VoxCeleb2 dataset using short-time Fourier transform (STFT) power spectrograms (x). The STFT is computed using a 64-ms Hann window (1024 samples) and a 70% overlap, resulting in sequences of D = 513-dimensional discrete Fourier coefficients. The VQ-VAE architecture is symmetrical with respect to the encoder and the decoder, with three 1D convolution (encoder) or transposed convolution (decoder) layers on the frequency axis and a residual convolution layer. The model processes each frame independently with no time dependency. For each speech power spectrogram frame of size to increase the use of the different codes in the codebook, as in  [18] . We evaluate the impact of different encoder architectures on the performance of VQ-MAE-S. The architecture is denoted as VQ-MAE-S-n, where n refers to the number of attention blocks in the encoder. The decoder is fixed at four attention blocks. Each selfattention layer in these blocks is subdivided into four heads. The model is trained using the AdamW optimizer  [19]  with a cosine scheduler to adjust the learning rate, with a 100-epoch warm-up period. The parameters of the optimizer, similar to  [10] , are β2 = 0.9, β2 = 0.95, and weight decay= 0.05. The base learning rate follows the linear scaling rule  [20]  lr = (base lr = 1e -3) × (batchsize = 512)/256. We distributed the pre-training of VQ-MAE-S on 4 NVIDIA HGX A100. As in  [14] , no data augmentation is performed. For more information on the architecture, please refer to our publicly available implementation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ser Fine-Tuning Details",
      "text": "Only the encoder of the VQ-MAE-S model is fine-tuned for SER. We propose two approaches: The first one uses the global token [CLS] as input to a single linear layer, followed by a max operation to perform emotion classification. The second approach, referred to as Query2Emo and inspired by  [21] , involves cross-attention between all sequence tokens as value/key and the emotion classes represented by trainable embedding as query. Query2Emo has a single attention block for both the encoder and decoder. For these two approaches, we use the AdamW optimizer  [19]  with a cosine scheduler to adjust the learning rate and with a 40-epoch warm-up period. The parameters of the optimizer, similar to  [10] , are β2 = 0.9, β2 = 0.95, and weight decay = 0.05. The base learning rate is 1e-4. For the loss function, we adopt the asymmetric loss  [22]  adapted for single labels instead of the cross entropy as it yields better results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotional Databases For Fine-Tuning And Evaluation",
      "text": "We fine-tune and evaluate the proposed approaches on four emotional speech audio databases.  [23] : This English database consists of 1440 audio files recorded by 24 professional actors and labeled with eight different emotional expressions (neutral, calm, happy, sad, angry, fearful, disgust, surprised). RAVDESS-Song  [23] : Same as the RAVDESS-Speech database, but utterances are sung a capella. This database contains a total of 1012 audio files recorded by 23 actors and labeled with six emotions (neutral, calm, happy, sad, angry, and fearful). IEMOCAP  [24] : This database comprises approximately 12 hours of audio, annotated with several emotions, but only four emotions (neutral, happy, angry, and disgusted) have been retained to ensure a balanced distribution. It consists of dyadic sessions in which actors participate in improvisations or scripted scenarios. EMODB  [25] : The German EMODB database consists of 535 utterances spoken by ten professional speakers. It includes seven emotions (anger, boredom, anxiety, happiness, sadness, disgust, and neutral).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ravdess-Speech",
      "text": "For a fair comparison with the literature, we perform 5-fold cross-validation by separating the speakers' identity between the fine-tuning phase and the evaluation phase.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance On Speech Emotion Recognition",
      "text": "Table  1  highlights the importance of pre-training and fine-tuning the VQ-MAE-S-12 model for SER. Pre-training significantly improves the SER performance, with the accuracy increasing from 28.6% to 76.7% on the RAVDESS-Speech dataset. Fine-tuning the encoder is also crucial, as freezing the encoder results in a 19.1% accuracy loss. We also compared our approach with SpecMAE-12, an MAE model that directly uses speech power spectrogram patches and aims to reconstruct masked patches using L2 loss. Our approach outperforms SpecMAE-12 by 24.5% in terms of accuracy, which clearly shows the benefit of working on the discrete representation of the VQ-VAE instead of the raw spectrogram representation for training the MAE.\n\nTable  5  compares the SER performance (accuracy and F1-score) of the proposed VQ-MAE-S model (with classification from the [CLS] token), its improved version VQ-MAE-S + Query2Emo, the SpecMAE-12 baseline, and three state-of-the-art methods: SSAST  [5] , MAE-AST  [13] , and a supervised self-attention-based approach  [26]  on the four evaluation databases. Two configurations of masking are considered: random frame masking (Frame) and ran- We conducted several experiments to assess the importance of several hyperparameters of the proposed VQ-MAE-S model, which are presented and discussed in the following paragraphs. Impact of the masking strategy The choice of the masking strategy in MAE-based self-supervised models can have a significant impact on the performance of auxiliary tasks  [10] . As shown in Table  2 , which compares the performance of emotion recognition across four masking types discussed in Section 2.2, random frame-based masking outperforms random patch-based masking. In particular, the frame-based approach achieves performance gains of 15.1%, 12.3%, and 4.1% compared to random patch-based masking on the time, frequency, and time-frequency axes, respectively. These results are consistent with those reported in prior studies  [5, 13] , and are highlighted in Table  5  for the four evaluated databases. Impact of the masking ratio Figure  2  shows the relationship between masking ratio (x-axis) and SER accuracy (y-axis) for the patch (blue) and frame (red) masking using VQ-MAE-S. The results indicate that patch masking achieves maximum accuracy at around 80% masking ratio, after which performance declines. The trend can be attributed to the complexity of the task, where higher masking ratios improve representation learning and SER accuracy. In contrast, the performance of frame-based masking remains relatively stable. However, pushing the complexity too high (e.g., 90%) leads to a loss of relevant contextual information and decreased SER accuracy. Impact of the encoder depth The impact of the encoder depth on SER performance is presented in Table  4 , where the number of atten-tion blocks is varied. The results show that, to some extent, increasing the number of blocks in the encoder leads to improved performance. When the number of blocks becomes very high (in this case, VQ-MAE-S-20), there is no further improvement, and performance actually decreases (-2.4% accuracy) compared to VQ-MAE-S-16. Impact of the continuous embedding token size The impact of the continuous embedding token size on the SER performance on the RAVDESS-Speech dataset is shown in Table  3 , by varying the dimension (e) of the codes in the dictionary. Although we did not conduct an extensive study on this parameter, we tested three exponentially increasing token sizes (160, 320, 640). It is observed that using a size of 320 leads to an improvement in SER performance compared to sizes 160 and 640, with an increase of +5.8% accuracy and +1.3% accuracy, respectively. Impact of the discrete index token size Figure  3  illustrates the impact of the dimensions of the discrete index tokens (t and d) on SER performance on the RAVDESS-Speech dataset. d represents the discrete token size on the frequency axis, while t represents the discrete token size on the time axis. Our study reveals that both t and d significantly affect SER performance. Therefore, it is important to carefully select the values of (t, d). Based on our experiment, we suggest fixing them to (t = 10, d = 4).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "VQ-MAE-S is a novel approach that uses a pre-trained VQ-VAE model to adapt the MAE for speech representation learning, using discrete tokens obtained from the quantization step of the VQ-VAE. Our method outperforms other MAE methods based on spectrograms or mel-spectrograms for SER on several standard datasets. Several experiments also highlighted the impact of VQ-MAE-S hyperparameters on SER performance. For future work, we plan to investigate other masking strategies to further improve SER performance, and we aim to combine the MAE with contrastive methods to improve the learned audio representation and achieve even better SER performance.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The model takes as input",
      "page": 1
    },
    {
      "caption": "Figure 1: and discussed in the next subsections, the pro-",
      "page": 1
    },
    {
      "caption": "Figure 1: VQ-MAE-S model structure.",
      "page": 2
    },
    {
      "caption": "Figure 1: , we build discrete",
      "page": 2
    },
    {
      "caption": "Figure 2: Impact of the masking ratio for VQ-MAE-S-12.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the relationship be-",
      "page": 4
    },
    {
      "caption": "Figure 3: Impact of the discrete index token size (t, d) on SER.",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DATASET": "Metrics",
          "RAVDESS-Speech": "Accuracy\nf1-score",
          "RAVDESS-Song": "Accuracy\nf1-score",
          "IEMOCAP": "Accuracy\nf1-score",
          "EMODB": "Accuracy\nf1-score"
        },
        {
          "DATASET": "Self-attention audio [26]\nSSAST [5] (Patch-tf )\nMAE-AST [13] (Patch-tf )\nSpecMAE-12 (Patch-tf )",
          "RAVDESS-Speech": "58.3\n-\n-\n-\n-\n-\n52.2\n52.0",
          "RAVDESS-Song": "-\n-\n-\n-\n-\n-\n54.5\n53.9",
          "IEMOCAP": "-\n-\n54.3\n-\n58.6\n-\n46.7\n45.9",
          "EMODB": "-\n-\n-\n-\n-\n-\n57.2\n57.0"
        },
        {
          "DATASET": "VQ-MAE-S-12 (Patch-tf )\nVQ-MAE-S-12 (Patch-tf ) + Query2Emo\nVQ-MAE-S-12 (Frame)\nVQ-MAE-S-12 (Frame) + Query2Emo",
          "RAVDESS-Speech": "76.7\n75.9\n78.2\n77.5\n80.8\n80.5\n84.1\n84.4",
          "RAVDESS-Song": "84.0\n84.0\n83.7\n83.4\n84.2\n84.3\n85.8\n85.7",
          "IEMOCAP": "61.9\n61.2\n63.1\n62.5\n65.2\n64.9\n66.4\n65.8",
          "EMODB": "85.7\n85.8\n88.4\n88.3\n87.0\n87.0\n90.2\n89.1"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "5",
      "title": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "6",
      "title": "Ssast: Self-supervised audio spectrogram transformer",
      "authors": [
        "Yuan Gong",
        "-I Cheng",
        "Yu-An Lai",
        "James Chung",
        "Glass"
      ],
      "year": "2022",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "8",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "Shuo Liu",
        "Adria Mallol-Ragolta",
        "Emilia Parada-Cabaleiro",
        "Kun Qian",
        "Xin Jing",
        "Alexander Kathan",
        "Bin Hu",
        "Bjoern Schuller"
      ],
      "year": "2022",
      "venue": "Audio self-supervised learning: A survey"
    },
    {
      "citation_id": "9",
      "title": "A survey on masked autoencoder for self-supervised learning in vision and beyond",
      "authors": [
        "Chaoning Zhang",
        "Chenshuang Zhang",
        "Junha Song",
        "John Seon Keun",
        "Kang Yi",
        "In Zhang",
        "So Kweon"
      ],
      "year": "2022",
      "venue": "A survey on masked autoencoder for self-supervised learning in vision and beyond",
      "arxiv": "arXiv:2208.00173"
    },
    {
      "citation_id": "10",
      "title": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "Manon Macary",
        "Marie Tahon",
        "Yannick Estève",
        "Anthony Rousseau"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "13",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "14",
      "title": "Mae-ast: Masked autoencoding audio spectrogram transformer",
      "authors": [
        "Alan Baade",
        "Puyuan Peng",
        "David Harwath"
      ],
      "year": "2022",
      "venue": "Mae-ast: Masked autoencoding audio spectrogram transformer",
      "arxiv": "arXiv:2203.16691"
    },
    {
      "citation_id": "15",
      "title": "Masked autoencoders that listen",
      "authors": [
        "Hu Xu",
        "Juncheng Li",
        "Alexei Baevski",
        "Michael Auli",
        "Wojciech Galuba",
        "Florian Metze",
        "Christoph Feichtenhofer"
      ],
      "year": "2022",
      "venue": "Masked autoencoders that listen",
      "arxiv": "arXiv:2207.06405"
    },
    {
      "citation_id": "16",
      "title": "Neural discrete representation learning",
      "authors": [
        "Aaron Van Den",
        "Oriol Oord",
        "Vinyals"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Chung",
        "Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "18",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Vector-quantized image modeling with improved vqgan",
      "authors": [
        "Jiahui Yu",
        "Xin Li",
        "Jing Yu Koh",
        "Han Zhang",
        "Ruoming Pang",
        "James Qin",
        "Alexander Ku",
        "Yuanzhong Xu",
        "Jason Baldridge",
        "Yonghui Wu"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "20",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "21",
      "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "authors": [
        "Priya Goyal",
        "Piotr Dollár",
        "Ross Girshick",
        "Pieter Noordhuis",
        "Lukasz Wesolowski",
        "Aapo Kyrola",
        "Andrew Tulloch",
        "Yangqing Jia",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
      "arxiv": "arXiv:1706.02677"
    },
    {
      "citation_id": "22",
      "title": "Query2label: A simple transformer way to multi-label classification",
      "authors": [
        "Shilong Liu",
        "Lei Zhang",
        "Xiao Yang",
        "Hang Su",
        "Jun Zhu"
      ],
      "year": "2021",
      "venue": "Query2label: A simple transformer way to multi-label classification",
      "arxiv": "arXiv:2107.10834"
    },
    {
      "citation_id": "23",
      "title": "Asymmetric loss for multi-label classification",
      "authors": [
        "Tal Ridnik",
        "Emanuel Ben-Baruch",
        "Nadav Zamir",
        "Asaf Noy",
        "Itamar Friedman",
        "Matan Protter",
        "Lihi Zelnik-Manor"
      ],
      "year": "2021",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "24",
      "title": "The ryerson audiovisual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "International Speech Communication Association (Interspeech)"
    },
    {
      "citation_id": "27",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "Kateryna Chumachenko",
        "Alexandros Iosifidis",
        "Moncef Gabbouj"
      ],
      "year": "2022",
      "venue": "International Conference on Pattern Recognition (ICPR)"
    }
  ]
}