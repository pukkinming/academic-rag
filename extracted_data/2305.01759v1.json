{
  "paper_id": "2305.01759v1",
  "title": "Evaluation Of Speaker Anonymization On Emotional Speech",
  "published": "2023-04-15T20:50:29Z",
  "authors": [
    "Hubert Nourtel",
    "Pierre Champion",
    "Denis Jouvet",
    "Anthony Larcher",
    "Marie Tahon"
  ],
  "keywords": [
    "Speaker Anonymization",
    "Voice Privacy",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech data carries a range of personal information, such as the speaker's identity and emotional state. These attributes can be used for malicious purposes. With the development of virtual assistants, a new generation of privacy threats has emerged. Current studies have addressed the topic of preserving speech privacy. One of them, the VoicePrivacy initiative aims to promote the development of privacy preservation tools for speech technology. The task selected for the VoicePrivacy 2020 Challenge (VPC) is about speaker anonymization. The goal is to hide the source speaker's identity while preserving the linguistic information. The baseline of the VPC makes use of a voice conversion. This paper studies the impact of the speaker anonymization baseline system of the VPC on emotional information present in speech utterances. Evaluation is performed following the VPC rules regarding the attackers' knowledge about the anonymization system. Our results show that the VPC baseline system does not suppress speakers' emotions against informed attackers. When comparing anonymized speech to original speech, the emotion recognition performance is degraded by 15% relative to IEMOCAP data, similar to the degradation observed for automatic speech recognition used to evaluate the preservation of the linguistic information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Voice-controlled applications, such as smart speakers, have become widely popular. Large amount of data is required to train such applications. This motivates service providers to collect, process, and store personal data in centralized servers. Voice is one of the most sensitive modalities as it encapsulates many discernible attributes of a speaker such as age, gender, health, personality traits, socioeconomic status, geographical origin, biometric identity, moods, and emotions  [1, 2] . Given that speech data falls under the category of personal data  [3] , speech privacy-preserving solutions are becoming increasingly important. Additionally, recent regulations, e.g., the General Data Protection Regulation (GDPR)  [4]  in the European Union, emphasize on privacy preservation and protection of personal data. The research reported in this article has been done using the Voice privacy Challenge (VPC) framework  [5]  which is one of the first attempts of the speech community to evaluate research on this topic by producing dedicated protocols, metrics, datasets, and baselines.\n\nThe goal of the VPC system is to anonymize the speaker. This task is performed to suppress the personally identifiable paralinguistic information from a speaker's speech utterance while maintaining the linguistic content. The VPC baseline system uses a speaker anonymization approach  [6]  based on x-vectors and voice conversion. The quality of anonymization in the VPC is assessed using a speaker verification system, which evaluates the speaker concealing capability (privacy metric) and using an automatic speech recognition system to evaluate the preservation and intelligibility of the linguistic content (utility metric)  [5] . In this work, we investigate the extent to which an utterance's emotional content can be retrieved after anonymization.\n\nSpeaker recognition and voice privacy usually focus on socalled \"neutral\" speech. However, in spontaneous expressive speech, the audio signal carries speaker information, linguistic content and emotional cues. The anonymization process can be altered by emotional speech, for which the speech signal strongly differs from the \"neutral\" speech. Human emotion is usually described in psychological theories using diverse and complementary theories  [7, 8] . For a long time, the collection of emotional data mainly focused on acted and semantically controlled data from a few speakers  [9] . However, the actual trend is to capture the diversity of humans expressively in real-life conditions in order to model social aspects or induced interactions such as laughter  [10]  or disfluencies  [11] . The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [12]  is in-between acted and spontaneous speech and has the advantage of being used as a benchmark in the community.\n\nLinguistic cues mainly rely on the words pronounced by the speaker, while paralinguistic cues are directly related to the acoustic content of the speech signal. More precisely, prosodic features such as the fundamental frequency (F0), intensity and rhythm, are often considered as the most important cues in the field of speech emotion recognition (SER). Although most SER systems intend to capture prosody in input, another option is to extract Mel frequency cepstral coefficients (MFCCs)  [13]  or even spectrograms  [14] . The HUMAINE association also took an inventory of acoustic features in the CEICES initiative  [15]  which conducts to a set of a hundred descriptors selected over several corpora with various techniques  [16] . These features have the advantage of being easily interpretable. However, their extraction in degraded signals is error-prone. The use of input pre-trained features, i.e., embeddings extracted with neural models trained for speech processing tasks different from SER, are currently extensively used. The advantage of such an approach is to benefit from a large amount of data from a different task such as automatic speech recognition (ASR)  [17]  or speaker recognition  [18] .\n\nRemarkably few works have handled the problem of privacy preservation in the context of emotional speech. In  [19] , the authors proposed distance-preserving hashing techniques and homomorphic encryption to protect sensitive data such as emotions. Generative adversarial networks have been used as an intermediate layer between users and cloud services to sanitize the input speech  [20] . We aim to investigate how applying an anonymization process on emotional data, which is supposed to hide the speaker identity, impacts SER performance.\n\nThe paper is organized as follows. Section 2 presents the anonymization framework. It recalls the VPC baseline system, details F0 transformation enhancements, and presents the attack scenarios. Section 3 details the experiments conducted and the evaluation protocol with respect to the emotions and discusses the results. A conclusion ends the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Anonymization Framework",
      "text": "This section presents the speaker anonymization baseline system of the Voice Privacy Challenge and the attack scenarios. The baseline system introduced in [6] aims at separating speaker identity and linguistic content from an input speech utterance. Assuming that those features are disentangled, an anonymized speech waveform is generated by altering only the features that encode the speaker's identity. The anonymization system depicted in Figure  1  can be decomposed into three groups of modules. Modules from the group A extract different features from the source signal: the fundamental frequency (F0), the phonetic features encoding articulation of speech sounds and the speaker's x-vector. The module B derives a new pseudo-speaker target x-vector identity. The x-vector from each source input speaker is compared to a pool of external x-vectors to select the 200 furthest x-vectors; 100 of them are randomly selected and averaged to create an anonymized pseudo-speaker x-vector identity. Finally, the module C synthesizes a speech waveform from the target x-vector together with the original phonetic features and F0. Speaker anonymization is achieved through the selection of the pseudo-speaker target x-vector. The triphone extractor has been trained on the trainclean-100 and train-other-500 subsets of LibriSpeech. The xvector extractor has been trained on VoxCeleb-1,2. The speech synthesis system has been trained on the train-clean-100 subset of LibriTTS. Finally, the train-other-500 subset of LibriTTS has been used to create a pool of x-vector and F0 statistics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Vpc Speaker Anonymization System",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The F0 Transformations",
      "text": "In the original VPC anonymization system, the F0 values extracted from the source speech are directly used (unchanged) by the speech synthesizer, even though a different pseudo-speaker target x-vector is selected. Multiple studies have investigated F0 conditioned voice conversion  [21, 22, 23, 24, 25] . They conclude that modifying the F0 improves the quality of the converted voice. Motivated by those results, and also by the fact that emotions undoubtedly affect intonation, we propose to modify the F0 values of a source utterance from a given speaker (cf. module D in Figure  1 ) by using a linear transformation and a random warping.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "F0 Linear Transformation",
      "text": "In this method  [21, 22] , to feed the synthesizer with F0 values close to the pseudo-speaker selected, the F0 features of the source speaker are transformed using a linear transformation:\n\nwhere xt represents the F0 of the source speaker at frame t, µx and σx represent the mean and standard deviation of the logscaled F0 for the source speaker computed on all his/her utterances. µy and σy represent the mean and standard deviation of the log-scaled F0 for the pseudo-speaker. The linear transformation and statistical calculation are only performed on voiced frames. The mean and standard deviation for the target pseudospeaker is calculated by averaging the F0 of voiced frames from the 100 speakers selected to derive the pseudo-speaker x-vector.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "F0 Random Warping",
      "text": "In this method  [25, 26] , the contour of the F0 values are randomly modified to increase or decrease the range of the F0 variation using a warping factor:\n\nwhere α is sampled from a uniform distribution between 0.8 and 1.2, xt represents the F0 value at frame t and µx represents the mean F0 of the utterance to transform. The α warping factor is randomly sampled for each utterance. This F0 random warping is applied after the F0 linear transformation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Vpc Attack Scenarios",
      "text": "In the Voice Privacy Challenge, multiple sets of tests were performed depending on the attacker's knowledge of the anonymization algorithm. In this work, we focus on the Ignorant, and the Informed attacker scenarios  [27, 28] . In the Ignorant scenario, the attacker is unaware that speech is transformed. Thus, privacy measurement is assessed using models trained on original, non-anonymized data, while the evaluation is performed using anonymized data. This mismatch leads to the measurement of a rather good anonymization performance. On the opposite, the Informed attacker scenario is entirely aware of the anonymization algorithm. Such attackers are able to anonymize a training dataset in the same manner as the service provider. This anonymized dataset is later used to train the evaluation model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "The global aim here is to assess the emotion recognition performance once a speaker anonymization system has transformed the voices. Thus, the following sections present the emotional dataset, the evaluation protocol (based on the VPC attack scenarios), and the evaluation results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "The IEMOCAP dataset  [12]  is an emotional dataset used for experiment purposes such as emotion recognition and speech  recognition. It is composed of 12h of audio-visual data. Improvised and scripted dialogues between 10 female and male actors in the English language were recorded. Directional microphones have been used to capture each speaker's speech. It implies that in audio files, the two speakers can appear simultaneously (overlap). In the case of overlapping speech, the closest speaker to the microphone is considered dominant, and its speech will only be transcribed in the reference transcriptions. Because of directional microphones used, the level of the overlapping voice is much lower than the level of the \"dominant\" speaker voice.\n\nThe data is segmented by (dominant) speaker turns. Each turn has been annotated with emotion categories by six human annotators. Only the recordings that had the majority of annotators agreed on were used. Figure  2  shows the distribution of emotional labels in the dataset. Following previous works  [18, 29] , we consider only five emotions: neutral, frustration, sadness, anger, and happiness. Happiness combines the original annotations of happiness and excitement to balance the number of utterances in each emotion class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "In this paper, we focus on evaluating emotional information present in the speech signal, both in the original utterances and in the corresponding anonymized utterances. These evaluations are carried out using a speech emotion recognition system. The emotion recognition system used is based on a Support Vector Machine (SVM) model. In the SER literature, SVM with the radial basis function non linear kernel is widely used  [30, 31]  as baseline. As input features, the eGeMAPS features are used as they provide a minimalist yet efficient representa-tion of emotion  [16] . We also experimented MFCCs as input, and the results are similar. Following the VPC attack scenarios, the emotion recognition is evaluated using the Ignorant and the Informed attacker scenarios. In the Ignorant scenario, the SVM model used to evaluate the emotion information is trained on non-anonymized original speech data. For the Informed scenario, the SVM is trained on anonymized speech data. The standard Unweighted Average Recall (UAR) metric score (defined in Equation  3 ) is used to measure the emotion recognition performance. High UAR values means good emotion recognition. Regardless of the attack scenario, and to accommodate for the small dataset, the training and evaluation are performed using leave-one-session-out cross-validation protocol. The global performance is obtained by computing the UAR globally on the five test folds.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Uar =",
      "text": "Recall per class # class\n\nUtility evaluation, which assesses the preservation and intelligibility of the linguistic content, is performed using two Automatic Speech Recognition (ASR) systems provided by the VPC organizers. Results are reported with the Word Error Rate (WER). The lower the WER is, the more intelligible the speech is. Evaluation results are obtained using an ASR system trained on original non-anonymized LibriSpeech train-clean-360 data and a second one trained on the corresponding anonymized data. Retraining the ASR system on anonymized speech significantly decreases the WER when decoding anonymized speech data in comparison to the case when the ASR model is trained on the original data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "This section presents the experimental results for the evaluation protocol detailed in Section 3.2.\n\nTable  1  shows WER results on the IEMOCAP dataset, both for the original speech data, and for the anonymized speech data. Original speech is evaluated using the ASR model trained on original speech, while anonymized speech is evaluated with the model retrained on anonymized speech. The corresponding LibriSpeech scores from the VPC post-evaluation analysis  [32]  are presented for comparison purposes. In the IEMOCAP dataset, from original to anonymized, the utility score drops from 34.62 WER % to 38.97 WER % , which represents a relative degradation of 13%. Similar behavior is observed on the LibriSpeech dataset, meaning the VPC baseline anonymization system performs the transformation properly on emotional speech. The high WER on IEMOCAP can be explained by the presence of overlaps, and non-neutral speech, which is not  present in the LibriSpeech data. Following the attack scenarios defined by the VPC (see Section 2.3), the emotion recognition scores under the Ignorant and Informed attackers are summarized in Table  2 . For an Ignorant attacker, the UAR is 21.97%, which is nearly equal to random guessing. For an Informed attacker, the UAR is 37.92%, which is a 15% degradation compared to the UAR measured on original speech. The emotion recognition performance in terms of UAR seems to be impacted in the same manner as the utility performance (measured by WER).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Prosodic Parameters",
      "text": "If one wants to hide emotional information in the anonymized speech, modification of the prosodic parameters (i.e., fundamental frequency, intensity, and rhythm) are needed. Note that F0 mean, variability, and range are included in the eGeMAPS  [16]  input features used in our experimental setup. As F0 was available in the anonymization baseline system, we carried out some experiments involving random modifications of the F0 values (as described in Section 2.2.2).\n\nResults are shown in Table  2  together with the 95% confidence interval. We can see that, regardless of the attacker scenario, applying or not the F0 linear transformation alone, or the F0 linear transformation followed by a F0 random warping, leads to very similar results in terms of emotion recognition scores (UAR). In the Informed attacker scenario, where the SVM classifier is trained on anonymized speech, applying the F0 linear transformation does not modify the emotion recognition performance. In the future, we will investigate other modifications of the F0 curves in the anonymization process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have evaluated the application of the Voice Privacy baseline system on emotional speech. Concerning the utility metric, based on automatic speech recognition performance, we observed a 13% degradation of the Word Error Rate (WER) on the IEMOCAP anonymized data compared to the WER measured on original speech. This degradation is similar to the one reported on the LibriSpeech data. However, the WER is much higher on the emotional data (IEMOCAP), also impacted by the presence of overlapping speech, than on the clean neutral data (LibriSpeech).\n\nFor what concerns the emotion information carried by the speech signal, we have measured it through the standard Unweighted Average Recall (UAR) metric. We have observed a 15% degradation of the UAR when measured on the anonymized data compared to its measure on original data. The degradation observed for emotion recognition is similar to the degradation observed on the Word Error Rate (that measures the utility), which is fine if one considers the emotion a valuable information to be kept in the anonymized speech signal. However, one can also consider emotion as personal information that the anonymization system should remove. The preliminary experiments reported in this paper regarding simple random modifications of the F0 values show that such simple modifications are not enough to hide emotional information. Hence, further research will investigate other modifications of the F0 values and modifications of the duration and energy, which are other prosodic parameters that carry emotion information.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Voice Privacy speaker anonymization pipeline.",
      "page": 2
    },
    {
      "caption": "Figure 1: can be decomposed into three",
      "page": 2
    },
    {
      "caption": "Figure 1: ) by using a linear transformation and",
      "page": 2
    },
    {
      "caption": "Figure 2: Original distribution of emotion categories in the",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the distribution",
      "page": 3
    },
    {
      "caption": "Figure 3: WER performance on IEMOCAP with respect to the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: together with the 95% con- Est. Experiments presented in this paper were carried out",
      "data": [
        {
          "Column_1": "Original data - Original ASR model\nAnonymised data - Original ASR model",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Preserving privacy in speaker and speech characterisation",
      "authors": [
        "A Nautsch",
        "A Jiménez",
        "A Treiber",
        "J Kolberg",
        "C Jasserand",
        "E Kindt",
        "H Delgado",
        "M Todisco",
        "M Hmani",
        "A Mtibaa",
        "M Abdelraheem",
        "A Abad",
        "F Teixeira",
        "D Matrouf",
        "M Gomez-Barrero",
        "D Petrovska-Delacrétaz",
        "G Chollet",
        "N Evans",
        "C Busch"
      ],
      "year": "2019",
      "venue": "Comput. Speech Lang"
    },
    {
      "citation_id": "3",
      "title": "Privacy implications of voice and speech analysis -information disclosure by inference",
      "authors": [
        "J Kröger",
        "-M Lutz",
        "P Raschke"
      ],
      "year": "2019",
      "venue": "Privacy and Identity Management"
    },
    {
      "citation_id": "4",
      "title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding",
      "authors": [
        "A Nautsch",
        "C Jasserand",
        "E Kindt",
        "M Todisco",
        "I Trancoso",
        "N Evans"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC",
      "authors": [
        "E Parliament",
        "Council"
      ],
      "year": "2016",
      "venue": "General Data Protection Regulation"
    },
    {
      "citation_id": "6",
      "title": "Introducing the VoicePrivacy Initiative",
      "authors": [
        "N Tomashenko",
        "B Srivastava",
        "X Wang",
        "E Vincent",
        "A Nautsch",
        "J Yamagishi",
        "N Evans",
        "J Patino",
        "J.-F Bonastre",
        "P.-G Noé",
        "M Todisco"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Speaker Anonymization Using X-vector and Neural Waveform Models",
      "authors": [
        "F Fang",
        "X Wang",
        "J Yamagishi",
        "I Echizen",
        "M Todisco",
        "N Evans",
        "J.-F Bonastre"
      ],
      "year": "2019",
      "venue": "10th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "8",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "What are emotions? and how can they be measured?",
      "authors": [
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Social science information"
    },
    {
      "citation_id": "10",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Multimodal data collection of human-robot humorous interactions in the joker project",
      "authors": [
        "L Devillers",
        "S Rosset",
        "G Duplessis",
        "M Sehili",
        "L Béchade",
        "A Delaborde",
        "C Gossart",
        "V Letard",
        "F Yang",
        "Y Yemez",
        "B Turker",
        "M Sezgin",
        "K Haddad",
        "S Dupont",
        "D Luzzati",
        "Y Esteve",
        "E Gilmartin",
        "N Campbell"
      ],
      "year": "2015",
      "venue": "Proc. of International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "12",
      "title": "Capturing chat: Annotation and tools for multiparty casual conversation",
      "authors": [
        "E Gilmartin",
        "N Campbell"
      ],
      "year": "2016",
      "venue": "Proc. of the International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "13",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "14",
      "title": "Multi-corpus Experiment on Continuous Speech Emotion Recognition: Convolution or Recurrence?\" in 22nd International Conference on Speech and Computer SPECOM",
      "authors": [
        "M Macary",
        "M Lebourdais",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "Multi-corpus Experiment on Continuous Speech Emotion Recognition: Convolution or Recurrence?\" in 22nd International Conference on Speech and Computer SPECOM"
    },
    {
      "citation_id": "15",
      "title": "Towards Discriminative Representations and Unbiased Predictions: Class-Specific Angular Softmax for Speech Emotion Recognition",
      "authors": [
        "Z Li",
        "L He",
        "J Li",
        "L Wang",
        "W.-Q Zhang"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "CEICES : Combining efforts for improving automatic classification of emotional user states: a forced cooperation initiative",
      "authors": [
        "A Batliner",
        "S Steidl",
        "B Schuller",
        "D Seppi",
        "K Laskowski",
        "T Vogt"
      ],
      "year": "2006",
      "venue": "Language and Technologies Conference"
    },
    {
      "citation_id": "17",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing"
    },
    {
      "citation_id": "18",
      "title": "Speech Representation Learning for Emotion Recognition Using End-to-End ASR with Factorized Adaptation",
      "authors": [
        "S.-L Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Xvectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Exploring hashing and cryptonet based approaches for privacy-preserving speech emotion recognition",
      "authors": [
        "M Dias",
        "A Abad",
        "I Trancoso"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Emotionless: Privacypreserving speech analysis for voice assistants",
      "authors": [
        "A Ranya",
        "H Hamed",
        "B David"
      ],
      "year": "2019",
      "venue": "Privacy Preserving in Machine Learning (CCS19) Workshop"
    },
    {
      "citation_id": "22",
      "title": "Convolutional neural network based speaker de-identification",
      "authors": [
        "F Bahmaninezhad",
        "C Zhang",
        "J Hansen"
      ],
      "year": "2018",
      "venue": "Convolutional neural network based speaker de-identification"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised representation disentanglement using cross domain features and adversarial learning in variational autoencoder based voice conversion",
      "authors": [
        "W.-C Huang",
        "H Luo",
        "H.-T Hwang",
        "C.-C Lo",
        "Y.-H Peng",
        "Y Tsao",
        "H.-M Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "24",
      "title": "F0consistent many-to-many non-parallel voice conversion via conditional autoencoder",
      "authors": [
        "K Qian",
        "Z Jin",
        "M Hasegawa-Johnson",
        "G Mysore"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Individualitypreserving spectrum modification for articulation disorders using phone selective synthesis",
      "authors": [
        "R Ueda",
        "R Aihara",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Speaker-specific pitch contour modeling and modification",
      "authors": [
        "D Chappell",
        "J Hansen"
      ],
      "year": "1998",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "The frequency range of the voice fundamental in the speech of male and female adults",
      "authors": [
        "H Traunmüller",
        "A Eriksson"
      ],
      "year": "1993",
      "venue": "The frequency range of the voice fundamental in the speech of male and female adults"
    },
    {
      "citation_id": "28",
      "title": "Evaluating voice conversion-based privacy protection against informed attackers",
      "authors": [
        "B Srivastava",
        "N Vauquier",
        "M Sahidullah",
        "A Bellet",
        "M Tommasi",
        "E Vincent"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Privacy and utility of x-vector based speaker anonymization",
      "authors": [
        "B Srivastava",
        "M Maouche",
        "M Sahidullah",
        "E Vincent",
        "A Bellet"
      ],
      "year": "2021",
      "venue": "Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "32",
      "title": "Towards a Small Set of Robust Acoustic Features for Emotion Recognition: Challenges",
      "authors": [
        "M Tahon",
        "L Devillers"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Post-evaluation analysis for the voice privacy 2020 challenge: Using anonymized speech data to train attack models and asr",
      "authors": [
        "N Tomashenko",
        "B Srivastava",
        "X Wang",
        "E Vincent",
        "A Nautsch",
        "J Yamagishi",
        "N Evans",
        "J Patino",
        "J.-F Bonastre",
        "P.-G Noé",
        "M Todisco"
      ],
      "year": "2020",
      "venue": "Post-evaluation analysis for the voice privacy 2020 challenge: Using anonymized speech data to train attack models and asr"
    }
  ]
}