{
  "paper_id": "2201.09933v2",
  "title": "Do Smart Glasses Dream Of Sentimental Visions? Deep Emotionship Analysis For Eyewear Devices",
  "published": "2022-01-24T19:52:26Z",
  "authors": [
    "Yingying Zhao",
    "Yuhu Chang",
    "Yutian Lu",
    "Yujiang Wang",
    "Mingzhi Dong",
    "Qin Lv",
    "Robert P. Dick",
    "Fan Yang",
    "Tun Lu",
    "Ning Gu",
    "Li Shang"
  ],
  "keywords": [
    "Smart Eyewear System",
    "Emotionship",
    "Emotion Recognition",
    "Sentiment Analysis",
    "Image Captioning",
    "Visual Question Answering"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in smart eyewear devices is highly valuable but challenging. One key limitation of previous works is that the expression-related information like facial or eye images is considered as the only emotional evidence. However, emotional status is not isolated; it is tightly associated with people's visual perceptions, especially those sentimental ones. However, little work has examined such associations to better illustrate the cause of different emotions. In this paper, we study the emotionship analysis problem in eyewear systems, an ambitious task that requires not only classifying the user's emotions but also semantically understanding the potential cause of such emotions. To this end, we devise EMOShip, a deep-learning-based eyewear system that can automatically detect the wearer's emotional status and simultaneously analyze its associations with semantic-level visual perceptions. Experimental studies with 20 participants demonstrate that, thanks to the emotionship awareness, EMOShip not only achieves superior emotion recognition accuracy over existing methods (80.2% vs. 69.4%), but also provides a valuable understanding of the cause of emotions. Pilot studies with 20 participants further motivate the potential use of EMOShip to empower emotion-aware applications, such as emotionship self-reflection and emotionship life-logging. CCS Concepts: â€¢ Human-centered computing â†’ Mobile devices.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Research in social and psychology science indicates that our emotional state can considerably affect different aspects of our daily life, including our thoughts and behaviors  [13] , decision making  [49] , cognitive focuses  [18] , performance on assessments  [46] , physical health  [17] , and mental well-beings  [57] . Given the significant impacts of emotions, emotion recognition is one of the most crucial research topics in affective computing  [45] , and it can be applied to a wide range of human-computer interaction (HCI) scenarios to improve user experience. Intelligent eyewear systems are especially well suited to carry out and benefit from emotion recognition.\n\nA common goal of smart eyewear devices is to deliver intelligent services with personalized experiences. This requires understanding the users, especially their affective status. As indicated by previous studies  [4, [13] [14] [15] 67] , the ability to recognize emotion can greatly enhance user experience in various HCI scenarios. More importantly, an emotion-sensitive wearable front-end would enable a variety of personalized back-end applications, such as emotional self-reflection  [18, 23] , emotional life-logging  [6] , emotional retrieving and classification  [64] , and mood tracking  [56] .\n\nRecognizing emotions using smart eyewear devices is challenging. The majority of state-of-the-art emotion recognition techniques  [19, 30, 32, 34, 35]  use deep learning models to classify expressions from full facial images. However, it is typically difficult to capture the entire face using sensors that can economically be integrated into current eyewear devices. This mismatch between economical sensors and analysis techniques hinders the practical application of existing emotion recognition methods in eyewear.\n\nTo address this challenging problem, previous works  [21, 41, 51]  adopted engineering-based approaches to extract hand-crafted features from eye regions instead of the whole facial images to compute the affective status. With the embedding of eye-tracking cameras in commercial eyewear devices, recent eyewear systems developed convolutional neural networks (CNN) to extract deep affective features from eye-camera-captured images (typically eye regions) for head-mounted virtual reality (VR) glasses  [27]  and smart glasses  [61] . Besides Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices â€¢ 38:3 the limited recognition accuracy, those prior works predict human emotions based on the expression information of eyes solely and exclusively, ignoring the subtle yet crucial associations between people's emotional status and visual perceptions. In fact, the hints to the user's emotional state can be discovered in both expressions and visual experiences. Learning additional sentimental clues in the latter will inevitably benefit emotion recognition from the former.\n\nAs shown in studies of behaviors and neuroscience  [12, 43, 44] , the sentimental content in the scene is generally prioritized by people's visual attention over those emotionally neutral ones. These emotional-arousing contents are also known as emotional stimuli. For example, viewing a child playing with parents can lead to joyfulness, while we will feel sad if we perceive a crying woman who just lost her husband. In other words, emotion is not an isolated property; instead, it is tightly connected with the emotional stimuli of our visual attention. The arising of our emotions can be closely associated with the varying sentimental visions of our views, especially for eyewear devices with rapidly altering scenes.\n\nBased on such observations, we study the emotionship analysis problem in eyewear devices. The term emotionship conceptualizes the association of emotional status with the relevant hints in expression and visual attention. Through emotionship analysis, we aim to recognize emotions with better accuracy and understand the semantic cause  1  for such emotions through a quantitative measurement of the emotional contributions from vision attentions. In this paper, we adopt the widely accepted emotion categorization system that classifies emotions into six basic categories  [20]  plus the extra neutrality following  [61]  to define the status of emotions. It is important to note that a semantic-level understanding of visual experiences is necessary, since a certain attention region may consist of multiple objects and therefore can be ambiguous to establish the associations. In other words, we need to capture the semantic attributes of the visual perceptions. Compared with traditional emotion recognition techniques, the proposed emotionship analysis is arguably more ambitious and more difficult, as additional challenges arise from the semantic analysis of the human visual perception, its association with the emotional status, etc. However, a successful emotionship analysis framework will clearly lead to a truly personalized eyewear system that is capable of performing unseen and valuable emotion-aware downstream tasks.\n\nIn this work, we present such an emotionship-aware eyewear system for the first time. As shown in Fig.  1 , our eyewear system, called EMOShip, is equipped with cutting-edge deep learning techniques and is capable of recognizing the semantic attributes of the visual attentive regions, the expression-related information in eye images, and the emotional states based on the associations from both pieces of evidence. At the heart of EMOShip is EMOShip-Net, a deep neural network that is designed to address the new challenges in emotionship analysis. To extract the semantic attributes of visual perceptions, we combine gaze points from eye-tracking  [29]  with the visual features model VinVL  [66]  plus a vision-language (VL) fusion model OSCAR+  [66] . The sentimental clues in visual perceptions are synthesized with the expression-related information in eye images to predict emotion status more accurately and robustly. Visual perception's contributions to emotional states, which is subtle and challenging to measure, is quantitatively evaluated by a Squeeze-and-Excitation (SE) network  [28]  that fuses the scene and eye features. To evaluate the in-lab performance of EMOShip, we collect and construct a new dataset, named EMO-Film. With the availability of visual perceptions' semantic attributes, the emotional states and the emotional impacts of visual attentions, our smart glasses system EMOShip outperforms baseline methods on EMO-Film dataset in terms of emotion recognition accuracy, and more importantly, EMOShip provides a semantic understanding of the potential cause of such emotions. In-field pilot studies have been conducted to illustrate the effectiveness and superiority of this emotionship-aware eyewear systems, and demonstrate its potential  applications to a number of emotionship-relevant tasks such as emotionship self-reflection and emotionship life-logging.\n\nIn summary, this paper makes the following contributions.\n\n(1) This work designs a smart eyewear EMOShip to measure the relationship between the semantic attributes of visual attention and the emotional status of the wearer, and using this learned relationship increases the accuracy of emotional state recognition. (2) The proposed EMOShip is equipped with a deep neural network EMOShip-Net, which is designed to extract expression-related affective features and sentimental clues in visual attention. Most importantly, EMOShip-Net fuses them to achieve more accurate emotion recognition and quantify their emotionship associations. (3) On the self-collected EMO-Film dataset, EMOShip achieves approximately 10.8% higher emotion recognition accuracy than the baseline methods, and demonstrates its potential capability of providing valuable sentimental clues for emotionship understanding. (4) We perform in-field pilot studies on two inspiring down-stream applications -emotionship self-reflection and emotionship life-logging, to illustrate the potential use of EMOShip. With three-week studies on 20 participants, we have shown that EMOShip has achieved 82.8% precision in terms of emotional moment capturing. The questionnaire survey shows that 16 out of 20 users embrace the idea of emotionship selfreflection and admit its benefits, while 15 out of 20 users give positive feedback on emotion life-logging applications.\n\nThe rest of the paper is organized as follows. Section 2 surveys related works. Section 3 presents the proposed EMOShip algorithm design and system implementation. Section 4 presents the experimental results. Section 5 demonstrates the in-field pilot studies. We conclude the work in Section 6. This section surveys related work in the field of (1) emotion recognition, (2) image sentiment analysis, (3) visionlanguage models, and (4) gating mechanisms. We also highlight the key contributions of EMOShip compared to prior work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Ekman proposed a well-known and widely-adopted emotion categorization system that divided emotions into six basic categories: happiness, sadness, fear, anger, disgust, and surprise  [20] . And the seventh emotion is neutrality  [61] , which represents the absence of emotion. The seven basic emotions are widely accepted  [1, 61] , and this work also adopts this emotion categorization system.\n\nMost recent works involve deep models to classify the seven basic emotions from the whole facial images  [19, 30, 32, 34, 35] , as facial expressions are one of the most common channels for humans to express feelings  [22] . When it comes to smart eyewear devices, however, the whole facial images are not easy to obtain, and therefore alternatives should be established. Eye region image has been shown to contain sufficient expressionrelated information  [61] , and since it can be easily fetched through matured eye cameras, eye images and eye analysis techniques have become a promising choice for emotion recognition in eyewear systems.  Tarnowski et al.  proposed to utilize eye-tracking information, mainly regarding the eye movements and pupil diameters, for emotion recognition  [55] . Aracena et al. presented an emotion recognition method based only on pupil size and gaze position  [3] . Later, Wu et al. proposed a deep-learning-based network to extract emotional features from the single-eye-area images, and classified them into seven basic emotions  [61] .\n\nIn this work, we take one further step from the traditional emotion recognition and study the emotionship analysis, a task that not only requires to learn the emotional states but also should consider the impacts of sentimental visual perceptions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Image Sentiment Analysis",
      "text": "Different from emotion recognition based on facial expressions, visual sentiment analysis aims to predict the intended emotions from images. This work mainly investigates visual sentiment analysis based on categorical approaches that divide the intended emotions from images into six categories  [53] , which is usually consistent with the emotion categorization system  [20] .\n\nEarly sentiment prediction used hand-crafted features to recognize intended emotions. Those features included color variance, composition, and image semantics  [40] , etc. Recently, with the advancing of deep Convolutional Neural Networks (deep CNNs), numerous deep-learning-based sentiment prediction approaches have been proposed to extract deep features for more effective sentiment prediction  [53, 63] . Campos et al. conducted extensive experiments and compared the performance of several fine-tuned CNNs for visual sentiment prediction  [7] . Zhu et al. proposed a unified CNN-RNN model to predict image emotions based on both low-level and high-level features by considering the dependencies of the features  [70] . Rao et al. classified image emotions based on a proposed multi-level deep network that combined the local emotional information from emotional regions with global information from the whole image  [48] . Yang et al. proposed a weakly supervised coupled convolutional network to provide effective emotion recognition by utilizing the local information of images  [63] . Later, they extended the proposed weakly supervised detection framework through a more detailed analysis for visual sentiment prediction  [53] .\n\nWe acquire from those works the idea of image sentimental analysis and extract the sentimental features in scene images through a Vision-Language (VL) model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vision-Language Models",
      "text": "Vision-Language (VL) models are a relatively new field in computer vision, and they are designed for the VL tasks  [9, 33, 37, 54, 69] . VL models usually consist of two stages: 1. An object detection model is involved to predict the Region of Interests (RoIs) of each object and also to extract the feature embedding for each RoI, 2. a cross-modal fusion model to generate short descriptions of each RoI's semantic attributes. Therefore, a successful VL model will generate all RoIs of a scene image, the feature embedding for each RoI, and also the semantic attributes per RoI. VinVL model  [66]  improves the performance of the vision module to extract visual presentations at higher qualities, and employs OSCAR  [36]  which is based on transformer  [58]  to perform the cross-modal semantic attributes predictions. It is shown in  [66]  that the usage of VinVL features and training on multiple datasets can significantly improve the performance of the original OSCAR on a variety of downstream Natural Language Processing (NLP) tasks, and therefore the learned Vision-language fusion model is named as OSCAR+. VinVL model  [66]  has achieved the state-of-the-art performance in VL tasks, and the performance of its proposed OSCAR+ has also surpassed that of others on downstream NLP tasks.\n\nInspired by recent progress in VL models and the requirements of semantic understanding in emotionship analysis, we have adopted VinVL  [66]  and its proposed OSCAR+  [66]  in EMOShip. The benefits of using VinVL and OSCAR+ are threefold. First, the semantic attributes of RoIs can be predicted, which can be essential to achieve emotionship-awareness. Another advantage is that the semantic features of RoIs are also provided in VinVL, and these semantic features have encoded sufficient sentimental clues that can be fused together with eye-expression-related information to achieve more accurate emotion prediction. Last but not least, we are able to perform language analysis tasks like Question Answering (QA) through using OSCAR+, which allows our eyewear system to capture the summary tag of a visual region. To the best of our knowledge, we are the first to integrate VL and NLP model into an eyewear system to enable the awareness of semantic attributes.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Gating Mechanisms",
      "text": "Gating mechanism 2    [5, 38, 58, 59 ] is an approach of spending more resources on those more informative parts of the input data. Typically, for an input signal, the importance degree for each of its position is weighted through a gating model and the output will be a signal with properly enlarged or shrunk values. There are a variety of gating models like the transformer  [58]  and the non-local network  [59] , and they are widely utilized in different fields like lip-reading  [39]  and image captioning  [62] . Among those gating models, Squeeze-and-Excitation (SE) network  [28]  is most closely related to our smart glasses EMOShip. For a deep feature, SE network can learn the pattern of importance degree in a channel-wise manner, generating a scaling vector that adjusts each feature channel. In EMOShip, SE is employed when fusing the semantic features from VL models and eye features to predict the emotional state, and more importantly, to learn the emotional impacts from scene images.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "System Design",
      "text": "This section presents EMOShip system design. It first defines the emotionship analysis problem, highlights the corresponding challenges, and then presents EMOShip-Net, the proposed deep emotionship analysis network, and details EMOShip system software-hardware design and operation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Problem Definition",
      "text": "Emotion recognition methods for eyewear devices aim to identify the emotional state from expressions (typically using the eye images). There can be various criteria regarding the emotional state, and we adopt the widelyaccepted standard following the works of  [20, 61] . Specifically, the emotion is discretely classified into six basic Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices â€¢ 38:7 categories [20] -happiness, surprise, anger, fear, disgust and sadness. In addition, we employ neutrality to represent the absence of emotions as in  [61] . Let ğ‘’ ğ‘¡ âˆˆ {0, 1, 2, 3, 4, 5, 6} represent the emotional state at time step ğ‘¡ and let E ğ‘¡ âˆˆ R ğ» 1 Ã—ğ‘Š 1 Ã—3 be the eye images with height ğ» 1 and width ğ‘Š 1 . Recent smart eyewear devices  [27, 61]  utilized a deep network N ğ‘’ğ‘¦ğ‘’ to obtain emotional predictions from eye images, i.e., ğ‘’ ğ‘¡ = N ğ‘’ğ‘¦ğ‘’ (E ğ‘¡ ).\n\nIn this work, we aim to solve the emotionship analysis problem for eyewear devices, a task that is related to emotion recognition but is more sophisticated and ambitious. In particular, the emotional state is learned from both eye images and visual perceptions, and the impacts of visual perceptions on this emotional state, i.e., emotionship, should also be quantitatively evaluated. Since the visual attentive region usually covers multiple semantic objects, the semantic attributes of the visual perceptions should be distinguished to avoid confusion of those objects.\n\nLet\n\nrepresent the scene image with height ğ» 2 and width ğ‘Š 2 , the user's visual attentive region is the priority to determine. In other words, we need to know which part of the scene image is attended by the user, which is formally known as Region of Interest (RoI). We denote this RoI as r ğ‘¡ âˆˆ R 4 , and r ğ‘¡ can be described as a rectangular area (ğ‘¥ The awareness of emotional state ğ‘’ ğ‘¡ and the influential score ğ¼ğ‘† ğ‘¡ is not sufficient to fully reveal the emotionship, as we still need to understand the semantic attributes of visual attentions to unambiguously describe the potential cause for ğ‘’ ğ‘¡ . The semantic attribute is defined as a summary tag of the attentive region I ğ‘¡ ğ‘ğ‘¡ğ‘¡ , e.g., \"white, warm beaches\" if I ğ‘¡ ğ‘ğ‘¡ğ‘¡ depicts a white beach in summer. We denote this summary tag as s ğ‘¡ and it clarifies the semantic cause for ğ‘’ ğ‘¡ at an abstract level, which is typically overlooked in previous works.\n\nLet ES ğ‘¡ represent the emotionship and it can be formulated as\n\nDifferent from traditional emotion recognition that isolates user's emotional state from surroundings and only predicts ğ‘’ ğ‘¡ , our emotionship ES ğ‘¡ additionally encodes the potential causes for ğ‘’ ğ‘¡ , i.e., visual perceptions I ğ‘¡ ğ‘ğ‘¡ğ‘¡ with semantic attributes s ğ‘¡ , while the degrees of their emotional influences are also indicated by ğ¼ğ‘† ğ‘¡ . With the awareness of emotionship, eyewear devices can understand the semantic causes for emotions and also learn how visual experiences can affect emotions in a personalized manner, which is highly desirable and attractive in wearable systems. However, there are a number of challenges.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Challenges",
      "text": "To achieve emotionship analysis, the challenges are threefold. The first is how to appropriately discover the semantic attributes of visual attention. With the embedding of the forward-facing world camera, smart eyewear devices can already estimate the gaze points  [8]  using eye-tracking techniques. Gaze points can be a valuable guidance to track human attentions. However, knowing merely the gaze point is insufficient, as there can be multiple semantic objects near this point that can potentially lead to the current emotional state. To avoid ambiguity, we need to clearly identify the semantic meanings around the gaze point. In other words, the semantic summary tag s ğ‘¡ of the visual perceptions I ğ‘¡ ğ‘ğ‘¡ğ‘¡ is necessary, yet s ğ‘¡ can be  challenging to obtain, especially for eyewear devices. In this work, we take inspirations from recent progress in visual features models  [66]  to extract the tag s ğ‘¡ , as detailed in Section 3.3. After the visual attentive regions have been located with semantic understandings, another challenge is how to establish the associations of human visual attention with the emotional state. The reason for emotion alternations can be subtle and difficult to determine. It could be highly associated with sentimental visual perceptions, e.g., when a user observes that a child is playing with parents and then cheers up, we can reasonably assume that the happiness is caused by this scene. However, in cases that sentimentally-neutral visions are captured, a sudden change of emotional signals does not necessarily link with them. Therefore, it is crucial to correctly identify visual attentions' emotional contribution, i.e., to compute its influential score ğ¼ğ‘† ğ‘¡ . In our workflow, ğ¼ğ‘† ğ‘¡ is automatically and implicitly learned with deep models, which is further described in Section 3.3.\n\nThere is one more challenge when it comes to the prediction of emotional state ğ‘’ ğ‘¡ . Sentimental information in visual perceptions indeed provides insights on the potential cause of emotions, yet it is not reliable enough for recognizing emotion. Therefore, the utilization of expression-related information like eye-images in  [61]  is still indispensable. In other words, we infer the emotional state ğ‘’ ğ‘¡ from both shreds of evidence, i.e., the sentimental clues in visual perceptions and the expression-related information in eye images, leading to more robust emotion recognition performance. We have incorporated the Squeeze-and-Excitation network  [28]  to fuse them together, as described in Section 3.3.\n\nTo address the aforementioned challenges of emotionship analysis, we have devised a deep network named EMOShip-Net, the workflow of which is described in Section 3.3.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Emoship-Net Workflow",
      "text": "3.3.1 Overall Pipeline. Fig.  2  illustrates the workflow of EMOShip-Net. At time step ğ‘¡, the input of the network contains two video streams: the eye images E ğ‘¡ taken by an inward-facing eye camera, and the scene images I t recorded by another forward-facing world camera. The eye camera keeps tracking eye images E ğ‘¡ and monitors the rough emotional state, i.e., neutral or non-neutral. When a non-neutral emotion is spotted, the scene camera will be triggered to record scene I t . A vision-language (VL) model  [66]  is applied to extract all potential regions of interest (RoIs) with semantic tags in I t . The visual attentive region is determined from those RoIs based on the gaze point, and the summary tag for the selected area is obtained by a Question Answering (QA) process using the OSCAR+  [36]  vision-language fusion model. The features of the attentive regions, which are also provided by the VL model, are fused with the eye features using a Squeeze-and-Excitation (SE) network  [28]  to generate the final prediction on the emotional state. The scaling vector obtained after the SE network's excitation operation reveals a very important relationship, i.e., the emotional impact from visual attentions, or the influential score ğ¼ğ‘† ğ‘¡ as defined in Section 3.1.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Extracting Eye Features.",
      "text": "Eye images E ğ‘¡ contain the information regarding expressions. This work follows the EMO method  [61]  to extract expression-related features but makes necessary improvements to enhance the emotion recognition accuracy and suit our application scenarios. Specifically, EMO  [61]  consists of a feature extractor for eye images and a customized emotional classifier. Since emotion recognition in eye images is not the major pursuit of this work, we only adopt the former (feature extractor based on ResNet-18 backbone  [26]  which is denoted as\n\nbut replace the latter one (the customized classifier) with a binary classifier for neutral/non-neutral predictions. More importantly, we have appended pupil information to f ğ‘¡ ğ‘’ before feeding it into the binary classifier. This is inspired by  [3] , a work showing that statistical eye information such as pupil size can help to improve the emotion recognition accuracy. Denoting the pupil size information as ps ğ‘¡ âˆˆ R 2 , we treat ps ğ‘¡ as an expert information, and following  [60, 68] , we concatenate this expert information ps ğ‘¡ with f ğ‘¡ ğ‘’ , which can be written as\n\nwhere the square bracket indicates the channel-wise concatenations. Eye features f ğ‘¡ ğ‘’ğ‘¦ğ‘’ âˆˆ R 130 encode the expression-related information within eye regions and can be seen as an effective emotional indicator. Note that N ğ‘’ğ‘¦ğ‘’ will only be applied to eye images when a particular eye attention pattern  [8]  is spotted to save energy, which will be detailed in Section 3.4.2. The trigger of the world camera, on the other hand, depends on eye feature f ğ‘¡ ğ‘’ğ‘¦ğ‘’ .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "3.3.3",
      "text": "Triggering the World Camera. The high-resolution world camera is more energy-intensive than the lowresolution light-weight eye camera, thus it would be too energy costly to capture scene frames nonstop. Considering the energy limitations of wearable devices, we have designed a \"trigger\" mechanism for the world camera. The idea is to skip those emotional-neutral frames (there is no need to analyze the emotionship for neutral emotions) and focus on those frames with non-neural emotions. In particular, we design a binary classifier C ğ‘’ğ‘¦ğ‘’ to separate f ğ‘¡ ğ‘’ğ‘¦ğ‘’ into neutral or non-neutral emotion, respectively. If f ğ‘¡ ğ‘’ğ‘¦ğ‘’ is believed to fall into the neutral-emotional category, the world camera will be disabled to avoid unnecessary energy costs. Otherwise, it will be triggered to enable the following operations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Selecting Roi Candidates.",
      "text": "The triggered forward-facing world camera records the scene images I ğ‘¡ . Using the eye-tracking technique in  [29] , we first estimate from E ğ‘¡ the gaze point g ğ‘¡ = (ğ‘¥ ğ‘¡ ğ‘” , ğ‘¦ ğ‘¡ ğ‘” ) where ğ‘¥ ğ‘¡ ğ‘” and ğ‘¦ ğ‘¡ ğ‘” refer to the 2D coordinates of this gaze point with respect to scene image I ğ‘¡ . Since g ğ‘¡ is a 2D point, we still need to find the region of visual perceptions r ğ‘¡ = (ğ‘¥ ğ‘¡ ğ‘Ÿ , ğ‘¦ ğ‘¡ ğ‘Ÿ , ğ‘¤ ğ‘¡ ğ‘Ÿ , â„ ğ‘¡ ğ‘Ÿ ). In this work, we use the VinVL model  [66]  to generate all potential regions in I ğ‘¡ , and then we perform a filtering process to select certain RoI candidates for r ğ‘¡ from all those regions.\n\nIn particular, denote the VinVL model  [66]  as N ğ‘‰ ğ¿ . Given the scene image I ğ‘¡ , N ğ‘‰ ğ¿ is able to generate a total of ğ¾ potential regions {R ğ‘¡ 1 , R ğ‘¡ 2 , . . . , R ğ‘¡ ğ¾ } where R ğ‘¡ ğ‘– âˆˆ R 4 represents the ğ‘–-th candidate. Note that for a RoI R ğ‘¡ ğ‘– , its corresponding visual representation (or feature) f ğ‘¡ ğ‘… ğ‘– âˆˆ R 2048 and the semantic representation q ğ‘¡ ğ‘… ğ‘– (e.g., a tag) is already given by N ğ‘‰ ğ¿ . We have designed a filter process to select the ten most suitable regions out of all those ğ¾ regions based on the gaze point g ğ‘¡ . That is, for each candidate R ğ‘¡ ğ‘– , we compute the Euclidean distance from its central point to the gaze point g ğ‘¡ . Then we empirically select the top ten regions with the smallest distances, i.e., the ten regions that are closest to the gaze point. Denoted those regions as R ğ‘¡ ğ‘ = {R ğ‘¡ ğ‘1 , R ğ‘¡ ğ‘2 , ..., R ğ‘¡ ğ‘10 }, they are the most relevant RoIs with the visual attentive region. After this filtering process, there are still ten RoI candidates, and we need to determine a final visual attention region and also to generate its summary tag. To achieve this, we perform two Question Answering (QA) sessions.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Determining",
      "text": "Visual Attentive Region and Summary Tag. Recall that we have already selected ten candidates of regions R ğ‘¡ ğ‘ = {R ğ‘¡ ğ‘1 , R ğ‘¡ ğ‘2 , . . . , R ğ‘¡ ğ‘10 }, and for the ğ‘–-th region R ğ‘ğ‘– , its visual feature f ğ‘¡ ğ‘ğ‘– âˆˆ R 2048 and its semantic representation q ğ‘¡ ğ‘ğ‘– are already provided by N ğ‘‰ ğ¿ . To select the actual visual attentive region and to generate it summary tags, we perform a Visual Question Answering (VQA)  [25]  and an Image Captioning  [2] , based on the OSCAR+ vision-language fusion model  [66] . Specially, denote the visual features of selected ten regions as\n\n. . , f ğ‘¡ ğ‘10 }, denote the semantic attributes as q ğ‘¡ ğ‘ = {q ğ‘¡ ğ‘1 , q ğ‘¡ ğ‘2 , . . . , q ğ‘¡ ğ‘10 }, and denote the OSCAR+ model  [66]  as N ğ‘‚ğ‘† . The VQA session aims to determine the appropriate visual attentive region r ğ‘¡ . First we invoke N ğ‘‚ğ‘† to answer the question Q 1 \"What object makes people feel happy/surprised/sad/angry/feared/disgusted?\" by also inferring to f ğ‘¡ ğ‘ and q ğ‘¡ ğ‘ and obtain an answer a ğ‘¡ from N ğ‘‚ğ‘† , which written as\n\nThen among the ten attributes q ğ‘¡ ğ‘ , we find the one q ğ‘¡ ğ‘ ğ‘— whose word2vec embedding  [47]  is closest to that of answer a ğ‘¡ than all other attributes, and q ğ‘¡ ğ‘ ğ‘— 's corresponding region R ğ‘¡ ğ‘ ğ‘— is seen as the visual attentive region r ğ‘¡ , i.e. r ğ‘¡ = R ğ‘¡ ğ‘ ğ‘— . As for the Image Captioning (IC) session, the target is to generate an appropriate tag that summarize the semantic attributes of visual perception region. In this session, there is no question to answer, and N ğ‘‚ğ‘† only looks at the visual features f ğ‘¡ ğ‘ to generate the tags, which can be expressed as\n\nwhere s ğ‘¡ is the summary tag for visual attentive region.\n\n. This concatenated feature f ğ‘¡ ğ‘’ğ‘£ âˆˆ R 260 contains emotional evidences from both the eye and scene images, and it is fed into a Squeeze-and-Excitation (SE) network  [28]  N ğ‘†ğ¸ to obtain a scaling vector u ğ‘¡ âˆˆ R 260 , i.e. u ğ‘¡ = N ğ‘†ğ¸ (f ğ‘¡ ğ‘’ğ‘£ ). This scaling vector u ğ‘¡ is channel-wisely multiplied with the the concatenated features f ğ‘¡ ğ‘’ğ‘£ to obtain feature\n\nwhere * represents the channel-wise multiplication. Note that the scaling vector u ğ‘¡ is learned from the SE gating mechanisms and it reflects the importance degree of each channel in f ğ‘¡ ğ‘’ğ‘£ . The obtained feature f ğ‘¡ ğ‘’ğ‘£ğ‘ is then input into a soft-max classifier C ğ¸ğ‘€ğ‘‚ to generate the final emotion prediction ğ‘’ ğ‘¡ , i.e. ğ‘’ ğ‘¡ = C ğ¸ğ‘€ğ‘‚ (f ğ‘¡ ğ‘’ğ‘£ğ‘ ). 3.3.7 Computing the Influential Score. The Influential Score indicates the degree of emotional impacts from visual perceptions, and it can be computed from the scaling vector u ğ‘¡ learned from SE gating mechanism. Recall that u ğ‘¡ represents the importance degree of each channels in f ğ‘¡ ğ‘’ğ‘£ , while f ğ‘¡ ğ‘’ğ‘£ is concatenated from eye features f ğ‘¡ ğ‘’ğ‘¦ğ‘’ and visual perception's feature f ğ‘¡ ğ‘£ . We are therefore able to evaluate the importance degree of visual perception's feature f ğ‘¡ ğ‘£ in predicting emotional state, or the Influential Score IS, through the usage of u ğ‘¡ , which can be written as\n\nwhere u ğ‘¡ ğ‘– denotes the ğ‘–-th scalar of u ğ‘¡ , and we assume the first 130 scalars of u ğ‘¡ corresponds to channels of f ğ‘¡ ğ‘£ . Using the influential scores in Equation  4 , we can determine to which degree an emotional state was affected by the sentimental visions. For instance, if a really small ğ¼ğ‘† ğ‘¡ is computed, we could conclude that the current emotional status is not related to the observed visual perceptions. In contrast, a large ğ¼ğ‘† ğ‘¡ value implies that the current emotion is highly related to the attentive scene regions.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Emotional State In Video",
      "text": "Sequence. To maintain simplicity, we only illustrate how to predict emotion for a certain time step ğ‘¡ in the descriptions so far. However, emotions are temporally consistent processes instead of static ones, and therefore we also need to consider how to aggregate the emotion prediction of different time steps. For an emotion video clip of ğ‘‡ frames, assuming that we have already computed emotion prediction for each frame, i.e. {ğ‘’ 1 , ğ‘’ 2 , ...ğ‘’ ğ‘‡ }, we use the majority emotion class ğ‘’ ğ‘š as the emotion prediction of this sequence. Formally, the majority emotion class ğ‘’ ğ‘š can be computed as\n\nwhere ğ‘– âˆˆ {0, 1, 2, 3, 4, 5, 6}, 1 represents the indicator function, 1(ğ‘’ ğ‘— = ğ‘–) = 1 if ğ‘’ ğ‘— = ğ‘– and 1(ğ‘’ ğ‘— = ğ‘–) = 0 if ğ‘’ ğ‘— â‰  ğ‘–.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emoship System",
      "text": "3.4.1 Hardware Design. The EMOShip prototype is a smart eyewear system equipped with two cameras, including one outward-facing world camera and one customized inward-facing eye camera. The outward-facing world camera collects the visual content aligned with the wearer's field of view. We adopt Logitech B525 1080p HD Webcam (1280Ã—960@30fps). The inward-facing eye camera supports continuous eye tracking and eye-related expression feature capture. We adopt a camera module with GalaxyCore GC0308 sensor (320Ã—240@30fps), equipped with an IR LED light to illuminate the iris. This hardware module is inspired by the Pupil 3  , but we have re-implemented it accordingly to suit our needs. The EMOShip prototype is equipped with Qualcomm's wearable system-on-module solution, combining Qualcomm Snapdragon XR1 chipset with an eMCP(4GB LPDDR/64GB eMMC). Its typical power consumption is 1 W, which is suitable for battery-powered wearable design.\n\n3.4.2 Software Operation. In EMOShip, EMOShip-Net performs emotion recognition and emotionship analysis. Targeting energy-constrained wearable scenarios, EMOShip-Net is equipped with a carefully designed energyefficient workflow as follows.\n\nâ€¢ First, EMOShip-Net continuously senses the eye camera to perform eye tracking. To minimize the energy cost of eye tracking, EMOShip-Net uses a computationally efficient pupil detection and tracking method  [29]  to detect potential attention events. Following the work by Chang et al.  [8] , a potential attention event needs to satisfy two conditions simultaneously: (1) there is a temporal transition from saccade to smooth pursuit, which suggests a potential visual attention shift; and (2) the gaze follows a moving target or fixating on a stationary target. We modify the open-source Pupil software  4  to achieve more accurate eye movement pattern detection that can better satisfy the requirement of our system. Pupil software predicts two eye movements-fixation and non-fixation, based on the degree of visual angles  [29] . However, when 38:12 â€¢ Zhao et al.\n\nwe deployed it in our system, we needed more eye movements such as saccade and smooth pursuit to detect a potential visual attention event. To address this issue, we follow the method proposed in  [8]  and leverage the historical gaze trajectory profile to give a more accurate eye movement detection. The behind intuition is that possible smooth pursuit or fixation eye movements occur when the historical gaze points are located in a spatially close region for a while; otherwise, a saccade eye movement occurs. What we want is a stable detection of eye movements, and therefore we pay more attention to the recall of our method. The overall recall is 99.3%, which shows our eye movement detection method is working robustly and reliably. Also, the inference time of the eye-tracking method used in EMOShip-Net is 115.1 fps, or 8.7 ms/frame. â€¢ Once a potential attention event is detected by the computationally efficient eye-tracking method, EMOShip-Net takes the eye images as the input of the light-weight network N eye to extract eye-related expressionrelated information and performs neutral vs. non-neutral emotional state classification. N eye is computationally efficient, which only requires 20.3 ms to perform emotional state classification for each eye image frame.\n\nâ€¢ Only when a non-neutral emotional state is detected, EMOShip-Net turns on the high-resolution world camera to sense the scene content for semantic analysis. In other words, the high-resolution (more energy expensive) scene content capture and processing the pipeline remains off most of the time, avoiding unnecessary data sensing and processing, thereby effectively improving the overall system energy efficiency. â€¢ Finally, EMOShip-Net leverages a cloud infrastructure to perform computation-intensive semantic attribute feature extraction and eye-scene feature aggregation to support final emotion recognition and emotionship analysis, thus avoiding the energy consumption on the eyewear device side. The energy consumption of the EMOShip eyewear device is estimated as follows:\n\nwhere ğ‘‡ always-on is the overall operation time of the EMOShip eyewear device, ğ‘ƒ eye camera and ğ‘ƒ world camera is the power consumption of the eye camera and the world camera, respectively, ğ‘‡ N eye is the operation time of the light-weight eye analysis network N eye , and ğ‘‡ captured is the operation time of the high-resolution video recording when non-neutral emotional states are detected.  Fig.  3 (a) illustrates the run-time operation of EMOShip on the eyewear. Targeting the Qualcomm Snapdragon wearable platform, physical measurement shows that ğ‘ƒ eye camera = 0.07ğ‘Š , ğ‘ƒ eye tracking = 0.1ğ‘Š , ğ‘ƒ world camera = 1.3ğ‘Š , and ğ‘ƒ N eye = 1.1ğ‘Š . Targeting the real-world pilot studies described in Section 5, physical measurement shows that N eye and the world camera remain off during 86.8% and 94.6% of the system operation time, respectively. We can estimate that, considering a 2.1 Wh battery (similarly to Google Glass Explorer Edition), EMOShip can support 5.5 hours continuous operation, which can meet typical daily usage needs without frequent charging.\n\nFor comparison purposes, let's consider the record-everything case (shown in Figure  3 (b)), with both eye camera and world camera remaining on all the time. In this case, the overall system energy consumption is ğ¸ always-on = ğ‘‡ always-on Ã— (ğ‘ƒ eye camera + ğ‘ƒ world camera ), and the system battery lifetime is approximately 1.5 hours. Compared with the record-everything case, EMOShip improves the system battery lifetime by 3.6X.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Evaluation",
      "text": "This section presents the in-lab experiments to evaluate the performance of EMOShip.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Dataset",
      "text": "To evaluate EMOShip, we need the scene images observed by the wearer, the wearer's eye images, and also the emotional states during this observation process. In other words, an eligible dataset should cover both the scene and eye timelines and also contain emotion annotations of the same duration. However, most publiclyavailable emotion datasets cannot satisfy those requirements, since they either lack the scene images like the MUG dataset  [1]  or do not provide the facial or eye regions such as the FilmStim dataset  [50] . Therefore, we collect and build a new dataset named EMO-Film to suit our needs, which is available online at 5  and detailed below.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Data Collection.",
      "text": "The data of EMO-Film dataset is collected in a controlled laboratory environment. As shown in Fig.  4  (left), participants equipped with EMOShip was instructed to watch several emotion-eliciting video clips displayed on a desktop monitor. A total of 20 volunteers attended the data collection of EMO-Film, including 8 females and 12 males.\n\n(1) Video Data Preparation. The video clips were selected from the FilmStim dataset 6    [50]  as FilmStim is one of the widely-used emotion-eliciting video dataset  [61] . We first divide all videos of FilmStim dataset (64 video clips in total) into 7 categories based on the provided sentiment labels, each category corresponding to one emotional class (the neutral plus six basic emotion). Then we randomly sample at least one video clip from each category summing up to 6-7 for a participant to watch, which may take approximately 20 minutes to complete. Note that the film clips in FilmStim dataset are already sentimentally sufficient to evoke emotional reactions at certain degrees, and such design can ensure that all the six basic emotions will be evenly covered. We also ensure that each film clip was watched by at least two subjects.\n\n(2) Data Collection Process. During the watching process, we kept recording the eye regions of participants using the eye camera. To ensure the video scenes can be captured properly, we pre-adjusted the field of view of the world camera to be aligned with the monitor and recorded the displayed video simultaneously. In this way, we are able to gather the eye/scene data and the emotion ground-truths with aligned timelines, as shown in Fig.  4  (right). This recording session can typically take around 20 minutes per people.\n\n(3) Labeling Process. After all the scheduled movie clips displayed, the participant takes a short break (around 20 minutes) and then is instructed to label their emotional states. This labeling process can take up to 70 minutes (compared with 20 minutes of watching the films) and the generated emotional annotations are arguably accurate since the videos were just shown around 20 minutes ago. We develop a labeling tool with a GUI window to facilitate this process. Each participant is orally told how to use this tool, i.e. for each eye/scene image pair, the participant indicates their emotional state by clicking on the corresponding button or using a keyboard shortcut. There are a total of seven emotional states to choose from: neutral plus the six basic emotions. We consider only one emotional state per time instant for simplicity. This process is repeated until all eye/scene image pairs have been assigned labels.\n\nThe whole data collection process takes approximately four days, and the gathered data and labeling last for approximately 1.5 hours per participant.  The overall percentages of video sequences belong to \"anger\"/\"disgust\"/\"dear\"/\"happiness\"/\"sadness\"/\"surprise\" are 2.9%/18.2%/20.8%/20.0%/20.8%/17.3%, respectively. As shown in Table  1 , there are a total of 144,145/45,338 eye-scene image pairs in the training/testing set, respectively. Each eye-scene frame pair is properly aligned in timelines, and the frame-level emotion groundtruths is also provided. The resolution for scene image is 1280Ã—960, while that of eye images is 320Ã—240. The distribution of the seven emotion classes is also shown in Table  1 . As we can see, \"fear\" accounts for the largest non-neutral emotion, while the clips representing \"anger\" are the fewest. The number of \"neutrality\" clips is similar to that of \"fear\". We also apply the data augmentation techniques, including the rotations, flips, and affine transforms, to balance the distribution of different emotional states during the training stage, which can be important to the training of EMOShip-Net. When viewing identical sentimental contents, different participants may demonstrate different emotional reactions. This inter-participant variability is an interesting phenomenon to be examined and to be discussed. To examine this inter-participant variability, we first divide the videos into six sentimental categories excluding the neutrality, each category corresponding to one emotional class that they are likely to arouse, and then for each category, we calculate the percentage of video frames where all subjects demonstrate exactly the same emotional reactions. As shown in In Fig.  5 , we can see that \"surprise\" can be most easily aroused and shared among people watching the same videos, while there are also a comparatively high proportion of subjects who share the same emotional feelings from viewing content related with \"disgust\", \"fear\" and \"sadness\". \"Happiness\" and \"Anger\", however, have the lowest repeating probabilities, potentially because people have more personalized tastes on videos of those kinds.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Anger Disgust Fear Happiness",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Evaluation Metrics.",
      "text": "The emotionship, as defined in Equation  1 , depicts the emotional state of the users, and also describes the potential cause of such emotions. Since the frame-level ground-truths of emotions are already provided in our EMO-Film dataset, the evaluation of the former (emotional state prediction) is comparatively straight-forward. Following  [65] , we adopt the multilabel-based macro-averaging metric to evaluate the performance of emotional state predictions, as define in Equation  (6) .\n\nwhere ğµ(TP ğ‘— , FP ğ‘— , TN ğ‘— , FN ğ‘— ) represents binary classification performance on label ğ‘— (ğµ âˆˆ {Accuracy, Precision, Recall}). ğ¶ is the number of emotion classes, in this study, ğ¶ = 6. That is, we only recognize the six non-neutral emotions. TP ğ‘— , FP ğ‘— , TN ğ‘— , and FN ğ‘— denote the number of true positive, false positive, true negative, and false negative test samples with respect to the ğ‘— class label, respectively. However, as emotionship itself is a new concept, there is no existing metric that can be used to evaluate the quality of the latter, i.e. the understanding of potential causes of emotions. Besides, it is also difficult to objectively annotate such a potential cause, as it is highly personalized, subjective and subtle. In this work, we follow an intuitive way of examining the quality of such understandings. Several representative samples are visualized to compare the qualities of semantic attributes generated by EMOShip and a baseline based on VinVL model  [66] , and we also plot the varying trends of Influential Score from different scenarios to demonstrate that EMOShip has correctly captured the emotional impacts from scene images.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Baselines.",
      "text": "To evaluate the performance of emotion recognition, we have selected four works as baselines, which are: 1). the emotion-aware smart glasses EMO  [61] , 2). EMO+ which is an improved version of EMO, 3). VinVL model  [61]  that extracts semantic scene features for emotion recognition, and 4). VinVL+ that is modified to focus on the attentive regions of users.\n\n(1) EMO  [61]  utilizes a deep CNN to recognize emotions from eye images and is closely related with our work, and therefore it is used as a primary baseline. Note that we have discarded the classifier in EMO as it requires the construction of an auxiliary face recognition database, which is resource-costly but only introduces very limited improvement. (2) Inspired by  [3] , we integrate the information of pupil sizes with EMO to improve its recognition accuracy.\n\nIn particular, the pupil size of eyes is seen as a kind of expert information, and this expert information is concatenated to the second last Fully Connected (FC) layer of the CNN in a similar way with  [60] . This baseline method is denoted as EMO+.\n\n(3) Both EMO and EMO+ predict emotions from the eye images. However, hints on emotional states can also be fetched from scene images, especially from those sentimental visions that are more likely to evoke emotions. To validate this, we devise the third baseline method that only looks at the scene image and tries to predict emotional states from the sentimental clues in it. In details, we utilize the VinVL model  [61]  to extract visual features from scene images containing sentimental information. Then, those visual features are fed to a classifier consisting of two layers to obtain the emotion predictions. Regarding the summary tag generation, all the visual features are input into the OSCAR+ model  [66]  to obtain summary tags. This approach is called VinVL for simplicity. (4) The visual features of VinVL contain information from all potential Regions of Interest (RoIs). There can be various sentiment clues in those RoIs, however, it is the sentimental information from the user's attentive region that really matters. Therefore, we have set VinVL to focus on the user's visual attention and discard information from other irrelevant areas. This is achieved by only using visual features from the user's attentive region. This method is named VinVL+.\n\nAs for the understanding of the potential cause of emotions, we compare summary tags generated by our EMOShip with VinVL+ to provide an intuitive illustration of the qualities.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "4.2.3",
      "text": "Training EMOShip-Net. The structure EMOShip-Net is complicated, as it involves several backbone networks with significantly different architectures and design purposes. Instead of end-to-end training, we adopt an iterative way to learn EMOShip-Net, i.e. each component network is trained individually while freezing the weights of other parts.\n\nThe eye network N ğ‘’ğ‘¦ğ‘’ is used frame-wisely and serves as the trigger for the scene camera, and therefore it is the first component to learn. We generally follow the training procedures in  [61]  and pre-train N ğ‘’ğ‘¦ğ‘’ with cross-entropy loss on FER2013 dataset  [24]  and MUG dataset  [1] . Note that on MUG dataset the eye regions are cropped out of the whole facial image, as shown in Fig.  6 . The pre-trained N ğ‘’ğ‘¦ğ‘’ is further fine-tuned the training set of our collected EMO-Film dataset.\n\nConsidering the high complexity in visual features model N ğ‘‰ ğ¿ and the vision-Language model N ğ‘‚ğ‘† , we directly utilize the pre-trained weights provided by authors of those two models. The Squeeze-and-Excitation model N ğ‘†ğ¸ , is trained together with the FC layer F C ğ‘£ on EMO-Film dataset. We use Adam  [31]  optimizer with an initial learning rate of 0.001 and the batch size is set to 512. The whole training process lasts for a total of 500 epochs. (1) Binary Emotion Classification . In our system, the neutral/non-neutral classification results from ğ‘‡ N eye are serving as a trigger to capture emotional moments, and the accuracy of this binary classification can directly affect the performance of the whole system. Therefore, we first examine the quality of binary classification model EMO+ to ensure the trigger system is reliable enough. As shown in Table  2 , the proposed EMO+ significantly outperforms the baseline EMO model and achieves 80.7% precision, 79.0% recall, and 80.4% accuracy on this binary classification task, This demonstrates the value of adding pupil information in EMO models. The high accuracy achieved by EMO+ also indicates that our EMOShip-Net can be very sentiment to those emotional moments. (2) Multiple Emotion Classification. Table  4  demonstrates the emotion recognition performance of the four baseline methods and our EMOShip-Net on EMO-Film dataset.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Emotion Recognition.",
      "text": "The performance of EMO  [61]  has significantly outperformed that of VinVL  [66]  in terms of precision, recall, and accuracy. This is in line with our expectation, as the emotional clues within eye images are more generally straightforward when compared with the indirect and subtle sentimental clues in scene images. EMO+, the improvement version of EMO, has shown superior performance than EMO, and therefore the effectiveness of integrating pupil size information has been verified. The performance of VinVL+ also surpasses that of VinVL, which illustrates the importance of involving the user's attention. However, VinVL+ still cannot outperform EMO and EMO+, indicating the necessity of using expression-related features.\n\nDifferent from those baselines, EMOShip-Net fuses emotional evidence of both scene and eye images to achieve more comprehensive and accurate emotion predictions. Noticeably, EMOShip-Net has significantly outperformed the best baseline EMO+ by 5.3% precision, 5.8% recall, and 6.0% accuracy. This reveals the necessity and importance of inferring from both expression and visual perceptions, and the superiority of EMOShip-Net in terms of recognizing emotion states has been shown.\n\nWe have also plotted the confusion matrices of different methods to provide a more intuitive comparison. As shown in Fig.  7a , Fig.  7b , and Fig.  7c , we discover that EMOShip-Net achieves a better recognition rate on most of the emotions, demonstrating its superior generalization ability. We can also observe that EMOShip performs slightly worse on \"disgust\" than EMO+. That is because EMO+ determines emotional states exclusively based on eye images, while EMOShip takes both the visual region and eye images into consideration. Therefore, EMOShip may also suffer from this design when receiving strong misleading signals from visual attentive regions. A typical example is that when negatively-sentimental scene images are captured, it can be challenging for EMOShip to determine which kind of negative emotions (such as \"disgust\" or\"fear\") should be related to this visual information since they are all likely to happen as a result of viewing negative scenes. As shown in Fig.  7a  and Fig.  7b , the VinVL+ method that only utilizes the visual information generally delivers lower classification rates on negative emotions such as \"disgust\" and \"anger\" than EMO+, while their recognition accuracy on other classes, such as \"happiness\" and \"sadness\", are relatively close with each other. This validates that the associations between negative sentimental visions and negative emotions can be challenging to establish. Fig.  8  shows an exemplary case to provide further intuition. It presents successive scene/eye image sequence and corresponding emotion predictions within an approximate six-second clip of \"fear\" emotions. Both VinVL+ and EMO+ baseline have produced inconsistent emotion predictions during this clip, while our method has successfully given the fear predictions for all those frames. This verifies that our method can also generate more temporal-consistent emotional predictions, thanks to the fusion of both shreds of emotional evidence from scene and eye regions.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Understanding Of Potential Cause Of Emotions.",
      "text": "Understanding the potential cause of emotions can be too subtle and subjective to be quantitatively evaluated, especially when we are aiming to compute the Influential Score ğ¼ğ‘†, i.e. the degree of emotional impacts from visual perceptions. Despite those challenges, our EMOShip is while VinVL has used some inappropriate words, that is, \"A young girl is talking on a cellphone\". Such a kind of semantic accurateness is also an advantage of EMOShip.\n\nA man with curly hair standing in front of a building.\n\nA man holding a toothbrush in his hand.\n\nA man with a hat on his head and a beard.\n\nA man with a mustache is looking down at his hands.\n\nA man with a hat on his head.\n\nA man with his hands on his head.\n\nA young child is talking on a cell phone.\n\nA young girl is screaming while sitting on a bench.\n\nA person is in a dirty bathroom with a toilet.\n\nA child in a dirty room with a broken wall.\n\nA person is laughing while laying in bed.\n\nA person with a screaming face in a dark room. Next, we investigate how different emotional states can be associated with scene features through the use of Influential Score ğ¼ğ‘†. Fig.  10  shows the normalized average ğ¼ğ‘† of six non-neutral emotional categories. We can observe that the emotion \"sadness\" exhibits the highest the ğ¼ğ‘† value. This indicates that emotion \"sadness\" is generally more tightly associated with our visual perceptions than others. Also, emotion \"surprise\" presents the lowest ğ¼ğ‘† score and is therefore considered to be less related with scene features than all other emotions.",
      "page_start": 18,
      "page_end": 20
    },
    {
      "section_name": "Anger Disgust Fear",
      "text": "",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Generalization Ability.",
      "text": "We also examine the generalization ability of EMOShip-Net on unseen users. Specifically, 5 new participants out of the EMO-Film dataset were recruited, and we follow identical data collection procedures as in Section 4.1.1 to formulate an extra evaluate set that is strictly subject-independent with the EMO-Film dataset of training our models. This new evaluation set totals up to approximate 105 minutes, and then we evaluate the in-lab emotion recognition performance of EMOShip-Net on this newly-collected unseen dataset. In particular, we compare the performance of EMOShip-Net on this new test set with that of the EMO and EMO+ baseline methods (the two most out-standing baseline methods). The results are shown in Table  4 . We can see that EMOShip-Net demonstrates superior performance than EMO and EMO+. This exhibits that EMOShip-Net can generalize well to unseen subjects. We further examine the performance regarding F1 score of different methods on two test sets. Even for the same method, the performance on two different test sets is not very comparable, as one set may be more challenging than another. To validate the assumption, we compute F1 scores of different methods on the original/new test set along with the drop rates. As can be seen from Table  5 , all methods have shown a significant drop on this new test set, and therefore we may reasonably assume that this new test set can be a challenging set than the original one. Despite the raised difficulties, our proposed EMOShip-Net still exhibits the lowest performance degradation on this new set, which verifies the importance of exploiting emotionship.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Pilot Study",
      "text": "In additional to in-lab experiments, we also perform approximate three-week in-field pilot studies to evaluate the performance of EMOShip under realistic scenarios. In this section, we present two valuable real-life applications to fulfill the potentials of EMOShip and to demonstrate its usability for everyday life, while we also discuss the current limitations of EMOShip and the future works.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Applications",
      "text": "The most significant advantage of EMOShip is that it captures emotionship instead of emotions. Compared with other emotional-aware glasses like EMO  [61] , our EMOShip not only predicts emotions at higher accuracy but also provides intuitive explanations on the potential causes of those emotional states. This awareness of emotionship opens a gate into unseen attractive applications. Multiple rounds of user interviews lead to two representative and promising applications, which are Emotionship Self-reflection and Emotionship Life-logging, respectively. In psychology, the term \"self-reflection\" refers to the process of analyzing past behaviors to achieve better efficiency in the future  [16, 23] . Self-reflection is indispensable, especially for people affected by negative emotions. As indicated in relevant studies  [17] , negative emotions can lead to mental well-being issues. To maintain mental health, we need to self-reflect on negative emotional moments, and we also need to find what evokes those emotions such that our times of exposure to those causes can be intentionally minimized. This is a good scenario of applying EMOShip. The ability to record emotional moments, to retrieve negative emotional moments, and to discover the emotional causes, can all be satisfied using EMOShip. This application is named Emotionship Self-reflection.\n\nLife-logging is usually considered as a digital self-tracking or recording of everyday life  [52] , which is already a popular application. Current life-logging applications commonly record scenes with commercial glasses like GoPro and Google Clip, which lack personalized experiences. Besides, it is also difficult to categorize those recordings into different emotional categories, since those eyewear devices also lack emotion awareness. Manually classifying those emotional moments can be a practical way, yet it is extremely time-consuming and tedious, and the user may not be able to recall the extracted emotional activities. Therefore, we incorporated EMOShip with life-logging to set up a new application Emotionship Life-logging. Different from conventional life-logging, our Emotionship Life-logging can automatically detect moments of different emotions and record those memorable moments as can be customized by wearers, and the potential emotional causes are also documented. Moreover, Emotionship Life-logging also enables various interesting and promising down-stream tasks such as emotional retrieving and classification  [64] .",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Procedure Of Pilot Study",
      "text": "In-field pilot studies are performed for those two applications. A total of 20 volunteers, including 14/6 males/females aged between 23 to 40, was recruited to participate in pilot studies. The research target of understanding the potential causes of emotions was briefed to all participants before the launching of the pilot studies. Those volunteers were also informed that their daily activities will be recorded for research purposes.\n\nDuring this in-field pilot study, participants were introduced to take on EMOShip whenever they are convenient such that their everyday life can be covered as densely as possible. When equipped, EMOShip automatically recorded those emotional moments along with the potential causes from visual attention. The complete scene videos taken by the world camera were also saved for reference and are referred to as the baseline video.\n\nThe pilot studies lasted for around three weeks, and at the end of the study, volunteers were asked to assist the evaluation of EMOShip in terms of understanding daily emotions and their causes. In particular, we require the participants: 1). to watch those emotional moments taken by EMOShip and to mark those clips that they believed to have correctly reflected their emotional states, 2). also to retrieve from the baseline video those emotional moments that EMOShip failed to capture. In addition, the participants were asked to complete a questionnaire survey regarding their opinions on the two emotionship applications, the usability of EMOShip, the free feedback, etc.  ), respectively. That is consistent with the short-term property of non-neutral emotions. As indicated in relevant research  [56] , non-neutral emotions are typically aroused by a sudden emotional stimuli, and they are shortterm mental processes that can vanish in a few seconds. In other words, non-neutral emotions are much rarer than neutral ones in our daily life. We inspect P6 for a detailed understanding. One of the scenarios of P6 is watching a basketball game lasting for around 12 minutes and our system has detected 0.4 minutes of non-neutral Emotional Moments (EM). Those EMs occurred exactly when the wearer has seen two scoring shoots that leads to his emotional reactions, each one lasting for around 0.2 minute. Given a 30 fps sampling rate, the 2 EMs contain approximate 720 image frames (2Ã—30Ã—0.2Ã—60). Apart from those moments, P6 remain emotionally neutral. EMOShip correctly captures those non-netural emotional moments.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Performance Of Emoship In Pilot Study",
      "text": "Based on emotional moments marked by users at the end of the pilot study, we are able to evaluate the performance of EMOShip in practice. Generally, users pay attention to how many emotional moments are correctly We also plot a confusion matrix on pilot studies to provide a more intuitive understanding. As shown in Fig.  11 , EMOShip achieves a satisfying classification rate on the emotional categories. Besides, positive emotions  [10]  (171 of \"Happiness\" and \"Surprise\") are much more frequent than negative ones (53 of \"Sadness\", \"Anger\", \"Digust\" and \"Fear\"), indicating that positive emotions are the dominating emotional states in daily life of those wearers during the pilot studies.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Emotionship Analysis.",
      "text": "(1) Emotional States Summary. To give participants an overall understanding of their past emotional states, we briefly summarize the past emotional states for each user by roughly categorizing the six basic non-neutral emotional states as positive and negative. Intuitively, we categorize \"happiness\" and \"surprise\" as positive emotional states, while the rest four emotions are regarded as negative ones. For each user, we use Pr and Nr to denote the proportion of positive emotions and negative emotions, respectively. For a certain time window, we can suggest two rough emotional patterns as follows:\n\nâ€¢ Type I: Pr > Nr, indicating that the overall emotional state of a user lean towards positive.\n\nsocial events. In the long run, it would be significantly beneficial for me to understand and manage my emotions by utilizing EMOShip to analyze my emotions and record my emotional moments. \"-P1 Similarly, another participant appreciated the application of EMOShip to long-term mood perception and management, as figured out by this volunteer:\n\n\"EMOShip shows that I have two significantly different states of mind when driving or walking to work. When I commute on foot, the emotions appear to be more positive and I tend to feel happy more frequently. My driving emotions, on the other hand, often seem to be negative, such as fear and anger. ...... I may feel negative or get road rage encountering rule-breaking behaviours such as slow left-lane driving or unsafe lane changes. In addition, with the help of EMOShip I also noticed that I seem to be overly cheerful during business meetings, which may leave an unintended impression of me being unprofessional or unreliable. EMOShip unveils the importance of facial expression management to me. I need to be more aware of my social environment whether I should be more happy or serious.",
      "page_start": 23,
      "page_end": 26
    },
    {
      "section_name": "\"-P2",
      "text": "The third user stated that EMOShip can significantly ease the logging of emotional moments, which can be of importance:\n\n\"EMOShip can assist me to record some interesting or important moments and my emotions at that time, both of which are crucial for me to get these moments rapidly reviewed. ...... . Reviewing the meeting materials that are important to me by watching the videos EMOShip recorded can save me a great amount of time. Plus, my emotions may also shift during interesting moments in life. For example, EMOShip records intense game sessions and sensational videos when I feel happy or sad. It would have been very inconvenient for me to record them manually clip by clip while playing games or watching videos, whereas EMOShip can easily record them for me to review or share quickly afterwards. \"-P6\n\nOn the other hand, there is also a volunteer who disregarded the importance of recording emotional moments, and we quote his feedback below: \"I used EMOShip while playing cards. Since this is a highly enjoyable entertainment, there was little change in my recorded emotion types. Moreover, I probably didn't pay much attention to the changes in my emotions. \"-P7",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Limitations And Future Works",
      "text": "We have demonstrated the technical capabilities of EMOShip to recognize emotion states and understand their causes. However, we also observe several limitations from its applications to real-world scenarios and from users' feedback. In this section, we briefly discuss some potential future works that will further improve EMOShip system. 5.4.1 Personalized Emotional Management. Although most users have provided positive feedback on EMOShip, a consensus is that they would like to also receive suggestions on how to reduce the occurrences of negative emotional moments. Since different people have different situations and hence requires personalized service, we are planning to integrate into EMOShip a long-term emotion tracking, emotional management, and regulation system which can be personalized to suggest how to avoid causes of negative emotions.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Privacy",
      "text": "Concern and Privacy Protection. Although participants are interested in perceiving their emotional states, some participants are uncomfortable with exposing their personal affective information to any third parties. Future system design should carefully consider how to address privacy concerns. Another common feedback from users is that they are worried about the disclosing of their emotional information, especially to malicious third parties. Therefore, we set plans on enhancing the privacy protection of using EMOShip and also on ensuring the safety of recorded personal data, from both software and hardware sides.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Multi-Modality In Emotional",
      "text": "Causes. EMOShip considers the visual stimuli as the most likely reason for stimulating emotions. However, visual perceptions are not the only perception that can arouse emotions. For example, the auditory perception, like a sharp, annoying sound, can also affect emotional states. How to fuse the emotional hints from multi-modality data remains a challenging topic for us to address in the future.",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose and address the emotionship analysis problem for eyewear devices, an ambitious yet challenging task. To this end, we develop a deep network EMOShip-Net that can predict the semantic attributes from scene images and can synthesis emotional clues from both eye and scene images with an awareness of each feature's importance factor. Based on EMOShip-Net, we present EMOShip, the first-ever intelligent eyewear system that is capable of emotionship analysis. Experimental results on FilmStim dataset of 20 participants demonstrate that EMOShip achieves 80.2% emotional moment capturing accuracy, which significantly outperforms baseline methods. We also demonstrate that EMOShip can provide a valuable understanding of the cause of emotions. We develop two promising applications using EMOShip, i.e. emotionship self-reflection and emotionship life-logging, and we conduct 20 in-field pilot studies to demonstrate the usability and advantages. Most participants like EMOShip and think EMOShip helps them realize and understand their emotions that they usually ignore. Most participants provide positive feedback on EMOShip and admit its potentials. EMOShip is the first practice of emotionship-aware eyewear system, and it can be inspiring to the incoming age of intelligent wearable systems. Will smart glasses dream of sentiment visions? We will know soon.",
      "page_start": 27,
      "page_end": 27
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed EMOShip smart eyewear system.",
      "page": 4
    },
    {
      "caption": "Figure 2: The workflow of EMOShip-Net (Best Seen in Color).",
      "page": 8
    },
    {
      "caption": "Figure 2: illustrates the workflow of EMOShip-Net. At time step ğ‘¡, the input of the network",
      "page": 9
    },
    {
      "caption": "Figure 3: The run-time operation of EMOShip (a), as well as that of the recording-everything case (b).",
      "page": 12
    },
    {
      "caption": "Figure 3: (a) illustrates the run-time operation of EMOShip on the eyewear. Targeting the Qualcomm Snapdragon",
      "page": 13
    },
    {
      "caption": "Figure 3: (b)), with both eye",
      "page": 13
    },
    {
      "caption": "Figure 4: (left), participants equipped with EMOShip was instructed to watch several emotion-eliciting",
      "page": 13
    },
    {
      "caption": "Figure 4: (right). This recording session can typically take around 20 minutes per people.",
      "page": 13
    },
    {
      "caption": "Figure 4: Data collection of EMO-Film: The laboratory setting (left), and the eye and scene images of seven emotional states",
      "page": 14
    },
    {
      "caption": "Figure 5: , we can see that â€œsurpriseâ€ can be most easily aroused and shared among people",
      "page": 15
    },
    {
      "caption": "Figure 5: The percentage of video frames where all subjects demonstrate exactly the same emotional reactions for different",
      "page": 15
    },
    {
      "caption": "Figure 6: The pre-trained Nğ‘’ğ‘¦ğ‘’is further fine-tuned the training",
      "page": 16
    },
    {
      "caption": "Figure 6: Seven emotional expressions of the original MUG facial expression examples (top row), our fine-tuning single-eye-area",
      "page": 17
    },
    {
      "caption": "Figure 7: a, Fig. 7b, and Fig. 7c, we discover that EMOShip-Net achieves a better recognition rate on most",
      "page": 17
    },
    {
      "caption": "Figure 7: a and Fig. 7b, the",
      "page": 18
    },
    {
      "caption": "Figure 7: Confusion matrix of individual emotional moments when using the two baseline methods and the proposed method.",
      "page": 18
    },
    {
      "caption": "Figure 8: shows an exemplary case to provide further intuition. It presents successive scene/eye image sequence",
      "page": 18
    },
    {
      "caption": "Figure 8: An example of emotion recognition comparison between the proposed EMOShip-Net and the two baseline methods.",
      "page": 19
    },
    {
      "caption": "Figure 9: , we provide examples of the semantic summary tags generated by our method and the VinVL baseline.",
      "page": 19
    },
    {
      "caption": "Figure 9: Examples of the semantic summary tags generated by our method (a) and the VinVL baseline (b). The summary tag is",
      "page": 20
    },
    {
      "caption": "Figure 10: shows the normalized average ğ¼ğ‘†of six non-neutral emotional categories. We can",
      "page": 20
    },
    {
      "caption": "Figure 10: Degree of emotional impacts from visual perceptions.",
      "page": 20
    },
    {
      "caption": "Figure 11: Confusion matrix of individual emotional moments when using EMOShip in pilot studies.",
      "page": 24
    },
    {
      "caption": "Figure 12: , we can observe that 17 out of 20 users belong to Type I, while 3 users fall into Type II (P8,",
      "page": 24
    },
    {
      "caption": "Figure 12: Profile of emotional states for all 20 participants.",
      "page": 24
    },
    {
      "caption": "Figure 13: shows the temporally-consistent emotional states for a participant (P6). The reason for selecting this",
      "page": 24
    },
    {
      "caption": "Figure 13: , during the whole timeline, the major emotional state is â€œhappinessâ€. This is not",
      "page": 24
    },
    {
      "caption": "Figure 13: Time series emotional states for a participant (P6). Emojis are taken from [42].",
      "page": 25
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , there are a total of 144,145/45,338 eye-scene image pairs in the training/testing set,",
      "data": [
        {
          "Emotional States": "Number of eye-scene image pairs\nin training set",
          "Anger": "3,519",
          "Disgust": "21,844",
          "Fear": "25,000",
          "Happiness": "23,807",
          "Sadness": "24,080",
          "Surprise": "20,895",
          "Neutrality": "25,000"
        },
        {
          "Emotional States": "Number of eye-scene image pairs\nin testing set",
          "Anger": "990",
          "Disgust": "2,843",
          "Fear": "8,693",
          "Happiness": "4,214",
          "Sadness": "7,068",
          "Surprise": "3,801",
          "Neutrality": "17,729"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "F1 Score": "Original Test Set\n(subject-dependent)",
          "Drop Rate": ""
        },
        {
          "Method": "EMOShip-Net (Ours)\nEMO+\nEMO",
          "F1 Score": "74.9%\n69.4%\n66.4%",
          "Drop Rate": "5.4%\n7.6%\n8.6%"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scenario #1": "Scenario #2"
        },
        {
          "Scenario #1": ""
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Scenario #1": "Scenario #2",
          "A couple of people\nare playing\nbasketball in a gym": "A close up of\na loaf of bread\non a counter"
        },
        {
          "Scenario #1": "Scenario #3",
          "A couple of people\nare playing\nbasketball in a gym": "A view from a car\nwindow of traffic\non a street"
        }
      ],
      "page": 25
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The MUG facial expression database",
      "authors": [
        "Niki Aifanti",
        "Christos Papachristou",
        "Anastasios Delopoulos"
      ],
      "year": "2010",
      "venue": "11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10"
    },
    {
      "citation_id": "2",
      "title": "Convolutional image captioning",
      "authors": [
        "Jyoti Aneja",
        "Aditya Deshpande",
        "Alexander Schwing"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Neural networks for emotion recognition based on eye tracking data",
      "authors": [
        "Claudio Aracena",
        "SebastiÃ¡n Basterrech",
        "VÃ¡clav SnÃ¡el",
        "Juan VelÃ¡squez"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "4",
      "title": "Embodied semantics for actions: Findings from functional brain imaging",
      "authors": [
        "Lisa Aziz",
        "Antonio Damasio"
      ],
      "year": "2008",
      "venue": "Journal of Physiology-Paris"
    },
    {
      "citation_id": "5",
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "Proceedings of ICLR"
    },
    {
      "citation_id": "6",
      "title": "Insense: Interest-based life logging",
      "authors": [
        "Mark Blum",
        "Alex Pentland",
        "Gerhard Troster"
      ],
      "year": "2006",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "7",
      "title": "From pixels to sentiment: Fine-tuning CNNs for visual sentiment prediction",
      "authors": [
        "Victor Campos",
        "Brendan Jou",
        "Xavier Giro-I Nieto"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "8",
      "title": "MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-Capture",
      "authors": [
        "Yuhu Chang",
        "Yingying Zhao",
        "Mingzhi Dong",
        "Yujiang Wang",
        "Yutian Lu",
        "Qin Lv",
        "Robert Dick",
        "Tun Lu",
        "Ning Gu",
        "Li Shang"
      ],
      "year": "2021",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
      "doi": "10.1145/3463509"
    },
    {
      "citation_id": "9",
      "title": "Uniter: Learning universal image-text representations",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu",
        "Ahmed Kholy",
        "Faisal Ahmed",
        "Zhe Gan",
        "Yu Cheng",
        "Jingjing Liu"
      ],
      "year": "2019",
      "venue": "Uniter: Learning universal image-text representations"
    },
    {
      "citation_id": "10",
      "title": "Equivalent perceptual asymmetries for free viewing of positive and negative emotional expressions in chimeric faces",
      "authors": [
        "D Stephen",
        "Michelle Christman",
        "Hackworth"
      ],
      "year": "1993",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "11",
      "title": "Perceiving causality in character perception: A metaphorical study of causation in film",
      "authors": [
        "Maarten CoÃ«gnarts",
        "Peter Kravanja"
      ],
      "year": "2016",
      "venue": "Metaphor and Symbol"
    },
    {
      "citation_id": "12",
      "title": "The interface between emotion and attention: A review of evidence from psychology and neuroscience",
      "authors": [
        "Rebecca Compton"
      ],
      "year": "2003",
      "venue": "Behavioral and cognitive neuroscience reviews"
    },
    {
      "citation_id": "13",
      "title": "EmotionCheck: leveraging bodily signals and false feedback to regulate our emotions",
      "authors": [
        "Jean Costa",
        "Alexander Adams",
        "Malte F Jung",
        "FranÃ§ois GuimbretiÃ¨re",
        "Tanzeem Choudhury"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "15",
      "title": "The emotional brain",
      "authors": [
        "Tim Dalgleish"
      ],
      "year": "2004",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Learning from experience through reflection",
      "authors": [
        "Marilyn Wood"
      ],
      "year": "1996",
      "venue": "Organizational dynamics"
    },
    {
      "citation_id": "17",
      "title": "Emotion-Aware Systems for Promoting Human Well-being",
      "authors": [
        "Elena Di"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers"
    },
    {
      "citation_id": "18",
      "title": "Unobtrusive Assessment of Students' Emotional Engagement during Lectures Using Electrodermal Activity Sensors",
      "authors": [
        "Elena Di Lascio",
        "Shkurta Gashi",
        "Silvia Santini"
      ],
      "year": "2018",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
      "doi": "10.1145/3264913"
    },
    {
      "citation_id": "19",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Vincent Michalski",
        "Kishore Konda",
        "Roland Memisevic",
        "Christopher Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "20",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "21",
      "title": "A smile/laughter recognition mechanism for smile-based life logging",
      "authors": [
        "Kurara Fukumoto",
        "Tsutomu Terada",
        "Masahiko Tsukamoto"
      ],
      "year": "2013",
      "venue": "Proceedings of the 4th Augmented Human International Conference"
    },
    {
      "citation_id": "22",
      "title": "Emotion detection: a technology review",
      "authors": [
        "Jose Maria",
        "Garcia- Garcia",
        "Maria Victor Mr Penichet",
        "Lozano"
      ],
      "year": "2017",
      "venue": "Proceedings of the XVIII international conference on human computer interaction"
    },
    {
      "citation_id": "23",
      "title": "Towards Improving Emotion Self-Report Collection Using Self-Reflection",
      "authors": [
        "Surjya Ghosh",
        "Bivas Mitra",
        "Pradipta De"
      ],
      "year": "2020",
      "venue": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "24",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "Ian Goodfellow",
        "Dumitru Erhan",
        "Pierre Carrier",
        "Aaron Courville",
        "Mehdi Mirza",
        "Ben Hamner",
        "Will Cukierski",
        "Yichuan Tang",
        "David Thaler",
        "Dong-Hyun Lee"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "25",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Eyemotion: Classifying facial expressions in VR using eye-tracking cameras",
      "authors": [
        "Steven Hickson",
        "Nick Dufour",
        "Avneesh Sud",
        "Vivek Kwatra",
        "Irfan Essa"
      ],
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "28",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction",
      "authors": [
        "Moritz Kassner",
        "William Patera",
        "Andreas Bulling"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication"
    },
    {
      "citation_id": "30",
      "title": "Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition",
      "authors": [
        "Bo-Kyeong Kim",
        "Hwaran Lee",
        "Jihyeon Roh",
        "Soo-Young Lee"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "31",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition in the wild via convolutional neural networks and mapped binary patterns",
      "authors": [
        "Gil Levi",
        "Tal Hassner"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "33",
      "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
      "authors": [
        "Gen Li",
        "Nan Duan"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "36",
      "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
      "authors": [
        "Xiujun Li",
        "Xi Yin",
        "Chunyuan Li",
        "Pengchuan Zhang",
        "Xiaowei Hu",
        "Lei Zhang",
        "Lijuan Wang",
        "Houdong Hu",
        "Li Dong",
        "Furu Wei"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
      "arxiv": "arXiv:1908.02265"
    },
    {
      "citation_id": "38",
      "title": "Effective Approaches to Attention-based Neural Machine Translation",
      "authors": [
        "Thang Luong",
        "Hieu Pham",
        "Christopher Manning"
      ],
      "year": "2015",
      "venue": "Proceedings of EMNLP",
      "doi": "10.18653/v1/d15-1166"
    },
    {
      "citation_id": "39",
      "title": "Stavros Petridis, and Maja Pantic. 2021. Lip-reading with densely connected temporal convolutional networks",
      "authors": [
        "Pingchuan Ma",
        "Yujiang Wang",
        "Jie Shen"
      ],
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "40",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "Jana Machajdik",
        "Allan Hanbury"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "AffectiveWear: towards recognizing affect in real life",
      "authors": [
        "Katsutoshi Masai",
        "Yuta Sugiura",
        "Katsuhiro Suzuki",
        "Sho Shimamura",
        "Kai Kunze",
        "Masa Ogata",
        "Masahiko Inami",
        "Maki Sugimoto"
      ],
      "year": "2015",
      "venue": "Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "42",
      "title": "Streamline Emoji, Free Icons from the Streamline Icons Pack",
      "authors": [
        "Vincent Le"
      ],
      "year": "2017",
      "venue": "Streamline Emoji, Free Icons from the Streamline Icons Pack"
    },
    {
      "citation_id": "43",
      "title": "Emotion drives attention: detecting the snake in the grass",
      "authors": [
        "Arne Ã–hman",
        "Anders Flykt",
        "Francisco Esteves"
      ],
      "year": "2001",
      "venue": "Journal of experimental psychology: general"
    },
    {
      "citation_id": "44",
      "title": "Neural control of vascular reactions: impact of emotion and attention",
      "authors": [
        "Hadas Okon-Singer",
        "Jan Mehnert",
        "Jana Hoyer",
        "Lydia Hellrung",
        "Herma Schaare",
        "Juergen Dukart",
        "Arno Villringer"
      ],
      "year": "2014",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "45",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "Maja Pantic"
      ],
      "year": "2003",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "46",
      "title": "Measuring emotions in students' learning and performance: The Achievement Emotions Questionnaire (AEQ)",
      "authors": [
        "Reinhard Pekrun",
        "Thomas Goetz",
        "Anne Frenzel",
        "Petra Barchfeld",
        "Raymond Perry"
      ],
      "year": "2011",
      "venue": "Contemporary educational psychology"
    },
    {
      "citation_id": "47",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "48",
      "title": "Multi-level region-based convolutional neural network for image emotion classification",
      "authors": [
        "Tianrong Rao",
        "Xiaoxu Li",
        "Haimin Zhang",
        "Min Xu"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "49",
      "title": "How Do You Feel Online: Exploiting Smartphone Sensors to Detect Transitory Emotions during Social Media Use",
      "authors": [
        "Mintra Ruensuk",
        "Eunyong Cheon",
        "Hwajung Hong",
        "Ian Oakley"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "50",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "Alexandre Schaefer",
        "FrÃ©dÃ©ric Nils",
        "Xavier Sanchez",
        "Pierre Philippot"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "51",
      "title": "Expression glasses: a wearable device for facial expression recognition",
      "authors": [
        "Jocelyn Scheirer",
        "Raul Fernandez",
        "Rosalind Picard"
      ],
      "year": "1999",
      "venue": "CHI'99 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "52",
      "title": "Lifelogging: Digital self-tracking and Lifelogging-between disruptive technology and cultural transformation",
      "authors": [
        "Stefan Selke"
      ],
      "year": "2016",
      "venue": "Lifelogging: Digital self-tracking and Lifelogging-between disruptive technology and cultural transformation"
    },
    {
      "citation_id": "53",
      "title": "Wscnet: Weakly supervised coupled networks for visual sentiment classification and detection",
      "authors": [
        "Dongyu She",
        "Jufeng Yang",
        "Ming-Ming Cheng",
        "Yu-Kun Lai",
        "Paul Rosin",
        "Liang Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "Lxmert: Learning cross-modality encoder representations from transformers",
      "arxiv": "arXiv:1908.07490"
    },
    {
      "citation_id": "55",
      "title": "Eye-Tracking Analysis for Emotion Recognition",
      "authors": [
        "PaweÅ‚ Tarnowski",
        "Marcin KoÅ‚odziej",
        "Andrzej Majkowski"
      ],
      "year": "2020",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "56",
      "title": "Mobile Mood Tracking: An Investigation of Concise and Adaptive Measurement Instruments",
      "authors": [
        "Helma Torkamaan",
        "JÃ¼rgen Ziegler"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "57",
      "title": "Resilient individuals use positive emotions to bounce back from negative emotional experiences",
      "authors": [
        "M Michele",
        "Barbara Tugade",
        "Fredrickson"
      ],
      "year": "2004",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "58",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of NIPS. 5998-6008"
    },
    {
      "citation_id": "59",
      "title": "Abhinav Gupta, and Kaiming He",
      "authors": [
        "Xiaolong Wang",
        "Ross Girshick"
      ],
      "year": "2018",
      "venue": "Proceedings of CVPR"
    },
    {
      "citation_id": "60",
      "title": "Dynamic Face Video Segmentation via Reinforcement Learning",
      "authors": [
        "Yujiang Wang",
        "Mingzhi Dong",
        "Jie Shen",
        "Yang Wu",
        "Shiyang Cheng",
        "Maja Pantic"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "61",
      "title": "EMO: Real-time emotion recognition from single-eye images for resource-constrained eyewear devices",
      "authors": [
        "Hao Wu",
        "Jinghao Feng",
        "Xuejin Tian",
        "Edward Sun",
        "Yunxin Liu",
        "Bo Dong",
        "Fengyuan Xu",
        "Sheng Zhong"
      ],
      "year": "2020",
      "venue": "Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services"
    },
    {
      "citation_id": "62",
      "title": "Show, attend and tell: Neural image caption generation with visual attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov",
        "Rich Zemel",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "63",
      "title": "Weakly supervised coupled networks for visual sentiment analysis",
      "authors": [
        "Jufeng Yang",
        "Dongyu She",
        "Yu-Kun Lai",
        "Paul Rosin",
        "Ming-Hsuan Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "64",
      "title": "Retrieving and classifying affective images via deep metric learning",
      "authors": [
        "Jufeng Yang",
        "Dongyu She",
        "Yu-Kun Lai",
        "Ming-Hsuan Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "65",
      "title": "A Review on Multi-Label Learning Algorithms",
      "authors": [
        "M Zhang",
        "Z Zhou"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Knowledge & Data Engineering"
    },
    {
      "citation_id": "66",
      "title": "Vinvl: Revisiting visual representations in vision-language models",
      "authors": [
        "Pengchuan Zhang",
        "Xiujun Li",
        "Xiaowei Hu",
        "Jianwei Yang",
        "Lei Zhang",
        "Lijuan Wang",
        "Yejin Choi",
        "Jianfeng Gao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "67",
      "title": "Emotion recognition using wireless signals",
      "authors": [
        "Mingmin Zhao",
        "Fadel Adib",
        "Dina Katabi"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking"
    },
    {
      "citation_id": "68",
      "title": "A Reinforcement-Learning-based Energy-Efficient Framework for Multi-Task Video Analytics Pipeline",
      "authors": [
        "Yingying Zhao",
        "Mingzhi Dong",
        "Yujiang Wang",
        "Da Feng",
        "Qin Lv",
        "Robert Dick",
        "Dongsheng Li",
        "Tun Lu",
        "Ning Gu",
        "Li Shang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2021.3076612"
    },
    {
      "citation_id": "69",
      "title": "Unified vision-language pre-training for image captioning and vqa",
      "authors": [
        "Luowei Zhou",
        "Hamid Palangi",
        "Lei Zhang",
        "Houdong Hu",
        "Jason Corso",
        "Jianfeng Gao"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "70",
      "title": "Dependency exploitation: A unified CNN-RNN approach for visual emotion recognition",
      "authors": [
        "Xinge Zhu",
        "Liang Li",
        "Weigang Zhang",
        "Tianrong Rao",
        "Min Xu",
        "Qingming Huang",
        "Dong Xu"
      ],
      "year": "2017",
      "venue": "proceedings of the 26th international conf"
    }
  ]
}