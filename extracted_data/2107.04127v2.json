{
  "paper_id": "2107.04127v2",
  "title": "Multitask Multi-Database Emotion Recognition",
  "published": "2021-07-08T21:57:58Z",
  "authors": [
    "Manh Tu Vu",
    "Marie Beurton-Aimar"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we introduce our submission to the 2nd Affective Behavior Analysis in-the-wild (ABAW) 2021 competition. We train a unified deep learning model on multi-databases to perform two tasks: seven basic facial expressions prediction and valence-arousal estimation. Since these databases do not contain labels for all the two tasks, we have applied the distillation knowledge technique to train two networks: one teacher and one student model. The student model will be trained using both ground truth labels and soft labels derived from the pretrained teacher model. During the training, we have added one more task, which is the combination of the two mentioned tasks, for better exploiting inter-task correlations. We also exploit the sharing videos between the two tasks of the AffWild2 database that is used in the competition, to further improve the performance of the network. Experiment results show that the network has achieved promising results on the validation set of the AffWild2 database. Code and pretrained model are publicly available at https://github.com/glmanhtu/multitask-abaw-2021",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition and analysis is a crucial part of many applications and systems, especially in health care and medical fields  [1, 2]  as it is directly related to the health state of a patient. As results, more and more works have been conducted to try to analyse human emotions and behaviours  [3, 4, 5] . In the same sense, the 2nd Affective Behavior Analysis in-the-wild (ABAW 2021) competition by Kollias et al.  [6, 7, 8, 9, 10, 11, 12, 13]  provides a large-scale dataset Aff-Wild2 for analysing human emotion in-the-wild settings. This dataset includes annotations for three tasks, which are including: valence-arousal estimation, action unit (AU) detection, and seven basic facial expression classification. Valence represents how positive the person is while arousal describes how active this person is. The seven basic facial expressions include neutral, anger, disgust, fear, happiness, sadness, and surprise. AUs are the basic actions of individuals or groups of muscles for portraying emotions.\n\nIn this paper, we focus on two tasks, which are including seven basic facial expressions classification and valence-arousal estimation. Inspired by the multitask training with incomplete label method from Deng et al.  [14]  we propose a method to futher exploit the inter-task correlations between the two tasks. Similar to Deng et al.  [14]  we also apply the distillation knowledge technique to train two multitask models: a teacher model and a student model. However, instead of treating each task independently when training teacher model as in  [14] , we add one more task to the training process, which is the combination of the two tasks above to train the network using data comming from AffectNet database  [15] , in which contains labels for both the two tasks. A part from that, we also try to exploit the shared videos (videos annotated both seven basic facial expression and valence-arousal labels) in the Affwild2 database by integrating this information to the student model's training process (see  Equation 10) . With these improvements, our model achieves a competitive results on the validation set of the official dataset Affwild2 of the competition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section we introduce our multitask multi-databases method. Visual images are extracted from video and fed into a ResNet 50  [16]  networks to train for analysing human's emotion in-the-wild. Then, features extracted from this network will go through a GRU  [17]  network to capture temporal information and finally, perform both the seven basic facial expressions classification and valence-arousal estimation. Because in our dataset, we don't always have all labels for all of our tasks. Therefore, we have applied the multitask training with missing labels method that is described in  [14]  with some enhancements, which we will describe in the sections below.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Imbalancing",
      "text": "Similar to  [14] , we also use some external datasets to address the data imbalance problem in the Affwild2 dataset  [18] . The external datasets are including Expression in-the-Wild (ExpW) dataset  [19]  for expression classification and AFEW-VA dataset  [20]  for valence-arousal estimation. After merging these datasets, we have also applied the same dataset balancing protocol, which is described in  [14]  to improve the balance of the dataset.\n\nDifferent from  [14] , we perform only two tasks: seven basic facial expressions prediction and valence-arousal estimation. Since detecting AUs is out of our interest and we would like to perform the two mentioned tasks as best as possible. we exclude the task of facial AU detection out of our training process. A part from that, we also want to include the AffectNet database  [15]  into the training, since in this database annotation for both seven basic expressions and valence-arousal are available. Now, for the training process, our dataset is including three parts: Mixed EXPR The mixing set of the AffWild 2 (expressions part) and ExpW datasets for seven basic expressions. This dataset has no information about valence and arousal.\n\nMixed VA The mixing set of the AffWild 2 (valence-arousal part) and AFEW-VA datasets for valence and arousal. This dataset has no information about the seven basic expressions.\n\nAffect EXPR_VA The AffectNet dataset, for both seven basic expressions and valence-arousal.\n\nCorresponding to these three dataset's parts are the three training tasks T ∈ {1, 2, 3}, which are including: expression classification (EXPR), valence-arousal estimation (VA) and the mixed of these two tasks (EXPR_VA). Since the data for the last task EXPR_VA has annotated for both seven basic expressions and valence-arousal, this task will play the role of guiding the training, i.e. re-balancing the gradient back propagation for the first two tasks and exploiting the inter-task correlations. One can note that even though we have three training tasks, our model has only two outputs, which are EXPR and VA, since the last training task reuses these two outputs for computing loss.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Multitask Training With Missing Labels",
      "text": "Besides the excluding of facial AU detection task, we also would like to take into account one important aspect of the Affwild2 dataset, which is the fact that there are 164 videos that are being annotated with both VA and EXPR. Instead of treating all the training videos as if they are annotated with only one label like  [14] , we check if the given video frame has annotated with one or both EXPR and VA labels. Then, we compute the objective loss of the secondary task using the distillation loss alone or supervision loss plus distillation loss, respectively.\n\nLet (X, Y) be the training dataset, where X is a set of input vectors and Y is a set of ground truth training labels. Since our dataset contains three parts including: Mixed EXPR, Mixed VA and Affect EXPR_VA, therefore (X, Y) = {(X (i) , Y (i) )} 3 i=1 . For convenience of notation, we assume each subset i includes an equal number N of instances within a batch, i.e (X (i) ,\n\nwhere n indexes the instance. Because the data from the last set Affect EXPR_VA is including both EXPR and VA annotations, we denote 3 expr and 3 va as the EXPR annotation and the VA annotation of this set, respectively. For example, instance x (3,1) belongs to Affect EXPR_VA dataset and has two annotations: y (3 expr ,1) and y (3 va ,1)   The inputs for all instances have the same dimensionality, regardless of task. However, the ground truth labels for different tasks have different dimensionality. The label for the first task (EXPR) is y (1) ∈ {0, 1} 7 . The label for the second task (VA) is y (2) ∈ [-1, 1] 2 . The label for the last task (EXPR_VA) is the mixed of the two tasks above.\n\nSimilar to  [14] , we also apply the two step training for capturing inter-task correlations. We train a single teacher model using only the ground truth labels in the first step. In the second step, we replace the missing labels with soft labels derived from the output of the teacher model. We then use the ground truth and soft labels to train a single student model. Different from  [14] , we do not train multi student models for model ensemble because this approach is too costly in term of computation. We believe that by applying an appropriate training method, the model will be able to reach the same or even higher in performance, comparing to the ensemble approach. The overview of  To be in the same line with  [14]  in the sense of notation, we also denote the output of our multitask network by f (i) θ (•) where θ contains the model parameters of either teacher model or student model, and i ∈ {1, 2} indicates the current task. For example, f (1)  θ (x (3) indicates the output of the network for task 1 (EXPR) for an instance in the Affect EXPR_VA set. To avoid clutter, we will often refer to the output of the teacher network on task i by t (i) irrespective of what the input label is, i.e. t (i) = f (i)  θ (x ( j) ) for some j ∈ {1, 2, 3} and similarly to the output of the student network on task i by s (i) .\n\nRegarding the objective loss functions, similar to  [14] , we also treat the problem of expression classification as a multiclass classification problem, and the problem of valence-arousal estimation as a combination of multiclass classification and regression problem. We will use the same soft-max function S F, the cross entropy function CE and the concordance correlation coefficient function CCC, which have already been defined in  [14] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Supervision Loss Functions",
      "text": "Here we denote the loss functions that are used for optimizing our models parameters with the supervision of the ground truth labels for each of our training tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Expr Task",
      "text": "The supervision loss for the samples from the Mixed EXPR set is denoted as:\n\nL (1) (y (1) , t (1) ) = CE y (1) , S F(t (1) , 1)\n\n(\n\nVA task The supervision loss for the samples from the Mixed VA set is denoted as:\n\nCE onehot(y (2)  i ), S F(t (2)  i , 1)\n\nEXPR_VA task For the samples from Affect EXPR_VA set, since this set is annotated using both VA and EXPR, the supervision loss for this task is denoted as: ) ), 1) ) ), 1)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Distillation Loss Functions",
      "text": "Here we denote the loss functions that are used to optimize our student model parameters with the supervision of both the ground truth labels (hard targets) and the pretrained teacher model's outputs (soft targets) for each of our training tasks. Similar to  [14] , we use the KL divergence KL(p, q) = i p i log p i q i to measure the different between two probability distributions (output of teacher model and student model).\n\nEXPR task Distillation loss for the samples from the Mixed EXPR set:\n\nH (1) (t (1) , s (1) ) = KL S F(t (1) , T ), S F(s (1) , T )\n\nVA task Distillation loss for the samples from the Mixed VA set:\n\nEXPR_VA task Distillation loss for the samples from the Affect EXPR_VA set is the combination of the EXPR and VA distillation losses, which is denoted as:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Batch-Wise Loss Functions",
      "text": "Given a batch of data (X, Y) = {{(x (i,n) , y (i,n) )} N n=1 } 3 i=1 , the parameters of teacher network and student networks are denoted as θ t and θ s , respectively. Since our last dataset Affect EXPR_VA contains annotation for both EXPR and VA, therefore, when i = 3 then y (3,n) contains both y (3 expr ,n) and y (3 va ,n) .\n\nThe training teacher loss:\n\nThe loss of sample x with ground truth y from dataset i with i ∈ {1, 2, 3} will be denote as:\n\nSimilar to  [14] , we also use the parameter λ to weight the supervision loss versus the distillation loss. The λ parameter is set to 0.6 to weight the ground truth slightly more than the soft labels.\n\nThe student loss is denoted as:\n\nAs we have mentioned earlier, there are 164 videos that are annotated with both EXPR and VA in the Affwild2 database.\n\nTo exploit these sharing annotations, we propose a method to compute student loss with taking into account this characteristic, which is denoted as:\n\nAs described in Equation  10 , we can see that for each sample, instead of treating it as having only one label, we check if it contains a secondary label or not, then compute the distillation loss only or supervision loss plus distillation loss for this secondary label, correspondingly.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual Images Analysis",
      "text": "For the visual images, face images with size of height × width pixels are aligned and extracted from each video frame. Then, we use these images to train a ResNet 50 model using the method mentioned in Section 2.2. During training, we have applied some image-wise augmentation filters to improve the performance of the model. These filters are including: random image translation  [21]  and random image horizontal flip.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Information Exploiting Using Gru Network",
      "text": "Once the student model of the ResNet 50 network have been trained, we use this model to extract features from each video frame. Then, we group these features together to form a new dataset ds of feature's sequences with sequence length of 32 frame per sequence. Finally, we fed data from this new dataset ds into a bidirectional GRU network for exploiting temporal information, as well as performing seven basic emotions classification and valence-arousal estimation. Regarding the GRU model's parameters, we also use the training method in Section 2.2 to train this model's parameters. During the training, we have used the same augmentation filters that are mentioned in Section 2.3 but in sequence level.\n\n3 Experiments and Results",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "The whole network system is implemented using PyTorch framework  [22] . During the training phase, Adam optimizer  [23]  were employed with the initial learning rate is set of 1e -4 . The maximum number of epochs is 40 and the training process will stop when there is no improvement after five consecutive epochs.\n\nThe number of batch size for the CNN part of the network is set to 64. For RNN network, the batch size is 16. The training and validating processes were performed on an Intel Workstation machine with a NVIDIA Gerforce RTX 2080 Ti 11G GPU.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Here we report the results of different experiments to demonstrate the effectiveness of each of our changes compare to the original method  [14] . For the evaluation metrics, we use the same criterion as outlined in  [6] . Valence and Arousal estimation is based on the mean Concordance Correlation Coefficient (CCC). The seven basic expressions classification is measured by 0.67 × F 1 score + 0.33 × total accuracy. For each of our experiments, we run it 10 times and report the mean of the evaluation results on the Validation set of the AffWild2 dataset.\n\nTable  1  shows the performance of the teacher network when training using Equation 7 with only the first two tasks (T ∈ {1, 2}) and with all three tasks (T ∈ {1, 2, 3}). From this table, we can see that when training with only two tasks, our model has already outperformed the baseline results of the competition. This finding is in the same line with  [14] , which is the proof of the effectiveness of data balancing and multitask training method. Now, when we add the third task EXPR_VA into the training process (T ∈ {1, 2, 3}), we can see that the value of both EXPR and Valence have increased quite a lot, especiately the later with 17% of improvement. Despite of having a slightly decreasing in term of Arousal (about 2%), the performance of the network has improved in overall by a large margin, compared to the model trained without the EXPR_VA task. After training the teacher model, we train student models with the supervision of both ground truth and the pretrained teacher model using Equation  9 for the case of not using the shared annotations (No sharing), and using Equation  10 for the case of using the shared annotations (With sharing). The results are showed in Table  2 . From this table, it can be seen that the performance of the model trained using the shared annotations (With sharing) is better than the one trained without using it (No sharing). This results indicate the important of exploiting the sharing annotations in the database. Once the student model is trained, we use this CNN model to extract features to train GRU network for exploiting temporal information. We train a teacher model using Equation 7 and a student model using Equation  10 . Table  3  shows the results of these models. From this table, we can see that: the performance of student model is better the teacher model in all cases. And when comparing with the CNN model (in Table  2 ), the CNN + GRU model outperformed it by a large margin.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With State Of The Art",
      "text": "Here we compare the performance of our model with the state of the art on the validation set of Affwild2 dataset. Although in this 2nd challenge, the database has been updated by adding more videos and labels for the AU detection task. But since the data for seven basic expression detection task (EXPR) and valence-arousal estimation task (VA) are almost unchanged, we are still able to compare the performance of our model with the works on the previous challenge.\n\nTable  4  shows the comparison results between the works on the same dataset, which including Zhang et al.  [24]  with their M 3 T model, Gera et al.  [25]  with spatio-channel attention network, Deng et al.  [14]  with their multitask model trained on multiple   [27, 28, 29] , our method seems to have less complexity and time-consuming compared to their method. When we compare our results with the work of Deng et al.  [14] , which is the most closed work compared to us, we can see that our model is outperformed their model ensemble in all tasks, in the same time our model is five times faster compared to their model because their model are the combination of five models ensemble.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented a method to optimize multitask training with imcomplete labels. By adding a new task to train deep neural network on a dataset which contains both seven basic expressions and valence-arousal values, along with exploiting the shared annotations inside the Affwild2 database when training student model, resulting a model that is better than state of the art in term of seven basic expression classification and valence estimation on the validation set of the Affwild2 database.\n\nIn future work, we will investigate about multimodal network, e.g. multitask visual-aural neural network for analyzing both visual and aural streams from a video recording.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overview of our multitask training with missing",
      "page": 2
    },
    {
      "caption": "Figure 2: The model architecture of our networks.",
      "page": 2
    },
    {
      "caption": "Figure 1: and the architecture of our",
      "page": 2
    },
    {
      "caption": "Figure 2: To be in the same line with [14] in the sense of notation, we also",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Zhang et al. [24]",
          "EXPR": "-",
          "Valence": "0.32",
          "Arousal": "0.55"
        },
        {
          "Method": "Gera et al. [25]",
          "EXPR": "0.465",
          "Valence": "-",
          "Arousal": "-"
        },
        {
          "Method": "Deng et al. [14]",
          "EXPR": "0.493",
          "Valence": "0.335",
          "Arousal": "0.515"
        },
        {
          "Method": "Kuhnke et al. [26]",
          "EXPR": "0.546",
          "Valence": "0.493",
          "Arousal": "0.613"
        },
        {
          "Method": "Our model",
          "EXPR": "0.555",
          "Valence": "0.526",
          "Arousal": "0.551"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on computer vision for assistive medical diagnosis from faces",
      "authors": [
        "Jérôme Thevenot",
        "Miguel Lopez",
        "Abdenour Hadid"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2017.2754861"
    },
    {
      "citation_id": "2",
      "title": "Deep-learning-based models for pain recognition: A systematic review",
      "authors": [
        "Rasha Al-Eidan",
        "Hend Al-Khalifa",
        "Abdulmalik Al-Salman"
      ],
      "year": "2020",
      "venue": "Applied Sciences",
      "doi": "10.3390/app10175984"
    },
    {
      "citation_id": "3",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "Nicu Sebe",
        "Ira Cohen",
        "Theo Gevers",
        "Thomas Huang"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition and detection methods: A comprehensive survey",
      "authors": [
        "Anvita Saxena",
        "Ashish Khanna",
        "Deepak Gupta"
      ],
      "year": "2020",
      "venue": "Journal of Artificial Intelligence and Systems"
    },
    {
      "citation_id": "5",
      "title": "Automatic recognition methods supporting pain assessment: A survey",
      "authors": [
        "Philipp Werner",
        "Daniel Lopez-Martinez",
        "Steffen Walter",
        "Ayoub Al-Hamadi",
        "Sascha Gruss",
        "Rosalind Picard"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2019.2946774"
    },
    {
      "citation_id": "6",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "7",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "8",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "9",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "10",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "11",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "13",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Irene Kotsia",
        "Elnar Hajiyev",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Analysing affective behavior in the second abaw2 competition",
      "arxiv": "arXiv:2106.15318"
    },
    {
      "citation_id": "14",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "Multitask emotion recognition with incomplete labels"
    },
    {
      "citation_id": "15",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "16",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "17",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merrienboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation"
    },
    {
      "citation_id": "18",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition"
    },
    {
      "citation_id": "19",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "21",
      "title": "Evaluation of data augmentation techniques for facial expression recognition systems",
      "authors": [
        "Simone Porcu",
        "Alessandro Floris",
        "Luigi Atzori"
      ],
      "year": "2020",
      "venue": "Electronics"
    },
    {
      "citation_id": "22",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas Kopf",
        "Edward Yang",
        "Zachary Devito",
        "Martin Raison",
        "Alykhan Tejani",
        "Sasank Chilamkurthy",
        "Benoit Steiner",
        "Lu Fang",
        "Junjie Bai",
        "Soumith Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "24",
      "title": "t: Multi-modal continuous valence-arousal estimation in the wild",
      "authors": [
        "Yuan-Hang Zhang",
        "Rulin Huang",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2020",
      "venue": "t: Multi-modal continuous valence-arousal estimation in the wild"
    },
    {
      "citation_id": "25",
      "title": "Affect expression behaviour analysis in the wild using spatio-channel attention and complementary context information",
      "authors": [
        "Darshan Gera",
        "S Balasubramanian"
      ],
      "year": "2020",
      "venue": "Affect expression behaviour analysis in the wild using spatio-channel attention and complementary context information"
    },
    {
      "citation_id": "26",
      "title": "Twostream aural-visual affect analysis in the wild",
      "authors": [
        "Felix Kuhnke",
        "Lars Rumberg",
        "Jorn Ostermann"
      ],
      "year": "2020",
      "venue": "15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)",
      "doi": "10.1109/FG47880.2020.00056"
    },
    {
      "citation_id": "27",
      "title": "Lp-3dcnn: Unveiling local phase in 3d convolutional neural networks",
      "authors": [
        "Sudhakar Kumawat",
        "Shanmuganathan Raman"
      ],
      "year": "2019",
      "venue": "Lp-3dcnn: Unveiling local phase in 3d convolutional neural networks"
    },
    {
      "citation_id": "28",
      "title": "Efficient spatio-temporal modeling methods for real-time violence recognition",
      "authors": [
        "Min-Seok Kang",
        "Rae-Hong Park",
        "Hyung-Min Park"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "29",
      "title": "Investigation of lung cancer detection using 3d convolutional deep neural network",
      "authors": [
        "Srinivas Arukonda",
        "S Sountharrajan"
      ],
      "year": "2020",
      "venue": "2020 2nd International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)",
      "doi": "10.1109/ICACCCN51052.2020.9362857"
    }
  ]
}