{
  "paper_id": "2207.13965v1",
  "title": "Extending Rnn-T-Based Speech Recognition Systems With Emotion And Language Classification",
  "published": "2022-07-28T09:11:39Z",
  "authors": [
    "Zvi Kons",
    "Hagai Aronowitz",
    "Edmilson Morais",
    "Matheus Damasceno",
    "Hong-Kwang Kuo",
    "Samuel Thomas",
    "George Saon"
  ],
  "keywords": [
    "speech recognition",
    "language identification",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech transcription, emotion recognition, and language identification are usually considered to be three different tasks. Each one requires a different model with a different architecture and training process. We propose using a recurrent neural network transducer (RNN-T)-based speech-to-text (STT) system as a common component that can be used for emotion recognition and language identification as well as for speech recognition. Our work extends the STT system for emotion classification through minimal changes, and shows successful results on the IEMOCAP and MELD datasets. In addition, we demonstrate that by adding a lightweight component to the RNN-T module, it can also be used for language identification. In our evaluations, this new classifier demonstrates state-of-the-art accuracy for the NIST-LRE-07 dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human speech is a rich signal that carries with it a vast amount of information. In addition to words, the speech signal encodes information about the speaker's identity, language, accent, emotions, and physical state. The human brain is tuned to extract and combine all these types of information from the speech signal. Yet, from a machine learning perspective, recognizing each of these parts is considered an independent challenging problem. Usually, a separate model is trained to extract information from each part of the speech data.\n\nWith the recent advances in self-supervised learning for speech, there are many general purpose models available which have been pre-trained on large speech corpora. Such models are more easily used for several different tasks  [1, 2] . Nevertheless, in most cases, the best results are achieved by fine-tuning the model to a specific task.\n\nAutomatic speech recognition (ASR) models based on RNN-T architecture are also becoming common  [3] . These models are trained on large speech corpora, but in a supervised, task-oriented manner. The main task of these models is speech transcription, but there have been some recent attempts to expand them to incorporate other tasks such as diarization  [4]  and intent classification  [5] .\n\nIn this paper, we present a practical guide to extending an ASR-based RNN-T model to perform two additional tasks: speech emotion recognition (SER) and language identification (LID). We explain how we added these two features without affecting the performance of the original speech transcription functionality.\n\nAn ASR system that can perform several tasks at once, such as the one we propose, has many advantages over several singletask systems. First, it can generate a rich transcript. This can be helpful in many scenarios, such as call center interactions, where understanding users' emotions and identifying their language correctly can improve the interaction. From a deployment point of view, having a single model that can perform multiple tasks is a huge advantage since it requires fewer resources and less maintenance.\n\nState-of-the-art audio-based SER classifiers use audio encoding models to first extract frame-level features and then apply a classifier on them  [6] . These audio-based classifiers can benefit from additional information that can be extracted from semantic text analysis models  [7] .\n\nWe added the SER functionality to an STT model by training it to output additional emotion tags along with the text transcription; this is somewhat similar to what was done in  [5] . Using an RNN-T model for SER is very advantageous because it has both an audio analysis network and a text prediction network. This allows it to leverage multi-modal information from both audio and text when it generates the emotion tags. The SER models and the related experiments are described in Section 2.\n\nThe second part of our work focuses on automatic speech recognition for audio samples, where the underlying language is not known a priori. The commonly used approach to this problem is to first use an LID classifier to identify the correct language and then apply the corresponding, single-language STT model. Our goal was to simplify this process into a single step.\n\nAnother option for speech recognition is to use multilingual STT models. These models  [8] [9] [10]  can be trained on more than one language so they are able to identify the language and transcribe it at the same time. Multilingual STT systems can be harder to train than single language models and can require additional resources. In many cases, these models depend on internal or external LID classifiers. In addition, multilingual STT models require transcribed speech samples for training in all of their languages. This can lead to problems when a language has very few resources of this kind to use for training.\n\nIn Section 3, we describe a new method for LID classification that is based only on the audio encoder part of a singlelanguage RNN-T model. Using this model, we show stateof-the-art results for the NIST-LRE-07 dataset. With this approach, it is much easier to train the LID classifier because we do not need transcribed audio. We can also identify the language quickly without waiting for the text decoding and do this with only minimal overhead. This allows the addition of LID functionality to existing ASR systems, without any change to their STT models. arXiv:2207.13965v1 [eess.AS] 28 Jul 2022",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Joint Asr And Speech Emotion Recognition",
      "text": "Our first step was to extend an RNN-T model  [11]  to produce both a transcript of the speech and emotion tags. We started with a pre-trained US-English model that is trained to output the transcript as a sequence of symbols representing alphabetical characters. We extended the output symbols by adding a new symbol for each one of the emotions we wanted to identify. In the transcript, these symbols are translated into unique emotion tags. For example, an output transcript might look like this:\n\nwhere \"<HAPPY>\" is the emotion tag produced by the symbol that indicates happy speech.\n\nIn the training stage, for each utterance, we append the corresponding emotion tag to the end of its target text. The location of the tag within the text is important. While the RNN-T encoder is bidirectional, the prediction network is unidirectional. By placing the tag at the end of the utterance, we allow the system to accumulate information from both the speech and the transcribed text before it makes a decision. We trained the system to output both the target text and the tag at the end. We did this using the RNN-T loss function in the same way as it was initially trained to output only the transcript  [11] .\n\nIn the course of our work, we found that the values of the loss function were not good indicators for the accuracy of the emotion classification. Consequently, we calculated the accuracy of the emotion classification over a development set, and used this as a stop criterion and for best model selection.\n\nDuring inference, we used our modified STT model to generate the text transcript with the emotion tags. For each utterance, we searched the output text for the last emotion tag that was output. This tag was then used as the predicted emotion for the utterance. If no tag was output, we labeled the utterance as neutral.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iemocap Dataset",
      "text": "Our first experiment was done with the IEMOCAP dataset  [12] . The dataset is split into five sessions, which allowed us to do a five-fold cross validation by training on four sessions and testing on the fifth. Out of the seven emotions labeled in the dataset, we used only four: neutral, happy, angry, and sad. We randomly split the training data into training and development sets. The development set was used during the training for the stop criterion and for selecting the best model.\n\nWe measured the emotion classification accuracy over the test set and calculated the character error rate (CER) and word error rate (WER) from the generated transcript. We then compared the CER and WER to the baseline model to verify that our training did not make the transcription less accurate.\n\nThe results of the experiments are presented in Table  1 . The first line shows the baseline CER and WER for the speech transcription, using the baseline STT before any of our training. The second line shows the results after we fine tuned the whole model. As can be seen, the new model provides good accuracy on emotion classification while also improving the transcription accuracy.\n\nIn the next experiment, we repeated the training but the acoustic encoding network was frozen, so only the prediction and joint networks were tuned. This is somewhat similar to what we describe in Section 3 where we use the encoder for LID. This experiment was designed to examine whether we could attain good emotion classification without acoustic adaptation to the speech in the dataset. The results show a considerable drop of more than 10% in accuracy.\n\nFor comparison, we also include state-of-the-art results that were achieved on the same data, using LID classifiers build on top of the wav2vec 2.0 and HuBERT encoders, reported in  [7]  and  [2] . These results were achieved by a single task systems that were trained to produce only emotion classification. As can be seen, the results we report in this paper are close, but there is still a gap to the state-of-the-art models.\n\nThe IEMOCAP data is a combination of scripted and improvised sessions. In the scripted part, many phrases are common to different sessions. We wanted to verify that the classifier does not simply learn the emotions from the text, so we repeated the above experiments on each type of session independently. The results are also presented in Table  1 . We found only a small difference in the accuracy of the emotion classification between the two session types. This means that the models learn relevant audio features and not just the text.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": ". Meld Dataset",
      "text": "Our second experiment was done with the MELD dataset  [13] .\n\nOut of the seven emotions labeled in the dataset, we used only four: neutral, joy (happy), angry, and sad. We divided the MELD dataset into train, development, and test sets following the standard data split suggested in  [13] . We repeated the same experiments measuring emotion classification accuracy, CER, and WER. The results of the experiment are shown in Table  2 . The first three lines show the same experiment as in the previous table. Again, the results show good emotion classification accuracy with an improvement in the transcription accuracy. However, there is a considerable drop of more than 4% in the accuracy when the acoustic encoder network is not trained.\n\nFor comparison, we also include state-of-the-art results that were achieved in our previous experiments using the wav2vec 2.0 and HuBERT encoders following the same experimental method described in  [7] . Here too, our current accuracy is still a bit lower than the state-of-the-art.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Language Identification",
      "text": "We create a language classifier by incorporating a lightweight classifier into an RNN-T network. The classifier's input is the output from the acoustic encoder part of the RNN-T model (Figure  1 ). The role of the encoder network is to convert audio into frame-level embedding features that are useful for STT. This is somewhat similar to what is done by self-supervised speech rep- resentation models such as HuBERT  [14]  or wav2vec 2.0  [15] . In contrast, the RNN-T encoder is trained in a supervised manner so its output features are likely to contain more phonetic and linguistic information and less information about the speaker and the acoustic channel. These features are highly useful for LID.\n\nFigure  1 : Structure of the LID classifier network.\n\nThere are several advantages to our configuration over other types of LID classifiers or multilingual STT. First, the training process can be much faster because we already have a trained RNN-T model, so we only need to train a lightweight classifier network. Second, as we show in Section 3.1, we do not need a multilingual STT. We can use an encoder from a singlelanguage RNN-T model and only train the classifier on the required languages. For the classifier training, we need training samples labeled by their languages. We do not need transcriptions of those samples. This makes it much easier to obtain the required data.\n\nAnother advantage is that we do not need to run the text decoding part of the RNN-T model, which is much slower than running just the encoder. This is useful, for example, if our STT is expecting a default language. We apply the encoder on the input speech and before continuing to the decoding, we can verify that the input is in the correct language. If it is, then we can continue with the decoding using the encoder output that we already have. If not, we can then switch to an STT model for the correct language.\n\nThe classifier structure is shown in Figure  2 . The output of the RNN-T encoder is processed by a bidirectional long shortterm memory (LSTM) layer. This is followed by a multi-head weighted-average pooling layer:\n\nwhere xt is the LSTM output vector at time t, wt is the weight vector for this frame, Prn() are linear projections, σ() is the log-sigmoid function, and y is the pooled weighted average. The final score for each language is calculated from y using a linear projection and a SoftMax function.\n\nFigure  2 : The structure of the classifier network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments With The Nist-Lre-07 Dataset",
      "text": "We tested our LID classifier on the NIST-LRE-07 dataset  [16] .\n\nWe followed a similar procedure to the one in  [2] , except we used the 8KHz audio. The dataset contains training and test data for 14 languages. We applied the acoustic encoder part of a pre-trained, US-English, RNN-T model on each one of these samples and used the output features to train and test the classifier. Part of the training data was held out to form a validation set that was used for a stopping criterion and for selecting the best model.\n\nTable  3  reports our results measured in equal-error-rate (EER). For comparison we add two more experiments. In the first, we use the same system but this time allow fine-tuning of the RNN-T encoder network. This makes the LID classifier more accurate but now, the encoder part cannot be reused for text decoding.\n\nFor the second experiment, we replace the RNN-T encoder network with an HuBERT network and fine-tune the whole network. This is similar to the experiments in  [2]  but this time we use LSTM with weighted average pooling as in (1) and (2) instead of a simple mean pooling. For easier comparison, the same results are also shown in Figure  3  on a log-log scale.\n\nThe results demonstrate that our model performs better for speech with longer durations. Apparently, for longer durations, the ability of the STT encoder to capture phonetic information becomes very useful. However, for shorter durations, the Hu-BERT model may be able to capture different information, such as an accent. This is likely what helps it identify the language even before enough words are spoken.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we showed that we can extend an RNN-T-based STT model to perform additional tasks, such as emotion classification and language identification. This brings us closer to the goal of a single ASR system that can perform multiple tasks and produce a rich transcript. This system might not be as good in one task as a single-task system, but it still provides significant benefits in term of training, deployment, maintenance, and user experience.\n\nOur results for the emotion classification are currently not as good as other state-of-the-art classifiers. This might be because the original training of the STT model causes it to ignore much of the information in the speech that is useful for correctly identifying the emotions. When we adapt the model for emotion classification, it fails to properly learn how to extract the required information. Additional research is needed to overcome this problem by using a larger training dataset or by combining emotion classification objectives into the original training of the STT.\n\nOn the other hand, we found that the RNN-T model is very powerful for language classification and can produce results that match and even surpass previous state-of-the-art models. Additional work is needed to improve the results for short speech segments and for using this ability to create multilingual STT models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Structure of the LID classiﬁer network.",
      "page": 3
    },
    {
      "caption": "Figure 2: The output of",
      "page": 3
    },
    {
      "caption": "Figure 2: The structure of the classiﬁer network.",
      "page": 3
    },
    {
      "caption": "Figure 3: on a log-log scale.",
      "page": 3
    },
    {
      "caption": "Figure 3: EER for the LID classiﬁer vs. speech duration (log-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IBM Research AI": "hidden.zvi@il.ibm.com.hidden"
        },
        {
          "IBM Research AI": "Abstract"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "Speech transcription, emotion recognition, and language identi-"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "ﬁcation are usually considered to be three different tasks. Each"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "one requires a different model with a different architecture and"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "training process. We propose using a recurrent neural network"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "transducer\n(RNN-T)-based speech-to-text\n(STT)\nsystem as a"
        },
        {
          "IBM Research AI": "common component\nthat can be used for emotion recognition"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "and language identiﬁcation as well as for speech recognition."
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "Our work extends\nthe STT system for emotion classiﬁcation"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "through minimal changes, and shows successful results on the"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "IEMOCAP and MELD datasets.\nIn addition, we demonstrate"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "that by adding a lightweight component to the RNN-T module,"
        },
        {
          "IBM Research AI": "it can also be used for\nlanguage identiﬁcation.\nIn our evalua-"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "tions,\nthis new classiﬁer demonstrates state-of-the-art accuracy"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "for the NIST-LRE-07 dataset."
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "Index Terms:\nspeech\nrecognition,\nlanguage\nidentiﬁcation,"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "emotion recognition"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "1.\nIntroduction"
        },
        {
          "IBM Research AI": ""
        },
        {
          "IBM Research AI": "Human speech is a rich signal that carries with it a vast amount"
        },
        {
          "IBM Research AI": "of information. In addition to words, the speech signal encodes"
        },
        {
          "IBM Research AI": "information about the speaker’s identity, language, accent, emo-"
        },
        {
          "IBM Research AI": "tions, and physical state. The human brain is tuned to extract"
        },
        {
          "IBM Research AI": "and combine all these types of information from the speech sig-"
        },
        {
          "IBM Research AI": "nal. Yet, from a machine learning perspective, recognizing each"
        },
        {
          "IBM Research AI": "of\nthese parts is considered an independent challenging prob-"
        },
        {
          "IBM Research AI": "lem. Usually, a separate model is trained to extract information"
        },
        {
          "IBM Research AI": "from each part of the speech data."
        },
        {
          "IBM Research AI": "With the recent advances\nin self-supervised learning for"
        },
        {
          "IBM Research AI": "speech, there are many general purpose models available which"
        },
        {
          "IBM Research AI": "have been pre-trained on large speech corpora. Such models are"
        },
        {
          "IBM Research AI": "more easily used for several different tasks [1, 2]. Nevertheless,"
        },
        {
          "IBM Research AI": "in most cases,\nthe best\nresults are achieved by ﬁne-tuning the"
        },
        {
          "IBM Research AI": "model to a speciﬁc task."
        },
        {
          "IBM Research AI": "Automatic\nspeech\nrecognition\n(ASR) models\nbased\non"
        },
        {
          "IBM Research AI": "RNN-T architecture are also becoming common [3].\nThese"
        },
        {
          "IBM Research AI": "models are trained on large speech corpora, but in a supervised,"
        },
        {
          "IBM Research AI": "task-oriented manner. The main task of these models is speech"
        },
        {
          "IBM Research AI": "transcription, but\nthere have been some recent attempts to ex-"
        },
        {
          "IBM Research AI": "pand them to incorporate other tasks such as diarization [4] and"
        },
        {
          "IBM Research AI": "intent classiﬁcation [5]."
        },
        {
          "IBM Research AI": "In this paper, we present a practical guide\nto extending"
        },
        {
          "IBM Research AI": "an ASR-based RNN-T model\nto perform two additional\ntasks:"
        },
        {
          "IBM Research AI": "speech emotion recognition (SER) and language identiﬁcation"
        },
        {
          "IBM Research AI": "(LID). We explain how we added these two features without"
        },
        {
          "IBM Research AI": "affecting the performance of\nthe original speech transcription"
        },
        {
          "IBM Research AI": "functionality."
        },
        {
          "IBM Research AI": "An ASR system that can perform several tasks at once, such"
        },
        {
          "IBM Research AI": "as the one we propose, has many advantages over several single-"
        },
        {
          "IBM Research AI": "task systems. First,\nit can generate a rich transcript. This can"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "could attain good emotion classiﬁcation without acoustic adap-"
        },
        {
          "2.\nJoint ASR and speech emotion": "recognition",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "tation to the speech in the dataset. The results show a consider-"
        },
        {
          "2.\nJoint ASR and speech emotion": "Our ﬁrst step was to extend an RNN-T model [11] to produce",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "able drop of more than 10% in accuracy."
        },
        {
          "2.\nJoint ASR and speech emotion": "both a transcript of\nthe speech and emotion tags. We started",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "For comparison, we also include state-of-the-art results that"
        },
        {
          "2.\nJoint ASR and speech emotion": "with a pre-trained US-English model that is trained to output the",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "were achieved on the same data, using LID classiﬁers build on"
        },
        {
          "2.\nJoint ASR and speech emotion": "transcript as a sequence of symbols representing alphabetical",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "top of the wav2vec 2.0 and HuBERT encoders, reported in [7]"
        },
        {
          "2.\nJoint ASR and speech emotion": "characters. We extended the output symbols by adding a new",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "and [2]. These results were achieved by a single task systems"
        },
        {
          "2.\nJoint ASR and speech emotion": "symbol for each one of the emotions we wanted to identify.\nIn",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "that were trained to produce only emotion classiﬁcation. As can"
        },
        {
          "2.\nJoint ASR and speech emotion": "the transcript, these symbols are translated into unique emotion",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "be seen, the results we report in this paper are close, but there is"
        },
        {
          "2.\nJoint ASR and speech emotion": "tags. For example, an output transcript might look like this:",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "still a gap to the state-of-the-art models."
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "The IEMOCAP data is a combination of scripted and im-"
        },
        {
          "2.\nJoint ASR and speech emotion": "I\nFEEL HAPPY TODAY <HAPPY>",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "provised sessions.\nIn the scripted part, many phrases are com-"
        },
        {
          "2.\nJoint ASR and speech emotion": "where “<HAPPY>” is the emotion tag produced by the symbol",
          "LID. This experiment was designed to examine whether we": "mon to different sessions. We wanted to verify that\nthe clas-"
        },
        {
          "2.\nJoint ASR and speech emotion": "that indicates happy speech.",
          "LID. This experiment was designed to examine whether we": "siﬁer does not simply learn the emotions from the text, so we"
        },
        {
          "2.\nJoint ASR and speech emotion": "In the training stage, for each utterance, we append the cor-",
          "LID. This experiment was designed to examine whether we": "repeated the above experiments on each type of session inde-"
        },
        {
          "2.\nJoint ASR and speech emotion": "responding emotion tag to the end of its target text. The location",
          "LID. This experiment was designed to examine whether we": "pendently. The results are also presented in Table 1. We found"
        },
        {
          "2.\nJoint ASR and speech emotion": "of\nthe tag within the text\nis important. While the RNN-T en-",
          "LID. This experiment was designed to examine whether we": "only a small difference in the accuracy of the emotion classiﬁca-"
        },
        {
          "2.\nJoint ASR and speech emotion": "coder is bidirectional,\nthe prediction network is unidirectional.",
          "LID. This experiment was designed to examine whether we": "tion between the two session types. This means that the models"
        },
        {
          "2.\nJoint ASR and speech emotion": "By placing the tag at the end of the utterance, we allow the sys-",
          "LID. This experiment was designed to examine whether we": "learn relevant audio features and not just the text."
        },
        {
          "2.\nJoint ASR and speech emotion": "tem to accumulate information from both the speech and the",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "transcribed text before it makes a decision. We trained the sys-",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Dataset\nExperiment\nEmotion (%)\nCER (%)\nWER (%)"
        },
        {
          "2.\nJoint ASR and speech emotion": "tem to output both the target text and the tag at the end. We did",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Baseline STT\n17.0\n24.2"
        },
        {
          "2.\nJoint ASR and speech emotion": "this using the RNN-T loss function in the same way as it was",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Full\nWhole model\n72.0\n11.4\n20.8"
        },
        {
          "2.\nJoint ASR and speech emotion": "initially trained to output only the transcript [11].",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Encoder frozen\n58.2\n15.7\n26.7"
        },
        {
          "2.\nJoint ASR and speech emotion": "In the course of our work, we found that\nthe values of the",
          "LID. This experiment was designed to examine whether we": "wav2vec 2.0\n76.5"
        },
        {
          "2.\nJoint ASR and speech emotion": "loss function were not good indicators for the accuracy of the",
          "LID. This experiment was designed to examine whether we": "HuBERT\n75.2"
        },
        {
          "2.\nJoint ASR and speech emotion": "emotion classiﬁcation. Consequently, we calculated the accu-",
          "LID. This experiment was designed to examine whether we": "Baseline STT\n12.5\n19.9"
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Scripted"
        },
        {
          "2.\nJoint ASR and speech emotion": "racy of the emotion classiﬁcation over a development set, and",
          "LID. This experiment was designed to examine whether we": "Whole model\n71.5\n6.8\n15.4"
        },
        {
          "2.\nJoint ASR and speech emotion": "used this as a stop criterion and for best model selection.",
          "LID. This experiment was designed to examine whether we": "Baseline STT\n20.9\n28.1"
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Improvised"
        },
        {
          "2.\nJoint ASR and speech emotion": "During inference, we used our modiﬁed STT model to gen-",
          "LID. This experiment was designed to examine whether we": "Whole model\n73.1\n17.1\n27.1"
        },
        {
          "2.\nJoint ASR and speech emotion": "erate the text\ntranscript with the emotion tags. For each utter-",
          "LID. This experiment was designed to examine whether we": "Table 1: Experiment results for the IEMOCAP dataset"
        },
        {
          "2.\nJoint ASR and speech emotion": "ance, we searched the output\ntext for the last emotion tag that",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "was output. This tag was then used as the predicted emotion for",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "the utterance.\nIf no tag was output, we labeled the utterance as",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "neutral.",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "2.1.2. MELD dataset"
        },
        {
          "2.\nJoint ASR and speech emotion": "2.1. Experimental evaluation",
          "LID. This experiment was designed to examine whether we": "Our second experiment was done with the MELD dataset [13]."
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "Out of the seven emotions labeled in the dataset, we used only"
        },
        {
          "2.\nJoint ASR and speech emotion": "2.1.1.\nIEMOCAP dataset",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "four:\nneutral,\njoy (happy),\nangry,\nand sad. We divided the"
        },
        {
          "2.\nJoint ASR and speech emotion": "Our ﬁrst experiment was done with the IEMOCAP dataset [12].",
          "LID. This experiment was designed to examine whether we": "MELD dataset\ninto train, development, and test sets following"
        },
        {
          "2.\nJoint ASR and speech emotion": "The dataset is split into ﬁve sessions, which allowed us to do a",
          "LID. This experiment was designed to examine whether we": "the standard data split suggested in [13]. We repeated the same"
        },
        {
          "2.\nJoint ASR and speech emotion": "ﬁve-fold cross validation by training on four sessions and test-",
          "LID. This experiment was designed to examine whether we": "experiments measuring emotion classiﬁcation accuracy, CER,"
        },
        {
          "2.\nJoint ASR and speech emotion": "ing on the ﬁfth. Out of the seven emotions labeled in the dataset,",
          "LID. This experiment was designed to examine whether we": "and WER."
        },
        {
          "2.\nJoint ASR and speech emotion": "we used only four: neutral, happy, angry, and sad. We randomly",
          "LID. This experiment was designed to examine whether we": "The results of the experiment are shown in Table 2. The ﬁrst"
        },
        {
          "2.\nJoint ASR and speech emotion": "split\nthe training data into training and development sets. The",
          "LID. This experiment was designed to examine whether we": "three lines show the same experiment as in the previous table."
        },
        {
          "2.\nJoint ASR and speech emotion": "development set was used during the training for the stop crite-",
          "LID. This experiment was designed to examine whether we": "Again,\nthe results show good emotion classiﬁcation accuracy"
        },
        {
          "2.\nJoint ASR and speech emotion": "rion and for selecting the best model.",
          "LID. This experiment was designed to examine whether we": "with an improvement\nin the transcription accuracy. However,"
        },
        {
          "2.\nJoint ASR and speech emotion": "We measured the emotion classiﬁcation accuracy over the",
          "LID. This experiment was designed to examine whether we": "there is a considerable drop of more than 4% in the accuracy"
        },
        {
          "2.\nJoint ASR and speech emotion": "test set and calculated the character error rate (CER) and word",
          "LID. This experiment was designed to examine whether we": "when the acoustic encoder network is not trained."
        },
        {
          "2.\nJoint ASR and speech emotion": "error rate (WER) from the generated transcript. We then com-",
          "LID. This experiment was designed to examine whether we": "For comparison, we also include state-of-the-art results that"
        },
        {
          "2.\nJoint ASR and speech emotion": "pared the CER and WER to the baseline model\nto verify that",
          "LID. This experiment was designed to examine whether we": "were achieved in our previous experiments using the wav2vec"
        },
        {
          "2.\nJoint ASR and speech emotion": "our training did not make the transcription less accurate.",
          "LID. This experiment was designed to examine whether we": "2.0 and HuBERT encoders\nfollowing the same experimental"
        },
        {
          "2.\nJoint ASR and speech emotion": "The results of the experiments are presented in Table 1. The",
          "LID. This experiment was designed to examine whether we": "method described in [7]. Here too, our current accuracy is still"
        },
        {
          "2.\nJoint ASR and speech emotion": "ﬁrst line shows the baseline CER and WER for the speech tran-",
          "LID. This experiment was designed to examine whether we": "a bit lower than the state-of-the-art."
        },
        {
          "2.\nJoint ASR and speech emotion": "scription, using the baseline STT before any of our\ntraining.",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "The second line shows the results after we ﬁne tuned the whole",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "",
          "LID. This experiment was designed to examine whether we": "3. Language identiﬁcation"
        },
        {
          "2.\nJoint ASR and speech emotion": "model. As can be seen, the new model provides good accuracy",
          "LID. This experiment was designed to examine whether we": ""
        },
        {
          "2.\nJoint ASR and speech emotion": "on emotion classiﬁcation while also improving the transcription",
          "LID. This experiment was designed to examine whether we": "We create a language classiﬁer by incorporating a lightweight"
        },
        {
          "2.\nJoint ASR and speech emotion": "accuracy.",
          "LID. This experiment was designed to examine whether we": "classiﬁer into an RNN-T network. The classiﬁer’s input\nis the"
        },
        {
          "2.\nJoint ASR and speech emotion": "In the next experiment, we repeated the training but\nthe",
          "LID. This experiment was designed to examine whether we": "output from the acoustic encoder part of the RNN-T model (Fig-"
        },
        {
          "2.\nJoint ASR and speech emotion": "acoustic encoding network was frozen, so only the prediction",
          "LID. This experiment was designed to examine whether we": "ure 1). The role of the encoder network is to convert audio into"
        },
        {
          "2.\nJoint ASR and speech emotion": "and joint networks were tuned.\nThis\nis\nsomewhat\nsimilar\nto",
          "LID. This experiment was designed to examine whether we": "frame-level embedding features that are useful for STT. This is"
        },
        {
          "2.\nJoint ASR and speech emotion": "what we describe in Section 3 where we use the encoder\nfor",
          "LID. This experiment was designed to examine whether we": "somewhat similar to what is done by self-supervised speech rep-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: reports our results measured in equal-error-rate",
      "data": [
        {
          "(cid:80)": "t ewt ReLU(Pr3(xt))"
        },
        {
          "(cid:80)": "y =\n(2)"
        },
        {
          "(cid:80)": "(cid:80)"
        },
        {
          "(cid:80)": "t ewt"
        },
        {
          "(cid:80)": ""
        },
        {
          "(cid:80)": "is the weight\nwhere xt\nis the LSTM output vector at time t, wt"
        },
        {
          "(cid:80)": ""
        },
        {
          "(cid:80)": "vector\nfor\nthis frame, Prn() are linear projections, σ() is the"
        },
        {
          "(cid:80)": ""
        },
        {
          "(cid:80)": "log-sigmoid function,\nand y is\nthe pooled weighted average."
        },
        {
          "(cid:80)": ""
        },
        {
          "(cid:80)": "The ﬁnal score for each language is calculated from y using"
        },
        {
          "(cid:80)": ""
        },
        {
          "(cid:80)": "a linear projection and a SoftMax function."
        },
        {
          "(cid:80)": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[1]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "RNN-T LID\n13.9\n6.4\n4.9\n0.56\n0.14",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "Y\n. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "RNN-T LID ﬁne-tuned\n10.4\n5.1\n3.3\n0.44\n0.19",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "W.-C. Tseng, K.\ntik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "HuBERT\n9.9\n4.1\n3.6\n1.0\n0.2",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "Li, S. Watanabe, A. Mohamed, and H. yi Lee, “SUPERB: Speech"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "Table 3: LID results (EER in %) for the NIST-LRE-07 tests",
          "5. References": "Processing Universal PERformance Benchmark,” in Proc. Inter-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "speech 2021, 2021, pp. 1194–1198."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[2] H. Aronowitz, I. Gat, E. Morais, W. Zhu, and R. Hoory, “Towards"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "a common speech analysis engine,” 2022.\n[Online]. Available:"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "https://arxiv.org/abs/2203.00613"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[3] A.\nGraves,\n“Sequence\ntransduction\nwith\nrecurrent\nneural"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "networks,” 2012. [Online]. Available: https://arxiv.org/abs/1211."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "3711"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[4]\nL.\nE.\nShafey,\nH.\nSoltau,\nand\nI.\nShafran,\n“Joint\nspeech"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "recognition and speaker diarization via sequence transduction,”"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "Interspeech\nin\n2019.\nISCA, Sep.\n2019.\n[Online]. Available:"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "https://doi.org/10.21437/interspeech.2019-1943"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[5]\nS. Thomas, H.-K. J. Kuo, G. Saon, Z. T¨uske, B. Kingsbury, G. Ku-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "rata, Z. Kons, and R. Hoory, “RNN transducer models for spo-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "ken language understanding,” in ICASSP 2021 - 2021 IEEE Inter-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "national Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "(ICASSP), 2021, pp. 7493–7497."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[6]\nS.\nPoria,\nE.\nCambria,\nR.\nBajpai,\nand\nA.\nHussain,\n“A"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "review of\naffective\ncomputing:\nFrom unimodal\nanalysis\nto"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "Information\nmultimodal\nfusion,”\nFusion,\nvol.\n37,\npp.\n98–"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "125, 2017.\n[Online]. Available:\nhttps://www.sciencedirect.com/"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "science/article/pii/S1566253517300738"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[7]\nE. Morais, R. Hoory, W. Zhu,\nI. Gat, M. Damasceno,\nand"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "H. Aronowitz, “Speech emotion recognition using self-supervised"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "features,” 2022.\n[Online]. Available:\nhttps://arxiv.org/abs/2202."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "03896"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[8] V. Joshi, A. Das, E. Sun, R. Mehta, J. Li, and Y. Gong, “Multi-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "ple softmax architecture for streaming multilingual end-to-end asr"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "systems,” in Interspeech 2021, August 2021."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "speech duration (log-\nFigure 3: EER for the LID classiﬁer vs.",
          "5. References": "[9]\nS. Punjabi, H. Arsikere, Z. Raeesy, C. Chandak, N. Bhave,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "log scale)",
          "5. References": "A. Bansal, M. M¨uller, S. Murillo, A. Rastrow, S. Garimella,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "R. Maas,\nM.\nHans,\nA. Mouchtaris,\nand\nS.\nKunzmann,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "“Streaming end-to-end bilingual asr systems with joint\nlanguage"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "identiﬁcation,” 2020.\n[Online]. Available:\nhttps://arxiv.org/abs/"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "2007.03900"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "4. Conclusions",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[10] A. Waters, N. Gaur, P. Haghani, P. Moreno, and Z. Qu, “Lever-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "aging language id in multilingual end-to-end speech recognition,”"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "In this paper, we showed that we can extend an RNN-T-based",
          "5. References": "in 2019 IEEE Automatic Speech Recognition and Understanding"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "STT model to perform additional tasks, such as emotion classi-",
          "5. References": "Workshop (ASRU), 2019, pp. 928–935."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "ﬁcation and language identiﬁcation. This brings us closer to the",
          "5. References": "[11] G. Saon, Z. T¨uske, D. Bolanos, and B. Kingsbury, “Advancing rnn"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "goal of a single ASR system that can perform multiple tasks and",
          "5. References": "transducer technology for speech recognition,” in ICASSP 2021 -"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "produce a rich transcript. This system might not be as good in",
          "5. References": "2021 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "Signal Processing (ICASSP), 2021, pp. 5654–5658."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "one task as a single-task system, but it still provides signiﬁcant",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "beneﬁts in term of training, deployment, maintenance, and user",
          "5. References": "[12] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP:"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "experience.",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "interactive emotional dyadic motion capture database,” Language"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "Our results for the emotion classiﬁcation are currently not",
          "5. References": "Resources and Evaluation, vol. 42, no. 4, pp. 335–359, Nov. 2008."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "as good as other state-of-the-art classiﬁers. This might be be-",
          "5. References": "[Online]. Available: https://doi.org/10.1007/s10579-008-9076-6"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "cause the original training of the STT model causes it to ignore",
          "5. References": "[13]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "much of the information in the speech that is useful for correctly",
          "5. References": "R. Mihalcea, “MELD: A multimodal multi-party dataset for emo-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "identifying the emotions. When we adapt\nthe model for emo-",
          "5. References": "tion recognition in conversations,” CoRR, vol. abs/1810.02508,"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "2018. [Online]. Available: http://arxiv.org/abs/1810.02508"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "tion classiﬁcation, it fails to properly learn how to extract the re-",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "quired information. Additional research is needed to overcome",
          "5. References": "[14] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhut-"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "dinov,\nand A. Mohamed,\n“HuBERT:\nSelf-supervised\nspeech"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "this problem by using a larger training dataset or by combining",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "representation learning by masked prediction of hidden units,”"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "emotion classiﬁcation objectives into the original training of the",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "2021. [Online]. Available: https://arxiv.org/abs/2106.07447"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "STT.",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "[15] A. Baevski, H. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "On the other hand, we found that the RNN-T model is very",
          "5. References": "2.0:\nA framework\nfor\nself-supervised\nlearning\nof\nspeech"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "powerful for language classiﬁcation and can produce results that",
          "5. References": "representations,” 2020. [Online]. Available: https://arxiv.org/abs/"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "2006.11477"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "match and even surpass previous state-of-the-art models. Addi-",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "tional work is needed to improve the results for short speech",
          "5. References": "[16] Martin, Alvin and Le, Audrey, “2007 nist\nlanguage recognition"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "evaluation test set,” 2009. [Online]. Available: https://catalog.ldc."
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "segments and for using this ability to create multilingual STT",
          "5. References": ""
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "",
          "5. References": "upenn.edu/LDC2009S04"
        },
        {
          "System\n1s\n2s\n3s\n10s\n30s": "models.",
          "5. References": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Li",
        "A Watanabe",
        "H Mohamed",
        "Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Towards a common speech analysis engine",
      "authors": [
        "H Aronowitz",
        "I Gat",
        "E Morais",
        "W Zhu",
        "R Hoory"
      ],
      "year": "2022",
      "venue": "Towards a common speech analysis engine"
    },
    {
      "citation_id": "5",
      "title": "Sequence transduction with recurrent neural networks",
      "authors": [
        "A Graves"
      ],
      "year": "2012",
      "venue": "Sequence transduction with recurrent neural networks"
    },
    {
      "citation_id": "6",
      "title": "Joint speech recognition and speaker diarization via sequence transduction",
      "authors": [
        "L Shafey",
        "H Soltau",
        "I Shafran"
      ],
      "year": "2019",
      "venue": "Interspeech 2019. ISCA",
      "doi": "10.21437/interspeech.2019-1943"
    },
    {
      "citation_id": "7",
      "title": "RNN transducer models for spoken language understanding",
      "authors": [
        "S Thomas",
        "H.-K Kuo",
        "G Saon",
        "Z Tüske",
        "B Kingsbury",
        "G Kurata",
        "Z Kons",
        "R Hoory"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "Speech emotion recognition using self-supervised features"
    },
    {
      "citation_id": "10",
      "title": "Multiple softmax architecture for streaming multilingual end-to-end asr systems",
      "authors": [
        "V Joshi",
        "A Das",
        "E Sun",
        "R Mehta",
        "J Li",
        "Y Gong"
      ],
      "year": "2021",
      "venue": "Interspeech 2021"
    },
    {
      "citation_id": "11",
      "title": "Streaming end-to-end bilingual asr systems with joint language identification",
      "authors": [
        "S Punjabi",
        "H Arsikere",
        "Z Raeesy",
        "C Chandak",
        "N Bhave",
        "A Bansal",
        "M Müller",
        "S Murillo",
        "A Rastrow",
        "S Garimella",
        "R Maas",
        "M Hans",
        "A Mouchtaris",
        "S Kunzmann"
      ],
      "year": "2020",
      "venue": "Streaming end-to-end bilingual asr systems with joint language identification"
    },
    {
      "citation_id": "12",
      "title": "Leveraging language id in multilingual end-to-end speech recognition",
      "authors": [
        "A Waters",
        "N Gaur",
        "P Haghani",
        "P Moreno",
        "Z Qu"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "13",
      "title": "Advancing rnn transducer technology for speech recognition",
      "authors": [
        "G Saon",
        "Z Tüske",
        "D Bolanos",
        "B Kingsbury"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "15",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "17",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "18",
      "title": "2007 nist language recognition evaluation test set",
      "authors": [
        "Alvin Martin",
        "Audrey Le"
      ],
      "year": "2009",
      "venue": "2007 nist language recognition evaluation test set"
    }
  ]
}