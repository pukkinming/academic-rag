{
  "paper_id": "2503.20919v1",
  "title": "Gatedxlstm: A Multimodal Affective Computing Approach For Emotion Recognition In Conversations",
  "published": "2025-03-26T18:46:18Z",
  "authors": [
    "Yupei Li",
    "Qiyang Sun",
    "Sunil Munthumoduku Krishna Murthy",
    "Emran Alturki",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Affective Computing(AC)",
    "Emotion Recognition in Conversation (ERC)",
    "Multimodal Emotion Recognition (MER)",
    "Extended Long Short-Term Memory (xLSTM)",
    "Gated Mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective Computing (AC) is essential for advancing Artificial General Intelligence (AGI), with emotion recognition serving as a key component. However, human emotions are inherently dynamic, influenced not only by an individual's expressions but also by interactions with others, and single-modality approaches often fail to capture their full dynamics. Multimodal Emotion Recognition (MER) leverages multiple signals but traditionally relies on utterance-level analysis, overlooking the dynamic nature of emotions in conversations. Emotion Recognition in Conversation (ERC) addresses this limitation, yet existing methods struggle to align multimodal features and explain why emotions evolve within dialogues. To bridge this gap, we propose GatedxLSTM, a novel speech-text multimodal ERC model that explicitly considers voice and transcripts of both the speaker and their conversational partner(s) to identify the most influential sentences driving emotional shifts. By integrating Contrastive Language-Audio Pretraining (CLAP) for improved cross-modal alignment and employing a gating mechanism to emphasise emotionally impactful utterances, GatedxLSTM enhances both interpretability and performance. Additionally, the Dialogical Emotion Decoder (DED) refines emotion predictions by modelling contextual dependencies. Experiments on the IEMOCAP dataset demonstrate that GatedxLSTM achieves state-of-the-art (SOTA) performance among open-source methods in four-class emotion classification. These results validate its effectiveness for ERC applications and provide an interpretability analysis from a psychological perspective.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Artificial General Intelligence (AGI) represents a key future direction in AI development, with Affective Computing (AC) playing a crucial role in enhancing AGI's ability to interact effectively with humans. Sun et al.  [1]   (e-mail:yupei.li22@imperial.ac.uk;q.sun23@imperial.ac.uk; e.alturki24@imperial.ac.uk).\n\nSunil Munthumoduku Krishna Murthy is with CHI -Chair of Health Informatics, MRI, Technical University of Munich, Germany (e-mail: sunil.munthumoduku@tum.de).\n\nBjörn W. Schuller is with GLAM, Department of Computing, Imperial College London, UK; CHI -Chair of Health Informatics, Technical University of Munich, Germany; relAI -the Konrad Zuse School of Excellence in Reliable AI, Munich, Germany; MDSI -Munich Data Science Institute, Munich, Germany; and MCML -Munich Center for Machine Learning, Munich, Germany (e-mail: schuller@tum.de).\n\nYupei Li and Qiyang Sun contributed equally to this work.\n\nand essential technique within AC. Emotional recognition has been extensively explored in the literature. For instance, Venkataramanan et al.  [2]  examine various audio features to predict emotions, while text-based emotion recognition has also been developed  [3] . However, relying on a single modality is insufficient for capturing emotions, as they are expressed through audio, text, and visual cues from different perspectives  [4] . Consequently, Multimodal Emotion Recognition (MER) has gained attention for its ability to incorporate diverse emotional expressions, for example audio and text, in order to mitigate misinterpretation. MER encompasses various model types, which differ based on the granularity of the corpus. Word-level MER models, for instance, focus on short phrases. Local attention mechanisms have been applied to frame-level acoustic features in Recurrent Neural Networks (RNNs)  [5] , and word clustering models have also been utilised  [6] . These approaches often enhance recognition speed by prioritising key emotional words, which is particularly advantageous for real-time MER systems requiring immediate responses. However, such models may sacrifice a comprehensive understanding of contextual or situational factors. Utterance-level models address this limitation by leveraging entire sentences for emotion prediction  [7] . Nevertheless, human emotions are inherently dynamic, exhibiting momentary and situational variations  [8] . Relying solely on utterance-level representations may result in the loss of critical contextual information, thereby failing to capture the evolving nature of emotions. To mitigate this issue, Emotion Recognition in Conversation (ERC) has been explored through AC models  [9] , which incorporate broader contextual background information alongside the current utterance, thereby enhancing the accuracy of emotion recognition.\n\nDespite advancements in MER within ERC, several challenges remain. First, multimodal fusion methodologies have not been universally effective. Extensive research has explored early, late, and model fusion strategies; however, the integration of features across modalities remains suboptimal, particularly due to alignment issues  [10] . Although cross-attention mechanisms  [11]  have been employed for alignment, they primarily adapt one modality to another rather than achieving comprehensive fusion. Additionally, the incorporation of contextual information in conversations remains limited. Many approaches simply concatenate features from previous utterances without explicitly determining their contributions to emotion recognition  [12] . Furthermore, ERC is inherently a sequential classification problem, typically addressed using time-series models. While Long Short-Term Memory (LSTM) and RNN architectures  [13]  are commonly employed, they struggle to retain information over long sequences, as their processing tends to prioritise recent states. Transformer-based architectures  [14]  mitigate this limitation by leveraging selfattention mechanisms, yet, they do not inherently emphasise the sequential order of data to the same extent as traditional time-series models.\n\nTo address these challenges, Contrastive Language-Audio Pretraining (CLAP) has been introduced for fusing text and audio modalities by utilising a shared embedding space, which we adopt in this study  [15] . Additionally, xLSTM has been proposed as a hybrid approach that integrates the advantages of both LSTMs and Transformers; however, it has not yet been applied to this domain  [16] . Although large language models (LLMs) have been explored for ERC  [17] , their computational cost remains prohibitively high. Consequently, this study focuses on deep learning models that offer a more practical balance between performance and efficiency. Furthermore, to address the issue of contextual dependency, we propose a novel gate-based architecture that explicitly determines the contributions of different utterances within a conversation. This mechanism enhances interpretability and provides structured guidance for the model to refine its focus. Additionally, we find the Dialogical Emotion Decoder (DED)  [18]  to be a useful approach for addressing this challenge.\n\nOur contributions are as follows:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Emotion Recognition In Conversation (Erc)",
      "text": "Research shows that emotions can be conveyed through multiple channels, including vocal prosody, linguistic content, visual expressions, and physiological signals  [19] ,  [20] . MER integrates information from different modalities to improve recognition accuracy. Compared to single-modal approaches, multimodal interaction captures emotional characteristics more comprehensively and enhances model robustness  [10] . Existing studies adopt different modality combinations to optimise feature representation. Wang et al.  [21]  developed the MED4 multimodal database integrating electroencephalogram (EEG), speech, video, and photoplethysmography. Their feature-level and decision-level fusion framework demonstrated enhanced recognition accuracy in noisy environments through EEGspeech synergy. This revealed complementary interactions between physiological and behavioural signals. Middya et al.  [22]  employed a deep convolutional network to extract MFCC features from audio and spatiotemporal features from video. They applied model-level fusion to develop a lightweight multimodal model, achieving 86% and 99% accuracy on the RAVDESS and SAVEE datasets, respectively. Yin et al.  [23]  explored the combination of speech, video, and text, proposing the Token-Disentangling Mutual Transformer (TMT) for crossmodal feature interaction. Their approach first disentangles emotion-related features across modalities and then applies bidirectional query learning within a Transformer framework for feature fusion. Experiments on the CMU-MOSI, CMU-MOSEI, and CH-SIMS datasets achieved SOTA performance in single-utterance emotion recognition. While these methods have contributed to MER, most research still focuses on utterance-level emotion classification. They do not consider the evolution of emotional states in conversations  [24] .\n\nERC has been proposed to capture the evolution of emotions in dialogue  [25] . It emphasises contextual modelling to track how emotions change throughout an interaction. Currently, various studies have explored multimodal approaches to model emotions in conversation. Meng et al.  [26]  proposed the Graph-Spectrum-based Multimodal Consistency and Complementary Learning Framework (GS-MCC), integrating speech, text, and video for ERC. This method constructs a multimodal interaction graph and employs a Fourier Graph Neural Network to extract high-and low-frequency components over long-range dependencies. It enhances consistency and complementarity across modalities using contrastive learning, improving classification accuracy. Xie et al.  [27]  introduced the Crossmodality Transformer, a fusion model for multimodal emotion recognition. It incorporates the EmbraceNet structure and applies the Transformer mechanism to integrate speech, video, and text features. Experiments on the MELD dataset demonstrate its effectiveness, surpassing unimodal models in performance. The model captures contextual dependencies in conversations, improving emotion recognition accuracy. Experiments on the MELD dataset, which contains multiturn dialogues, demonstrate its effectiveness, outperforming unimodal models. Wang et al.  [28]  proposed the Contextual Attention Network (CAN) for multimodal ERC, combining speech, text, and video information. This model is based on a Gated recurrent units (GRU) network to capture intraand inter-speaker emotional dynamics. It integrates BERT for text embeddings, a Convolutional Neural Network (CNN) for speech processing, and 3D-CNN for video analysis. Further, an attention mechanism is designed to refine cross-modal weight adjustments. CAN achieved 64.6% accuracy on the IEMOCAP dataset, outperforming multiple baselines.\n\nWhile these methods demonstrate the feasibility of contextual modelling in ERC, multimodal ERC still faces challenges. One issue is modalities' fusion, as the effective integration of multiple modalities while minimising redundancy and conflict remains unresolved  [29] .\n\nYou're a lucky, lucky, man.\n\nI'm going to marry a blue blood.\n\nIncredible. Fig.  1 . The pipeline of GatedxLSTM. First, each audio sample and its corresponding transcription are processed using the CLAP model, which maps them into a shared embedding space and aligns their embeddings. Next, for a given utterance at time T , we identify its preceding utterance spoken by the interlocutor.\n\nBoth the audio and text representations are then passed through four distinct xLSTM blocks to extract features. To incorporate contextual information, we retrieve relevant features from several preceding utterances (e. g., at T -1). These extracted features are then processed through a gating mechanism to determine their contribution to the final emotion recognition task, assigning the audio of the current utterance a weight of 1. After applying the gating mechanism, the features are passed through a fully connected layer. Finally, the prediction is refined using DED in the last stage of the pipeline.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Contrastive Language-Audio Pretraining (Clap)",
      "text": "MER relies on feature extraction from different modalities. In speech and text-based scenarios, traditional methods typically model these modalities independently. The speech modality often uses Low-Level Descriptors (LLD) or pretrained model-based high-dimensional features. The text modality relies on word embeddings or pretrained language models for semantic representation. Since these features are extracted in separate spaces, semantic misalignment between speech and text representations occurs; consequently, it affects the effectiveness of multimodal fusion  [4] .\n\nCLAP  [30]  is designed to align the representations of audio and language. It applies contrastive learning to project speech and text into a shared embedding space, improving crossmodal alignment. CLAP follows the principles of Contrastive Language-Image Pretraining (CLIP)  [31]  and aims to maximise the similarity between matching speech-text pairs while minimising the similarity between mismatched samples. It uses a dual-encoder architecture, employing a speech encoder and a text encoder to process inputs separately before computing their similarity in a shared feature space.\n\nSome studies explore the use of CLAP in emotion-related tasks. ParaCLAP  [32]  focuses on paralinguistic analysis, including speech emotion recognition (SER). It enhances feature alignment through speech-text contrastive learning and constructs speech-text pairs using emotion labels and expert features. The model learns cross-modal representations in a shared space, focusing primarily on optimising data construction rather than modifying CLAP's pretraining strategy. GEmo-CLAP  [33]  applies CLAP to SER, incorporating contrastive learning with Kullback-Leibler (KL) divergence loss to refine the speech-text similarity matrix. It also introduces gender information and employs multi-task learning and soft label training to improve SER performance.\n\nUnlike these methods, we utilise CLAP as a feature extraction and alignment tool. This ensures alignment of features from these text and audio modalities into approximately the same feature space. The extracted features serve as inputs to our proposed Gated-xLSTM model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Xlstm For Sequential Modelling",
      "text": "The traditional LSTM architecture faces limitations in capturing long-range dependencies, enforcing strict sequential computation, and handling constrained memory storage  [34] . To address these issues, Extended LSTM (xLSTM)  [16]  enhances model expressiveness and computational efficiency through exponential gating and an improved memory structure. xLSTM introduces two key memory mechanisms: Scalar Memory LSTM (sLSTM) and Matrix Memory LSTM (mL-STM). sLSTM utilises multi-head memory units, improving hierarchical information modelling and enabling better feature disentanglement. mLSTM utilises a matrix memory structure to store key information and applies a covariance update rule to enhance long-term memory retention. It also supports parallel computation, reducing computational bottlenecks. These improvements make xLSTM more effective in long-sequence modelling, cross-time-step information storage, and computational parallelism. Therefore, xLSTM is particularly suitable for multi-turn dialogues, reinforcement learning, and temporal reasoning tasks.\n\nSeveral studies demonstrate the advantages of xLSTM in long-term dependency modelling. Schmidinger et al.  [35]  proposed Bio-xLSTM for genomics, protein structure analysis, and small molecule modelling. Their findings show that xLSTM's exponential gating mechanism effectively captures long-range dependencies, improving DNA and RNA sequence modelling accuracy. Zendehbad et al.  [36]  introduced the TraxVBF framework in electromyography (EMG) processing, combining xLSTM and Transformers for neurological rehabilitation and motion prediction. The study shows that xLSTM outperforms conventional LSTM and GRU in motion trajectory forecasting and assistive technology applications. Huang et al.  [37]  applied xLSTM-FER to facial expression recognition (FER). The model processes facial feature patches using xLSTM, establishing temporal dependencies across time steps. It achieves superior performance over CNNs and Vision Transformers (ViT) on the CK+, RAF-DB, and FERplus datasets. Derington et al.  [38]  evaluated xLSTM and other acoustic models in SER. Their study focuses on model robustness and fairness across different emotional dimensions.\n\nAlthough these studies highlight xLSTM's strength in longterm dependency modelling, its potential remains underexplored in AC tasks, particularly in multimodal ERC.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Dialogical Emotion Decoding (Ded)",
      "text": "DED  [39]  is an approximate inference algorithm that refines dialogue-based emotion prediction through sequential decoding and contextual rescoring. Unlike models that classify utterances independently, DED incorporates historical emotional patterns to improve prediction accuracy. It follows three key principles: 1) frequently occurring emotions in a dialogue are more likely to persist, 2) posterior probabilities are dynamically updated rather than relying on rigid labels, and 3) the emotional states of different speakers are interdependent and should be modelled jointly. Given predictions from a pretrained emotion recognition model, DED refines results in the following steps. First, emotion shifts are modelled using a Bernoulli distribution. Then, emotion states are clustered with a distance-dependent Chinese Restaurant Process (ddCRP)  [40] . Finally, Beam Search  [41]  is applied to ensure a contextually coherent prediction sequence. This post-processing approach has been widely validated in recent ERC research  [42] -  [44] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Based on the motivation and the identified gaps, we propose our pipeline model, as illustrated in Figure  1 . Each stage of the model is explained in detail in the subsequent sections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Feature Extraction",
      "text": "Effective alignment across different modalities is crucial, as misalignment can lead to suboptimal feature extraction. To explore this, we conducted preliminary experiments on the IEMOCAP dataset (discussed in Section IV-A), evaluating uni-modal performance and simple concatenation using an xLSTM model, as presented in Table  I . The results indicate that simple concatenation outperforms both uni-modal approaches, confirming that integrating multiple modalities helps compensate for limitations in individual modalities. However, this approach still falls short of SOTA methods, highlighting that simple concatenation is insufficient for effectively fusing modalities while preserving alignment features. Instead, CLAP employs a pretraining strategy to align text and audio features, mapping them into a shared embedding space. Unlike cross-attention mechanisms, which allow features to influence each other implicitly, CLAP explicitly matches text and audio representations. This alignment has been shown to be beneficial for downstream classification tasks. Therefore, we utilise a CLAP pre-trained model to acquire the aligned features, shown in Equation  1 , trained in contrastive loss function as discussed in  [15] .\n\nThe g audio and g text are the audio and text encoder, along with the MLP, used as a projector to obtain the fixed shared embedding space representation. A j and T j are the raw audio and text from the dataset, and the corresponding E a j and E t j are the aligned embeddings.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Gated-Xlstm 1) Base Xlstm:",
      "text": "As previously mentioned, the ERC task is a sequential modelling problem for which LSTM has demonstrated strong performance. However, attention mechanisms have also been introduced to enhance the model's ability to capture global contextual information. The xLSTM model effectively combines the advantages of both approaches while incorporating a memory matrix to improve memory efficiency compared to traditional Transformers. Therefore, we selected xLSTM as the base model. As shown in Table  I , xLSTM outperforms LSTM, further validating our choice.\n\n2) Gate mechanism: While we have aligned feature extraction methods and sequence models for this task, we still lack a clear understanding of why we obtained final emotion classification results. Intuitively, as suggested by previous literature, preceding sentences are expected to influence the emotional state at the current stage. However, it remains unclear which modality from which contextual utterance exerts the dominant influence. Furthermore, we have not examined whether the interlocutor or the speaker themselves plays a greater role in shaping the current emotional state. Psychologically, evidence suggests that emotions influence both the self and others  [46] , yet determining which has a more significant impact remains a complex challenge. To better capture the importance of different features, we design a gating mechanism that facilitates the selection and refinement of the aligned features we extract. We first pass the aligned features to xLTSM shown in Equation  2 , where x j represents audio or text features we extract.\n\nNotably, xLSTM comprises sLSTM and mLSTM blocks as its key components. We posit that the integration of LSTM and the attention mechanism in mLSTM addresses the challenges associated with each individual component discussed previously. This integration, in addition to the standard LSTM gating mechanism, is illustrated in Equation 3  [16] ,\n\nwhere C t is the current cell state, f t is the forget gate, and q, k, v follow the same definitions as in the attention mechanism. Additionally, i t is the input gate, o t is the output gate, h t is the hidden state, n t is the normalised state, and E i is the embedding obtained from CLAP. Notably, the integration of the attention mechanism into the cell and hidden state updates is a novel approach, enabling sequence modelling to capture the entire context more effectively.\n\nAfter we get the output of xLSTM, inspired by the forget gate in LSTM, we hypothesise that neural networks have the capability to determine which features are important and should be retained for a longer duration when explicitly guided by emphasis and structured queries regarding their retention. Therefore, we adopt a similar strategy, enabling the model to learn to prioritise features based on the current context, as shown in Equation  4 .\n\nwhere w j is the forget weight, ranging from 0 to 1, as a scalar value, obtained through a sigmoid activation function.\n\nThese forget weights are then applied to multiple xLSTM layers to obtain sequence feature representations. The multiplication follows an element-wise extension, meaning the weights are applied across all dimensions of the feature modality for a given utterance, shown in Equation  5 .\n\nwhere x j is each value in the hidden dimension, and we build Z from concatenation of z j . Also, y j is the predicted emotion classification. We do not consider the sequence length here, as we use the final token to represent the features of the sentence. Notably, we deliberately cluster the current utterance with the utterance spoken by the previous interlocutor at time k to ensure they are processed together. It is possible that at time k -1, the utterance still belongs to the current speaker; however, we continue iterating through previous time frames to identify the utterance of the interlocutor. This approach is intended to compel the model to learn the importance of the four features: the audio and text, from both the speaker and the interlocutor. Additionally, we standardise the current utterance's audio by representing it as a unit vector. Moreover, we take contextual information into account, although we focus only on a limited number of previous utterances. While subsequent sentences may provide additional valuable insights, such as maintaining emotional consistency, it is impractical to anticipate future utterances in real-time applications. The features of the previous utterances, each encompassing four distinct features per time frame, are passed together through the gate mechanism to assess their relative importance.\n\nSubsequently, we perform another fusion of the features through simple concatenation via a fully connected layer, even though multimodal features were already fused in CLAP. This step aims to project the features Z, which have the shape (batch size × number of sentences × hidden dimension), into the appropriate classification space. Additionally, it enables model-level fusion of the xLSTM output features, providing the model with an additional opportunity to learn the interdependencies among the multimodal features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Post Adjustment",
      "text": "DED  [39]  is employed for the post adjustment stage. Given a dialogue U = {u 1 , u 2 , ..., u k }, the goal is to decode the emotion state y k of each utterance u k based on the predicted emotion states Y 1:k-1 . The joint probability distribution is formulated as:\n\nwhere x k is the feature vector of u k , y k ∈ {anger, happiness, neutrality, sadness} represents the predicted emotion label, and S = {s 1 , ..., s K } is a sequence of binary variables where each s k indicates whether an emotion shift occurs at utterance u k .\n\nEmotion shifts follow a Bernoulli distribution with probability p 0 , which is estimated from training data based on observed emotion transitions.\n\nEmotion assignment is performed using a ddCRP. In this process, the probability of assigning an emotion label y k depends on whether the emotion has appeared in the previous utterances. If emotion l has already been observed in earlier utterances, its assignment probability is proportional to the size of the corresponding emotion block, denoted as N l,k-1 .\n\nOtherwise, a new emotion state is introduced with a probability controlled by the hyperparameter α, which defaults to 1.\n\nThe optimal emotion sequence is determined by maximising the posterior probability:\n\nwhere p(X, Y ) incorporates probabilities from the emotion classification module, emotion shift modelling, and emotion assignment. Finally, Beam Search is selected for decoding, narrowing down the search to the most promising sequences. This approach refines emotion predictions and aligns them with the natural flow of conversation.\n\nIV. EXPERIMENT",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Iemocap Dataset",
      "text": "IEMOCAP  [47]  is a multimodal database designed for emotion recognition and analysis. It comprises speech, video, and emotion annotations provided by three independent annotators. The dataset includes five sessions, each featuring a dyadic interaction between a male and a female actor. These sessions contain both scripted and improvised dialogues. In this study, we focus on four emotion categories as commonly done on this database: happiness, neutrality, anger, and sadness. The dataset we filtered consists of 151 dialogues and 4,490 utterances, distributed as follows: happiness (13.25%), neutrality (38.04%), anger (24.57%), and sadness (24.14%).\n\nIn this study, we utilise both textual and acoustic modalities from the IEMOCAP dataset, while the features are provided in the original dataset. Given that our experimental design incorporates contextual information, we employ a dialoguelevel data split to prevent data leakage. Specifically, we ensure that sentences from the same dialogue appear within the same training, validation or test subset. The dataset is partitioned into training, validation, and test sets in an 8:1:1 ratio, with a random seed of 42 to split data for reproducibility. Additionally, we provide the dataset in the supplementary materials to facilitate replication. The final results represent the average of five independent experiments with different seeds.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation And Results",
      "text": "Our experiments were conducted on a V100-PCIE-32GB GPU. For CLAP, we utilised the default pre-trained model from the laion clap library, keeping it frozen throughout the entire training pipeline. This decision was made as the model has been trained on a large-scale dataset, and we prioritised allocating our limited computational resources to further training the xLSTM component. The parameters for xLSTM are detailed in Table  II . While xLSTM requires a sequential input, the complete representation from CLAP is not inherently sequential. To approximate the temporal structure of the audio clip, we segment the representation into 16 time steps, ensuring an even distribution across the sequence. Moreover, to incorporate contextual information from preceding utterances, we selected the two most recent utterance blocks, thus providing us 12 xLTSM blocks. We use seed values of  42, 43, 45, 46 , and 50 to obtain the final weighted accuracy and weighted F1-score, which are then compared against existing open-source SOTA models. These two metrics are selected due to the class imbalance inherent in the dataset. Weighted accuracy ensures that each class contributes proportionally to the overall performance evaluation, to get us equal evaluation of each class. Similarly, the weighted F1-score accounts for both precision and recall across imbalanced classes, providing a more comprehensive assessment of model performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Discussion",
      "text": "Our proposed GatedxLSTM model, which leverages both audio and text modalities, achieves superior performance with a weighted accuracy of 76.34±1.31% and a weighted F1-score of 75.97±1.38%. These results highlight the effectiveness of our model in multimodal learning. Furthermore, our approach achieves SOTA performance among open-source models and approaches SOTA performance in general, with the exception of  [56] .\n\nTo further assess the effectiveness of our model, we evaluate its performance across individual emotion subclasses, as presented in Table  IV . The reported results reflect the average scores from five independent experiments. Our model demonstrates consistently high performance in classifying Neutral, Sad, and Angry emotions, indicating its reliability in these categories. However, performance decreases in the Happy category, suggesting inherent challenges in recognising happiness. We hypothesise that happiness may be more difficult to detect because individuals often feign happiness for social politeness rather than expressing genuine emotion  [57] . Additionally, research suggests that happiness diminishes over time in response to sustained positive stimuli, a phenomenon known as the Hedonic Treadmill  [58] . Consequently, incorporating additional feature extraction techniques or data augmentation strategies may be necessary to enhance the model's ability to recognise Happy emotions accurately.\n\nMoreover, certain studies, such as  [56] , report higher performance by redefining the happiness category to include excitement. From a psychological perspective, however, this classification is questionable. While both happiness and excitement are positive emotions, excitement exhibits a more explicit expression and is often elicited by novel or stimulating experiences. According to the circumplex model of affect  [59] , excitement is characterised by high arousal and lower in valence, whereas happiness is associated with lower arousal. Therefore, although merging these two categories may improve classification accuracy, we argue that treating happiness and excitement as a single class does not align with theoretical expectations. Additionally, one of the key contributions of our model is its ability to explain the importance of each input feature. Using the 48-seed model, the weights are visualised in Figure  2 . This visualisation provides interpretability, demonstrating that audio has a greater influence on the ERC tasks, which aligns with our intuition. We think it is due to the presence of more acoustic features, such as pitch and tone variations, whereas text is limited to key words. It gives us also the insight that xLSTM may be better at processing audio features. Furthermore, the analysis reveals that the audio from the interlocutor has a relatively smaller influence compared to the speaker's own audio. However, the interlocutor plays an important role in emotional contagion and affective interaction  [60] , which is the reason we add them to our feature sets.\n\nWe conducted an ablation study to systematically evaluate the contributions of each component in our framework toward achieving SOTA performance. Our framework incorporates three key mechanisms: CLAP for multimodal feature alignment, a Gated mechanism for weighted feature integration, and DED as a post-adjustment module. The results of this ablation study are presented in Table  V .\n\nThe experimental results demonstrate that the CLAP feature alignment mechanism improves performance over an IAAN model, underscoring the importance of effectively aligning multimodal features. Further gains are achieved by incor-",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we proposed a novel framework based on GatedxLSTM, which is the first to employ xLSTM-based models in the field of MER. We have designed a pipeline that incorporates multimodality alignment, selective and concentrated feature extraction, and a post-adjustment mechanism. This framework achieves SOTA performance in fourclass emotion recognition on the popular IEMOCAP dataset. Furthermore, our project enhances the interpretability of model decisions from a psychological perspective, providing deeper insights into the mechanisms of affective computing. These insights offer potential inspiration for the future development of AC. In future work, we aim to expand our model research to real-time emotion recognition, with the goal of advancing AC intelligence and fostering the development of more userfriendly AGI.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethic Impact Statement",
      "text": "Our research advances affective computing by improving the accuracy and explainability of emotion recognition systems. By leveraging state-of-the-art methodologies, our work enhances model interpretability, ensuring that predictions are more transparent and trustworthy. This contributes to the development of fairer and more reliable emotion recognition technologies, supporting applications in healthcare, humancomputer interaction, and assistive technologies. Our approach does not introduce any obvious ethical concerns.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The pipeline of GatedxLSTM. First, each audio sample and its corresponding transcription are processed using the CLAP model, which maps them into",
      "page": 3
    },
    {
      "caption": "Figure 1: Each stage of",
      "page": 4
    },
    {
      "caption": "Figure 2: This visualisation provides interpretability, demonstrating",
      "page": 7
    },
    {
      "caption": "Figure 2: Average absolute weights for each output neuron. ‘0’ represents the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ak-n\n...": "ak-3"
        },
        {
          "ak-n\n...": "ak-2"
        },
        {
          "ak-n\n...": "ak-1\nak"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xLSTM\nxLSTM -0A1 xLSTM -0T1 xLSTM -0A2 xLSTM -0T2": "xLSTM -1A1",
          "Column_2": "xLSTM -1T1",
          "Column_3": "xLSTM -1A2",
          "Column_4": "xLSTM -1T2"
        },
        {
          "xLSTM\nxLSTM -0A1 xLSTM -0T1 xLSTM -0A2 xLSTM -0T2": "xLSTM -2A1",
          "Column_2": "xLSTM -2T1",
          "Column_3": "xLSTM -2A2",
          "Column_4": "xLSTM -2T2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "w�\n4": "w\n8",
          "�1\nw\n5": "w\n9",
          "w\n2\nw\n6": "w\n10",
          "w\n3\nw\n7": "w\n11"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "...": "tk-3"
        },
        {
          "...": "tk-2"
        },
        {
          "...": "tk-1\ntk"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "��−3\n�\n��−2\n�\n��−1\n�",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Towards friendly ai: A comprehensive review and new perspectives on human-ai alignment",
      "authors": [
        "Q Sun",
        "Y Li",
        "E Alturki",
        "S Murthy",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Towards friendly ai: A comprehensive review and new perspectives on human-ai alignment"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech"
    },
    {
      "citation_id": "3",
      "title": "Challenges and opportunities of text-based emotion detection: a survey",
      "authors": [
        "A Al Maruf",
        "F Khanam",
        "M Haque",
        "Z Jiyad",
        "M Mridha",
        "Z Aung"
      ],
      "year": "2024",
      "venue": "IEEE access"
    },
    {
      "citation_id": "4",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "5",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Use of word clustering to improve emotion recognition from short text",
      "authors": [
        "S Yuan",
        "H Huang",
        "L Wu"
      ],
      "year": "2016",
      "venue": "Journal of Computing Science and Engineering"
    },
    {
      "citation_id": "7",
      "title": "Survey of deep emotion recognition in dynamic data using facial, speech and textual cues",
      "authors": [
        "T Zhang",
        "Z Tan"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "8",
      "title": "Emotion theory and research: Highlights, unanswered questions, and emerging issues",
      "authors": [
        "C Izard"
      ],
      "year": "2009",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "9",
      "title": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "authors": [
        "G Hu",
        "Y Xin",
        "W Lyu",
        "H Huang",
        "C Sun",
        "Z Zhu",
        "L Gui",
        "R Cai",
        "E Cambria",
        "H Seifi"
      ],
      "year": "2024",
      "venue": "Recent trends of multimodal affective computing: A survey from nlp perspective",
      "arxiv": "arXiv:2409.07388"
    },
    {
      "citation_id": "10",
      "title": "A review of key technologies for emotion analysis using multimodal information",
      "authors": [
        "X Zhu",
        "C Guo",
        "H Feng",
        "Y Huang",
        "Y Feng",
        "X Wang",
        "R Wang"
      ],
      "year": "2024",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "11",
      "title": "Memocmt: multimodal emotion recognition using cross-modal transformerbased feature fusion",
      "authors": [
        "M Khan",
        "P.-N Tran",
        "N Pham",
        "A Saddik",
        "A Othmani"
      ],
      "year": "2025",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "12",
      "title": "Modeling continuous emotions in text data using iemocap database",
      "authors": [
        "A Messaoudi",
        "H Boughrara",
        "Z Lachiri"
      ],
      "year": "2024",
      "venue": "2024 IEEE 7th International Conference on Advanced Technologies, Signal and Image Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition in continuous space using iemocap database",
      "authors": [
        "A Messaoudi",
        "H Boughrara",
        "Z Lachiri"
      ],
      "year": "2024",
      "venue": "2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)"
    },
    {
      "citation_id": "14",
      "title": "Using transformers for multimodal emotion recognition: Taxonomies and state of the art review",
      "authors": [
        "S Hazmoune",
        "F Bougamouza"
      ],
      "year": "2024",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Y Wu",
        "K Chen",
        "T Zhang",
        "Y Hui",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "xlstm: Extended long short-term memory",
      "authors": [
        "M Beck",
        "K Pöppel",
        "M Spanring",
        "A Auer",
        "O Prudnikova",
        "M Kopp",
        "G Klambauer",
        "J Brandstetter",
        "S Hochreiter"
      ],
      "year": "2024",
      "venue": "xlstm: Extended long short-term memory",
      "arxiv": "arXiv:2405.04517"
    },
    {
      "citation_id": "17",
      "title": "Llm supervised pre-training for multimodal emotion recognition in conversations",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2025",
      "venue": "Llm supervised pre-training for multimodal emotion recognition in conversations"
    },
    {
      "citation_id": "18",
      "title": "A dialogical emotion decoder for speech emotion recognition in spoken dialog",
      "authors": [
        "S.-L Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Emotional expression: Advances in basic emotion theory",
      "authors": [
        "D Keltner",
        "D Sauter",
        "J Tracy",
        "A Cowen"
      ],
      "year": "2019",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "20",
      "title": "The perception of emotion in body expressions",
      "authors": [
        "B Gelder",
        "A De Borst",
        "R Watson"
      ],
      "year": "2015",
      "venue": "Wiley Interdisciplinary Reviews: Cognitive Science"
    },
    {
      "citation_id": "21",
      "title": "Multi-modal emotion recognition using eeg and speech signals",
      "authors": [
        "Q Wang",
        "M Wang",
        "Y Yang",
        "X Zhang"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "22",
      "title": "Deep learning based multimodal emotion recognition using model-level fusion of audio-visual modalities",
      "authors": [
        "A Middya",
        "B Nag",
        "S Roy"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "23",
      "title": "Tokendisentangling mutual transformer for multimodal emotion recognition",
      "authors": [
        "G Yin",
        "Y Liu",
        "T Liu",
        "H Zhang",
        "F Fang",
        "C Tang",
        "L Jiang"
      ],
      "year": "2024",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "A survey of deep learning-based multimodal emotion recognition: Speech, text, and face",
      "authors": [
        "H Lian",
        "C Lu",
        "S Li",
        "Y Zhao",
        "C Tang",
        "Y Zong"
      ],
      "year": "2023",
      "venue": "Entropy"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "26",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "T Meng",
        "F Zhang",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "27",
      "title": "Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion",
      "authors": [
        "B Xie",
        "M Sidulova",
        "C Park"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "28",
      "title": "A contextual attention network for multimodal emotion recognition in conversation",
      "authors": [
        "T Wang",
        "Y Hou",
        "D Zhou",
        "Q Zhang"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "29",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "A Gandhi",
        "K Adhvaryu",
        "S Poria",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "30",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "32",
      "title": "Paraclap-towards a general language-audio model for computational paralinguistic tasks",
      "authors": [
        "X Jing",
        "A Triantafyllopoulos",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Paraclap-towards a general language-audio model for computational paralinguistic tasks",
      "arxiv": "arXiv:2406.07203"
    },
    {
      "citation_id": "33",
      "title": "Gemo-clap: Gender-attribute-enhanced contrastive language-audio pretraining for accurate speech emotion recognition",
      "authors": [
        "Y Pan",
        "Y Hu",
        "Y Yang",
        "W Fei",
        "J Yao",
        "H Lu",
        "L Ma",
        "J Zhao"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "A critical review of rnn and lstm variants in hydrological time series predictions",
      "authors": [
        "M Waqas",
        "U Humphries"
      ],
      "year": "2024",
      "venue": "MethodsX"
    },
    {
      "citation_id": "35",
      "title": "Bio-xlstm: Generative modeling, representation and incontext learning of biological and chemical sequences",
      "authors": [
        "N Schmidinger",
        "L Schneckenreiter",
        "P Seidl",
        "J Schimunek",
        "P.-J Hoedt",
        "J Brandstetter",
        "A Mayr",
        "S Luukkonen",
        "S Hochreiter",
        "G Klambauer"
      ],
      "year": "2024",
      "venue": "Bio-xlstm: Generative modeling, representation and incontext learning of biological and chemical sequences",
      "arxiv": "arXiv:2411.04165"
    },
    {
      "citation_id": "36",
      "title": "Traxvbf: A hybrid transformer-xlstm framework for emg signal processing and assistive technology development in rehabilitation",
      "authors": [
        "S Zendehbad",
        "A Razavi",
        "M Sanjani",
        "Z Sedaghat",
        "S Lashkari"
      ],
      "year": "2025",
      "venue": "Sensing and Bio-Sensing Research"
    },
    {
      "citation_id": "37",
      "title": "xlstm-fer: Enhancing student expression recognition with extended vision long short-term memory network",
      "authors": [
        "Q Huang",
        "J Chen"
      ],
      "year": "2024",
      "venue": "Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data"
    },
    {
      "citation_id": "38",
      "title": "Testing correctness, fairness, and robustness of speech emotion recognition models",
      "authors": [
        "A Derington",
        "H Wierstorf",
        "A Özkil",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "A dialogical emotion decoder for speech emotion recognition in spoken dialog",
      "authors": [
        "S.-L Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "40",
      "title": "Distance dependent chinese restaurant processes",
      "authors": [
        "D Blei",
        "P Frazier"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "41",
      "title": "Improvements in beam search",
      "authors": [
        "V Steinbiss",
        "B.-H Tran",
        "H Ney"
      ],
      "year": "1994",
      "venue": "ICSLP"
    },
    {
      "citation_id": "42",
      "title": "Multimodal emotion recognition from raw audio with sinc-convolution",
      "authors": [
        "X Zhang",
        "W Fu",
        "M Liang"
      ],
      "year": "2024",
      "venue": "Multimodal emotion recognition from raw audio with sinc-convolution",
      "arxiv": "arXiv:2402.11954"
    },
    {
      "citation_id": "43",
      "title": "Speech emotion recognition using dialogue emotion decoder and cnn classifier",
      "authors": [
        "S Atkar",
        "R Agrawal",
        "C Dhule",
        "N Morris",
        "P Saraf",
        "K Kalbande"
      ],
      "year": "2023",
      "venue": "2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)"
    },
    {
      "citation_id": "44",
      "title": "Emotion-shift aware crf for decoding emotion sequence in conversation",
      "authors": [
        "C.-Y Chen",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2022",
      "venue": "Emotion-shift aware crf for decoding emotion sequence in conversation"
    },
    {
      "citation_id": "45",
      "title": "An interaction-aware attention network for speech emotion recognition in spoken dialogs",
      "authors": [
        "S.-L Yeh",
        "Y.-S Lin",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Dynamic self-and other-focused emotional intelligence: A theoretical framework and research agenda",
      "authors": [
        "K Pekaar",
        "D Van Der Linden",
        "A Bakker",
        "M Born"
      ],
      "year": "2020",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "47",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "48",
      "title": "Fatrer: Full-attention topic regularizer for accurate and robust conversational emotion recognition",
      "authors": [
        "Y Mao",
        "D Lu",
        "Y Zhang",
        "X Wang"
      ],
      "year": "2023",
      "venue": "ECAI 2023"
    },
    {
      "citation_id": "49",
      "title": "Bioserc: Integrating biography speakers supported by llms for erc tasks",
      "authors": [
        "J Xue",
        "M.-P Nguyen",
        "B Matheny",
        "L.-M Nguyen"
      ],
      "year": "2024",
      "venue": "International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "50",
      "title": "A voice-based real-time emotion detection technique using recurrent neural network empowered feature modelling",
      "authors": [
        "S Chamishka",
        "I Madhavi",
        "R Nawaratne",
        "D Alahakoon",
        "D Silva",
        "N Chilamkurti",
        "V Nanayakkara"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "51",
      "title": "A contextual attention network for multimodal emotion recognition in conversation",
      "authors": [
        "T Wang",
        "Y Hou",
        "D Zhou",
        "Q Zhang"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "52",
      "title": "Multimodal prompt transformer with hybrid contrastive learning for emotion recognition in conversation",
      "authors": [
        "S Zou",
        "X Huang",
        "X Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "54",
      "title": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2022",
      "venue": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "arxiv": "arXiv:2202.08974"
    },
    {
      "citation_id": "55",
      "title": "Multimodal emotion recognition based on audio and text by using hybrid attention networks",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "R Liu",
        "X Tao",
        "W Guo",
        "Y Xu",
        "X Zhao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "56",
      "title": "Distribution-based emotion recognition in conversation",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "57",
      "title": "Facial displays are tools for social influence",
      "authors": [
        "C Crivelli",
        "A Fridlund"
      ],
      "year": "2018",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "58",
      "title": "The hedonic treadmill",
      "authors": [
        "S Byrnes",
        "N Strohminger"
      ],
      "year": "2005",
      "venue": "Science B"
    },
    {
      "citation_id": "59",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "60",
      "title": "Emotional contagion",
      "authors": [
        "E Hatfield",
        "J Cacioppo",
        "R Rapson"
      ],
      "year": "1993",
      "venue": "Current directions in psychological science"
    }
  ]
}