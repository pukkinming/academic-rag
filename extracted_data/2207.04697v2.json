{
  "paper_id": "2207.04697v2",
  "title": "Multi-Level Fusion Of Wav2Vec 2.0 And Bert For Multimodal Emotion Recognition",
  "published": "2022-07-11T08:20:53Z",
  "authors": [
    "Zihan Zhao",
    "Yanfeng Wang",
    "Yu Wang"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "multigranularity framework",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The research and applications of multimodal emotion recognition have become increasingly popular recently. However, multimodal emotion recognition faces the challenge of lack of data. To solve this problem, we propose to use transfer learning which leverages state-of-the-art pre-trained models including wav2vec 2.0 and BERT for this task. Multi-level fusion approaches including coattention-based early fusion and late fusion with the models trained on both embeddings are explored. Also, a multi-granularity framework which extracts not only frame-level speech embeddings but also segment-level embeddings including phone, syllable and word-level speech embeddings is proposed to further boost the performance. By combining our coattention-based early fusion model and late fusion model with the multi-granularity feature extraction framework, we obtain result that outperforms best baseline approaches by 1.3% unweighted accuracy (UA) on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "People are highly rich in emotions and expressions of emotions can be found throughout lives. People also possess a strong ability to recognize these emotions in order to generate appropriate responses. Similarly, machines that can recognize emotions to make them more human-like can have many application scenarios  [1, 2, 3] . People use multimodal information when perceiving emotions, like text, speech, vision and motion. In this paper, we focus our research on text and speech modalities.\n\nText is a highly relevant modality for emotion recognition, because the meanings of words and their relations express one's emotion  [4] . The main challenge of using emotion datasets directly for text emotion recognition is the lack of large-scale data  [5] . This is mainly due to the difficulty posed by the subjective nature of labeling the emotion. One common solution to this problem is to leverage transfer learning-based approaches. For natural language processing, recently proposed pre-trained models such as Bidirectional Encoder Representations from Transformers (BERT)  [6] , Generative Pre-training 2 (GPT-2)  [7] , Text-to-Text Transfer Transformer (T5)  [8]  become popular because of their excellent performance on a number of important tasks. The difference between GPT-2, T5 and BERT is that GPT-2 and T5 are better at sequence generation while BERT is more appropriate for extracting embeddings  [8, 9, 10] . For text-based emotion recognition, it is more vital to extract embeddings considering the characteristics of the task, so there are some previous works leveraging BERT for text emotion recognition and achieving promising results  [11, 12] .\n\nIn addition to text, speech is also commonly recognized as a modality with high importance for emotion recognition. When comparing to text, information conveyed by speech such as the intonation and pitch can be used to recognize emotions. Therefore the use of speech for emotion recognition has also been popular, but it faces the similar problem of insufficient data. Similarly, this problem can be mitigated through the use of speech-based pre-trained models such as wav2vec  [13] , VQ-wav2vec  [14] , wav2vec 2.0  [15] . Recently, wav2vec 2.0 is becoming popular on speech processing tasks such as ASR  [16] , speaker verification  [17]  and it has also shown promising performance in speech emotion recognition in  [5] .\n\nGiven the importance of both text and speech for emotion recognition tasks, it is natural to leverage multimodal models that combine text and speech information to improve the performance of the emotion recognition and there are a number of works have explored this  [18, 19, 20, 21, 22, 23] . However, to the best of our knowledge, few of them have yet explored how to effectively combine pre-trained models for multimodal emotion recognition. The recent work in  [19]  considers both wav2vec 2.0 and BERT for emotion recognition. However, it mainly focuses on disentanglement representation learning but not on multimodal fusion, and the only fusion method considered is the score fusion. In this paper, we explore extensively the fusion of wav2vec 2.0 and BERT-based embeddings and models for the multimodal emotion recognition. In other recent papers with state-of-the-art results,  [22]  utilizes attention-based Gated Recurrent Unit for speech and BERT for text respectively and then concatenates them together to get the final prediction. Furthermore,  [23]  proposes a method combining self-attentional bidirectional contextual LSTM and self-attentional multi-channel CNN with multi-scale fusion framework including feature-level fusion and decision-level fusion. We summarize our major contributions as follows:\n\n• We explore the multi-level fusion of text and speech for emotion recognition leveraging both wav2vec 2.0 and BERT with both early fusion and late fusion, and also the combination of them. For the early fusion, both simple embedding concatenation and more sophisticated attention mechanism-based fusion methods  [24, 25]  are investigated. To the best of our knowledge, this is the first work that explores the multi-level fusion of state-of-theart pre-trained model for the multimodal emotion recognition task.\n\n• We propose a novel multi-granularity fusion framework which makes use of not only frame-level speech embeddings but also explores segment-level speech embeddings including word, syllable and phone-level embeddings to further improve the recognition performance.\n\n• The proposed models are experimented on the popular IEMOCAP dataset  [26] . Experimental results show that they can outperform existing state-of-the-art multimodal approaches using both speech and text modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methods",
      "text": "For emotion recognition, we mainly explore two fusion mechanisms including late fusion and early fusion, which can also be combined to further boost the performance. In all three models we utilize multi-granularity frameworks to extract embeddings.\n\nIt is worth noting that in this part we discuss the scenario with all the considered segment-level embeddings, while in the experiments we will explore the performance of both single and combination of these embeddings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Granularity Framework",
      "text": "In this paper, we explore a multi-granularity framework to extract speech embeddings from multiple levels of subwords.\n\nAs illustrated in the blue part of Figure  1 , the embeddings wav2vec 2.0 extracted are normally frame-level embeddings and it has shown to be effective in obtaining abundant framelevel information. However, it lacks the ability to capture segment-level information which is useful for emotion recognition. Thus, in addition to frame-level embeddings, we introduce segment-level embeddings including word, phone and syllablelevel embeddings which are closely related to the prosody  [27, 28, 29] . Prosody can convey characteristics of the utterance like emotional state  [30, 31]  because it contains the information of the cadence of speech signals. As a result, segment-level embeddings may be helpful for multimodal emotion recognition.\n\nUsing force alignment approaches, the temporal boundaries of phonemes can be obtained, which can then be grouped to get the boundaries of the syllables. Force alignment information is provided in  [26] . The speech segments corresponding to those units can be extracted thereafter. The segment-level embeddings can then be obtained by:\n\nHere u i,l,n k is the k th segmentation of the segment-level embedding of the l th layer of the n th sample, f is the f th frame of the frame-level embedding, s is the starting frame, e is the end frame and F, P, W, S represent frame, phone, word and syllable-level speech embeddings respectively. Thereafter, we apply weighted average for layers, following  [5] , where they took embeddings from all 12 transformer encoder layers in wav2vec 2.0 as the frame-level embeddings and then learned a weight w l for each of the layer. Then, the weighted embeddings are obtained by:\n\nA similar approach is applied for extracting text embeddings.\n\nHere LN represents Layer Normalization  [32] , w i l is the learnable weight for the layer l and T represents text embeddings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Late Fusion",
      "text": "The orange part of Figure  1  illustrates the late fusion model. There are five branches in the model and for every branch of the   late fusion model we use the same structure as that used in  [5] . First, the extracted embeddings u i,n k obtained using Equation 1 are input into two feed-forward layers:\n\n, i ∈ {P, W, S, F, T} Then a global average module is applied to fuse embeddings from different segmentation or frames or wordpieces:\n\nHere l i,n is the sequence length. Finally, the embeddings are sent to the last feed-forward layer to generate logits, and logits from different branches are added together to generate prediction ỹn for the late fusion model:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Early Fusion",
      "text": "Figure  2  illustrates the coattention-based early fusion model we adopt. Here multiple levels including phone, word, syllable and frame-level speech embeddings are sent to the coattention model with text embeddings respectively:\n\nwhere the coattention operation is denoted as ⊗, and GA represents the global average as the same in Equation  2 , U i,n = u i,n 1 , u i,n 2 , ..., u i,n l i,n and is of size [seq_length, embed_size], where u i,n k is calculated using Equation 1. c i,n is the embedding of the n th sample produced by the coattention model and the global average module, the size of which is [embed_size]. Then the embeddings are concatenated together as:\n\nwhere the concatenation operation is denoted as ⊕. Finally, the concatenated embedding is sent to feed-forward layers for multi-granularity fusion and outputs the estimated posterior probability of the emotion classes ỹn :\n\nThe structure of the coattention model in Figure  2  is illustrated in Figure  3 . There are two branches in one coattention layer, each of which has the same structure but different modalities as Q or K,V  [33] . Without loss of generality, the calculation process of one-head attention is given here  [34] :\n\nwhere the notations i and j indicate that they are from different modalities. Then we can get the processed embeddings as:\n\nwhere d represents the dimension of the embeddings. Multihead attention performs this process multiple times in order to learn information from various representation subspaces. After this, the model follows the classic transformer encoder structure in every branch. In our configuration, the coattention layer is repeated three times. Therefore, the output from every branch can incorporate enough information from another modality and as a result they are almost identical in our experiment, so we simply calculate the mean to obtain the final output for our coattention model in order to reduce the consumption of computational resources.\n\nConcatenation-based early fusion is also investigated for comparison. For this fusion approach, we concatenate text and frame-level speech embeddings before feeding them into feedforward layers which is the same as one branch of our late fusion model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Combination Of Early Fusion And Late Fusion",
      "text": "We have introduced the models and processes for the early fusion and late fusion approaches in the previous two subsections. In this subsection, we will describe the combination of them to achieve better performance. The reason for the combination is that early fusion merges two modalities with low-level pre-trained features while late fusion merges them with highlevel features. Thus, these two fusion schemes make predictions based on embeddings from various levels and it is expected that  they can provide complementary information that can be leveraged to boost the performance.\n\nIn this work, score combination of best early-fused and latefused models is considered. Their output logits are averaged as the combined prediction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "The IEMOCAP dataset  [26]  is an acted, multimodal and multispeaker dataset with five sessions. Following the work in  [5] , we use 4 emotional classes: anger, happiness, sadness and neutral. We relabeled excitement utterances as happiness and discard utterances from other classes, which is a common practice using IEMOCAP. As a result, the number of utterances representing angry, happy, sad and neutral were 1103, 1636, 1084 and 1708 respectively. A 5-fold cross-validation (CV) configuration was implemented to evaluate our model, leaving one session out as test set in each fold, since it is the regular evaluation method for IEMOCAP. It is worth mentioning that there was no fixed manner to determine the validation set  [35] , so we randomly selected 10% of the utterances in the training set as the validation set in each fold.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Settings And Metrics",
      "text": "Cross-entropy loss was adopted as our loss function, and an Adam optimizer  [36]  was applied using a learning rate of 1e-3 for the feed-forward layer-based models and 5e-5 for transformer-based models. The models were trained using a batch size of 32 and early stopping was also applied. Dropout  [37]  was applied with a probability of 0.2 after every feed-forward layer except the output layer to prevent overfitting. We use wav2vec 2.0-base and BERT-base un-cased models, both of which have 768-dimensional embeddings. All speech samples are normalized by global normalization which is a frequently used setting for this dataset.\n\nWe adopted the widely used metric unweighted accuracy (UA) as our evaluation metric. The results were taken from an average of 25 experiments. (we implemented five-fold crossvalidation for 5 times. )",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Table 2: Unweighted Accuracy (Ua) Of The 5-Fold Cv Results Of Coattention-Based Early Fusion And Late Fusion Models With Different Input Embeddings. F -Frame, P -Phone, S -Syllable, W -Word, Coattention -Coattention-Based Early Fusion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Embeddings Fusion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results Of The Single Modality Models",
      "text": "Table  1  shows the results of the single modality. \"Linear\" means the linear model used in  [5] , which is also adopted in every branch of our late fusion model. The \"Transformer Encoder\" has three layers of transformer encoders followed by two feed-forward layers, which is used for comparison with our coattention-based early fusion model. We can observe that linear models outperform transformer encoder models in every embeddings. This is consistent with the observation in  [25] , and we think it may be caused by the overfitting problem. It also can be seen that the text embeddings always outperform speech embeddings. This is because there are not many complex contexts where speech is more effective, such as sarcasm, in this dataset. When comparing to the three segment-level speech embeddings, frame-level speech embeddings perform better in linear models, while syllable-level embeddings perform better in transformer encoder models. The segment-level speech embeddings have more information linked to prosody, thus it can provide more information when combined with the frame-level speech embeddings and text embeddings. This will be validated in the next subsection. It is worth noting that when using framelevel speech embeddings, it corresponds to a reproduction of the approach in  [5]  and the UA (65.44%) is very similar to that (65.80%) given in  [5] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results Of The Multi-Level Fusions",
      "text": "Table  2  shows the results of our coattention-based early fusion model using Equation  4 and late fusion model using Equation  3 with combinations of different segment-level speech embeddings. It shows that, compared to unimodal results in table 1, any combination can yield substantial performance improvement. It demonstrates that there is indeed complementarity between two modalities on this task. It can also be seen from Table  2  that late fusion models generally have better results than early fusion models. The multi-granularity frame works quite well on the late fusion model and also achieves better results in the early fusion model than inputs without segmentlevel speech embeddings. It can be seen from both fusions that adding more embeddings cannot always lead to better performance, thus the improvement brought by the introduction of segment-level speech embeddings does not result from the effect of ensemble learning, but result from the introduction of prosodic information which is relevant to emotion recognition. This subsection further validates the effectiveness of the proposed approaches. In the first block of Table  3 , the effectiveness of the coattention-based early fusion approach is first validated, as it can be seen that with the frame-level and phone-level speech embeddings, the coattention can give good performance gains when comparing with the concatenation-based fusion, improving the UA by about 3%. Here we provide the comparison between the coattention and the concatenation-based early fusion only with frame-level and phone-level speech embeddings due to space limitation, but there are similar results in the remaining cases. We copy the best results of coattention-based early fusion (74.57%) and late fusion (75.80%) from Table  2 . This demonstrates that by combining the best configurations of both early (F+P) and late (F+S) fusions (so used embeddings are F+P+S), it gives another 0.5% UA increase (76.31%) over the best performing late fusion model. This proves that combining the multi-modal models at multiple levels can improve the performance.\n\nIn the second block of Table  3 , the proposed approaches are also compared with other state-of-the-art multi-modal speechtext emotion recognition approaches. For a fair comparison, all the approaches are based on the 5-fold cross validation configuration and use the same way of preprocessing of IEMO-CAP. It shows that our model using text and only frame-level speech embeddings already surpasses the performance of most baseline approaches, demonstrating the advantage of combining wav2vec 2.0 and BERT. In addition, the best multi-level fusion surpasses the best baseline approach by 1.3% UA.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we have proposed to leverage the state-of-the-art wav2vec 2.0 and BERT embeddings with a multi-level fusion framework to mitigate the issue of data sparsity in multimodal emotion recognition and also explored the multi-granularity framework. Our best fusion configuration achieves an accuracy of 76.31% UA for the 5-fold CV on IEMOCAP. In the future, we plan to explore the fine-tuning of wav2vec 2.0 and BERT in our model while in this paper we fix them and only use them as feature extractor.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the embeddings",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the late fusion model.",
      "page": 2
    },
    {
      "caption": "Figure 1: Proposed late fusion model.",
      "page": 2
    },
    {
      "caption": "Figure 2: Proposed coattention-based early fusion model.",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the coattention-based early fusion model we",
      "page": 2
    },
    {
      "caption": "Figure 2: is illus-",
      "page": 3
    },
    {
      "caption": "Figure 3: There are two branches in one coattention",
      "page": 3
    },
    {
      "caption": "Figure 3: Coattention model.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Unweighted accuracy (UA) of the 5-fold CV results Table 3: Performance comparison between our models and",
      "data": [
        {
          "F": "F+P\nF+S\nF+W",
          "74.28%": "74.57%\n74.05%\n74.27%",
          "74.88%": "75.15%\n75.80%\n75.22%"
        },
        {
          "F": "F+P+S\nF+P+W\nF+S+W",
          "74.28%": "73.35%\n73.96%\n73.53%",
          "74.88%": "74.67%\n75.52%\n75.59%"
        },
        {
          "F": "F+P+S+W",
          "74.28%": "73.64%",
          "74.88%": "74.60%"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Towards emotion-aware recommender systems: an affective coherence model based on emotion-driven behaviors",
      "authors": [
        "M Polignano",
        "F Narducci",
        "M De Gemmis",
        "G Semeraro"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "5",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "6",
      "title": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "7",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "8",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "9",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2019",
      "venue": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "arxiv": "arXiv:1910.10683"
    },
    {
      "citation_id": "10",
      "title": "Leveraging BERT for extractive text summarization on lectures",
      "authors": [
        "D Miller"
      ],
      "year": "2019",
      "venue": "Leveraging BERT for extractive text summarization on lectures",
      "arxiv": "arXiv:1906.04165"
    },
    {
      "citation_id": "11",
      "title": "Learning to answer by learning to ask",
      "authors": [
        "T Klein",
        "M Nabi"
      ],
      "year": "2019",
      "venue": "Getting the best of GPT-2 and BERT worlds",
      "arxiv": "arXiv:1911.02365"
    },
    {
      "citation_id": "12",
      "title": "Transformer models for text-based emotion detection: a review of BERT-based approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "13",
      "title": "Comparative analyses of BERT, RoBERTa, DistilBERT, and XLNet for text-based emotion recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "17th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)"
    },
    {
      "citation_id": "14",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "15",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2019",
      "venue": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "16",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Pushing the limits of semi-supervised learning for automatic speech recognition",
      "authors": [
        "Y Zhang",
        "J Qin",
        "D Park",
        "W Han",
        "C.-C Chiu",
        "R Pang",
        "Q Le",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Pushing the limits of semi-supervised learning for automatic speech recognition",
      "arxiv": "arXiv:2010.10504"
    },
    {
      "citation_id": "18",
      "title": "Exploring wav2vec 2.0 on speaker verification and language identification",
      "authors": [
        "Z Fan",
        "M Li",
        "S Zhou",
        "B Xu"
      ],
      "year": "2020",
      "venue": "Exploring wav2vec 2.0 on speaker verification and language identification",
      "arxiv": "arXiv:2012.06185"
    },
    {
      "citation_id": "19",
      "title": "Group Gated Fusion on Attention-Based Bidirectional Alignment for Multimodal Emotion Recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Multimodal Emotion Recognition with High-level Speech and Text Features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "21",
      "title": "A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "authors": [
        "A Patil"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
      "authors": [
        "P Kumar",
        "V Kaushik",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "25",
      "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "A Gelbukh",
        "A Zadeh",
        "L -P. Morency",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "27",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "28",
      "title": "The where and when of linguistic word-level prosody",
      "authors": [
        "J Arciuli",
        "L Slowiaczek"
      ],
      "year": "2007",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "29",
      "title": "Mixture density network for phone-level prosody modelling in speech synthesis",
      "authors": [
        "C Du",
        "K Yu"
      ],
      "year": "2021",
      "venue": "Mixture density network for phone-level prosody modelling in speech synthesis"
    },
    {
      "citation_id": "30",
      "title": "Attention and feature selection for automatic speech emotion recognition using utterance and syllable-level prosodic features",
      "authors": [
        "S Alex",
        "L Mary",
        "B Babu"
      ],
      "year": "2020",
      "venue": "Circuits, Systems, and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels",
      "authors": [
        "C.-H Wu",
        "W.-B Liang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2013",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "33",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "34",
      "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-andlanguage tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "36",
      "title": "Speech emotion recognition with data augmentation and layer-wise learning rate adjustment",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "37",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "38",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    }
  ]
}