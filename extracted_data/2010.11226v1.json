{
  "paper_id": "2010.11226v1",
  "title": "Dynamic Layer Customization For Noise Robust Speech Emotion Recognition In Heterogeneous Condition Training",
  "published": "2020-10-21T18:07:32Z",
  "authors": [
    "Alex Wilf",
    "Emily Mower Provost"
  ],
  "keywords": [
    "Deep learning",
    "domain adaptation",
    "affective computing",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Robustness to environmental noise is important to creating automatic speech emotion recognition systems that are deployable in the real world. Prior work on noise robustness has assumed that systems would not make use of sampleby-sample training noise conditions, or that they would have access to unlabelled testing data to generalize across noise conditions. We avoid these assumptions and introduce the resulting task as heterogeneous condition training. We show that with full knowledge of the test noise conditions, we can improve performance by dynamically routing samples to specialized feature encoders for each noise condition, and with partial knowledge, we can use known noise conditions and domain adaptation algorithms to train systems that generalize well to unseen noise conditions. We then extend these improvements to the multimodal setting by dynamically routing samples to maintain temporal ordering, resulting in significant improvements over approaches that do not specialize or generalize based on noise type.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition provides an opportunity to understand how emotion patterns in daily life are associated with health, both mental and physical  [1, 2] . The inexpensive production of audio recording-capable devices has made speech emotion recognition (SER) an attractive avenue for the deployment of emotion recognition systems and recent advances in machine learning have led to improved accuracy in state-of-the-art SER systems, but robustness to additive noise in SER is still an open problem. In this work, we introduce a novel task for training and testing SER systems for noise robustness that simulates real-world use; demonstrate that we can successfully customize feature encoders to noise conditions known at training time; apply domain adaptation methods commonly used to generalize performance across datasets to the task of generalizing across noise conditions; and extend these improvements to the multimodal setting using a process we describe as Dynamic Layer Customization (DLC).\n\nIn considering how a SER system bound for real-world deployment would be developed, it is reasonable to assume that the system's designers may have some knowledge of the noise conditions that will appear at deployment, either through empirical studies or expert knowledge. Prior works on noise robust speech tasks have either assumed that when a system trains with all test noise conditions known during training, the system makes no use of the noise conditions on a sample-by-sample basis, or when a system trains on a subset of the test noise conditions, it is able to use the unlabelled samples noised with the \"unseen\" noise conditions in training  [3] [4] [5] . In this paper, we introduce a novel noise robustness task, heterogeneous condition training, where systems either have access to all or some of the test noise conditions on a sample-by-sample basis during training, and \"unseen\" noise conditions remain unseen during training. We show, for the first time, that networks which dynamically route samples based on noise condition can enhance noise robust SER performance when training noise conditions match testing noise conditions, and that domain adaptation can be used without unlabelled target data to improve performance when some noise conditions are unseen in training.\n\nOur first experiment examines the case where a network has access to all test noise conditions at training, and shows that it is possible to specialize \"expert\" feature encoders for each noise condition -which we call full customizationand improve performance over the same system implemented with a single feature encoder. The situation where a system designer has access to all test noise conditions at train time is justified in a deployment scenario in which a small number noise conditions make up such a majority of cases or a certain noise condition is so frequent as to justify a system designed with a noise condition predictor and a specialized feature encoder to handle it. This is a common paradigm for technology deployed in a static environment, for example, a single clinic.\n\nA simple implementation of full customization would be to split the dataset into subdatasets by noise type and train and test networks with separate feature encoders on each subdataset. This approach, which we will refer to as statically customizing layers, introduces a subtle yet insidious prob-lem: splitting the dataset corrupts the temporal order of the samples when neighboring samples contain varying types of noise -a common phenomenon in natural collections of emotional data  [6] . While unimodal acoustic SER models usually consider each utterance independently, multimodal models frequently require temporal consistency, as they use context from neighboring samples to predict the emotional content of each sample  [7, 8] . To solve this problem, we introduce a novel paradigm in noise robust SER: dynamically routing samples to different feature encoders based on noise condition and recombining the outputs in the original order through DLC. Full customization can be thought of as a special case of a mixture-of-experts (MoE) model  [9] , in which the number of experts in each ensemble is one.\n\nOur second experiment examines the case where a network has access to some, but not all, test noise conditions at training. We support the finding, from related speech tasks, that domain adaptation can be used to generalize across noise conditions  [4, 5] , and extend these findings to the case where unlabelled target information is not provided to the network during training. We show that domain adaptation methods can lead to significant performance improvements over single and fully customized networks. We choose the highest performing domain adaptation method from our unimodal tests, Domain Separation Network (DSN), to extend to the multimodal setting. As DSN uses separate feature encoders for each domain, we use DLC to extend DSN to the multimodal setting so as not to disrupt the temporal ordering of samples.\n\nIn rest of this paper, we detail the methods, data, and experiments used and show that specialized feature encoders can improve emotion recognition in the presence of known noise conditions; domain adaptation can be used without unlabelled target data to generalize to unseen noise conditions; and dynamic routing through DLC can extend these performance improvements to the multimodal setting.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "Our baseline network consists of a feature encoder layer linked to both an emotion classifier (trained with cross entropy loss) and a decoder (trained with mean squared error loss, comparing the output with the clean input to encourage denoising). The feature encoder layer either contains a single feature or encoder or multiple (in the cases of full customization and DSN). Our architectures for each component (feature encoder, classifier, decoder, and adversary) are constant across all networks, and are based on Khorram et al.'s approach to unimodal acoustic emotion recognition using dilated convolutions  [10] .\n\nThe feature encoder is implemented with three 1-D convolution layers with kernel size 16, 128 feature maps, and dilation rates increasing by powers of two with each successive layer as in  [10, 11] , followed by a 1-D MaxPool with pool size 4 and 4 strides. The decoder layer for feature reconstruction consists of two 1-D convolution layers with 128 and 40 feature maps, kernel size 3, and 2 strides, followed by a single 1-D convolution layer with 40 feature maps, kernel size 3, and 1 stride. The adversary and classifier layers each consist of three dense layers with two 128 unit layers followed by a layer where the number of units is either the number of noise conditions (adversary) or the number of emotion bins (classifier). For each method, we use the Adadelta optimizer with learning rate 1e-3.\n\nWe test unimodal and multimodal variations of this architecture that leverage customization and domain adaptation for heterogeneous condition training. In the subsections that follow, we describe the details of the domain adaptation methods we test, DLC, and the multimodal setting of the task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dann",
      "text": "The Domain Adversarial Neural Network  [12]  is an approach to domain adaptation in which features are passed through a feature encoder, then the encoded features are passed through a task classifier and an adversarial domain classifier (the latter is preceded by a gradient reversal layer). In this way, the feature encoder is encouraged to output encodings such that using those encodings, the task classifier is able to predict the task, but the domain classifier is unable to predict the domain. DANN has shown promising results on cross-corpus vision tasks, and has been successfully applied to SER, though the authors noted that it had difficulty converging  [13] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Maddog",
      "text": "The Multiclass Adversarial Discriminative Domain Generalization network  [11]  is a variation on DANN where the adversary (called a \"critic\") uses a linear activation with loss based on WGAN-style \"earth mover's distance\"  [14]  instead of cross-entropy loss with a softmax activation. The critic is trained separately at the beginning of each epoch and then is frozen. MADDoG has shown promising results on the task of SER in domain generalization -a domain adaptation variation where some labelled test samples are available at training.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dsn",
      "text": "The purpose of the Domain Separation Network (DSN)  [15]  is to learn a \"shared\" encoder that extracts features that are generalizable across domains. DSN achieves this by learning \"private\" feature encoders that encode the parts of a sample unique to each domain, trained with losses to encourage that, for each sample: the shared and private encoders yield different representations; all information relevant to reconstruction is captured by the normalized sum of the outputs of the shared and private encoders; the shared encoding is sufficient to classify the task; and the outputs of the shared encoder for samples from different domains are so similar as to be indistinguishable by a DANN-style adversary. In the original paper, DSN created batches by randomly sampling from each domain, but as doing so disrupts the original ordering we require to extend to the multimodal setting (described in Section 2.4), we use DLC to dynamically route samples to different private encoders and back for reconstruction and classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Setting",
      "text": "We extract transcripts over the noise enhanced audio files using Deepspeech  [16]  and pre-process the transcripts using BERT embeddings  [17] . We use a state-of-the-art multimodal fusion network: Hierarchical Feature Fusion Network (HFFN)  [8] . HFFN extracts independent features for each modality (i.e., lexical, acoustic) before \"fusing\" them and learning to recognize emotion using cross-utterance context. We refer the reader to Mai et al.  [8]  for additional details.\n\nWe use DLC to test different methods as unimodal acoustic feature encoders for HFFN, maintaining the original ordering HFFN requires for cross-utterance learning. To do this, we split each batch of input samples into sub-batches based on noise condition. We process each sub-batch through its respective feature encoder, then recombine samples using their original indices in the batch before passing the encodings on to the rest of the network. Doing so enables a unimodal feature encoder layer to use multiple specialized experts, either for full customization or for domain adaptation (in the case of DSN), as part of an end-to-end multimodal network.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "We consider three datasets in our experiments. MSP-Improv is an acted, audiovisual emotional database which aims to approach the naturalness of unsolicited human interactions by asking the actors to embed a \"target sentence\" into an improvised interaction  [18] . MSP-Improv was collected over six sessions with twelve actors and contains 8,438 utterances, each labelled for valence and activation on a scale of 1-5. We convert the n valence ratings into a three bin vector describing the distribution of the sample over \"low\", \"medium\", and \"high\" valences by binning ratings below, equal to, and greater than the midpoint (3) and dividing by n as in  [11] . MOSEI contains 23,500 utterances extracted from \"in the wild\" videos on Youtube, labelled for sentiment in the range -3 to 3  [19] . We also consider negative, neutral, and positive bins for MOSEI, this time by partitioning ratings with 0 as the midpoint. IEMOCAP was collected over five sessions from ten actors (five male, five female)  [20] . Each of the 10,039 utterances is labelled with emotional categories (e.g., anger, happiness, sadness, neutrality) and dimensional labels (i.e., valence, activation, dominance). Though we use dimensional labels to evaluate results on MSP-Improv and MOSEI, we evaluate performance on IEMOCAP using categorical labels to be consistent with prior work  [7, 8] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "We use the Librosa Python library  [21]  to extract 40 dimensional log Mel Filterbanks (MFB), which have shown effectiveness in SER  [10, 11] . We also use Librosa, along with the ESC-50 environmental noise dataset  [22] , to overlay additive noise with different signal to noise ratios (SNR), detailed below. Our experiments were run across a single machine using 3 GPUs: 1x GTX 1080, 2x Titan X. The code to reproduce our results will be posted by the authors.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In our experiments, we add noise to each dataset in different profiles, selecting randomly from among three noise conditions comprising both a noise type -either \"natural\", \"human\", or \"interior\" -and a signal to noise ratio (SNR). After selecting a noise condition, we overlay the original sample with a randomly chosen audio file from that category of the ESC-50 dataset  [22]  at the given SNR. Our noise profiles are inspired by real life situations. h1 is inspired by a grocery store environment, with (natural, interior, human) SNR values of (-5, -20, -20). h2 is inspired by a sidewalk environment, with values of (-20, -1, -5). h3 is inspired by an interior environment, with values of (-5, -30, -10). Each result reported in Table  1  is the average of five trials.\n\nWe need access to the noise condition of samples at test time for our full customization network to be able to route samples to different feature encoders. However, this is not realistic for a deployed algorithm, so we train a noise predictor to generate noise condition labels for test samples. It is a feature encoder followed by a classifier layer, and averages 87% accuracy at predicting noise condition labels on the test set (across noise profiles).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment 1",
      "text": "We first consider the case where all test noise conditions are known at train time, and test whether a fully customized network with multiple feature encoders -one for each noise condition -will outperform the baseline network (labelled \"single\" in Table  1 ). We examine splitting both statically (labelled \"multi\") and dynamically (labelled \"multi-DLC\"), and extend our baseline and multi-DLC models to the multimodal setting with HFFN (as extending multi without DLC would corrupt the order of the samples).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment 2",
      "text": "Next, we investigate if performance can be improved in the case where we leave one noise condition out during training (a common testing approach in domain adaptation problems  [11, 15] ). In this experiment, we examine whether the domain adaptation methods described in Section 2 can encourage the feature encoders to generalize across noise conditions. We choose the most promising method from our unimodal tests (DSN-DLC) to extend to the multimodal setting, using DLC to maintain the order of samples. The reported performance is the average performance of the model, leaving each noise condition out once as the unseen noise condition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "Results for both experiments are listed in Table  1 . In Experiment 1, our results support our hypothesis that a fully customized network with multiple feature encoders specialized for particular noise conditions (multi) outperforms our baseline network with a single feature encoder (single). In the unimodal setting, the fully customized network implemented dynamically using DLC performs similarly to the same network implemented statically, and shows an average improvement of 2.13% UAR (unweighted average recall) across noise profiles and datasets over the baseline network. In the multimodal setting, this difference is less pronounced: 1.63%. We hypothesize that this is due to the fact that lexical embeddings play a prominent role in SER, so differences only applied to acoustic features will result in a smaller improvement. Future work may find more improvements in the multimodal setting by using denoised audio for transcription.\n\nIn Experiment 2, we found that the fully customized network with multiple feature encoders did not outperform the baseline with a single feature encoder. We believe that this is because there is no generalization within the specialized feature encoders, so each encoder is poorly equipped to handle samples noised with the test noise condition. Our results showed that the domain adaptation methods significantly improved upon the baseline (single) and fully customized (multi) approaches. DSN performed the best, improving on a single, ungeneralized feature encoder by an average of 2.49%.\n\nWe implement DSN (the highest performing method in the unimodal setting) with DLC in the multimodal setting, and find an average improvement of 2.06% over our multimodal network with the single ungeneralized feature encoder.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present heterogeneous condition training as a novel training and evaluation task for noise robustness in SER that permits full or partial knowledge of test noise conditions at training time. We improve performance on this task over our baseline network by training specialized subnetworks and applying domain adaptation methods in the absence of unlabelled target data, treating noise conditions as domains. We extend these findings to the multimodal setting by dynamically routing samples to and from specialized feature encoders, maintaining the temporal order of the samples. We believe that heterogeneous condition training provides a useful task for future work in noise robustness to test against, and that our findings -that individual feature encoders can effectively specialize to specific noise conditions, and that domain adaptation methods can be used to generalize to unseen noise conditions -will help shape the deployment of SER systems in the real world.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nNoise Proﬁle": "Experiment 1\nnone\nsingle\nmulti\nmulti-DLC\nHFFN-single\nHFFN-multi-DLC",
          "MSP\nh1\nh2\nh3": "48.95\n51.17\n51.59\n51.39\n53.51\n54.99\n53.05\n55.10\n56.95\n53.12\n54.97\n56.89\n57.47\n59.75\n60.68\n58.98\n61.05\n62.79",
          "IEMOCAP\nh1\nh2\nh3": "55.14\n56.49\n57.29\n58.29\n59.01\n60.55\n60.21\n62.41\n62.45\n60.36\n62.23\n62.70\n63.98\n66.17\n66.88\n65.37\n67.96\n68.48",
          "MOSEI\nh1\nh2\nh3": "39.26\n39.42\n40.13\n42.25\n42.98\n43.40\n44.50\n44.75\n45.61\n44.74\n44.86\n45.72\n49.41\n48.97\n50.22\n51.44\n50.19\n51.97"
        },
        {
          "Dataset\nNoise Proﬁle": "Experiment 2\nnone\nsingle\nmulti\nmulti-DLC\nDANN\nMADDoG\nDSN-DLC\nHFFN-single\nHFFN-dsn-DLC",
          "MSP\nh1\nh2\nh3": "48.45\n49.58\n50.18\n49.58\n51.28\n51.70\n48.21\n49.78\n49.16\n48.16\n49.96\n49.21\n51.50\n52.10\n52.29\n51.88\n52.88\n52.37\n52.22\n53.13\n54.44\n55.83\n57.29\n58.53\n58.28\n59.31\n60.66",
          "IEMOCAP\nh1\nh2\nh3": "53.25\n53.99\n55.67\n55.81\n56.94\n57.92\n52.24\n51.43\n54.25\n52.18\n51.43\n54.29\n57.40\n58.26\n59.80\n58.16\n58.28\n60.48\n57.92\n60.12\n61.61\n62.70\n64.83\n66.02\n66.08\n66.05\n67.90",
          "MOSEI\nh1\nh2\nh3": "37.17\n37.82\n37.63\n39.38\n40.53\n40.91\n38.21\n39.08\n39.44\n38.19\n39.12\n39.41\n40.15\n41.23\n42.18\n40.19\n41.75\n42.74\n41.21\n42.50\n43.32\n45.62\n46.66\n47.23\n46.66\n48.48\n49.83"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Automatic voice emotion recognition of child-parent conversations in natural settings",
      "authors": [
        "-C Law",
        "S Soleimani",
        "D Watkins",
        "J Barwick"
      ],
      "year": "2020",
      "venue": "Behaviour & Information Technology"
    },
    {
      "citation_id": "3",
      "title": "The role of affect and emotion in hci",
      "authors": [
        "R Beale",
        "C Peter"
      ],
      "year": "2008",
      "venue": "Affect and emotion in human-computer interaction"
    },
    {
      "citation_id": "4",
      "title": "Improving generalisation and robustness of acoustic affect recognition",
      "authors": [
        "F Eyben",
        "B Schuller",
        "G Rigoll"
      ],
      "year": "2012",
      "venue": "Proceedings of the 14th ACM international conference on Multimodal interaction"
    },
    {
      "citation_id": "5",
      "title": "Noise adaptive speech enhancement using domain adversarial training",
      "authors": [
        "C.-F Liao",
        "Y Tsao",
        "H.-Y Lee",
        "H.-M Wang"
      ],
      "year": "2018",
      "venue": "Noise adaptive speech enhancement using domain adversarial training",
      "arxiv": "arXiv:1807.07501"
    },
    {
      "citation_id": "6",
      "title": "Unsupervised adaptation with domain separation networks for robust speech recognition",
      "authors": [
        "Z Meng",
        "Z Chen",
        "V Mazalov",
        "J Li",
        "Y Gong"
      ],
      "year": "2017",
      "venue": "2017 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "7",
      "title": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with lstm neural networks",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "J Han",
        "J Deng",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Interspeech 2016. International Speech Communication Association (ISCA)"
    },
    {
      "citation_id": "8",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "9",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "authors": [
        "N Shazeer",
        "A Mirhoseini",
        "K Maziarz",
        "A Davis",
        "Q Le",
        "G Hinton",
        "J Dean"
      ],
      "year": "2017",
      "venue": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "arxiv": "arXiv:1701.06538"
    },
    {
      "citation_id": "11",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "arxiv": "arXiv:1708.07050"
    },
    {
      "citation_id": "12",
      "title": "Improving cross-corpus speech emotion recognition with adversar-ial discriminative domain generalization (addog)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "14",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Wasserstein gan",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "Wasserstein gan",
      "arxiv": "arXiv:1701.07875"
    },
    {
      "citation_id": "16",
      "title": "Domain separation networks",
      "authors": [
        "K Bousmalis",
        "G Trigeorgis",
        "N Silberman",
        "D Krishnan",
        "D Erhan"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "17",
      "title": "Deep speech: Scaling up end-to-end speech recognition",
      "authors": [
        "A Hannun",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "G Diamos",
        "E Elsen",
        "R Prenger",
        "S Satheesh",
        "S Sengupta",
        "A Coates"
      ],
      "year": "2014",
      "venue": "Deep speech: Scaling up end-to-end speech recognition",
      "arxiv": "arXiv:1412.5567"
    },
    {
      "citation_id": "18",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "19",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "23",
      "title": "Esc: Dataset for environmental sound classification",
      "authors": [
        "K Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia"
    }
  ]
}