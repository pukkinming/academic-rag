{
  "paper_id": "2409.08907v1",
  "title": "Affective Computing Has Changed: The Foundation Model Disruption",
  "published": "2024-09-13T15:20:18Z",
  "authors": [
    "Björn Schuller",
    "Adria Mallol-Ragolta",
    "Alejandro Peña Almansa",
    "Iosif Tsangko",
    "Mostafa M. Amin",
    "Anastasia Semertzidou",
    "Lukas Christ",
    "Shahin Amiriparian"
  ],
  "keywords": [
    "Affective Computing",
    "Foundation Models",
    "Large Language Models",
    "Large Multimodal Models",
    "Disruption"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The dawn of Foundation Models has on the one hand revolutionised a wide range of research problems, and, on the other hand, democratised the access and use of AI-based tools by the general public. We even observe an incursion of these models into disciplines related to human psychology, such as the Affective Computing domain, suggesting their affective, emerging capabilities. In this work, we aim to raise awareness of the power of Foundation Models in the field of Affective Computing by synthetically generating and analysing multimodal affective data, focusing on vision, linguistics, and speech (acoustics). We also discuss some fundamental problems, such as ethical issues and regulatory aspects, related to the use of Foundation Models in this research area.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "\"The world of Affective Computing has changed. I see it in the vision modality. I read it in the linguistic modality. I hear it in the speech modality. Much that once was is outdated...\" This quote, which might sound slightly familiar to the J. R. R. Tolkien's readers, aims to literary exemplify how the disruption of Foundation Models (FM) might be impacting the Affective Computing research as we knew it. Before centring the discussion on this topic, we summarise where we come from.\n\nThe Affective Computing research can be broadly clustered into three main categories: recognition of affect, generation of affective content, and response to affect  [1] . The development of systems with affective features opens a vast arsenal of use cases, ranging from digital psychology recognising depression  [2]  to security applications (e. g., stress prediction in driving  [3] ). However, Affective Computing applications are mostly centred in Human-Computer and Human-Robot Interaction (HCI/HRI)  [4] [5] [6] , where the ability to interpret the human affect is not only desirable to improve the communication  [7, 8] , but a necessary capability to correctly understand the message. Early work in the field of psychology indicated that human affect is communicated in a multimodal manner through physical channels such as facial expressions, language, or voice  [9] . Consequently, the Affective Computing community has paid significant attention to visual  [10, 11] , linguistic  [12, 13] , and speech (acoustic)  [14, 15]  data processing, including multimodal configurations  [16, 17] .\n\nOf the different dimensions of human affect, the emotional states have attracted major attention for its impact in a wide range of aspects of peoples' lives. Given such relevance, the Affective Computing community has been putting major efforts in the development of automated systems for the recognition and understanding of the human feelings. Early research in the field of emotion recognition relied on conventional Machine Learning (ML) pipelines, in which expert-crafted features were first extracted from the raw data such as pixels, words, or an audio signal, and then processed utilising traditional statistical methods; e. g., Support Vector Machines (SVM). The key to this conventional approach was to try to design a suitable set of features that captures emotional content; i. e., the hand-crafted features. In the visual domain, the emotional content was assumed to be mainly reflected in the facial expression information, from which the features were extracted; e. g., through Principal Component Analysis (PCA), or traditional Computer Vision (CV) techniques, such as Gabor wavelets or texture filters. In addition, prior knowledge of the Action Units (AUs) defined in the Facial Action Coding System (FACS)  [18]  helped in the design of these features. In linguistics, feature extraction typically relied on n-grams or bag-of-words -a vector representation of text by sparse vectors that represent some form of vocabulary and some form of frequency of occurrence of the words in the current text -, after carefully preprocessing the raw text (e. g., by stemming or stopping). In audio, several works noticed the rich emotional information reflected in the prosodic features  [19] , which in combination with spectral features -e. g., the Mel Frequency Cepstral Coefficients (MFCC) -, formed the basis for Speech Emotion Recognition (SER).\n\nThe success of Deep Learning (DL) in the early 2010's entailed a disruption to the entire field of Artificial Intelligence. The development of affective systems embraced the new trend as well, which was mainly marked by the use of representational learning via Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN) proved to be extremely useful for extracting appropriate features when large amounts of data were available. Consequently, feature engineering took a backseat when data-driven approaches unleashed their potential. This could be seen as the first disruption of the field: the learning of representations. In the visual domain, end-to-end CNN-based systems accepted raw images as input, discarding the need for any feature engineering. In linguistics, complex preprocessing steps and statistical representations lost their relevance with the success of word embedding models, which could not only be utilised as input for a DNN model, but also contained semantic meaning in their own space, including affective information  [20] . Moreover, some speech systems began to process the raw time signal  [21]  or the spectrogram representations with CNNs to take advantage of their enhanced representational capabilities.\n\nTherefore, the efforts were ultimately shifted from experts crafting representations to experts choosing model architectures to learn these representations. A second, albeit less noted and exploited potential disruption came with the possibility of (neural) architecture search by reinforcement learning  [22]  or more efficient approaches  [23] . This meant that in principle, once having affective (labelled) data, the representation could be learnt to then analyse affective data as well as the best architecture to do so.\n\nThe data-driven disruption meant an improvement as well for synthesis of emotional data. In the visual domain, the introduction of adversarial learning  [24]  paved the way for Generative Adversarial Network-(GAN) based generative models, in which the emotional state was also controlled by explicit indicators  [25]  or by style transfer  [26] . Some works improved the semantic control over the output by identifying and traversing emotional directions in the GAN latent space  [27] . For text, for instance, RNNs could similarly produce affective language  [28] . In speech (acoustics), where traditional approaches mostly relied on Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM), new encoder-decoder architectures based on Long Short-Term Memory (LSTM) networks were devised, in which the emotional state could be controlled by both explicit labels  [29]  or style transfer from a reference  [30] .\n\nYet, a critical issue in this era was the acquisition of such reliable, annotated data. The subjectivity of measuring inner emotion through self-assessment shifted the focus to perceived emotion. However, 'measuring' outer perceived emotion usually requires several labellers to reduce uncertainty, hence coming at high effort and cost. In addition, the lack of spontaneous data sources -e. g., due to privacy restrictions -favoured the use of acted/elicited, non-spontaneous data samples  [31, 32] . Whilst non-or lessspontaneous data eased the problem of data availability, it came with the drawback that the analysis of real-world emotion struggles with subtlety not met in training. Further, generated data samples may reflect unrealistic emotional responses if systems are only trained on such data. The acquisition of data from Internet sources (e. g., social media, films) allowed the collection of large \"in-the-wild\" databases  [33, 34] , whose annotations were obtained through semi-automatic methods, crowdsourcing, or based on the criteria of experts in affect.\n\nNowadays, a third disruption is taking place in the Artificial Intelligence (AI) community. Whilst the first disruption established that (almost) no feature engineering was needed -just a powerful model and annotated data -, and the second allowed to learn also the optimal architectures, in current developments even specialised annotated affective data for training the models may no longer be needed, as affective computing abilities start to emerge in (general large data) pre-trained models as we will discuss in the following section. Nevertheless, curating high-quality sets of annotated data to some extent remains crucial to assess the performance of the models. New architectures, such as the Diffuser  [35]  or the Transformer  [36] , together with self-supervised learning strategies  [37]  and inter-modality alignment techniques  [38] , have led to new FMs  [39] [40] [41] [42] . These models have demonstrated surprising capabilities using prompt-based instructions, to the point that they can generate realistic data samples or perform zero-shot classification. The extent of the affective capabilities of these models, and the potential they open up, is still uncertain. Herein, we aim to shed some light on this topic, and explore how the emergence of FMs and the subsequent AI regulation are influencing the Affective Computing community.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Emergence In Foundation Models",
      "text": "One of the main characteristics of the Foundation Models (FMs) is that they are trained on a broad range of data, so that the resulting models can be utilised in a wide range of problems. In addition, they exploit large amounts of learning parameters. Given sufficient learning material, from a certain number of such parameters hence well trained, knowledge 'emerges' in the FMs, and they achieve competitive performances in tasks they have not been specifically trained for. This can, however, be difficult to predict  [43] . In this paper, we aim to investigate the 'emergent' affective capabilities of FMs. Focusing on the vision (cf. Section 2.1), linguistics (cf. Section 2.2), and speech (acoustics) (cf. Section 2.3) modalities, we assess the capabilities of current FMs to i) generate synthetic affective samples, from which we infer the conveyed emotions with pre-trained emotion recognition classifiers 1  , and ii) analyse well-established datasets in the field in a zero-shot manner. To favour the comparability among the different modalities investigated, we focus on the 'Big Six' Ekman emotions  [44]  (i. e., fear, anger, happiness, sadness, disgust, and surprise), in addition to the neutral state.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Vision Modality Has Changed",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Generation",
      "text": "In the visual domain, data synthesis started to obtain pseudorealistic results in the last decade thanks to the Generative Adversarial Network-(GAN) based models  [25, 30] . Nowadays, a boost in the quality of the synthesised images has been achieved via text prompt inputs-based models, due to i) the CLIP model  [38]  and ii) the Diffuser model  [45] . The former was presented in conjunction with DALL-E  [46] , as a model to predict how well a given caption describes an image; i. e., a text-to-image alignment.\n\nThe latter is a model that learns to reconstruct images by removing an added Gaussian noise through a Markov Chain. During inference, the model is able to generate new images from Gaussian noise, being more efficient than other generative architectures. Models such as Stable Diffusion (SD)  [35]  or DALL-E 2  [47]  integrate the CLIP and the Diffuser models to efficiently synthesise images with high semantic control, an important feature to generate affective samples. Further, the decision to make generative models, such as SD, open-source has allowed the general public to discover the potential of the technology, ultimately speeding up its advancements.\n\nWe have leveraged one of the latest versions of SD 2  -i. e., Stable Diffusion XL (SDXL)  [42]  -to synthetically generate a face emotion dataset 3  . This dataset is generated utilising prompts based on a fixed template with three sources of variation: i) the emotion, ii) the style (photorealistic, cartoon-painting, anime, and 3D), and iii) the demographic group. The template, along with the values explored for each attribute, are detailed in Table  1 . Table  2  presents a summary of the gathered dataset. Although our emotion model is based on the 'Big Six' Ekman emotions  [44]  plus the neutral state, we employed these basic emotions together with a higher intensity variation, as defined in Plutchik's model  [48] , to further emphasise the desired affective states. Also, note that the generation process spans 18 different demographic groups; determined by age, biological sex, and skin tone. Visual examples for each emotion and style are provided in Figure  1  for the demographic group < young, woman, white >. For the case of the photorealistic style, we played with the background to generate some samples in realistic scenarios (e. g., outdoors, office, park). The generation process involved two experts, the SDXL base model and the SDXL refiner  4  , with a total of 40 steps (80 % in the base model, 20 % in the refiner) and a guidance value of 7.5. Together with the desired prompt, we utilised a negative prompt to highlight what we do not want to see in the output. Once the images were generated, we filtered them according to four principles: the presence of disfigurations or artifacts, nudity, the quality of the emotion generated, and the plausibility of the style. For the sake of this experiment, only one annotator conducted this data curation process.\n\nWe now aim to automatically verify the affective quality of the generated facial images with Face Emotion Recognition (FER) models. We employ the manually annotated subset of the AffectNet dataset  [33]  to develop these models. The images belonging to this dataset are annotated in terms of eleven emotions. Nevertheless, we select the images corresponding to the emotions fear, anger, happiness, sadness, disgust, and surprise in addition to the neutral class. We process the selected images with OpenFace  [49]  to extract features related to a subset of the Action Units (AU) defined in the Facial Action Coding System (FACS). Specifically, OpenFace extracts 35 features per facial image, indicating the presence (0 or 1) and the intensity (in a scale from 0 -not present -to 5 -present with maximum intensity) of a subset of the AUs. We discard the images that OpenFace fails to process; for instance, due to the absence of a face in the image. Table  3  summarises the resulting data in terms of the number of images per emotion in the training and the validation partitions.\n\nTable  1 : Attributes defined in the input prompts to synthesise emotional facial images with Stable Diffusion XL  [42] . The prompt template considers three different sources of variation: the emotion, the style, and the demographic group. The latter is determined by three different demographic attributes: age, biological sex, and skin tone. We have also utilised a negative prompt, which includes all the styles that are not desired in the current synthesis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attribute Values",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prompt Template",
      "text": "Face image of a < age > < sex > with < skin > skin, with a < emotion > face, in a < style > style, realistic eyes, white background, ultra quality, frontal picture, looking at camera Negative prompt disfigured, unrealistic eyes, blurry, b&w, < style > Fig.  1 : Synthetic facial images of a white-skin, young woman conveying the 'Big Six' Ekman emotions  [44] , in addition to the neutral state. All the images were generated with Stable Diffusion XL  [42] , conditioned on four different styles, namely photorealistic (first row), cartoon-painting (second row), anime (third row), and 3D (fourth row). We start our preliminary investigation by training FER models with Support Vector Classifiers (SVC), as these are considered a standard machine learning technique with excellent results in a wide range of problems. We compare their performance when utilising a linear and a Radial Basis Function (RBF) kernel. One challenge associated with the selected dataset is the imbalanced training samples in terms of the emotional classes (cf. Table  3 ), which impacts the performance of the trained models. To overcome this issue, we consider weighting the training data, so that the samples corresponding to the least represented classes have more importance than the samples corresponding to the most represented classes when training the models. We fine-tune our models optimising the regularisation parameter C ∈ [10 -2 , 10 -1 , 1, 10, 10 2 ]. The performance of the optimal models on the validation partition is depicted in Table  4 .\n\nWe also contrast the performance of the SVC-based FER models with a stateof-the-art Vision Transformer  [50]  for Facial Expression Recognition 5  (ViT -FER), trained on the Facial Expression Recognition 2013 (FER-2013) dataset  [51] . In this case, we use the pre-trained model off-the-shelf -without fine-tuning -and evaluate Table  4 : Performance summary of the trained Support Vector Classifier-based models for Face Emotion Recognition on the validation partition of the considered subset of AffectNet. We also include the performance of a state-of-the-art Vision Transformer for Facial Expression Recognition (ViT -FER). We select the accuracy (ACC) as the metric to assess the model performances. its performance on the validation partition of our subset of AffectNet. The obtained results are reported in Table  4 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Model",
      "text": "The best performance on the validation partition of the AffectNet dataset (cf. Table  4 ) is obtained with the ViT -FER model. Thus, we use this model to assess the affective quality of the generated facial images. The results obtained from the conducted experiments exemplify the breakthrough of working with end-to-end approaches, operating on the raw images instead of on the features extracted from them. Nevertheless, it is also worth mentioning that AffectNet was collected in the wild, which may complicate the estimation of the AUs from the facial images and, in turn, worsen the performance of the SVC-based models.\n\nTable  5  summarises the results obtained when analysing the generated facial images with the four different styles utilising the ViT -FER pre-trained model. Figure  2  presents the confusion matrices computed, comparing the model inferences and the ground truth. Across styles, the worst results are those of the photorealistic style, as denoted by both accuracy -Weighted Average Recall (WAR) -and Unweighted  Average Recall (UAR)  6  . In contrast, the best results are obtained with the 3D style. Interestingly, in all the 4 styles both the neutral and the happiness emotions consistently obtain the best results. These classes are traditionally over-represented in face datasets (e. g., see the training set distribution of AffectNet in Table  3 ), probably due to the nature of the data sources  [52] . Consequently, it is expected for the generative model to be biased towards those emotions, which would also explain the difficulty to generate samples for classes like disgust or fear, which obtain the worst results.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Analysis",
      "text": "In order to evaluate the affective analytical capabilities of FMs in the image domain, we explore their performance in a zero-shot emotion recognition task under different configurations. Our experiments are conducted on the considered validation set of AffectNet  [33]  (cf. Section 2.1.1), as it is balanced, in-the-wild, and manually annotated. We compare three different approaches relying on model-prompting. In the first two, we start by extracting AU-based features with OpenFace  [49] , and we provide these features in a textual format as input to a FM. Recalling from Section 2.1.1, OpenFace predicts both the presence and the intensity of a subset of AUs; hence, two approaches can be derived from its predictions. On the other hand, the third approach consists in directly feeding the images within the prompt of a FM. Note that the first two approaches can be addressed with a Language Foundation Model (LFM), for which we select the LLaMA2 7B model. We utilise a Multimodal Foundation Model (MFM) for the third scenario; specifically, the LLaVA1.5 7B model 7    [53, 54] . This is an open-source MFM with visual capabilities trained by fine-tuning Vicuna 8  , an already fine-tuned version of LLaMA, with GPT-4 generated data. The selected models allow them having the same number of parameters.\n\nIn Table  7 , we present the results of the aforementioned scenarios. The prompts utilised for each scenario are detailed in Table  6 . We also include in Table  7  the results of the ViT -FER model on the considered validation set of AffectNet for comparability Table  6 : Prompts employed to perform zero-shot emotion recognition with Foundation Models in different scenarios. The prompts including (i. e., first two rows) AU information are injected in LLaMA2  [55] , while the prompt including the raw image is injected to a LLaVA1.5 model  [54] . purposes. Among the three zero-shot approaches, we obtain the best results when feeding the prompts with the raw images. Both AU-based prompt approaches exhibit accuracy results close to chance. The LLaVA model achieves an accuracy only 4 points below the ViT -FER model, which was explicitly trained on the FER-2013 dataset to recognise emotions. This is an interesting result, which suggests that the LLaVA1.5 model presents emergent affective capabilities, despite not being specifically trained on affective computing tasks.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Approach",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "The Linguistic Modality Has Changed",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Generation",
      "text": "The evolution of text generation experienced an important shift in the mid-2010s with the development of neural models  [56] . The introduction of the Transformer model  [36]  has revolutionised the Natural Language Processing (NLP) field and marked the beginning of the Large Language Models (LLM) era. Since then, this architecture has been   [40, 55] , democratised the access and fostered the innovation in the field. From an Affective Computing perspective, LLMs present a novel approach to inject and transfer emotional content in linguistic data, with recent works demonstrating their intrinsic emotional capabilities in a variety of domains  [59] [60] [61] [62] [63] .\n\nIn this section, we aim to investigate and leverage the affective style transfer capabilities of cutting-edge open-source LLMs. For this experiment, we select LLaMA2  [55]  and Mistral  [64] , given their proven high performance in both language understanding and text generation tasks. Additionally, we include the Mixtral 9 LLM  [65] , which utilises the Sparse Mixture of Experts (SMoE) method  [66] . It is important to note that the implementation of SMoE in Mixtral notably influences its size. The LLaMA2 and Mistral LLMs each comprise 7 billion parameters. We start with the text generation by compiling a corpus comprised of 122 human-curated neutral phrases. The dataset encompasses various topics, ranging from mundane personal activities to formal professional interactions, thus providing a targeted platform to evaluate how well LLMs handle affective style transfer across diverse topics. Utilising the gathered corpus, we task the aforementioned LLMs (i. e., LLaMA2, Mistral, and Mixtral) to generate six emotional phrases from each original neutral phrase, conveying a specific target emotion: fear, anger, happiness, sadness, disgust, and surprise. The generation is described by the formula M(text, emotion), text ∈ N , emotion ∈ E, where M is the LLM at hand, N the set of the neutral phrases, and E = {f ear, anger, happiness, sadness, disgust, surprise} (the prompted emotion). We generate three synthetic sentences for each combination of text and emotion, yielding a corpus of 3 * |N | * |E| = 2 194 emotional phrases 10 . The data was synthesised with 9 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1 10 For further information and access to the dataset, please contact the authors. settings chosen to enhance variability and creativity. The generation parameters used a temperature of 0.9, top-p of 0.6, and a repetition penalty of 1.2. An example of the generated sentences is shown in Table  8 , and Figure  3  visually summarises the followed pipeline. We observe that the models tend to exaggerate the injected emotion, adopting an over-dramatic style that results in more formal, yet affectively adapted phrases. Despite these exaggerated and dramatic adaptations, the primary objective was to investigate how these models perceive and express emotions under basic setup conditions. Our findings highlight the models' propensity to amplify emotional content, which was an anticipated aspect of this exploratory study.\n\nIn order to investigate the quality of the generated sentences by the LLMs, we implement two baseline models trained on the GoEmotions dataset  [67] , a wellrenowned corpus in the field and commonly utilised for benchmarking purposes due to its comprehensive labelling and categorisation of the emotions. The GoEmotions dataset consists of English Reddit comments annotated according to 27 distinct emotions, plus the neutral state, by 3 or 5 labellers each. Due to the nature of the annotations in the GoEmotions dataset, we begin by tailoring the data to meet the specific requirements of our experiments. First, we select instances from the dataset annotated with a single emotion, in order to tackle the task as a single-label classification problem, instead of a multi-label classification one. As previously, we adopt the 'Big Six' Ekman emotions  [44] , in addition to a seventh neutral state. This restructuring of the GoEmotions taxonomy to the Ekman taxonomy is achieved by aggregating Fig.  3 : Pipeline of the affective text style transfer process for generating the affective sentences with 'surprise' as the prompted emotion. After that, we classify the synthesised sentences using RoBERTa, GPT-3.5, and GPT-4.\n\nthe original labels into the targeted, broader categories  [67] . For example, emotions like annoyance and irritation, originally distinct, were grouped under 'anger' to fit the Ekman model. Table  9  summarises the statistics of the considered subset of the GoEmotions dataset.\n\nFor the two baselines, we employ two different architectures: a Bi-directional Long Short-Term Memory (BiLSTM) network and a fine-tuned version of the RoBERTabase model  [68] . Both models are trained on the selected subset of the GoEmotions dataset. The BiLSTM consists of two bidirectional LSTM layers with 128 units each. It is trained with a learning rate of 5 × 10 -3 and a batch size of 96 for 40 epochs, while RoBERTa-base was fine-tuned at a conservative learning rate of 5 × 10 -5 and a smaller batch size of 12. The models are trained on the training partition of the dataset, and the weights yielding the highest validation Unweighted Average Recall (UAR) are selected for each model. The test scores for both baseline models (BiLSTM and RoBERTa) on the test partition of the GoEmotions dataset are shown in Table  10 . The results are consistent with  [67] , but with some differences, given that we model the problem as a single-label classification, instead of the original multi-label classification task. Given the superior performance of RoBERTa, we utilise it for analysing the synthetically generated emotional sentences.\n\nTo evaluate the performance of the various LLMs on the emotion injection task, we test the generated sentences with the RoBERTa baseline model, in addition to GPT-4 as an approximation for human evaluation, and its weaker variant GPT-3.5. GPT-4 has shown superior performance in many affective computing problems  [62] , often better than fine-tuned, specialised models, especially with problems related to sentiment or emotions. Table  11  demonstrates the prompt templates used for the GPT models, following a similar pattern like  [62, 69] . The versions of GPT variants   used are 'gpt-3.5-turbo-0125' 11  for GPT-3.5 and 'gpt-4-turbo-2024-04-09'  12  for GPT-4. The results of this evaluation are depicted in Figure  4 . Notably, these results can be considered to reflect a better agreement between models than with the ground truth labels, which are not human-annotated. However, the results of GPT-4 should be the closest to the human evaluations  [62, 70] .\n\nGPT-4 as the most superior model achieves very high UAR scores on all six emotions. Its inferior model GPT-3.5 achieves slightly worse results in most cases, but it experiences a performance drop in the recognition of surprise. On the other hand, RoBERTa has a different behaviour in comparison. It is generally much worse than GPT-4 and GPT-3.5 in most of the cases, but it obtains a higher score than GPT-3.5 for the surprise emotion with the LLaMA2-and the Mixtral-generated sentences. Additionally, RoBERTa is showing very low performance for disgust, which seems to be a weakness of the model, consistent with the results on the GoEmotions dataset  [67] .\n\nFigure  5  depicts the confusion matrices obtained with the RoBERTa baseline model on the LLaMA2-, the Mistral-, and the Mixtral-generated sets. We also include its performance on the test set of GoEmotions as a reference. A common issue with the RoBERTa model is the confusion among anger and disgust. Analysing the confusion Table  11 : Prompts to use LLMs for zero-shot emotion recognition, following a similar pattern as in  [62]  and  [69] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Prompt Template",
      "text": "You are an expert at affective computing. Given a text by the user, analyze which emotion is most dominant in the given text. Only classify one of the seven Ekman emotions, namely: 'neutral', 'fear', 'anger', 'happiness', 'sadness', 'disgust', 'surprise'. You are only allowed to answer with EXACTLY ONE word corresponding to the aforementioned seven emotions. In case of multiple emotions, use ONLY the ONE emotion you are most confident about.\n\nUse the following format: * You are only allowed to answer with one of the following seven words: \"neutral\", \"fear\", \"anger\", \"happiness\", \"sadness\", \"disgust\", \"surprise\". * Don't write an explanation of the answer. * Don't write things like \"My guess is...\", or \"I think ...\". Just write the emotion, and nothing else. competitive performance on the emotion recognition task, reinforcing, one more time, the emergent affective capabilities of the models.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "2.3",
      "text": "The Speech Modality Has (Not Yet) Changed",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Generation",
      "text": "Research on generating affective speech has been conducted for more than three decades  [72] . The first approaches were rule-based, while contemporary methods typically rely on deep learning. A detailed overview can be found in  [72] . All of these methods are explicitly engineered to produce emotional speech. Thus, this line of research can be dubbed as a subfield of 'traditional' Affective Computing, while technically belonging to the Text-To-Speech (TTS) field. In recent years, similar to the developments in Natural Language Processing (NLP) and Computer Vision (CV), research on TTS has heavily been influenced by the success of the Foundation Model (FM) paradigm  [73, 74] . UniAudio  [73]  is a Transformer-based general-purpose audio synthesis system. It is pretrained on seven generative audio tasks, including TTS. In its pretrained state, however, it does not support affective speech synthesis. The authors demonstrate that their pretrained model serves as a basis for adaptation to different downstream tasks which opens up the possibility to equip the model with affective speech synthesis capabilities by mere finetuning. Another recent generative audio FM, PromptTTS2  [74] , synthesises speech based on text prompts that include descriptions of the voice to be generated. The controllable attributes of the synthesised speech must be defined during the training time. The authors of  [74]  do not investigate emotion as one such attribute, though their proposed framework would permit this. Models such as UniAudio and PromptTTS2 can thus be categorised as generative audio FMs that could be adapted to emotional speech synthesis with moderate effort. To the best of our knowledge, no evidence for affective speech synthesis as an emergent capability of such models has been published so far. However, the demos for GPT-4o 13  claim affective speech synthesis capabilities. At the moment, though, neither a technical report on GPT-4o, nor a systematic evaluation of affective speech produced by GPT-4o is available. Considering the development towards releasing large pretrained models in the NLP and CV areas, we assume that in the near future powerful speech synthesis models will also be made publicly available and, hence, investigated more thoroughly, allowing us to carry out according experiments to the above for vision and linguistics. In addition, we expect a continuing trend toward truly multimodal FMs that may not just take in but also produce natural speech with controllable prosodic properties. Several multimodal models that also produce audio output data have been proposed, for an overview see  [75, 76] . To the best of our knowledge, none of them exhibit emotional speech synthesis capabilities. We expect that emergent affective speech synthesis will eventually be achieved via a multimodal approach. As shown in the previous sections, large pretrained vision and language models already encode affective information. Multimodal approaches leveraging such pretrained models in combination with multimodal affectrelated data may learn to associate affective speech characteristics with corresponding affective states as expressed in the visual and the textual data.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Analysis",
      "text": "A system capable of analysing arbitrary affective properties of speech data without any tuning must ingest both audio and text inputs. Several FM approaches fulfilling this requirement have been proposed. However, the vast majority of them are not pretrained on speech data at all.\n\nExamples include AnyMAL  [77] , X-InstructBLIP  [78] , and ModaVerse  [79] . In only a few models, speech is part of the pretraining data. QWEN-Audio's training data comprises several labelled speech datasets, including emotionality already. Hence, QWEN-Audio  [80]  in the proposed form is not a candidate for exploring 'emergent' affective recognition capabilities. X-LLM  [81]  processes video, text, and audio inputs and is explicitly designed to process speech. The authors, however, do not report any experiments related to predicting affect in speech. As of now, the pretrained X-LLM model is not publicly available, hence, unfortunately again, not allowing us to carry out experiments analogous to the vision and the linguistic ones.\n\nSimilar to the affective speech synthesis problem, near-future multimodal FMs can be expected to be capable of analysing affective speech in a zero-shot manner, even if not explicitly pretrained in this regard. As of now, however, we are not aware of any such model.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "The Evaluation Is Changing",
      "text": "One of the reasons to understand the impressive performance of currently available Foundation Models (FM) is that they use massive amounts of data from \"the Internet\" for training. Nevertheless, the indiscriminate use of data poses the following challenge to the scientific community: can we guarantee that the data we feed to these models for testing -or even for evaluation -has not been used for training? In case of a negative answer, how fair and representative of the model capabilities can standard evaluation metrics be? Although we do not have a concrete answer yet, we hope these challenges engage the research community into looking for methods and metrics that allow a proper scientific evaluation of these emerging FMs in the field of Affective Computing. As is, the current state of such models in Affective Computing may resemble a shell game: many different tools and approaches are shuffled and mixed until some partially less, partially more convincing performances are obtained. Especially because it is the popular 'Big Six' Ekman emotions we considered herein, chances are high that the models only reverberate with what they have already seen. Testing on more subtle models such as the dimensional approach or less considered affective states is therefore urgently needed.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Concerns And Regulations Have Changed",
      "text": "In April 2021, the European Commission presented the AI Act: the first-ever legal framework worldwide to regulate the use of AI-based technologies in the European territory. The Act was endorsed by all Member States in February 2024 after being approved by the EU Council. The regulation follows a risk-based approach, so instead of regulating specific systems and applications, it defines measures and requirements based on a classification system that encapsulates varying degrees of risks posed by the AI systems. The proposed classification system spans four different levels of risk: unacceptable, high, limited, and minimal. According to the final version of the Act 14  , systems that fall into the unacceptable risk category will be prohibited.\n\nThis regulation is of special interest for the Affective Computing community, since it singles out affective systems in several ways. The regulation defines an emotion recognition system in its Article 3  (34)  as an \"AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data\". Thus, any emotion recognition system using speech or facial data, as well as other physical signals such as the electroencephalogram, falls under this definition. Article 5 enumerates different practices and systems considered as prohibited, including the placing on the market or the use of AI systems to infer emotions of a natural person in the workplace or in education institutions. Whilst this point only prohibits emotion recognition systems in two specific contexts (workplace and education), the list of high-risk systems presented in Annex III includes \"AI systems intended to be used for emotion recognition\" in the category of biometric systems. Article 6 provides a nuance on the referred systems, stating that AI systems \"shall not be considered as high risk if they do not pose a significant risk of harm to the health, safety, or fundamental rights of natural persons\". However, this exception shall not apply for systems performing profiling as defined in Article 4 of the General Data Protection Regulation (GDPR). Considering the definition of profiling, it is unclear whether any emotion recognition system, even in non-harmful applications such as entertainment, could be considered of limited risk, except for those based on non-identifiable data. In any case, Article 52 imposes that any system including an emotion recognition component must notify the users of the operation of such system, notwithstanding its risk.\n\nIn this context, the regulation imposes several obligations that high-risk systems shall comply with, and for which the provider (i. e., a natural person/agency that develops and places on the market the AI system) is responsible. These obligations are detailed across Chapters 2-5 of Title III -more than 40 articles -and include conducting post-market surveillance of risk, informing the competent authorities about the product, or providing technical documentation of both the system and the data used in development (i. e., data governance), among others. Note that the definition of provider is quite vague from a research perspective, so it is unclear how this will affect research on Affective Computing. Article 2 (5a) specifies that the regulation does not apply to models developed for the sole purpose of scientific research, but by the time the model is made available, researchers may face some of the previously cited obligations depending on how it is used, after which they may become providers. By considering cases in which the system is made available free of charge, the regulation seems to cover open-source systems and situations in which the source code is made publicly available. However, this does not seem to be the case when two academic institutions share code in a confidential manner for academic purposes.\n\nAttending now to Foundation Models (FM), which are referred in the text as \"General-purpose AI models\", Article 52 requires any AI-synthesised content to be marked as such, which clearly covers generation of emotional samples with models such as LLaMA, or SD, among others. Besides, Article 52 includes several subparagraphs with the obligations that providers shall meet when deploying this kind of systems, such as providing documentation on how it was trained and validated, and the content used for that purpose. In addition, an FM may be considered to pose systemic risk if it has high computational capabilities, or if it is marked as such by the Commission. In this case, providers shall comply with more obligations, including testing and mitigating foreseeable risks, data governance measures, maintaining appropriate levels of performance and interpretability. In this context, there are some concerns that current FMs do not comply with all the measures required  [82] , hence accepting to slow down innovation and AI development (particularly) in Europe, to assure highest safety and ethical standards.\n\nBeyond, emotion recognition based on physiological data has been less investigated than other modalities, such as facial expressions or speech. Some types of physiological signals include electroencephalogram (EEG), electrocardiogram (ECG), electromyogram (EMG), electrodermal activity (EDA) or galvanic skin response (GSR), respiration rate (RSP), and pulse rate. Emotions are complex and sometimes cannot be solely recognised by analysing speech or image data. It is quite easy for people to control -or even hide -their real current emotional state under certain circumstances, mainly because of social pressure. For instance, people may pretend to smile and laugh while they are feeling sad or angry  [83] . Ethical considerations regarding privacy and data security are of high importance when analysing physiological data for affective purposes to avoid both: '(Full) Affective Mind Reading' and 'Affective Mind Writing'.\n\nMany further challenges and ethical concerns remain and will also become more apparent once such technology is broadly used -hopefully, the community can provide technical and legal means of protection to ensure we are all enjoying the positive side of Affective Computing only.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Outlook And Conclusions",
      "text": "In this paper, we analysed the affective capabilities of currently available Foundation Models (FM) exploiting the vision, the linguistics, and the speech (acoustic) modalities. While the affective generation and analysis capabilities of the visionand the linguistic-based FMs are plausible, the affective generation and analysis of speech-based FMs is not yet mature enough. Nonetheless, it is reasonable to imagine a not-too-distant future where this technology achieves similar results as with the other two modalities. Despite not being currently available, we also envision physiological-based FMs to be developed and explored in the near future.\n\nOne of the main outcomes of this work is the collection of two syntheticallygenerated affective corpora generated with FMs -one containing facial images, the other textual sentences that will be publicly available. The models training and the analyses reported herein were performed assuming that the synthetically generated instances conveyed the prompted emotions. Nonetheless, we acknowledge this could not always be the case. To overcome this limitation, we plan to run a data collection with human annotators to annotate the generated samples, assessing the affective capabilities of the selected FMs from a human perspective.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: for the demographic group < young, woman, white >.",
      "page": 5
    },
    {
      "caption": "Figure 1: Synthetic facial images of a white-skin, young woman conveying the ‘Big Six’",
      "page": 6
    },
    {
      "caption": "Figure 2: presents the confusion matrices computed, comparing the model inferences and the",
      "page": 8
    },
    {
      "caption": "Figure 2: Confusion matrices obtained by analysing the facial images generated accord-",
      "page": 9
    },
    {
      "caption": "Figure 3: visually summarises the followed",
      "page": 12
    },
    {
      "caption": "Figure 3: Pipeline of the affective text style transfer process for generating the affective",
      "page": 13
    },
    {
      "caption": "Figure 4: UAR scores obtained with the RoBERTa, GPT-3.5, and GPT-4 models when",
      "page": 14
    },
    {
      "caption": "Figure 4: Notably, these results can be",
      "page": 14
    },
    {
      "caption": "Figure 5: depicts the confusion matrices obtained with the RoBERTa baseline model",
      "page": 14
    },
    {
      "caption": "Figure 5: Confusion matrices showing the performance (in %) of the fine-tuned",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective Computing Has Changed: The": "Foundation Model Disruption"
        },
        {
          "Affective Computing Has Changed: The": "Bj¨orn Schuller1,2,3,4,5*, Adria Mallol-Ragolta1,3,"
        },
        {
          "Affective Computing Has Changed: The": "Alejandro Pe˜na Almansa5,6,\nIosif Tsangko1,3,5,"
        },
        {
          "Affective Computing Has Changed: The": "Mostafa M. Amin1,5,7, Anastasia Semertzidou1,\nLukas Christ1,"
        },
        {
          "Affective Computing Has Changed: The": "Shahin Amiriparian1"
        },
        {
          "Affective Computing Has Changed: The": "1CHI – Chair of Health Informatics, MRI, Technical University of Munich, Germany."
        },
        {
          "Affective Computing Has Changed: The": "2MDSI – Munich Data Science Institute, Germany."
        },
        {
          "Affective Computing Has Changed: The": "3MCML – Munich Center for Machine Learning, Germany."
        },
        {
          "Affective Computing Has Changed: The": "4GLAM – Group on Language, Audio, & Music, Imperial College London, UK."
        },
        {
          "Affective Computing Has Changed: The": "5EIHW – Chair of Embedded Intelligence for Health Care & Wellbeing, University of"
        },
        {
          "Affective Computing Has Changed: The": "Augsburg, Germany."
        },
        {
          "Affective Computing Has Changed: The": "6School of Engineering, Universidad Autonoma de Madrid, Spain."
        },
        {
          "Affective Computing Has Changed: The": "7AI R&D Team, SyncPilot GmbH, Germany."
        },
        {
          "Affective Computing Has Changed: The": "*Corresponding author(s). E-mail(s): schuller@ieee.org;"
        },
        {
          "Affective Computing Has Changed: The": "Abstract"
        },
        {
          "Affective Computing Has Changed: The": "The dawn of Foundation Models has on the one hand revolutionised a wide range"
        },
        {
          "Affective Computing Has Changed: The": "of research problems, and, on the other hand, democratised the access and use"
        },
        {
          "Affective Computing Has Changed: The": "of AI-based tools by the general public. We even observe an incursion of\nthese"
        },
        {
          "Affective Computing Has Changed: The": "models into disciplines related to human psychology, such as the Affective Com-"
        },
        {
          "Affective Computing Has Changed: The": "puting domain, suggesting their affective, emerging capabilities. In this work, we"
        },
        {
          "Affective Computing Has Changed: The": "aim to raise awareness of the power of Foundation Models in the field of Affective"
        },
        {
          "Affective Computing Has Changed: The": "Computing by synthetically generating and analysing multimodal affective data,"
        },
        {
          "Affective Computing Has Changed: The": "focusing on vision,\nlinguistics, and speech (acoustics). We also discuss some fun-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 Introduction": "“The world of Affective Computing has changed. I see it in the vision modality. I read"
        },
        {
          "1 Introduction": "it\nin the linguistic modality.\nI hear it\nin the speech modality. Much that once was is"
        },
        {
          "1 Introduction": "outdated...” This quote, which might sound slightly familiar to the J. R. R. Tolkien’s"
        },
        {
          "1 Introduction": "readers, aims\nto literary exemplify how the disruption of Foundation Models\n(FM)"
        },
        {
          "1 Introduction": "might be impacting the Affective Computing research as we knew it. Before centring"
        },
        {
          "1 Introduction": "the discussion on this topic, we summarise where we come from."
        },
        {
          "1 Introduction": "The Affective Computing research can be broadly clustered into three main cate-"
        },
        {
          "1 Introduction": "gories: recognition of affect, generation of affective content, and response to affect [1]."
        },
        {
          "1 Introduction": "The development of systems with affective features opens a vast arsenal of use cases,"
        },
        {
          "1 Introduction": "ranging from digital psychology recognising depression [2]\nto security applications"
        },
        {
          "1 Introduction": "(e. g., stress prediction in driving [3]). However, Affective Computing applications are"
        },
        {
          "1 Introduction": "mostly centred in Human-Computer and Human-Robot Interaction (HCI/HRI) [4–6],"
        },
        {
          "1 Introduction": "where the ability to interpret\nthe human affect\nis not only desirable to improve the"
        },
        {
          "1 Introduction": "communication [7, 8], but a necessary capability to correctly understand the message."
        },
        {
          "1 Introduction": "Early work in the field of psychology indicated that human affect is communicated in"
        },
        {
          "1 Introduction": "a multimodal manner through physical channels such as facial expressions,\nlanguage,"
        },
        {
          "1 Introduction": "or voice [9]. Consequently,\nthe Affective Computing community has paid significant"
        },
        {
          "1 Introduction": "attention to visual\n[10, 11],\nlinguistic\n[12, 13], and speech (acoustic)\n[14, 15] data"
        },
        {
          "1 Introduction": "processing,\nincluding multimodal configurations [16, 17]."
        },
        {
          "1 Introduction": "Of the different dimensions of human affect, the emotional states have attracted"
        },
        {
          "1 Introduction": "major attention for\nits\nimpact\nin a wide\nrange of aspects of peoples’\nlives. Given"
        },
        {
          "1 Introduction": "such relevance, the Affective Computing community has been putting major efforts in"
        },
        {
          "1 Introduction": "the development of automated systems for the recognition and understanding of the"
        },
        {
          "1 Introduction": "human feelings. Early research in the field of emotion recognition relied on conventional"
        },
        {
          "1 Introduction": "Machine Learning (ML) pipelines, in which expert-crafted features were first extracted"
        },
        {
          "1 Introduction": "from the raw data such as pixels, words, or an audio signal, and then processed utilising"
        },
        {
          "1 Introduction": "traditional statistical methods; e. g., Support Vector Machines (SVM). The key to this"
        },
        {
          "1 Introduction": "conventional approach was\nto try to design a suitable set of\nfeatures\nthat captures"
        },
        {
          "1 Introduction": "emotional content;\ni. e., the hand-crafted features. In the visual domain, the emotional"
        },
        {
          "1 Introduction": "content was assumed to be mainly reflected in the facial expression information,\nfrom"
        },
        {
          "1 Introduction": "which the features were extracted; e. g., through Principal Component Analysis (PCA),"
        },
        {
          "1 Introduction": "or traditional Computer Vision (CV) techniques, such as Gabor wavelets or texture"
        },
        {
          "1 Introduction": "filters.\nIn addition, prior knowledge of the Action Units (AUs) defined in the Facial"
        },
        {
          "1 Introduction": "Action Coding System (FACS) [18] helped in the design of these features. In linguistics,"
        },
        {
          "1 Introduction": "feature extraction typically relied on n-grams or bag-of-words – a vector representation"
        },
        {
          "1 Introduction": "of\ntext by sparse vectors\nthat\nrepresent\nsome form of vocabulary and some form of"
        },
        {
          "1 Introduction": "frequency of occurrence of the words in the current text –, after carefully preprocessing"
        },
        {
          "1 Introduction": "the raw text (e. g., by stemming or stopping). In audio, several works noticed the rich"
        },
        {
          "1 Introduction": "emotional\ninformation reflected in the prosodic features\n[19], which in combination"
        },
        {
          "1 Introduction": "with spectral\nfeatures – e. g.,\nthe Mel Frequency Cepstral Coefficients\n(MFCC) –,"
        },
        {
          "1 Introduction": "formed the basis for Speech Emotion Recognition (SER)."
        },
        {
          "1 Introduction": "The success of Deep Learning (DL) in the early 2010’s entailed a disruption to the"
        },
        {
          "1 Introduction": "entire field of Artificial Intelligence. The development of affective systems embraced the"
        },
        {
          "1 Introduction": "new trend as well, which was mainly marked by the use of representational learning via"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "or Recurrent Neural Networks\n(RNN) proved to be extremely useful\nfor extracting"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "appropriate features when large amounts of data were available. Consequently, feature"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "engineering took a backseat when data-driven approaches unleashed their potential."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "This\ncould be\nseen as\nthe first disruption of\nthe field:\nthe\nlearning of\nrepresenta-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "tions.\nIn the visual domain, end-to-end CNN-based systems accepted raw images as"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "input, discarding the need for any feature engineering. In linguistics, complex prepro-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "cessing steps and statistical\nrepresentations\nlost\ntheir\nrelevance with the success of"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "word embedding models, which could not only be utilised as input for a DNN model,"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "but also contained semantic meaning in their own space,\nincluding affective informa-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "tion [20]. Moreover,\nsome speech systems began to process\nthe raw time signal\n[21]"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "or\nthe spectrogram representations with CNNs\nto take advantage of\ntheir enhanced"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "representational capabilities."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "Therefore, the efforts were ultimately shifted from experts crafting representations"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "to experts choosing model architectures to learn these representations. A second, albeit"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "less noted and exploited potential disruption came with the possibility of\n(neural)"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "architecture search by reinforcement\nlearning [22] or more efficient approaches\n[23]."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "This meant that in principle, once having affective (labelled) data, the representation"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "could be learnt to then analyse affective data as well as the best architecture to do so."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "The data-driven disruption meant an improvement as well\nfor\nsynthesis of emo-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "tional data. In the visual domain, the introduction of adversarial\nlearning [24] paved"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "the way for Generative Adversarial Network- (GAN) based generative models, in which"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "the emotional\nstate was also controlled by explicit\nindicators\n[25] or by style trans-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "fer [26]. Some works improved the semantic control over the output by identifying and"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "traversing emotional directions in the GAN latent space [27]. For text,\nfor instance,"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "RNNs\ncould similarly produce affective\nlanguage\n[28].\nIn speech (acoustics), where"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "traditional approaches mostly relied on Gaussian Mixture Models (GMM) or Hidden"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "Markov Models\n(HMM), new encoder-decoder architectures based on Long Short-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "Term Memory (LSTM) networks were devised,\nin which the emotional state could be"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "controlled by both explicit labels [29] or style transfer from a reference [30]."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "Yet, a critical\nissue in this era was the acquisition of such reliable, annotated data."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "The subjectivity of measuring inner emotion through self-assessment shifted the focus"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "to perceived emotion. However,\n‘measuring’ outer perceived emotion usually requires"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "several\nlabellers to reduce uncertainty, hence coming at high effort and cost. In addi-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "tion, the lack of spontaneous data sources – e. g., due to privacy restrictions – favoured"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "the use of acted/elicited, non-spontaneous data samples [31, 32]. Whilst non- or less-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "spontaneous data eased the problem of data availability,\nit came with the drawback"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "that\nthe analysis of\nreal-world emotion struggles with subtlety not met\nin training."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "Further, generated data samples may reflect unrealistic emotional responses if systems"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "are only trained on such data. The acquisition of data from Internet\nsources\n(e. g.,"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "social media, films) allowed the collection of\nlarge “in-the-wild” databases\n[33, 34],"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "whose annotations were obtained through semi-automatic methods, crowdsourcing, or"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "based on the criteria of experts in affect."
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "Nowadays, a third disruption is taking place in the Artificial Intelligence (AI) com-"
        },
        {
          "Deep Neural Networks (DNN). Models such as Convolutional Neural Networks (CNN)": "munity. Whilst\nthe first disruption established that\n(almost) no feature engineering"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "learn also the optimal architectures,\nin current developments even specialised anno-"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "tated affective data for\ntraining the models may no longer be needed, as affective"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "computing abilities start to emerge in (general\nlarge data) pre-trained models as we"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "will discuss in the following section. Nevertheless, curating high-quality sets of anno-"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "tated data to some extent\nremains crucial\nto assess\nthe performance of\nthe models."
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "New architectures,\nsuch as\nthe Diffuser\n[35] or\nthe Transformer\n[36],\ntogether with"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "self-supervised learning strategies [37] and inter-modality alignment\ntechniques\n[38],"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "have led to new FMs\n[39–42]. These models have demonstrated surprising capabilities"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "using prompt-based instructions,\nto the point\nthat\nthey can generate realistic data"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "samples or perform zero-shot classification. The extent of the affective capabilities of"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "these models, and the potential they open up, is still uncertain. Herein, we aim to shed"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "some light on this topic, and explore how the emergence of FMs and the subsequent"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "AI regulation are influencing the Affective Computing community."
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "2 Emergence in Foundation Models"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "One of\nthe main characteristics of\nthe Foundation Models\n(FMs)\nis\nthat\nthey are"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "trained on a broad range of data, so that the resulting models can be utilised in a wide"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "range of problems.\nIn addition,\nthey exploit\nlarge amounts of\nlearning parameters."
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "Given sufficient learning material, from a certain number of such parameters hence well"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "trained, knowledge ‘emerges’\nin the FMs, and they achieve competitive performances"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "in tasks they have not been specifically trained for. This can, however, be difficult to"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "predict [43]. In this paper, we aim to investigate the ‘emergent’ affective capabilities of"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "FMs. Focusing on the vision (cf. Section 2.1),\nlinguistics (cf. Section 2.2), and speech"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "(acoustics) (cf. Section 2.3) modalities, we assess the capabilities of current FMs to i)"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "generate synthetic affective samples,\nfrom which we infer the conveyed emotions with"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "pre-trained emotion recognition classifiers1, and ii) analyse well-established datasets"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "in the field in a zero-shot manner. To favour\nthe comparability among the different"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "modalities\ninvestigated, we\nfocus on the\n‘Big Six’ Ekman emotions\n[44]\n(i. e.,\nfear,"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "anger, happiness, sadness, disgust, and surprise),\nin addition to the neutral state."
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "2.1 The Vision Modality Has Changed"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "2.1.1 Generation"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "In the visual domain, data synthesis started to obtain pseudorealistic results in the last"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "decade thanks to the Generative Adversarial Network- (GAN) based models [25, 30]."
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "Nowadays, a boost\nin the quality of\nthe\nsynthesised images has been achieved via"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "text prompt inputs-based models, due to i) the CLIP model\n[38] and ii) the Diffuser"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "model\n[45]. The former was presented in conjunction with DALL-E [46], as a model to"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "predict how well a given caption describes an image;\ni. e., a text-to-image alignment."
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "1Note that in principle, this can lead to a ‘closed loop’, as we cannot be sure whether the data used in"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "the pre-trained emotion classifiers has not also been used in the training of\nthe Foundation Models, but"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "generally,\nit is unlikely, as high-quality affective data are rarely freely available on the Internet due to their"
        },
        {
          "was needed – just a powerful model and annotated data –, and the second allowed to": "privacy restrictions."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: summarises the resulting data in terms of the",
      "data": [
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "sian noise through a Markov Chain. During inference, the model\nis able to generate"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "new images from Gaussian noise, being more efficient than other generative architec-"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "tures. Models such as Stable Diffusion (SD) [35] or DALL-E 2 [47]\nintegrate the CLIP"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "and the Diffuser models to efficiently synthesise images with high semantic control, an"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "important feature to generate affective samples. Further, the decision to make gener-"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "ative models, such as SD, open-source has allowed the general public to discover the"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "potential of the technology, ultimately speeding up its advancements."
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "We have leveraged one of\nthe latest versions of SD 2 – i. e., Stable Diffusion XL"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "(SDXL) [42] – to synthetically generate a face emotion dataset3. This dataset is gener-"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "ated utilising prompts based on a fixed template with three sources of variation:\ni) the"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "emotion,\nii)\nthe style (photorealistic, cartoon-painting, anime, and 3D), and iii)\nthe"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "demographic group. The template, along with the values explored for each attribute,"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "are detailed in Table 1. Table 2 presents a summary of the gathered dataset. Although"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "our emotion model\nis based on the ‘Big Six’ Ekman emotions\n[44] plus\nthe neutral"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "state, we employed these basic emotions\ntogether with a higher\nintensity variation,"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "as defined in Plutchik’s model\n[48], to further emphasise the desired affective states."
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "Also, note that the generation process spans 18 different demographic groups; deter-"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "mined by age, biological\nsex, and skin tone. Visual examples\nfor each emotion and"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "style are provided in Figure 1 for the demographic group < young, woman, white >."
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "For\nthe case of\nthe photorealistic style, we played with the background to generate"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "some samples in realistic scenarios (e. g., outdoors, office, park). The generation pro-"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "cess involved two experts, the SDXL base model and the SDXL refiner4, with a total"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "of 40 steps (80 % in the base model, 20 % in the refiner) and a guidance value of 7.5."
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "Together with the desired prompt, we utilised a negative prompt\nto highlight what"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "we do not want\nto see in the output. Once the images were generated, we filtered"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "them according to four principles: the presence of disfigurations or artifacts, nudity,"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "the quality of the emotion generated, and the plausibility of the style. For the sake of"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "this experiment, only one annotator conducted this data curation process."
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "We now aim to automatically verify the affective quality of\nthe generated facial"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "images with Face Emotion Recognition (FER) models. We\nemploy\nthe manually"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "annotated subset of the AffectNet dataset [33] to develop these models. The images"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "belonging to this dataset are annotated in terms of eleven emotions. Nevertheless, we"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "select the images corresponding to the emotions fear, anger, happiness, sadness, dis-"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "gust, and surprise in addition to the neutral class. We process\nthe selected images"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "with OpenFace [49] to extract features related to a subset of the Action Units (AU)"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "defined in the Facial Action Coding System (FACS). Specifically, OpenFace extracts"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "35 features per facial\nimage,\nindicating the presence (0 or 1) and the intensity (in a"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "scale from 0 – not present – to 5 – present with maximum intensity) of a subset of the"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "AUs. We discard the images that OpenFace fails to process;\nfor instance, due to the"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "absence of a face in the image. Table 3 summarises the resulting data in terms of the"
        },
        {
          "The latter is a model that learns to reconstruct images by removing an added Gaus-": "number of\nimages per emotion in the training and the validation partitions."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Attributes defined in the input prompts to synthesise emotional facial images": "with Stable Diffusion XL [42]. The prompt template considers three different sources of"
        },
        {
          "Table 1: Attributes defined in the input prompts to synthesise emotional facial images": "variation: the emotion, the style, and the demographic group. The latter is determined"
        },
        {
          "Table 1: Attributes defined in the input prompts to synthesise emotional facial images": "by three different demographic attributes: age, biological sex, and skin tone. We have"
        },
        {
          "Table 1: Attributes defined in the input prompts to synthesise emotional facial images": "also utilised a negative prompt, which includes all"
        },
        {
          "Table 1: Attributes defined in the input prompts to synthesise emotional facial images": "the current synthesis."
        },
        {
          "Table 1: Attributes defined in the input prompts to synthesise emotional facial images": "Attribute\nValues"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "also utilised a negative prompt, which includes all\nthe styles that are not desired in"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "the current synthesis."
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Attribute\nValues"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Face image of a < age > < sex > with < skin > skin, with a < emotion > face,"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Prompt"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "in a < style > style, realistic eyes, white background, ultra quality,\nfrontal picture,"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "template"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "looking at camera"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Negative"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "disfigured, unrealistic eyes, blurry, b&w, < style >"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "prompt"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "neutral,\nfear and terror, anger and rage, happiness and joy,"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Emotion"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "sadness and grief, disgust and loathing, surprise and amazement"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Age\nyoung, middle-aged, old"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Sex\nman, woman"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Skin tone\nwhite, brown, black"
        },
        {
          "by three different demographic attributes: age, biological sex, and skin tone. We have": "Style\nphotorealistic, cartoon and painting, anime, 3D Pixar animation"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Summary of the face images generated with the Stable Diffusion XL",
      "data": [
        {
          "Summary\nof": "[42]. The prompts for the generation are detailed in Table 1.",
          "the\nface\nimages": "",
          "generated with": "",
          "the": ""
        },
        {
          "Summary\nof": "Emotion",
          "the\nface\nimages": "Photorealistic",
          "generated with": "Cartoon",
          "the": "Anime"
        },
        {
          "Summary\nof": "Neutral",
          "the\nface\nimages": "233",
          "generated with": "132",
          "the": "131"
        },
        {
          "Summary\nof": "Fear",
          "the\nface\nimages": "185",
          "generated with": "55",
          "the": "94"
        },
        {
          "Summary\nof": "Anger",
          "the\nface\nimages": "183",
          "generated with": "165",
          "the": "119"
        },
        {
          "Summary\nof": "Happiness",
          "the\nface\nimages": "223",
          "generated with": "202",
          "the": "118"
        },
        {
          "Summary\nof": "Sadness",
          "the\nface\nimages": "179",
          "generated with": "156",
          "the": "137"
        },
        {
          "Summary\nof": "Disgust",
          "the\nface\nimages": "173",
          "generated with": "90",
          "the": "56"
        },
        {
          "Summary\nof": "Surprise",
          "the\nface\nimages": "184",
          "generated with": "147",
          "the": "120"
        },
        {
          "Summary\nof": "Σ",
          "the\nface\nimages": "1 360",
          "generated with": "947",
          "the": "775"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Summary of the face images generated with the Stable Diffusion XL",
      "data": [
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Emotions"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Neutral"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Fear"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Anger"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Happiness"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Sadness"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Disgust"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Surprise"
        },
        {
          "Table 3: Summary of the face images selected from AffectNet [33]": "Σ"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: Accuracy (ACC) and Unweighted Average Recall (UAR) scores obtained",
      "data": [
        {
          "Table 4: Performance summary of the trained Support Vector Classifier-based models": "for Face Emotion Recognition on the validation partition of the considered subset of"
        },
        {
          "Table 4: Performance summary of the trained Support Vector Classifier-based models": "AffectNet. We also include the performance of a state-of-the-art Vision Transformer"
        },
        {
          "Table 4: Performance summary of the trained Support Vector Classifier-based models": "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the"
        },
        {
          "Table 4: Performance summary of the trained Support Vector Classifier-based models": "metric to assess the model performances."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: Accuracy (ACC) and Unweighted Average Recall (UAR) scores obtained",
      "data": [
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": "metric to assess the model performances."
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": "SVC"
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        },
        {
          "for Facial Expression Recognition (ViT – FER). We select the accuracy (ACC) as the": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: ), probably due",
      "data": [
        {
          "Neutral": "Fear",
          "93.1 —\n0.9\n3.9\n2.1\n—": "77.3 3.8\n3.8\n3.2 10.8 —",
          "—": "2.1",
          "81.1 —\n— 10.6 8.3\n—\n—": "50.9 29.1 3.6\n1.8 10.9 —\n3.6",
          "66.4 0.8\n6.9\n3.1 22.9 —": "31.9 37.2 11.7 1.1 16.0 —",
          "86.0 0.7\n0.7 12.6 —": "16.9 52.2 4.4\n9.6\n—",
          "—\n—": "— 16.9"
        },
        {
          "Neutral": "Anger",
          "93.1 —\n0.9\n3.9\n2.1\n—": "67.8 2.2 18.6 0.5 10.9 —",
          "—": "0.8",
          "81.1 —\n— 10.6 8.3\n—\n—": "45.5 — 26.7 1.8 26.1 —\n—",
          "66.4 0.8\n6.9\n3.1 22.9 —": "18.5 — 48.7 2.5 29.4 —",
          "86.0 0.7\n0.7 12.6 —": "16.0 9.2 65.5 6.7\n—",
          "—\n—": "—\n2.5"
        },
        {
          "Neutral": "Happiness",
          "93.1 —\n0.9\n3.9\n2.1\n—": "16.1 —\n— 83.9 —\n—",
          "—": "—",
          "81.1 —\n— 10.6 8.3\n—\n—": "1.5\n—\n— 98.0 0.5\n—\n—",
          "66.4 0.8\n6.9\n3.1 22.9 —": "8.5\n0.8\n— 83.1 7.6\n—",
          "86.0 0.7\n0.7 12.6 —": "4.2\n0.7\n0.7 94.4 —",
          "—\n—": "—\n—"
        },
        {
          "Neutral": "Sadness",
          "93.1 —\n0.9\n3.9\n2.1\n—": "84.4 —\n3.9\n0.6 11.2 —",
          "—": "—",
          "81.1 —\n— 10.6 8.3\n—\n—": "79.5 0.6\n1.3\n0.6 17.9 —\n—",
          "66.4 0.8\n6.9\n3.1 22.9 —": "59.9 1.5\n5.8\n0.7 32.1 —",
          "86.0 0.7\n0.7 12.6 —": "63.6 19.2 5.1\n—\n9.1",
          "—\n—": "—\n3.0"
        },
        {
          "Neutral": "Disgust",
          "93.1 —\n0.9\n3.9\n2.1\n—": "75.1 — 12.7 1.2 11.0 —",
          "—": "—",
          "81.1 —\n— 10.6 8.3\n—\n—": "47.8 5.6 12.2 10.0 24.4 —\n—",
          "66.4 0.8\n6.9\n3.1 22.9 —": "39.3 8.9 21.4 8.9 21.4 —",
          "86.0 0.7\n0.7 12.6 —": "15.0 30.0 35.0 5.0\n5.0",
          "—\n—": "— 10.0"
        },
        {
          "Neutral": "Surprise",
          "93.1 —\n0.9\n3.9\n2.1\n—": "Neutral\nFear\nAnger\nSadness\nDisgust\n66.3 1.1\n5.4 19.6 1.6\n—",
          "—": "Surprise\n5.8",
          "81.1 —\n— 10.6 8.3\n—\n—": "Neutral\nFear\nAnger\nSadness\nDisgust\nSurprise\n25.9 6.1\n2.7 39.5 10.2 — 15.6",
          "66.4 0.8\n6.9\n3.1 22.9 —": "Neutral\nFear\nAnger\nSadness\nDisgust\n19.2 14.2 10.8 30.0 20.0 —",
          "86.0 0.7\n0.7 12.6 —": "Neutral\nFear\nAnger\nSadness\n13.7 10.1 — 36.7 —",
          "—\n—": "Disgust\nSurprise\n— 39.6"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: ), probably due",
      "data": [
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "Fig. 2: Confusion matrices obtained by analysing the facial\nimages generated accord-"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "ing to the four different styles with the ViT – FER pre-trained model."
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "Average Recall (UAR)6. In contrast, the best results are obtained with the 3D style."
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "Interestingly,\nin all the 4 styles both the neutral and the happiness emotions consis-"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "tently obtain the best results. These classes are traditionally over-represented in face"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "datasets (e. g., see the training set distribution of AffectNet in Table 3), probably due"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "to the nature of the data sources [52]. Consequently,\nit is expected for the generative"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "model to be biased towards those emotions, which would also explain the difficulty to"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "generate samples for classes like disgust or fear, which obtain the worst results."
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "2.1.2 Analysis"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "In order to evaluate the affective analytical capabilities of FMs in the image domain,"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "we\nexplore\ntheir performance\nin a zero-shot\nemotion recognition task under differ-"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "ent configurations. Our experiments are conducted on the considered validation set"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "of AffectNet [33] (cf. Section 2.1.1), as it is balanced,\nin-the-wild, and manually anno-"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "tated. We compare three different approaches relying on model-prompting. In the first"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "two, we start by extracting AU-based features with OpenFace [49], and we provide"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "these features\nin a textual\nformat as\ninput\nto a FM. Recalling from Section 2.1.1,"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "OpenFace predicts both the presence and the intensity of a subset of AUs; hence, two"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "approaches can be derived from its predictions. On the other hand, the third approach"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "consists\nin directly feeding the\nimages within the prompt of a FM. Note\nthat\nthe"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "first two approaches can be addressed with a Language Foundation Model (LFM), for"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "which we select the LLaMA2 7B model. We utilise a Multimodal Foundation Model"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "(MFM) for the third scenario; specifically, the LLaVA1.5 7B model 7 [53, 54]. This is an"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "open-source MFM with visual capabilities trained by fine-tuning Vicuna8, an already"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "fine-tuned version of LLaMA, with GPT-4 generated data. The selected models allow"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "them having the same number of parameters."
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "In Table 7, we present\nthe results of\nthe aforementioned scenarios. The prompts"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "utilised for each scenario are detailed in Table 6. We also include in Table 7 the results"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "of the ViT – FER model on the considered validation set of AffectNet for comparability"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "6UAR is the sum of recall per class divided by the number of classes – this reflects imbalances and is a"
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "standard measure in the field."
        },
        {
          "(a) Photorealistic\n(b) Cartoon-painting\n(c) Anime\n(d) 3D": "7https://huggingface.co/liuhaotian/llava-v1.5-7b"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 6: Prompts employed to perform zero-shot emotion recognition with Founda-",
      "data": [
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "information are injected in LLaMA2 [55], while the prompt including the raw image"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "is injected to a LLaVA1.5 model\n[54]."
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "Approach\nPrompt template"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "<s>[INST] <<SYS>>"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "You are a highly skilled Affective Computing system with an expertise in"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "accurately predicting emotion classes from Action Units. I will provide you a list"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "of Action Units present in a face. Your task is to answer the most likely emotion"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "class, without any further explanation. Please, provide only one of the following"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "AU presence"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "classes as answer: Neutral, Fear, Anger, Happiness, Sadness, Disgust, Surprise."
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "The question is, which is the most likely emotion if the following Action Units"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "are present<< /SYS>>"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "“{AU}”.[/INST]"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "###Response:"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "<s>[INST] <<SYS>>"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "You are a highly skilled Affective Computing system with an expertise in"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "accurately predicting emotion classes from Action Units. I will provide you a"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "JSON object with the intensities of the Action Units present in a face. Your"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "task is to answer the most likely emotion class, without any further explanation."
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "AU intensity\nPlease, provide only one of the following classes as answer: Neutral, Fear,"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "Anger, Happiness, Sadness, Disgust, Surprise."
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "The question is, which is the most likely emotion if the following Action Units"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "are present<< /SYS>>"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "“{AU}”.[/INST]"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "###Response:"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "<image>"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "USER: You are provided with a face image of a person. Classify the most likely"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "Image\nemotional state depicted into one of the classes between brackets [Neutral,"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "Fear, Anger, Happiness, Sadness, Disgust, Surprise]"
        },
        {
          "tion Models\nin different\nscenarios. The prompts\nincluding (i. e., first\ntwo rows) AU": "ASSISTANT:"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-",
      "data": [
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "dation Models on the validation set of AffecNet"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "performance obtained with the ViT – FER model"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "comparison purposes."
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "Model"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "LLaMA2 7B"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "LLaMA2 7B"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "LLaMA2 7B"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "LLaMA2 7B"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "LLaVA1.5 7B"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "LLaVA1.5 7B"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "ViT – FER"
        },
        {
          "Table 7: Accuracy scores obtained with the LLaMA2 [55] and LLaVA1.5 [54] Foun-": "Chance level"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 8: Affective style transfer example towards the emotion ‘surprise’ with",
      "data": [
        {
          "Table 9: Statistics of the considered sub-": "set of the GoEmotions dataset."
        },
        {
          "Table 9: Statistics of the considered sub-": "Emotion"
        },
        {
          "Table 9: Statistics of the considered sub-": "Neutral"
        },
        {
          "Table 9: Statistics of the considered sub-": "Fear"
        },
        {
          "Table 9: Statistics of the considered sub-": "Anger"
        },
        {
          "Table 9: Statistics of the considered sub-": "Happiness"
        },
        {
          "Table 9: Statistics of the considered sub-": "Sadness"
        },
        {
          "Table 9: Statistics of the considered sub-": "Disgust"
        },
        {
          "Table 9: Statistics of the considered sub-": "Surprise"
        },
        {
          "Table 9: Statistics of the considered sub-": "Σ"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 8: Affective style transfer example towards the emotion ‘surprise’ with",
      "data": [
        {
          "Disgust\n498\n61\n76": "Surprise\n3 553\n435\n449"
        },
        {
          "Disgust\n498\n61\n76": "Σ\n36 308\n4 548\n4 590"
        },
        {
          "Disgust\n498\n61\n76": "settings chosen to enhance variability and creativity. The generation parameters used"
        },
        {
          "Disgust\n498\n61\n76": "a temperature of 0.9, top-p of 0.6, and a repetition penalty of 1.2. An example of the"
        },
        {
          "Disgust\n498\n61\n76": "generated sentences is shown in Table 8, and Figure 3 visually summarises the followed"
        },
        {
          "Disgust\n498\n61\n76": "pipeline. We observe that the models tend to exaggerate the injected emotion, adopt-"
        },
        {
          "Disgust\n498\n61\n76": "ing an over-dramatic style that results in more formal, yet affectively adapted phrases."
        },
        {
          "Disgust\n498\n61\n76": "Despite these exaggerated and dramatic adaptations,\nthe primary objective was\nto"
        },
        {
          "Disgust\n498\n61\n76": "investigate how these models perceive and express emotions under basic setup con-"
        },
        {
          "Disgust\n498\n61\n76": "ditions. Our findings highlight the models’ propensity to amplify emotional content,"
        },
        {
          "Disgust\n498\n61\n76": "which was an anticipated aspect of this exploratory study."
        },
        {
          "Disgust\n498\n61\n76": "In order\nto investigate the quality of\nthe generated sentences by the LLMs, we"
        },
        {
          "Disgust\n498\n61\n76": "implement\ntwo baseline models\ntrained on the GoEmotions dataset\n[67],\na well-"
        },
        {
          "Disgust\n498\n61\n76": "renowned corpus in the field and commonly utilised for benchmarking purposes due"
        },
        {
          "Disgust\n498\n61\n76": "to its comprehensive labelling and categorisation of\nthe emotions. The GoEmotions"
        },
        {
          "Disgust\n498\n61\n76": "dataset consists of English Reddit comments annotated according to 27 distinct emo-"
        },
        {
          "Disgust\n498\n61\n76": "tions, plus\nthe neutral\nstate, by 3\nor\n5\nlabellers\neach. Due\nto\nthe nature\nof\nthe"
        },
        {
          "Disgust\n498\n61\n76": "annotations in the GoEmotions dataset, we begin by tailoring the data to meet\nthe"
        },
        {
          "Disgust\n498\n61\n76": "specific requirements of our experiments. First, we select instances from the dataset"
        },
        {
          "Disgust\n498\n61\n76": "annotated with a single emotion,\nin order to tackle the task as a single-label classifi-"
        },
        {
          "Disgust\n498\n61\n76": "cation problem,\ninstead of a multi-label classification one. As previously, we adopt the"
        },
        {
          "Disgust\n498\n61\n76": "‘Big Six’ Ekman emotions [44],\nin addition to a seventh neutral state. This restructur-"
        },
        {
          "Disgust\n498\n61\n76": "ing of the GoEmotions taxonomy to the Ekman taxonomy is achieved by aggregating"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 9: summarises the statistics of the considered subset of the",
      "data": [
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "sentences with ‘surprise’ as the prompted emotion. After that, we classify the synthe-"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "sised sentences using RoBERTa, GPT-3.5, and GPT-4."
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "the original\nlabels\ninto the targeted, broader categories\n[67]. For example, emotions"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "like annoyance and irritation, originally distinct, were grouped under\n‘anger’\nto fit"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "the Ekman model. Table 9 summarises the statistics of the considered subset of the"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "GoEmotions dataset."
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "For the two baselines, we employ two different architectures: a Bi-directional Long"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "Short-Term Memory (BiLSTM) network and a fine-tuned version of\nthe RoBERTa-"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "base model\n[68]. Both models are trained on the selected subset of the GoEmotions"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "dataset. The BiLSTM consists of two bidirectional LSTM layers with 128 units each."
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "It\nis\ntrained with a learning rate of 5 × 10−3 and a batch size of 96 for 40 epochs,"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "while RoBERTa-base was fine-tuned at a conservative learning rate of 5 × 10−5 and"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "a smaller batch size of 12. The models are trained on the training partition of\nthe"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "dataset, and the weights yielding the highest validation Unweighted Average Recall"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "(UAR) are selected for each model. The test scores for both baseline models (BiLSTM"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "and RoBERTa) on the test partition of the GoEmotions dataset are shown in Table 10."
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "The results are consistent with [67], but with some differences, given that we model the"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "problem as a single-label classification, instead of the original multi-label classification"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "task. Given the\nsuperior performance of RoBERTa, we utilise\nit\nfor analysing the"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "synthetically generated emotional sentences."
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "To evaluate the performance of the various LLMs on the emotion injection task,"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "we\ntest\nthe generated sentences with the RoBERTa baseline model,\nin addition to"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "GPT-4 as an approximation for human evaluation, and its weaker variant GPT-3.5."
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "GPT-4 has shown superior performance in many affective computing problems [62],"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "often better than fine-tuned, specialised models, especially with problems related to"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "sentiment or\nemotions. Table 11 demonstrates\nthe prompt\ntemplates used for\nthe"
        },
        {
          "Fig. 3: Pipeline of the affective text style transfer process for generating the affective": "GPT models,\nfollowing a similar pattern like [62, 69]. The versions of GPT variants"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 10: Performance scores of the implemented models when inferring the emo-",
      "data": [
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "Anger",
          "14.29": "Surprise"
        },
        {
          "Chance Level": "0.91",
          "14.29": ""
        },
        {
          "Chance Level": "0.87",
          "14.29": "0.78"
        },
        {
          "Chance Level": "",
          "14.29": "0.65"
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": "0.75\n0.79"
        },
        {
          "Chance Level": "0.88",
          "14.29": ""
        },
        {
          "Chance Level": "1.00",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "Happiness",
          "14.29": "Disgust"
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": "RoBERTa"
        },
        {
          "Chance Level": "",
          "14.29": ""
        },
        {
          "Chance Level": "",
          "14.29": "GPT-3.5"
        },
        {
          "Chance Level": "",
          "14.29": "GPT-4"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 10: Performance scores of the implemented models when inferring the emo-",
      "data": [
        {
          "0.84\n0.88": "0.89\nRoBERTa\nRoBERTa\nRoBERTa"
        },
        {
          "0.84\n0.88": "0.98"
        },
        {
          "0.84\n0.88": "GPT-3.5\nGPT-3.5\nGPT-3.5"
        },
        {
          "0.84\n0.88": "Sadness\nSadness\nSadness\nGPT-4\nGPT-4\nGPT-4"
        },
        {
          "0.84\n0.88": "(a) LLaMA2-based synthesis.\n(b) Mistral-based synthesis.\n(c) Mixtral-based synthesis."
        },
        {
          "0.84\n0.88": "Fig. 4: UAR scores obtained with the RoBERTa, GPT-3.5, and GPT-4 models when"
        },
        {
          "0.84\n0.88": "recognising the emotions conveyed by the synthetic sentences generated by LLaMA2"
        },
        {
          "0.84\n0.88": "(left), Mistral (centre), and Mixtral (right)."
        },
        {
          "0.84\n0.88": "used are ‘gpt-3.5-turbo-0125’11 for GPT-3.5 and ‘gpt-4-turbo-2024-04-09’12 for GPT-"
        },
        {
          "0.84\n0.88": "4. The results of this evaluation are depicted in Figure 4. Notably, these results can be"
        },
        {
          "0.84\n0.88": "considered to reflect a better agreement between models than with the ground truth"
        },
        {
          "0.84\n0.88": "labels, which are not human-annotated. However, the results of GPT-4 should be the"
        },
        {
          "0.84\n0.88": "closest to the human evaluations [62, 70]."
        },
        {
          "0.84\n0.88": "GPT-4 as the most superior model achieves very high UAR scores on all six emo-"
        },
        {
          "0.84\n0.88": "tions.\nIts\ninferior model GPT-3.5 achieves\nslightly worse results\nin most cases, but"
        },
        {
          "0.84\n0.88": "it experiences a performance drop in the recognition of surprise. On the other hand,"
        },
        {
          "0.84\n0.88": "RoBERTa has a different behaviour\nin comparison.\nIt\nis generally much worse than"
        },
        {
          "0.84\n0.88": "GPT-4 and GPT-3.5 in most of\nthe cases, but\nit obtains a higher score than GPT-"
        },
        {
          "0.84\n0.88": "3.5 for the surprise emotion with the LLaMA2- and the Mixtral-generated sentences."
        },
        {
          "0.84\n0.88": "Additionally, RoBERTa is showing very low performance for disgust, which seems to"
        },
        {
          "0.84\n0.88": "be a weakness of the model, consistent with the results on the GoEmotions dataset [67]."
        },
        {
          "0.84\n0.88": "Figure 5 depicts the confusion matrices obtained with the RoBERTa baseline model"
        },
        {
          "0.84\n0.88": "on the LLaMA2-,\nthe Mistral-, and the Mixtral-generated sets. We also include its"
        },
        {
          "0.84\n0.88": "performance on the test set of GoEmotions as a reference. A common issue with the"
        },
        {
          "0.84\n0.88": "RoBERTa model\nis the confusion among anger and disgust. Analysing the confusion"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 11: ). Prompt engineering",
      "data": [
        {
          "Neutral": "Fear",
          "71.5 0.5\n9.0": "5.2 74.0 6.5",
          "7.5": "1.3",
          "1.7": "9.1",
          "0.6\n9.2": "2.6\n1.3"
        },
        {
          "Neutral": "Anger",
          "71.5 0.5\n9.0": "31.2 0.8 54.4 4.6",
          "7.5": "",
          "1.7": "2.3",
          "0.6\n9.2": "2.1\n4.6"
        },
        {
          "Neutral": "Happiness",
          "71.5 0.5\n9.0": "17.4 0.4",
          "7.5": "3.1 76.9 0.8",
          "1.7": "",
          "0.6\n9.2": "—\n1.6"
        },
        {
          "Neutral": "Sadness",
          "71.5 0.5\n9.0": "",
          "7.5": "17.4 2.7 13.5 3.9 56.8 1.9",
          "1.7": "",
          "0.6\n9.2": "3.9"
        },
        {
          "Neutral": "Disgust",
          "71.5 0.5\n9.0": "21.1 1.3 27.6 2.6",
          "7.5": "",
          "1.7": "",
          "0.6\n9.2": "1.3 44.7 1.3"
        },
        {
          "Neutral": "Surprise",
          "71.5 0.5\n9.0": "Neutral\nFear\nAnger\n21.7 0.9\n5.8",
          "7.5": "7.6",
          "1.7": "Sadness\n1.6",
          "0.6\n9.2": "Disgust\nSurprise\n1.1 61.4"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 11: ). Prompt engineering",
      "data": [
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "Fig.\n5: Confusion matrices\nshowing\nthe\nperformance\n(in %)\nof\nthe\nfine-tuned"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "RoBERTa baseline on the synthesised benchmarks, generated by LLaMA2, Mistral,"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "and Mixtral, respectively,\nin addition to the GoEmotions test benchmark."
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "matrices, we also observe an interesting effect: most of the model’s mispredictions are"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "assigned to the neutral class."
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "2.2.2 Analysis"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "In this section, we analyse the zero-shot sentiment analysis capabilities of the follow-"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "ing LLMs: Mistral, Mixtral, and two versions of LLaMA2 (7 billion and 13 billion"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "parameters). We assess their zero-shot capabilities on the test partition of the GoE-"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "motions dataset. We design a prompt that requires the selected LLMs to predict the"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "corresponding emotion,\nincluding the neutral state (cf. Table 11). Prompt engineering"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "is crucial\nfor\ninfluencing LLMs, as\nit enhances nuanced responses and ensures more"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "accurate behaviour [71]. To minimise randomness and increase confidence in the pre-"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "dictions, we reduce the temperature setting to 0.1. This lower temperature sharpens"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "the probability distribution, ensuring that the predicted classes reflect those that the"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "LLMs are predicting with the highest probabilities\n[69]. However,\nthe LLM outputs"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "sometimes include irrelevant or multiple emotions. To address this without intrusively"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "altering the model’s outputs, we select the first listed emotion as the most reliable pre-"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "diction. This approach aligns with the operational principle of decoder-based models,"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "where the first valid emotion is mathematically the one with the highest confidence"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "score, thus considered the valid class prediction."
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "Table 12 summarises the comparative performance of the tested LLMs. We include"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "the performance of the RoBERTa baseline model trained on the GoEmotions dataset"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "(cf. Section 2.2.1) for benchmarking purposes. The first observation from the obtained"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "results is that the UAR scores obtained by all the investigated LLMs surpass the chance"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "level (14.3 %), underscoring the emergent affective capabilities of the LLMs tested in"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "a zero-shot manner. Nevertheless, none of the LLMs outperforms the RoBERTa base-"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "line model, fine-tuned on the GoEmotions dataset, which confirms the advantage of"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "model-specific tuning. Nevertheless,\nit is worth highlighting that the difference in the"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "UAR scores obtained by the best-performing GPT-3.5 and GPT-4 models in compar-"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "ison to the RoBERTa baseline model\nis around 15 %. This is an interesting result, as"
        },
        {
          "(a) LLaMA2\n(b) Mistral\n(c) Mixtral\n(d) GoEmotions": "these models have not been trained on the GoEmotions dataset, but still obtained a"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 11: Prompts to use LLMs for zero-shot emotion recognition, following a",
      "data": [
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "recognising the corresponding emotion on the sentences belonging to the test partition"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "of the GoEmotions dataset."
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": ""
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "Model"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": ""
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "LLaMA2-7B"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "LLaMA2-13B"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "Mistral"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "Mixtral"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "GPT-3.5"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "GPT-4"
        },
        {
          "Table 12: Performance scores of the different LLMs tested on a zero-shot fashion for": "RoBERTa"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "which opens up the possibility to equip the model with affective speech synthesis capa-"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "bilities by mere finetuning. Another recent generative audio FM, PromptTTS2 [74],"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "synthesises speech based on text prompts that include descriptions of the voice to be"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "generated. The controllable attributes of the synthesised speech must be defined during"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "the training time. The authors of [74] do not investigate emotion as one such attribute,"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "though their proposed framework would permit\nthis. Models\nsuch as UniAudio and"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "PromptTTS2 can thus be categorised as generative audio FMs that could be adapted"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "to emotional speech synthesis with moderate effort. To the best of our knowledge, no"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "evidence for affective speech synthesis as an emergent capability of such models has"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "been published so far. However, the demos for GPT-4o13 claim affective speech syn-"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "thesis capabilities. At the moment, though, neither a technical report on GPT-4o, nor"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "a systematic evaluation of affective speech produced by GPT-4o is available. Consid-"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "ering the development towards releasing large pretrained models in the NLP and CV"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "areas, we assume that\nin the near\nfuture powerful speech synthesis models will also"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "be made publicly available and, hence,\ninvestigated more thoroughly, allowing us to"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "carry out according experiments to the above for vision and linguistics. In addition, we"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "expect a continuing trend toward truly multimodal FMs that may not just take in but"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "also produce natural speech with controllable prosodic properties. Several multimodal"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "models that also produce audio output data have been proposed,\nfor an overview see"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "[75, 76]. To the best of our knowledge, none of\nthem exhibit emotional\nspeech syn-"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "thesis capabilities. We expect that emergent affective speech synthesis will eventually"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "be achieved via a multimodal approach. As shown in the previous sections,\nlarge pre-"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "trained vision and language models already encode affective information. Multimodal"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "approaches leveraging such pretrained models in combination with multimodal affect-"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "related data may learn to associate affective speech characteristics with corresponding"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "affective states as expressed in the visual and the textual data."
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "2.3.2 Analysis"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "A system capable of analysing arbitrary affective properties of\nspeech data without"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "any tuning must ingest both audio and text inputs. Several FM approaches fulfilling"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "this\nrequirement have been proposed. However,\nthe vast majority of\nthem are not"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "pretrained on speech data at all."
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "Examples\ninclude AnyMAL [77], X-InstructBLIP [78], and ModaVerse\n[79].\nIn"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "only a few models,\nspeech is part of\nthe pretraining data. QWEN-Audio’s\ntraining"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "data comprises several labelled speech datasets, including emotionality already. Hence,"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "QWEN-Audio [80]\nin the proposed form is not a candidate for exploring ‘emergent’"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "affective recognition capabilities. X-LLM [81] processes video, text, and audio inputs"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "and is explicitly designed to process speech. The authors, however, do not report any"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "experiments related to predicting affect in speech. As of now, the pretrained X-LLM"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "model\nis not publicly available, hence, unfortunately again, not allowing us to carry"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "out experiments analogous to the vision and the linguistic ones."
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "Similar to the affective speech synthesis problem, near-future multimodal FMs can"
        },
        {
          "their pretrained model serves as a basis for adaptation to different downstream tasks": "be expected to be capable of analysing affective speech in a zero-shot manner, even if"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "such model."
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "2.4 The Evaluation Is Changing"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "One of\nthe reasons\nto understand the impressive performance of currently available"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "Foundation Models (FM) is that they use massive amounts of data from “the Internet”"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "for training. Nevertheless, the indiscriminate use of data poses the following challenge"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "to the scientific community: can we guarantee that the data we feed to these models for"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "testing – or even for evaluation – has not been used for training? In case of a negative"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "answer, how fair and representative of the model capabilities can standard evaluation"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "metrics be? Although we do not have a concrete answer yet, we hope these challenges"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "engage\nthe\nresearch community into looking for methods and metrics\nthat allow a"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "proper scientific evaluation of these emerging FMs in the field of Affective Computing."
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "As is, the current state of such models in Affective Computing may resemble a shell"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "game: many different tools and approaches are shuffled and mixed until some partially"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "less, partially more convincing performances are obtained. Especially because it is the"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "popular\n‘Big Six’ Ekman emotions we considered herein, chances are high that\nthe"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "models only reverberate with what\nthey have already seen. Testing on more subtle"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "models such as the dimensional approach or less considered affective states is therefore"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "urgently needed."
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "3 Concerns and Regulations Have Changed"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "In April 2021,\nthe European Commission presented the AI Act:\nthe first-ever\nlegal"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "framework worldwide to regulate the use of AI-based technologies\nin the European"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "territory. The Act was endorsed by all Member States in February 2024 after being"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "approved by the EU Council. The regulation follows a risk-based approach, so instead"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "of regulating specific systems and applications,\nit defines measures and requirements"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "based on a classification system that encapsulates varying degrees of risks posed by"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "the AI systems. The proposed classification system spans four different levels of risk:"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "unacceptable, high,\nlimited, and minimal. According to the final version of the Act14,"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "systems that fall\ninto the unacceptable risk category will be prohibited."
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "This regulation is of special\ninterest for the Affective Computing community, since"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "it\nsingles out affective\nsystems\nin several ways. The\nregulation defines an emotion"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "recognition system in its Article 3 (34) as an “AI system for the purpose of identifying"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "or inferring emotions or intentions of natural persons on the basis of their biometric"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "data”. Thus, any emotion recognition system using speech or\nfacial data, as well as"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "other physical\nsignals\nsuch as\nthe electroencephalogram,\nfalls under\nthis definition."
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "Article 5 enumerates different practices and systems considered as prohibited,\ninclud-"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "ing the placing on the market or the use of AI systems to infer emotions of a natural"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "person in the workplace or in education institutions. Whilst this point only prohibits"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "emotion recognition systems in two specific contexts (workplace and education), the"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "list of high-risk systems presented in Annex III\nincludes “AI systems intended to be"
        },
        {
          "not explicitly pretrained in this regard. As of now, however, we are not aware of any": "used for emotion recognition” in the category of biometric systems. Article 6 provides"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "high risk if they do not pose a significant risk of harm to the health, safety, or funda-"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "mental rights of natural persons”. However, this exception shall not apply for systems"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "performing profiling as defined in Article 4 of the General Data Protection Regulation"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "(GDPR). Considering the definition of profiling,\nit\nis unclear whether any emotion"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "recognition system, even in non-harmful applications such as entertainment, could be"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "considered of limited risk, except for those based on non-identifiable data. In any case,"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "Article 52 imposes that any system including an emotion recognition component must"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "notify the users of the operation of such system, notwithstanding its risk."
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "In this context, the regulation imposes several obligations that high-risk systems"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "shall\ncomply with, and for which the provider\n(i. e., a natural person/agency that"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "develops and places on the market\nthe AI\nsystem)\nis\nresponsible. These obligations"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "are detailed across Chapters 2-5 of Title III – more than 40 articles – and include"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "conducting post-market surveillance of risk, informing the competent authorities about"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "the product, or providing technical documentation of both the system and the data"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "used in development (i. e., data governance), among others. Note that the definition"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "of provider\nis quite vague from a research perspective, so it\nis unclear how this will"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "affect research on Affective Computing. Article 2 (5a) specifies that the regulation does"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "not apply to models developed for the sole purpose of scientific research, but by the"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "time the model\nis made available, researchers may face some of the previously cited"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "obligations depending on how it is used, after which they may become providers. By"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "considering cases in which the system is made available free of charge, the regulation"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "seems to cover open-source systems and situations in which the source code is made"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "publicly available. However,\nthis does not\nseem to be the case when two academic"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "institutions share code in a confidential manner for academic purposes."
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "Attending now to Foundation Models\n(FM), which are\nreferred in the\ntext as"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "“General-purpose AI models”, Article 52 requires any AI-synthesised content\nto be"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "marked as such, which clearly covers generation of emotional samples with models such"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "as LLaMA, or SD, among others. Besides, Article 52 includes several subparagraphs"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "with the obligations\nthat providers shall meet when deploying this kind of\nsystems,"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "such as providing documentation on how it was trained and validated, and the content"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "used for that purpose. In addition, an FM may be considered to pose systemic risk if"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "it has high computational capabilities, or if\nit is marked as such by the Commission."
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "In this case, providers shall comply with more obligations,\nincluding testing and mit-"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "igating foreseeable risks, data governance measures, maintaining appropriate levels of"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "performance and interpretability. In this context, there are some concerns that current"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "FMs do not comply with all the measures required [82], hence accepting to slow down"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "innovation and AI development (particularly) in Europe, to assure highest safety and"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "ethical standards."
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "Beyond,\nemotion recognition based on physiological data has been less\ninvesti-"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "gated than other modalities,\nsuch as\nfacial\nexpressions\nor\nspeech. Some\ntypes\nof"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "physiological signals include electroencephalogram (EEG), electrocardiogram (ECG),"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "electromyogram (EMG),\nelectrodermal\nactivity\n(EDA)\nor\ngalvanic\nskin\nresponse"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "(GSR), respiration rate (RSP), and pulse rate. Emotions are complex and sometimes"
        },
        {
          "a nuance on the referred systems, stating that AI systems “shall not be considered as": "cannot be solely recognised by analysing speech or\nimage data.\nIt\nis quite easy for"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "circumstances, mainly because of\nsocial pressure. For\ninstance, people may pretend"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "to smile and laugh while\nthey are\nfeeling sad or angry [83]. Ethical\nconsiderations"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "regarding privacy and data security are of high importance when analysing physio-"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "logical data for affective purposes to avoid both:\n‘(Full) Affective Mind Reading’ and"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "‘Affective Mind Writing’."
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "Many further challenges and ethical concerns remain and will also become more"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "apparent once such technology is broadly used – hopefully, the community can provide"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "technical and legal means of protection to ensure we are all enjoying the positive side"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "of Affective Computing only."
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "4 Outlook and Conclusions"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "In this paper, we analysed the affective\ncapabilities of\ncurrently available Founda-"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "tion Models\n(FM)\nexploiting the vision,\nthe\nlinguistics, and the\nspeech (acoustic)"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "modalities. While\nthe\naffective\ngeneration and analysis\ncapabilities\nof\nthe\nvision-"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "and the linguistic-based FMs are plausible,\nthe affective generation and analysis of"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "speech-based FMs\nis not yet mature enough. Nonetheless,\nit\nis\nreasonable to imag-"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "ine a not-too-distant\nfuture where\nthis\ntechnology achieves\nsimilar\nresults as with"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "the\nother\ntwo modalities. Despite not being\ncurrently\navailable, we\nalso\nenvision"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "physiological-based FMs to be developed and explored in the near future."
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "One\nof\nthe main outcomes\nof\nthis work is\nthe\ncollection of\ntwo\nsynthetically-"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "generated affective corpora generated with FMs – one containing facial\nimages,\nthe"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "other textual sentences that will be publicly available. The models training and the"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "analyses\nreported herein were performed assuming that\nthe synthetically generated"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "instances conveyed the prompted emotions. Nonetheless, we acknowledge this could"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "not always be the case. To overcome this\nlimitation, we plan to run a data collec-"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "tion with human annotators to annotate the generated samples, assessing the affective"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "capabilities of the selected FMs from a human perspective."
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "Funding"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "This\nproject\nhas\nreceived\nfunding\nfrom the DFG’s Reinhart Koselleck\nproject"
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "No. 442218748 (AUDI0NOMOUS)."
        },
        {
          "people to control – or even hide – their\nreal current emotional\nstate under certain": "References"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "Interaction: Recent Advances and Future Perspectives. Frontiers in Robotics and"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "AI 7 (2020). Paper ID: 532279."
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "[5] Liu, Z.\net al.\nA Facial Expression Emotion Recognition Based Human-robot"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "Interaction System. IEEE/CAA Journal of Automatica Sinica 4, 668–676 (2017)."
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "[6] Crumpton, J. & Bethel, C. L.\nA Survey of Using Vocal Prosody to Convey"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "Emotion in Robot Speech.\nInternational Journal of Social Robotics 8, 271–285"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "(2016)."
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "[7] Tan, L. et al. Speech Emotion Recognition Enhanced Traffic Efficiency Solution"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "for Autonomous Vehicles\nin a 5G-Enabled Space–Air–Ground Integrated Intel-"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "ligent Transportation System.\nIEEE Transactions on Intelligent Transportation"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "Systems 23, 2830–2842 (2022)."
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "[8] Amiriparian, S. et al. Speech-Based Classification of Defensive Communication:"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "A Novel Dataset and Results,\nIn Proceedings of the 24th Annual Conference of"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,"
        },
        {
          "[4] Spezialetti, M., Placidi, G. & Rossi, S. Emotion Recognition for Human-robot": "Ireland, 2023)."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "Ireland, 2023)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[9] Mehrabian, A.\nin Communication Without Words\n(Taylor & Francis, 1968)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[10] Li, S. & Deng, W.\nDeep Facial Expression Recognition: A Survey.\nIEEE"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "Transactions on Affective Computing 13, 1195–1215 (2022)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[11] Takalkar, M., Xu, M., Wu, Q. & Chaczko, Z. A survey:\nfacial micro-expression"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "recognition. Multimedia Tools and Applications 77, 19301–19325 (2018)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[12] Birjali, M., Kasri, M. & Beni-Hssane, A.\nA comprehensive\nsurvey on senti-"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "ment analysis: Approaches, challenges and trends. Knowledge-Based Systems 226"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "(2021). Paper ID: 107134."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[13] Poria, S., Majumder, N., Mihalcea, R. & Hovy, E. Emotion Recognition in Con-"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "versation: Research Challenges, Datasets, and Recent Advances.\nIEEE Access 7,"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "100943–100953 (2019)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[14] Schuller, B. W.\nSpeech Emotion Recognition: Two Decades\nin\na Nutshell,"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "Benchmarks,\nand Ongoing Trends.\nCommunications\nof\nthe ACM 61,\n90–99"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "(2018)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[15] Khalil, R. A. et al. Speech Emotion Recognition Using Deep Learning Techniques:"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "A Review.\nIEEE Access 7, 117327–117345 (2019)."
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "[16] Wang, Y. et al. A systematic review on affective computing: emotion models,"
        },
        {
          "the International Speech Communication Association, 2703–2707 (ISCA, Dublin,": "databases, and recent advances.\nInformation Fusion 83-84, 19–52 (2022)."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "From unimodal analysis\nto multimodal\nfusion.\nInformation Fusion 37, 98–125"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "(2017)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[18] Ekman, P. & Friesen, W. Facial action coding system: a technique for the mea-"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "surement of\nfacial movement\n(Consulting Psychologist Press, Palo Alto, CA,"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "1978)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[19] Murray,\nI. R. & Arnott, J. L.\nToward the simulation of emotion in synthetic"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "speech: A review of the literature on human vocal emotion. The Journal of\nthe"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Acoustical Society of America 93, 1097–1108 (1993)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[20] Caliskan, A., Bryson, J. J. & Narayanan, A.\nSemantics derived automatically"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "from language corpora contain human-like biases. Science 356, 183–186 (2017)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[21] Trigeorgis, G. et al.\nAdieu Features? End-to-End Speech Emotion Recognition"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "using a Deep Convolutional Recurrent Network, In Proceedings of the 41st Inter-"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "national Conference\non Acoustics, Speech,\nand Signal Processing,\n5200–5204"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "(IEEE, Shanghai, China, 2016)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[22] Zhang, Z., Han, J., Qian, K. & Schuller, B. Evolving Learning for Analysing Mood-"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Related Infant Vocalisation, In Proceedings of the 19th Annual Conference of the"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "International Speech Communication Association, 142–146 (ISCA, Hyderabad,"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "India, 2018)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[23] Rajapakshe, T.\net al.\nemoDARTS: Joint Optimisation of CNN & Sequential"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Neural Network Architectures for Superior Speech Emotion Recognition.\nIEEE"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Access 12, 110492–110503 (2024)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[24] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B. et al. Generative Adversar-"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "ial Nets,\nIn Proceedings of\nthe 28th Annual Conference on Neural\nInformation"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Processing Systems (NIPS, Montr´eal, Canada, 2014). 9 pages."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[25] Ding, H., Sricharan, K. & Chellappa, R. ExprGAN: Facial Expression Editing"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "with Controllable Expression Intensity, In Proceedings of the 32nd Conference on"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Artificial Intelligence, 6781–6788 (AAAI, Austin, TX, USA, 2018)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[26] Karras, T. et al. Analyzing and Improving the Image Quality of StyleGAN,\nIn"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Proceedings of\nthe Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "8110–8119 (CVF, Virtual Conference, 2020)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[27] Balakrishnan, G., Xiong, Y., Xia, W. & Perona, P. Towards Causal Benchmark-"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "ing of Bias\nin Face Analysis Algorithms. Deep Learning-Based Face Analytics."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "Advances in Computer Vision and Pattern Recognition 327–359 (2021)."
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "[28] Ghosh, S., Chollet, M., Laksana, E., Morency, L.-P. & Scherer, S. Affect-LM:"
        },
        {
          "[17] Poria, S., Cambria, E., Bajpai, R. & Hussain, A. A review of affective computing:": "A Neural Language Model\nfor Customizable Affective Text Generation (2017)."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[29] Zhou, K., Sisman, B., Rana, R., Schuller, B. W. & Li, H. Speech Synthesis With"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Mixed Emotions.\nIEEE Transactions on Affective Computing\n14, 3120–3134"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "(2023)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[30] Wang, Y. et al. Style Tokens: Unsupervised Style Modeling, Control and Trans-"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "fer\nin End-to-End Speech Synthesis,\nIn Proceedings of\nthe 35th International"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Conference on Machine Learning, 5180–5189 (PMLR, Stockholm, Sweden, 2018)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[31] Lucey, P., Cohn,\nJ. F., Kanda, T., Saragih,\nJ.\net\nal.\nThe Extended Cohn-"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "expression, In Workshop Proceedings of the Conference on Computer Vision and"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Pattern Recognition, 94–101 (IEEE, San Francisco, CA, USA, 2010)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[32] Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W. F. & Weiss, B. A database"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "of\ngerman emotional\nspeech,\nIn Proceedings of\nthe 6th Annual Conference of"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "the International Speech Communication Association, 1517–1520 (ISCA, Lisbon,"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Portugal, 2005)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[33] Mollahosseini, A., Hasani, B. & Mahoor, M. H. AffectNet: A Database for Facial"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Expression, Valence, and Arousal Computing in the Wild. IEEE Transactions on"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Affective Computing 10, 18–31 (2019)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[34] Kossaifi, J. et al.\nSEWA DB: A Rich Database for Audio-Visual Emotion and"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Sentiment Research in the Wild.\nIEEE Transactions on Pattern Analysis and"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Machine Intelligence 43, 1022–1040 (2021)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[35] Rombach, R., Blattmann, A., Lorenz, D., Esser, P. et al. High-Resolution Image"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Synthesis with Latent Diffusion Models, In Proceedings of the International Con-"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "ference on Computer Vision and Pattern Recognition, 10684–10695 (CVF, New"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Orleans, LA, USA, 2022)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[36] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit,\nJ.\net\nal.\nAttention Is All"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "You Need, In Proceedings of the 31st Annual Conference on Neural Information"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Processing Systems (NIPS, Long Beach, CA, USA, 2017). 10 pages."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[37] Devlin, J., Chang, M., Lee, K. & Toutanova, K.\nBERT: Pre-training of Deep"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Bidirectional Transformers\nfor Language Understanding,\nIn Proceedings of\nthe"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Conference of the North American Chapter of the Association for Computational"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Linguistics: Human Language Technologies, 4171–4186 (ACL, Minnesota, MN,"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "USA, 2019)."
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "[38] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A.\net al.\nLearning Transfer-"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "able Visual Models\nfrom Natural Language Supervision,\nIn Proceedings of\nthe"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "38th International Conference on Machine Learning, 8748–8763 (PMLR, Virtual"
        },
        {
          "Preprint at https://arxiv.org/abs/1704.06851.": "Conference, 2021)."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "to follow instructions with human feedback,\nIn Proceedings of\nthe 36th Annual"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Conference on Neural Information Processing Systems, 27730–27744 (NIPS, New"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Orleans, LA, USA, 2022)."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[40] Touvron, H. et al.\nLLaMA: Open and Efficient Foundation Language Models"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "(2023). Preprint at https://arxiv.org/abs/2302.13971."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[41] Wang, C. et al. Neural Codec Language Models are Zero-shot Text\nto Speech"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Synthesizers (2023). Preprint at https://arxiv.org/abs/2301.02111."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[42] Podell, D. et al.\nSDXL:\nImproving Latent Diffusion Models for High-resolution"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Image Synthesis (2023). Preprint at https://arxiv.org/abs/2307.01952."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[43] Schaeffer, R., Miranda, B. & Koyejo, S. Are Emergent Abilities of Large Lan-"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "guage Models a Mirage?, In Proceedings of the 37th Annual Conference on Neural"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Information Processing Systems (NIPS, New Orleans, LA, USA, 2023). 17 pages."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[44] Ekman, P. & Friesen, W. V. Constants across cultures in the face and emotion."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Journal of Personality and Social Psychology 17, 124–129 (1971)."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[45] Ho,\nJ.,\nJain, A. & Abbeel, P.\nDenoising Diffusion Probabilistic Models,\nIn"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Proceedings of\nthe 34th Annual Conference on Neural\nInformation Processing"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Systems (NIPS, Virtual Conference, 2020). 12 pages."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[46] Ramesh, A. et al.\nZero-Shot Text-to-Image Generation,\nIn Proceedings of\nthe"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "38th International Conference on Machine Learning, 8821–8831 (PMLR, Virtual"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Conference, 2021)."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[47] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical Text-"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Conditional\nImage Generation with CLIP Latents\n(2022).\nPreprint at https:"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "//arxiv.org/abs/2204.06125."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[48] Plutchik, R.\nEmotions\nand\nlife: Perspectives\nfrom psychology,\nbiology,\nand"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "evolution (American Psychological Association, 2003)."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[49] Baltruˇsaitis, T., Robinson, P. & Morency,\nL.\nOpenFace:\nan Open\nSource"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Facial Behavior Analysis Toolkit,\nIn Proceedings of\nthe Winter Conference on"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Applications of Computer Vision (IEEE, Lake Placid, NY, USA, 2016). 10 pages."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[50] Dosovitskiy, A. et al. An Image Is Worth 16x16 Words: Transformers for Image"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Recognition at Scale,\nIn Proceedings\nof\nthe\n9th International Conference\non"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Learning Representations (ICLR, Virtual Conference, 2021). 21 pages."
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "[51] Goodfellow, I. J. et al. Challenges in Representation Learning: A Report on Three"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Machine Learning Contests,\nIn Proceedings of\nthe International Conference on"
        },
        {
          "[39] Ouyang, L., Wu, J., Jiang, X., Almeida, D.\net al.\nTraining\nlanguage models": "Neural\nInformation Processing, 117–124 (Springer, Daegu, Republic of Korea,"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2013).": "[52] Pe˜na, A., Morales, A., Serna,\nI., Fierrez, J. & Lapedriza, A.\nFacial Expres-"
        },
        {
          "2013).": "sions as a Vulnerability in Face Recognition, In Proceedings of the International"
        },
        {
          "2013).": "Conference on Image Processing, 2988–2992 (IEEE, Anchorage, AK, USA, 2021)."
        },
        {
          "2013).": "[53] Liu, H., Li, C., Wu, Q. & Lee, Y. J. Visual Instruction Tuning, In Proceedings of"
        },
        {
          "2013).": "the 37th Annual Conference on Neural\nInformation Processing Systems (NIPS,"
        },
        {
          "2013).": "New Orleans, LA, USA, 2023). 25 pages."
        },
        {
          "2013).": "[54] Liu, H., Li, C., Li, Y. & Lee, Y. J.\nImproved Baselines with Visual\nInstruc-"
        },
        {
          "2013).": "tion Tuning, In Proceedings of the Conference on Computer Vision and Pattern"
        },
        {
          "2013).": "Recognition, 26296–26306 (CVF, Seattle, WA, USA, 2024)."
        },
        {
          "2013).": "[55] Touvron, H.\net al.\nLlama 2: Open Foundation and Fine-Tuned Chat Models"
        },
        {
          "2013).": "(2023). Preprint at https://arxiv.org/abs/2307.09288."
        },
        {
          "2013).": "[56] Mikolov, T., Karafi´at, M., Burget, L., ˇCernock´y, J. & Khudanpur, S. Recurrent"
        },
        {
          "2013).": "neural network based language model, In Proceedings of the 11th Annual Confer-"
        },
        {
          "2013).": "ence of the International Speech Communication Association, 1045–1048 (ISCA,"
        },
        {
          "2013).": "2010)."
        },
        {
          "2013).": "[57] Radford, A., Narasimhan, K., Salimans, T., Sutskever,\nI. et al.\nImproving Lan-"
        },
        {
          "2013).": "guage Understanding by Generative Pre-Training\n(2018).\nPreprint\nat https:"
        },
        {
          "2013).": "//openai.com/research/language-unsupervised."
        },
        {
          "2013).": "[58] Achiam, J. et al. GPT-4 Technical Report (2023). Preprint at https://arxiv.org/"
        },
        {
          "2013).": "abs/2303.08774."
        },
        {
          "2013).": "[59] Li, C.\net\nal.\nLarge Language Models Understand and Can be Enhanced by"
        },
        {
          "2013).": "Emotional Stimuli (2023). Preprint at https://arxiv.org/abs/2307.11760."
        },
        {
          "2013).": "[60] Broekens, J. et al. Fine-grained Affective Processing Capabilities Emerging from"
        },
        {
          "2013).": "Large Language Models, In Proceedings of the 11th International Conference on"
        },
        {
          "2013).": "Affective Computing and Intelligent\nInteraction (IEEE, Cambridge, MA, USA,"
        },
        {
          "2013).": "2023). 8 pages."
        },
        {
          "2013).": "[61] Amin, M., Cambria, E. & Schuller, B. W. Will Affective Computing Emerge"
        },
        {
          "2013).": "From Foundation Models and General Artificial Intelligence? A First Evaluation"
        },
        {
          "2013).": "of ChatGPT.\nIEEE Intelligent Systems 38, 15–23 (2023)."
        },
        {
          "2013).": "[62] Amin, M. M., Mao, R., Cambria, E. & Schuller, B. W.\nA Wide Evaluation"
        },
        {
          "2013).": "of ChatGPT on Affective Computing Tasks.\nIEEE Transactions on Affective"
        },
        {
          "2013).": "Computing (2024). 9 pages."
        },
        {
          "2013).": "[63] Wang, X., Li, X., Yin, Z., Wu, Y. & Liu, J.\nEmotional\nIntelligence of Large"
        },
        {
          "2013).": "Language Models. Journal of Pacific Rim Psychology 17 (2023)."
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "06825."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[65] Jiang, A. Q. et al. Mixtral of Experts (2024). Preprint at https://arxiv.org/abs/"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "2401.04088."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[66] Fedus, W., Zoph, B. & Shazeer, N. Switch transformers: Scaling to trillion param-"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "eter models with simple and efficient sparsity. The Journal of Machine Learning"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Research 23, 1–39 (2022)."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[67] Demszky, D. et al. GoEmotions: A Dataset of Fine-Grained Emotions,\nIn Pro-"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "ceedings\nof\nthe\n58th Annual Meeting\nof\nthe Association\nfor Computational"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Linguistics, 4040–4054 (ACL, Virtual Conference, 2022)."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[68] Liu, Y., Ott, M., Goyal, N., Du, J. et al. RoBERTa: A Robustly Optimized BERT"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Pretraining Approach (2019). Preprint at https://arxiv.org/abs/1907.11692."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[69] Amin, M. M. & Schuller, B. W. On Prompt Sensitivity of ChatGPT in Affective"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Computing,\nIn Proceedings of\nthe 12th International Conference on Affective"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Computing and Intelligent Interaction (IEEE, Glasgow, Scotland, 2024)."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[70] Zheng, L. et al.\nJudging LLM-as-a-Judge with MT-Bench and Chatbot Arena,"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "In Proceedings of the 37th Annual Conference on Neural Information Processing"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Systems (NIPS, New Orleans, LA, USA, 2023). 29 pages."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[71] Zhou, Y.\net al.\nLarge Language Models are Human-Level Prompt Engineers"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "(2022). Preprint at https://arxiv.org/abs/2211.01910."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[72] Triantafyllopoulos, A.\net al.\nAn Overview of Affective Speech Synthesis and"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Conversion in the Deep Learning Era. Proceedings of\nthe IEEE 111, 1355–1381"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "(2023)."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[73] Yang, D. et al. UniAudio: An Audio Foundation Model Toward Universal Audio"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Generation (2023). Preprint at https://arxiv.org/abs/2310.00704."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[74] Leng, Y.\net\nal.\nPromptTTS 2: Describing\nand Generating Voices with Text"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Prompt (2023). Preprint at https://arxiv.org/abs/2309.02285."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[75] Wu, J., Gan, W., Chen, Z., Wan, S. & Philip, S. Y. Multimodal Large Language"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Models: A Survey,\nIn Proceedings of the International Conference on Big Data,"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "2247–2256 (IEEE, Sorrento, Italy, 2023)."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[76] Zhang, D. et al. MM-LLMs: Recent Advances\nin MultiModal Large Language"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Models (2024). Preprint at https://arxiv.org/abs/2401.13601."
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "[77] Moon, S. et al. AnyMAL: An Efficient and Scalable Any-Modality Augmented"
        },
        {
          "[64] Jiang, A. Q. et al. Mistral 7B (2023). Preprint at https://arxiv.org/abs/2310.": "Language Model (2023). Preprint at https://arxiv.org/abs/2309.16058."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "(2023). Preprint at https://arxiv.org/abs/2311.18799."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "[79] Wang, X., Zhuang, B. & Wu, Q. ModaVerse: Efficiently Transforming Modalities"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "with LLMs,\nIn Proceedings of the Conference on Computer Vision and Pattern"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "Recognition, 26606–26616 (CVF, Seattle, WA, USA, 2024)."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "[80] Chu, Y. et al. Qwen-Audio: Advancing Universal Audio Understanding via Uni-"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "fied Large-Scale Audio-Language Models (2023). Preprint at https://arxiv.org/"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "abs/2311.07919."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "[81] Chen, F. et al.\nX-LLM: Bootstrapping Advanced Large Language Models by"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "Treating Multi-Modalities as Foreign Languages\n(2023).\nPreprint at https://"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "arxiv.org/abs/2305.04160."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "[82] Bommasani, R., Klyman, K., Zhang, D. & Liang, P.\nDo Foundation Model"
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "Providers Comply with the EU AI Act?\n(2023).\nAvailable\nin https://crfm."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "stanford.edu/2023/06/15/eu-ai-act.html."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "[83] Shu, L. et al.\nA Review of Emotion Recognition Using Physiological Signals."
        },
        {
          "[78] Panagopoulou, A. et al.\nX-InstructBLIP: A Framework for aligning X-Modal": "Sensors 18 (2018)."
        }
      ],
      "page": 27
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Depression Detection Using Emotion Artificial Intelligence",
      "authors": [
        "M Deshpande",
        "V Rao"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Conference on Intelligent Sustainable Systems"
    },
    {
      "citation_id": "3",
      "title": "Detecting Stress during Real-world Driving Tasks Using Physiological Sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "4",
      "title": "Emotion Recognition for Human-robot Interaction: Recent Advances and Future Perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "5",
      "title": "A Facial Expression Emotion Recognition Based Human-robot Interaction System",
      "authors": [
        "Z Liu"
      ],
      "year": "2017",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "6",
      "title": "A Survey of Using Vocal Prosody to Convey Emotion in Robot Speech",
      "authors": [
        "J Crumpton",
        "C Bethel"
      ],
      "year": "2016",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "7",
      "title": "Speech Emotion Recognition Enhanced Traffic Efficiency Solution for Autonomous Vehicles in a 5G-Enabled Space-Air-Ground Integrated Intelligent Transportation System",
      "authors": [
        "L Tan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "8",
      "title": "Speech-Based Classification of Defensive Communication: A Novel Dataset and Results",
      "authors": [
        "S Amiriparian"
      ],
      "year": "2023",
      "venue": "Proceedings of the 24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "9",
      "title": "Communication Without Words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1968",
      "venue": "Communication Without Words"
    },
    {
      "citation_id": "10",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "A survey: facial micro-expression recognition",
      "authors": [
        "M Takalkar",
        "M Xu",
        "Q Wu",
        "Z Chaczko"
      ],
      "year": "2018",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "12",
      "title": "A comprehensive survey on sentiment analysis: Approaches, challenges and trends",
      "authors": [
        "M Birjali",
        "M Kasri",
        "A Beni-Hssane"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "13",
      "title": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "15",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "R Khalil"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "A systematic review on affective computing: emotion models, databases, and recent advances",
      "authors": [
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "17",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: a technique for the measurement of facial movement"
    },
    {
      "citation_id": "19",
      "title": "Toward the simulation of emotion in synthetic speech: A review of the literature on human vocal emotion",
      "authors": [
        "I Murray",
        "J Arnott"
      ],
      "year": "1993",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "20",
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "authors": [
        "A Caliskan",
        "J Bryson",
        "A Narayanan"
      ],
      "year": "2017",
      "venue": "Science"
    },
    {
      "citation_id": "21",
      "title": "Adieu Features? End-to-End Speech Emotion Recognition using a Deep Convolutional Recurrent Network",
      "authors": [
        "G Trigeorgis"
      ],
      "year": "2016",
      "venue": "Proceedings of the 41st International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Evolving Learning for Analysing Mood-Related Infant Vocalisation",
      "authors": [
        "Z Zhang",
        "J Han",
        "K Qian",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "23",
      "title": "emoDARTS: Joint Optimisation of CNN & Sequential Neural Network Architectures for Superior Speech Emotion Recognition",
      "authors": [
        "T Rajapakshe"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Generative Adversarial Nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu"
      ],
      "year": "2014",
      "venue": "Proceedings of the 28th Annual Conference on Neural Information Processing Systems (NIPS"
    },
    {
      "citation_id": "25",
      "title": "ExprGAN: Facial Expression Editing with Controllable Expression Intensity",
      "authors": [
        "H Ding",
        "K Sricharan",
        "R Chellappa"
      ],
      "year": "2018",
      "venue": "Proceedings of the 32nd Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Analyzing and Improving the Image Quality of StyleGAN",
      "authors": [
        "T Karras"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Towards Causal Benchmarking of Bias in Face Analysis Algorithms. Deep Learning-Based Face Analytics",
      "authors": [
        "G Balakrishnan",
        "Y Xiong",
        "W Xia",
        "P Perona"
      ],
      "year": "2021",
      "venue": "Advances in Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Neural Language Model for Customizable Affective Text Generation",
      "authors": [
        "S Ghosh",
        "M Chollet",
        "E Laksana",
        "L.-P Morency",
        "S Scherer",
        "Affect-Lm"
      ],
      "year": "2017",
      "venue": "Neural Language Model for Customizable Affective Text Generation"
    },
    {
      "citation_id": "29",
      "title": "Speech Synthesis With Mixed Emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",
      "authors": [
        "Y Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 35th International Conference on Machine Learning"
    },
    {
      "citation_id": "31",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanda",
        "J Saragih"
      ],
      "year": "2010",
      "venue": "Workshop Proceedings of the Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of the 6th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "33",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild",
      "authors": [
        "J Kossaifi"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NIPS"
    },
    {
      "citation_id": "37",
      "title": "Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "38",
      "title": "Learning Transferable Visual Models from Natural Language Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "39",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang",
        "D Almeida"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and Efficient Foundation Language Models"
    },
    {
      "citation_id": "41",
      "title": "Neural Codec Language Models are Zero-shot Text to Speech Synthesizers",
      "authors": [
        "C Wang"
      ],
      "year": "2023",
      "venue": "Neural Codec Language Models are Zero-shot Text to Speech Synthesizers"
    },
    {
      "citation_id": "42",
      "title": "SDXL: Improving Latent Diffusion Models for High-resolution Image Synthesis",
      "authors": [
        "D Podell"
      ],
      "year": "2023",
      "venue": "SDXL: Improving Latent Diffusion Models for High-resolution Image Synthesis"
    },
    {
      "citation_id": "43",
      "title": "Are Emergent Abilities of Large Language Models a Mirage?",
      "authors": [
        "R Schaeffer",
        "B Miranda",
        "S Koyejo"
      ],
      "year": "2023",
      "venue": "Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NIPS"
    },
    {
      "citation_id": "44",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "45",
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": [
        "J Ho",
        "A Jain",
        "P Abbeel"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th Annual Conference on Neural Information Processing Systems (NIPS, Virtual Conference"
    },
    {
      "citation_id": "46",
      "title": "Zero-Shot Text-to-Image Generation",
      "authors": [
        "A Ramesh"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "47",
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "authors": [
        "A Ramesh",
        "P Dhariwal",
        "A Nichol",
        "C Chu",
        "M Chen"
      ],
      "year": "2022",
      "venue": "Hierarchical Text-Conditional Image Generation with CLIP Latents"
    },
    {
      "citation_id": "48",
      "title": "Emotions and life: Perspectives from psychology, biology, and evolution",
      "authors": [
        "R Plutchik"
      ],
      "year": "2003",
      "venue": "Emotions and life: Perspectives from psychology, biology, and evolution"
    },
    {
      "citation_id": "49",
      "title": "OpenFace: an Open Source Facial Behavior Analysis Toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "50",
      "title": "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2021",
      "venue": "Proceedings of the 9th International Conference on Learning Representations (ICLR, Virtual Conference"
    },
    {
      "citation_id": "51",
      "title": "Challenges in Representation Learning: A Report on Three Machine Learning Contests",
      "authors": [
        "I Goodfellow"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Conference on Neural Information Processing"
    },
    {
      "citation_id": "52",
      "title": "Facial Expressions as a Vulnerability in Face Recognition",
      "authors": [
        "A Peña",
        "A Morales",
        "I Serna",
        "J Fierrez",
        "A Lapedriza"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Image Processing"
    },
    {
      "citation_id": "53",
      "title": "Visual Instruction Tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NIPS"
    },
    {
      "citation_id": "54",
      "title": "Improved Baselines with Visual Instruction Tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Y Li",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
    },
    {
      "citation_id": "56",
      "title": "Recurrent neural network based language model",
      "authors": [
        "T Mikolov",
        "M Karafiát",
        "L Burget",
        "J Černocký",
        "S Khudanpur"
      ],
      "year": "2010",
      "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "57",
      "title": "Improving Language Understanding by Generative Pre-Training (2018)",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "venue": "Improving Language Understanding by Generative Pre-Training (2018)"
    },
    {
      "citation_id": "58",
      "title": "GPT-4",
      "authors": [
        "J Achiam"
      ],
      "year": "2023",
      "venue": "GPT-4"
    },
    {
      "citation_id": "59",
      "title": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli",
      "authors": [
        "C Li"
      ],
      "year": "2023",
      "venue": "Large Language Models Understand and Can be Enhanced by Emotional Stimuli"
    },
    {
      "citation_id": "60",
      "title": "Fine-grained Affective Processing Capabilities Emerging from Large Language Models",
      "authors": [
        "J Broekens"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "61",
      "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "62",
      "title": "A Wide Evaluation of ChatGPT on Affective Computing Tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Emotional Intelligence of Large Language Models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "64",
      "title": "Mistral",
      "authors": [
        "A Jiang"
      ],
      "year": "2023",
      "venue": "Mistral"
    },
    {
      "citation_id": "65",
      "title": "Mixtral of Experts",
      "authors": [
        "A Jiang"
      ],
      "year": "2024",
      "venue": "Mixtral of Experts"
    },
    {
      "citation_id": "66",
      "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "authors": [
        "W Fedus",
        "B Zoph",
        "N Shazeer"
      ],
      "year": "2022",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "67",
      "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
      "authors": [
        "D Demszky"
      ],
      "year": "2022",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "68",
      "title": "A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du"
      ],
      "year": "2019",
      "venue": "A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "citation_id": "69",
      "title": "On Prompt Sensitivity of ChatGPT in Affective Computing",
      "authors": [
        "M Amin",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proceedings of the 12th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "70",
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "authors": [
        "L Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 37th Annual Conference on Neural Information Processing Systems (NIPS"
    },
    {
      "citation_id": "71",
      "title": "Large Language Models are Human-Level Prompt Engineers",
      "authors": [
        "Y Zhou"
      ],
      "year": "2022",
      "venue": "Large Language Models are Human-Level Prompt Engineers"
    },
    {
      "citation_id": "72",
      "title": "An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era",
      "authors": [
        "A Triantafyllopoulos"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "73",
      "title": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation",
      "authors": [
        "D Yang"
      ],
      "year": "2023",
      "venue": "UniAudio: An Audio Foundation Model Toward Universal Audio Generation"
    },
    {
      "citation_id": "74",
      "title": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
      "authors": [
        "Y Leng"
      ],
      "year": "2023",
      "venue": "PromptTTS 2: Describing and Generating Voices with Text Prompt"
    },
    {
      "citation_id": "75",
      "title": "Multimodal Large Language Models: A Survey",
      "authors": [
        "J Wu",
        "W Gan",
        "Z Chen",
        "S Wan",
        "S Philip"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Big Data"
    },
    {
      "citation_id": "76",
      "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
      "authors": [
        "D Zhang"
      ],
      "year": "2024",
      "venue": "MM-LLMs: Recent Advances in MultiModal Large Language Models"
    },
    {
      "citation_id": "77",
      "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model",
      "authors": [
        "S Moon"
      ],
      "year": "2023",
      "venue": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"
    },
    {
      "citation_id": "78",
      "title": "Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning",
      "authors": [
        "A Panagopoulou"
      ],
      "year": "2023",
      "venue": "Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"
    },
    {
      "citation_id": "79",
      "title": "ModaVerse: Efficiently Transforming Modalities with LLMs",
      "authors": [
        "X Wang",
        "B Zhuang",
        "Q Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "80",
      "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
      "authors": [
        "Y Chu"
      ],
      "year": "2023",
      "venue": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models"
    },
    {
      "citation_id": "81",
      "title": "Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
      "authors": [
        "F Chen"
      ],
      "year": "2023",
      "venue": "Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"
    },
    {
      "citation_id": "82",
      "title": "Do Foundation Model Providers Comply with the EU AI Act?",
      "authors": [
        "R Bommasani",
        "K Klyman",
        "D Zhang",
        "P Liang"
      ],
      "year": "2023",
      "venue": "Do Foundation Model Providers Comply with the EU AI Act?"
    },
    {
      "citation_id": "83",
      "title": "A Review of Emotion Recognition Using Physiological Signals",
      "authors": [
        "L Shu"
      ],
      "year": "2018",
      "venue": "Sensors"
    }
  ]
}