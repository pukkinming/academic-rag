{
  "paper_id": "2304.02181v2",
  "title": "On The Impact Of Voice Anonymization On Speech Diagnostic Applications: A Case Study On Covid-19 Detection",
  "published": "2023-04-05T01:09:58Z",
  "authors": [
    "Yi Zhu",
    "Mohamed Imoussaïne-Aïkous",
    "Carolyn Côté-Lussier",
    "Tiago H. Falk"
  ],
  "keywords": [
    "Voice anonymization",
    "health diagnostics",
    "COVID-19 detection"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With advances seen in deep learning, voice-based applications are burgeoning, ranging from personal assistants, affective computing, to remote disease diagnostics. As the voice contains both linguistic and para-linguistic information (e.g., vocal pitch, intonation, speech rate, loudness), there is growing interest in voice anonymization to preserve speaker privacy and identity. Voice privacy challenges have emerged over the last few years and focus has been placed on removing speaker identity while keeping linguistic content intact. For affective computing and disease monitoring applications, however, the para-linguistic content may be more critical. Unfortunately, the effects that anonymization may have on these systems are still largely unknown. In this paper, we fill this gap and focus on one particular health monitoring application: speech-based COVID-19 diagnosis. We test three anonymization methods and their impact on five different state-of-the-art COVID-19 diagnostic systems using three public datasets. We validate the effectiveness of the anonymization methods, compare their computational complexity, and quantify the impact across different testing scenarios for both within-and across-dataset conditions. Additionally, we provided a comprehensive evaluation of the importance of different speech aspects for diagnostics and showed how they are affected by different types of anonymizers. Lastly, we show the benefits of using anonymized external data as a data augmentation tool to help recover some of the COVID-19 diagnostic accuracy loss seen with anonymization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Speech-Based Covid-19 Diagnostics",
      "text": "Speech-based diagnostic systems can be categorized into two groups: ones that rely on carefully designed hand-crafted features coupled with conventional machine learning classifiers, and ones that input raw signals directly into a deep learning model for classification. In the latter 'end-to-end' scenario, the deep learning model serves as a feature extractor and feature mapping function in one.\n\nWhen it comes to feature extraction from speech, the openSMILE toolkit  [31]  is by far the most popular. The largest feature set of openSMILE extracts over 6,000 acoustic features, including mel-frequency cepstral coefficients (MFCC), pitch contours, voicing-related information, as well as several other low-level descriptors (LLDs). This feature set has been used together with conventional classifiers, such as support vector machines (SVM), for the detection of different diseases  [32] -  [35] . More recently, it has been employed as a benchmark feature set for the INTERSPEECH 2021 ComParE COVID-19 Detection Challenge  [36] . For in-the-wild speech analysis, on the other hand, the modulation spectral representation (MSR) has shown benefits over openSMILE features for different applications (e.g.,  [37] ,  [38] ), including disease characterization (e.g.,  [39] ,  [40] ) and COVID-19 detection  [10] .\n\nExisting end-to-end systems, in turn, have relied on variants of the spectrogram representation as input, including the mel-spectrogram or the log-mel-spectrogram, as well as convolutional or recurrent neural network architectures for classification. Han et al., for example, showed that VGGish neural networks outperformed conventional methods in classifying different COVID-19 symptoms  [35] . Akman et al. developed a ResNet-like architecture for speech and coughbased COVID-19 detection  [41] . The Bi-directional Long-Short-Term-Memory (BiLSTM) neural network was used in the top-performing system competing in the second Diagnosis of COVID-19 using Acoustics (DiCOVA2) Challenge  [12] . Compared to conventional systems, end-to-end systems have demonstrated overall higher performance on several datasets without the need for a separate feature extraction step  [12] ,  [42] ,  [43] . Nonetheless, recent research has shown that while end-to-end models achieve state-of-the-art accuracy on a particular dataset, those results do not transfer well to other unseen datasets, where accuracy can drop to below chance levels  [44] ; this was not the case with hand-crafted features and conventional classifiers.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Speech Anonymization",
      "text": "Anonymization techniques comprise two categories: speech transformation and speech conversion. The former refers to modifications directly to the original speech, such as pitch shifting and warping  [45] ,  [46] , to remove personal identifiable information from the speech signal. The latter, in turn, converts one's voice to sound like that of another without changes in linguistic content  [47] . As voice privacy concerns are on the rise, voice anonymization has gained popularity recently and, in 2020, the Voice Privacy Challenge (VPC) was created  [24] . A popular method from the 2020 and 2022 VPCs employs the so-called McAdams coefficients  [24] ,  [25] , where shifts in the pole positions derived from linear predictive coding (LPC) analysis of speech signals  [48]  are used to achieve anonymization. Another popular voice transformation method is termed voicemask  [49] , where certain frequency components are compressed (or stretched) to generate a lower-pitched (or higher-pitched) voice signal. Voice conversion systems, on the other hand, have usually relied on modifications to speaker embeddings, such as the x-vector  [50]  and the ECAPA-TDNN embeddings  [51] , which are assumed to only carry nonverbal information that pertains to the speaker identity alone. The modified speaker embeddings are then input with speech content sequence to a speech synthesis module to reconstruct a new speech waveform  [26] . Several innovations have been proposed to the speech synthesis module to make the outcome sound more natural and of greater quality and intelligibility  [27] ,  [52] -  [54] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Anonymized Speech Diagnostics Systems",
      "text": "A. System Overview Figure  1  depicts the diagram of an anonymized speech diagnostics (SD) system. Conventionally, the original voice of user X is input to a diagnostic system that will generate a positive or negative output for the tested disease and/or symptom. If an automatic speaker verification (ASV) system was trained with data from user X, the ASV system would be able to detect user X's voice. In practice, SD systems are complex and models are often stored on the cloud, thus requiring the user's voice (or features) to be uploaded to the cloud. This transmission of data could result in privacy concerns. To overcome this, voice anonymization can be employed locally and anonymized data (or features) are sent to the cloud. In this case, user X would not be identified by the ASV system and speech-based diagnostics could proceed in a more secure and private manner.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Speech-Based Diagnostic Systems",
      "text": "Based on previous experiments on COVID-19 detection (e.g.,  [10] ), the five top-performing diagnostics systems are explored herein:\n\n1) openSMILE+SVM: A total of 6,373 static acoustics features were firstly extracted using the openSMILE toolbox  [31] , which were then input to a SVM classifier with a linear kernel. This system was used as the benchmark in the 2021 ComParE COVID-19 Speech Sub-challenge  [36] . 2) openSMILE+PCA+SVM: The high dimensionality of the openSMILE features can be problematic for smaller datasets. In  [55] , principal component analysis (PCA)  [56]  was used to compress the 6,000+ features into 300 components.\n\nHere, the number of principal components was treated as a hyper-parameter and a value of 100 was found to strike a good balance in accuracy and dimensionality.\n\n3) MSR+SVM: The MSR features have been used in  [10] ,  [44]  and shown to outperform openSMILE-based systems and to provide improved generalizability across datasets. The interested reader is referred to  [39] ,  [57]  for more details about the modulation spectrum. The modulation spectrum decomposes each frequency component along time into different modulation frequencies, which captures the abnormalities in respiration and articulation by focusing on long-term dynamics of speech. Each modulation spectrum comprises 23 frequency bins and 8 modulation frequency bins, which is then flattened into a vector and used as input to a linear SVM classifier.\n\n4) MSR+PCA+SVM: For more direct comparisons with the openSMILE system, here we also explore the compression of the 184-dimensional (23 × 8) vector via PCA, resulting in a final 100-dimensional vector for classification.\n\n5) Logmelspec+BiLSTM: The winning system in the Di-COVA2 Challenge was employed  [12]  as a benchmark. This system adopts the conventional log-mel-spectrogram (logmelspec) with first-and second-order deltas as input, along with a BiLSTM as the classifier. More details about the network architecture can be found in  [12] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Speech Anonymization Methods",
      "text": "A voice transformation and two voice conversion methods are explored here to gauge their differences in speech diagnostics performance. More details are provided below.\n\n1) McAdams coefficient: This approach uses a classical signal processing technique and does not require model training. It employs the so-called McAdams coefficient method  [48] ,  [58]  to shift the position of formants measured using linear predictive coding (LPC)  [59] . For each short-time speech frame, the method first separates the linear prediction residuals and linear prediction (LP) coefficients. The LP coefficients are then converted to pole positions in the z-plane by polynomial root-finding, where each pole position represents the position of one formant. The phase of the poles with imaginary parts is then raised to the power of the McAdams coefficient α. The new set of poles is then converted back to LP coefficients. Together with the original residuals, a new speech frame can be synthesized.\n\n2) Ling-GAN: For voice conversion, we implemented two systems based on generative adversarial networks (GAN). The overall architecture of these systems can be found in Figure  2 . The first system, abbreviated as 'Ling-GAN', was an off-theshelf anonymizer from  [27] , where all modules were already trained and applied to COVID-19 data without any fine-tuning. In general, it preserves the linguistic content (i.e., phoneme sequence) and uses a generator to generate fake, yet realistic speaker embeddings to substitute the original speaker embeddings. The original speech is first input to an automatic speech recognition (ASR) model to extract the phone sequence. The ASR model used here is based on the hybrid CTC (Connectionist Temporal Classification)/attention architecture  [60]  with a Conformer encoder  [61]  and a Transformer decoder. It should be emphasized that the output of the ASR is a phoneme sequence, detailing not only the phonemes uttered but also the pauses. In our exploratory analysis, we found that the removal of these pauses would change the rhythm of the generated speech and lead to degraded diagnostic performance. We hence kept all pauses in the extracted phoneme sequences. The ASR model used here supports English as the default input language, hence may lead to erroneous transcriptions when other languages are used. Although such issue can be potentially tackled by replacing with other multi-language ASR models, their compatibility with the anonymization and synthesizer blocks has not been tested. Hence, we remain using the same architecture as is proposed in  [27] , and leave the language compatibility for future investigation.\n\nThe anonymization is divided into two stages. During the first stage, the 512-dimensional x-vector  [50]  and the 192dimensional ECAPA-TDNN vector  [51]  are extracted using the SpeechBrain toolkit  [62]  and concatenated as the final speaker embeddings. At the second stage, a Wasserstein GAN with Quadratic Transport Cost (WGAN-QC)  [63]  is used to generate a pool of 5,000 'converted' speaker embeddings and saved for later use. When a new recording is input to the system, the model iteratively looks through the pool, and stops when it finds one with a cosine distance above 0.3 with the original speaker embeddings. This set of new embeddings are then used to substitute the original one for synthesis. The 0.3 threshold value of cosine distance was suggested from  [27] , which ensures sufficient difference in speaker traits while maintaining the naturalness. Finally, the FastSpeech 2 model  [64]  is used to synthesize the phone sequence into a spectrogram, followed by a HiFiGAN vocoder  [52]  to convert the spectrogram into a final speech waveform. The synthesizer is conditioned on the anonymized speaker embedding, hence keeping the linguistic content while obfuscating the speaker identity.\n\nIt is important to emphasize that this off-the-shelf GAN has not seen pathological speech data during its training  [65] . As a consequence, the generated speaker embeddings may not encapsulate health-related attributes, thus affecting diagnostic accuracy. The last anonymization system used overcomes this limitation, as detailed next.\n\n3) Ling-Pros-GAN: The second GAN-based system, abbreviated as 'Ling-Pros-GAN', was modified from  [65]  which can be seen as a more advanced version of the Ling-GAN. While sharing similar architecture, such as the ASR module and the synthesizer, the Ling-Pros-GAN further preserves prosody (i.e., pitch, energy, and duration) during anonymization and uses the style embeddings from  [66]  to represent speaker attributes. In addition, we fine-tuned the generator and discriminator using the aggregated training set data from all three COVID-19 datasets employed in this study. The goal of finetuning was to enable the GAN to generate COVID-like speaker embeddings.\n\nThe generator and discriminator were jointly trained via 2,000 iterations, with the batch size of 128 and learning rate of .00005. Other fine-tuning hyperparameters remained the same as reported in  [65] , which can also be found in our code repository. Figure  3  depicts the t-distributed stochastic neighbor embedding (t-SNE) plots  [67]  showing a 2-dimensional representation of the speaker embeddings in the COVID-19 datasets (red dots), those produced by the generator without fine-tuning (blue), and after fine-tuning (green). As can be seen, using just the pre-trained generator is not sufficient to model the COVID-19 speaker embedding distribution. With 2,000 iterations of fine-tuning, the generator was able to generate embeddings following a similar distribution of the COVID-19 embeddings. Different from the original implementation in  [65] , where a pre-generated pool of speaker embeddings were used, we modified Ling-Pros-GAN in a way that it randomly generates a small set of different speaker embeddings each time it receives a new recording, then chooses which embeddings to swap by iteratively examining the cosine similarity. In other words, Ling-Pros-GAN is guaranteed to generate an unseen version of anonymized speech even with the exact same input recording. In contrary, since Ling-GAN always chooses embeddings from a pre-generated pool, there is a slight chance that two recordings may be anonymized with the same generated embeddings. Such possibility becomes higher when the number of speakers increases. While such modification to Ling-pros-GAN improves the privacy, the computing time increases simultaneously due to the online generation process of speaker embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Databases",
      "text": "At the time of writing, most existing COVID-19 sound datasets target cough sound, such as the COUGHVID  [68] , Tos COVID-19  [69] , Virufy  [70] , and NoCoCoDa  [71] . Speech sound, on the other hand, is included in fewer datasets. To maximize the variability of data distribution and avoid biased results from one single dataset, we included three publicly available COVID-19 speech datasets, namely the multilingual 2021 ComParE COVID-19 Speech Sub-challenge (CSS) dataset  [11] , the second DiCOVA Challenge dataset  [12] , and the English subset from the Cambridge COVID-19 sound database  [55] . These datasets are referred to hereinafter as CSS, DiCOVA2, and Cambridge set, respectively. The demographics of three datasets are summarized in Table  I . It should be noted that though the full Cambridge database contains more speech samples, the English subset has been more carefully examined by the data holders to avoid potential confounding factors (e.g., languages, data quality, class balance, etc.)  [55] , hence is considered more suitable for our analysis.\n\nAll three datasets were crowdsourced, volunteers across the globe were encouraged to upload their voice data and metadata via apps. The same speech content was required per dataset. With CSS, participants were asked to utter the sentence \"I hope my data can help to manage the virus pandemic\" at most three times in their mother tongue, with the majority of samples being uttered in English, Portuguese, Italian, and Spanish. The same speech content was used for the Cambridge set but in English only. With DiCOVA2, participants did number counting from 1 to 10 at a normal pace in English. For all datasets, participants were asked to self-declare whether they were COVID-negative (including healthy or having COVID-like symptoms) or COVID-positive (including symptomatic and asymptomatic cases). It can be noticed from Table I that all three sets contained 10% to 30% asymptomatic COVID-positive cases. Additionally, nearly half of the COVID-negative samples in CSS and Cambridge are symptomatic, which is three times higher than that in Di-COVA2.\n\nThe CSS and Cambridge datasets were partitioned into three separate subsets by the challenge organizers, namely training, validation, and test. For comparisons, we employed the same challenge partition in this study. It should be emphasized that in the CSS dataset, several COVID-positive recordings were originally sampled at 8 kHz while the majority of the other files were sampled at 16 kHz. As suggested in  [36]  and our previous exploration  [10] , keeping these up-sampled recordings has been shown to lead to overly-optimistic results since classifiers learned to capture the difference in sampling rates instead of the actual pathological pattern. Thus, we removed them from our analysis. The DiCOVA2 dataset, in turn, is comprised of development and evaluation subsets, with the evaluation data being accessible only to challenge participants. Hence, we performed a speaker-independent training-test split (80/20%) using the development subset only and left the evaluation set for testing.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Tasks",
      "text": "As our final goal is to not only provide accurate diagnostics decisions but also ensure the protection of privacy of speaker identity, the evaluation was divided into three tasks. In the first task, we compared the effectiveness and complexity of different anonymization techniques. In Task-2, we then quantified the impact of anonymization techniques on diagnostics accuracy in different conditions. Finally, we provided explanations for the impact seen in Task-2, and explored solutions for improving the proposed systems.\n\n1) Task-1: Evaluating anonymization performance: As is shown in Figure  4 , for each speech recording, the speaker embeddings were extracted separately from the original version, the McAdams-anonymized version, the Ling-GAN anonymized version, and the Ling-Pros-GAN anonymized version. Cosine similarity was then computed between the embeddings of each two signals, where higher cosine similarity values represented higher resemblance between two speech samples. Meanwhile, we employed the pre-trained ECAPA-TDDN speaker verification model from SpeechBrain  [62]  to detect if two recordings are from the same speaker, then evaluated the misclassification rate, where higher values suggest more successful anonymization. Since multiple evaluation scenarios were considered in this study, where training and test data were processed with different anonymization methods, the cosine similarity and the misclassification rate were computed between not only the clean and anonymized data, but also data processed by different anonymization methods. Additionally, we measured the computation time spent by the three methods per recording, and calculated the average and standard deviation for each dataset. This helps to quantify and compare the time efficiency of the three anonymization methods.\n\n2) Task-2: Evaluating diagnostics accuracy: As aforementioned in Section I, training and test data could be anonymized using different methods. To mimic a realistic setting, we explore four different scenarios, as detailed below. Table II summarizes these conditions. Scenario-A: Unprotected: Here, both training and test data are original, thus anonymization is not performed. This encompasses the traditional diagnostic system evaluation and serves as a baseline of the maximum diagnostics accuracy that can be achieved by each model.   As data distributions vary across datasets  [72] , diagnostics performance obtained under within-dataset conditions may lack external validity and has been shown to be over-optimistic  [41] ,  [44] ,  [72] . To ensure the generalizability of the tested methods, for each scenario we explore both within-and crossdataset results. In the latter, models are trained on one dataset and tested on data from another set. As the CSS is a subset of the Cambridge set, we avoid using both datasets in the crossdatabase condition to avoid overly-optimistic results  [36] ,  [55] .\n\n3) Task-3: Anonymization for data augmentation: Data augmentation has been widely used in speech applications based on deep neural networks to improve accuracy, especially under mismatched train-test data distributions. One of the approaches to increase model generalizability is to use external data augmentation, which refers to the case where data from external datasets curated for similar tasks are pooled with the in-domain data to increase training sample size  [73] ,  [74] . In our case, we aim to improve the generalizability of diagnostic models to samples anonymized by unknown algorithms. We propose to combine anonymized external data with the original data as an augmentation approach to mitigate the degradation caused by anonymization. We focus on two cases, namely augmenting the ignorant and semi-informed scenarios, in which we observed diagnostic models performed the worst. As shown in Figure  5 , we experimented with four versions of the augmentation data, including the clean version (i.e.,",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Training And Inference Strategies 1) Training:",
      "text": "For the systems that rely on hand-crafted features, training data normalization was achieved by removing the mean and scaling to unit variance. The fitted scaler was then applied to the validation and test data. Hyper-parameters were tuned on the held-out validation set. The optimal SVM regularization parameter was searched between 1e -5 and 1; the SVM kernel was set to linear; and the number of PCs was experimented from 100 to 300. To train the BiLSTM classifier, in turn, recordings were first zero-padded to 10second length to ensure a fixed shape for logmelspec input; the spectrogram was then mean-variance normalized. Each minibatch was composed of 64 samples with random shuffling, forced to contain both COVID-positive and COVID-negative samples. Unlike  [12] , no oversampling of minority class or any other data augmentation techniques were used, as their effect on anonymization has yet to be quantified. The following hyper-parameters were used for training: Binary Cross Entropy (BCE) loss; Adam optimizer with an initial learning rate of 1e -4 and l 2 regularization set to 1e -4 . During the validation phase, an initial patience factor was set to 5 and reduced by 1 if the validation score did not increase. Training stopped whenever the patience factor reduced to 0, the number of training epochs was saved for the inference phase.\n\n2) Inference: For the first four systems, the pre-trained model with the highest validation score was then used for testing. As the BiLSTM classifier is more data-hungry, the optimal hyper-parameters found in the training phase were then used to train the classifier from scratch with the aggregated training and validation data. The number of training epochs maintained the same as that saved in the training phase.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "Since all three datasets are imbalanced, the area under the receiver-operating-characteristic curve (AUC-ROC) was chosen as the primary metric to measure the diagnostics accuracy. We further calculated the 95% confidence intervals (CIs) using 1000× bootstrap with replacement on the test set. According to  [75] , CIs can reflect the variability of diagnostics accuracy when the model is applied to a different population.\n\nAs mentioned previously, cosine similarity and misclassification rate were used to quantify the effectiveness of the three anonymization methods. The similarity scores are averaged across samples from all three datasets per method, where 0 represents no resemblance and 1 represents a perfect match between two tested speech conditions. While the Equal Error Rate (EER) is commonly used to evaluate anonymization efficacy, ground-truth speaker identifiers are required for each recording in order to verify if samples are from the same or different speakers. However, speaker identifiers were not available for the CSS and DiCOVA2 datasets. Instead, we rely on misclassification rate by employing a pre-trained speaker verification model from  [62] . For each recording, the model outputs a binary decision (yes/no) if it believes a pair of anonymized and clean speech signals come from the same speaker. The misclassification rate is then calculated by dividing the number of misclassified pairs over the total number of pairs per method, which reflects the percentage of successfully anonymized recordings. For an ideal anonymization system, the misclassification rate is expected to be 100%, i.e., the model should decide that all anonymized signals do not come from the same speaker as the clean signal counterpart. Lastly, computation time was recorded for each anonymization method, including the loading and exporting of the audio files. In the case of the two GAN methods, the loading time of the model itself was not taken into account in this computation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Experimental Results And Discussion",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Task-1: Anonymization Results",
      "text": "The average cosine similarity scores between the speaker embeddings of speech files anonymized by the different methods together with the misclassification rates are shown in Figure  6 . As can be seen, near perfect anonymization performance was achieved with both GAN-based methods (misclassification rates), with almost no similarity with either the original speech or the speech anonymized by other  methods. On the other hand, nearly half of the McAdamsanonymized samples can be successfully detected, suggesting some speaker-unique information still remained.\n\nThe computational complexity of the three anonymization methods is presented in Table  III  for all three datasets. While the GAN-based methods are shown to provide better anonymization effectiveness, it requires computational times approximately 10-20 times longer than using the McAdams coefficient method. The longest time was seen with Ling-Pros-GAN, since it requires extra time to extract prosody, which involves an online training loop, and to generate and find embeddings in real-time. As model loading time was not taken into account, the computation footprint of the GANbased methods could be larger in real-world settings. Additionally, the GAN-based methods rely on several pre-trained neural networks with millions of parameters (e.g., 22.3 million for ECAPA-TDNN embedding extractor; 10 million for the generator), which could make it challenging to be deployed on mobile devices.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Task-2: Within-Dataset Performance",
      "text": "The within-dataset performance of the five diagnostics systems under different anonymization scenarios is demonstrated in Figure  7 . As can be seen from the average AUC-ROC scores per scenario, the highest performance is achieved under scenario A, i.e., when anonymization is not performed. When the test data are anonymized using the McAdams coefficient (scenario B1), the average AUC-ROC score over all systems dropped by 8.9% (CSS), 5.9% (DiCOVA2), and 6.3% (Cambridge) relative to scenario A. A substantial decrease was observed when using both the Ling-GAN and Ling-Pros-GAN anonymizers (scenario B2 and B3), where an average relative drop 22.5% and 18.1% was achieved respectively. Moreover, nearly all systems degraded to chance levels under scenario C where models were trained with data anonymized by one method and tested with data anonymized with another, suggesting that anonymization may drastically remove COVID-19 speech information. Diagnostic performance in the fully-informed scenarios is shown to be close to scenario A. Among the three anonymizers, McAdams anonymization leads to higher diagnostic performance on average in scenario D.\n\nCompared to the Ling-GAN, Ling-Pros-GAN shows higher performance on the English datasets (DiCOVA2 and Cambridge) and lower performance on the multilingual one (CSS). Next, we evaluate the sensitivity of different diagnostics systems to anonymization and explore the relative drop in accuracy from scenario A to scenario B. Table IV reports the average drops seen per dataset. As can be seen, the two GANbased methods resulted in a substantially higher degradation relative to the McAdams coefficient method, with the Ling-GAN leading to the most severe decrease. This was expected and corroborates Task-1 results, where speaker embeddings of the GAN-anonymized speech showed practically no similarity to the original speech. Meanwhile, since Ling-Pros-GAN leaves the prosody intact and generates more COVID-like embeddings, it is likely to preserve more COVID-19 attributes than the Ling-GAN, thus rendering higher anonymized diagnostic performance. Previous studies have shown that speaker embeddings (e.g., x-vector) also contain other nonverbal information and can be used for speech para-linguistic tasks  [76] ,  [77] , such as speech emotion recognition  [78]  and disease detection [79],  [80] . While the GAN-based anonymizers substitute the original speaker embedding with a dissimilar speaker embedding, the obtained results suggest that healthrelated vocal characteristics are likely also discarded, thus resulting in significant drops in diagnostics accuracy.\n\nLastly, we use scenario A as the baseline and calculate the average drop in accuracy for scenario C, showing the impact that training models completely on anonymized data would have. For both openSMILE and MSR methods, we use the PCA-SVM pipeline to avoid the effects of difference in the number of features. The comparative results are reported in Table  V . As can be seen, all three diagnostic systems show degradaded performance, with the logmelspec+BiLSTM system shown to be on average more robust (21.6%) to the semiinformed anonymization scenario. Notwithstanding, it should  be highlighted that the logmelspec+BiLSTM system achieved the lowest AUC-ROC in scenario A. Interestingly, with the CSS dataset, the diagnostic system based on a BiLSTM and log-mel spectrogram input resulted in substantially lower degradation percentage compared to the two other systems based on traditional engineered features and classifiers. CSS is a multilingual dataset, thus hand-crafted features (e.g., syllabic rate, speech production features) used in these models may show more sensitivity to language.\n\nC. Task-2: Cross-dataset Performance\n\nFigure  8  shows the cross-dataset performance under the thirteen different testing scenarios. In line with previous studies  [44] ,  [81] , all five diagnostics systems demonstrated significantly lower performance relative to within-dataset results; the logmelspec+BiLSTM achieved the greatest drop in performance. Interestingly, in a few scenarios anonymization helped systems become more generalizable relative to the unprotected setting (e.g., scenarios B2 and C3 for the CSS-DiCOVA2 cross-database experiment). Figure  9  depicts the average change in accuracy relative to scenario A for all scenarios and diagnostic systems. While on average a 6.6% drop in accuracy was seen across all five systems, an increase of 2% and 5% was achieved with MSR+SVM and logmel-spec+BiLSTM systems for scenarios C4 and C2, respectively. It is important to note that both scenarios involved GANbased anonymized test data, thus had typically the lower crossdataset results to start off with.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Explaining The Degradation Caused By Different Anonymizers",
      "text": "While our study shows that typical anonymization systems lead to degraded diagnostic performance, it is unclear why different systems caused different levels of degradation and why some diagnostic models could still perform decently after anonymization. To answer these questions, we performed a comprehensive evaluation of the impact of different speech aspects on diagnostic performance, including the linguistic content, speaker representation, and prosody. Similar to the experimental setup of Task-1, we now compare the withindataset performance obtained by three categories of speech features, namely (1) the phoneme-level features, including the number of mispronunciations (as opposed to the speech script), number of pauses, and number of phonemes uttered per second; (2) the speaker representation extracted by concatenating  the pre-trained x-vector and ECAPA-TDNN embeddings  [51] ; and (3) prosodic features, such as the low-level descriptors of the F0 contour.\n\nA Linear Discriminant Analysis (LDA) classifier is applied on top of each of the feature sets for classification. The results achieved by these features are reported in Table VI. Among the three feature sets, speaker embeddings appear to be the most crucial features for all datasets, corroborating with Task-1 results where the GANs suffered the most severe degradation, where the original speaker embeddings were entirely substituted. Such finding also suggests that speakerunique attributes and health-related information are highly entangled in the speaker embeddings. Considering that existing anonymization systems rely heavily on these off-the-shelf speaker embeddings, it remains challenging to preserve the health information while altering only the speaker identifier.\n\nWhile a group of studies reported prosody as a key biomarker to characterize speech disorders, such as dysarthria  [82] -  [84] , our results show that phoneme-level linguistic features outperform prosodic features for COVID-19 detection. Specifically, we found the number of pauses and number of mispronunciations to be the most important phoneme-level features, with COVID-positive samples demonstrating more mispronunciations and fewer pauses. While the correlation between phoneme-level features and COVID-19 status has not been systematically studied, similar features have been examined for other diseases affecting speech production. For example,  [85]  shows that individuals with Parkinson's disease produced fewer pauses at syntactic boundaries; the statistics of pauses have been shown crucial for diagnosing neuromuscular  disorders, such as dysarthria  [86] . Since GAN-based systems left linguistic content intact during anonymization, these findings help explain why the diagnostic models could perform above chance-level even when only the phoneme sequences were preserved during anonymization.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "E. Visualizing Speech Processed By Different Anonymizers",
      "text": "To better understand the impact of different anonymization methods on speech characteristics, we first visualize the waveform of the speech processed by the three anonymizers (see Fig.  10 ) for a direct comparison. As can be seen, those processed by the McAdams anonymizer and Ling-Pros-GAN share higher similarities in the waveform envelope shape with the original signal compared to the one generated by Ling-GAN. The difference seen in the plot is in line with the architecture design of different anonymizers. Among the three, Ling-GAN loses prosody and most of the speaker attributes, hence is expected to cause the highest amount of changes in the anonymized speech. The Ling-Pros-GAN and McAdams anonymizer, in turn, leave the speech rhythm untouched (i.e., duration and energy of phonemes), hence leading to higher resemblance in the waveform envelope.\n\nNext, t-SNE plots are used to visualize the distribution of the speech features in two dimensions. Figure  11  shows the clusters of speech anonymized with different methods (computed from the training and validation data) and for the three features modalities explored herein: openSMILE (subplots a), MSR (b), and logmelspec (c). As can be seen, for all three feature sets, the distribution of clean speech (blue) is closer to that of the McAdams anonymized speech (orange) and Ling-Pros-GAN anonymized speech (red), while the Ling-GAN anonymized speech (green) shows the least similarity with the other two, corroborating findings from Tasks 1 and 2.\n\nMoreover, it can be seen from Figure  11a  and Figure  11b  that the clusters computed from openSMILE and MSR features show little overlap, while clusters of the logmelspec features show great overlap (Figure  11c ). Together with Task-2 results, this shift in the feature space is likely the main cause of the higher decrease observed in the openSMILE and MSR systems under different anonymization settings. Meanwhile, since all anonymization methods keep the speech content intact and change only the nonverbal attributes, a greater shift in feature space may indicate a stronger correlation with the para-linguistic aspect and less with the linguistic aspect. This echoes with previous studies which showed that openSMILE and MSR features are preferred over logmelspec features in characterizing emotional and unnatural speech  [87] ,  [88] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "F. Task-3: Improving Diagnostics Performance With Data Augmentation",
      "text": "Lastly, we investigate the impact of using anonymized external data for data augmentation and see its impact on the performance achieved with scenarios B and C. With scenario C, we chose sub-condition C1 and C3. To quantify the relative improvement, we used the within-dataset performance achieved in scenario A as the baseline and calculated the amount of performance increase seen (in percentage). The relative changes observed with the three diagnostics systems are reported in Table  VII . Here, we explore augmentation with two different datasets and with four different methods: original, McAdams, Ling-GAN, and Ling-Pros-GAN. As can be seen, when test data are anonymized using the McAdams coefficient (B1), the highest improvement is generally achieved when the diagnostic system is augmented with the original data. In turn, when the test data are anonymized using the GANbased method (B2), augmenting the set with GAN-anonymized data from another dataset leads to a higher increase. Similar results are shown in scenarios C1 and C2, where clean and GAN-anonymized augment data result in more significant improvements. While not the top-performer, the McAdams method is shown to be a reliable augmentation strategy, especially for the openSMILE features. Overall, these findings suggest that anonymization has the potential to be used as a data augmentation approach to improve COVID-19 diagnostics accuracy when tested on anonymized data.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "G. Limitations, Biases, And Future Work",
      "text": "The study's principal aim was to validate the effectiveness of anonymization methods within and across datasets in the context of assessing voice-based COVID-19 diagnostic accuracy. While we investigated three anonymization methods, other methods are emerging continuously (e.g.,  [89] ,  [90] ); thus, the findings reported herein should be validated with more recent methods. In the present study, the ASR anonymization method developed on English speech was applied to the multilingual CSS dataset. The finding that GAN-based anonymization had the lowest cross-dataset performance results may suggest challenges in applying this method in multilingual datasets and non-English speaking populations. In the future, multilingual GANs should be explored to avoid unfair outcomes  [91]  due to certain languages or cultural settings being excluded from the training and testing datasets. Moreover, while the injection of anonymized external data showed to be a useful data augmentation strategy, the final results were still at times lower than those achieved in the classical \"unprotected\" setting. This suggests that health-related information is being discarded during the anonymization process, thus future work could explore the development of diagnostic-aware anonymization methods that keep such discriminatory information intact.\n\nBeyond tackling these limitations mentioned above, future work into voice-based diagnostics should be mindful of potential biases during data collection that could lead to confounds for both the anonymization and diagnostic steps. These confounders, if not properly dealt with, can reinforce the systemic nature of biases, for instance in relation to gender and racioethnic groups, that already exist within the healthcare system, thus transferring them to automated diagnostic systems. While  [35]  already showed some impact of sampling rate on diagnostic accuracy, several other potential biases may exist at the methodological level. For example, sociodemographic biases may emerge if age is not taken into consideration, as cognitive limitations (e.g., difficulty in speech planning or lexical access) associated with aging could alter speech patterns that could affect overall diagnostic accuracy. Recent work has shown that socioeconomic status could serve as a bias in COVID-19 detection  [92] . For example, as data was collected from participants from home, those living in crowded conditions could have resulted in increased background noise levels that negatively affected anonymization and diagnostic efficacy. Moreover, disadvantaged populations have been shown to have more chronic respiratory diseases  [93]  and higher levels of mood disorders and psychological distress  [94] . As such, anonymization processes affecting para-linguistic features associated with depressed mood may disproportionately affect those with a low socioeconomic status. Addressing biases in automated voice anonymization and diagnosis systems is beyond the scope of this paper and is left for future studies.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we comprehensively evaluated the impact of three voice anonymization methods on the accuracy of five leading COVID-19 detection systems as well as the anonymization efficacy. All anonymization methods showed to degrade diagnostics accuracy, where the most severe degradation was seen with the systems that directly altered speaker embeddings. Our findings suggest that existing methods lack the capability of effectively preserving diagnostic information while obfuscating speaker identifiers. Lastly, we explored the use of anonymized external data as a data augmentation tool and promising results were obtained.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: depicts the diagram of an anonymized speech diag-",
      "page": 3
    },
    {
      "caption": "Figure 1: Block diagram of a speech-based diagnostics system with (protected)",
      "page": 3
    },
    {
      "caption": "Figure 2: The first system, abbreviated as ‘Ling-GAN’, was an off-the-",
      "page": 3
    },
    {
      "caption": "Figure 2: Diagram of the two GAN-based anonymizers implemented in this",
      "page": 4
    },
    {
      "caption": "Figure 3: Distribution of speaker embeddings (‘emd’) generated by Ling-Pros-",
      "page": 4
    },
    {
      "caption": "Figure 3: depicts the t-distributed stochastic neigh-",
      "page": 4
    },
    {
      "caption": "Figure 4: , for each speech recording, the speaker",
      "page": 5
    },
    {
      "caption": "Figure 4: Evaluation of the effectiveness of different voice anonymization",
      "page": 6
    },
    {
      "caption": "Figure 5: , we experimented with four versions",
      "page": 6
    },
    {
      "caption": "Figure 5: Data augmentation schemes in (top) scenario B (ignorant) and",
      "page": 7
    },
    {
      "caption": "Figure 6: As can be seen, near perfect anonymization",
      "page": 7
    },
    {
      "caption": "Figure 6: Cosine similarity between speech signals under different anonymiza-",
      "page": 8
    },
    {
      "caption": "Figure 7: As can be seen from the average AUC-ROC",
      "page": 8
    },
    {
      "caption": "Figure 7: Within-dataset performance under different anonymization scenarios. Error bars represent the 95% CIs. The line plot values correspond to the average",
      "page": 9
    },
    {
      "caption": "Figure 8: shows the cross-dataset performance under the",
      "page": 9
    },
    {
      "caption": "Figure 9: depicts the",
      "page": 9
    },
    {
      "caption": "Figure 8: Cross-dataset performance under different anonymization scenarios. Error bars represent the 95% CIs. The line plot values correspond to the average",
      "page": 10
    },
    {
      "caption": "Figure 9: Relative changes in the AUC-ROC under different anonymization",
      "page": 10
    },
    {
      "caption": "Figure 10: A comparison of the waveforms processed by the three anonymizers",
      "page": 11
    },
    {
      "caption": "Figure 10: ) for a direct comparison. As can be seen, those",
      "page": 11
    },
    {
      "caption": "Figure 11: t-SNE clusters of anonymized speech features for different feature",
      "page": 11
    },
    {
      "caption": "Figure 11: a and Figure 11b",
      "page": 11
    },
    {
      "caption": "Figure 11: c). Together with Task-2 results,",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Alexa, siri, cortana, and more: an introduction to voice assistants",
      "authors": [
        "M Hoy"
      ],
      "year": "2018",
      "venue": "Medical reference services quarterly"
    },
    {
      "citation_id": "2",
      "title": "An overview of speaker identification: Accuracy and robustness issues",
      "authors": [
        "R Togneri",
        "D Pullella"
      ],
      "year": "2011",
      "venue": "IEEE Circuits and Systems Magazine"
    },
    {
      "citation_id": "3",
      "title": "A literature review on COVID-19 disease diagnosis from respiratory sound data",
      "authors": [
        "K Lella",
        "A Pja"
      ],
      "year": "2021",
      "venue": "A literature review on COVID-19 disease diagnosis from respiratory sound data",
      "arxiv": "arXiv:2112.07670"
    },
    {
      "citation_id": "4",
      "title": "Assessment of chronic pulmonary disease patients using biomarkers from natural speech recorded by mobile devices",
      "authors": [
        "V Nathan",
        "K Vatanparvar",
        "M Rahman",
        "E Nemati",
        "J Kuang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 16th International Conference on Wearable and Implantable Body Sensor Networks (BSN)"
    },
    {
      "citation_id": "5",
      "title": "A systematic literature review of automatic alzheimer's disease detection from speech and language",
      "authors": [
        "U Petti",
        "S Baker",
        "A Korhonen"
      ],
      "year": "2020",
      "venue": "Journal of the American Medical Informatics Association"
    },
    {
      "citation_id": "6",
      "title": "Speech production",
      "authors": [
        "P Macneilage"
      ],
      "year": "1980",
      "venue": "Language and Speech"
    },
    {
      "citation_id": "7",
      "title": "Clinical features of COVID-19",
      "authors": [
        "P Vetter",
        "D Vu",
        "A L'huillier",
        "M Schibler",
        "L Kaiser",
        "F Jacquerioz"
      ],
      "year": "2020",
      "venue": "Clinical features of COVID-19"
    },
    {
      "citation_id": "8",
      "title": "A framework for biomarkers of COVID-19 based on coordination of speech-production subsystems",
      "authors": [
        "T Quatieri",
        "T Talkar",
        "J Palmer"
      ],
      "year": "2020",
      "venue": "IEEE Open J. Engineering in Medicine and Biology"
    },
    {
      "citation_id": "9",
      "title": "Neuromuscular presentations in patients with COVID-19",
      "authors": [
        "V Paliwal",
        "R Garg",
        "A Gupta",
        "N Tejan"
      ],
      "year": "2020",
      "venue": "Neurological Sciences"
    },
    {
      "citation_id": "10",
      "title": "Fusion of modulation spectral and spectral features with symptom metadata for improved speech-based COVID-19 detection",
      "authors": [
        "Y Zhu",
        "T Falk"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "The interspeech 2021 computational paralinguistics challenge: COVID-19 cough, COVID-19 speech, escalation & primates",
      "authors": [
        "B Schuller",
        "A Batliner",
        "C Bergler",
        "C Mascolo",
        "J Han",
        "I Lefter",
        "H Kaya",
        "S Amiriparian",
        "A Baird",
        "L Stappen"
      ],
      "year": "2021",
      "venue": "The interspeech 2021 computational paralinguistics challenge: COVID-19 cough, COVID-19 speech, escalation & primates",
      "arxiv": "arXiv:2102.13468"
    },
    {
      "citation_id": "12",
      "title": "The second dicova challenge: Dataset and performance analysis for diagnosis of COVID-19 using acoustics",
      "authors": [
        "N Sharma",
        "S Chetupalli",
        "D Bhattacharya",
        "D Dutta",
        "P Mote",
        "S Ganapathy"
      ],
      "year": "2022",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Deep learning towards mobile applications",
      "authors": [
        "J Wang",
        "B Cao",
        "P Yu",
        "L Sun",
        "W Bao",
        "X Zhu"
      ],
      "year": "2018",
      "venue": "2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)"
    },
    {
      "citation_id": "14",
      "title": "Fraudsters used ai to mimic ceo's voice in unusual cybercrime case",
      "authors": [
        "C Stupp"
      ],
      "year": "2019",
      "venue": "The Wall Street Journal"
    },
    {
      "citation_id": "15",
      "title": "The ai-based cyber threat landscape: A survey",
      "authors": [
        "N Kaloudi",
        "J Li"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "16",
      "title": "Weaponized ai for cyber attacks",
      "authors": [
        "M Yamin",
        "M Ullah",
        "H Ullah",
        "B Katt"
      ],
      "year": "2021",
      "venue": "Journal of Information Security and Applications"
    },
    {
      "citation_id": "17",
      "title": "Your echos are heard: Tracking, profiling, and ad targeting in the amazon smart speaker ecosystem",
      "authors": [
        "U Iqbal",
        "P Bahrami",
        "R Trimananda",
        "H Cui",
        "A Gamero-Garrido",
        "D Dubois",
        "D Choffnes",
        "A Markopoulou",
        "F Roesner",
        "Z Shafiq"
      ],
      "year": "2022",
      "venue": "Your echos are heard: Tracking, profiling, and ad targeting in the amazon smart speaker ecosystem",
      "arxiv": "arXiv:2204.10920"
    },
    {
      "citation_id": "18",
      "title": "Voice-based determination of physical and emotional characteristics of users",
      "authors": [
        "H Jin",
        "S Wang"
      ],
      "year": "2018",
      "venue": "Patent"
    },
    {
      "citation_id": "19",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif",
        "J Qadir",
        "A Qayyum",
        "M Usama",
        "S Younis"
      ],
      "year": "2020",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "20",
      "title": "Acoustic characteristics of parkinsonian speech: a potential biomarker of early disease progression and treatment",
      "authors": [
        "B Harel",
        "M Cannizzaro",
        "H Cohen",
        "N Reilly",
        "P Snyder"
      ],
      "year": "2004",
      "venue": "Journal of Neurolinguistics"
    },
    {
      "citation_id": "21",
      "title": "Automated assessment of psychiatric disorders using speech: A systematic review",
      "authors": [
        "D Low",
        "K Bentley",
        "S Ghosh"
      ],
      "year": "2020",
      "venue": "Laryngoscope investigative otolaryngology"
    },
    {
      "citation_id": "22",
      "title": "Incompatible: The gdpr in the age of big data",
      "authors": [
        "T Zarsky"
      ],
      "year": "2016",
      "venue": "Seton Hall L. Rev"
    },
    {
      "citation_id": "23",
      "title": "Citizens' data privacy in china: The state of the art of the personal information protection law (pipl)",
      "authors": [
        "I Calzada"
      ],
      "year": "2022",
      "venue": "Smart Cities"
    },
    {
      "citation_id": "24",
      "title": "The voiceprivacy 2020 challenge: Results and findings",
      "authors": [
        "N Tomashenko",
        "X Wang",
        "E Vincent",
        "J Patino",
        "B Srivastava",
        "P.-G Noé",
        "A Nautsch",
        "N Evans",
        "J Yamagishi",
        "B O'brien"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "25",
      "title": "The voiceprivacy 2022 challenge evaluation plan",
      "authors": [
        "N Tomashenko",
        "X Wang",
        "X Miao",
        "H Nourtel",
        "P Champion",
        "M Todisco",
        "E Vincent",
        "N Evans",
        "J Yamagishi",
        "J Bonastre"
      ],
      "year": "2022",
      "venue": "The voiceprivacy 2022 challenge evaluation plan",
      "arxiv": "arXiv:2203.12468"
    },
    {
      "citation_id": "26",
      "title": "Speaker anonymization using x-vector and neural waveform models",
      "authors": [
        "F Fang",
        "X Wang",
        "J Yamagishi",
        "I Echizen",
        "M Todisco",
        "N Evans",
        "J.-F Bonastre"
      ],
      "year": "2019",
      "venue": "Speaker anonymization using x-vector and neural waveform models",
      "arxiv": "arXiv:1905.13561"
    },
    {
      "citation_id": "27",
      "title": "Speaker anonymization with phonetic intermediate representations",
      "authors": [
        "S Meyer",
        "F Lux",
        "P Denisov",
        "J Koch",
        "P Tilli",
        "N Vu"
      ],
      "year": "2022",
      "venue": "Speaker anonymization with phonetic intermediate representations",
      "arxiv": "arXiv:2207.04834"
    },
    {
      "citation_id": "28",
      "title": "Evaluation of speaker anonymization on emotional speech",
      "authors": [
        "H Nourtel",
        "P Champion",
        "D Jouvet",
        "A Larcher",
        "M Tahon"
      ],
      "year": "2021",
      "venue": "SPSC 2021-1st ISCA Symposium on Security and Privacy in Speech Communication"
    },
    {
      "citation_id": "29",
      "title": "Sine-wave speech and privacy-preserving depression detection",
      "authors": [
        "S Dumpala",
        "R Uher",
        "S Matwin",
        "M Kiefte",
        "S Oore"
      ],
      "year": "2021",
      "venue": "Proc. SMM21, Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "30",
      "title": "Medical big data: promise and challenges",
      "authors": [
        "C Lee",
        "H.-J Yoon"
      ],
      "year": "2017",
      "venue": "Kidney research and clinical practice"
    },
    {
      "citation_id": "31",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. ACM international conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Detecting alzheimer's disease using gated convolutional neural network from audio data",
      "authors": [
        "T Warnita",
        "N Inoue",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Detecting alzheimer's disease using gated convolutional neural network from audio data",
      "arxiv": "arXiv:1803.11344"
    },
    {
      "citation_id": "33",
      "title": "Detection of copd exacerbation from speech: comparison of acoustic features and deep learning based speech breathing models",
      "authors": [
        "V Nallanthighal",
        "A Härmä",
        "H Strik"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "An early study on intelligent analysis of speech under COVID-19: Severity, sleep quality, fatigue, and anxiety",
      "authors": [
        "J Han",
        "K Qian",
        "M Song"
      ],
      "year": "2020",
      "venue": "An early study on intelligent analysis of speech under COVID-19: Severity, sleep quality, fatigue, and anxiety",
      "arxiv": "arXiv:2005.00096"
    },
    {
      "citation_id": "35",
      "title": "Sounds of COVID-19: exploring realistic performance of audio-based digital testing",
      "authors": [
        "J Han",
        "T Xia",
        "D Spathis",
        "E Bondareva",
        "C Brown",
        "J Chauhan",
        "T Dang",
        "A Grammenos",
        "A Hasthanasombat",
        "A Floto"
      ],
      "year": "2022",
      "venue": "NPJ digital medicine"
    },
    {
      "citation_id": "36",
      "title": "A summary of the compare COVID-19 challenges",
      "authors": [
        "H Coppock",
        "A Akman",
        "C Bergler",
        "M Gerczuk",
        "C Brown",
        "J Chauhan",
        "A Grammenos",
        "A Hasthanasombat",
        "D Spathis",
        "T Xia"
      ],
      "year": "2022",
      "venue": "A summary of the compare COVID-19 challenges",
      "arxiv": "arXiv:2202.08981"
    },
    {
      "citation_id": "37",
      "title": "Modulation spectral features for robust far-field speaker identification",
      "authors": [
        "T Falk",
        "W.-Y Chan"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Feature pooling of modulation spectrum features for improved speech emotion recognition in the wild",
      "authors": [
        "A Avila",
        "Z Akhtar",
        "J Santos",
        "D O'shaughnessy",
        "T Falk"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Modulation spectral signal representation for quality measurement and enhancement of wearable device data: A technical note",
      "authors": [
        "A Tiwari",
        "R Cassani",
        "S Kshirsagar",
        "D Tobon",
        "Y Zhu",
        "T Falk"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "40",
      "title": "Spectro-temporal analysis of auscultatory sounds",
      "authors": [
        "T Falk",
        "W.-Y Chan",
        "E Sejdic",
        "T Chau"
      ],
      "year": "2010",
      "venue": "New Developments in Biomedical Engineering"
    },
    {
      "citation_id": "41",
      "title": "Evaluating the COVID-19 identification resnet (cider) on the interspeech covid-19 from audio challenges",
      "authors": [
        "A Akman",
        "H Coppock",
        "A Gaskell",
        "P Tzirakis",
        "L Jones",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Evaluating the COVID-19 identification resnet (cider) on the interspeech covid-19 from audio challenges",
      "arxiv": "arXiv:2107.14549"
    },
    {
      "citation_id": "42",
      "title": "Audio, speech, language, & signal processing for COVID-19: A comprehensive overview",
      "authors": [
        "G Deshpande",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Audio, speech, language, & signal processing for COVID-19: A comprehensive overview",
      "arxiv": "arXiv:2011.14445"
    },
    {
      "citation_id": "43",
      "title": "COVID-19 and computer audition: An overview on what speech & sound analysis could contribute in the sars-cov-2 corona crisis",
      "authors": [
        "B Schuller",
        "D Schuller",
        "K Qian",
        "J Liu",
        "H Zheng",
        "X Li"
      ],
      "year": "2021",
      "venue": "Frontiers in digital health"
    },
    {
      "citation_id": "44",
      "title": "How generalizable and interpretable are speech-based COVID-19 detection systems?: A comparative analysis and new system proposal",
      "authors": [
        "Y Mariakakis",
        "E Lara",
        "T Falk"
      ],
      "year": "2022",
      "venue": "2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)"
    },
    {
      "citation_id": "45",
      "title": "Voice transformation: a survey",
      "authors": [
        "Y Stylianou"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Evaluating voice conversion-based privacy protection against informed attackers",
      "authors": [
        "B Srivastava",
        "N Vauquier",
        "M Sahidullah",
        "A Bellet",
        "M Tommasi",
        "E Vincent"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "An overview of voice conversion systems",
      "authors": [
        "S Mohammadi",
        "A Kain"
      ],
      "year": "2017",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "48",
      "title": "Spectral fusion, spectral parsing and the formation of auditory images",
      "authors": [
        "S Mcadams"
      ],
      "year": "1984",
      "venue": "Spectral fusion, spectral parsing and the formation of auditory images"
    },
    {
      "citation_id": "49",
      "title": "Voicemask: Anonymize and sanitize voice input on mobile devices",
      "authors": [
        "J Qian",
        "H Du",
        "J Hou",
        "L Chen",
        "T Jung",
        "X.-Y Li",
        "Y Wang",
        "Y Deng"
      ],
      "year": "2017",
      "venue": "Voicemask: Anonymize and sanitize voice input on mobile devices",
      "arxiv": "arXiv:1711.11460"
    },
    {
      "citation_id": "50",
      "title": "Xvectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "51",
      "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "arxiv": "arXiv:2005.07143"
    },
    {
      "citation_id": "52",
      "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong",
        "J Kim",
        "J Bae"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "53",
      "title": "Design choices for x-vector based speaker anonymization",
      "authors": [
        "B Srivastava",
        "N Tomashenko",
        "X Wang",
        "E Vincent",
        "J Yamagishi",
        "M Maouche",
        "A Bellet",
        "M Tommasi"
      ],
      "year": "2020",
      "venue": "Design choices for x-vector based speaker anonymization",
      "arxiv": "arXiv:2005.08601"
    },
    {
      "citation_id": "54",
      "title": "Speaker Anonymization with Phonetic Intermediate Representations",
      "authors": [
        "S Meyer",
        "F Lux",
        "P Denisov",
        "J Koch",
        "P Tilli",
        "N Vu"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "55",
      "title": "COVID-19 sounds: a large-scale audio dataset for digital respiratory screening",
      "authors": [
        "T Xia",
        "D Spathis",
        "J Ch",
        "A Grammenos",
        "J Han",
        "A Hasthanasombat",
        "E Bondareva",
        "T Dang",
        "A Floto",
        "P Cicuta"
      ],
      "venue": "Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "56",
      "title": "Principal component analysis",
      "authors": [
        "S Wold",
        "K Esbensen",
        "P Geladi"
      ],
      "year": "1987",
      "venue": "Chemometrics and intelligent laboratory systems"
    },
    {
      "citation_id": "57",
      "title": "An improved non-intrusive intelligibility metric for noisy and reverberant speech",
      "authors": [
        "J Santos",
        "M Senoussaoui",
        "T Falk"
      ],
      "year": "2014",
      "venue": "2014 14th International Workshop on Acoustic Signal Enhancement (IWAENC)"
    },
    {
      "citation_id": "58",
      "title": "Speaker anonymisation using the mcadams coefficient",
      "authors": [
        "J Patino",
        "N Tomashenko",
        "M Todisco",
        "A Nautsch",
        "N Evans"
      ],
      "venue": "Interspeech 2021. ISCA, 2021"
    },
    {
      "citation_id": "59",
      "title": "Linear predictive coding",
      "authors": [
        "D Shaughnessy"
      ],
      "year": "1988",
      "venue": "IEEE potentials"
    },
    {
      "citation_id": "60",
      "title": "Hybrid ctc/attention architecture for end-to-end speech recognition",
      "authors": [
        "S Watanabe",
        "T Hori",
        "S Kim",
        "J Hershey",
        "T Hayashi"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "61",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition",
      "arxiv": "arXiv:2005.08100"
    },
    {
      "citation_id": "62",
      "title": "Speechbrain: A general-purpose speech toolkit",
      "authors": [
        "M Ravanelli",
        "T Parcollet",
        "P Plantinga",
        "A Rouhe",
        "S Cornell",
        "L Lugosch",
        "C Subakan",
        "N Dawalatabad",
        "A Heba",
        "J Zhong"
      ],
      "year": "2021",
      "venue": "Speechbrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "63",
      "title": "Wasserstein gan with quadratic transport cost",
      "authors": [
        "H Liu",
        "X Gu",
        "D Samaras"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "64",
      "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
      "authors": [
        "Y Ren",
        "C Hu",
        "X Tan",
        "T Qin",
        "S Zhao",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2020",
      "venue": "Fastspeech 2: Fast and high-quality end-to-end text to speech",
      "arxiv": "arXiv:2006.04558"
    },
    {
      "citation_id": "65",
      "title": "Prosody is not identity: A speaker anonymization approach using prosody cloning",
      "authors": [
        "S Meyer",
        "F Lux",
        "J Koch",
        "P Denisov",
        "P Tilli",
        "N Vu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "66",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R.-S Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "Y Jia",
        "F Ren",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "67",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "68",
      "title": "The coughvid crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms",
      "authors": [
        "L Orlandic",
        "T Teijeiro",
        "D Atienza"
      ],
      "year": "2021",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "69",
      "title": "Iatos: Ai-powered pre-screening tool for COVID-19 from cough audio samples",
      "authors": [
        "D Pizzo",
        "S Esteban"
      ],
      "year": "2021",
      "venue": "Iatos: Ai-powered pre-screening tool for COVID-19 from cough audio samples",
      "arxiv": "arXiv:2104.13247"
    },
    {
      "citation_id": "70",
      "title": "Virufy: Global applicability of crowdsourced and clinical datasets for ai detection of COVID-19 from cough",
      "authors": [
        "G Chaudhari",
        "X Jiang",
        "A Fakhry",
        "A Han",
        "J Xiao",
        "S Shen",
        "A Khanzada"
      ],
      "year": "2020",
      "venue": "Virufy: Global applicability of crowdsourced and clinical datasets for ai detection of COVID-19 from cough",
      "arxiv": "arXiv:2011.13320"
    },
    {
      "citation_id": "71",
      "title": "Novel coronavirus cough database: Nococoda",
      "authors": [
        "M Cohen-Mcfarlane",
        "R Goubran",
        "F Knoefel"
      ],
      "year": "2020",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "72",
      "title": "Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and ct scans",
      "authors": [
        "M Roberts",
        "D Driggs",
        "M Thorpe",
        "J Gilbey",
        "M Yeung",
        "S Ursprung",
        "A Aviles-Rivero",
        "C Etmann",
        "C Mccague",
        "L Beer"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "73",
      "title": "Improving adversarial robustness via unlabeled out-of-domain data",
      "authors": [
        "Z Deng",
        "L Zhang",
        "A Ghorbani",
        "J Zou"
      ],
      "year": "2021",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "74",
      "title": "In-domain and out-of-domain data augmentation to improve children's speaker verification system in limited data scenario",
      "authors": [
        "S Shahnawazuddin",
        "W Ahmad",
        "N Adiga",
        "A Kumar"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "75",
      "title": "Bootstrap confidence intervals for the sensitivity of a quantitative diagnostic test",
      "authors": [
        "R Platt",
        "J Hanley",
        "H Yang"
      ],
      "year": "2000",
      "venue": "Statistics in medicine"
    },
    {
      "citation_id": "76",
      "title": "Probing the information encoded in x-vectors",
      "authors": [
        "D Raj",
        "D Snyder",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "77",
      "title": "Measuring voice quality parameters after speaker pseudonymization",
      "authors": [
        "R Van Son"
      ],
      "year": "2021",
      "venue": "Measuring voice quality parameters after speaker pseudonymization"
    },
    {
      "citation_id": "78",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "79",
      "title": "Using x-vectors to automatically detect parkinson's disease from speech",
      "authors": [
        "L Moro-Velazquez",
        "J Villalba",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "80",
      "title": "Using state of the art speaker recognition and natural language processing technologies to detect alzheimer's disease and assess its severity",
      "authors": [
        "R Pappagari",
        "J Cho",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "81",
      "title": "End-to-end convolutional neural network enables COVID-19 detection from breath and cough audio: a pilot study",
      "authors": [
        "H Coppock",
        "A Gaskell",
        "P Tzirakis",
        "A Baird",
        "L Jones",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "BMJ innovations"
    },
    {
      "citation_id": "82",
      "title": "An automatic diagnosis and assessment of dysarthric speech using speech disorder specific prosodic features",
      "authors": [
        "G Vyas",
        "M Dutta",
        "J Prinosil",
        "P Harár"
      ],
      "year": "2016",
      "venue": "2016 39th International Conference on Telecommunications and Signal Processing"
    },
    {
      "citation_id": "83",
      "title": "Discriminative prosodic features to assess the dysarthria severity levels",
      "authors": [
        "K Kadi",
        "S Selouani",
        "B Boudraa",
        "M Boudraa"
      ],
      "year": "2013",
      "venue": "Proceedings of the World Congress on Engineering"
    },
    {
      "citation_id": "84",
      "title": "Acoustic features to characterize sentence accent production in dysarthric speech",
      "authors": [
        "V Ramos",
        "H Hernandez-Diaz",
        "-D Huici",
        "H Martens",
        "G Van Nuffelen",
        "M Bodt"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "85",
      "title": "The impact of parkinson's disease on breath pauses and their relationship to speech impairment: A longitudinal study",
      "authors": [
        "M Darling-White",
        "J Huber"
      ],
      "year": "2020",
      "venue": "American Journal of Speech-Language Pathology"
    },
    {
      "citation_id": "86",
      "title": "What speech can tell us: A systematic review of dysarthria characteristics in multiple sclerosis",
      "authors": [
        "G Noffs",
        "T Perera",
        "S Kolbe",
        "C Shanahan",
        "F Boonstra",
        "A Evans",
        "H Butzkueven",
        "A Van Der Walt",
        "A Vogel"
      ],
      "year": "2018",
      "venue": "Autoimmunity reviews"
    },
    {
      "citation_id": "87",
      "title": "Characterization of atypical vocal source excitation, temporal dynamics and prosody for objective measurement of dysarthric word intelligibility",
      "authors": [
        "T Falk",
        "W.-Y Chan",
        "F Shein"
      ],
      "year": "2012",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "88",
      "title": "Real-time speech and music classification by large audio feature space extraction",
      "authors": [
        "F Eyben"
      ],
      "year": "2015",
      "venue": "Real-time speech and music classification by large audio feature space extraction"
    },
    {
      "citation_id": "89",
      "title": "Vcloak: Intelligibility-, naturalness-& timbre-preserving real-time voice anonymization",
      "authors": [
        "J Deng",
        "F Teng",
        "Y Chen",
        "X Chen",
        "Z Wang",
        "W Xu"
      ],
      "year": "2022",
      "venue": "Vcloak: Intelligibility-, naturalness-& timbre-preserving real-time voice anonymization",
      "arxiv": "arXiv:2210.15140"
    },
    {
      "citation_id": "90",
      "title": "Speaker anonymization using orthogonal householder neural network",
      "authors": [
        "X Miao",
        "X Wang",
        "E Cooper",
        "J Yamagishi",
        "N Tomashenko"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "91",
      "title": "The ethics of ai in health care: a mapping review",
      "authors": [
        "J Morley",
        "C Machado",
        "C Burr",
        "J Cowls",
        "I Joshi",
        "M Taddeo",
        "L Floridi"
      ],
      "year": "2020",
      "venue": "Social Science & Medicine"
    },
    {
      "citation_id": "92",
      "title": "Investigating biases in COVID-19 diagnostic systems processed with automated speech anonymization algorithms",
      "authors": [
        "Y Zhu",
        "M Imoussaine",
        "C Côté-Lussier",
        "T Falk"
      ],
      "year": "2023",
      "venue": "Proc. 3rd Symposium on Security and Privacy in Speech Communication"
    },
    {
      "citation_id": "93",
      "title": "Defining and targeting health disparities in chronic obstructive pulmonary disease",
      "authors": [
        "R Pleasants",
        "I Riley",
        "D Mannino"
      ],
      "year": "2016",
      "venue": "International journal of chronic obstructive pulmonary disease"
    },
    {
      "citation_id": "94",
      "title": "Mental illnesses-understanding, prediction and control",
      "authors": [
        "A Drapeau",
        "A Marchand",
        "D Beaulieu-Prevost"
      ],
      "year": "2012",
      "venue": "Mental illnesses-understanding, prediction and control"
    }
  ]
}