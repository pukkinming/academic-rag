{
  "paper_id": "2503.05858v3",
  "title": "Bimodal Connection Attention Fusion For Speech Emotion Recognition",
  "published": "2025-03-08T10:20:57Z",
  "authors": [
    "Jiachen Luo",
    "Huy Phan",
    "Lin Wang",
    "Joshua D. Reiss"
  ],
  "keywords": [
    "deep learning",
    "conversational emotion recognition",
    "multi-modal fusion",
    "modality connection",
    "modality interaction",
    "attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-modal emotion recognition is challenging due to the difficulty of extracting features that capture subtle emotional differences. Understanding multi-modal interactions and connections is key to building effective bimodal speech emotion recognition systems. In this work, we propose Bimodal Connection Attention Fusion (BCAF) method, which includes three main modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network uses an encoder-decoder architecture to model modality connections between audio and text while leveraging modality-specific features. The bimodal attention network enhances semantic complementation and exploits intraand inter-modal interactions. The correlative attention network reduces cross-modal noise and captures correlations between audio and text. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed BCAF method outperforms existing state-of-the-art baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "T HE proliferation of mobile internet and smartphones has led to the widespread use of social networking platforms where users create and share content in various modalities, such as audio, text, and video. Extracting and analyzing emotions from this multimodal content has extensive applications in human-computer interaction, surveillance, robotics, and gaming  [1] ,  [2] ,  [3] . However, effectively integrating multiple modalities remains a significant challenge in emotion recognition research.\n\nPrevious multi-modal emotion recognition methods have achieved good performance  [4] ; however, key challenges remain in multi-modal emotion recognition. Different modalities require independent preprocessing and feature extraction designs due to their heterogeneous nature  [5] ,  [6] ,  [7] . To develop a model that is both applicable and generalizable across individual modalities and fusion models, it is essential to learn modality interactions and connections for learning discriminative emotional content.\n\nIn multi-modal learning, modality connection in multimodal learning refers to the extent to which information is shared across modalities, shaping a unified representation  [8] . Unlike correlation, which quantifies modality dependence, modality connection captures the semantic alignment between modalities. For example, a higher vocal pitch may correlate with excitement, but modality connection determines if the audio tone, facial expressions, and text consistently reinforce that excitement. A strong modality connection ensures meaningful interaction among modalities, whereas a weak connection may lead to ambiguity. Leveraging modality connections enables a more nuanced understanding of emotions, improving the robustness and accuracy of emotion recognition systems.\n\nOn the other hand, modality interaction refers to the relationships and dependencies between modalities, which can vary from strong correlations to weak or even conflicting signals  [9] . While modality connection emphasizes shared content, modality interaction captures how different modalities influence one another. For example, imagine a meeting scenario where a participant says, \"That's great\" in a flat, monotone voice while avoiding eye contact and crossing their arms. The verbal content suggests positivity, but the tone and body language convey disengagement or sarcasm. Such conflicting signals highlight the complexity of multi-modal interactions. Accurately interpreting these interactions requires understanding the context and modeling how modalities influence one another.\n\nTwo core types of interactions are frequently encountered in multi-modal learning: intra-modal and inter-modal interactions  [10] ,  [11] . Intra-modal interactions measure relationships within the same modality, while inter-modal interactions capture relationships between different modalities. Understanding and learning these interactions are essential for building an effective multi-modal emotion recognition system that achieves three key goals: multi-modal integration, robustness, and contextual awareness.\n\nMany multi-modal emotion recognition simply focused on exploring interactions across different modalities while often overlooking the importance of modality connections. Existing approaches simply concatenated data for joint fusion and aggregates decisions from various modalities through weighted averaging  [5] ,  [12] ,  [13] , neglecting intra-modal interactions. Recently attention mechanisms have gained attraction due to their effectiveness in modeling modality interactions  [14] ,  [15] ,  [16] . However, such approaches often lack a deeper understanding of modality interactions and connections. This limitation can lead to several issues, including suboptimal connection strategies, loss of critical cross-modal information, and reduced robustness when handling conflicting or incomplete data from specific modalities. Addressing these challenges is crucial for enhancing the performance and generalizability of multi-modal emotion recognition systems in real-world applications. Motivated by the above observations (Fig.  1 ), we propose a Bimodal Connection Attention Fusion (BCAF) framework for bimodal emotion recognition, which consists of three key modules: the uni-modal representation module, the connection attention fusion module, and the classification module (Fig.  2 ). The contribution is summarized as follows. First, we propose the interactive connection network to capture modalityspecific features and analyze multi-modal connection between audio and text. Second, We investigate the bimodal attention network, which assigns dynamic weights to comprehensively learn intra-and inter-modal interactions between audio and text. Finally, We design the correlative attention network to effectively filter out incorrect cross-modal relationships and enhance learning of both intra-and inter-modal interactions between audio and text. Experimental results validate the effectiveness of the proposed method on public datasets.\n\nThe remainder of this paper is organized as follows: Section II presents a brief literature review. Section III describes our method in detail. Section IV outlines the experiments conducted. Section V discusses the results. Finally, Section VI provides conclusions based on this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion recognition in conversations has found widespread applications across various fields  [4] . Humans convey emotions through multiple modalities, including speech, facial expressions, and body postures  [12] ,  [17] ,  [18] . Among these, speech is a crucial modality, carrying emotional cues through both paralinguistic features and linguistic content. Since different modalities provide complementary information, relying on a single modality is often insufficient for accurate emotion recognition  [5] ,  [19] . Therefore, combining information from multiple modalities enhances the ability to discern emotions, as each modality can complement or augment others, providing richer emotion-relevant information. Consequently, multimodal approaches generally yield superior results compared to uni-modal methods, leading to substantial efforts in developing and exploring multi-modal fusion techniques for more accurate emotion recognition in conversations.\n\nMulti-modal fusion methods are broadly categorized into early fusion, late fusion, and hybrid fusion  [1] ,  [2] ,  [4] ,  [20] . Early fusion combines features from different modalities at the input level but often overlooks complex inter-modal dependencies  [21] ,  [22] . Late fusion integrates decision-level outputs from unimodal classifiers  [23] ,  [24] , simplifying the process but limiting cross-modal interactions. Hybrid fusion addresses these limitations by combining intermediate representations of multiple modalities  [25] ,  [26] .\n\nRecent multi-modal emotion recognition approaches employ advanced fusion strategies to capture intra-and inter-modal interactions. Transformer-based models  [27]  and gated recurrent units  [28]  have been explored for cross-modal fusion. Multi-head attention mechanisms, as used in M2FNet  [14] , effectively learn intra-and inter-modal relationships. Similarly, HCAM  [29]  leverages recurrent and co-attention networks for improved fusion. CFN-ESA  [30]  introduces emotion-shift awareness in cross-modal fusion, while TelME  [31]  enhances non-verbal modalities through knowledge transfer. Mamba  [32]  employs probability-guided multi-modal fusion to maintain consistency across modalities, and AGF-IB  [33]  uses contrastive learning for capturing inter-class and intra-class semantic relationships.\n\nDespite these advances, challenges remain. Many methods underutilize audio representations, often prioritizing text due to its rich contextual information. Additionally, existing approaches struggle to fully capture cross-modal dependencies across different levels, limiting their ability to model longterm emotional context. To address these issues, we propose the BCAF method, which enhances modality interactions for bimodal speech emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "Our proposed BCAF method simultaneously models modality connections and interactions for bimodal speech recognition system. Fig.  1  illustrates the architecture of BCAF, which consists of the uni-modal representation module, the connection attention fusion module, and the classification module. The core connection attention fusion module comprises of the the interactive connection network, the bimodal attention network and the correlative attention network (see Figs.  3 4 5 ).\n\nA. Uni-modal Representation Module 1) Acoustic Encoder: We use the large wav2vec model as the audio modality encoder to obtain a 1024-dimensional utterance-level audio representation from raw audio signals  [34] . The large wav2vec model is an advanced self-supervised learning framework designed for speech representation learning. It consists of three key components: a feature encoder, a Transformer-based contextual representation module, and a quantization module. In total, 1024-dimensional utterancelevel acoustic features were extracted (H a ).\n\n2) Textual Encoder: We use the RoBERTa model as the text modality encoder to extract a 1024-dimensional utterancelevel text representation from raw text  [35] . RoBERTa takes the utterance transcript as input and generates rich contextual representations from the final four layers. This process produces four 1024-dimensional vectors for each token in the input. We then average these vectors to obtain a contextual utterance feature vector with a dimension of 1024 (H l ).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Connection Attention Fusion Module",
      "text": "We propose the connection attention fusion module to learn modality connections and interactions between audio and text for bimodal speech emotion recognition. The module comprises three main components: the interactive connection network, the bimodal attention network, and the correlative attention network (see Figs.  3 4 5 ). Detailed explanations of these components are provided in the following subsections.\n\n1) Interactive Connection Network: The interactive connection network uses an encoder-decoder architecture to learn modality connections between audio and text (see Fig.  3 ). Both the encoder and decoder consist of three fully connected layers, with each layer applying a linear transformation followed by a non-linear ReLU activation function. In the encoder, the fully connected layers progressively reduce the dimensionality of the input uni-modal representation H m , extracting important features and compressing them into a fixed-size latent vector e m . This process of hierarchical dimensionality reduction enables the encoder to capture the essential characteristics of the input modality while filtering out irrelevant or redundant information.\n\nThe decoder, which also consists of three fully connected layers, reverses this process by gradually increasing the dimensionality of the latent vector e m . It reconstructs the original uni-modal representation d m using the latent features as a guide. Each layer in the decoder applies a linear transformation and ReLU activation function to ensure that the reconstructed output closely resembles the original input. This encoderdecoder architecture effectively enables feature compression and reconstruction, facilitating the robust learning of modality connections.\n\nThis symmetric encoder-decoder architecture facilitates efficient feature extraction, compression, and reconstruction. By designing the encoder to focus on compact and meaningful representations and the decoder to restore the input features accurately, the network ensures that modality-specific information is preserved while enabling cross-modality learning.\n\nThe interactive connection network adopts an architecture comprising stacked fully connected layers followed by a dropout layer in both the encoder and decoder (see Fig.  3 ). For each modality m ∈ {a, l}, a simple encoder-decoder is formulated as:\n\nwhere E m (•) denotes the encoder function for modality m, with θ e m as its trainable parameters, and D m (•) represents the  The objective function models the learning problem using the self-supervised spirit. We design the connection loss function to maximize modality connections between audio and text.\n\nIt is defined as follows:\n\nwhere ∥ • ∥ 2 F is the squared Frobenius norm, which calculates the sum of squared elements in a matrix. I e and I l represent the identity matrix, and µ is a non-negative hyperparameter that controls the balance between terms.\n\nThe first term of the objective function, ∥H a -\n\nF , measures the reconstruction error between the original input representations and their reconstructed outputs for both the audio (H a , d a ) and text (H l , d l ) modalities. By minimizing this term, the model ensures that the decoder can accurately reconstruct the original input representations, preserving the modality-specific information unique to each modality. This encourages the encoder-decoder mechanism to retain the essential features of each modality while discarding irrelevant or redundant information. As a result, the model achieves high-quality reconstruction for both modalities, which is critical for ensuring robust performance in downstream tasks.\n\nThe second term, ∥I e -e ⊤ a e l ∥ 2\n\nF , focuses on learning meaningful connectionsbetween audio and text. The first component, ∥I e -e ⊤ a e l ∥ 2 F , aligns the latent representations (e a , e l ) in the feature space, ensuring strong modality connections during the encoding process. The second component,\n\n, maintains these connections in the reconstructed space by aligning the outputs (d a , d l ). Together, these terms promote consistency between the latent and reconstructed spaces, enabling the model to capture robust cross-modal rela-tionships. This ensures that the learned modality connections are both meaningful and preserved across all stages of the network.\n\n2) Bimodal Attention Network: The bimodal attention network introduces self-attention and cross-attention mechanisms to learn intra-and inter-modal interactions between audio and text. Fig.  5  illustrates the architecture of the bimodal attention network. The input to this network consists of queries, keys, and values. The dot product of the query and each key is computed, and a softmax function is applied to generate weights for the values  [36] . The bimodal attention network comprises stacked self-attention and cross-attention layers, along with feed-forward layers.\n\nThe core idea of this module is to learn intra-and intermodal interactions between audio and text, then propagate information from both modality-specific patterns and modality associations based on the attention weights. Technically, the self-attention layer aims to learn intra-modal interactions within each modality, such as audio or text. The query, key, and value are derived from the same modality. Given weight matrices W Q m , W K m , and W V m , the modality representation H m is projected into the query matrix (Q m ), key matrix (K m ), and value matrix (V m ) through linear projections without bias. The self-attention representation can then be summarized as follows:\n\nwhere ζH m represents the self-attention representation with a dimensionality of 1024. d k represents the dimension of the key vector.\n\nTo further enhance the representation capacity, the selfattention representation is passed through a LayerNorm layer followed by an AddNorm layer to obtain the enhanced selfattention representation.\n\nwhere h s a and h s l represent the enhanced self-attention representations for audio and text, respectively. The dimensions of h s a and h s l are both 1024. Parallel to the self-attention layer, the cross-attention layer captures inter-modal interactions between audio and text. It learns associations between the two modalities and propagates information from one modality to the other based on these associations. Specifically, the cross-attention mechanism follows a similar principle to the self-attention mechanism, with the key difference being that the query, key, and value are derived from different modalities. The enhanced crossattention representation is summarized as follows:\n\nwhere ζH a-l and ζH l-a represent the propagated information from audio to text and from text to audio, respectively, both with dimensions of 1024. The variables h c a and h c l denote the enhanced cross-attention representations for audio and text, respectively, and their dimensions are also both 1024. d k represents the dimension of the key vector.\n\n3) Correlative Attention Network: The correlative attention network is designed to enhance sentiment analysis by explicitly modeling the relationships between uni-modal and bimodal representations. It takes as input the latent uni-modal representations from the interactive correlation network and correlates them with the bimodal representation to capture both intra-modal and inter-modal dependencies. This network is crucial for bridging the gap between individual modality features and their combined representation, ensuring that the interactions between modalities are accurately captured.\n\nThe correlative attention network comprises two submodules: the joint attention network and the bimodal correlation evaluation network (see Fig.  5 ). The joint attention network integrates the uni-modal latent features from the audio and text modalities to produce a comprehensive bimodal representation, allowing the model to focus on the most salient features from each modality. Meanwhile, the bimodal correlation evaluation network assesses the correlations between these modalities by quantifying their interdependencies, offering a deeper understanding of how features from one modality influence those of the other.\n\nBy incorporating the correlative attention network, the model achieves a more robust and nuanced representation of the input data, improving its ability to analyze sentiment effectively. This approach ensures that the unique contributions of each modality are not only preserved but also synergistically leveraged to enhance overall performance.\n\nJoint Attention Network: The joint attention network aims to enhance the cross-modal relationship between audio and text by reducing noise and highlighting meaningful interactions. This is achieved using a pair of softmax functions, which help focus on the most relevant features from both modalities while suppressing less informative or noisy elements. The network effectively refines the cross-modal representation, enabling the model to better capture salient patterns for downstream tasks such as sentiment analysis.\n\nThe attention mechanism operates by mapping query, key, and value vectors to outputs. Specifically, it computes attention scores using query and key vectors and applies these scores to calculate a weighted sum of the value vectors.   projects these representations into query, key, and value vectors (Q j a , Q j l , K j a , K j l , V j a , V j l ) as follows:\n\nThe joint attention network computes the joint bimodal representation using the following equation:\n\nwhere h * denotes the joint bimodal representation with a dimension of 1024. λ is a learnable scalar that adjusts the weight of the second term, which represents cross-modal attention. V j represents the average of the value vectors V j a and V j l , encapsulating information from both modalities. d k represents the dimension of the key vector.\n\nThe use of a pair of softmax functions serves a dual purpose. The first softmax term emphasizes intra-modal relationships by combining attention scores from the query and key vectors within each modality. This term ensures that strong intramodal signals are preserved. The second softmax term focuses on cross-modal relationships, computing the attention scores between the query of one modality and the key of the other. By subtracting the cross-modal scores (weighted by λ), the network suppresses noisy or irrelevant interactions while retaining meaningful cross-modal dependencies. This design improves the robustness of the joint attention mechanism and enhances the overall representation quality for tasks requiring fine-grained cross-modal understanding.\n\nBimodal Correlation Evaluation: The latent uni-modal representations from the interactive correlation network (e a for audio and e l for text), along with the joint bimodal representation from the joint attention network (h * ), are input into the bimodal correlation evaluation network to assess the correlations between audio and text. Inspired by CLIP  [37] , cosine similarity is employed to measure the correlations between different modalities, resulting in the pairwise crossmodal correlation coefficients, cor a-m and cor l-m (see Fig.  4 ). The joint multi-modal representation h * is passed through a linear layer, reducing its dimension to 512, resulting in h b , which serves as the fused representation used for bimodal correlation evalution.\n\nwhere ⟨•⟩ denotes cosine similarity, and cor a-b and cor l-b represent the correlation coefficients for the audio-bimodal and text-bimodal pairs, respectively. The pairwise cross-modal correlation coefficients are subsequently integrated into the enhanced self-attention representations (h s a for audio and h s l for text) to produce correlative representations. This process ensures that the resulting representations fully capture inter-modal interactions by utilizing the correlation information to strengthen the interplay between modalities. Through this integration, the model becomes better equipped to align and fuse audio and text features effectively, thereby facilitating a more comprehensive understanding of cross-modal relationships, as described below:\n\nwhere h * a and h * l denote the correlative uni-modal representations, enhanced by their respective modality correlation coefficients. These coefficients represent the weighted importance of one modality's features in the context of the other, effectively capturing the interdependence between modalities. The dimensions of both h * a and h * l are 512. Finally, h c a , h c l , h * a , and h * l are concatenated to form the aggregated bimodal representation, which is then passed through four fully connected layers for classification.\n\nwhere h * m represents the aggregated bimodal representation with a dimension of 3072.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Classification",
      "text": "The overall optimization objective consists of the connection loss L c , the audio loss L a , the text loss L l , and the bimodal loss L m . Specifically, the audio loss, the text loss, and the bimodal loss are processed independently to generate predictions, which are then combined to make a final decision.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Training",
      "text": "For the classification task, the audio loss, the text loss, and the bimodal loss are defined as the standard cross-entropy loss. Finally, the overall loss function is expressed as a linear combination of L c , L a , L l , and L m :\n\nwhere α, and β are weighting factors that balance the contribution of each loss term.\n\n• The audio loss L a is used to capture distinctive audio information for emotion prediction: where the parameters W a e and b a e are learnable weights and biases. c represents the emotion categories, and ŷc a and y c a denote the predicted and true labels, respectively.\n\n• The text loss L l is used to capture distinctive textual information for emotion prediction:\n\nwhere the parameters W l e and b l e are learnable weights and biases. c represents the emotion categories, and ŷc l and y c l denote the predicted and true labels, respectively. • The bimodal loss L m is used to capture interactions between modalities for emotion prediction:\n\nwhere the parameters W m e and b m e are learnable weights and biases. c represents the emotion categories, and ŷc m and y c m denote the predicted and true labels, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments A. Database And Metrics",
      "text": "We evaluated our proposed BCAF method on the two benchmark datasets: MELD  [21]  and IEMOCAP  [38] . Both these two commonly used public datasets are multi-modal, containing audio, text and video modality for every utterance. Due to the natural imbalance across various emotions, we choose weighted average F1-score as the evaluation metric. Table  I  shows the distribution of train and validation and test samples for both two datasets.\n\n• MELD is a multi-modal and multi-party dataset for conversational emotion recognition  [21] . It consists of 13,708 utterances in 1,433 dialogues collecting from the Friends TV shows. Each utterance is labeled with one in seven emotions: anger, joy, sadness, neutral, disgust, fear and surprise. • IEMOCAP contains videos of dyadic conversations of ten speakers, spanning 12.46 hours  [38] . Each utterance is annotated using the following discrete categories: happy, sad, neutral, angry, excited, and frustrated. Given the inherent imbalance across different emotion classes, we used the weighted F1-score as our primary evaluation metric. The weighted F1-score calculates the F1-score for each class and applies weights based on the proportion of samples in each class. This approach ensures that the evaluation metric reflects the contribution of each class to the overall performance, addressing the impact of class imbalance effectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Baseline And State-Of-The-Art Methods",
      "text": "To comprehensively evaluate the performance of the proposed method, we compared our results with those of the bidirectional LSTM (bc-LSTM) baseline  [21] . This baseline system leveraged an utterance-level LSTM to model contextaware representations from surrounding utterances. Additionally, we compared the proposed BCAF method to various existing state-of-the-art methods:\n\n• M2FNet [14] employed a multi-head attention-based fusion mechanism to combine emotion-rich latent representations of emotion-relevant features from visual, audio, and text modalities, learning both intra-and inter-modal relationships.\n\n• HCAM  [29]  used a combination of recurrent and coattention neural network to capture intra-and inter-modal interactions for emotion classification. • CFN-ESA  [30]  incorporated a cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition.\n\n• TelME  [31]  used cross-modal knowledge transfer, using a language model (as the teacher) to enhance non-verbal modalities (as the student), thereby optimizing the performance of weaker modalities.\n\n• Mamba  [32]  designed a multi-modal fusion strategy based on probability guidance to maximize information consistency across modalities and capture intra-and intermodal interactions for conversational emotion recognition. • AGF-IB  [33]  introduced graph contrastive representation learning to capture intra-and inter-modal complementary semantic information, as well as intra-class and inter-class boundary information for emotion categories.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Model Configuration",
      "text": "We implemented our proposed BCAF method using the PyTorch 1.11.0 framework. The model was trained with the Adam optimizer with an initial learning rate of 1e-4 and an early-stopping strategy with a patience of 15 epochs. To aid convergence and improve generalization, we applied L 2 regularization with a weight of 0.0001 and used dropout with a rate of p = 0.3 to mitigate overfitting.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "We start by conducting comparative experiments against previous state-of-the-art baselines. Next, we ablate core module to verify the effectiveness of our proposed BCAF method. Following that, we emphasize the importance of bimodal attention network and qualitative analysis. Finally, we conduct case studies and error analysis.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Comparison With State-Of-The-Art Baselines",
      "text": "We compared our proposed BCAF model against state-ofthe-art baseline systems on two datasets, MELD and IEMO-CAP. As shown in Fig.  6 , BCAF demonstrates superior performance compared to state-of-the-art baseline systems in terms of the weighted F1-score, achieving a 3.15% improvement over HCAM  [29]  on the MELD dataset and a 4.11% improvement over Mamba  [32]  on the IEMOCAP dataset. These encouraging results demonstrate the superior expressive power and effectiveness of integrating the interactive connection network, bimodal attention network, and correlative attention network for bimodal speech emotion recognition.\n\nOur BCAF model showed significant improvements over contextual models such as bc-LSTM  [21] , M2FNet  [14] , and TelME  [31] . We attribute this improvement to the fact that many contextual models fail to effectively learn modality connections and model intra-and inter-modality interactions between audio and text. In contrast, BCAF addresses these interactions through its interactive connection network, bimodal attention network, and correlative attention network. Specifically, BCAF demonstrated a strong ability to infer major emotion categories, such as neutral and joy, though it occasionally misclassified minority classes, such as anger, on both the MELD and IEMOCAP datasets. We suggest that this may be due to the implicit expression and limited sample size of these emotions.\n\nOur BCAF model exhibited smaller performance improvements on the MELD dataset compared to the IEMOCAP dataset. Specifically, while BCAF outperformed baseline systems on both datasets, the relative performance gain over stateof-the-art baseline systems (M2FNet  [14]  on MELD versus TelME  [31]  on IEMOCAP) was more pronounced on the IEMOCAP dataset. Upon further analysis, we observed that dialogues in the MELD dataset are relatively shorter, typically consisting of 5 to 9 utterances, whereas dialogues in the IEMOCAP dataset average around 70 utterances per dialogue. Additionally, the MELD dataset, derived from real-world scenarios, contains significant background noise (e.g., honking, barking). Such noise, often uncontrollable and undesirable, can hinder the ability of individual modalities to effectively capture and convey emotional information. Furthermore, this noise may have introduced interference in the model, particularly for emotions like fear and frustration, which are more susceptible to being masked by background noise. As a result, the BCAF model achieved relatively better performance on the IEMOCAP dataset, where dialogues are longer and less affected by background noise, enabling the model to better capture emotional nuances.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Ablation Study",
      "text": "In this section, we conducted ablation studies to verify the effectiveness of our proposed BCAF model. We systematically ablated the model's interactive connection network, bimodal attention network, and correlative attention network (see Fig.  6 ). Additionally, we investigated the contribution of each component by removing it from BCAF. Overall, we observed that the full version of BCAF achieved the best performance on Fig.  6 . Overall performance comparison between the BCAF method, the state-of-the-art methods, and the baselines.\n\nboth the MELD and IEMOCAP datasets. Particular emphasis was placed on the removal of the interactive connection network, the bimodal attention network, and the correlative attention network, each of which adversely impacted the model's results.\n\nSpecifically, the performance decline exhibits a consistent pattern when different components are removed from the BCAF model: the correlative attention network > the bimodal attention network > the interactive connection network (see Fig.  6 ). This suggests that the integration of these three core networks significantly enhances the performance of bimodal speech emotion recognition. The results demonstrate that our proposed BCAF method improves the representation capability of multi-modal features and effectively models correlations, as well as intra-and inter-modal interactions,between audio and text.\n\n1) Effect of the Interactive Connection Network: We first investigated the effect of the interactive connection network in our proposed BCAF. The interactive connection network was designed to leverage an encoder-decoder architecture to learn modality-specific features and capture modality connections between audio and text.\n\nWe observed that the interactive connection network contributed to a performance improvement of 2.81% on the MELD dataset and 2.83% on the IEMOCAP dataset compared to the BCAF model without this component (see Fig.  6 ). This highlights the importance of the interactive connection network in enhancing the model's ability to capture and model intra-and inter-modal interactions between audio and text, leading to more accurate bimodal speech emotion recognition.\n\nIn the interactive connection network, each modality was processed using an encoder-decoder architecture to extract modality-specific features and analyze modality connections between audio and text. Specifically, the encoder transformed the raw input into a latent representation that captured essential characteristics specific to each modality. The decoder then reconstructed the original input from this latent space, effectively preserving intrinsic features within each modality, such as nuances in speech patterns or subtleties in textual expressions.\n\nThe connection loss encourages the BCAF model to learn modality connections by capturing shared features, enhancing emotion understanding. Previous approaches often processed modalities independently  [30] ,  [33] , potentially overlooking synergistic information from their interactions. By introducing regularization, the connection loss facilitates semantic complementation across modalities and maximizes the utility of modality connections.\n\n2) Effect of the Bimodal Attention Network: We then explored the role of the bimodal attention network, which aimed to leverage dynamic attention weights to learn intraand inter-modal interactions between audio and text. Ablation results in Fig.  6  demonstrated an absolute improvement of 4.1% on the MELD dataset and 5.12% on the IEMOCAP dataset when the bimodal attention network was included in our BCAF model, compared to the version without this network. This improvement underscores the critical role of the bimodal attention network in enhancing the model's ability to focus on emotion-relevant features by effectively capturing interactions between audio and text. By selectively attending to important modality-specific information and their interactions, the bimodal attention network significantly contributed to improved bimodal speech emotion recognition performance.\n\nIn the ablation study of the multi-modal fusion module, we attributed the performance improvement to the bimodal attention network, which facilitated intra-and inter-modal interactions within and between audio and text while capturing long-term contextual information. By introducing dynamic self-and cross-modal attention weights to uni-modal representations, we enhanced these representations to build a robust and effective bimodal representation.\n\nWe proposed that the bimodal fusion module leveraged heterogeneous knowledge in a high-dimensional space to capture detailed information embedded in each modality while adaptively fusing implicit complementary content. This strengthened interactions and correlations, encouraging the model to explore complementary information and dynamic interactions between audio and text. As a result, the multi-modal fusion module significantly improved performance in bimodal speech emotion recognition tasks.\n\n3) Effect of the Correlative Attention Network: Finally, we examined the impact of the correlative attention network, which was designed to filter out noise in cross-modal relationships and learn inter-correlations between uni-modal and bimodal representations.\n\nAs shown in Fig.  6 , removing the correlative attention network from the BCAF model resulted in a performance decrease of 5.24% on the MELD dataset and 6.68% on the IEMOCAP dataset compared to the complete model with all components included. This comparison underscores the significant role of the correlative attention network in improving emotion recognition by accurately integrating audiotext inter-correlations. The absence of this network hindered the model's ability to effectively filter noise and capture meaningful relationships across modalities.\n\nThe joint attention mechanism within the correlative attention network utilized a pair of softmax functions to simultaneously consider both self-and cross-attention weights for joint bimodal representation. This mechanism allowed the model to refine incorrect cross-modal information and dynamically adjust the influence of each modality, enabling comprehensive learning of intra-and inter-modal interactions between audio and text.",
      "page_start": 8,
      "page_end": 13
    },
    {
      "section_name": "C. Impact Of Attention",
      "text": "Furthermore, the bimodal correlation evaluation explicitly measured similarity by incorporating correlation coefficients into the uni-modal representations. This process aligned the uni-modal features with the joint bimodal space, enhancing the model's ability to capture and leverage complementary information across modalities for more accurate emotion recognition.\n\nIn this section, we examined the impact of different attention variants (see Fig.  7 ). For each modality, self-attention and cross-attention mechanisms were employed to capture intraand inter-modal interactions between audio and text. To evaluate the effectiveness of the proposed attention mechanism in modeling these interactions, we implemented four comparison systems. The experimental results are presented in Fig.  65  and Table  II .\n\n• Bimodal Attention Network (BAN): This network employed separate softmax functions to independently apply self-attention and cross-attention weights, enabling distinct modeling of intra-and inter-modal interactions (see Fig.  7 (a) ). . We observed that the BAN achieved the best performance on both the MELD and IEMOCAP datasets. As shown in Fig.  6  and Table  II , the inclusion of the BAN contributed to a performance improvement of 3.6% on the MELD dataset and 6.6% on the IEMOCAP dataset compared to the baseline model without BAN. This comparison highlights the effectiveness of BAN in leveraging both intra-modal and inter-modal interactions between audio and text.\n\nAdditionally, the decline in performance followed a consistent trend: BAN > JAN > BAN-SA > BAN-CA. This means that removing certain attention mechanisms resulted in progressively worse performance. Specifically, the absence of JAN caused a performance decrease of 2.85% on the MELD dataset and 3.34% on the IEMOCAP dataset. Similarly, excluding BAN-SA led to a decrease of 1.72% on MELD and 2.53% on IEMOCAP, while removing BAN-CA resulted in the smallest decline of 1.41% on the MELD and 1.69% on the IEMOCAP. These results underscore the relative importance of each attention mechanism in modeling effective intra-and inter-modal interactions between audio and text.\n\nThe bimodal attention network independently enabled complementary processing of intra-modal and inter-modal interactions between audio and text. We argue that intra-modal interactions provided the most essential information for accurate predictions, as they captured key modality-specific features directly tied to emotion recognition. Although inter-modal interactions added valuable complementary insights, core emotional cues were typically best represented within each individual modality, making intra-modal processing a primary factor in the model's effectiveness. Moreover, maintaining sufficient layers in the BAN was crucial for optimal performance. Reducing the number of layers negatively impacted the model's ability to process intra-and inter-modal interactions effectively, thereby degrading overall performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Case Studies",
      "text": "To illustrate the effectiveness of our proposed BCAF model, we selected samples from two datasets and conducted extensive experiments comparing BCAF with baseline models. The results, as shown in Fig.  8 , revealed the following insights:    • Unexpectedly, while our proposed BCAF method outperformed the baseline models in terms of weighted F1score, it performed worse than the state-of-the-art HCAM model. Upon further analysis, we attributed this to the emotional cues learned by the graph convolutional network in HCAM, which played a critical role in modeling and leveraging speaker information to capture intra-and inter-speaker dependencies for emotion recognition.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Error Analysis",
      "text": "We present the confusion matrices for the MELD and IEMOCAP datasets in Fig.  9 . As shown, although our BCAF model achieved significant improvements over previous methods, as discussed in Section V.A, its performance on several rare emotions (such as disgust, fear, and happiness) remained unsatisfactory. Similar observations have been reported in previous studies, likely due to the implicit expression of these emotions.\n\nBy inferring predefined emotions in conversations, we observed that the errors made by the BCAF model were primarily caused by the following factors:\n\n• Emotions are complex interactions between subjective and objective factors, leading to potential biases in affect annotations. The emotion experienced by the speaker and the emotion perceived by human annotators may differ.   Consequently, the BCAF model sometimes confused and misclassified similar or closely related emotions, such as fear and sadness (see Fig.  9 ). • The MELD and IEMOCAP datasets closely mirror realworld scenarios, exhibiting significant class imbalances and substantial background noise. This observation aligns with the fact that humans remain neutral most of the time. In bimodal speech emotion recognition systems, this often resulted in minority emotions being predominantly misclassified as major classes. • Humans express emotions through various modalities, including facial expressions, body posture, speech, and the linguistic content of verbal communication. However, the BCAF method relied solely on audio and text modalities for predicting emotions in conversations.\n\nTo address these challenges, we plan to implement several effective strategies, including resampling techniques, data augmentation, and transfer learning. Additionally, integrating information from multiple modalities in human communication could enhance the accuracy of emotion recognition, as different modalities complement and enrich each other, providing a more comprehensive set of emotion-relevant information.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We proposed the Bimodal Connection Attention Fusion (BCAF) method for efficient and robust bimodal speech emotion recognition. BCAF consists of three key modules: the interactive connection network, the bimodal attention network, and the correlative attention network. The interactive connection network enables the model to learn modality connections between audio and text. The bimodal attention network facilitates semantic complementation and optimally leverages intra-and inter-modal interactions between audio and text. Finally, the correlative attention network filters noise from cross-modal relationships and learns inter-correlations between uni-modal and bimodal representations. Experimental results demonstrated that BCAF outperforms state-of-the-art methods in bimodal speech emotion recognition task.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the proposed Bimodal Connection Attention Fusion (BCAF) method. The method consists of three modules: the unimodal representation",
      "page": 2
    },
    {
      "caption": "Figure 1: ), we propose",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the architecture of BCAF, which",
      "page": 3
    },
    {
      "caption": "Figure 2: Architecture of our proposed the Bimodal Connection Attention Fusion method. The method consists of three modules, the uni-modal representation,",
      "page": 4
    },
    {
      "caption": "Figure 3: Update scheme of the interactive connection network.",
      "page": 4
    },
    {
      "caption": "Figure 5: illustrates the architecture of the bimodal attention",
      "page": 5
    },
    {
      "caption": "Figure 5: ). The joint attention network",
      "page": 5
    },
    {
      "caption": "Figure 4: Update scheme of the bimodal attention network.",
      "page": 6
    },
    {
      "caption": "Figure 5: Update scheme of the correlative attention network consisting of the",
      "page": 6
    },
    {
      "caption": "Figure 4: ). The joint multi-modal representation h∗is passed through",
      "page": 6
    },
    {
      "caption": "Figure 6: , BCAF demonstrates superior perfor-",
      "page": 8
    },
    {
      "caption": "Figure 6: ). Additionally, we investigated the contribution of each",
      "page": 8
    },
    {
      "caption": "Figure 6: Overall performance comparison between the BCAF method, the state-of-the-art methods, and the baselines.",
      "page": 9
    },
    {
      "caption": "Figure 6: ). This suggests that the integration of these three core",
      "page": 9
    },
    {
      "caption": "Figure 6: demonstrated an absolute improvement of",
      "page": 9
    },
    {
      "caption": "Figure 6: , removing the correlative attention",
      "page": 10
    },
    {
      "caption": "Figure 7: ). For each modality, self-attention and",
      "page": 10
    },
    {
      "caption": "Figure 6: and Table II, the inclusion of the BAN contributed to",
      "page": 10
    },
    {
      "caption": "Figure 8: , revealed the following insights:",
      "page": 10
    },
    {
      "caption": "Figure 7: Different attention variants: (a) BAN, (b) JAN, (c) BAN-SA, and (d) BAN-CA.",
      "page": 11
    },
    {
      "caption": "Figure 9: As shown, although our BCAF",
      "page": 11
    },
    {
      "caption": "Figure 8: Inputs from bimodal data and predictions using bc-LSTM, HACM, and our proposed BCAF method on the MELD and IEMOCAP datasets are",
      "page": 12
    },
    {
      "caption": "Figure 9: Visualization of confusion matrices on the test sets of MELD (a) and IEMOCAP (b).",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ayerNor\nLLaayyeerrNNoo",
          "Column_2": ""
        },
        {
          "Column_1": "LLaa",
          "Column_2": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "continuour": "high pitch"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "rising": "pitch"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "84.",
          "Column_2": "95",
          "Column_3": "2.",
          "Column_4": "63",
          "Column_5": "0.",
          "Column_6": "72",
          "Column_7": "2.0",
          "Column_8": "7",
          "Column_9": "5.",
          "Column_10": "65",
          "Column_11": "0.",
          "Column_12": "88",
          "Column_13": "3.",
          "Column_14": "11"
        },
        {
          "Column_1": "14.",
          "Column_2": "23",
          "Column_3": "61.",
          "Column_4": "21",
          "Column_5": "1.",
          "Column_6": "78",
          "Column_7": "3.2",
          "Column_8": "0",
          "Column_9": "6.",
          "Column_10": "41",
          "Column_11": "2.",
          "Column_12": "14",
          "Column_13": "11.",
          "Column_14": "03"
        },
        {
          "Column_1": "16.",
          "Column_2": "00",
          "Column_3": "12.",
          "Column_4": "00",
          "Column_5": "34.",
          "Column_6": "00",
          "Column_7": "10.",
          "Column_8": "00",
          "Column_9": "6.",
          "Column_10": "00",
          "Column_11": "4.",
          "Column_12": "00",
          "Column_13": "18.",
          "Column_14": "00"
        },
        {
          "Column_1": "30.",
          "Column_2": "29",
          "Column_3": "8.",
          "Column_4": "65",
          "Column_5": "4.",
          "Column_6": "33",
          "Column_7": "34.",
          "Column_8": "13",
          "Column_9": "2.",
          "Column_10": "88",
          "Column_11": "2.",
          "Column_12": "88",
          "Column_13": "16.",
          "Column_14": "83"
        },
        {
          "Column_1": "18.",
          "Column_2": "66",
          "Column_3": "8.",
          "Column_4": "21",
          "Column_5": "1.",
          "Column_6": "24",
          "Column_7": "2.2",
          "Column_8": "4",
          "Column_9": "65.",
          "Column_10": "17",
          "Column_11": "0.",
          "Column_12": "75",
          "Column_13": "3.",
          "Column_14": "73"
        },
        {
          "Column_1": "22.",
          "Column_2": "06",
          "Column_3": "2.",
          "Column_4": "94",
          "Column_5": "1.",
          "Column_6": "47",
          "Column_7": "2.9",
          "Column_8": "4",
          "Column_9": "7.",
          "Column_10": "35",
          "Column_11": "47",
          "Column_12": ".06",
          "Column_13": "16.",
          "Column_14": "18"
        },
        {
          "Column_1": "17.",
          "Column_2": "68",
          "Column_3": "7.",
          "Column_4": "25",
          "Column_5": "2.",
          "Column_6": "03",
          "Column_7": "1.7",
          "Column_8": "4",
          "Column_9": "10.",
          "Column_10": "72",
          "Column_11": "3.",
          "Column_12": "19",
          "Column_13": "57.",
          "Column_14": "39"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "61.",
          "Column_2": "81",
          "Column_3": "9.0",
          "Column_4": "3",
          "Column_5": "4.",
          "Column_6": "17",
          "Column_7": "6.",
          "Column_8": "25",
          "Column_9": "17.",
          "Column_10": "36",
          "Column_11": "1.3",
          "Column_12": "9"
        },
        {
          "Column_1": "2.",
          "Column_2": "86",
          "Column_3": "84.",
          "Column_4": "49",
          "Column_5": "2.",
          "Column_6": "45",
          "Column_7": "2.",
          "Column_8": "04",
          "Column_9": "2.",
          "Column_10": "86",
          "Column_11": "5.3",
          "Column_12": "1"
        },
        {
          "Column_1": "6.",
          "Column_2": "51",
          "Column_3": "3.3",
          "Column_4": "9",
          "Column_5": "75.",
          "Column_6": "26",
          "Column_7": "4.",
          "Column_8": "69",
          "Column_9": "2.",
          "Column_10": "34",
          "Column_11": "7.8",
          "Column_12": "1"
        },
        {
          "Column_1": "2.",
          "Column_2": "94",
          "Column_3": "1.7",
          "Column_4": "6",
          "Column_5": "4.",
          "Column_6": "12",
          "Column_7": "75.",
          "Column_8": "88",
          "Column_9": "2.",
          "Column_10": "94",
          "Column_11": "12.",
          "Column_12": "35"
        },
        {
          "Column_1": "9.",
          "Column_2": "36",
          "Column_3": "3.0",
          "Column_4": "1",
          "Column_5": "5.",
          "Column_6": "35",
          "Column_7": "4.",
          "Column_8": "01",
          "Column_9": "76.",
          "Column_10": "59",
          "Column_11": "1.6",
          "Column_12": "7"
        },
        {
          "Column_1": "2.",
          "Column_2": "89",
          "Column_3": "6.3",
          "Column_4": "0",
          "Column_5": "6.",
          "Column_6": "56",
          "Column_7": "10.",
          "Column_8": "24",
          "Column_9": "3.",
          "Column_10": "94",
          "Column_11": "70.",
          "Column_12": "08"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition using different sensors, emotion models, methods and datasets: A comprehensive review",
      "authors": [
        "Yujian Cai",
        "Xingguang Li",
        "Jinsong Li"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "A systematic literature review of speech emotion recognition approaches",
      "authors": [
        "Beer Youddha",
        "Shivani Singh",
        "Goel"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "Linecongraphs: Line conversation graphs for effective emotion recognition using graph neural networks",
      "authors": [
        "Sarala Gokul S Krishnan",
        "Craig Padi",
        "Balaraman Greenberg",
        "Dinesh Ravindran",
        "Ram Manocha",
        "Sriram"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Universality, domain-specificity and development of psychological responses to music",
      "authors": [
        "Manvir Singh",
        "Samuel Mehr"
      ],
      "year": "2023",
      "venue": "Nature reviews psychology"
    },
    {
      "citation_id": "5",
      "title": "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods",
      "authors": [
        "Kaoru Bei Pan",
        "Zhiyang Hirota",
        "Yaping Jia",
        "Dai"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "6",
      "title": "Tdfnet: Transformer-based deep-scale fusion network for multimodal emotion recognition",
      "authors": [
        "Zhengdao Zhao",
        "Yuhua Wang",
        "Guang Shen",
        "Yuezhu Xu",
        "Jiayuan Zhang"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Cold fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "W Björn",
        "Elisabeth Schuller",
        "Timo André",
        "Michel Giesbrecht",
        "Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "9",
      "title": "Alignment in multimodal interaction: An integrative framework",
      "authors": [
        "Marlou Rasenberg",
        "Asli Özyürek",
        "Mark Dingemanse"
      ],
      "year": "2020",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "10",
      "title": "Intra-and inter-modal curriculum for multimodal learning",
      "authors": [
        "Yuwei Zhou",
        "Xin Wang",
        "Hong Chen",
        "Xuguang Duan",
        "Wenwu Zhu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Modeling intra-and inter-modal relations: Hierarchical graph contrastive learning for multimodal sentiment analysis",
      "authors": [
        "Zijie Lin",
        "Bin Liang",
        "Yunfei Long",
        "Yixue Dang",
        "Min Yang",
        "Min Zhang",
        "Ruifeng Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "Kaouther Ezzameli",
        "Hela Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Cross-modal fusion techniques for utterance-level emotion recognition from text and speech",
      "authors": [
        "Jiachen Luo",
        "Huy Phan",
        "Joshua Reiss"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Pankaj Shah",
        "Naoyuki Wasnik",
        "Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Coordination attention based transformers with bidirectional contrastive loss for multimodal speech emotion recognition",
      "authors": [
        "Weiquan Fan",
        "Xiangmin Xu",
        "Guohua Zhou"
      ],
      "year": "2025",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "16",
      "title": "Triagedmsa: Triaging sentimental disagreement in multimodal sentiment analysis",
      "authors": [
        "Yuanyi Luo",
        "Wei Liu",
        "Qiang Sun",
        "Sirui Li",
        "Jichunyang Li",
        "Rui Wu",
        "Xianglong Tang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion classification with multi-level semantic reasoning network",
      "authors": [
        "Tong Zhu",
        "Leida Li",
        "Jufeng Yang",
        "Sicheng Zhao",
        "Xiao Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "A review on methods and applications in multimodal deep learning",
      "authors": [
        "Summaira Jabeen",
        "Xi Li",
        "Muhammad Shoib Amin",
        "Omar Bourahla",
        "Songyuan Li",
        "Abdul Jabbar"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "20",
      "title": "Fer-former: Multimodal transformer for facial expression recognition",
      "authors": [
        "Yande Li",
        "Mingjie Wang",
        "Minglun Gong",
        "Yonggang Lu",
        "Li Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "A multimodal multiparty dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea",
        "Meld"
      ],
      "year": "2018",
      "venue": "A multimodal multiparty dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "22",
      "title": "Affect recognition from face and body: early fusion vs. late fusion",
      "authors": [
        "Hatice Gunes",
        "Massimo Piccardi"
      ],
      "year": "2005",
      "venue": "2005 IEEE international conference on systems, man and cybernetics"
    },
    {
      "citation_id": "23",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "Carlos Busso",
        "Zhigang Deng",
        "Serdar Yildirim",
        "Murtaza Bulut",
        "Min Chul",
        "Abe Lee",
        "Sungbok Kazemzadeh",
        "Ulrich Lee",
        "Shrikanth Neumann",
        "Narayanan"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th international conference on Multimodal interfaces"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Qin Jin",
        "Chengxin Li",
        "Shizhe Chen",
        "Huimin Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao",
        "Qi Tian"
      ],
      "year": "2017",
      "venue": "IEEE transactions on circuits and systems for video technology"
    },
    {
      "citation_id": "26",
      "title": "Lstm-modeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "Martin Wöllmer",
        "Moritz Kaiser",
        "Florian Eyben",
        "Björn Schuller",
        "Gerhard Rigoll"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "27",
      "title": "Transformerbased visual grounding with cross-modality interaction",
      "authors": [
        "Kun Li",
        "Jiaxiu Li",
        "Dan Guo",
        "Xun Yang",
        "Meng Wang"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "28",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "29",
      "title": "Hcam-hierarchical cross attention model for multi-modal emotion recognition",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "year": "2023",
      "venue": "Hcam-hierarchical cross attention model for multi-modal emotion recognition",
      "arxiv": "arXiv:2304.06910"
    },
    {
      "citation_id": "30",
      "title": "Cfn-esa: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Yingjian Liu",
        "Zhigang Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "Taeyang Yun",
        "Hyunkuk Lim",
        "Jeonghwan Lee",
        "Min Song"
      ],
      "year": "2024",
      "venue": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "arxiv": "arXiv:2401.12987"
    },
    {
      "citation_id": "32",
      "title": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "33",
      "title": "Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Yuntao Shou",
        "Tao Meng",
        "Wei Ai",
        "Fuchen Zhang",
        "Nan Yin",
        "Keqin Li"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "34",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "38",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    }
  ]
}