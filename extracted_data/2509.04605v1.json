{
  "paper_id": "2509.04605v1",
  "title": "Spoken In Jest, Detected In Earnest: A Systematic Review Of Sarcasm Recognition -Multimodal Fusion, Challenges, And Future Prospects",
  "published": "2025-09-04T18:40:52Z",
  "authors": [
    "Xiyuan Gao",
    "Shekhar Nayak",
    "Matt Coler"
  ],
  "keywords": [
    "Systematic review",
    "sarcasm",
    "multimodal",
    "affective computing",
    "sentiment analysis",
    "speech emotion recognition",
    "human machine interaction",
    "prosody"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sarcasm, a common feature of human communication, poses challenges in interpersonal interactions and humanmachine interactions. Linguistic research has highlighted the importance of prosodic cues, such as variations in pitch, speaking rate, and intonation, in conveying sarcastic intent. Although previous work has focused on text-based sarcasm detection, the role of speech data in recognizing sarcasm has been underexplored. Recent advancements in speech technology emphasize the growing importance of leveraging speech data for automatic sarcasm recognition, which can enhance social interactions for individuals with neurodegenerative conditions and improve machine understanding of complex human language use, leading to more nuanced interactions. This systematic review is the first to focus on speech-based sarcasm recognition, charting the evolution from unimodal to multimodal approaches. It covers datasets, feature extraction, and classification methods, and aims to bridge gaps across diverse research domains. The findings include limitations in datasets for sarcasm recognition in speech, the evolution of feature extraction techniques from traditional acoustic features to deep learning-based representations, and the progression of classification methods from unimodal approaches to multimodal fusion techniques. In so doing, we identify the need for greater emphasis on cross-cultural and multilingual sarcasm recognition, as well as the importance of addressing sarcasm as a multimodal phenomenon, rather than a text-based challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S Arcasm is a ubiquitous feature of everyday conversations, where receiving compliments or praise from friends can often leave us questioning their sincerity. This moment of doubt emerges when we detect subtle cues beyond the spoken words, such as a wink or a trace of sarcasm in the melody. Adding to its complexity, sarcasm does not always function as a mere polarity switcher; it can convey messages that deviate from the literal content  [1] . It operates subtly yet remarkably effective in a social context. Researchers have highlighted its multifaceted role. Jorgensen  [2]  and Brown  [3]  mentioned sarcasm is applied to address complaints and criticism to close relationships in a less harmful way. Seckman and Couch  [4]  claimed that sarcasm fortifies the solidarity within work groups. Perceiving sarcasm, however, demands more Manuscript submitted on  November 19, 2024 ; revised on May 24, 2025. (Corresponding author: Xiyuan  Gao)  X.Gao, S.Nayak and M.Coler are with Campus Fryslân, University of Groningen, Leeuwarden 8911 CE, the Netherlands (e-mails: xiyuan.gao@rug.nl; s.nayak@rug.nl; m.coler@rug.nl) cognitive effort than understanding direct expressions  [5] ,  [6] , primarily because sarcasm transcends the literal content of the spoken words  [7] , and relies on pragmatic cues, such as information from preceding discourse, shared knowledge  [8] , perceptual indicators, linguistic signals, societal norms, and even the speaker's preferences or background  [6] . The accurate interpretation of sarcasm is vital for effective social interaction, yet it poses significant challenges for individuals with neurodegenerative conditions, such as semantic dementia, or for those on the autism spectrum  [9] ,  [10]  The importance of speech prosody in sarcasm perception has been validated by linguists across languages. Previous research in linguistics related fields has predominantly investigated how sarcasm is conveyed through various sound patterns in language, highlighting the intricate relationship between speech delivery and the intended sarcastic meaning. For instance, Rockwell  [11]  observed that in English, sarcasm is often conveyed through a slower tempo, increased intensity, and a lower pitch. Similarly, Cheang and Pell  [12]  found that sarcastic speech in Cantonese is characterized by an elevated pitch, reduced intensity, a slower rate, and less vocal noise. Scharrer and Christmann  [13]  identified sarcasm in German by features such as a lower pitch, heightened intensity, and extended vowel duration. Loevenbruck et al.  [14]  revealed that sarcastic expressions are marked by longer duration, as well as increased pitch level and range. These findings collectively emphasize the nuanced role of prosodic features in signaling sarcasm across different languages.\n\nEarly efforts in recognizing sarcasm through computational models primarily focused on leveraging speech data for potential applications in dialogue systems  [15] ,  [16] . Sequentially, researchers have delved into the development of models capable of recognizing sarcasm in text over the past decades. Sarcasm text expressions are often described as \"noisy\" data, introducing ambiguity that can disrupt the accuracy of language models. This poses a significant challenge for tasks like Sentiment Analysis (SA), which are heavily influenced by the presence of sarcasm. Review works in computational processing of sarcasm have primarily concentrated on text data sourced from social networks  [17] -  [19] .\n\nWith the rapid development of speech technology and spreading use of voice-assisted devices, such as Alexa and Google Assistant, it is increasingly important to acknowledge the role of speech data in sarcasm recognition. Sarcasm recognition in speech holds the potential to assist individuals with neurodegenerative conditions by enhancing their ability arXiv:2509.04605v1 [cs.CL] 4 Sep 2025 to navigate and integrate into social conversations. Furthermore, in the realm of Human-Machine Interaction (HMI), the capability to accurately detect sarcasm enables machines to better grasp the subtleties of human communication, thereby fostering more nuanced and effective interactions. In light of this, this paper presents a comprehensive systematic review of sarcasm recognition, with a dedicated focus on speech data, presenting the development from unimodal to multimodal approaches, aiming to fill the vacancy of a systematic review on such a topic. In detail, we inspect three aspects including datasets, features, classification methods. Through the systematic review, we answer the following research questions (RQs): RQ1: What are the available datasets for sarcasm recognition using speech data, and what limitations do they present? What guidelines should be followed for creating high-quality datasets in this field? RQ2: As the field progresses from unimodal to multimodal sarcasm recognition, how have feature extraction developed? What are the challenges and limitations in current feature extraction methods? RQ3: As the field progresses from unimodal to multimodal sarcasm recognition, how have classification methods evolved? What are the constraints of current classification methods? RQ4: What are the primary challenges and limitations in the current development of sarcasm recognition technology, and what are the practical implications in real-world scenarios? This is the first systematic review of sarcasm recognition that focuses on speech data, traversing from unimodal to multimodal approaches. We aim to be a valuable resource for researchers and practitioners, offering insights into the latest developments and discussing challenges for future growth. This review serves as a bridge connecting various related research domains. Our goal is to inspire collaborative efforts in finding collective solutions for this multifaceted challenge.\n\nThe structure of this paper is as follows: Section II discusses the methods applied to operate the systematic review. Section III summarizes the results and analyzes accordingly to address the research questions. Section IV discusses the results of this systematic review and points out the limitations of the current research. Finally, Section V provides the conclusion of the review.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methods",
      "text": "This paper uses systematic review which follows the guidelines suggested by PRISMA  [20] . This systematic review follows the PRISMA checklist 1 . Items related to risk of bias  (#12, 15, 19, 22) , effect measures (#13), additional analyses  (#16, 23)  are excluded as they are beyond the scope of this systematic review.\n\nTo gather articles addressing our research questions, we selected the following databases: Scopus, IEEE Xplore, ISCA archive, ScienceDirect, and ACM Digital Library. These databases were chosen because they contain large amounts of peer-reviewed articles published in the field of computer science. Each database was initially searched between 1st Aug. 2023 and 5th Aug. 2023. To ensure the currency of our review 1 https://www.prisma-statement.org/prisma-2020-checklist and in response to reviewer feedback, we conducted a followup search on 24 Mar. 2025, covering publications up to Dec. 2024. The same methodology and search strings were applied to maintain consistency with the initial search protocol.\n\n1) To define the methodology involved in the articles, we applied automatic, machine learning, deep-learning; 2) To include articles that availed a multimodal approach, we used multimodal, multi-modal, multimodality; 3) To allocate the main phenomenon we concern, we included sarcasm, irony, detection, recognition; 4) To define the scope of the applied data type, we indicated speech, audio, voice, sound, conversation, dialogue. These terms above were chosen based on their prevalence in the preliminary search. To ensure alignment with the search logic employed by the chosen databases, we further combined these terms as follows in our practical application:\n\n• (\"sarcasm\" OR \"irony\") AND (\"detection\" OR \"recognition\") AND (\"speech\" OR \"audio\" OR \"voice\" OR \"sound\" OR \"conversation\") • (\"multimodal\") AND (\"sarcasm\" OR \"irony\") AND (\"detection\" OR \"recognition\") AND (\"speech\" OR \"audio\" OR \"voice\" OR \"sound\" OR \"conversation\" OR \"dialogue\") • (\"machine learning\" OR \"deep learning\" OR \"automatic\") AND (\"sarcasm\" OR \"irony\") AND (\"detection\" OR \"recognition\") AND (\"speech\" OR \"audio\" OR \"voice\" OR \"sound\" OR \"conversation\" OR \"dialogue\") When conducting search in the mentioned databases, we applied a few search limits to exclude articles beyond our research scope. First, we limited the research domain to computer science. Second, the article type was defined to be empirical study. Third, only articles written in English were included. Fourth, the selected articles should be published. Initially, we limited the publication window to the period from 2006 to 2023. In the follow-up search in Mar. 2025, we extended the publication window to include articles published in 2024. Due to database constraints that allow filtering by year but not by month, our follow-up search included the full year of 2023, which overlapped with the initial search conducted in Aug. 2023. To address this, we combined the records from both searches and conducted deduplication prior to screening.\n\nTo gather the eligible articles, a number of inclusion and exclusion criteria were defined for both the abstract and fulltext screening processes:\n\n1) The selected papers are in the computer science domain.\n\n2) The selected papers are empirical study instead of review, report, or commentaries. 3) The selected papers contain sarcasm detection/recognition in speech data. 4) The selected papers include a machine learning based method. 5) The selected papers are written in English. 6) Any unrelated, duplicated, unavailable full-texts, abstract-only papers will be removed. From the selected articles, we conducted a structured data analysis process. First, we stored the articles in Zotero for easy access. Second, to answer the research questions, we created a data extraction form in a spreadsheet. This form included information (e.g., dataset language, data size, annotation process, etc.) related to the research questions, and as we reviewed the selected papers, we recorded the pertinent information in the corresponding cells. This meticulous data extraction process ensured that all research questions were adequately addressed. In terms of data analysis, we access this from three perspectives: data, features extraction, and classification. These perspectives reflect the major components of the entire process of sarcasm recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Findings And Analysis",
      "text": "A complete screening and selection process is illustrated in Figure  1 . During the initial search phase, a total of 9,321 records were collected using the predefined search strings. After applying search limits across selected databases, 2,117 records remained. Following deduplication, 1,113 articles were retained and imported into Zotero for screening. We applied a two-stage screening process: (1) title and abstract screening using defined inclusion and exclusion criteria, followed by  (2)  full-text screening based on the same criteria. Along with this full-text screening, we also investigated the related works and references of these articles to extract additional qualified articles. It resulted in 21 articles from the full-text screening and 7 more from related work references, yielding 28 studies in total. To ensure comprehensive coverage of recent literature, Fig.  2 . Trend of the publications. \"Multimodal\" approaches include audio with text, audio with articulatory features, or audio with both text and visual modalities, while \"Unimodal\" refers to audio-only approaches.\n\nwe conducted a follow-up search phase on 24th Mar. 2025, collecting articles from 2023 to 2024. This search identified 2,756 records, from which 1,223 remained after applying search limits. After conducting cross-phase deduplication, 441 unique records remained. At last, 11 articles were gathered after full-text screening and one was added via reference chaining, creating an additional 12 studies. In total, 40 articles were included in the final synthesis.\n\nFigure  2  offers an overview of the publication years of the selected articles. A noteworthy trend emerges: prior to 2019, there were sporadic publications related to sarcasm recognition in speech. However, an increase is evident after 2019, with the majority of recent articles adopting a multimodal approach. Except for the emerging research interest in multimodality, this surge in research can be attributed to the release of the MUStARD dataset  [21]  in 2019, which catalyzed interest and innovation in multimodal sarcasm recognition. The availability of such resources has fueled new research directions in the field.\n\nTo provide a unified view of the overall process of sarcasm recognition, Figure  3  illustrates a general pipeline. The pipeline begins with a multimodal dataset that typically comprises audio, textual, and visual signals. These inputs undergo preprocessing steps such as normalization, noise removal, and signal alignment. Next, modality-specific features are extracted, for example, prosodic cues like pitch and duration from audio; lexical and contextual embeddings from text; and visual cues such as eye gaze and facial expressions. The system then proceeds to classification via two distinct paths: unimodal and multimodal. With a special focus on speech, the unimodal classification relies on features from the audio modality, employing traditional machine learning or deep learning models. In contrast, multimodal classification integrates features across modalities through appropriate fusion strategies before passing them to a final decision layer. The final output is typically a binary label indicating whether the input instance is sarcastic or not. In the following content, we follow the pattern of datasets, feature extraction, and classification to present the analysed results.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset For Sarcasm Recognition",
      "text": "In addressing RQ1, we conducted a comprehensive examination of the existing datasets utilized for sarcasm recognition, presenting key features, such as data source, modality, labeling, and more. Through this comparative assessment, we are able to elucidate the respective strengths and weaknesses inherent in these datasets. In our review, two distinct categories of datasets emerged: those that are curated to cater to a wider user base, and those that are designed to serve the needs of a particular research endeavor. As the latter resource is not available for open-source use or public access, the eligibility is inaccessible; we focus on the former category of open-source datasets in this review. Further, we delved into the development processes of these datasets, dissecting their unique characteristics. This exploration served to shed light on the processes involved in creating high-quality datasets for sarcasm recognition.\n\n1) Characteristics of existing datasets: An overview of the prominent datasets is presented in Table  I , offering a comprehensive view of their key attributes. The entries are listed in ascending order, organized according to the year of publication. Our review of the datasets revealed the following key characteristics: a) Source: In our review of sarcasm recognition datasets containing speech data, we identified four primary data sources: recordings, TV series, animations, and podcasts. As shown in Figure  4 , most of the surveyed studies use TV series as a data source. This is due to two major merits of the TV-sourced data. First, compared to acted data, TV series ought to be more natural as the actors are performing a reallife scenario. Second, the collection cost is comparatively low compared to recording in a studio environment. b) Language: As shown in Figure  4 , the language diversity within these datasets is limited, with the exception of IITKGP-SEHSC (Hindi)  [22] , MaSaC (Hindi and English)  [23] , Spanish Dataset (Spanish)  [24] , and CMMA (Chinese)  [25] . The majority of the datasets are available exclusively in English.\n\nc) Modality: IITKGP-SEHSC and Irony-Recognition  [26]  exclusively contain audio data, while the remaining datasets are meticulously curated to facilitate multimodal (text, audio, visual) sarcasm recognition.\n\nd) Labeling: SEmoji MUStARD  [27]  and SEEmoji MUStARD  [28]  are extensions of the original MUStARD. MUStARD++  [29]  are sourced from both MUStARD and MELD  [30] . The extensions primarily focus on enhancing the labels, transitioning from a mere binary classification (i.e., sarcasm and non-sarcasm) to a more comprehensive representation of associated sentiments and emotions. Beyond assigning affective labels, CMMA provides labels capturing the relevance between sentiment and emotion, and between sarcasm and humor. These labels are represented on a fivelevel scale: [-2, -1, 0, 1, 2], indicating negative, neutral, and positive relevance. This enables informed selection of main and auxiliary tasks in multi-task learning models.\n\ne) Context: Context is defined as the preceding dialogue turns of a labeled utterance. Given the pragmatic nature of sarcasm, linguists have posited that context plays a crucial role in reducing ambiguity in sarcasm interpretation  [31] -  [33] . Consistent with this view, several datasets collected for sarcasm research include contextual information along with the labeled utterances  [21] ,  [25] ,  [27] -  [29] .\n\nf) Speaker information: In the aspect of sarcasm perception, Fan et al.  [34]  found that anticipation of sarcastic intent is crucial for the efficient comprehension of sarcasm. That is to say, when sarcasm is a recurring style of a particular speaker, the likelihood of sarcastic expression from that individual increases, thereby speaker information significantly shapes the interpretation of the statements. In our review, speaker information has been leveraged by researchers  [21] ,  [27] -  [29] ,  [35]  to establish two evaluation settings: speakerdependent and speaker-independent. In the speaker-dependent v setting, utterances from the same speakers are used in both the training and testing sets, whereas the speaker-independent setting involves different speakers in the training and testing sets. g) Size: Sarcasm datasets are typically limited in size, with only one large dataset encompassing 15.2 hours of video that includes both sarcastic and non-sarcastic instances  [25] . This constraint reflects the challenge in the annotation sarcasm, particularly in achieving consensus among different annotators. Despite this challenge, there is a noticeable trend of gradual expansion in the size of these datasets over years, reflecting ongoing efforts to accumulate more comprehensive data sets for sarcasm research.\n\n2) Dataset development: The creation of a sarcasm dataset typically involves the following stages: data collection, data preprocess and data annotation. Each of them plays a crucial role in shaping the quality and usability of the resultant dataset.\n\na) Data collection: This initial phase involves the acquisition of raw data from a diverse array of sources. In our findings, examples of acted speech datasets, such as IITKGP-SEHSC and the dataset created by Geng et al.  [36] , involve trained vocal professionals who generate utterances, based on carefully designed stimuli. Notably, our review highlights a growing trend of utilizing TV series as a source of acted speech data. Although TV series involve acted performances, the actors often strive to replicate real-life scenarios, resulting in datasets that tend to exhibit a more natural conversational tone. Another type of data source is used by Gent et al.  [26] , they collected 4.68 hours of speech data from a podcast. Innovatively, Burkhardt et al.  [37]  built an interactive mobile phone-based interface to collect sarcastic speech. Users were guided to record their sarcastic reaction to visual stimuli and labeled their speech sequentially.\n\nb) Data preprocessing: The data preprocessing stage is necessary for enhancing data quality and consistency. It encompasses various tasks, such as format conversion, audio intensity normalization  [38] , segmentation  [21] ,  [39] , and signal alignment  [23] ,  [24] . Signal alignment, in particular, holds significant importance when working with multimodal data. However, it's also frequently seen that cross-modality alignment is implemented as part of the modeling process. For instance, Wu et al.  [35]  used GENTLE 2  to align video and audio after extracting the related features. Hasan et al.  [40]  applied the P2FA forced alignment 3  to extract the timing of key words and trace the aligned visual and acoustic features. In multimodal analysis, transcription is often needed. Zhang et al.  [25]  utilized the Google Cloud Speech-to-Text service  4  to transcribe collected conversations with timestamps. Subsequently, Adobe Premiere Pro 5  was used to segment the corresponding video clips for alignment.\n\nc) Data annotation: The annotation stage is of importance as it directly influences the dataset's utility, particularly in the context of supervised machine learning approaches. Given the ambiguous and diverse nature of sarcasm, many studies  [21] ,  [25] -  [29]  provide training or guidelines to annotators to ensure unbiased labeling. Some studies even enlist the assistance of professional linguists or experts for annotation. Conversely, a few studies  [16] ,  [37]  opt for an unguided approach, avoiding any guidance to prevent potential biases and encourage subjective interpretations of sarcasm. Contextual information is often provided to annotators, considering the significance of context in sarcasm perception. Additionally, in the dataset development process, a minimum of two annotators is employed, and inter-annotator agreement (IAA) is calculated as a metric to assess data quality  [15] ,  [23] ,  [25] -  [29] ,  [37] . Some researchers  [16] ,  [21] ,  [38]  also engage third parties, such as a third evaluator or survey participants, to evaluate the data's quality. During annotation, Kappa score  [41]  is frequently used to measure the agreement level.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Feature Extraction Analysis",
      "text": "This section focuses on presenting insights pertinent to RQ2. To facilitate a more extensive understanding of the evolution from unimodal to multimodal sarcasm recognition, we have structured the relevant content into three categories: audio features, textual features and visual features. Each category allows us to delve into feature representations, the choice of toolkits used for feature extraction, and the feature extraction level that evolved from unimodal to multimodal contexts. Figure  5  summarizes key trends across unimodal and multimodal systems, presenting a hierarchical taxonomy of the most prominent features used in each modality. For a complete listing of specific features, classifiers, evaluation, and performance in the unimodal (audio-only) setting, see Appendix Table  A2 . For the corresponding summary in the multimodal setting, see Appendix Table  A1 .\n\n1) Audio features: We begin by examining the audio features employed in unimodal systems, as highlighted under Audio (unimodal) in Figure  5 . We have classified the applied speech features into three distinct types as follows.\n\na) Spectral features: These features serve as a crucial representation of short-time speech signals, offering insights into the distribution of frequencies within a specific segment of the signal. These features are extracted through the application of Fast Fourier Transform, enabling the transformation of signal characteristics from the frequency domain into the time domain. In the context of sarcasm recognition, one of the prominent spectral features is Mel frequency cepstral coefficients (MFCCs). Six out of the eight unimodal studies reviewed have utilized this feature in their analysis. MFCCs are representations of the short-term power spectrum of a sound signal, and it is derived from the Mel frequency scale, which is a perceptually motivated frequency scale that better represents the way humans perceive the pitch of sounds. Therefore, they are well-suited for tasks involving human speech and sound recognition. More importantly, MFCCs offer a robust representation of spectral attributes within speech signals. For instance, Tepperman et al.  [15]  employed the first twelve MFCCs along with their delta and acceleration coefficients.\n\nAn alternative widely adopted choice is the Mel spectrogram (MelSpec), a time-frequency representation of audio employing the Mel frequency scale. Gao et al.  [42]  utilized MelSpecs as input features for a convolutional neural network (CNN)based architecture. This representation is highlighted for effective in capturing critical spectral nuances. Like MFCCs, MelSpecs are based on the Mel scale, making them suitable for speech and audio processing tasks. But they have higher dimensionality because of the retained spectral information in each frame. Furthermore, the application of Perceptual Linear Predictive (PLPs) features has been noted in Atassi et al.  [38] , leveraging their specialization in mirroring the perceptual characteristics of speech as perceived by human listeners. In addition, a few studies  [37] ,  [43]  have extended their feature set to encompass additional spectral attributes, including chroma, Spectral Contrast (SCt), Spectral Rolloff (SR), Spectral Bandwidth (SBW), Tonnetz, and Zero Crossing Rate (ZCR). They demonstrated the effectiveness of applying spectral features in sarcasm recognition.\n\nb) Prosodic features: The features refer to the acoustic characteristics of speech related to its rhythm, melody, and intonation rather than its linguistic content. Koolagudi et al.  [22]  divided prosodic features mainly in three categories: pitch, intensity and intonation. Rooted in variations in air pressure within the vocal fold, prosodic features serve as valuable indicators of human emotions  [44] . Many studies collectively applied statistical metrics such as mean, minimum, maximum, range, standard deviation to prosodic features, affirming their fundamental role in the context of sarcasm recognition in speech. For example, Atassi et al.  [38]  employed pitch contours, while Rakov and Rosenberg  [16]  incorporated word-level pitch and intensity contours, as well as sentencelevel statistical measures of pitch, intensity, and speaking rate. Mathur et al.  [45]  and Arun et al.  [39]  utilized a combination of spectral and prosodic features in their analyses.\n\nc) Voice quality features: The features are recognized as the individual voice characteristic, including bandwidth, glottal attributes, harmonic-to-noise ratio (HNR), jitter, and shimmer. These features describe the manner in which an individual produces speech, relating to aspects of vocal cord vibration and the resultant acoustic output. To enhance feature representation, voice quality features are often combined with spectral and prosodic features. For example, in addition to the spectral and prosodic feature sets mentioned ealier, Atassi et al.  [38]  employed a voice quality feature set that included harmonicity, as well as the frequencies and bandwidths of the first three for-mants along with their first and second derivatives. Similarly, Burkhardt et al.  [37]  utilized the larger-scale Interspeech 2013 Computational Paralinguistic Challenge (ComParE) feature set  [46] , which encompasses a comprehensive range of features from the spectral, prosodic, and voice quality dimensions.\n\nd) From unimodal to multimodal analysis: In recent years, the multimodal approach has emerged as an effective solution for sarcasm recognition, offering a significant advancement over traditional methods that relied solely on text or audio. The audio features applied in these systems are summarized in Figure  5 , under the category Audio (multimodal).\n\nNotably, spectral features like MFCCs, MelSpec, ZCR, chroma, Spectral Centroid (SC), deltas and bandwidth have consistently gained prominence in the multimodal context  [21] ,  [23] ,  [24] ,  [27] ,  [29] ,  [35] ,  [40] ,  [47] -  [56] . Among them, MFCCs are the most frequently applied features. The prosodic features have proven to play a vital complementary role as well. For instance, Alnajjar and Hämäläinen  [24]  and Hasan et al.  [40]  applied pitches. Geng et al.  [36]  and Gent et al.  [26]  particularly utilized mean and average values of duration, pitch, intensity and speaking rate. Zhang et al.  [47]  collected pitch tracking, voice/unvoiced segment feature (VUVs), peak slope parameters, and maxima dispersion quotients (MDQ). Although less frequently employed, voice quality features have also been utilized in some studies. Hiremath and Patil  [57]  utilized jitter, simmer, HNR; Hasan et al.  [40]  included normalized amplitude quotient (NAQ), quasi open quotient (qOQ), glottal source parameters, harmonic model and phase distortions, and the formants in their speech feature set. Pramanick et al.  [51]  used glottal source parameters. Gent et al.  [26]  integrated HNR.\n\nA new trend in multimodal context is the use of deep learning models such as VGGish  [58]  and Wav2Vec2.0.  [59]  VGGish is a CNN-based deep learning model, which takes MelSpec inputs and produces 128-dimensional embeddings that capture high-level semantic information from audio signals. Compared to traditional features (MFCCs, pitch and intensity), VGGish provides learned representations that encodes acoustic patterns without requiring manual feature design. However, it is trained primarily for general audio event classification, not for fine-grained speech nuances like sarcasm. Additionally, VGGish's fixed size embeddings might miss fine-grained temporal dynamics critical for detecting sarcasm. In contrast, Wav2Vec 2.0 is a transformer-based model trained using self-supervised learning on raw waveforms, producing contextualized frame-level embeddings that preserve both phonetic and prosodic information.\n\nAdditionally, there has been a concerted effort to enhance feature representations using advanced techniques. A number of studies  [40] ,  [60]  have explored the application of Transformer  [61] , and have shown promise in refining the extracted speech features and capturing intricate contextual information. Additionally, the integration of recurrent neural networks (RNNs) like the Gated Recurrent Unit (GRU) and attention mechanisms has been adopted in Zhang et al.  [47] . Chauhan et al.  [28]  took a similar approach, implementing Bidirectional GRU (BiGRU)  [62]  to leverage contextual relationships and enhance feature representation.   II  lists the toolkits that are mentioned in the selected papers. The choice of an audio analysis toolkit depends on the specific sarcasm-related speech features being analyzed. Praat  [63]  excels in detailed and accurate phonetic analysis, offering features for pitch, formants, intensity, and spectral analysis, making it ideal for detecting tonal variations in sarcasm. openSMILE  [64] , designed for real-time processing, efficiently extracts a broad range of emotion-related features, making it well-suited for sarcasm-related emotional cues like tone and intensity. Librosa  [65]  integrates easily with Python and simplifies complex audio analyses, but it lacks sarcasm-related speech features such as speech rate. A recent addition to the toolkit inventory is COVAREP  [66] , which is an opensource toolkit that researchers can leverage to collect a rich set of features, including pitch, intensity, and spectral features. Each toolkit has its strengths and limitations, so choosing the appropriate tool should be guided by the specific sarcasmrelated speech features being analyzed.\n\nf) Level of feature extraction: Among the selected articles, two distinct categories of feature level emerge: local features and global features. Local features are characterized by their short-term nature, computed within small, often overlapping time windows in an audio signal. These features excel in capturing dynamic changes and nuanced variations in speech, as exemplified in the work of Tepperman et al.  [15] . Conversely, global features are computed over more extensive segments of an audio signal, typically encompassing the entire speech utterance. These features capture characteristics that manifest over a more extended period, and a number of studies  [38] ,  [39] ,  [42] ,  [45]  have employed global features in their experiments. It's noteworthy that a common practice is to initially extract local features from individual windows within an utterance and subsequently aggregate them across the entire utterance. Nevertheless, it's also applicable to combine these two types of features  [16] ,  [26] .\n\nOur review reveals a notable inclination toward global features among researchers. This preference can be attributed to the comparative computational efficiency of global features, as they involve fewer parameters than their local counterparts. However, it's essential to acknowledge that extracting globallevel features comes at the cost of disregarding word-level re-lationships within the utterance. From unimodal to multimodal settings, an intriguing development is the utilization of deep learning models to process local features, therefore achieve a global representation, effectively transforming the feature level. Bedi et al.  [23] , for instance, employed a 1D CNN to convert the speech features to a fixed-length of sequence. Zhang et al.  [60]  used an average pooling layer and attention mechanism to generate the final feature vector. Ding et al.  [49]  applied a nonlinear mapping to achieve the same goal.\n\n2) Textual features: It's seen from Figure  5  that a significant majority of research endeavors in multimodal sarcasm recognition have used word embeddings extracted from BERT  [67]  as a fundamental feature representation for the text modality  [21] ,  [25] ,  [35] ,  [47] ,  [49] -  [52] ,  [54] ,  [55] ,  [60] ,  [68] -  [70] . BERT utilizes a bidirectional transformer architecture to pre-train deep bidirectional representations of text. Trained on extensive corpora like the Toronto Book Corpus 6  and Wikipedia  7  , BERT embeddings can capture nuanced semantic content. The incorporation of BERT embeddings in sarcasm recognition has gained prominence, as it equips models with the capacity to comprehend the intricate web of context and relationships among words. BART  [71] , alongside its multilingual variation mBART are transformer architectures that combine the strengths of bidirectional and auto-regressive pre-training. This dual approach enables them to effectively grasp the contextual nuances and semantic meanings embedded in text, further enhancing the quality of extracted features.\n\nSome studies have taken this approach a step further, enhancing word embeddings by integrating them with RNNs or transformer-based models. For instance, a few works  [27] ,  [28] ,  [53] ,  [60]  enriched the word embeddings with BiGRU. This addition allows them to to capture contextual information, enrich the semantics of text, and to generate a fixed-length of features. Furthermore, Liu et al.  [68]  introduced a complexvalued multi-modal encoder that leverages quantum probability to enrich features with contextual relationships, showcasing a novel approach to feature enhancement.\n\nResearch in multimodal sarcasm recognition predominantly leans towards the use of global textual features. The preference for global features aligns with the trend observed in audio features. To transition from local to global features, researchers employ mathematical averaging or employ DNNs, facilitating the conversion of local feature information into a comprehensive global representation.\n\n3) Visual features: As it's shown in Figure  5 , the visual modality predominantly relies on deep learning based spatial features  [21] ,  [24] ,  [25] ,  [29] ,  [35] ,  [47] ,  [49] ,  [51] ,  [52] ,  [54] ,  [60] ,  [68] ,  [69] ,  [72] ,  [73] , which are essentially attributes of images capturing information related to the spatial distribution of pixels within each frame. These spatial features play a pivotal role in discerning the visual cues within the video content. With the help of DNNs such as ResNets  [74]  and EfficientNet  [75] , the extracted features are assigned with semantic meanings, thus enriching their interpretability.\n\nAnother category of extracted features in the visual modality pertains to facial features. Given the strong connection between sarcasm and facial expressions, using facial features directly avoids noisy features from the background or unrelated scenes of the video. For instance, Wu et al.  [35]  employed MTCNN  [76]  for face detection in video frames. They then cropped these detected faces, subsequently processing them with a ResNet to extract features. Similarly, Sun et al.  [50]  utilized Face DLIB  8  for face extraction, followed by feature extraction using the ResNet-152. On the other hand, Hasen and Patil  [40]  implemented OpenFace 2  [77]  to derive Facial Action Unit (FAU) features, which are crucial for analyzing human emotions and affect. Furthermore, Hiremath et al.  [57]  incorporated the eye aspect ratio metric in their analysis, adding another dimension to their facial feature domain.\n\nSeveral research endeavors have made contributions to feature representation by applying both preprocessing and postprocessing techniques. For instance, Pramanick et al.  [51]  enhanced the spatial features by leveraging the action recognition tool 13D  [78] . Meanwhile, Zhang et al.  [60]  carried out further processing on spatial features, incoprprating BiGRU and the attention mechanisms, leading to the creation of weighted feature vectors as the final representation. Hasan et al.  [40]  harnessed modified transformer encoder to enrich the facial features. To mitigate the inclusion of noisy features resulting from video frames, Ray et al.  [29]  implemented Katna 9  on the extracted spatial features. This selective approach towards key frame extraction aids in enhancing the overall quality of the spatial feature.\n\nSimilar to the audio and text modalities, global features are the preferred choice in the visual modality. Typically, these features are derived by averaging values across multiple video frames or images that are synchronized with the corresponding text and audio. In a few instances, under the same mechanism, alternative methods have been employed. For example, Bedi et al.  [23]  involved a 1D CNN for generating a global feature vector. Zhang et al.  [60]  included average pooling and attention mechanisms to transform local features to global features.\n\nIn addition to harnessing features from the audio, text, and visual modalities, Some studies  [27] ,  [28]  ventured beyond the conventional boundaries by incorporating additional elements into their training sets. Specifically, they introduced speaker information, considering the identity of the speaker delivering the utterance as an influential factor in the recognition process. Chauhan et al.  [28]  delved into this multidimensional approach by exploring the role of emojis in the context of sarcasm recognition, recognizing the potential significance of these visual cues in enhancing the overall performance of the task.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "C. Classification Method And Fusion",
      "text": "The classification task typically involves using machine learning models to analyze the extracted features and determine whether it contains sarcasm. These classification models exhibit a spectrum of complexity, spanning traditional machine learning techniques such as Support Vector Machine (SVM) and Random Forest (RF) to the more advanced deep learning methodologies, which incorporate RNNs, CNNs, or Transformers.\n\n1) Unimodal classification: Unimodal sarcasm recognition in the audio modality involves the application of various classifiers, each with its own unique characteristics. Figure  6  provides a comprehensive overview. These utilized classifiers can be broadly categorized into two main groups: a) Rule-based classifiers: Such classifiers as Decision Trees used by Tepperman et al.  [15] , are known for their interpretability and rule-based structure. They can provide a clear insight into the decision-making process, making it a valuable tool for understanding the reasons behind classification outcomes. However, it may not handle complex data distributions effectively.\n\nb) Machine learning-based classifiers: Our review identified three main approaches to machine learning-based sarcasm classification.\n\n• Statistical models include classifiers like GMM proposed by Atassi et al.  [38]  and Hidden Markov Model (HMM) used by Mathur et al.  [45] . These models offer a principled and interpretable approach to capturing complex data distributions. GMMs are commonly used to model speech features because they can capture variations in human vocal expression. However, sarcastic speech may involve non-Gaussian patterns in feature space, and GMMs might not fully capture these nuances. On the other hand, HMMs are particularly effective for handling sequential data, making them well-suited for capturing temporal patterns in sarcasm. However, HMMs assume that the current state depends only on the previous state, which might not always capture more complex dependencies. • Traditional supervised learning models encompass a spectrum of classifiers, such as Logistic Regression (LR)  [16] , SVM  [48] , RF  [39] . These classifiers leverage established techniques within the domain of supervised learning to make informed decisions. LR is valued for its simplicity and interpretability, making it particularly effective for binary classification problems. However, it performs poorly when dealing with complex, nonlinear relationships. SVM is known for the ability to find optimal hyperplanes that separate different classes, even in high-dimensional spaces. RF, an ensemble learning method, combines multiple decision trees to improve accuracy and robustness. It can provide valuable insights into the importance of different features in making predictions.\n\n• Deep learning models encompass advanced models such as the fine-tuned VGGish employed in Gao et al.  [42] . These models harness CNN to capture intricate patterns in the audio data. Due to their deep architecture, they can model intricate relationships within the data. However, training deep learning models requires a substantial amount of labeled data. Acquiring and annotating large datasets can be resource-intensive and time-consuming.\n\nIn addition, DNNs are often considered \"black boxes\" due to their complex architectures, making it difficult to interpret how they arrive at specific decisions or predictions.\n\n2) Multimodal fusion strategies: Recent experimental studies have consistently demonstrated that incorporating audio, textual, and visual data significantly enhances sarcasm recognition performance compared to unimodal approaches, highlighting the inherently multimodal nature of sarcastic expressions  [21] ,  [29] ,  [35] ,  [40] . This reflects the linguistic understanding that sarcasm often relies on the interplay of audio, textual and visual cues -an interaction that is difficult to capture through a single modality alone  [79] ,  [80] . As pointed by Jacob et al.  [81] , the incongruity between verbal and non-verbal cues is facilitating comprehension of sarcasm. Sarcasm is neither simply a tone of voice or verbal irony, its complexity emerges from the dynamic integration of multiple communication channels.\n\nFurthermore, the multimodal nature of sarcasm is supported by the taxonomy provided by Attardo  [79] , which categorizes sarcasm markers into two primary types. Metacommunicative alerts are explicit signals that directly inform the listener that an utterance should be interpreted sarcastically. These include verbal indicators such as \"just kidding\" or \"I'm being sarcastic,\" as well as non-verbal cues such as a sarcastic smile or tongue-in-cheek expression. These markers serve as overt cues, providing a clear signal of the speaker's sarcastic intent. In contrast, paracommunicative alerts are more subtle, involving indirect cues that, when paired with the literal statement, suggest a sarcastic interpretation. For example, a speaker might use a blank facial expression or exaggerated nodding to imply sarcasm, while phonetic cues could include delivering an enthusiastic statement in a bored or monotonous tone or using flat intonation with minimal pitch variation. Unlike metacommunicative signals, paracommunicative alerts create a contrast with the literal content, prompting the listener to infer sarcasm. Furthermore, different modalities contribute varying degrees of significance to sarcasm detection. For instance, some speakers rely heavily on prosodic features in the audio modality, such as pitch variations, to convey sarcasm, while others may primarily use semantic cues embedded in the text modality. This underscores the value of multimodal integration in resolving ambiguity and improving robustness in sarcasm recognition systems.\n\nThe effectiveness of multimodal sarcasm recognition hinges on appropriately weighting these different modalities to reflect their respective contributions. To present this in detail, we delve into the various model fusion techniques in the field, discussing their roles in enhancing the effectiveness of multimodal sarcasm recognition systems. Figure  7  presents the distribution of the fusion strategies. For a complete listing of fusion methods, see Appendix Table  A1 .\n\nTraditional taxonomies classify fusion methods into three categories: Early fusion, where raw data from each modality is combined before being input to the model; Intermediate fusion, where features extracted from various modalities are merged and then fed into the model for decision-making; Late fusion, where decisions made independently by each modality are combined to produce the final prediction. However, traditional classifications do not adequately capture the complexities of contemporary architectures, where feature representation, modality integration, and classification are intricately intertwined. To address these complexities, we adopt the more recent taxonomy proposed by Zhao et al.  [82] , which provides a more nuanced framework for understanding the fusion techniques employed in multimodal sarcasm recognition.\n\na) Encoder-decoder: This architecture is structured into two primary components: the encoder and the decoder. The encoder processes input data into a latent representation, retaining the essential semantic information while filtering out extraneous noise. The decoder then utilizes this latent representation to generate predictions. In our review, a number of studies have adopted this approach. For instance, Castro et al.  [21]  used encoders to extract textual, speech, and visual representations, which were then concatenated and fed into a SVM decoder for classification.\n\nOther variants within this framework include Ding et al.  [49] , who proposed a multimodal learning framework consisting of three layers of feature fusion networks for encoding text, audio, and visual modalities. The output of the third layer was then combined with speaker features and contextual information before being passed to a fully connected neural network (FCNN) decoder for final prediction. Gent et al.\n\n[26] utilized a Long Short-Term Memory (LSTM) network to encode audio features, and a CNN to encode the textual data, with the resultant representations concatenated and processed by a FCNN functioning as the decoder.\n\nb) Attention mechanism: Introduced by Vaswani et al.  [61] , the attention mechanism has become a central topic in the deep learning community. This mechanism allows models to assign varying weights to different modalities, enabling the extraction of information that is critical to the task at hand. By doing so, attention mechanisms enhance prediction accuracy without significantly consuming computational costs. Recently, attention mechanisms have emerged as key tools in multimodal data fusion tasks. Several variations of attentionbased methods have been developed to leverage this capability.\n\nFirst, the intra-modality self-attention focus exclusively on data within a single modality, facilitating a undiluted analysis of relationships within that modality. Pramanick et al.  [51]  proposed the MuLOT system, which utilizes self-attention to enhance intra-modal correspondence while incorporating optimal transport methods to address cross-modal alignment.\n\nSecond, the inter-modality cross-attention focuses on exploiting the relationship among different modalities by integrating data from multiple sources in each attention operation. This approach enables the model to capture intricate connections across modalities. For example, Chauhan et al.  [27]  employed cross-attention to discern relationships between segments across modalities. Hasan et al.  [40]  and Zhang et al.  [60]  utilized a pair of cross-modal attention mechanisms to explore bimodal interactions (text-audio, text-visual interactions). Wu et al.  [35]  applied a cross-modal attention mechanism to emphasize specific words, particularly those exhibiting a high degree of incongruity between positive spoken text and negative nonverbal (audio and visual) cues. Similarly, Zhang et al.  [47]  proposed a framework that assessed crossmodal incongruity between text and visual, as well as text and audio modalities. Furthermore, Chauhan et al.  [28]  developed an architecture to explore the interplay between emojis and other multimodal data, highlighting the versatility and evolving nature of cross-attention techniques.\n\nThird, the hybrid intra-modality and inter-modality attention combines the previous two. Zhang et al.  [60]  implemented an intra-modality attention mechanism to identify the most relevant features within each modality individually. Following this, they employed three inter-modal attention mechanisms to capture and analyze the pairwise relationships between modalities (text-visual, text-audio, and visual-audio interactions). This dual-layered approach enables a comprehensive understanding of how different types of data contribute to sarcasm detection, facilitating a more nuanced and effective integration of multimodal information. c) Collaborative gating mechanism: Unlike attention mechanism which distributes weights across elements within or across modalities, gating mechanisms leverage learned gates to control which modality influences the fused representation. For example, Ray et al.  [29]  and Tomar et al.  [72]  used the collaborative gating mechanism to evaluate all modalities in parallel and learned to assign importance scores that reflect their combined informative strength. This coordinated gating xi strategy enables the model to selectively attenuate less relevant or noisy modalities while amplifying those contribute saliently.\n\nd) Quantum based method: Last, beyond the multimodal fusion methods previously discussed, innovative approaches are emerging to enhance multimodal fusion. For instance, Liu et al.  [68]  introduced QUIET, an architecture featuring a quantum-based inter-modal fusion layer designed to integrate multimodal data. At the heart of this framework, bimodal representations are derived through quantum interference, a process grounded in quantum probability theory. Quantum probability offers a mathematical and conceptual foundation for modeling the inherently uncertain behaviors of microscopic particles, providing a novel mechanism for capturing complex inter-modal relationships. Tiwari et al.  [73]  proposed QFNN that incorporates quantum computation to enhance feature expressiveness. Furthermore, quantum entanglement is utilized to model interactions between modalities, capturing nuanced dependencies that are difficult to express in classical space.\n\nTo summarize, the strength of encoder-decoder methods lies in their ability to integrate and aggregate features from multiple abstraction levels, thereby enhancing the model's capacity to capture cross-modal relationships. However, multiple encodes for different modalities demand substantial computational resources, especially when the number of modalities is increased. Attention mechanisms play a crucial role in facilitating interactions both within and across modalities, refining features by leveraging learned associations. This process enhances the model's ability to capture and understand the underlying relationships within individual modalities and the intricate connections between them. However, this approach inherently introduces structural complexity, as it requires the seamless integration of diverse data types, each with its own unique characteristics. Ensuring that these modalities are highquality and properly synchronized is critical to optimizing the attention process and accurately representing the nuanced relationships between them. Moreover, gating mechanisms act as dynamic filters, allowing the model to selectively focus on the most relevant features from each modality while limiting less informative ones. Furthermore, by leveraging quantum computing principles, such as superposition and entanglement, quantum based methods can potentially handle complex, highdimensional multimodal data.\n\n3) Performance meta-analysis: We conducted a metaanalysis to aggregate F1-scores from multiple studies on sarcasm recognition, offering a comprehensive view of fusion method performance. Results are reported separately for speaker-dependent and speaker-independent settings, as each presents unique challenges: speaker-dependent involves the same speakers for training and testing, while speakerindependent employs different speakers, providing a more realistic test of model generalizability. The following sections detail the data extraction and analysis process.\n\na) Data extraction: To assess the effectiveness of different fusion methods, we first selected studies with consistent contexts, such as dataset used, reported metrics, and evaluation methods. Initially, 31 multimodal sarcasm recognition studies were gathered. We refined the selection using inclusion and exclusion criteria: Studies employing the MUStARD dataset",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Fusion Method F1-Score (%) Reference",
      "text": "Encoder-decoder 63.1 SVM decoder  [21]  Encoder-decoder 71.3 FCNN decoder  [49]  Attention mechanism 65.90 cross-attention  [27]  Attention mechanism 70.00 cross-attention  [35]  Attention mechanism 70.90 cross-attention  [54]  Attention mechanism 80.00 cross-attention  [28]  or its extensions were retained (22 articles). Given sarcasm recognition involves class imbalance, studies reporting F1score, a balanced metric, were included (19 articles). We excluded five studies without explicit evaluation methods, and five used train-test splits that were insufficient for stable statistical analysis, resulting in nine articles for the analysis. b) Analysis: The primary goal is to determine whether major fusion methods (encoder-decoder VS. attention mechanism) significantly impact F1-scores in sarcasm recognition. To achieve this, we conducted a random-effects meta-analysis, assessing the heterogeneity across studies and computing effect sizes to quantify the magnitude of differences in F1-scores between the fusion methods. The effect size was calculated using Hedges ′ g  [83] , which adjusts for small sample sizes and quantifies differences in mean F1-scores. We used the Qstatistic and I 2 statistic to assess heterogeneity across studies. The Q-statistic tests whether the variation in effect sizes across studies is greater than expected by chance, while I 2 quantifies the percentage of total variation due to heterogeneity  [84] . After assessing heterogeneity across studies, a random-effects model was applied. The model incorporated tau-squared (τ 2 ), an estimate of the between-study variance, and provided a pooled mean effect size  [85] .\n\nTable  III  summarizes studies with speaker-independent evaluation. We applied the aforementioned analysis framework to the speaker-independent evaluation and observed significant heterogeneity across studies, with a Q-statistic of 30.00 and an I 2 of 83.33%. Using the random-effects model, the weighted mean F1-score for the encoder-decoder method was 67.20, with a 95% confidence interval of  (59.16, 75.24) . In comparison, the attention mechanism yielded a higher mean F1-score of 71.70, with a 95% confidence interval of  (65.87, 77.53) . However, the overall p-value for the difference between the two methods was 0.4630, suggesting that the difference in performance was not statistically significant.\n\nTable  IV  covers those with speaker-dependent evaluation. The speaker-dependent evaluation also demonstrated substantial heterogeneity, with a Q-statistic of 72.00 and an I 2 of 88.89%. Using the random-effects model, the weighted mean F1-score for the encoder-decoder method was 73.99, with a 95% confidence interval of  (70.19, 77.79) . In comparison, the attention mechanism achieved a comparable mean F1-score of 75.11, with a 95% confidence interval of (73.15, 77.07). The overall p-value for the difference between the two methods was 0.6303, confirming no statistically significant difference.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Xii Table Iv Performance Of Speaker-Dependent 5-Fold Cross-Validation",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Fusion Method F1-Score (%) Reference",
      "text": "Encoder-decoder 70.35 SVM decoder  [48]  Encoder-decoder 71.60 SVM decoder  [21]  Encoder-decoder 75.01 FCNN decoder  [49]  Encoder-decoder 79.00 FCNN decoder  [50]  Attention mechanism 72.57 cross-attention  [27]  Attention mechanism 74.50 cross-attention  [35]  Attention mechanism 78.70 cross-attention  [28]  Attention mechanism 74.60 hybrid attention  [60]  Attention mechanism 75.20 hybrid attention  [54]   Sarcasm Nonsarcasm  [28]  The analysis shows no statistically significant difference in performance between attention mechanism and encoderdecoder fusion methods in both speaker-independent and speaker-dependent evaluations. While attention mechanism exhibits higher mean F1-scores in both cases, the overlapping confidence intervals and high p-values indicate that these differences are not statistically reliable. Significant heterogeneity among studies suggests that model performance can be affected by other factors like feature granularity and model architecture. Additionally, the limited study pool restricts statistical power, highlighting the need for more research to detect subtle differences between fusion methods. c) Error analysis and failure modes: While aggregate F1-scores offer insights, understanding misclassification reasons is essential for refining model performance. Below, we examine common failure modes in sarcasm recognition:\n\n• Mismatch between modalities: Sarcasm is misclassified due to mismatches between modalities. For instance, Castro et a  [21]  noted errors arising when facial expressions and emotional cues conflicted. Similarly, Chauhan et al.  [27]  observed confusion between explicit and implicit emotions, as in utterance 1 in Table  V , where Chandler's happy demeanor (explicit emotion) contrasts with his sarcastic anger (implicit emotion), leading to misclassification.   [28]  noted cases where sentences with identical sentiment and emoji annotations were classified differently. For example, utterance 3 (Table  V ) was labeled as non-sarcastic while sentence 4 with the same annotations was sarcastic, leading to model confusion. Limited training data further hinders the ability to capture these nuanced cues. To summarize, key challenges identified include modality mismatches, such as inconsistencies between facial expressions and expressed emotions, and difficulties detecting sarcasm with neutral cues. Additionally, models struggle when labeling fails to capture sarcasm's subtlety, indicating a need for more sophisticated multimodal fusion techniques. Existing labeling systems may not fully encapsulate sarcasm's complexity; developing more granular systems that incorporate factors like sarcasm intensity and confidence level, beyond sentiment and emotion, could help reduce misclassification.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Iv. Discussion",
      "text": "In this discussion, we synthesize key findings from our review to highlight emerging trends in sarcasm recognition research. Specifically, we identify evolving patterns in data curation, advances in feature extraction techniques, and the increasing sophistication of multimodal fusion architectures. We also examine limitations in each of these dimensions, based on which, we outline promising future directions, arguing that these trends and constraints indicate the need for more interdisciplinary and linguistics informed approaches.\n\nA. Challenges in existing datasets RQ1.1: What are the available datasets for sarcasm recognition using speech data, and what limitations do they present?\n\nIn our study, we have collected a total of nine primary datasets for the purpose of sarcasm recognition across different languages. These datasets include MUStARD, IITKGP-SEHSC, SEmoji MUStARD, The Best Sarcasm Annotated Dataset in Spanish, SEEmoji MUStARD, Irony-Recognition, MUStARD++, MaSaC and CMMA. These datasets empower multimodal research, augmenting the effectiveness of sarcasm recognition. Additionally, they can be used for multitask research due to extensive sentiment and emotion labelling. However, several areas for improvement have been identified in our review: (a). None of the existing datasets is derived from spontaneous speech -naturally occurring speech produced in real-life contexts without preparation or scripting. This type of data is critical for training sarcasm recognition systems that can function effectively in real-world applications. (b). The datasets available for sarcasm recognition are relatively small compared to those in related fields such as SA or xiii Speech Emotion Recognition (SER), which may limit the generalizability and robustness of the models trained on them. (c). The lack of language diversity restricts the scope of multilingual sarcasm research, potentially overlooking the cultural nuances inherent in sarcastic communication. Future work should prioritize the development of spontaneous, multimodal, richly labeled, and culturally diverse datasets that are accessible for global research efforts. (d). To establish the golden standard, some previous works employed participants who were not given any definition of sarcasm, leaving subjectivity in the generated dataset. The annotation generated by Tepperman et al.  [15]  received an IAA of 0.527 which is just above the chance. Applying such a dataset to evaluation is doubtful. (e). COST 2102 Italian Database of Emotional Speech  [86]  is a well-curated dataset comprising 216 samples labeled for various emotional states, including happiness, sarcasm/irony, fear, anger, surprise, and sadness. Moreover, Geng et al.  [36]  compiled a dataset containing Mandarin acted speech for sarcasm recognition. Regrettably, these resources are not available for open-source use or public access, limiting their utility in broader research contexts.\n\nRQ1.2: What guidelines should be followed for creating highquality datasets in this field? Our investigation into dataset development underscored the significance of data sources and the annotation process.\n\n1) Data source: Our review highlights a growing trend of utilizing TV series as a source of data. Such data source achieves a good balance between collection efficiency and data spontaneity. However, the performance may be compromised by a lack of authenticity, and the collected data frequently encounters challenges such as background noise and laugh tracks. Several notable issues have been identified with the widely used MUStARD dataset. For example, the presence of background laugh tracks often overlaps with sarcastic speech, introducing potential bias into models trained on this data. Moreover, many utterances in the dataset are less than one second long and consist of only a single syllable (e.g., \"No\"), making them dependent on preceding conversational turns for proper interpretation. Training models on such isolated utterances without the context of prior dialogue may lead to biased results. Furthermore, the character Sheldon, who is a major contributor of sarcastic remarks in the dataset, is portrayed as someone who struggles to recognize sarcasm. As a result, his sarcastic remarks are often minimally marked, affecting the presence of non-verbal cues such as prosody and facial expressions, which can further skew the dataset's representation of sarcasm. The source of the dataset is of significant importance as it reflects nuanced facets of sarcasm. Diverse sources, such as social media dialogues, public debates could potentially offer a broader spectrum of sarcasm nuances.\n\n2) Data annotation: It becomes evident from our review that providing annotators with guidelines for labeling sarcasm is crucial to ensure the consistency and accuracy of the annotation process. According to Camp  [87] , sarcasm encompasses a spectrum of sub-categories, each with its distinctive nuances, including Propositional Sarcasm, which is an implicit sentiment proposition, relying on contextual information for sarcasm identification. For instance, \"your plan is fantastic!\" may sound non-sarcastic if the context is missing. Illocutionary Sarcasm, which mainly relies on non-verbal cues, such as prosody and visual signals, to convey sarcastic intent instead of the textual information alone. An example of this would be saying \"that's right\" while rolling one's eyes or placing exaggerated emphasis on the word \"right.\" Embedded Sarcasm, which is characterized by the presence of embedded sentiment incongruity; and Like-prefixed Sarcasm, which introduces an implicit disagreement and it typically involves the use of the word \"like\" as a precursor to the sarcastic expression. Establishing annotation guidelines that encompass these nuanced sub-types of sarcasm is essential for creating well-organized and high-quality datasets. Moreover, this detailed classification can inspire the development of models that capture the intricacies of sarcasm and provide a more comprehensive analysis of model performance.\n\nConsidering the cultural variability in interpreting sarcasm, it is imperative to provide annotators with comprehensive guidelines. These guidelines should not constrain interpretations but offer a broad framework for understanding sarcasm across diverse cultural and linguistic contexts. Currently, the IAA of the existing datasets is still at a low point, with Ray et al.  [29]  achieving the existing highest score of 0.595 in English. Therefore, it's necessary to allow annotators to engage in discussions to establish a common annotation standard and increase the agreement.\n\nThe current approach to sarcasm labeling is limited to a binary classification, where utterances are categorized as either sarcastic or non-sarcastic. This simplistic labeling neglects the subtleties and gradations of sarcasm that occur in real-world communication. In reality, sarcasm exists along a spectrum, with varying degrees of intensity. For example, when someone arrives late to a meeting, a remark like \"Oh, it's not like we were waiting for you,\" delivered in a calm, flat tone, subtly conveys mild annoyance or disappointment through sarcasm. In contrast, responding to someone failing a simple test with, \"Congratulations! You've really outdone yourself this time!\" in an exaggeratedly excited tone amplifies the sarcasm. Sarcasm, often linked to negative sentiment, intensifies as the expression becomes more positive in valence and higher in arousal. In other words, the stronger the expressed emotion, the greater the perceived intensity of the sarcasm. Although the MUStARD++ dataset includes emotion annotations, it does not explicitly indicate the relationship between sarcasm and the annotated emotions. Therefore, to more accurately reflect real-life sarcasm, a labeling system that accounts for varying intensities is essential. This nuanced approach would better capture the range of sarcastic expressions, providing a more comprehensive framework for sarcasm analysis.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. Evolution Of Feature Extraction Techniques Rq2:",
      "text": "As the field progresses from unimodal to multimodal sarcasm recognition, how have feature extraction developed? What are the challenges and limitations in current feature extraction methods?\n\nIn our exploration of the progression from unimodal to multimodal approaches for sarcasm recognition, we find three xiv main trends: the increasing reliance on deep learning-based features, pre-and post-extraction refinement techniques, and the prevalent use of global feature representations.\n\n1) Deep learning-based features: In the transition to multimodal sarcasm recognition, the audio modality continues to play a pivotal role, but a shift has emerged. Recent trends indicate a growing adoption of DNNs to generate higher-level, semantically-rich speech features for audio feature extraction. We discover that several recent works have applied VGGish and Wav2Vec2.0, which allow for the representation of semantic meanings, marking a departure from earlier approaches that predominantly relied on tools like Librosa, OpenSMILE or Praat to extract low-level features.\n\nThis shift towards higher-level feature representation is even more pronounced in the text modality, where sarcasm detection has been a long-standing area of interest. By comparing the feature extraction (e.g., syntactic pattern, n-grams, sentimentrelated features, etc.) mentioned in the review conducted by Joshi et al.  [17]  and the contemporary techniques utilized for text feature extraction (e.g., BERT, BiGRU, etc.), we gain insights into the potential trajectory of feature extraction within the audio modality. This progression suggests a future where feature extraction in the audio domain may follow a similar trajectory, moving towards the integration of more advanced and semantics embedded methods.\n\nVisual features were integrated as the rise of multimedia content on social media. Our review underscores the prevalence of spatial visual features. These features are systematically extracted through the utilization of DNNs, including architectures like ResNets and EfficientNet. The choice of these DNNs is primarily motivated by their robustness and exceptional performance in feature extraction. Importantly, it is evident from our investigation that pre-trained models, specifically from CNNs pretrained on extensive image datasets, has emerged as a prevailing practice in video feature extraction.\n\n2) Pre-and post-extraction enhancements: In addition to the increasing application of DNN-based features, many studies have employed various techniques to improve feature representation. In both audio and text modalities, transformer-based modules or RNNs are integrated into the architecture after feature extraction to enhance the ability to capture contextual relationships, enrich semantic content, and generate fixedlength feature representations. For the visual modality, beyond the use of transformer-based post-extraction modules, several pre-extraction techniques are applied directly to raw data to strengthen visual representation. These include methods such as action recognition and face recognition.\n\n3) Prevalence of global features: Our analysis regarding feature level, whether local or global, distinctly reveals a strong inclination among researchers towards the utilization of global features. In contrast to local features, which are derived from each individual segment and thus remain stationary, global features are statistical aggregates of these local components. Their prevalence in the literature is primarily attributed to their computational efficiency. However, it's worth noting that global features have certain limitations. They overlook the word-level dynamics and temporal intricacies within an utterance, which can be crucial for capturing cues such as word stress, particularly in the context of sarcasm. Fortunately, in the context of the multimodal approach, our review uncovers a trend where researchers have introduced innovative techniques to enhance the extraction of global features. Rather than resorting to the conventional method of averaging feature vectors, these studies have opted for alternative approaches. For instance, the employment of 1D CNN to convert the speech feature  [23] , or the utilization of attention mechanism to generate the feature vector  [60] . These novel methods represent a valuable departure from the conventional reliance on averaged global features, thereby opening up new avenues for more effective feature extraction while accommodating the intricate temporal aspects involved in sarcasm recognition.\n\n4) Limitations of current feature extraction techniques and future research: In our investigation into the development of feature extraction, the following four limitations have emerged:\n\na) Unexplored parameters in spectral features: While spectral features such as MFCCs, MelSpecs, and their correlations have been widely applied in sarcasm recognition, critical aspects remain underexplored. None of the studies reviewed provide detailed insights into the specific utilization of these features. For instance, in general speaking, different MFCCs coefficients represent distinct parts of the Mel-frequency spectrum, with lower coefficients capturing broader, low-frequency patterns and higher coefficients detailing finer, high-frequency variations. We've known that 12-13 coefficients are used in automatic speech recognition, as they encapsulate key speechrelated features, like formants. Using higher coefficients introduces more granular details but can also add noise, potentially hindering recognition if not managed carefully. In sarcasm recognition, the precise role of these selected spectral features remains unclear. Therefore, future research should focus on a more granular analysis of spectral features to better understand their relationship and contribution to sarcasm detection.\n\nb) Absence of a best-performed speech feature set: Linguistic research underscores the importance of speech features such as pitch, speaking rate, and intensity in conveying sarcasm. Our review confirms their frequent use in sarcasm recognition, though a best-performed feature set remains undefined. Given the variations in datasets, evaluation protocols, and architectures employed across the reviewed studies, it is methodologically inappropriate to draw direct conclusions linking specific features to performance. However, we encourage future research to systematically evaluate the effectiveness of the features through controlled experimental setups that can unravel the effects of features, models, and data characteristics.\n\nMoreover, future research should emphasize the interdependence of feature and model selection in sarcasm recognition. While commonly used features such as MFCCs, pitch, and intensity often yield positive results, their effectiveness is modeldependent. For instance, MFCCs align well with Gaussian Mixture Models (GMMs) due to their uncorrelated nature, whereas mel-filterbank features are preferred with Deep Neural Networks (DNNs), which perform better with correlated inputs. Optimizing this alignment is crucial for improving sarcasm detection accuracy in speech-based systems. c) Potentials of Large Language Models (LLMs) and speech-based language models: In the audio domain, pretrained models like WavLM  [88]  and SpeechLM  [89] , which build upon Wav2Vec 2.0, utilize self-supervised learning on raw speech data to capture nuanced acoustic and prosodic patterns. While Wav2Vec 2.0 emphasizes phonetic features, WavLM expands the scope by capturing long-range dependencies and prosodic dynamics such as pitch variations and speech rate, which are often associated with sarcastic expressions. Their demonstrated effectiveness in emotion and sentiment recognition  [90]  suggests promising potential for adaptation in sarcasm recognition.\n\nIn the text modality, sarcasm often arises from the semantic incongruity, where the literal meaning of an utterance contrasts with its intended message, typically requiring contextual and world knowledge to interpret. LLMs such as ChatGPT 10  , GPT-4  [91] , Claude 3  11  , and LLaMA  [92]  excel at modeling such nuances through pre-training on massive corpora. However, challenges remain in enhancing LLMs' grasp of emotional tone, cultural context, and implicit social signals, which are factors critical to accurately recognizing sarcasm  [93] .\n\nMoreover, recent advancements in multimodal learning have introduced transformer architectures that jointly pre-train across modalities, learning unified representations that capture cross-modal dependencies. For instance, CLIP  [94]  aligns visual and textual modalities within a shared embedding space, improving interpretability and semantic alignment. Extending such approaches to incorporate the audio modality allows models to jointly learn linguistic, visual, and prosodic cues, facilitating more robust and generalizable sarcasm recognition.\n\nd) Insufficient research in the visual modality: While DNNs, especially CNNs, have exhibited remarkable efficacy within the domain of video-based sarcasm recognition, the need for a more profound understanding of the relationship between visual features and other modality features in interpreting sarcasm remains a critical area of inquiry. Linguistic research has highlighted the interplay between audio and visual cues in sarcasm perception, indicating that sarcastic meanings are often more readily discerned through visual cues than audio cues  [95] ,  [96] . However, further investigation is essential to unravel the complex interdependencies among cues from different modalities, which could provide valuable insights for the computational modeling of multimodal data. Additionally, improving the interpretability of these techniques is crucial for enhancing transparency and fostering continued progress in the field.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "C. Evolution Of Classification Methods Rq3:",
      "text": "As the field progresses from unimodal to multimodal sarcasm recognition, how have classification methods evolved? What are the constraints of current classification methods? 1) Multimodal sarcasm recognition: For the unimodal audio-only setting, classification method simply refers to the learning algorithm chosen to classify the speech features into sarcasm or non-sarcasm. This includes basic rule-based classifiers such as Decision Trees and traditional machine learningbased classifiers like GMM, HMM, Logistic Regression, SVM, etc. Research in this area primarily focuses on identifying the most relevant sarcasm features to input into these algorithms.\n\nThe transition to multimodal approaches has led to a leap in recognition accuracy, however, this advancement comes with increased complexity. The fusion of features from multiple modalities presents the core challenges for researchers due to their distinct characteristics, which necessitate careful consideration of how these diverse data sources are integrated. The core of multimodal sarcasm recognition lies in optimizing feature fusion strategies to enhance overall performance. Our review presents a variety of fusion strategies, ranging from foundational methods like the encoder-decoder approach to more sophisticated techniques, such as attention mechanisms. As the domain continues to evolve, more intricate fusion methodologies, such as quantum-based fusion, are being explored, further pushing the boundaries of sarcasm recognition technology.\n\nInconsistent evaluation metrics in prior studies have hindered meaningful comparisons of feature usage and classification methods. F1-score has identified as the preferred metric as it offers a balanced assessment of recall (sarcastic samples detected) and precision (reliability of detections). Future research is suggested to involve F1-score as a metric to enhance consistency and comparability across studies.\n\n2) Limitations of current methods and future research: a) Unexplored multimodal techniques: While advanced multimodal fusion techniques have achieved significant success in sarcasm recognition, several state-of-the-art DNNs have yet to be fully explored in this domain. For instance, Graph Neural Networks (GNNs), which are designed to process graph-structured data, have shown remarkable potential in aggregating information from neighboring nodes, enabling the fusion of spatially localized features across modalities. This unique ability to capture complex relationships within graph data suggests that GNNs could be highly effective in sarcasm recognition tasks, particularly in integrating visual and audio data. Another promising yet underutilized approach is Generative Neural Networks (GenNNs), which include models such as Generative Adversarial Networks (GANs) and diffusion-based models. The primary objective of GenNNs is to generate data that closely mirrors real-world distributions, either by directly modeling these distributions or by learning transformations from simpler distributions to more complex ones. Their versatility in generating high-quality data has made them a popular choice in both unimodal and multimodal tasks, tackling challenges such as data augmentation, imputation, and fusion. In the context of sarcasm detection, where labeled data is often scarce, GenNNs could offer a powerful solution by augmenting and enhancing existing datasets.\n\nb) Limited linguistic insights: The review of previous multimodal research has underscored the critical role that linguistic insights play in guiding computational modeling for sarcasm recognition. There exists a consensus among the selected works that textual, audio, and visual modalities collectively contribute to the overarching sarcasm classification task. However, the relative effectiveness of different combinations of modality remains inconsistent between studies. For example, Castro et al.  [21]  and Ray et al.  [29]  reported that combining textual and visual modalities led to better performance compared to audio-based combinations, whereas Chauhan et al.  [27]  and Zhang et al.  [47]  found that audiovisual combinations were more effective. These differences reflect the underlying linguistic insights. For instance, according to Camp's  [87]  taxonomy of sarcasm subtypes, propositional sarcasm involves implicit sentiment that relies on contextual interpretation; therefore, text models capable of capturing long-range dependencies perform better in this subtype. In contrast, illocutionary sarcasm is primarily conveyed through non-verbal signals such as prosody and facial expressions, highlighting the importance of audio and visual modalities in detecting this type of sarcasm. Embedded sarcasm, characterized by semantic incongruity, is most effective through textual features. Therefore, the integration of modalities should be guided by linguistic insights, ensuring that the models capture the nuanced ways sarcasm is expressed speech, text, and visual cues.\n\nAn integrated linguistic-computational framework is still lacking in current research. Li et al.  [97]  found that native Mandarin speakers tend to rely more on visual cues in interpreting sarcastic speech, while Bromberek-Dyzman et al.  [98]  observed faster sarcasm recognition in audio and audio-visual modalities for bilingual participants, suggesting a facilitative role for prosody in this case. However, Deliens et al.  [99]  reported the opposite that sarcastic tone did not aid interpretation in English, and that facial expressions could even hinder it. These contradictions point to the need for crosslinguistic and cross-cultural studies that explore how different modalities interact in sarcasm perception and modeling.",
      "page_start": 13,
      "page_end": 16
    },
    {
      "section_name": "D. Challenges, Limitations And Practical Implications",
      "text": "RQ4.1: What are the primary challenges and limitations in the current development of sarcasm recognition technology? In the preceding discussion, we outlined several limitations in the development of sarcasm recognition, including data collection and annotation, feature extraction, and the techniques employed in classification models. However, at a broader level, the field continues to face challenges and limitations that need to be addressed in future research.\n\n1) Cross-cultural and multilingual application: Our review identified a significant gap in cross-cultural and multilingual sarcasm detection research. Only seven out of 40 reviewed studies (17.5%) addressed non-English languages. However, linguistic studies revealed substantial differences in sarcasm markers across cultures. For instance, in various languages, including English, French, Italian, German, and Cantonese, fundamental frequency (F 0) (i.e., pitch) has been frequently linked to sarcasm  [11] -  [14] ,  [100] ,  [101] . In Cantonese, Italian, and French, sarcasm is often marked by an increase in mean F 0, whereas in German, sarcasm is signaled by a reduction in mean F 0. Except for F 0, reduced speech rate and lengthening of syllables or entire utterances are consistent sarcasm indicators across languages  [11] ,  [100] .\n\nThese variations suggest that sarcasm is language-and culturespecific, reflecting both how speakers produce prosodic cues and how listeners interpret them. However, existing sarcasm recognition systems are predominantly trained on English data, hindering their generalizability across cultures and languages. Developing multilingual datasets offers a viable path toward building more robust systems. In addition, further research on cross-lingual transfer learning could improve the effectiveness of sarcasm recognition in broader contexts.\n\nAnother underexplored area is how multilingual speakers perceive and express sarcasm. Mandler  [102]  suggests that second language learners interpret sarcasm through the lens of their first language knowledge. For instance, Kim et al.  [103]  found that Korean English speakers rely more on nonverbal cues than native Korean speakers, a trend also observed among English speakers. However, these perceptual dynamics are largely absent in current machine learning-based sarcasm detection systems. We highlight this gap as a direction for future research in multilingual speech modeling.\n\n2) Sarcasm recognition beyond text: Traditionally, sarcasm has been recognized as a text-based challenge in natural language processing, typically relying on semantic cues such as contrasts between positive and negative words within a single sentence. Numerous datasets have been developed to support these advancements, facilitating the creation of robust and generalizable models  [17] . In contrast, sarcasm recognition in speech has not received comparable attention. Although multimodal analysis has recently revitalized interest in incorporating speech data, the field still lags behind textbased approaches. Recognizing sarcasm as a purely text-based phenomenon is outdated and inconsistent with findings from linguistic research  [79] ,  [80] ,  [87] ,  [97]  which emphasize the multimodal nature of sarcasm. To bridge this gap, there is a need for more comprehensive research into how different modalities interact. Such research is crucial for developing sophisticated architectures capable of recognizing sarcasm more efficiently and effectively.\n\n3) Explainable and trustworthy AI:\n\nThe rapid advancements in deep learning have led to powerful AI models, yet many of these models suffer from the \"blackbox\" problem, where their decision-making processes are opaque and difficult to interpret. Unlike these models, which rely on vast amounts of data and complex calculations to produce predictions without revealing their inner working mechanisms, explainable AI aims to make the decision-making process transparent and comprehensible. In the context of sarcasm recognition, an explainable AI model allows us to understand how predictions are made, providing insight into the system's logic and enhancing its trustworthiness and accuracy. Research shows that transparency in AI systems fosters greater trust, particularly in critical areas such as healthcare and law enforcement. Furthermore, an interpretable model assists developers and researchers in diagnosing and refining the system by clarifying how it processes information. As sarcasm recognition research progresses, it is essential to incorporate explanations that align with linguistic theories of sarcasm cues. This focus on explainability will lead to more transparent, accountable, and reliable AI systems. xvii RQ4.2: What are the practical implications in real-world scenarios? Sarcasm, a commonly employed figure of speech, enables individuals to convey dissatisfaction or critique while conforming to social norms, posing a challenge in NLP. As we strive to enhance human-centered interactions with machines, the ability to recognize and understand sarcasm is crucial. Mastering sarcasm opens the door to addressing even more intricate aspects of human language, including situational irony, metaphor, hyperbole, euphemism, antithesis, and other forms of implicit expression. By tackling sarcasm effectively, we pave the way for more sophisticated and nuanced language understanding in AI systems.\n\nIn real-world deployments, the choice between unimodal and multimodal sarcasm recognition is guided by application context, resource constraints, and performance requirements. Unimodal approaches are often favored for their computational efficiency, which makes them ideal for resource-constrained environments such as social media monitoring (e.g., scanning tweets or online comments for customer sentiment) and lightweight chatbot integrations on websites to enhance customer service. However, these models struggle when sarcasm is conveyed through tone or facial expressions. In contrast, multimodal approaches that fuses textual, audio, and visual cues, offer higher systems accuracy by capturing enriched cross-modal relationships. For example, a voice assistant (e.g., Alexa, Google Assistant) equipped with sarcasm detection can use both words and tone to avoid misinterpreting a frustrated \"sure, that's just perfect\" as genuine praise. In clinical context, multimodal models can help detect sarcasm or distress more reliably by combining speech with facial expressions. Video content moderators on platforms like YouTube or TikTok can deploy multimodal systems to catch sarcasm that text alone would miss, supporting the identification of hate speech or misinformation effectively. Moreover, individuals with neurodegenerative conditions often face challenges interpreting non-literal language. By highlighting or translating sarcastic remarks into their literal intent, these systems can facilitate social integration for these users. However, multimodal systems often have high computation demands, relying on advances in high-performance hardware. Ultimately, practitioners should balance the trade-off between efficiency and accuracy in light of the specific goals of deployment when developing the system.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "V. Conclusion",
      "text": "This systematic review charts the evolution of speech-based sarcasm recognition, revealing how the field has progressed from simple audio analysis to sophisticated multimodal approaches. Through analysis of 40 papers, we find that while traditional prosodic and spectral features remain fundamental, deep learning architectures, particularly those leveraging attention mechanisms, have transformed how we detect sarcasm in speech. Our findings suggest that no single modality or feature set is sufficient; rather, it is the strategic fusion of acoustic, textual, and visual cues that yields the most robust results. This is evident in the success of attention-based architectures that can capture subtle cross-modal relationships. However, significant challenges remain: current datasets are limited in size and spontaneity, prosodic feature sets lack standardization, and cross-cultural aspects of sarcasm remain underexplored. By bridging linguistics and computational approaches, this review provides a foundation for researchers tackling these challenges while highlighting promising directions for future work.\n\nIn conclusion, this review reveals that while deep learning and multimodal approaches have advanced sarcasm recognition, critical challenges persist. The field stands at an intersection where computational advances in attention mechanisms and feature fusion meet linguistic insights about how sarcasm manifests across modalities and cultures. Future progress requires three key developments: first, larger, more diverse datasets that capture spontaneous sarcastic speech across languages; second, standardized prosodic feature sets informed by linguistic research on sarcasm markers; and third, more sophisticated fusion architectures that can better model the subtle interplay between acoustic, textual, and visual cues. Success in these areas would not only advance our theoretical understanding of sarcasm but also enable practical applications that could significantly impact HMI and assist individuals with pragmatic language difficulties. As sarcasm recognition continues to evolve, maintaining this bridge between linguistic theory and computational practice will be crucial for developing systems that can understand and respond to this nuanced aspect of human communication.",
      "page_start": 17,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: PRISMA-guided systematic review process. The initial search phase",
      "page": 3
    },
    {
      "caption": "Figure 1: During the initial search phase, a total of 9,321",
      "page": 3
    },
    {
      "caption": "Figure 2: Trend of the publications. “Multimodal” approaches include audio",
      "page": 3
    },
    {
      "caption": "Figure 2: offers an overview of the publication years of the",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates a general pipeline. The",
      "page": 3
    },
    {
      "caption": "Figure 3: The overall process of sarcasm recognition.",
      "page": 4
    },
    {
      "caption": "Figure 4: , most of the surveyed studies use TV",
      "page": 4
    },
    {
      "caption": "Figure 4: Distribution of datasets by source, language, and modality.",
      "page": 4
    },
    {
      "caption": "Figure 4: , the language di-",
      "page": 4
    },
    {
      "caption": "Figure 5: Hierarchical taxonomy of the most prominent features.",
      "page": 6
    },
    {
      "caption": "Figure 5: summarizes key trends across unimodal",
      "page": 6
    },
    {
      "caption": "Figure 5: We have classified the applied",
      "page": 6
    },
    {
      "caption": "Figure 5: , under the category Audio (multimodal).",
      "page": 7
    },
    {
      "caption": "Figure 5: that a significant",
      "page": 8
    },
    {
      "caption": "Figure 5: , the visual",
      "page": 8
    },
    {
      "caption": "Figure 6: The overview of classification strategies in unimodal sarcasm",
      "page": 9
    },
    {
      "caption": "Figure 6: provides a comprehensive overview. These utilized classifiers",
      "page": 9
    },
    {
      "caption": "Figure 7: The distribution of current multimodal fusion strategies.",
      "page": 10
    },
    {
      "caption": "Figure 7: presents the",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "cognitive effort\nthan understanding direct expressions [5], [6],"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "Abstract—Sarcasm, a common feature of human communica-",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "primarily\nbecause\nsarcasm transcends\nthe\nliteral\ncontent\nof"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "tion, poses challenges\nin interpersonal\ninteractions and human-",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "the\nspoken words\n[7],\nand\nrelies\non\npragmatic\ncues,\nsuch"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "machine\ninteractions. Linguistic\nresearch\nhas\nhighlighted\nthe",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "as\ninformation from preceding discourse,\nshared knowledge"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "importance of prosodic cues, such as variations in pitch, speaking",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "rate,\nand\nintonation,\nin\nconveying\nsarcastic\nintent. Although",
          ", Matt Coler": "[8], perceptual\nindicators,\nlinguistic\nsignals,\nsocietal norms,"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "previous work has\nfocused on text-based sarcasm detection,\nthe",
          ", Matt Coler": "and even the\nspeaker’s preferences or background [6]. The"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "role of\nspeech data in recognizing sarcasm has been underex-",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "accurate interpretation of sarcasm is vital\nfor effective social"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "plored. Recent\nadvancements\nin speech technology\nemphasize",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "interaction, yet\nit poses significant challenges for\nindividuals"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "the growing importance of\nleveraging speech data for automatic",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "with neurodegenerative conditions, such as semantic dementia,"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "sarcasm recognition, which can enhance\nsocial\ninteractions\nfor",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "individuals with neurodegenerative conditions and improve ma-",
          ", Matt Coler": "or\nfor\nthose on the autism spectrum [9],\n[10]"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "chine understanding\nof\ncomplex human language use,\nleading",
          ", Matt Coler": "The\nimportance of\nspeech prosody in sarcasm perception"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "to more\nnuanced\ninteractions. This\nsystematic\nreview is\nthe",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "has\nbeen\nvalidated\nby\nlinguists\nacross\nlanguages. Previous"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "first\nto focus on speech-based sarcasm recognition, charting the",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "research\nin\nlinguistics\nrelated fields\nhas\npredominantly\nin-"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "evolution from unimodal\nto multimodal\napproaches.\nIt\ncovers",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "vestigated how sarcasm is\nconveyed through various\nsound"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "datasets, feature extraction, and classification methods, and aims",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "to bridge\ngaps\nacross diverse\nresearch domains. The findings",
          ", Matt Coler": "patterns\nin\nlanguage,\nhighlighting\nthe\nintricate\nrelationship"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "include limitations in datasets for sarcasm recognition in speech,",
          ", Matt Coler": "between speech delivery and the intended sarcastic meaning."
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "the\nevolution of\nfeature\nextraction techniques\nfrom traditional",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "For instance, Rockwell [11] observed that\nin English, sarcasm"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "acoustic features to deep learning-based representations, and the",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "is often conveyed through a slower tempo,\nincreased intensity,"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "progression of classification methods from unimodal approaches",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "and a lower pitch. Similarly, Cheang and Pell\n[12]\nfound that"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "to multimodal fusion techniques. In so doing, we identify the need",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "for greater emphasis on cross-cultural and multilingual sarcasm",
          ", Matt Coler": "sarcastic speech in Cantonese is characterized by an elevated"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "recognition, as well as the importance of addressing sarcasm as",
          ", Matt Coler": "pitch,\nreduced intensity, a slower\nrate, and less vocal noise."
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "a multimodal phenomenon, rather than a text-based challenge.",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "Scharrer and Christmann [13]\nidentified sarcasm in German"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "Index Terms—Systematic review,\nsarcasm, multimodal, affec-",
          ", Matt Coler": "by features\nsuch as a lower pitch, heightened intensity, and"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "tive computing,\nsentiment analysis,\nspeech emotion recognition,",
          ", Matt Coler": "extended vowel duration. Loevenbruck et al. [14] revealed that"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "human machine interaction, prosody",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "sarcastic expressions are marked by longer duration, as well"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "as increased pitch level and range. These findings collectively"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "I.\nINTRODUCTION",
          ", Matt Coler": "emphasize the nuanced role of prosodic features in signaling"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "sarcasm across different\nlanguages."
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "S Arcasm is a ubiquitous feature of everyday conversations,",
          ", Matt Coler": "Early efforts in recognizing sarcasm through computational"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "models primarily focused on leveraging speech data for poten-"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "often\nleave\nus\nquestioning\ntheir\nsincerity. This moment\nof",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "tial applications in dialogue systems [15],\n[16]. Sequentially,"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "doubt emerges when we detect subtle cues beyond the spoken",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "researchers have delved into the development of models ca-"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "words,\nsuch as a wink or a trace of\nsarcasm in the melody.",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "pable of\nrecognizing sarcasm in text over\nthe past decades."
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "Adding to its complexity, sarcasm does not always function as",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "Sarcasm text expressions are often described as “noisy” data,"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "a mere polarity switcher;\nit can convey messages that deviate",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "introducing ambiguity that\ncan disrupt\nthe\naccuracy of\nlan-"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "from the literal content\n[1].\nIt operates subtly yet\nremarkably",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "guage models. This\nposes\na\nsignificant\nchallenge\nfor\ntasks"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "effective in a social context. Researchers have highlighted its",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "like Sentiment Analysis\n(SA), which are heavily influenced"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "multifaceted\nrole.\nJorgensen\n[2]\nand Brown\n[3] mentioned",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "by the presence of\nsarcasm. Review works\nin computational"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "sarcasm is\napplied\nto\naddress\ncomplaints\nand\ncriticism to",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "processing of sarcasm have primarily concentrated on text data"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "close\nrelationships\nin\na\nless\nharmful way.\nSeckman\nand",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "sourced from social networks [17]–[19]."
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "Couch [4] claimed that sarcasm fortifies the solidarity within",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "With\nthe\nrapid\ndevelopment\nof\nspeech\ntechnology\nand"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "work\ngroups. Perceiving\nsarcasm,\nhowever,\ndemands more",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "spreading use of voice-assisted devices,\nsuch as Alexa\nand"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "Manuscript\nsubmitted on November 19, 2024;\nrevised on May 24, 2025.",
          ", Matt Coler": "Google Assistant,\nit\nis increasingly important\nto acknowledge"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "(Corresponding author: Xiyuan Gao)",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "the\nrole\nof\nspeech\ndata\nin\nsarcasm recognition.\nSarcasm"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "X.Gao,\nS.Nayak\nand M.Coler\nare\nwith\nCampus\nFryslˆan,\nUniver-",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "",
          ", Matt Coler": "recognition in speech holds the potential\nto assist\nindividuals"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "sity\nof\nGroningen,\nLeeuwarden\n8911\nCE,\nthe\nNetherlands\n(e-mails:",
          ", Matt Coler": ""
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "xiyuan.gao@rug.nl; s.nayak@rug.nl; m.coler@rug.nl)",
          ", Matt Coler": "with neurodegenerative conditions by enhancing their ability"
        },
        {
          "Xiyuan Gao\n, Shekhar Nayak": "This work\nhas\nbeen\nsubmitted\nto\nthe",
          ", Matt Coler": "IEEE Transactions\non Affective Computing\nfor\npossible"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ii": "and in response to reviewer feedback, we conducted a follow-"
        },
        {
          "ii": "up search on 24 Mar. 2025, covering publications up to Dec."
        },
        {
          "ii": "2024. The same methodology and search strings were applied"
        },
        {
          "ii": "to maintain consistency with the initial search protocol."
        },
        {
          "ii": "1) To define the methodology involved in the articles, we"
        },
        {
          "ii": "applied automatic, machine learning, deep-learning;"
        },
        {
          "ii": "2) To include articles that availed a multimodal approach,"
        },
        {
          "ii": "we used multimodal, multi-modal, multimodality;"
        },
        {
          "ii": "3) To allocate the main phenomenon we concern, we in-"
        },
        {
          "ii": "cluded sarcasm,\nirony, detection, recognition;"
        },
        {
          "ii": "4) To define the scope of the applied data type, we indicated"
        },
        {
          "ii": "speech, audio, voice, sound, conversation, dialogue."
        },
        {
          "ii": ""
        },
        {
          "ii": "These terms above were chosen based on their prevalence"
        },
        {
          "ii": ""
        },
        {
          "ii": "in the preliminary search. To ensure alignment with the search"
        },
        {
          "ii": ""
        },
        {
          "ii": "logic employed by the chosen databases, we further combined"
        },
        {
          "ii": ""
        },
        {
          "ii": "these terms as follows in our practical application:"
        },
        {
          "ii": ""
        },
        {
          "ii": "•\n(“sarcasm” OR “irony”) AND (“detection” OR “recog-"
        },
        {
          "ii": ""
        },
        {
          "ii": "nition”) AND (“speech” OR “audio” OR “voice” OR"
        },
        {
          "ii": ""
        },
        {
          "ii": "“sound” OR “conversation”)"
        },
        {
          "ii": ""
        },
        {
          "ii": "•\n(“multimodal”)\nAND (“sarcasm” OR\n“irony”)\nAND"
        },
        {
          "ii": ""
        },
        {
          "ii": "(“detection” OR\n“recognition”)\nAND (“speech” OR"
        },
        {
          "ii": ""
        },
        {
          "ii": "“audio” OR “voice” OR “sound” OR “conversation”"
        },
        {
          "ii": ""
        },
        {
          "ii": "OR “dialogue”)"
        },
        {
          "ii": ""
        },
        {
          "ii": "•\n(“machine\nlearning” OR “deep\nlearning” OR “auto-"
        },
        {
          "ii": ""
        },
        {
          "ii": "matic”) AND (“sarcasm” OR “irony”) AND (“detec-"
        },
        {
          "ii": ""
        },
        {
          "ii": "tion” OR “recognition”) AND (“speech” OR “audio”"
        },
        {
          "ii": ""
        },
        {
          "ii": "OR “voice” OR “sound” OR “conversation” OR “dia-"
        },
        {
          "ii": ""
        },
        {
          "ii": "logue”)"
        },
        {
          "ii": ""
        },
        {
          "ii": "When conducting search in the mentioned databases, we"
        },
        {
          "ii": ""
        },
        {
          "ii": "applied a\nfew search limits\nto exclude\narticles beyond our"
        },
        {
          "ii": ""
        },
        {
          "ii": "research\nscope.\nFirst, we\nlimited\nthe\nresearch\ndomain\nto"
        },
        {
          "ii": ""
        },
        {
          "ii": "computer science. Second,\nthe article type was defined to be"
        },
        {
          "ii": ""
        },
        {
          "ii": "empirical\nstudy. Third, only articles written in English were"
        },
        {
          "ii": ""
        },
        {
          "ii": "included. Fourth,\nthe\nselected\narticles\nshould\nbe\npublished."
        },
        {
          "ii": ""
        },
        {
          "ii": "Initially, we\nlimited\nthe\npublication window to\nthe\nperiod"
        },
        {
          "ii": ""
        },
        {
          "ii": "from 2006 to 2023. In the follow-up search in Mar. 2025, we"
        },
        {
          "ii": ""
        },
        {
          "ii": "extended the publication window to include articles published"
        },
        {
          "ii": ""
        },
        {
          "ii": "in 2024. Due to database constraints that allow filtering by year"
        },
        {
          "ii": ""
        },
        {
          "ii": "but not by month, our follow-up search included the full year"
        },
        {
          "ii": ""
        },
        {
          "ii": "of 2023, which overlapped with the initial\nsearch conducted"
        },
        {
          "ii": ""
        },
        {
          "ii": "in Aug. 2023. To address this, we combined the records from"
        },
        {
          "ii": "both searches and conducted deduplication prior to screening."
        },
        {
          "ii": ""
        },
        {
          "ii": "To gather\nthe eligible articles, a number of\ninclusion and"
        },
        {
          "ii": ""
        },
        {
          "ii": "exclusion criteria were defined for both the abstract and full-"
        },
        {
          "ii": ""
        },
        {
          "ii": "text screening processes:"
        },
        {
          "ii": ""
        },
        {
          "ii": "1) The selected papers are in the computer science domain."
        },
        {
          "ii": ""
        },
        {
          "ii": "2) The selected papers are empirical\nstudy instead of\nre-"
        },
        {
          "ii": ""
        },
        {
          "ii": "view,\nreport, or commentaries."
        },
        {
          "ii": ""
        },
        {
          "ii": "3) The\nselected\npapers\ncontain\nsarcasm\ndetec-"
        },
        {
          "ii": ""
        },
        {
          "ii": "tion/recognition in speech data."
        },
        {
          "ii": ""
        },
        {
          "ii": "4) The selected papers\ninclude a machine learning based"
        },
        {
          "ii": ""
        },
        {
          "ii": "method."
        },
        {
          "ii": ""
        },
        {
          "ii": "5) The selected papers are written in English."
        },
        {
          "ii": ""
        },
        {
          "ii": "6) Any\nunrelated,\nduplicated,\nunavailable\nfull-texts,"
        },
        {
          "ii": ""
        },
        {
          "ii": "abstract-only papers will be removed."
        },
        {
          "ii": ""
        },
        {
          "ii": "From the selected articles, we conducted a structured data"
        },
        {
          "ii": "analysis process. First, we stored the articles in Zotero for easy"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "with text, audio with articulatory features, or audio with both text and visual"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "modalities, while “Unimodal” refers to audio-only approaches."
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "we conducted a follow-up search phase on 24th Mar. 2025,"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "collecting articles\nfrom 2023 to 2024. This\nsearch identified"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "2,756\nrecords,\nfrom which\n1,223\nremained\nafter\napplying"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "search limits. After conducting cross-phase deduplication, 441"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "unique\nrecords\nremained. At\nlast, 11 articles were gathered"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "after\nfull-text\nscreening\nand\none was\nadded\nvia\nreference"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "chaining, creating an additional 12 studies. In total, 40 articles"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "were included in the final synthesis."
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "Figure 2 offers an overview of\nthe publication years of\nthe"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "selected articles. A noteworthy trend emerges: prior\nto 2019,"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "there were sporadic publications related to sarcasm recognition"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "in speech. However, an increase is evident after 2019, with the"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "majority of\nrecent\narticles\nadopting a multimodal\napproach."
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "Except\nfor\nthe\nemerging research interest\nin multimodality,"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "this\nsurge in research can be attributed to the release of\nthe"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "MUStARD dataset [21] in 2019, which catalyzed interest and"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "innovation in multimodal sarcasm recognition. The availability"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "of\nsuch resources has\nfueled new research directions\nin the"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "field."
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "To provide\na unified view of\nthe overall process of\nsar-"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "casm recognition, Figure 3 illustrates a general pipeline. The"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "pipeline begins with a multimodal dataset\nthat\ntypically com-"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "prises audio,\ntextual, and visual signals. These inputs undergo"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "preprocessing\nsteps\nsuch\nas\nnormalization,\nnoise\nremoval,"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": ""
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "and\nsignal\nalignment. Next, modality-specific\nfeatures\nare"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "extracted,\nfor example, prosodic cues like pitch and duration"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "from audio;\nlexical\nand\ncontextual\nembeddings\nfrom text;"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "and\nvisual\ncues\nsuch\nas\neye\ngaze\nand\nfacial\nexpressions."
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "The\nsystem then proceeds\nto classification via\ntwo distinct"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "paths:\nunimodal\nand multimodal. With\na\nspecial\nfocus\non"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "speech,\nthe\nunimodal\nclassification\nrelies\non\nfeatures\nfrom"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "the audio modality, employing traditional machine learning or"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "deep learning models. In contrast, multimodal classification in-"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "tegrates features across modalities through appropriate fusion"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "strategies before passing them to a final decision layer. The"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "final output\nis typically a binary label\nindicating whether\nthe"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "input\ninstance\nis\nsarcastic or not.\nIn the\nfollowing content,"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "we\nfollow the\npattern\nof\ndatasets,\nfeature\nextraction,\nand"
        },
        {
          "Fig. 2.\nTrend of\nthe publications. “Multimodal” approaches\ninclude audio": "classification to present\nthe analysed results."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "b) Language: As\nshown in Figure 4,\nthe\nlanguage di-"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "versity within\nthese\ndatasets\nis\nlimited, with\nthe\nexception"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "of\nIITKGP-SEHSC (Hindi)\n[22], MaSaC (Hindi and English)"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "[23], Spanish Dataset\n(Spanish)\n[24], and CMMA (Chinese)"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "[25]. The majority of\nthe datasets are available exclusively in"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "English."
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "c) Modality:\nIITKGP-SEHSC\nand\nIrony-Recognition"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "[26]\nexclusively\ncontain\naudio\ndata, while\nthe\nremaining"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "datasets are meticulously curated to facilitate multimodal (text,"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "audio, visual) sarcasm recognition."
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "d) Labeling:\nSEmoji MUStARD [27]\nand\nSEEmoji"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "MUStARD [28]\nare\nextensions\nof\nthe\noriginal MUStARD."
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "MUStARD++ [29]\nare\nsourced\nfrom both MUStARD and"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "MELD [30]. The\nextensions\nprimarily\nfocus\non\nenhancing"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "the\nlabels,\ntransitioning\nfrom a mere\nbinary\nclassification"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "(i.e.,\nsarcasm and\nnon-sarcasm)\nto\na more\ncomprehensive"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "representation of associated sentiments and emotions. Beyond"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "assigning affective\nlabels, CMMA provides\nlabels\ncapturing"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "the\nrelevance between sentiment\nand emotion,\nand between"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "sarcasm and humor. These labels are represented on a five-"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "level\nscale:\n[-2,\n-1, 0, 1, 2],\nindicating negative, neutral, and"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "positive\nrelevance. This\nenables\ninformed selection of main"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "and auxiliary tasks in multi-task learning models."
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "e) Context: Context\nis defined as the preceding dialogue"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "turns of\na\nlabeled utterance. Given the pragmatic nature of"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": ""
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "sarcasm,\nlinguists have posited that\ncontext plays\na\ncrucial"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "role\nin\nreducing\nambiguity\nin\nsarcasm interpretation\n[31]–"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "[33]. Consistent with this view, several datasets collected for"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "sarcasm research include\ncontextual\ninformation along with"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "the labeled utterances [21],\n[25],\n[27]–[29]."
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "f)\nSpeaker information:\nIn the aspect of sarcasm percep-"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "tion, Fan et al.\n[34]\nfound that anticipation of sarcastic intent"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "is\ncrucial\nfor\nthe\nefficient\ncomprehension of\nsarcasm. That"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "is\nto say, when sarcasm is\na\nrecurring style of\na particular"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "speaker,\nthe\nlikelihood of\nsarcastic\nexpression from that\nin-"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "dividual\nincreases,\nthereby speaker\ninformation significantly"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "shapes\nthe\ninterpretation\nof\nthe\nstatements.\nIn\nour\nreview,"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "speaker\ninformation has been leveraged by researchers\n[21],"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "[27]–[29],\n[35]\nto establish two evaluation settings:\nspeaker-"
        },
        {
          "Fig. 4. Distribution of datasets by source,\nlanguage, and modality.": "dependent and speaker-independent. In the speaker-dependent"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Labels"
        },
        {
          "TABLE I": "Anger, disgust,\nfear, happiness, sadness, neutrality,"
        },
        {
          "TABLE I": "sarcasm, surprise"
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm"
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm,\nimplicit/explicit sentiment, disgust,"
        },
        {
          "TABLE I": "happy, surprised, neutral, anger"
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm, sarcasm sub-types"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm,\nimplicit/explicit sentiment, disgust,"
        },
        {
          "TABLE I": "happy, surprised, neutral, anger, emojis"
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Sarcasm, sarcasm sub-types, non-sarcasm,\nimplicit/explicit"
        },
        {
          "TABLE I": "sentiments, valence and arousal\nrating, excitement,\nfear,"
        },
        {
          "TABLE I": "sad,\nfrustrated, disgust, happy, surprised,\nridicule, neutral,"
        },
        {
          "TABLE I": "anger"
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm, humorous, non-humorous"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Sarcasm, non-sarcasm, humorous, non-humorous,"
        },
        {
          "TABLE I": "sentiments (positive, negative, neutral), emotions (joy,"
        },
        {
          "TABLE I": "sadness, anger,\nfear, surprise, disgust, neutral), pride,\nlove,"
        },
        {
          "TABLE I": "sentiment-emotion and sarcasm-humor\ninter-relatedness"
        },
        {
          "TABLE I": "measures"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "measures": "a T indicates text, A indicates audio, and V indicates video."
        },
        {
          "measures": "setting, utterances\nfrom the same speakers are used in both"
        },
        {
          "measures": "the training and testing sets, whereas the speaker-independent"
        },
        {
          "measures": "setting involves different\nspeakers\nin the training and testing"
        },
        {
          "measures": "sets."
        },
        {
          "measures": "g) Size:\nSarcasm datasets are typically limited in size,"
        },
        {
          "measures": "with\nonly\none\nlarge\ndataset\nencompassing\n15.2\nhours\nof"
        },
        {
          "measures": "video that\nincludes both sarcastic and non-sarcastic instances"
        },
        {
          "measures": "[25]. This constraint\nreflects\nthe challenge in the annotation"
        },
        {
          "measures": "sarcasm, particularly in achieving consensus among different"
        },
        {
          "measures": "annotators. Despite this challenge,\nthere is a noticeable trend"
        },
        {
          "measures": "of gradual expansion in the size of\nthese datasets over years,"
        },
        {
          "measures": ""
        },
        {
          "measures": "reflecting ongoing efforts to accumulate more comprehensive"
        },
        {
          "measures": "data sets for sarcasm research."
        },
        {
          "measures": "2) Dataset development: The creation of a sarcasm dataset"
        },
        {
          "measures": "typically involves\nthe following stages: data collection, data"
        },
        {
          "measures": "preprocess and data annotation. Each of\nthem plays a crucial"
        },
        {
          "measures": "role in shaping the quality and usability of the resultant dataset."
        },
        {
          "measures": "a) Data collection:\nThis\ninitial phase\ninvolves\nthe\nac-"
        },
        {
          "measures": "quisition of\nraw data from a diverse array of sources.\nIn our"
        },
        {
          "measures": "findings, examples of acted speech datasets, such as IITKGP-"
        },
        {
          "measures": "SEHSC and the dataset created by Geng et al.\n[36],\ninvolve"
        },
        {
          "measures": "trained vocal professionals who generate utterances, based on"
        },
        {
          "measures": "carefully designed stimuli. Notably, our\nreview highlights\na"
        },
        {
          "measures": "growing\ntrend\nof\nutilizing TV series\nas\na\nsource\nof\nacted"
        },
        {
          "measures": "speech data. Although TV series involve acted performances,"
        },
        {
          "measures": "the actors often strive to replicate real-life scenarios, resulting"
        },
        {
          "measures": "in datasets that\ntend to exhibit a more natural conversational"
        },
        {
          "measures": "et\ntone. Another\ntype\nof\ndata\nsource\nis\nused\nby Gent\nal."
        },
        {
          "measures": "[26],\nthey collected 4.68 hours of speech data from a podcast."
        },
        {
          "measures": ""
        },
        {
          "measures": "Innovatively, Burkhardt et al.\n[37] built an interactive mobile"
        },
        {
          "measures": ""
        },
        {
          "measures": "phone-based interface to collect sarcastic speech. Users were"
        },
        {
          "measures": "guided to record their sarcastic reaction to visual stimuli and"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vi": "coefficients\n(MFCCs). Six out of\nthe eight unimodal\nstudies"
        },
        {
          "vi": "reviewed have utilized this\nfeature in their analysis. MFCCs"
        },
        {
          "vi": "are\nrepresentations\nof\nthe\nshort-term power\nspectrum of\na"
        },
        {
          "vi": "sound signal, and it\nis derived from the Mel\nfrequency scale,"
        },
        {
          "vi": "which is a perceptually motivated frequency scale that better"
        },
        {
          "vi": "represents\nthe way\nhumans\nperceive\nthe\npitch\nof\nsounds."
        },
        {
          "vi": "Therefore,\nthey\nare well-suited\nfor\ntasks\ninvolving\nhuman"
        },
        {
          "vi": "speech and sound recognition. More importantly, MFCCs offer"
        },
        {
          "vi": "a\nrobust\nrepresentation\nof\nspectral\nattributes within\nspeech"
        },
        {
          "vi": "et\nsignals. For\ninstance, Tepperman\nal.\n[15]\nemployed\nthe"
        },
        {
          "vi": "first\ntwelve MFCCs\nalong with their delta\nand acceleration"
        },
        {
          "vi": "coefficients."
        },
        {
          "vi": "An alternative widely adopted choice is the Mel spectrogram"
        },
        {
          "vi": "(MelSpec), a time-frequency representation of audio employ-"
        },
        {
          "vi": "ing the Mel frequency scale. Gao et al. [42] utilized MelSpecs"
        },
        {
          "vi": "as\ninput\nfeatures\nfor a convolutional neural network (CNN)-"
        },
        {
          "vi": "based architecture. This\nrepresentation is highlighted for ef-"
        },
        {
          "vi": "fective\nin capturing critical\nspectral nuances. Like MFCCs,"
        },
        {
          "vi": "MelSpecs are based on the Mel\nscale, making them suitable"
        },
        {
          "vi": "for\nspeech and audio processing tasks. But\nthey have higher"
        },
        {
          "vi": ""
        },
        {
          "vi": "dimensionality because of\nthe\nretained spectral\ninformation"
        },
        {
          "vi": ""
        },
        {
          "vi": "in\neach\nframe.\nFurthermore,\nthe\napplication\nof\nPerceptual"
        },
        {
          "vi": ""
        },
        {
          "vi": "Linear Predictive\n(PLPs)\nfeatures has been noted in Atassi"
        },
        {
          "vi": ""
        },
        {
          "vi": "et\nal.\n[38],\nleveraging\ntheir\nspecialization\nin mirroring\nthe"
        },
        {
          "vi": ""
        },
        {
          "vi": "perceptual\ncharacteristics of\nspeech as perceived by human"
        },
        {
          "vi": ""
        },
        {
          "vi": "listeners.\nIn addition, a few studies\n[37],\n[43] have extended"
        },
        {
          "vi": ""
        },
        {
          "vi": "their\nfeature\nset\nto encompass\nadditional\nspectral\nattributes,"
        },
        {
          "vi": ""
        },
        {
          "vi": "including chroma, Spectral Contrast\n(SCt), Spectral Rolloff"
        },
        {
          "vi": ""
        },
        {
          "vi": "(SR), Spectral Bandwidth (SBW), Tonnetz, and Zero Crossing"
        },
        {
          "vi": "Rate (ZCR). They demonstrated the effectiveness of applying"
        },
        {
          "vi": ""
        },
        {
          "vi": "spectral\nfeatures in sarcasm recognition."
        },
        {
          "vi": "b) Prosodic features: The features\nrefer\nto the acoustic"
        },
        {
          "vi": "characteristics of\nspeech related to its\nrhythm, melody,\nand"
        },
        {
          "vi": "intonation rather\nthan its\nlinguistic content. Koolagudi et al."
        },
        {
          "vi": "[22]\ndivided\nprosodic\nfeatures mainly\nin\nthree\ncategories:"
        },
        {
          "vi": "pitch,\nintensity\nand\nintonation. Rooted\nin\nvariations\nin\nair"
        },
        {
          "vi": "pressure within\nthe\nvocal\nfold,\nprosodic\nfeatures\nserve\nas"
        },
        {
          "vi": "valuable\nindicators\nof\nhuman\nemotions\n[44]. Many\nstudies"
        },
        {
          "vi": "collectively applied statistical metrics such as mean, minimum,"
        },
        {
          "vi": "maximum,\nrange, standard deviation to prosodic features, af-"
        },
        {
          "vi": "firming their fundamental role in the context of sarcasm recog-"
        },
        {
          "vi": "nition in speech. For\nexample, Atassi\net al.\n[38]\nemployed"
        },
        {
          "vi": "pitch contours, while Rakov and Rosenberg [16] incorporated"
        },
        {
          "vi": "word-level pitch and intensity contours, as well as\nsentence-"
        },
        {
          "vi": "level statistical measures of pitch,\nintensity, and speaking rate."
        },
        {
          "vi": "Mathur et al. [45] and Arun et al. [39] utilized a combination"
        },
        {
          "vi": "of spectral and prosodic features in their analyses."
        },
        {
          "vi": "c) Voice quality features: The features are recognized as"
        },
        {
          "vi": "the individual voice characteristic, including bandwidth, glottal"
        },
        {
          "vi": "attributes, harmonic-to-noise ratio (HNR),\njitter, and shimmer."
        },
        {
          "vi": "These\nfeatures describe\nthe manner\nin which an individual"
        },
        {
          "vi": "produces\nspeech,\nrelating to aspects of vocal cord vibration"
        },
        {
          "vi": "and the resultant acoustic output. To enhance feature represen-"
        },
        {
          "vi": "tation, voice quality features are often combined with spectral"
        },
        {
          "vi": "and prosodic features. For example,\nin addition to the spectral"
        },
        {
          "vi": "and prosodic feature sets mentioned ealier, Atassi et al.\n[38]"
        },
        {
          "vi": "employed a voice quality feature set that included harmonicity,"
        },
        {
          "vi": "as well as the frequencies and bandwidths of the first three for-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "glottal\nfeatures": ""
        },
        {
          "glottal\nfeatures": ""
        },
        {
          "glottal\nfeatures": ""
        },
        {
          "glottal\nfeatures": "e) Extraction toolkits: Several\ntoolkits have been devel-"
        },
        {
          "glottal\nfeatures": "oped for extracting speech features from audios; Table II lists"
        },
        {
          "glottal\nfeatures": "the\ntoolkits\nthat\nare mentioned in the\nselected papers. The"
        },
        {
          "glottal\nfeatures": "choice of\nan audio analysis\ntoolkit depends on the\nspecific"
        },
        {
          "glottal\nfeatures": "sarcasm-related\nspeech\nfeatures\nbeing\nanalyzed. Praat\n[63]"
        },
        {
          "glottal\nfeatures": "excels\nin\ndetailed\nand\naccurate\nphonetic\nanalysis,\noffering"
        },
        {
          "glottal\nfeatures": "features\nfor pitch,\nformants,\nintensity,\nand spectral\nanalysis,"
        },
        {
          "glottal\nfeatures": "making\nit\nideal\nfor\ndetecting\ntonal\nvariations\nin\nsarcasm."
        },
        {
          "glottal\nfeatures": "openSMILE [64], designed for real-time processing, efficiently"
        },
        {
          "glottal\nfeatures": "extracts\na\nbroad\nrange\nof\nemotion-related\nfeatures, making"
        },
        {
          "glottal\nfeatures": "it well-suited\nfor\nsarcasm-related\nemotional\ncues\nlike\ntone"
        },
        {
          "glottal\nfeatures": "and intensity. Librosa [65]\nintegrates easily with Python and"
        },
        {
          "glottal\nfeatures": "simplifies complex audio analyses, but it lacks sarcasm-related"
        },
        {
          "glottal\nfeatures": "speech\nfeatures\nsuch\nas\nspeech\nrate. A recent\naddition\nto"
        },
        {
          "glottal\nfeatures": "the\ntoolkit\ninventory is COVAREP [66], which is\nan open-"
        },
        {
          "glottal\nfeatures": "source toolkit\nthat\nresearchers can leverage to collect a rich"
        },
        {
          "glottal\nfeatures": "set of features, including pitch, intensity, and spectral features."
        },
        {
          "glottal\nfeatures": "Each toolkit has its strengths and limitations, so choosing the"
        },
        {
          "glottal\nfeatures": "appropriate\ntool\nshould be guided by the\nspecific\nsarcasm-"
        },
        {
          "glottal\nfeatures": "related speech features being analyzed."
        },
        {
          "glottal\nfeatures": "f) Level of\nfeature\nextraction: Among the\nselected ar-"
        },
        {
          "glottal\nfeatures": "ticles,\ntwo distinct\ncategories of\nfeature\nlevel\nemerge:\nlocal"
        },
        {
          "glottal\nfeatures": "features and global\nfeatures. Local\nfeatures are characterized"
        },
        {
          "glottal\nfeatures": "by\ntheir\nshort-term nature,\ncomputed within\nsmall,\noften"
        },
        {
          "glottal\nfeatures": "overlapping time windows in an audio signal. These features"
        },
        {
          "glottal\nfeatures": "excel\nin capturing dynamic changes and nuanced variations in"
        },
        {
          "glottal\nfeatures": "speech, as exemplified in the work of Tepperman et al.\n[15]."
        },
        {
          "glottal\nfeatures": "Conversely, global features are computed over more extensive"
        },
        {
          "glottal\nfeatures": "segments of an audio signal, typically encompassing the entire"
        },
        {
          "glottal\nfeatures": "speech utterance. These\nfeatures\ncapture\ncharacteristics\nthat"
        },
        {
          "glottal\nfeatures": "manifest over a more extended period, and a number of studies"
        },
        {
          "glottal\nfeatures": "[38],\n[39],\n[42],\n[45] have employed global\nfeatures\nin their"
        },
        {
          "glottal\nfeatures": "experiments.\nIt’s\nnoteworthy\nthat\na\ncommon\npractice\nis\nto"
        },
        {
          "glottal\nfeatures": "initially extract\nlocal features from individual windows within"
        },
        {
          "glottal\nfeatures": "an utterance and subsequently aggregate them across the entire"
        },
        {
          "glottal\nfeatures": "utterance. Nevertheless,\nit’s also applicable to combine these"
        },
        {
          "glottal\nfeatures": "two types of\nfeatures [16],\n[26]."
        },
        {
          "glottal\nfeatures": "Our\nreview reveals\na\nnotable\ninclination\ntoward\nglobal"
        },
        {
          "glottal\nfeatures": "features among researchers. This preference can be attributed"
        },
        {
          "glottal\nfeatures": "to the comparative computational efficiency of global features,"
        },
        {
          "glottal\nfeatures": "as they involve fewer parameters than their local counterparts."
        },
        {
          "glottal\nfeatures": "However,\nit’s essential\nto acknowledge that extracting global-"
        },
        {
          "glottal\nfeatures": "level features comes at\nthe cost of disregarding word-level re-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vii": "TABLE II"
        },
        {
          "vii": "TOOLKITS FOR SPEECH FEATURE EXTRACTION"
        },
        {
          "vii": ""
        },
        {
          "vii": ""
        },
        {
          "vii": "Toolkit\nFeatures extracted\nCoding language"
        },
        {
          "vii": ""
        },
        {
          "vii": "Praat\nSpectral, pitch contour,\nformant\nPython, C++,"
        },
        {
          "vii": "contours,\nintensity contour,\nPraat script"
        },
        {
          "vii": "voice quality"
        },
        {
          "vii": ""
        },
        {
          "vii": "OpenSMILE\nLow-level and high-level\nPython, C++"
        },
        {
          "vii": ""
        },
        {
          "vii": "spectral,\ntemporal,\nintensity,"
        },
        {
          "vii": ""
        },
        {
          "vii": "pitch, voice quality"
        },
        {
          "vii": ""
        },
        {
          "vii": "Librosa\nSpectral,\ntemporal, harmonics,\nPython"
        },
        {
          "vii": "intensity, pitch"
        },
        {
          "vii": "COVAREP\nSpectral envelop, pitch tracking,\nMATLAB"
        },
        {
          "vii": "glottal\nfeatures"
        },
        {
          "vii": ""
        },
        {
          "vii": ""
        },
        {
          "vii": ""
        },
        {
          "vii": "e) Extraction toolkits: Several\ntoolkits have been devel-"
        },
        {
          "vii": "oped for extracting speech features from audios; Table II lists"
        },
        {
          "vii": "the\ntoolkits\nthat\nare mentioned in the\nselected papers. The"
        },
        {
          "vii": "choice of\nan audio analysis\ntoolkit depends on the\nspecific"
        },
        {
          "vii": "sarcasm-related\nspeech\nfeatures\nbeing\nanalyzed. Praat\n[63]"
        },
        {
          "vii": "excels\nin\ndetailed\nand\naccurate\nphonetic\nanalysis,\noffering"
        },
        {
          "vii": "features\nfor pitch,\nformants,\nintensity,\nand spectral\nanalysis,"
        },
        {
          "vii": "making\nit\nideal\nfor\ndetecting\ntonal\nvariations\nin\nsarcasm."
        },
        {
          "vii": "openSMILE [64], designed for real-time processing, efficiently"
        },
        {
          "vii": "extracts\na\nbroad\nrange\nof\nemotion-related\nfeatures, making"
        },
        {
          "vii": "it well-suited\nfor\nsarcasm-related\nemotional\ncues\nlike\ntone"
        },
        {
          "vii": "and intensity. Librosa [65]\nintegrates easily with Python and"
        },
        {
          "vii": "simplifies complex audio analyses, but it lacks sarcasm-related"
        },
        {
          "vii": "speech\nfeatures\nsuch\nas\nspeech\nrate. A recent\naddition\nto"
        },
        {
          "vii": "the\ntoolkit\ninventory is COVAREP [66], which is\nan open-"
        },
        {
          "vii": "source toolkit\nthat\nresearchers can leverage to collect a rich"
        },
        {
          "vii": "set of features, including pitch, intensity, and spectral features."
        },
        {
          "vii": "Each toolkit has its strengths and limitations, so choosing the"
        },
        {
          "vii": "appropriate\ntool\nshould be guided by the\nspecific\nsarcasm-"
        },
        {
          "vii": "related speech features being analyzed."
        },
        {
          "vii": "f) Level of\nfeature\nextraction: Among the\nselected ar-"
        },
        {
          "vii": "ticles,\ntwo distinct\ncategories of\nfeature\nlevel\nemerge:\nlocal"
        },
        {
          "vii": "features and global\nfeatures. Local\nfeatures are characterized"
        },
        {
          "vii": "by\ntheir\nshort-term nature,\ncomputed within\nsmall,\noften"
        },
        {
          "vii": "overlapping time windows in an audio signal. These features"
        },
        {
          "vii": "excel\nin capturing dynamic changes and nuanced variations in"
        },
        {
          "vii": "speech, as exemplified in the work of Tepperman et al.\n[15]."
        },
        {
          "vii": "Conversely, global features are computed over more extensive"
        },
        {
          "vii": "segments of an audio signal, typically encompassing the entire"
        },
        {
          "vii": "speech utterance. These\nfeatures\ncapture\ncharacteristics\nthat"
        },
        {
          "vii": "manifest over a more extended period, and a number of studies"
        },
        {
          "vii": "[38],\n[39],\n[42],\n[45] have employed global\nfeatures\nin their"
        },
        {
          "vii": "experiments.\nIt’s\nnoteworthy\nthat\na\ncommon\npractice\nis\nto"
        },
        {
          "vii": "initially extract\nlocal features from individual windows within"
        },
        {
          "vii": "an utterance and subsequently aggregate them across the entire"
        },
        {
          "vii": "utterance. Nevertheless,\nit’s also applicable to combine these"
        },
        {
          "vii": "two types of\nfeatures [16],\n[26]."
        },
        {
          "vii": "Our\nreview reveals\na\nnotable\ninclination\ntoward\nglobal"
        },
        {
          "vii": "features among researchers. This preference can be attributed"
        },
        {
          "vii": "to the comparative computational efficiency of global features,"
        },
        {
          "vii": "as they involve fewer parameters than their local counterparts."
        },
        {
          "vii": "However,\nit’s essential\nto acknowledge that extracting global-"
        },
        {
          "vii": "level features comes at\nthe cost of disregarding word-level re-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "viii": "Another category of extracted features in the visual modality"
        },
        {
          "viii": "pertains\nto facial\nfeatures. Given the\nstrong connection be-"
        },
        {
          "viii": "tween sarcasm and facial expressions, using facial features di-"
        },
        {
          "viii": "rectly avoids noisy features from the background or unrelated"
        },
        {
          "viii": "scenes of\nthe video. For\ninstance, Wu et al.\n[35] employed"
        },
        {
          "viii": "MTCNN [76]\nfor\nface detection in video frames. They then"
        },
        {
          "viii": "cropped these detected faces,\nsubsequently processing them"
        },
        {
          "viii": "with a ResNet\nto extract\nfeatures. Similarly, Sun et al.\n[50]"
        },
        {
          "viii": "utilized Face DLIB 8\nfor\nface extraction,\nfollowed by feature"
        },
        {
          "viii": "extraction using the ResNet-152. On the other hand, Hasen"
        },
        {
          "viii": ""
        },
        {
          "viii": "and Patil\n[40]\nimplemented OpenFace 2 [77]\nto derive Facial"
        },
        {
          "viii": ""
        },
        {
          "viii": "Action Unit\n(FAU)\nfeatures, which are crucial\nfor analyzing"
        },
        {
          "viii": ""
        },
        {
          "viii": "human emotions and affect. Furthermore, Hiremath et al. [57]"
        },
        {
          "viii": ""
        },
        {
          "viii": "incorporated\nthe\neye\naspect\nratio metric\nin\ntheir\nanalysis,"
        },
        {
          "viii": ""
        },
        {
          "viii": "adding another dimension to their\nfacial\nfeature domain."
        },
        {
          "viii": ""
        },
        {
          "viii": "Several research endeavors have made contributions to fea-"
        },
        {
          "viii": "ture representation by applying both preprocessing and post-"
        },
        {
          "viii": "processing techniques. For instance, Pramanick et al. [51] en-"
        },
        {
          "viii": "hanced the spatial features by leveraging the action recognition"
        },
        {
          "viii": "tool 13D [78]. Meanwhile, Zhang et al. [60] carried out further"
        },
        {
          "viii": "processing on spatial\nfeatures,\nincoprprating BiGRU and the"
        },
        {
          "viii": "attention mechanisms,\nleading\nto\nthe\ncreation\nof weighted"
        },
        {
          "viii": "feature vectors as\nthe final\nrepresentation. Hasan et al.\n[40]"
        },
        {
          "viii": "harnessed modified transformer\nencoder\nto enrich the\nfacial"
        },
        {
          "viii": "features. To mitigate the inclusion of noisy features resulting"
        },
        {
          "viii": "from video frames, Ray et al.\n[29]\nimplemented Katna 9 on"
        },
        {
          "viii": "the extracted spatial features. This selective approach towards"
        },
        {
          "viii": "key frame extraction aids in enhancing the overall quality of"
        },
        {
          "viii": ""
        },
        {
          "viii": "the spatial\nfeature."
        },
        {
          "viii": ""
        },
        {
          "viii": "Similar to the audio and text modalities, global features are"
        },
        {
          "viii": ""
        },
        {
          "viii": "the preferred choice\nin the visual modality. Typically,\nthese"
        },
        {
          "viii": ""
        },
        {
          "viii": "features are derived by averaging values across multiple video"
        },
        {
          "viii": ""
        },
        {
          "viii": "frames or images that are synchronized with the corresponding"
        },
        {
          "viii": ""
        },
        {
          "viii": "text and audio. In a few instances, under the same mechanism,"
        },
        {
          "viii": ""
        },
        {
          "viii": "alternative methods have been employed. For example, Bedi"
        },
        {
          "viii": ""
        },
        {
          "viii": "et\nal.\n[23]\ninvolved\na\n1D CNN for\ngenerating\na\nglobal"
        },
        {
          "viii": ""
        },
        {
          "viii": "et\nfeature\nvector. Zhang\nal.\n[60]\nincluded\naverage\npooling"
        },
        {
          "viii": ""
        },
        {
          "viii": "and attention mechanisms to transform local features to global"
        },
        {
          "viii": ""
        },
        {
          "viii": "features."
        },
        {
          "viii": ""
        },
        {
          "viii": "In addition to harnessing features from the audio,\ntext, and"
        },
        {
          "viii": ""
        },
        {
          "viii": "visual modalities, Some studies [27], [28] ventured beyond the"
        },
        {
          "viii": ""
        },
        {
          "viii": "conventional boundaries by incorporating additional elements"
        },
        {
          "viii": ""
        },
        {
          "viii": "into their\ntraining sets. Specifically,\nthey introduced speaker"
        },
        {
          "viii": ""
        },
        {
          "viii": "information, considering the identity of the speaker delivering"
        },
        {
          "viii": ""
        },
        {
          "viii": "the utterance as an influential factor in the recognition process."
        },
        {
          "viii": ""
        },
        {
          "viii": "Chauhan et al. [28] delved into this multidimensional approach"
        },
        {
          "viii": ""
        },
        {
          "viii": "by\nexploring\nthe\nrole\nof\nemojis\nin\nthe\ncontext\nof\nsarcasm"
        },
        {
          "viii": ""
        },
        {
          "viii": "recognition,\nrecognizing\nthe\npotential\nsignificance\nof\nthese"
        },
        {
          "viii": ""
        },
        {
          "viii": "visual cues in enhancing the overall performance of\nthe task."
        },
        {
          "viii": ""
        },
        {
          "viii": ""
        },
        {
          "viii": ""
        },
        {
          "viii": "C. Classification method and fusion"
        },
        {
          "viii": ""
        },
        {
          "viii": "The\nclassification\ntask\ntypically\ninvolves\nusing machine"
        },
        {
          "viii": ""
        },
        {
          "viii": "learning models\nto analyze the extracted features and deter-"
        },
        {
          "viii": ""
        },
        {
          "viii": "mine whether it contains sarcasm. These classification models"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ix": "effective\nfor\nbinary\nclassification\nproblems. However,"
        },
        {
          "ix": "it\nperforms\npoorly when\ndealing with\ncomplex,\nnon-"
        },
        {
          "ix": "linear relationships. SVM is known for the ability to find"
        },
        {
          "ix": "optimal hyperplanes that separate different classes, even"
        },
        {
          "ix": "in\nhigh-dimensional\nspaces. RF,\nan\nensemble\nlearning"
        },
        {
          "ix": "method,\ncombines multiple\ndecision\ntrees\nto\nimprove"
        },
        {
          "ix": "accuracy and robustness. It can provide valuable insights"
        },
        {
          "ix": "into the importance of different\nfeatures\nin making pre-"
        },
        {
          "ix": "dictions."
        },
        {
          "ix": "• Deep learning models encompass advanced models such"
        },
        {
          "ix": "as\nthe fine-tuned VGGish employed in Gao et al.\n[42]."
        },
        {
          "ix": "These models harness CNN to capture intricate patterns"
        },
        {
          "ix": "in the\naudio data. Due\nto their deep architecture,\nthey"
        },
        {
          "ix": "can model\nintricate relationships within the data. How-"
        },
        {
          "ix": "ever,\ntraining deep learning models requires a substantial"
        },
        {
          "ix": ""
        },
        {
          "ix": "amount of\nlabeled data. Acquiring and annotating large"
        },
        {
          "ix": "datasets can be resource-intensive and time-consuming."
        },
        {
          "ix": "In addition, DNNs\nare often considered “black boxes”"
        },
        {
          "ix": "due\nto\ntheir\ncomplex\narchitectures, making\nit\ndifficult"
        },
        {
          "ix": "to\ninterpret\nhow they\narrive\nat\nspecific\ndecisions\nor"
        },
        {
          "ix": "predictions."
        },
        {
          "ix": ""
        },
        {
          "ix": "2) Multimodal\nfusion strategies: Recent experimental stud-"
        },
        {
          "ix": "ies\nhave\nconsistently\ndemonstrated\nthat\nincorporating\nau-"
        },
        {
          "ix": "dio,\ntextual,\nand visual data\nsignificantly enhances\nsarcasm"
        },
        {
          "ix": "recognition performance\ncompared to unimodal\napproaches,"
        },
        {
          "ix": "highlighting\nthe\ninherently multimodal\nnature\nof\nsarcastic"
        },
        {
          "ix": "expressions\n[21],\n[29],\n[35],\n[40]. This\nreflects\nthe linguistic"
        },
        {
          "ix": "understanding that\nsarcasm often relies on the\ninterplay of"
        },
        {
          "ix": "audio,\ntextual and visual cues – an interaction that\nis difficult"
        },
        {
          "ix": "to\ncapture\nthrough\na\nsingle modality\nalone\n[79],\n[80]. As"
        },
        {
          "ix": "pointed by Jacob et al.\n[81],\nthe incongruity between verbal"
        },
        {
          "ix": "and non-verbal cues is facilitating comprehension of sarcasm."
        },
        {
          "ix": "Sarcasm is neither simply a tone of voice or verbal\nirony,\nits"
        },
        {
          "ix": "complexity emerges from the dynamic integration of multiple"
        },
        {
          "ix": "communication channels."
        },
        {
          "ix": "Furthermore,\nthe multimodal nature of sarcasm is supported"
        },
        {
          "ix": "by the taxonomy provided by Attardo [79], which categorizes"
        },
        {
          "ix": "sarcasm markers into two primary types. Metacommunicative"
        },
        {
          "ix": "alerts\nare\nexplicit\nsignals\nthat\ndirectly\ninform the\nlistener"
        },
        {
          "ix": "that\nan\nutterance\nshould\nbe\ninterpreted\nsarcastically. These"
        },
        {
          "ix": "include verbal\nindicators such as “just kidding” or “I’m being"
        },
        {
          "ix": "sarcastic,”\nas well\nas\nnon-verbal\ncues\nsuch\nas\na\nsarcastic"
        },
        {
          "ix": "smile or\ntongue-in-cheek expression. These markers serve as"
        },
        {
          "ix": "overt cues, providing a clear signal of\nthe speaker’s sarcastic"
        },
        {
          "ix": "intent. In contrast, paracommunicative alerts are more subtle,"
        },
        {
          "ix": "involving\nindirect\ncues\nthat, when\npaired with\nthe\nliteral"
        },
        {
          "ix": "statement,\nsuggest\na\nsarcastic\ninterpretation. For\nexample,\na"
        },
        {
          "ix": "speaker might use\na blank facial\nexpression or\nexaggerated"
        },
        {
          "ix": "nodding to imply sarcasm, while phonetic cues could include"
        },
        {
          "ix": "delivering an enthusiastic statement\nin a bored or monotonous"
        },
        {
          "ix": "tone\nor\nusing flat\nintonation with minimal\npitch\nvariation."
        },
        {
          "ix": "Unlike metacommunicative signals, paracommunicative alerts"
        },
        {
          "ix": "create a contrast with the literal content, prompting the listener"
        },
        {
          "ix": "to infer sarcasm. Furthermore, different modalities contribute"
        },
        {
          "ix": "varying\ndegrees\nof\nsignificance\nto\nsarcasm detection.\nFor"
        },
        {
          "ix": "instance,\nsome speakers\nrely heavily on prosodic features\nin"
        },
        {
          "ix": "the audio modality, such as pitch variations, to convey sarcasm,"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "x": "[26] utilized a Long Short-Term Memory (LSTM) network to"
        },
        {
          "x": "encode audio features, and a CNN to encode the textual data,"
        },
        {
          "x": "with the resultant\nrepresentations concatenated and processed"
        },
        {
          "x": "by a FCNN functioning as the decoder."
        },
        {
          "x": "b) Attention mechanism:\nIntroduced by Vaswani\net al."
        },
        {
          "x": "[61],\nthe attention mechanism has become a central\ntopic in"
        },
        {
          "x": "the deep learning community. This mechanism allows models"
        },
        {
          "x": "to\nassign\nvarying weights\nto\ndifferent modalities,\nenabling"
        },
        {
          "x": "the\nextraction\nof\ninformation\nthat\nis\ncritical\nto\nthe\ntask\nat"
        },
        {
          "x": "hand. By doing so, attention mechanisms enhance prediction"
        },
        {
          "x": "accuracy without significantly consuming computational costs."
        },
        {
          "x": "Recently, attention mechanisms have emerged as key tools in"
        },
        {
          "x": "multimodal data fusion tasks. Several variations of attention-"
        },
        {
          "x": ""
        },
        {
          "x": "based methods have been developed to leverage this capability."
        },
        {
          "x": "First,\nthe intra-modality self-attention focus exclusively on"
        },
        {
          "x": "data within a single modality, facilitating a undiluted analysis"
        },
        {
          "x": "of\nrelationships within that modality. Pramanick et al.\n[51]"
        },
        {
          "x": "proposed\nthe MuLOT system, which\nutilizes\nself-attention"
        },
        {
          "x": "to\nenhance\nintra-modal\ncorrespondence while\nincorporating"
        },
        {
          "x": "optimal\ntransport methods to address cross-modal alignment."
        },
        {
          "x": "inter-modality\nSecond,\nthe\ncross-attention focuses on ex-"
        },
        {
          "x": "ploiting the relationship among different modalities by inte-"
        },
        {
          "x": "grating\ndata\nfrom multiple\nsources\nin\neach\nattention\noper-"
        },
        {
          "x": "ation. This\napproach enables\nthe model\nto capture\nintricate"
        },
        {
          "x": "connections\nacross modalities. For\nexample, Chauhan et al."
        },
        {
          "x": "[27] employed cross-attention to discern relationships between"
        },
        {
          "x": "segments across modalities. Hasan et al. [40] and Zhang et al."
        },
        {
          "x": "[60] utilized a pair of\ncross-modal\nattention mechanisms\nto"
        },
        {
          "x": "explore bimodal\ninteractions\n(text-audio,\ntext-visual\ninterac-"
        },
        {
          "x": "tions). Wu et al.\n[35] applied a cross-modal attention mecha-"
        },
        {
          "x": "nism to emphasize specific words, particularly those exhibiting"
        },
        {
          "x": "a\nhigh\ndegree\nof\nincongruity\nbetween\npositive\nspoken\ntext"
        },
        {
          "x": "and\nnegative\nnonverbal\n(audio\nand\nvisual)\ncues. Similarly,"
        },
        {
          "x": "Zhang et al.\n[47] proposed a framework that assessed cross-"
        },
        {
          "x": "modal\nincongruity between text and visual, as well as text and"
        },
        {
          "x": "audio modalities. Furthermore, Chauhan et al. [28] developed"
        },
        {
          "x": "an architecture\nto explore\nthe\ninterplay between emojis\nand"
        },
        {
          "x": "other multimodal data, highlighting the versatility and evolving"
        },
        {
          "x": "nature of cross-attention techniques."
        },
        {
          "x": "Third,\nthe hybrid intra-modality and inter-modality atten-"
        },
        {
          "x": "tion combines the previous two. Zhang et al. [60] implemented"
        },
        {
          "x": "an intra-modality attention mechanism to identify the most"
        },
        {
          "x": "relevant features within each modality individually. Following"
        },
        {
          "x": "this,\nthey employed three\ninter-modal\nattention mechanisms"
        },
        {
          "x": "to\ncapture\nand\nanalyze\nthe\npairwise\nrelationships\nbetween"
        },
        {
          "x": "modalities\n(text-visual,\ntext-audio,\nand\nvisual-audio\ninterac-"
        },
        {
          "x": "tions). This dual-layered approach enables\na\ncomprehensive"
        },
        {
          "x": "understanding\nof\nhow different\ntypes\nof\ndata\ncontribute\nto"
        },
        {
          "x": "sarcasm detection,\nfacilitating a more nuanced and effective"
        },
        {
          "x": "integration of multimodal\ninformation."
        },
        {
          "x": "c) Collaborative\ngating mechanism:\nUnlike\nattention"
        },
        {
          "x": "mechanism which distributes weights across elements within"
        },
        {
          "x": "or across modalities, gating mechanisms leverage learned gates"
        },
        {
          "x": "to control which modality influences the fused representation."
        },
        {
          "x": "For example, Ray et al.\n[29] and Tomar et al.\n[72] used the"
        },
        {
          "x": "collaborative gating mechanism to evaluate all modalities\nin"
        },
        {
          "x": "parallel and learned to assign importance scores\nthat\nreflect"
        },
        {
          "x": "their combined informative strength. This coordinated gating"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "Attention mechanism\n80.00\ncross-attention [28]"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "or\nits\nextensions were\nretained (22 articles). Given sarcasm"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "recognition\ninvolves\nclass\nimbalance,\nstudies\nreporting F1-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "score,\na\nbalanced metric, were\nincluded\n(19\narticles). We"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "excluded\nfive\nstudies without\nexplicit\nevaluation methods,"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "and five used train-test splits that were insufficient\nfor stable"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "statistical analysis,\nresulting in nine articles for\nthe analysis."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": ""
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "b) Analysis: The primary goal\nis\nto determine whether"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "major\nfusion methods\n(encoder-decoder VS. attention mech-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "anism) significantly impact F1-scores in sarcasm recognition."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "To achieve this, we conducted a random-effects meta-analysis,"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "assessing the heterogeneity across studies and computing ef-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "fect sizes to quantify the magnitude of differences in F1-scores"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "between the\nfusion methods. The effect\nsize was calculated"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "using Hedges′g\n[83], which adjusts\nfor\nsmall\nsample\nsizes"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "and quantifies differences in mean F1-scores. We used the Q-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "statistic and I 2 statistic to assess heterogeneity across studies."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "The Q-statistic tests whether the variation in effect sizes across"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "studies is greater than expected by chance, while I 2 quantifies"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "the percentage of\ntotal variation due\nto heterogeneity [84]."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "After assessing heterogeneity across studies, a random-effects"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "model was applied. The model\nincorporated tau-squared (τ 2),"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "an estimate\nof\nthe\nbetween-study\nvariance,\nand\nprovided a"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "pooled mean effect size [85]."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "Table III summarizes studies with speaker-independent eval-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "uation. We applied the aforementioned analysis framework to"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "the\nspeaker-independent\nevaluation and observed significant"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "heterogeneity across studies, with a Q-statistic of 30.00 and an"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "I 2 of 83.33%. Using the random-effects model,\nthe weighted"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "mean F1-score\nfor\nthe\nencoder-decoder method was 67.20,"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "with a 95% confidence interval of (59.16, 75.24). In compari-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "son,\nthe attention mechanism yielded a higher mean F1-score"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "of 71.70, with a 95% confidence\ninterval of\n(65.87, 77.53)."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "However,\nthe overall p-value for\nthe difference between the"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "two methods was 0.4630,\nsuggesting that\nthe difference\nin"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "performance was not statistically significant."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "Table IV covers\nthose with speaker-dependent evaluation."
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "The speaker-dependent evaluation also demonstrated substan-"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "tial heterogeneity, with a Q-statistic of 72.00 and an I 2\nof"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "88.89%. Using the random-effects model,\nthe weighted mean"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "F1-score for\nthe encoder-decoder method was 73.99, with a"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "95% confidence interval of (70.19, 77.79). In comparison,\nthe"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "attention mechanism achieved a comparable mean F1-score of"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "75.11, with a 95% confidence interval of (73.15, 77.07). The"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "overall p-value\nfor\nthe difference between the\ntwo methods"
        },
        {
          "Attention mechanism\n70.90\ncross-attention [54]": "was 0.6303, confirming no statistically significant difference."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xi": "TABLE III"
        },
        {
          "xi": "PERFORMANCE OF SPEAKER-INDEPENDENT 5-FOLD CROSS-VALIDATION"
        },
        {
          "xi": ""
        },
        {
          "xi": ""
        },
        {
          "xi": "Fusion method\nF1-score (%)\nReference"
        },
        {
          "xi": ""
        },
        {
          "xi": "Encoder-decoder\n63.1\nSVM decoder\n[21]"
        },
        {
          "xi": ""
        },
        {
          "xi": "Encoder-decoder\n71.3\nFCNN decoder\n[49]"
        },
        {
          "xi": ""
        },
        {
          "xi": "Attention mechanism\n65.90\ncross-attention [27]"
        },
        {
          "xi": ""
        },
        {
          "xi": "Attention mechanism\n70.00\ncross-attention [35]"
        },
        {
          "xi": ""
        },
        {
          "xi": "Attention mechanism\n70.90\ncross-attention [54]"
        },
        {
          "xi": ""
        },
        {
          "xi": "Attention mechanism\n80.00\ncross-attention [28]"
        },
        {
          "xi": ""
        },
        {
          "xi": ""
        },
        {
          "xi": ""
        },
        {
          "xi": "or\nits\nextensions were\nretained (22 articles). Given sarcasm"
        },
        {
          "xi": ""
        },
        {
          "xi": "recognition\ninvolves\nclass\nimbalance,\nstudies\nreporting F1-"
        },
        {
          "xi": ""
        },
        {
          "xi": "score,\na\nbalanced metric, were\nincluded\n(19\narticles). We"
        },
        {
          "xi": ""
        },
        {
          "xi": "excluded\nfive\nstudies without\nexplicit\nevaluation methods,"
        },
        {
          "xi": ""
        },
        {
          "xi": "and five used train-test splits that were insufficient\nfor stable"
        },
        {
          "xi": ""
        },
        {
          "xi": "statistical analysis,\nresulting in nine articles for\nthe analysis."
        },
        {
          "xi": ""
        },
        {
          "xi": "b) Analysis: The primary goal\nis\nto determine whether"
        },
        {
          "xi": "major\nfusion methods\n(encoder-decoder VS. attention mech-"
        },
        {
          "xi": "anism) significantly impact F1-scores in sarcasm recognition."
        },
        {
          "xi": "To achieve this, we conducted a random-effects meta-analysis,"
        },
        {
          "xi": "assessing the heterogeneity across studies and computing ef-"
        },
        {
          "xi": "fect sizes to quantify the magnitude of differences in F1-scores"
        },
        {
          "xi": "between the\nfusion methods. The effect\nsize was calculated"
        },
        {
          "xi": "using Hedges′g\n[83], which adjusts\nfor\nsmall\nsample\nsizes"
        },
        {
          "xi": "and quantifies differences in mean F1-scores. We used the Q-"
        },
        {
          "xi": "statistic and I 2 statistic to assess heterogeneity across studies."
        },
        {
          "xi": "The Q-statistic tests whether the variation in effect sizes across"
        },
        {
          "xi": "studies is greater than expected by chance, while I 2 quantifies"
        },
        {
          "xi": "the percentage of\ntotal variation due\nto heterogeneity [84]."
        },
        {
          "xi": "After assessing heterogeneity across studies, a random-effects"
        },
        {
          "xi": "model was applied. The model\nincorporated tau-squared (τ 2),"
        },
        {
          "xi": "an estimate\nof\nthe\nbetween-study\nvariance,\nand\nprovided a"
        },
        {
          "xi": "pooled mean effect size [85]."
        },
        {
          "xi": "Table III summarizes studies with speaker-independent eval-"
        },
        {
          "xi": "uation. We applied the aforementioned analysis framework to"
        },
        {
          "xi": "the\nspeaker-independent\nevaluation and observed significant"
        },
        {
          "xi": "heterogeneity across studies, with a Q-statistic of 30.00 and an"
        },
        {
          "xi": "I 2 of 83.33%. Using the random-effects model,\nthe weighted"
        },
        {
          "xi": "mean F1-score\nfor\nthe\nencoder-decoder method was 67.20,"
        },
        {
          "xi": "with a 95% confidence interval of (59.16, 75.24). In compari-"
        },
        {
          "xi": "son,\nthe attention mechanism yielded a higher mean F1-score"
        },
        {
          "xi": "of 71.70, with a 95% confidence\ninterval of\n(65.87, 77.53)."
        },
        {
          "xi": "However,\nthe overall p-value for\nthe difference between the"
        },
        {
          "xi": "two methods was 0.4630,\nsuggesting that\nthe difference\nin"
        },
        {
          "xi": "performance was not statistically significant."
        },
        {
          "xi": "Table IV covers\nthose with speaker-dependent evaluation."
        },
        {
          "xi": "The speaker-dependent evaluation also demonstrated substan-"
        },
        {
          "xi": "tial heterogeneity, with a Q-statistic of 72.00 and an I 2\nof"
        },
        {
          "xi": "88.89%. Using the random-effects model,\nthe weighted mean"
        },
        {
          "xi": "F1-score for\nthe encoder-decoder method was 73.99, with a"
        },
        {
          "xi": "95% confidence interval of (70.19, 77.79). In comparison,\nthe"
        },
        {
          "xi": "attention mechanism achieved a comparable mean F1-score of"
        },
        {
          "xi": "75.11, with a 95% confidence interval of (73.15, 77.07). The"
        },
        {
          "xi": "overall p-value\nfor\nthe difference between the\ntwo methods"
        },
        {
          "xi": "was 0.6303, confirming no statistically significant difference."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xii": "monotone speech in sentence 2 (Table V) as difficult\nfor"
        },
        {
          "xii": ""
        },
        {
          "xii": "models\nto interpret without\nadded context. Chauhan et"
        },
        {
          "xii": "al.\n[28] also found models failed with neutral sentiment"
        },
        {
          "xii": ""
        },
        {
          "xii": "and expressionless emojis, as these cases rely heavily on"
        },
        {
          "xii": "contextual or commonsense knowledge that models often"
        },
        {
          "xii": "lack."
        },
        {
          "xii": ""
        },
        {
          "xii": "• Nuanced sarcastic expressions: Labeling limitations can"
        },
        {
          "xii": ""
        },
        {
          "xii": "obscure subtle sarcasm. Chauhan et al.\n[28] noted cases"
        },
        {
          "xii": ""
        },
        {
          "xii": "where sentences with identical sentiment and emoji anno-"
        },
        {
          "xii": ""
        },
        {
          "xii": "tations were classified differently. For example, utterance"
        },
        {
          "xii": ""
        },
        {
          "xii": "3 (Table V) was labeled as non-sarcastic while sentence"
        },
        {
          "xii": ""
        },
        {
          "xii": "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto"
        },
        {
          "xii": ""
        },
        {
          "xii": "model\nconfusion. Limited training data\nfurther hinders"
        },
        {
          "xii": ""
        },
        {
          "xii": "the ability to capture these nuanced cues."
        },
        {
          "xii": "To summarize, key challenges\nidentified include modality"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "model\nconfusion. Limited training data\nfurther hinders"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "the ability to capture these nuanced cues."
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "To summarize, key challenges\nidentified include modality"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "mismatches,\nsuch\nas\ninconsistencies\nbetween\nfacial\nexpres-"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "sions and expressed emotions, and difficulties detecting sar-"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "casm with neutral cues. Additionally, models\nstruggle when"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "labeling fails to capture sarcasm’s subtlety,\nindicating a need"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "for more sophisticated multimodal fusion techniques. Existing"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "labeling systems may not\nfully encapsulate\nsarcasm’s\ncom-"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "plexity;\ndeveloping more\ngranular\nsystems\nthat\nincorporate"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "factors\nlike\nsarcasm intensity and confidence\nlevel, beyond"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "sentiment and emotion, could help reduce misclassification."
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "IV. DISCUSSION"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": ""
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "In\nthis\ndiscussion, we\nsynthesize\nkey findings\nfrom our"
        },
        {
          "4 with\nthe\nsame\nannotations was\nsarcastic,\nleading\nto": "review to highlight\nemerging trends\nin sarcasm recognition"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "The\nanalysis\nshows\nno\nstatistically\nsignificant\ndifference"
        },
        {
          "Indian might come by and save us.\nsarcasm": "in\nperformance\nbetween\nattention mechanism and\nencoder-"
        },
        {
          "Indian might come by and save us.\nsarcasm": "decoder\nfusion methods\nin\nboth\nspeaker-independent\nand"
        },
        {
          "Indian might come by and save us.\nsarcasm": "speaker-dependent\nevaluations. While\nattention mechanism"
        },
        {
          "Indian might come by and save us.\nsarcasm": "exhibits higher mean F1-scores in both cases,\nthe overlapping"
        },
        {
          "Indian might come by and save us.\nsarcasm": "confidence\nintervals\nand\nhigh\np-values\nindicate\nthat\nthese"
        },
        {
          "Indian might come by and save us.\nsarcasm": "differences are not statistically reliable. Significant heterogene-"
        },
        {
          "Indian might come by and save us.\nsarcasm": "ity among studies\nsuggests\nthat model performance\ncan be"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "affected by other\nfactors\nlike\nfeature granularity and model"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "architecture. Additionally,\nthe\nlimited\nstudy\npool\nrestricts"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "statistical power, highlighting the need for more research to"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "detect subtle differences between fusion methods."
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "c) Error analysis and failure modes: While\naggregate"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "F1-scores offer\ninsights, understanding misclassification rea-"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "sons\nis essential\nfor\nrefining model performance. Below, we"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "examine common failure modes in sarcasm recognition:"
        },
        {
          "Indian might come by and save us.\nsarcasm": ""
        },
        {
          "Indian might come by and save us.\nsarcasm": "• Mismatch between modalities: Sarcasm is misclassified"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xiii": "information for sarcasm identification. For instance, “your plan"
        },
        {
          "xiii": "is fantastic!” may sound non-sarcastic if the context is missing."
        },
        {
          "xiii": "Illocutionary\nSarcasm, which mainly\nrelies\non\nnon-verbal"
        },
        {
          "xiii": "cues, such as prosody and visual signals,\nto convey sarcastic"
        },
        {
          "xiii": "intent\ninstead of\nthe\ntextual\ninformation alone. An example"
        },
        {
          "xiii": "of\nthis would\nbe\nsaying\n“that’s\nright” while\nrolling\none’s"
        },
        {
          "xiii": "eyes or placing exaggerated emphasis on the word “right.”"
        },
        {
          "xiii": "Embedded Sarcasm, which is characterized by the presence of"
        },
        {
          "xiii": "embedded sentiment\nincongruity; and Like-prefixed Sarcasm,"
        },
        {
          "xiii": "which\nintroduces\nan\nimplicit\ndisagreement\nand\nit\ntypically"
        },
        {
          "xiii": "involves\nthe\nuse\nof\nthe word\n“like”\nas\na\nprecursor\nto\nthe"
        },
        {
          "xiii": "sarcastic\nexpression. Establishing annotation guidelines\nthat"
        },
        {
          "xiii": "encompass these nuanced sub-types of sarcasm is essential for"
        },
        {
          "xiii": "creating well-organized and high-quality datasets. Moreover,"
        },
        {
          "xiii": "this\ndetailed\nclassification\ncan\ninspire\nthe\ndevelopment\nof"
        },
        {
          "xiii": "models\nthat capture the intricacies of\nsarcasm and provide a"
        },
        {
          "xiii": "more comprehensive analysis of model performance."
        },
        {
          "xiii": "Considering the cultural variability in interpreting sarcasm,"
        },
        {
          "xiii": "it\nis\nimperative\nto\nprovide\nannotators with\ncomprehensive"
        },
        {
          "xiii": "guidelines. These guidelines\nshould not constrain interpreta-"
        },
        {
          "xiii": "tions but offer a broad framework for understanding sarcasm"
        },
        {
          "xiii": "across diverse cultural and linguistic contexts. Currently,\nthe"
        },
        {
          "xiii": "IAA of\nthe existing datasets is still at a low point, with Ray"
        },
        {
          "xiii": "et al.\n[29]\nachieving the\nexisting highest\nscore of 0.595 in"
        },
        {
          "xiii": "English. Therefore, it’s necessary to allow annotators to engage"
        },
        {
          "xiii": "in discussions to establish a common annotation standard and"
        },
        {
          "xiii": "increase the agreement."
        },
        {
          "xiii": "The\ncurrent\napproach to sarcasm labeling is\nlimited to a"
        },
        {
          "xiii": "binary classification, where utterances are categorized as either"
        },
        {
          "xiii": "sarcastic or non-sarcastic. This simplistic labeling neglects the"
        },
        {
          "xiii": "subtleties and gradations of sarcasm that occur\nin real-world"
        },
        {
          "xiii": "communication.\nIn reality,\nsarcasm exists along a spectrum,"
        },
        {
          "xiii": "with varying degrees of intensity. For example, when someone"
        },
        {
          "xiii": "arrives late to a meeting, a remark like “Oh,\nit’s not\nlike we"
        },
        {
          "xiii": "were waiting for you,” delivered in a calm, flat\ntone,\nsubtly"
        },
        {
          "xiii": "conveys mild annoyance or disappointment\nthrough sarcasm."
        },
        {
          "xiii": "In\ncontrast,\nresponding\nto\nsomeone\nfailing\na\nsimple\ntest"
        },
        {
          "xiii": "with,\n“Congratulations! You’ve\nreally outdone yourself\nthis"
        },
        {
          "xiii": "time!” in an exaggeratedly excited tone amplifies the sarcasm."
        },
        {
          "xiii": "Sarcasm, often linked to negative sentiment,\nintensifies as the"
        },
        {
          "xiii": "expression becomes more positive\nin valence\nand higher\nin"
        },
        {
          "xiii": "arousal.\nIn other words,\nthe stronger\nthe expressed emotion,"
        },
        {
          "xiii": "the greater\nthe perceived intensity of\nthe sarcasm. Although"
        },
        {
          "xiii": "the MUStARD++ dataset includes emotion annotations, it does"
        },
        {
          "xiii": "not explicitly indicate the relationship between sarcasm and"
        },
        {
          "xiii": "the annotated emotions. Therefore,\nto more accurately reflect"
        },
        {
          "xiii": "real-life sarcasm, a labeling system that accounts for varying"
        },
        {
          "xiii": "intensities\nis\nessential. This nuanced approach would better"
        },
        {
          "xiii": "capture the range of\nsarcastic expressions, providing a more"
        },
        {
          "xiii": "comprehensive framework for sarcasm analysis."
        },
        {
          "xiii": ""
        },
        {
          "xiii": "B. Evolution of\nfeature extraction techniques"
        },
        {
          "xiii": ""
        },
        {
          "xiii": "the field progresses\nfrom unimodal\nto multimodal\nRQ2: As"
        },
        {
          "xiii": "sarcasm recognition, how have feature extraction developed?"
        },
        {
          "xiii": "What\nare\nthe\nchallenges\nand\nlimitations\nin\ncurrent\nfeature"
        },
        {
          "xiii": "extraction methods?"
        },
        {
          "xiii": "In\nour\nexploration\nof\nthe\nprogression\nfrom unimodal\nto"
        },
        {
          "xiii": "multimodal approaches for sarcasm recognition, we find three"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xiv": "stress, particularly in the context of\nsarcasm. Fortunately,\nin"
        },
        {
          "xiv": "the context of the multimodal approach, our review uncovers a"
        },
        {
          "xiv": "trend where researchers have introduced innovative techniques"
        },
        {
          "xiv": "to\nenhance\nthe\nextraction\nof\nglobal\nfeatures. Rather\nthan"
        },
        {
          "xiv": "resorting\nto\nthe\nconventional method\nof\naveraging\nfeature"
        },
        {
          "xiv": "vectors,\nthese studies have opted for alternative approaches."
        },
        {
          "xiv": "For\ninstance,\nthe\nemployment\nof\n1D CNN to\nconvert\nthe"
        },
        {
          "xiv": "speech feature [23], or\nthe utilization of attention mechanism"
        },
        {
          "xiv": "to\ngenerate\nthe\nfeature\nvector\n[60]. These\nnovel methods"
        },
        {
          "xiv": "represent a valuable departure from the conventional\nreliance"
        },
        {
          "xiv": "on averaged global\nfeatures,\nthereby opening up new avenues"
        },
        {
          "xiv": "for more effective feature extraction while accommodating the"
        },
        {
          "xiv": "intricate temporal aspects involved in sarcasm recognition."
        },
        {
          "xiv": "4) Limitations of current\nfeature extraction techniques and"
        },
        {
          "xiv": "future\nresearch:\nIn\nour\ninvestigation\ninto\nthe\ndevelopment"
        },
        {
          "xiv": "of\nfeature\nextraction,\nthe\nfollowing\nfour\nlimitations\nhave"
        },
        {
          "xiv": "emerged:"
        },
        {
          "xiv": "a) Unexplored\nparameters\nin\nspectral\nfeatures: While"
        },
        {
          "xiv": "spectral features such as MFCCs, MelSpecs, and their correla-"
        },
        {
          "xiv": "tions have been widely applied in sarcasm recognition, critical"
        },
        {
          "xiv": "aspects\nremain underexplored. None of\nthe studies\nreviewed"
        },
        {
          "xiv": "provide detailed insights into the specific utilization of\nthese"
        },
        {
          "xiv": "features. For\ninstance,\nin general\nspeaking, different MFCCs"
        },
        {
          "xiv": "coefficients represent distinct parts of the Mel-frequency spec-"
        },
        {
          "xiv": "trum, with lower coefficients capturing broader, low-frequency"
        },
        {
          "xiv": "patterns and higher coefficients detailing finer, high-frequency"
        },
        {
          "xiv": "variations. We’ve known that 12–13 coefficients are used in"
        },
        {
          "xiv": "automatic speech recognition, as they encapsulate key speech-"
        },
        {
          "xiv": "related features,\nlike formants. Using higher coefficients intro-"
        },
        {
          "xiv": "duces more granular details but can also add noise, potentially"
        },
        {
          "xiv": "hindering\nrecognition\nif\nnot managed\ncarefully.\nIn\nsarcasm"
        },
        {
          "xiv": "recognition,\nthe precise role of these selected spectral features"
        },
        {
          "xiv": "remains unclear. Therefore,\nfuture research should focus on a"
        },
        {
          "xiv": "more granular analysis of spectral features to better understand"
        },
        {
          "xiv": "their\nrelationship and contribution to sarcasm detection."
        },
        {
          "xiv": "b) Absence\nof\na\nbest-performed\nspeech\nfeature\nset:"
        },
        {
          "xiv": "Linguistic\nresearch\nunderscores\nthe\nimportance\nof\nspeech"
        },
        {
          "xiv": "features\nsuch as pitch,\nspeaking rate,\nand intensity in con-"
        },
        {
          "xiv": "veying\nsarcasm. Our\nreview confirms\ntheir\nfrequent\nuse\nin"
        },
        {
          "xiv": "sarcasm recognition,\nthough a best-performed feature set\nre-"
        },
        {
          "xiv": "mains undefined. Given the variations\nin datasets, evaluation"
        },
        {
          "xiv": "protocols,\nand\narchitectures\nemployed\nacross\nthe\nreviewed"
        },
        {
          "xiv": "studies,\nit\nis methodologically\ninappropriate\nto\ndraw direct"
        },
        {
          "xiv": "conclusions linking specific features to performance. However,"
        },
        {
          "xiv": "we\nencourage\nfuture\nresearch to systematically evaluate\nthe"
        },
        {
          "xiv": "effectiveness of\nthe features\nthrough controlled experimental"
        },
        {
          "xiv": "setups\nthat\ncan unravel\nthe\neffects of\nfeatures, models,\nand"
        },
        {
          "xiv": "data characteristics."
        },
        {
          "xiv": "Moreover, future research should emphasize the interdepen-"
        },
        {
          "xiv": "dence of\nfeature and model selection in sarcasm recognition."
        },
        {
          "xiv": "While commonly used features such as MFCCs, pitch, and in-"
        },
        {
          "xiv": "tensity often yield positive results, their effectiveness is model-"
        },
        {
          "xiv": "dependent. For\ninstance, MFCCs\nalign well with Gaussian"
        },
        {
          "xiv": "Mixture Models\n(GMMs)\ndue\nto\ntheir\nuncorrelated\nnature,"
        },
        {
          "xiv": "whereas mel-filterbank features are preferred with Deep Neu-"
        },
        {
          "xiv": "ral Networks\n(DNNs), which perform better with correlated"
        },
        {
          "xiv": "inputs. Optimizing\nthis\nalignment\nis\ncrucial\nfor\nimproving"
        },
        {
          "xiv": "sarcasm detection accuracy in speech-based systems."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xv": "sarcasm or non-sarcasm. This includes basic rule-based classi-"
        },
        {
          "xv": "fiers such as Decision Trees and traditional machine learning-"
        },
        {
          "xv": "based classifiers like GMM, HMM, Logistic Regression, SVM,"
        },
        {
          "xv": "etc. Research in this area primarily focuses on identifying the"
        },
        {
          "xv": "most\nrelevant sarcasm features to input\ninto these algorithms."
        },
        {
          "xv": "The transition to multimodal approaches has led to a leap in"
        },
        {
          "xv": "recognition accuracy, however,\nthis advancement comes with"
        },
        {
          "xv": "increased complexity. The\nfusion of\nfeatures\nfrom multiple"
        },
        {
          "xv": "modalities presents\nthe\ncore\nchallenges\nfor\nresearchers due"
        },
        {
          "xv": "to their distinct characteristics, which necessitate careful con-"
        },
        {
          "xv": "sideration of how these diverse data\nsources\nare\nintegrated."
        },
        {
          "xv": "The core of multimodal sarcasm recognition lies in optimizing"
        },
        {
          "xv": "feature fusion strategies to enhance overall performance. Our"
        },
        {
          "xv": "review presents\na variety of\nfusion strategies,\nranging from"
        },
        {
          "xv": "foundational methods\nlike\nthe\nencoder-decoder\napproach to"
        },
        {
          "xv": "more sophisticated techniques, such as attention mechanisms."
        },
        {
          "xv": "As\nthe\ndomain\ncontinues\nto\nevolve, more\nintricate\nfusion"
        },
        {
          "xv": "methodologies,\nsuch as quantum-based fusion, are being ex-"
        },
        {
          "xv": "plored, further pushing the boundaries of sarcasm recognition"
        },
        {
          "xv": "technology."
        },
        {
          "xv": "Inconsistent\nevaluation metrics\nin prior\nstudies have hin-"
        },
        {
          "xv": "dered meaningful\ncomparisons\nof\nfeature\nusage\nand\nclas-"
        },
        {
          "xv": "sification methods. F1-score\nhas\nidentified\nas\nthe\npreferred"
        },
        {
          "xv": "metric as\nit offers a balanced assessment of\nrecall\n(sarcastic"
        },
        {
          "xv": "samples\ndetected)\nand\nprecision\n(reliability\nof\ndetections)."
        },
        {
          "xv": "Future research is\nsuggested to involve F1-score as a metric"
        },
        {
          "xv": "to enhance consistency and comparability across studies."
        },
        {
          "xv": "2) Limitations of current methods and future research:"
        },
        {
          "xv": "a) Unexplored multimodal\ntechniques: While advanced"
        },
        {
          "xv": "multimodal\nfusion techniques have achieved significant\nsuc-"
        },
        {
          "xv": "cess\nin\nsarcasm recognition,\nseveral\nstate-of-the-art DNNs"
        },
        {
          "xv": "have yet\nto be\nfully explored in this domain. For\ninstance,"
        },
        {
          "xv": "Graph Neural Networks (GNNs), which are designed to pro-"
        },
        {
          "xv": "cess graph-structured data, have shown remarkable potential"
        },
        {
          "xv": "in aggregating information from neighboring nodes, enabling"
        },
        {
          "xv": "the\nfusion\nof\nspatially\nlocalized\nfeatures\nacross modalities."
        },
        {
          "xv": "This unique\nability to capture\ncomplex relationships within"
        },
        {
          "xv": "graph data suggests\nthat GNNs could be highly effective in"
        },
        {
          "xv": "sarcasm recognition\ntasks,\nparticularly\nin\nintegrating\nvisual"
        },
        {
          "xv": "and audio data. Another promising yet underutilized approach"
        },
        {
          "xv": "is Generative Neural Networks\n(GenNNs), which\ninclude"
        },
        {
          "xv": "models such as Generative Adversarial Networks (GANs) and"
        },
        {
          "xv": "diffusion-based models. The primary objective of GenNNs is"
        },
        {
          "xv": "to generate data that closely mirrors real-world distributions,"
        },
        {
          "xv": "either by directly modeling these distributions or by learning"
        },
        {
          "xv": "transformations\nfrom simpler distributions\nto more\ncomplex"
        },
        {
          "xv": "ones. Their versatility in generating high-quality data has made"
        },
        {
          "xv": "them a popular choice in both unimodal and multimodal tasks,"
        },
        {
          "xv": "tackling challenges such as data augmentation, imputation, and"
        },
        {
          "xv": ""
        },
        {
          "xv": "fusion. In the context of sarcasm detection, where labeled data"
        },
        {
          "xv": ""
        },
        {
          "xv": "is often scarce, GenNNs could offer a powerful\nsolution by"
        },
        {
          "xv": ""
        },
        {
          "xv": "augmenting and enhancing existing datasets."
        },
        {
          "xv": ""
        },
        {
          "xv": "b) Limited\nlinguistic\ninsights:\nThe\nreview of previous"
        },
        {
          "xv": ""
        },
        {
          "xv": "multimodal\nresearch\nhas\nunderscored\nthe\ncritical\nrole\nthat"
        },
        {
          "xv": ""
        },
        {
          "xv": "linguistic\ninsights\nplay\nin\nguiding\ncomputational modeling"
        },
        {
          "xv": ""
        },
        {
          "xv": "for sarcasm recognition. There exists a consensus among the"
        },
        {
          "xv": "selected works that\ntextual, audio, and visual modalities col-"
        },
        {
          "xv": "lectively contribute to the overarching sarcasm classification"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xvi": "These variations suggest that sarcasm is language- and culture-"
        },
        {
          "xvi": "specific,\nreflecting both how speakers produce prosodic cues"
        },
        {
          "xvi": "and how listeners\ninterpret\nthem. However, existing sarcasm"
        },
        {
          "xvi": "recognition systems are predominantly trained on English data,"
        },
        {
          "xvi": "hindering their generalizability across cultures and languages."
        },
        {
          "xvi": "Developing multilingual datasets offers a viable path toward"
        },
        {
          "xvi": "building more robust systems. In addition, further research on"
        },
        {
          "xvi": "cross-lingual\ntransfer learning could improve the effectiveness"
        },
        {
          "xvi": "of sarcasm recognition in broader contexts."
        },
        {
          "xvi": "Another underexplored area\nis how multilingual\nspeakers"
        },
        {
          "xvi": "perceive\nand\nexpress\nsarcasm. Mandler\n[102]\nsuggests\nthat"
        },
        {
          "xvi": "second language learners interpret sarcasm through the lens of"
        },
        {
          "xvi": "their first\nlanguage knowledge. For\ninstance, Kim et al. [103]"
        },
        {
          "xvi": "found that Korean English speakers\nrely more on nonverbal"
        },
        {
          "xvi": "cues\nthan\nnative Korean\nspeakers,\na\ntrend\nalso\nobserved"
        },
        {
          "xvi": "among English speakers. However,\nthese perceptual dynamics"
        },
        {
          "xvi": "are largely absent\nin current machine learning-based sarcasm"
        },
        {
          "xvi": "detection systems. We highlight\nthis gap as\na direction for"
        },
        {
          "xvi": "future research in multilingual speech modeling."
        },
        {
          "xvi": "2)\nSarcasm recognition beyond text: Traditionally, sarcasm"
        },
        {
          "xvi": "has\nbeen\nrecognized\nas\na\ntext-based\nchallenge\nin\nnatural"
        },
        {
          "xvi": "language processing,\ntypically relying on semantic cues such"
        },
        {
          "xvi": "as\ncontrasts\nbetween\npositive\nand\nnegative words within"
        },
        {
          "xvi": "a\nsingle\nsentence. Numerous\ndatasets\nhave\nbeen\ndeveloped"
        },
        {
          "xvi": "to\nsupport\nthese\nadvancements,\nfacilitating\nthe\ncreation\nof"
        },
        {
          "xvi": "robust\nand\ngeneralizable models\n[17].\nIn\ncontrast,\nsarcasm"
        },
        {
          "xvi": "recognition in speech has not\nreceived comparable attention."
        },
        {
          "xvi": "Although multimodal analysis has recently revitalized interest"
        },
        {
          "xvi": "in incorporating speech data,\nthe field still\nlags behind text-"
        },
        {
          "xvi": "based approaches. Recognizing sarcasm as a purely text-based"
        },
        {
          "xvi": "phenomenon is outdated and inconsistent with findings\nfrom"
        },
        {
          "xvi": "linguistic research [79],\n[80],\n[87],\n[97] which emphasize the"
        },
        {
          "xvi": "multimodal nature of\nsarcasm. To bridge\nthis gap,\nthere\nis"
        },
        {
          "xvi": "a need for more\ncomprehensive\nresearch into how different"
        },
        {
          "xvi": "modalities\ninteract. Such research is\ncrucial\nfor developing"
        },
        {
          "xvi": ""
        },
        {
          "xvi": "sophisticated\narchitectures\ncapable\nof\nrecognizing\nsarcasm"
        },
        {
          "xvi": "more efficiently and effectively."
        },
        {
          "xvi": "3) Explainable\nand\ntrustworthy\nAI:\nThe\nrapid"
        },
        {
          "xvi": "advancements\nin\ndeep\nlearning\nhave\nled\nto\npowerful AI"
        },
        {
          "xvi": "models, yet many of\nthese models\nsuffer\nfrom the\n“black-"
        },
        {
          "xvi": "box”\nproblem, where\ntheir\ndecision-making\nprocesses\nare"
        },
        {
          "xvi": "opaque and difficult\nto interpret. Unlike these models, which"
        },
        {
          "xvi": "rely\non\nvast\namounts\nof\ndata\nand\ncomplex\ncalculations\nto"
        },
        {
          "xvi": "produce\npredictions without\nrevealing\ntheir\ninner working"
        },
        {
          "xvi": "mechanisms, explainable AI aims to make the decision-making"
        },
        {
          "xvi": "process\ntransparent\nand\ncomprehensible.\nIn\nthe\ncontext\nof"
        },
        {
          "xvi": "sarcasm recognition,\nan explainable AI model\nallows us\nto"
        },
        {
          "xvi": "understand how predictions are made, providing insight\ninto"
        },
        {
          "xvi": "the\nsystem’s\nlogic\nand\nenhancing\nits\ntrustworthiness\nand"
        },
        {
          "xvi": "accuracy. Research\nshows\nthat\ntransparency\nin AI\nsystems"
        },
        {
          "xvi": "fosters\ngreater\ntrust,\nparticularly\nin\ncritical\nareas\nsuch\nas"
        },
        {
          "xvi": "healthcare and law enforcement. Furthermore, an interpretable"
        },
        {
          "xvi": "model\nassists developers\nand researchers\nin diagnosing and"
        },
        {
          "xvi": "refining the system by clarifying how it processes information."
        },
        {
          "xvi": "As sarcasm recognition research progresses,\nit\nis essential\nto"
        },
        {
          "xvi": "incorporate explanations that align with linguistic theories of"
        },
        {
          "xvi": "sarcasm cues. This focus on explainability will\nlead to more"
        },
        {
          "xvi": "transparent, accountable, and reliable AI systems."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xvii": "This is evident\nin the success of attention-based architectures"
        },
        {
          "xvii": "that\ncan capture\nsubtle\ncross-modal\nrelationships. However,"
        },
        {
          "xvii": "significant challenges\nremain: current datasets are limited in"
        },
        {
          "xvii": "size and spontaneity, prosodic feature sets lack standardization,"
        },
        {
          "xvii": "and cross-cultural aspects of\nsarcasm remain underexplored."
        },
        {
          "xvii": "By\nbridging\nlinguistics\nand\ncomputational\napproaches,\nthis"
        },
        {
          "xvii": "review provides\na\nfoundation for\nresearchers\ntackling these"
        },
        {
          "xvii": "challenges while highlighting promising directions for\nfuture"
        },
        {
          "xvii": "work."
        },
        {
          "xvii": "In conclusion,\nthis review reveals that while deep learning"
        },
        {
          "xvii": "and multimodal approaches have advanced sarcasm recogni-"
        },
        {
          "xvii": "tion, critical challenges persist. The field stands at an intersec-"
        },
        {
          "xvii": "tion where computational advances\nin attention mechanisms"
        },
        {
          "xvii": "and feature fusion meet\nlinguistic insights about how sarcasm"
        },
        {
          "xvii": "manifests\nacross modalities\nand\ncultures.\nFuture\nprogress"
        },
        {
          "xvii": "requires\nthree\nkey\ndevelopments: first,\nlarger, more\ndiverse"
        },
        {
          "xvii": "datasets that capture spontaneous sarcastic speech across lan-"
        },
        {
          "xvii": "guages;\nsecond,\nstandardized prosodic\nfeature\nsets\ninformed"
        },
        {
          "xvii": "by linguistic\nresearch on sarcasm markers;\nand third, more"
        },
        {
          "xvii": "sophisticated fusion architectures\nthat\ncan better model\nthe"
        },
        {
          "xvii": "subtle\ninterplay\nbetween\nacoustic,\ntextual,\nand\nvisual\ncues."
        },
        {
          "xvii": "Success in these areas would not only advance our theoretical"
        },
        {
          "xvii": "understanding of sarcasm but also enable practical applications"
        },
        {
          "xvii": "that\ncould\nsignificantly\nimpact HMI\nand\nassist\nindividuals"
        },
        {
          "xvii": "with pragmatic language difficulties. As\nsarcasm recognition"
        },
        {
          "xvii": "continues to evolve, maintaining this bridge between linguistic"
        },
        {
          "xvii": "theory and computational practice will be crucial for develop-"
        },
        {
          "xvii": "ing systems that can understand and respond to this nuanced"
        },
        {
          "xvii": "aspect of human communication."
        },
        {
          "xvii": ""
        },
        {
          "xvii": ""
        },
        {
          "xvii": "REFERENCES"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "[1] D. Sperber\nand D. Wilson,\n“Irony\nand\nuse-mention\ndistinction,”\nin"
        },
        {
          "xvii": "Radical Pragmatics, P. Cole, Ed.\nNew York, NY, USA: Academic"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "Press, 1981, pp. 295–318."
        },
        {
          "xvii": "[2]\nJ. Jorgensen, “The functions of sarcastic irony in speech,” J. Pragmat.,"
        },
        {
          "xvii": "vol. 26, no. 5, pp. 613–634, Nov. 1996."
        },
        {
          "xvii": "[3]\nP. Brown and S. C. Levinson, Politeness: Some Universals in Language"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "Usage.\nCambridge, UK: Cambridge University Press, 1987."
        },
        {
          "xvii": ""
        },
        {
          "xvii": "[4] M. A. Seckman and C.\nJ. Couch, “Jocularity,\nsarcasm, and relation-"
        },
        {
          "xvii": "ships: An empirical\nstudy,” J. Contemp. Ethnogr., vol. 18, no. 3, pp."
        },
        {
          "xvii": "327–344, July 1989."
        },
        {
          "xvii": ""
        },
        {
          "xvii": "[5] R. Giora, “On irony and negation,” Discourse Process., vol. 19, no. 2,"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "pp. 239–264, Nov. 1995."
        },
        {
          "xvii": "[6]\nS. McDonald, “Exploring the process of\ninference generation in sar-"
        },
        {
          "xvii": "casm: A review of normal and clinical studies,” Brain Lang., vol. 68,"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "no. 3, pp. 486–506, July 1999."
        },
        {
          "xvii": ""
        },
        {
          "xvii": "[7] H. P. Grice, “Logic and conversation,” in Syntax and Semantics, Vol."
        },
        {
          "xvii": "3: Speech Acts, P. Cole and J. L. Morgan, Eds.\nNew York, NY, USA:"
        },
        {
          "xvii": "Academic Press, 1975, pp. 41–58."
        },
        {
          "xvii": "[8]\nJ. Jorgensen, G. A. Miller, and D. Sperber, “Test of the mention theory"
        },
        {
          "xvii": "of\nirony,” J. Exp. Psychol. Gen., vol. 113, no. 1, pp. 112–120, 1984."
        },
        {
          "xvii": "[9] C. M. Kipps,\nP.\nJ. Nestor,\nJ. Acosta-Cabronero, R. Arnold,\nand"
        },
        {
          "xvii": "J. R. Hodges,\n“Understanding social dysfunction in the behavioural"
        },
        {
          "xvii": "variant of\nfrontotemporal dementia: The role of emotion and sarcasm"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "processing,” Brain, vol. 132, no. 3, pp. 592–603, Jan. 2009."
        },
        {
          "xvii": "[10] A. Persicke, J. Tarbox, J. Ranick, and M. S. Clair, “Teaching children"
        },
        {
          "xvii": "with autism to detect\nand respond to sarcasm,” Res. Autism Spectr."
        },
        {
          "xvii": "Disord., vol. 7, no. 1, pp. 193–198, Jan. 2013."
        },
        {
          "xvii": ""
        },
        {
          "xvii": "J.\n[11]\nP. Rockwell,\n“Lower,\nslower,\nlouder: Vocal\ncues\nof\nsarcasm,”"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "Psycholinguist. Res., vol. 29, no. 5, pp. 483–495, Sep. 2000."
        },
        {
          "xvii": "[12] H.\nS. Cheang\nand M. D.\nPell,\n“Acoustic markers\nof\nsarcasm in"
        },
        {
          "xvii": "cantonese and english,” J. Acoust. Soc. Am., vol. 126, no. 3, pp. 1394–"
        },
        {
          "xvii": ""
        },
        {
          "xvii": "1405, Sep. 2009."
        },
        {
          "xvii": ""
        },
        {
          "xvii": "[13]\nL. Scharrer and U. Christmann, “Voice modulations in german ironic"
        },
        {
          "xvii": "speech,” Lang. Speech, vol. 54, no. 4, pp. 435–465, Dec. 2011."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xviii": "[35] Y. Wu et al., “Modeling incongruity between modalities for multimodal"
        },
        {
          "xviii": "sarcasm detection,” IEEE MultiMedia, vol. 28, no. 2, pp. 86–95, Apr."
        },
        {
          "xviii": "2021."
        },
        {
          "xviii": "[36]\nP. Geng, S. Shi, and H. Guo, “The coding strategy for\nthe mandarin"
        },
        {
          "xviii": "speech\nconveying\nsarcasm in\nacoustic\nand\narticulatory\ndomain,”\nin"
        },
        {
          "xviii": "Proc. 5th Int. Conf. Digit. Signal Process.\n(DSP), Chengdu, China,"
        },
        {
          "xviii": "Feb. 2021, pp. 195–200."
        },
        {
          "xviii": "[37]\nF. Burkhardt, B. Weiss, F. Eyben, J. Deng, and B. Schuller, “Detecting"
        },
        {
          "xviii": "vocal\nirony,” in Proc. 27th Int. Conf. Lang. Technol. Challenges Digit."
        },
        {
          "xviii": "Age (GSCL), vol. 10713, 2018, pp. 11–22."
        },
        {
          "xviii": "[38] H. Atassi, M. T. Riviello, Z. Sm´ekal, A. Hussain,\nand A. Esposito,"
        },
        {
          "xviii": "“Emotional vocal expressions\nrecognition using the cost 2102 italian"
        },
        {
          "xviii": "Inter-\ndatabase of emotional\nspeech,” in Development of Multimodal"
        },
        {
          "xviii": "faces: Active Listening and Synchrony.\nBerlin, Germany: Springer,"
        },
        {
          "xviii": "2010, vol. 5967, pp. 255–267."
        },
        {
          "xviii": "[39] A. Arun,\nI. Rallabhandi,\nS. Hebbar, A. Nair,\nand R.\nJayashree,"
        },
        {
          "xviii": "“Emotion recognition in speech using machine learning techniques,”"
        },
        {
          "xviii": "in Proc. 12th Int. Conf. Comput. Commun. Netw. Technol.\n(ICCCNT),"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Kharagpur,\nIndia, Jul. 2021, pp. 1–7."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "et\n[40] M. K. Hasan\nal.,\n“Humor\nknowledge\nenriched\ntransformer\nfor"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "understanding multimodal humor,” in Proc. AAAI Conf. Artif.\nIntell.,"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "vol. 35, no. 14, May 2021, pp. 12 972–12 980."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[41]\nJ. L. Fleiss, “Measuring nominal scale agreement among many raters,”"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Psychol. Bull., vol. 76, no. 5, pp. 378–382, Nov. 1971."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[42] X. Gao, S. Nayak,\nand M. Coler,\n“Deep cnn-based inductive\ntrans-"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "fer\nlearning for\nsarcasm detection in speech,”\nin Proc.\nInterspeech,"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Incheon, South Korea, Sep. 2022, pp. 2323–2327."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[43] N. S. Sacheth\nand R.\nJayashree,\n“Emotion\nrecognition\nof\nspeech,”"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "14th\nInt. Conf.\nSoft Comput. Pattern Recognit.\nin Proc.\n(SoCPaR)."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Springer, 2023, vol. 490, pp. 359–371."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[44] K. S. Rao, R. Reddy, S. Maity, and S. G. Koolagudi, “Characterization"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "of emotions using the dynamics of prosodic features,” in Proc. Speech"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Prosody, Chicago, IL, USA, May 2010, [Online]. Available: https://ww"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "w.isca-archive.org/speechprosody\n2010/rao10\nspeechprosody.html."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[45] A. Mathur, V. Saxena,\nand S. K. Singh,\n“Understanding sarcasm in"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "speech using mel-frequency cepstral coefficient,” in Proc. 7th Int. Conf."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Cloud Comput., Data Sci. Eng.\n(Confluence), Noida,\nIndia, Jan. 2017,"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "pp. 728–732."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[46] B. Schuller et al., “The interspeech 2013 computational paralinguistics"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Inter-\nchallenge: Social\nsignals,\nconflict,\nemotion,\nautism,”\nin Proc."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "speech, Lyon, France, Aug. 2013, pp. 148–152."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[47] X. Zhang, Y. Chen, and G. Li, “Multi-modal sarcasm detection based"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "on contrastive attention mechanism,” in Proc. Nat. Lang. Process. Chin."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Comput., vol. 13028, 2021, pp. 822–833."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[48]\nS. K. Bharti, R. Gupta, P. K. Shukla, W. A. Hatamleh, H. Tarazi,"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "and S.\nJ. Nuagah,\n“Multimodal\nsarcasm detection: A deep learning"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "approach,” Wirel. Commun. Mob. Comput., vol. 2022, pp. 1–10, 2022,"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "doi: 10.1155/2022/1653696."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[49] N. Ding, S.-W. Tian,\nand L. Yu,\n“A multimodal\nfusion method for"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "sarcasm detection based on late fusion,” Multimed. Tools Appl., vol. 81,"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "no. 6, pp. 8597–8616, Feb. 2022."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[50] Y. Sun, H. Zhang, S. Yang, and J. Wang, “EFAFN: An efficient feature"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "adaptive\nfusion network with facial\nfeature\nfor multimodal\nsarcasm"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "detection,” Appl.\nSci.,\nvol.\n12,\nno.\n21,\np.\n11235, Nov.\n2022,\ndoi:"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "10.3390/app122111235."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[51]\nS. Pramanick, A. Roy, and V. M. Patel, “Multimodal\nlearning using"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "optimal transport for sarcasm and humor detection,” in Proc. IEEE/CVF"
        },
        {
          "xviii": ""
        },
        {
          "xviii": "Winter Conf. Appl. Comput. Vis.\n(WACV), Waikoloa, HI, USA,\nJan."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "2022, pp. 546–556."
        },
        {
          "xviii": ""
        },
        {
          "xviii": "[52] A. Pandey and D. K. Vishwakarma,\n“Multimodal\nsarcasm detection"
        },
        {
          "xviii": "(MSD) in videos using deep learning models,” in Proc. 2023 Int. Conf."
        },
        {
          "xviii": "Adv. Power, Signal,\nInf. Technol.\n(APSIT), 2023, pp. 811–814."
        },
        {
          "xviii": "[53] B. Azahouani, H. Elfaik, E. H. Nfaoui, and S. E. Garouani, “Multi-"
        },
        {
          "xviii": "modal sarcasm detection method using rnn and cnn,” in Proc. 6th Int."
        },
        {
          "xviii": "Conf.\nIntell. Comput. Data Sci.\n(ICDS), Marrakech, Morocco, 2024,"
        },
        {
          "xviii": "pp. 1–6."
        },
        {
          "xviii": "et\n[54] Y. Li\nal.,\n“An\nattention-based,\ncontext-aware multimodal\nfusion"
        },
        {
          "xviii": "method\nfor\nsarcasm detection\nusing\ninter-modality\ninconsistency,”"
        },
        {
          "xviii": "Knowl.-Based Syst., vol. 287, p. 111457, Mar. 2024."
        },
        {
          "xviii": "[55]\nJ. S. Murthy and G. M. Siddesh,\n“A smart video analytical\nframe-"
        },
        {
          "xviii": "work for\nsarcasm detection using novel adaptive fusion network and"
        },
        {
          "xviii": "sarcasnet-99 model,” Vis. Comput., vol. 40, no. 11, pp. 8085–8097,"
        },
        {
          "xviii": "Nov. 2024."
        },
        {
          "xviii": "[56] R. Manohar and S. Swamy, “Voice-based sarcasm detection in kannada"
        },
        {
          "xviii": "language,” Int. J. Intell. Syst. Appl. Eng., vol. 12, no. 14s, pp. 356–367,"
        },
        {
          "xviii": "Feb. 2024,\n[Online]. Available: https://ijisae.org/index.php/IJISAE/arti"
        },
        {
          "xviii": "cle/view/4672."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xix": "[80]\nJ. T. Hancock, “Verbal irony use in face-to-face and computer-mediated"
        },
        {
          "xix": "conversations,”\nJ. Lang. Soc. Psychol., vol. 23, no. 4, pp. 447–463,"
        },
        {
          "xix": "2004."
        },
        {
          "xix": "[81]\nJ. H., B. Kreifelts, S. Nizielski, A. Sch¨utz, and D. Wildgruber, “Effects"
        },
        {
          "xix": "of\nemotional\nintelligence on the\nimpression of\nirony created by the"
        },
        {
          "xix": "mismatch between verbal and nonverbal cues,” PLOS ONE, vol. 11,"
        },
        {
          "xix": "no. 10, p. e0163211, 2016, doi: 10.1371/journal.pone.0163211."
        },
        {
          "xix": "[82]\nF. Zhao, C. Zhang, and B. Geng, “Deep multimodal data fusion,” ACM"
        },
        {
          "xix": "Comput. Surv., vol. 56, no. 9, pp. 1–36, Oct. 2024."
        },
        {
          "xix": "Statistical Methods\n[83]\nL. V. Hedges\nand\nI. Olkin,\nfor Meta-Analysis."
        },
        {
          "xix": "Academic Press, 1985."
        },
        {
          "xix": "[84]\nJ. P. Higgins\nand S. G. Thompson,\n“Quantifying heterogeneity in a"
        },
        {
          "xix": "meta-analysis,” Stat. Med., vol. 21, no. 11, pp. 1539–1558, Jun. 2002."
        },
        {
          "xix": "[85] R. DerSimonian and N. Laird, “Meta-analysis in clinical\ntrials,” Con-"
        },
        {
          "xix": "trol. Clin. Trials, vol. 7, no. 3, pp. 177–188, Sep. 1986."
        },
        {
          "xix": "[86] A. Esposito, M. T. Riviello, and G. Maio, “The cost 2102 italian audio"
        },
        {
          "xix": "and video emotional database,” in Front. Artif. Intell. Appl., Jan. 2009,"
        },
        {
          "xix": "vol. 204, pp. 51–61."
        },
        {
          "xix": "[87]\nE. Camp,\n“Sarcasm, pretense,\nand the\nsemantics/pragmatics distinc-"
        },
        {
          "xix": "tion,” Noˆus, vol. 46, no. 4, pp. 587–634, Dec. 2012."
        },
        {
          "xix": "[88]\nS. Chen et al., “WavLM: Large-scale self-supervised pre-training for"
        },
        {
          "xix": "full stack speech processing,” IEEE J. Sel. Top. Signal Process., vol. 16,"
        },
        {
          "xix": "no. 6, pp. 1505–1518, Oct. 2022."
        },
        {
          "xix": "et\n[89]\nZ.\nZhang\nal.,\n“SpeechLM:\nEnhanced\nspeech\npre-training with"
        },
        {
          "xix": "unpaired textual data,” IEEE/ACM Trans. Audio Speech Lang. Process.,"
        },
        {
          "xix": "vol. 32, pp. 2177–2187, 2024."
        },
        {
          "xix": "[90] D. Diatlova, A. Udalov, V. Shutov, and E. Spirin, “Adapting WavLM"
        },
        {
          "xix": "for speech emotion recognition,” in Proc. Odyssey 2024: The Speaker"
        },
        {
          "xix": "and Language Recognition Workshop, 2024, pp. 303–308."
        },
        {
          "xix": "et\n[91]\nJ.\nAchiam\nal.,\n“GPT-4\ntechnical\nreport,”\narXiv\npreprint"
        },
        {
          "xix": "arXiv:2303.08774, 2023,\n[Online]. Available: https://doi.org/10.485"
        },
        {
          "xix": "50/arXiv.2303.08774."
        },
        {
          "xix": "[92] H. Touvron et al., “LLaMA: Open and efficient\nfoundation language"
        },
        {
          "xix": "models,” arXiv preprint arXiv:2302.13971, 2023,\n[Online]. Available:"
        },
        {
          "xix": "https://doi.org/10.48550/arXiv.2302.13971."
        },
        {
          "xix": "[93] Y. Zhang, C. Zou, Z. Lian, P. Tiwari,\nand J. Qin,\n“SarcasmBench:"
        },
        {
          "xix": "Towards evaluating large language models on sarcasm understanding,”"
        },
        {
          "xix": "arXiv preprint arXiv:2408.11319, 2024,\n[Online]. Available: https://do"
        },
        {
          "xix": "i.org/10.48550/arXiv.2408.11319."
        },
        {
          "xix": "[94] A. Radford et al., “Learning transferable visual models\nfrom natural"
        },
        {
          "xix": "language\nsupervision,”\narXiv preprint\narXiv:2103.00020, 2021,\n[On-"
        },
        {
          "xix": "line]. Available: https://doi.org/10.48550/arXiv.2103.00020."
        },
        {
          "xix": "[95] M. Aguert, “Paraverbal expression of verbal\nirony: Vocal cues matter"
        },
        {
          "xix": "and facial cues even more,” J. Nonverbal Behav., vol. 46, no. 1, pp."
        },
        {
          "xix": "45–70, Mar. 2022."
        },
        {
          "xix": "[96] K. Rothermich et al., “Tracking nonliteral\nlanguage processing using"
        },
        {
          "xix": "audiovisual scenarios,” Can. J. Exp. Psychol., vol. 75, no. 2, pp. 211–"
        },
        {
          "xix": "220, 2021."
        },
        {
          "xix": "[97]\nS. Li, A. Chen, Y. Chen, and P. Tang, “The role of auditory and visual"
        },
        {
          "xix": "cues in the interpretation of mandarin ironic speech,” J. Pragmat., vol."
        },
        {
          "xix": "201, pp. 3–14, 2022."
        },
        {
          "xix": "[98] K. Bromberek-Dyzman, K. Jankowiak, and P. Chełminiak, “Modality"
        },
        {
          "xix": "matters: Testing bilingual\nirony comprehension in the textual, auditory,"
        },
        {
          "xix": "and audio-visual modality,” J. Pragmat., vol. 180, pp. 219–231, 2021."
        },
        {
          "xix": "[99] G. Deliens, K. Antoniou, E. Clin, E. Ostashchenko, and M. Kissine,"
        },
        {
          "xix": "“Context,\nfacial expression and prosody in irony processing,” J. Mem."
        },
        {
          "xix": "Lang., vol. 99, pp. 35–48, 2018."
        },
        {
          "xix": "Speech\n[100] H.\nS. Cheang\nand M. D.\nPell,\n“The\nsound\nof\nsarcasm,”"
        },
        {
          "xix": "Commun., vol. 50, no. 5, pp. 366–381, May 2008."
        },
        {
          "xix": "[101]\nL. Anolli, R. Ciceri, and M. G.\nInfantino, “From ‘blame by praise’\nto"
        },
        {
          "xix": "‘praise by blame’: Analysis of vocal patterns in ironic communication,”"
        },
        {
          "xix": "Int. J. Psychol., vol. 37, no. 5, pp. 266–276, Oct. 2002."
        },
        {
          "xix": "[102]\nJ. M. Mandler, “Categorical and schematic organization in memory,”"
        },
        {
          "xix": "in Memory, Organization, and Structure, R. C. Puff, Ed.\nNew York,"
        },
        {
          "xix": "NY, USA: Academic Press, 1979, pp. 259–299."
        },
        {
          "xix": "[103]\nJ. Kim, “How Korean EFL learners understand sarcasm in L2 English,”"
        },
        {
          "xix": "J. Pragmat., vol. 60, pp. 193–206, 2014."
        },
        {
          "xix": "[104] Y. Zhang, Y. Yu, M. Wang, M. Huang,\nand M. S. Hossain,\n“Self-"
        },
        {
          "xix": "adaptive representation learning model\nfor multi-modal sentiment and"
        },
        {
          "xix": "sarcasm joint\nanalysis,” ACM Trans. Multimedia Comput. Commun."
        },
        {
          "xix": "Appl., vol. 20, no. 5, May 2024."
        },
        {
          "xix": "[105] R. Krishnamaneni, M. Kurni,\nS.\nSen,\nand A. Murthy,\n“Modified"
        },
        {
          "xix": "convolutional neural network with multiple\nfeatures\nfor multimodal"
        },
        {
          "xix": "Inf. Technol.\nsarcasm detection,” in Proc. 2nd Int. Conf. Recent Adv."
        },
        {
          "xix": "Sustain. Dev.\n(ICRAIS), Manipal,\nIndia, 2024, pp. 160–165."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "xix": "[80]\nJ. T. Hancock, “Verbal irony use in face-to-face and computer-mediated"
        },
        {
          "xix": "conversations,”\nJ. Lang. Soc. Psychol., vol. 23, no. 4, pp. 447–463,"
        },
        {
          "xix": "2004."
        },
        {
          "xix": "[81]\nJ. H., B. Kreifelts, S. Nizielski, A. Sch¨utz, and D. Wildgruber, “Effects"
        },
        {
          "xix": "of\nemotional\nintelligence on the\nimpression of\nirony created by the"
        },
        {
          "xix": "mismatch between verbal and nonverbal cues,” PLOS ONE, vol. 11,"
        },
        {
          "xix": "no. 10, p. e0163211, 2016, doi: 10.1371/journal.pone.0163211."
        },
        {
          "xix": "[82]\nF. Zhao, C. Zhang, and B. Geng, “Deep multimodal data fusion,” ACM"
        },
        {
          "xix": "Comput. Surv., vol. 56, no. 9, pp. 1–36, Oct. 2024."
        },
        {
          "xix": "Statistical Methods\n[83]\nL. V. Hedges\nand\nI. Olkin,\nfor Meta-Analysis."
        },
        {
          "xix": "Academic Press, 1985."
        },
        {
          "xix": "[84]\nJ. P. Higgins\nand S. G. Thompson,\n“Quantifying heterogeneity in a"
        },
        {
          "xix": "meta-analysis,” Stat. Med., vol. 21, no. 11, pp. 1539–1558, Jun. 2002."
        },
        {
          "xix": "[85] R. DerSimonian and N. Laird, “Meta-analysis in clinical\ntrials,” Con-"
        },
        {
          "xix": "trol. Clin. Trials, vol. 7, no. 3, pp. 177–188, Sep. 1986."
        },
        {
          "xix": "[86] A. Esposito, M. T. Riviello, and G. Maio, “The cost 2102 italian audio"
        },
        {
          "xix": "and video emotional database,” in Front. Artif. Intell. Appl., Jan. 2009,"
        },
        {
          "xix": "vol. 204, pp. 51–61."
        },
        {
          "xix": "[87]\nE. Camp,\n“Sarcasm, pretense,\nand the\nsemantics/pragmatics distinc-"
        },
        {
          "xix": "tion,” Noˆus, vol. 46, no. 4, pp. 587–634, Dec. 2012."
        },
        {
          "xix": "[88]\nS. Chen et al., “WavLM: Large-scale self-supervised pre-training for"
        },
        {
          "xix": "full stack speech processing,” IEEE J. Sel. Top. Signal Process., vol. 16,"
        },
        {
          "xix": "no. 6, pp. 1505–1518, Oct. 2022."
        },
        {
          "xix": "et\n[89]\nZ.\nZhang\nal.,\n“SpeechLM:\nEnhanced\nspeech\npre-training with"
        },
        {
          "xix": "unpaired textual data,” IEEE/ACM Trans. Audio Speech Lang. Process.,"
        },
        {
          "xix": "vol. 32, pp. 2177–2187, 2024."
        },
        {
          "xix": "[90] D. Diatlova, A. Udalov, V. Shutov, and E. Spirin, “Adapting WavLM"
        },
        {
          "xix": "for speech emotion recognition,” in Proc. Odyssey 2024: The Speaker"
        },
        {
          "xix": "and Language Recognition Workshop, 2024, pp. 303–308."
        },
        {
          "xix": "et\n[91]\nJ.\nAchiam\nal.,\n“GPT-4\ntechnical\nreport,”\narXiv\npreprint"
        },
        {
          "xix": "arXiv:2303.08774, 2023,\n[Online]. Available: https://doi.org/10.485"
        },
        {
          "xix": "50/arXiv.2303.08774."
        },
        {
          "xix": "[92] H. Touvron et al., “LLaMA: Open and efficient\nfoundation language"
        },
        {
          "xix": "models,” arXiv preprint arXiv:2302.13971, 2023,\n[Online]. Available:"
        },
        {
          "xix": "https://doi.org/10.48550/arXiv.2302.13971."
        },
        {
          "xix": "[93] Y. Zhang, C. Zou, Z. Lian, P. Tiwari,\nand J. Qin,\n“SarcasmBench:"
        },
        {
          "xix": "Towards evaluating large language models on sarcasm understanding,”"
        },
        {
          "xix": "arXiv preprint arXiv:2408.11319, 2024,\n[Online]. Available: https://do"
        },
        {
          "xix": "i.org/10.48550/arXiv.2408.11319."
        },
        {
          "xix": "[94] A. Radford et al., “Learning transferable visual models\nfrom natural"
        },
        {
          "xix": "language\nsupervision,”\narXiv preprint\narXiv:2103.00020, 2021,\n[On-"
        },
        {
          "xix": "line]. Available: https://doi.org/10.48550/arXiv.2103.00020."
        },
        {
          "xix": "[95] M. Aguert, “Paraverbal expression of verbal\nirony: Vocal cues matter"
        },
        {
          "xix": "and facial cues even more,” J. Nonverbal Behav., vol. 46, no. 1, pp."
        },
        {
          "xix": "45–70, Mar. 2022."
        },
        {
          "xix": "[96] K. Rothermich et al., “Tracking nonliteral\nlanguage processing using"
        },
        {
          "xix": "audiovisual scenarios,” Can. J. Exp. Psychol., vol. 75, no. 2, pp. 211–"
        },
        {
          "xix": "220, 2021."
        },
        {
          "xix": "[97]\nS. Li, A. Chen, Y. Chen, and P. Tang, “The role of auditory and visual"
        },
        {
          "xix": "cues in the interpretation of mandarin ironic speech,” J. Pragmat., vol."
        },
        {
          "xix": "201, pp. 3–14, 2022."
        },
        {
          "xix": "[98] K. Bromberek-Dyzman, K. Jankowiak, and P. Chełminiak, “Modality"
        },
        {
          "xix": "matters: Testing bilingual\nirony comprehension in the textual, auditory,"
        },
        {
          "xix": "and audio-visual modality,” J. Pragmat., vol. 180, pp. 219–231, 2021."
        },
        {
          "xix": "[99] G. Deliens, K. Antoniou, E. Clin, E. Ostashchenko, and M. Kissine,"
        },
        {
          "xix": "“Context,\nfacial expression and prosody in irony processing,” J. Mem."
        },
        {
          "xix": "Lang., vol. 99, pp. 35–48, 2018."
        },
        {
          "xix": "Speech\n[100] H.\nS. Cheang\nand M. D.\nPell,\n“The\nsound\nof\nsarcasm,”"
        },
        {
          "xix": "Commun., vol. 50, no. 5, pp. 366–381, May 2008."
        },
        {
          "xix": "[101]\nL. Anolli, R. Ciceri, and M. G.\nInfantino, “From ‘blame by praise’\nto"
        },
        {
          "xix": "‘praise by blame’: Analysis of vocal patterns in ironic communication,”"
        },
        {
          "xix": "Int. J. Psychol., vol. 37, no. 5, pp. 266–276, Oct. 2002."
        },
        {
          "xix": "[102]\nJ. M. Mandler, “Categorical and schematic organization in memory,”"
        },
        {
          "xix": "in Memory, Organization, and Structure, R. C. Puff, Ed.\nNew York,"
        },
        {
          "xix": "NY, USA: Academic Press, 1979, pp. 259–299."
        },
        {
          "xix": "[103]\nJ. Kim, “How Korean EFL learners understand sarcasm in L2 English,”"
        },
        {
          "xix": "J. Pragmat., vol. 60, pp. 193–206, 2014."
        },
        {
          "xix": "[104] Y. Zhang, Y. Yu, M. Wang, M. Huang,\nand M. S. Hossain,\n“Self-"
        },
        {
          "xix": "adaptive representation learning model\nfor multi-modal sentiment and"
        },
        {
          "xix": "sarcasm joint\nanalysis,” ACM Trans. Multimedia Comput. Commun."
        },
        {
          "xix": "Appl., vol. 20, no. 5, May 2024."
        },
        {
          "xix": "[105] R. Krishnamaneni, M. Kurni,\nS.\nSen,\nand A. Murthy,\n“Modified"
        },
        {
          "xix": "convolutional neural network with multiple\nfeatures\nfor multimodal"
        },
        {
          "xix": "Inf. Technol.\nsarcasm detection,” in Proc. 2nd Int. Conf. Recent Adv."
        },
        {
          "xix": "Sustain. Dev.\n(ICRAIS), Manipal,\nIndia, 2024, pp. 160–165."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[106]": "",
          "et\nF. Eyben": "(GeMAPS)",
          "al.,\n“The\ngeneva minimalistic": "for voice research and affective computing,” IEEE Trans.",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "",
          "al.,\n“The\ngeneva minimalistic": "Affect. Comput., vol. 7, no. 2, pp. 190–202, 2016.",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "[107] M. Spitale, F. Catania, and F. Panzeri, “Understanding non-verbal irony",
          "al.,\n“The\ngeneva minimalistic": "",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "",
          "al.,\n“The\ngeneva minimalistic": "markers: Machine learning insights versus human judgment,” in Proc.",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "",
          "al.,\n“The\ngeneva minimalistic": "26th ACM Int. Conf. Multimodal Interact. (ICMI), Nov. 2024, pp. 164–",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "172.",
          "al.,\n“The\ngeneva minimalistic": "",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "[108]",
          "et\nF. Eyben": "J.",
          "al.,\n“The\ngeneva minimalistic": "J. Godfrey, E. C. Holliman, and J. McDaniel, “SWITCHBOARD:",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "",
          "al.,\n“The\ngeneva minimalistic": "Telephone speech corpus for research and development,” in Proc. IEEE",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "Int. Conf. Acoust.,",
          "al.,\n“The\ngeneva minimalistic": "Speech,\nSignal Process.",
          "acoustic": "(ICASSP),",
          "parameter\nset": "vol.\n1,\nSan"
        },
        {
          "[106]": "",
          "et\nF. Eyben": "",
          "al.,\n“The\ngeneva minimalistic": "Francisco, CA, USA, Mar. 1992, pp. 517–520.",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "[109] C. Cieri, D. Miller, and K. Walker, “The Fisher corpus: A resource",
          "al.,\n“The\ngeneva minimalistic": "",
          "acoustic": "",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "for\nthe\nnext",
          "al.,\n“The\ngeneva minimalistic": "generations\nof\nspeech-to-text,”",
          "acoustic": "in Proc.",
          "parameter\nset": "4th\nInt. Conf."
        },
        {
          "[106]": "",
          "et\nF. Eyben": "Lang. Resour. Eval.",
          "al.,\n“The\ngeneva minimalistic": "",
          "acoustic": "(LREC), Lisbon, Portugal, May 2004, pp. 69–71,",
          "parameter\nset": ""
        },
        {
          "[106]": "",
          "et\nF. Eyben": "",
          "al.,\n“The\ngeneva minimalistic": "[Online]. Available: https://aclanthology.org/L04-1500/.",
          "acoustic": "",
          "parameter\nset": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xiyuan Gao received the Master’s degree in Speech": "and Language Processing from Konstanz University,"
        },
        {
          "Xiyuan Gao received the Master’s degree in Speech": "Konstanz, Germany. She is currently a Ph.D. candi-"
        },
        {
          "Xiyuan Gao received the Master’s degree in Speech": "date from Speech Technology at the Faculty Campus"
        },
        {
          "Xiyuan Gao received the Master’s degree in Speech": "Fryslˆan, University of Groningen,"
        },
        {
          "Xiyuan Gao received the Master’s degree in Speech": "Her\nresearch\ninterests\ninclude"
        },
        {
          "Xiyuan Gao received the Master’s degree in Speech": "driven\nsarcasm detection, multi-modal"
        },
        {
          "Xiyuan Gao received the Master’s degree in Speech": "speech technology."
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Modality: T = text, A = audio, V = visual."
        },
        {
          "TABLE A1": "Classifier"
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "FCNN"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "SVM"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "SVM"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "FCNN"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Sigmoid layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "Softmax layer"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": "FCNN"
        },
        {
          "TABLE A1": ""
        },
        {
          "TABLE A1": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ii": "F1-score: 74.67 [53]"
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 74.3 [70]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 74.2 [29]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 71.1 [23]"
        },
        {
          "ii": ""
        },
        {
          "ii": "Accuracy: 93.1 [24]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 75.72 [25]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 73.9 [26]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "Accuracy: 99.05 [55]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "Accuracy: 81.8 [57]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 80 [107]"
        },
        {
          "ii": ""
        },
        {
          "ii": "Accuracy: 74.1 [36]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": "F1-score: 76.5 [56]"
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        },
        {
          "ii": ""
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "Dataset"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "Switchboard [108]"
        },
        {
          "TABLE A2": "and Fisher\n[109]\n(en)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "COST 2102 Italian"
        },
        {
          "TABLE A2": "Database of"
        },
        {
          "TABLE A2": "Emotional Speech (it)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "Custom dataset\n(en)"
        },
        {
          "TABLE A2": "Custom dataset\n(hi)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "Custom dataset\n(de)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "Custom dataset\n(hi)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "MUStARD (en)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": "IIT-KGP-SEHSC (hi)"
        },
        {
          "TABLE A2": ""
        },
        {
          "TABLE A2": ""
        }
      ],
      "page": 22
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Irony and use-mention distinction",
      "authors": [
        "D Sperber",
        "D Wilson"
      ],
      "year": "1981",
      "venue": "Irony and use-mention distinction"
    },
    {
      "citation_id": "2",
      "title": "The functions of sarcastic irony in speech",
      "authors": [
        "J Jorgensen"
      ],
      "year": "1996",
      "venue": "J. Pragmat"
    },
    {
      "citation_id": "3",
      "title": "Politeness: Some Universals in Language Usage",
      "authors": [
        "P Brown",
        "S Levinson"
      ],
      "year": "1987",
      "venue": "Politeness: Some Universals in Language Usage"
    },
    {
      "citation_id": "4",
      "title": "Jocularity, sarcasm, and relationships: An empirical study",
      "authors": [
        "M Seckman",
        "C Couch"
      ],
      "year": "1989",
      "venue": "J. Contemp. Ethnogr"
    },
    {
      "citation_id": "5",
      "title": "On irony and negation",
      "authors": [
        "R Giora"
      ],
      "year": "1995",
      "venue": "On irony and negation"
    },
    {
      "citation_id": "6",
      "title": "Exploring the process of inference generation in sarcasm: A review of normal and clinical studies",
      "authors": [
        "S Mcdonald"
      ],
      "year": "1999",
      "venue": "Brain Lang"
    },
    {
      "citation_id": "7",
      "title": "Logic and conversation",
      "authors": [
        "H Grice"
      ],
      "year": "1975",
      "venue": "Syntax and Semantics"
    },
    {
      "citation_id": "8",
      "title": "Test of the mention theory of irony",
      "authors": [
        "J Jorgensen",
        "G Miller",
        "D Sperber"
      ],
      "year": "1984",
      "venue": "J. Exp. Psychol. Gen"
    },
    {
      "citation_id": "9",
      "title": "Understanding social dysfunction in the behavioural variant of frontotemporal dementia: The role of emotion and sarcasm processing",
      "authors": [
        "C Kipps",
        "P Nestor",
        "J Acosta-Cabronero",
        "R Arnold",
        "J Hodges"
      ],
      "year": "2009",
      "venue": "Brain"
    },
    {
      "citation_id": "10",
      "title": "Teaching children with autism to detect and respond to sarcasm",
      "authors": [
        "A Persicke",
        "J Tarbox",
        "J Ranick",
        "M Clair"
      ],
      "year": "2013",
      "venue": "Res. Autism Spectr. Disord"
    },
    {
      "citation_id": "11",
      "title": "Lower, slower, louder: Vocal cues of sarcasm",
      "authors": [
        "P Rockwell"
      ],
      "year": "2000",
      "venue": "J. Psycholinguist. Res"
    },
    {
      "citation_id": "12",
      "title": "Acoustic markers of sarcasm in cantonese and english",
      "authors": [
        "H Cheang",
        "M Pell"
      ],
      "year": "2009",
      "venue": "J. Acoust. Soc. Am"
    },
    {
      "citation_id": "13",
      "title": "Voice modulations in german ironic speech",
      "authors": [
        "L Scharrer",
        "U Christmann"
      ],
      "year": "2011",
      "venue": "Lang. Speech"
    },
    {
      "citation_id": "14",
      "title": "Prosodic cues of sarcastic speech in french: Slower, higher, wider",
      "authors": [
        "H Loevenbruck",
        "M Jannet",
        "M D'imperio",
        "M Spini",
        "M Champagne-Lavau"
      ],
      "year": "2013",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "'yeah right': Sarcasm recognition for spoken dialogue systems",
      "authors": [
        "J Tepperman",
        "D Traum",
        "S Narayanan"
      ],
      "year": "2006",
      "venue": "Proc. Interspeech, Pittsburgh"
    },
    {
      "citation_id": "16",
      "title": "'sure, i did the right thing': A system for sarcasm detection in speech",
      "authors": [
        "R Rakov",
        "A Rosenberg"
      ],
      "year": "2013",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2017",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "18",
      "title": "Sarcasm identification in textual data: Systematic review, research challenges and open directions",
      "authors": [
        "C Eke",
        "A Norman",
        "L Shuib",
        "H Nweke"
      ],
      "year": "2020",
      "venue": "Artif. Intell. Rev"
    },
    {
      "citation_id": "19",
      "title": "Literature survey of sarcasm detection",
      "authors": [
        "P Chaudhari",
        "C Chandankhede"
      ],
      "year": "2017",
      "venue": "Proc. 2017 Int. Conf. Wireless Commun., Signal Process. Netw. (WiSPNET)"
    },
    {
      "citation_id": "20",
      "title": "Preferred reporting items for systematic reviews and meta-analyses: The prisma statement",
      "authors": [
        "D Moher",
        "A Liberati",
        "J Tetzlaff",
        "D Altman",
        "T Group"
      ],
      "year": "2009",
      "venue": "PLOS Med",
      "doi": "10.1371/jour-nal.pmed.1000097"
    },
    {
      "citation_id": "21",
      "title": "Towards multimodal sarcasm detection (an obviously perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annu. Meeting Assoc. Comput. Linguistics (ACL)"
    },
    {
      "citation_id": "22",
      "title": "IITKGP-SEHSC: Hindi speech corpus for emotion analysis",
      "authors": [
        "S Koolagudi",
        "R Reddy",
        "J Yadav",
        "K Rao"
      ],
      "year": "2011",
      "venue": "Proc. 2011 Int. Conf. Devices Commun. (ICDeCom)"
    },
    {
      "citation_id": "23",
      "title": "Multi-modal sarcasm detection and humor classification in code-mixed conversations",
      "authors": [
        "M Bedi",
        "S Kumar",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "24",
      "title": "¡Qué maravilla! multimodal sarcasm detection in spanish: A dataset and a baseline",
      "authors": [
        "K Alnajjar",
        "M Hämäläinen"
      ],
      "year": "2021",
      "venue": "Proc. 3rd Workshop Multimodal Artif. Intell"
    },
    {
      "citation_id": "25",
      "title": "CMMA: Benchmarking multi-affection detection in Chinese multi-modal conversations",
      "authors": [
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Proc. 37th Int. Conf. Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "26",
      "title": "Deep learning for acoustic irony classification in spontaneous speech",
      "authors": [
        "H Gent",
        "C Adams",
        "C Shih",
        "Y Tang"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multimodal sarcasm, sentiment, and emotion analysis",
      "authors": [
        "D Chauhan",
        "D . R",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proc. 58th Annu. Meet. Assoc. Comput. Linguistics (ACL)"
    },
    {
      "citation_id": "28",
      "title": "An emoji-aware multitask framework for multimodal sarcasm detection",
      "authors": [
        "D Chauhan",
        "G Singh",
        "A Arora",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Knowl.-Based Syst",
      "doi": "10.1016/j.knosys.2022.109924"
    },
    {
      "citation_id": "29",
      "title": "A multimodal corpus for emotion recognition in sarcasm",
      "authors": [
        "A Ray",
        "S Mishra",
        "A Nunna",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Proc. 13th Lang. Resour. Eval. Conf. (LREC)"
    },
    {
      "citation_id": "30",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annu. Meet. Assoc. Comput. Linguistics (ACL)"
    },
    {
      "citation_id": "31",
      "title": "Which contextual and sociocultural information predict irony perception?",
      "authors": [
        "E Rivière",
        "M Champagne-Lavau"
      ],
      "year": "2020",
      "venue": "Discourse Process"
    },
    {
      "citation_id": "32",
      "title": "Verbal irony as implicit display of ironic environment: Distinguishing ironic utterances from nonirony",
      "authors": [
        "A Utsumi"
      ],
      "year": "2000",
      "venue": "J. Pragmat"
    },
    {
      "citation_id": "33",
      "title": "How about another piece of pie: The allusional pretense theory of discourse irony",
      "authors": [
        "S Kumon-Nakamura",
        "S Glucksberg",
        "M Brown"
      ],
      "year": "1995",
      "venue": "J. Exp. Psychol. Gen"
    },
    {
      "citation_id": "34",
      "title": "The exposure advantage: Early exposure to a multilingual environment promotes effective communication",
      "authors": [
        "S Fan",
        "Z Liberman",
        "B Keysar",
        "K Kinzler"
      ],
      "year": "2015",
      "venue": "Psychol. Sci"
    },
    {
      "citation_id": "35",
      "title": "Modeling incongruity between modalities for multimodal sarcasm detection",
      "authors": [
        "Y Wu"
      ],
      "year": "2021",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "36",
      "title": "The coding strategy for the mandarin speech conveying sarcasm in acoustic and articulatory domain",
      "authors": [
        "P Geng",
        "S Shi",
        "H Guo"
      ],
      "year": "2021",
      "venue": "Proc. 5th Int. Conf. Digit. Signal Process. (DSP)"
    },
    {
      "citation_id": "37",
      "title": "Detecting vocal irony",
      "authors": [
        "F Burkhardt",
        "B Weiss",
        "F Eyben",
        "J Deng",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. 27th Int. Conf. Lang. Technol. Challenges Digit. Age (GSCL)"
    },
    {
      "citation_id": "38",
      "title": "Emotional vocal expressions recognition using the cost 2102 italian database of emotional speech",
      "authors": [
        "H Atassi",
        "M Riviello",
        "Z Smékal",
        "A Hussain",
        "A Esposito"
      ],
      "year": "2010",
      "venue": "Development of Multimodal Interfaces: Active Listening and Synchrony"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition in speech using machine learning techniques",
      "authors": [
        "A Arun",
        "I Rallabhandi",
        "S Hebbar",
        "A Nair",
        "R Jayashree"
      ],
      "year": "2021",
      "venue": "Proc. 12th Int. Conf. Comput"
    },
    {
      "citation_id": "40",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "M Hasan"
      ],
      "year": "2021",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "41",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "J Fleiss"
      ],
      "year": "1971",
      "venue": "Psychol. Bull"
    },
    {
      "citation_id": "42",
      "title": "Deep cnn-based inductive transfer learning for sarcasm detection in speech",
      "authors": [
        "X Gao",
        "S Nayak",
        "M Coler"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition of speech",
      "authors": [
        "N Sacheth",
        "R Jayashree"
      ],
      "year": "2023",
      "venue": "Proc. 14th Int. Conf. Soft Comput. Pattern Recognit. (SoCPaR)"
    },
    {
      "citation_id": "44",
      "title": "Characterization of emotions using the dynamics of prosodic features",
      "authors": [
        "K Rao",
        "R Reddy",
        "S Maity",
        "S Koolagudi"
      ],
      "year": "2010",
      "venue": "Proc. Speech Prosody"
    },
    {
      "citation_id": "45",
      "title": "Understanding sarcasm in speech using mel-frequency cepstral coefficient",
      "authors": [
        "A Mathur",
        "V Saxena",
        "S Singh"
      ],
      "year": "2017",
      "venue": "Proc. 7th Int. Conf. Cloud Comput"
    },
    {
      "citation_id": "46",
      "title": "The interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "47",
      "title": "Multi-modal sarcasm detection based on contrastive attention mechanism",
      "authors": [
        "X Zhang",
        "Y Chen",
        "G Li"
      ],
      "year": "2021",
      "venue": "Proc. Nat"
    },
    {
      "citation_id": "48",
      "title": "Multimodal sarcasm detection: A deep learning approach",
      "authors": [
        "S Bharti",
        "R Gupta",
        "P Shukla",
        "W Hatamleh",
        "H Tarazi",
        "S Nuagah"
      ],
      "year": "2022",
      "venue": "Wirel. Commun. Mob. Comput",
      "doi": "10.1155/2022/1653696"
    },
    {
      "citation_id": "49",
      "title": "A multimodal fusion method for sarcasm detection based on late fusion",
      "authors": [
        "N Ding",
        "S.-W Tian",
        "L Yu"
      ],
      "year": "2022",
      "venue": "Multimed. Tools Appl"
    },
    {
      "citation_id": "50",
      "title": "EFAFN: An efficient feature adaptive fusion network with facial feature for multimodal sarcasm detection",
      "authors": [
        "Y Sun",
        "H Zhang",
        "S Yang",
        "J Wang"
      ],
      "year": "2022",
      "venue": "Appl. Sci",
      "doi": "10.3390/app122111235"
    },
    {
      "citation_id": "51",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "S Pramanick",
        "A Roy",
        "V Patel"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV)"
    },
    {
      "citation_id": "52",
      "title": "Multimodal sarcasm detection (MSD) in videos using deep learning models",
      "authors": [
        "A Pandey",
        "D Vishwakarma"
      ],
      "year": "2023",
      "venue": "Proc. 2023 Int. Conf. Adv. Power, Signal"
    },
    {
      "citation_id": "53",
      "title": "Multimodal sarcasm detection method using rnn and cnn",
      "authors": [
        "B Azahouani",
        "H Elfaik",
        "E Nfaoui",
        "S Garouani"
      ],
      "year": "2024",
      "venue": "Proc. 6th Int. Conf. Intell. Comput. Data Sci. (ICDS)"
    },
    {
      "citation_id": "54",
      "title": "An attention-based, context-aware multimodal fusion method for sarcasm detection using inter-modality inconsistency",
      "authors": [
        "Y Li"
      ],
      "year": "2024",
      "venue": "Knowl.-Based Syst"
    },
    {
      "citation_id": "55",
      "title": "A smart video analytical framework for sarcasm detection using novel adaptive fusion network and sarcasnet-99 model",
      "authors": [
        "J Murthy",
        "G Siddesh"
      ],
      "year": "2024",
      "venue": "Vis. Comput"
    },
    {
      "citation_id": "56",
      "title": "Voice-based sarcasm detection in kannada language",
      "authors": [
        "R Manohar",
        "S Swamy"
      ],
      "year": "2024",
      "venue": "Int. J. Intell. Syst. Appl. Eng"
    },
    {
      "citation_id": "57",
      "title": "Sarcasm detection using cognitive features of visual data by learning model",
      "authors": [
        "B Hiremath",
        "M Patil"
      ],
      "year": "2021",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "58",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)"
    },
    {
      "citation_id": "59",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "60",
      "title": "A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations",
      "authors": [
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "61",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Proc"
    },
    {
      "citation_id": "62",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "63",
      "title": "PRAAT, a system for doing phonetics by computer",
      "authors": [
        "P Boersma"
      ],
      "year": "2001",
      "venue": "Glot Int"
    },
    {
      "citation_id": "64",
      "title": "OpenSMILE: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proc. 18th ACM Int. Conf. Multimedia"
    },
    {
      "citation_id": "65",
      "title": "Librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee"
      ],
      "year": "2015",
      "venue": "Proc. Python in Sci. Conf. (SciPy)",
      "doi": "10.25080/Majora-7b98e3ed-003"
    },
    {
      "citation_id": "66",
      "title": "CO-VAREP: A collaborative voice analysis repository for speech technologies",
      "authors": [
        "G Degottex",
        "J Kane",
        "T Drugman",
        "T Raitio",
        "S Scherer"
      ],
      "year": "2014",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)"
    },
    {
      "citation_id": "67",
      "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Lang. Technol. (NAACL-HLT)"
    },
    {
      "citation_id": "68",
      "title": "A quantum probability driven framework for joint multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "Y Liu",
        "Y Zhang",
        "D Song"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "69",
      "title": "Learning multitask commonness and uniqueness for multimodal sarcasm detection and sentiment analysis in conversation",
      "authors": [
        "Y Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Artif. Intell"
    },
    {
      "citation_id": "70",
      "title": "Improving sarcasm detection from speech and text through attention-based fusion exploiting the interplay of emotions and sentiments",
      "authors": [
        "X Gao",
        "S Nayak",
        "M Coler"
      ],
      "year": "2024",
      "venue": "Proc. Meet. Acoust",
      "doi": "10.1121/2.0001918"
    },
    {
      "citation_id": "71",
      "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis"
      ],
      "year": "2020",
      "venue": "Proc. 58th Annu. Meet. Assoc. Comput. Linguistics (ACL)"
    },
    {
      "citation_id": "72",
      "title": "Your tone speaks louder than your face! modality order infused multi-modal sarcasm detection",
      "authors": [
        "M Tomar",
        "A Tiwari",
        "T Saha",
        "S Saha"
      ],
      "year": "2023",
      "venue": "Proc. 31st ACM Int. Conf. Multimedia (MM)"
    },
    {
      "citation_id": "73",
      "title": "Quantum fuzzy neural network for multimodal sentiment and sarcasm detection",
      "authors": [
        "P Tiwari",
        "L Zhang",
        "Z Qu",
        "G Muhammad"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "74",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)"
    },
    {
      "citation_id": "75",
      "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. 36th Int. Conf. Mach. Learn. (ICML)"
    },
    {
      "citation_id": "76",
      "title": "Joint face detection and alignment using multi-task cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "77",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. 13th IEEE Int. Conf. Autom. Face Gesture Recognit. (FG)"
    },
    {
      "citation_id": "78",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proc. IEEE Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "79",
      "title": "Multimodal markers of irony and sarcasm",
      "authors": [
        "S Attardo",
        "J Eisterhold",
        "J Hay",
        "I Poggi"
      ],
      "year": "2003",
      "venue": "Humor -Int. J. Humor Res"
    },
    {
      "citation_id": "80",
      "title": "Verbal irony use in face-to-face and computer-mediated conversations",
      "authors": [
        "J Hancock"
      ],
      "year": "2004",
      "venue": "J. Lang. Soc. Psychol"
    },
    {
      "citation_id": "81",
      "title": "Effects of emotional intelligence on the impression of irony created by the mismatch between verbal and nonverbal cues",
      "authors": [
        "B Kreifelts",
        "S Nizielski",
        "A Schütz",
        "D Wildgruber"
      ],
      "year": "2016",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0163211"
    },
    {
      "citation_id": "82",
      "title": "Deep multimodal data fusion",
      "authors": [
        "F Zhao",
        "C Zhang",
        "B Geng"
      ],
      "year": "2024",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "83",
      "title": "Statistical Methods for Meta-Analysis",
      "authors": [
        "L Hedges",
        "I Olkin"
      ],
      "year": "1985",
      "venue": "Statistical Methods for Meta-Analysis"
    },
    {
      "citation_id": "84",
      "title": "Quantifying heterogeneity in a meta-analysis",
      "authors": [
        "J Higgins",
        "S Thompson"
      ],
      "year": "2002",
      "venue": "Stat. Med"
    },
    {
      "citation_id": "85",
      "title": "Meta-analysis in clinical trials",
      "authors": [
        "R Dersimonian",
        "N Laird"
      ],
      "year": "1986",
      "venue": "Control. Clin. Trials"
    },
    {
      "citation_id": "86",
      "title": "The cost 2102 italian audio and video emotional database",
      "authors": [
        "A Esposito",
        "M Riviello",
        "G Maio"
      ],
      "year": "2009",
      "venue": "Front"
    },
    {
      "citation_id": "87",
      "title": "Sarcasm, pretense, and the semantics/pragmatics distinction",
      "authors": [
        "E Camp"
      ],
      "year": "2012",
      "venue": "Noûs"
    },
    {
      "citation_id": "88",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE J. Sel. Top. Signal Process"
    },
    {
      "citation_id": "89",
      "title": "SpeechLM: Enhanced speech pre-training with unpaired textual data",
      "authors": [
        "Z Zhang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "90",
      "title": "Adapting WavLM for speech emotion recognition",
      "authors": [
        "D Diatlova",
        "A Udalov",
        "V Shutov",
        "E Spirin"
      ],
      "year": "2024",
      "venue": "Proc. Odyssey 2024: The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "91",
      "title": "GPT-4 technical report",
      "authors": [
        "J Achiam"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report",
      "doi": "10.48550/arXiv.2303.08774",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "92",
      "title": "LLaMA: Open and efficient foundation language models",
      "authors": [
        "H Touvron"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and efficient foundation language models",
      "doi": "10.48550/arXiv.2302.13971",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "93",
      "title": "SarcasmBench: Towards evaluating large language models on sarcasm understanding",
      "authors": [
        "Y Zhang",
        "C Zou",
        "Z Lian",
        "P Tiwari",
        "J Qin"
      ],
      "year": "2024",
      "venue": "SarcasmBench: Towards evaluating large language models on sarcasm understanding",
      "doi": "10.48550/arXiv.2408.11319",
      "arxiv": "arXiv:2408.11319"
    },
    {
      "citation_id": "94",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision",
      "doi": "10.48550/arXiv.2103.00020",
      "arxiv": "arXiv:2103.00020"
    },
    {
      "citation_id": "95",
      "title": "Paraverbal expression of verbal irony: Vocal cues matter and facial cues even more",
      "authors": [
        "M Aguert"
      ],
      "year": "2022",
      "venue": "J. Nonverbal Behav"
    },
    {
      "citation_id": "96",
      "title": "Tracking nonliteral language processing using audiovisual scenarios",
      "authors": [
        "K Rothermich"
      ],
      "year": "2021",
      "venue": "Can. J. Exp. Psychol"
    },
    {
      "citation_id": "97",
      "title": "The role of auditory and visual cues in the interpretation of mandarin ironic speech",
      "authors": [
        "S Li",
        "A Chen",
        "Y Chen",
        "P Tang"
      ],
      "year": "2022",
      "venue": "J. Pragmat"
    },
    {
      "citation_id": "98",
      "title": "Modality matters: Testing bilingual irony comprehension in the textual, auditory, and audio-visual modality",
      "authors": [
        "K Bromberek-Dyzman",
        "K Jankowiak",
        "P Chełminiak"
      ],
      "year": "2021",
      "venue": "J. Pragmat"
    },
    {
      "citation_id": "99",
      "title": "Context, facial expression and prosody in irony processing",
      "authors": [
        "G Deliens",
        "K Antoniou",
        "E Clin",
        "E Ostashchenko",
        "M Kissine"
      ],
      "year": "2018",
      "venue": "J. Mem. Lang"
    },
    {
      "citation_id": "100",
      "title": "The sound of sarcasm",
      "authors": [
        "H Cheang",
        "M Pell"
      ],
      "year": "2008",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "101",
      "title": "From 'blame by praise' to 'praise by blame': Analysis of vocal patterns in ironic communication",
      "authors": [
        "L Anolli",
        "R Ciceri",
        "M Infantino"
      ],
      "year": "2002",
      "venue": "Int. J. Psychol"
    },
    {
      "citation_id": "102",
      "title": "Categorical and schematic organization in memory",
      "authors": [
        "J Mandler"
      ],
      "year": "1979",
      "venue": "Categorical and schematic organization in memory"
    },
    {
      "citation_id": "103",
      "title": "How Korean EFL learners understand sarcasm in L2 English",
      "authors": [
        "J Kim"
      ],
      "year": "2014",
      "venue": "J. Pragmat"
    },
    {
      "citation_id": "104",
      "title": "Selfadaptive representation learning model for multi-modal sentiment and sarcasm joint analysis",
      "authors": [
        "Y Zhang",
        "Y Yu",
        "M Wang",
        "M Huang",
        "M Hossain"
      ],
      "year": "2024",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl"
    },
    {
      "citation_id": "105",
      "title": "Modified convolutional neural network with multiple features for multimodal sarcasm detection",
      "authors": [
        "R Krishnamaneni",
        "M Kurni",
        "S Sen",
        "A Murthy"
      ],
      "year": "2024",
      "venue": "Proc. 2nd Int. Conf. Recent Adv"
    },
    {
      "citation_id": "106",
      "title": "The geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "F Eyben"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "107",
      "title": "Understanding non-verbal irony markers: Machine learning insights versus human judgment",
      "authors": [
        "M Spitale",
        "F Catania",
        "F Panzeri"
      ],
      "year": "2024",
      "venue": "Proc. 26th ACM Int. Conf. Multimodal Interact. (ICMI)"
    },
    {
      "citation_id": "108",
      "title": "SWITCHBOARD: Telephone speech corpus for research and development",
      "authors": [
        "J Godfrey",
        "E Holliman",
        "J Mcdaniel"
      ],
      "year": "1992",
      "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP)"
    },
    {
      "citation_id": "109",
      "title": "The Fisher corpus: A resource for the next generations of speech-to-text",
      "authors": [
        "C Cieri",
        "D Miller",
        "K Walker"
      ],
      "year": "2004",
      "venue": "Proc. 4th Int. Conf. Lang. Resour. Eval. (LREC)"
    }
  ]
}