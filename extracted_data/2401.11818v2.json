{
  "paper_id": "2401.11818v2",
  "title": "Mind: Improving Multimodal Sentiment Analysis Via Multimodal Information Disentanglement",
  "published": "2024-01-22T10:26:52Z",
  "authors": [
    "Weichen Dai",
    "Xingyu Li",
    "Zeyu Wang",
    "Pengbo Hu",
    "Ji Qi",
    "Jianlin Peng",
    "Yi Zhou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Learning effective joint representations has been a central task in multi-modal sentiment analysis. Previous works addressing this task focus on exploring sophisticated fusion techniques to enhance performance. However, the inherent heterogeneity of distinct modalities remains a core problem that brings challenges in fusing and coordinating the multi-modal signals at both the representational level and the informational level, impeding the full exploitation of multi-modal information. To address this problem, we propose the Multi-modal Information Disentanglement (MInD) method, which decomposes the multi-modal inputs into modality-invariant and modality-specific components through a shared encoder and multiple private encoders. Furthermore, by explicitly training generated noise in an adversarial manner, MInD is able to isolate uninformativeness, thus improves the learned representations. Therefore, the proposed disentangled decomposition allows for a fusion process that is simpler than alternative methods and results in improved performance. Experimental evaluations conducted on representative benchmark datasets demonstrate MInD's effectiveness in both multimodal emotion recognition and multi-modal humor detection tasks. Code will be released upon acceptance of the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, there has been a growing interest in Multi-modal Sentiment Analysis (MSA). The comprehension of sentiment is often enhanced by cross-modal input, such as visual, audio, and textual information. Consequently, researchers have focused on developing effective joint representations that integrate all relevant information from the collected data  [1] , while most of the models rely on designing sophisticated fusion techniques  [2]  for the exploration of the intra-modal and inter-modal dynamics. Although multi-modal learning has been theoretically shown to outperform uni-modal learning  [3] , in practice, the modality gap resulting from the inherent heterogeneity of distinct modalities hampers the full exploitation of the inter-modal information for effective multi-modal representations. This phenomenon persists across a broad range of multi-modal models, covering texts, natural images, videos, medical images, and amino-acid sequences  [4] . Therefore, prior approaches that address the representations of each modality through a comprehensive learning framework may lead to insufficiently refined and potentially redundant multi-modal representations.\n\nRecent studies have initiated an exploration into the learning of distinct multi-modal representations. Pham et al.  [5]  translates a source modality to a target modality for joint representations using cyclic reconstruction. Mai et al.  [6]  also provides a adversarial encoder-decoder classifier framework to learn a modality-invariant embedding space through translating the distributions. But these methods do not explicitly learn the modality-specific representations which reveal the unique characteristic of emotions from different perspectives. By adopting the shared-private learning frameworks  [7] , Hazarika et al.  [8]  and Yang et al.  [9]  attempt to incorporate a diverse set of information by learning different factorized subspace for each modality in order to obtain better representations for fusion. However, their approaches either utilizes simple constraints that fail to guarantee a perfect factorization, or relies on a complex fusion module which may indicate that the extracted information may be unrefined. Moreover, they both neglect the control in the information flow, which could result in the loss of practical information. Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to deal with the insufficient exploitation of information from heterogeneous modalities. The main strategy is to decompose features of each modality with information optimization. Specifically, the first component is the modality-invariant component, which can effectively capture the underlying commonalities and explore the shared information across modalities. Secondly, we train the modality-specific component to capture the distinctive information and characteristic features. Furthermore, as unknown noise of each modality may be categorized as complementary components, we explicitly train the generated noise in an adversarial manner to enhance the refinement of the learned information and mitigate the impact of uninformativeness on the quality of the representations. The combination of the modality-invariant components and the modality-specific components thus enables a more straightforward fusion process compared to alternative methods, resulting in enhanced performance.\n\nThe contributions of this paper can be summarized as:\n\n• We propose MInD, a disentanglement-based multi-modal sentiment analysis method driven by information optimization. MInD overcomes the challenge caused by modality heterogeneity via learning modality-invariant and modality-specific representations, thus aiding the subsequent fusion for prediction tasks.\n\n• We explicitly train the generated noise in a novel way to improve the quality of learned representations.\n\nTo the best of our knowledge, we are the first work to model uninformativeness for a better shared-private disentanglement in MSA.\n\n• MInD outperforms previous state-of-the-art methods on several standard multi-modal benchmarks only with a simple fusion strategy, which demonstrates the power of MInD in capturing diverse facets of multi-modal information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Sentiment Analysis",
      "text": "Learning effective joint representations is a critical challenge in MSA. Many previous works have contributed to sophisticated fusion techniques. Zadeh et al.  [2]  proposed tensor-based fusion network which applies outer product to model the uni-modal, bimodal and tri-modal interactions. Mai et al.  [6]  introduced graph fusion network which regards each interaction as a vertex and the corresponding similarities as weights of edges. Besides, the attention mechanisms  [10]  are widely used to identify important information  [11, 12, 13] . For instance, Tsai et al.  [14]  developed a novel transformer architecture that effectively integrates unaligned data from different modalities by directional pairwise cross-modal attention. Shenoy et al.  [11]  assigned weights to the importance differences between multiple modalities through the importance attention network. Delbrouck et al.  [15]  utilized a Transformer-based joint-encoding (TBJE) model, incorporating modular co-attention and a glimpse layer to effectively encode and analyze emotions and sentiments from one or more modalities. However, as multi-modal inputs have various characteristics and information properties, this inherent heterogeneity of different modalities complicates the analysis of data, thus leading to a significant challenge on the mining and integration of information and the learning of multi-modal joint embedding.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Disentanglement Learning",
      "text": "Disentanglement learning  [7, 16]  is designed to unravel complex data structures, isolating key components to extract desirable information for more insightful and efficient data processing. Therefore, this approach plays a pivotal role in aligning semantically related concepts across different modalities and effectively alleviates the problems caused by the modality gap. Furthermore, disentanglement learning significantly contributes to multi-modal fusion by offering a more structured and explicit representation  [8] . Such clarity and organization in the data representation are instrumental in enhancing the efficacy and precision of multi-modal integration processes. For this reason, following Salzmann et al.  [17] , many works have extended the shared-private learning strategies in various scenarios for excellent results, including retrieval  [18] , user representation in social network  [19] , and emotion recognition  [9] , etc. In comparison, to the best of our knowledge, we provide the first attempt that explicitly train the generated noise in addition to the modality-invariant and modality-specific components for a better disentangled decomposition in MSA.  Meanwhile, by passing Gaussian noise G m to the private encoders, we align the uninformativeness within the feature subspace. This process is guided by mutual information maximization, the consistency loss and difference loss. After that, we proposed the vanilla reconstruction module, the cyclic reconstruction module and the noise prediction module to further improve the representations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Overview",
      "text": "The overall framework of MInD is shown in Fig.  1 . We introduce our method within the context of a task scenario that incorporates three distinct modalities, namely visual, audio, and text. Each individual data point consists of three sequences of low-level features originating from the visual, acoustic, and textual modalities. We denote them as\n\nrespectively, where L (•) is the sequence length and d (•) is the embedding dimension.\n\nIn response to the challenges posed by modality heterogeneity, we aim to identify an approach that effectively mitigates the distributional discrepancy and enhances the information extraction, ensuring a comprehensive analysis of multi-modal inputs. To this end, we decompose the inputs into two parts: the modality-invariant components and the modality-specific components. The uninformativeness is modeled starting from generated Gaussian noise which is sent to the corresponding private encoder for feature subspace alignment, see details and explanation in the following subsections. These representations are then facilitated through the implementation of information constraints, consistency constraints, and difference constraints. The integration of these constraints contributes to better utilization of information embedded within the high-level feature, enabling efficient exploration of both cross-modal commonality and distinctive features. After that, we evaluate the completeness of decomposed information through a vanilla reconstruction module. Moreover, we employ a cyclic reconstruction module to further reduce information redundancy, and a noise prediction module to minimize the task-related information within the trained noise. The modality-invariant components and the modality-specific components are finally fused for prediction tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Extraction",
      "text": "Here we employ transformer-based models  [10]  to extract high-level semantic features from individual modalities. Specifically, we use the Bert  [20]  model for text modality, and we employ a standard transformer model for the remaining two modalities,\n\nThe refined features of each modality are in a fixed dimension as Z m ∈ R d k .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality-Invariant And -Specific Components.",
      "text": "While temporal model-based feature extractors effectively capture the long-range contextual dependencies presented in multi-modal sequences, they fail to effectively handle feature redundancy due to the divergence of different modalities  [21] . Furthermore, the efficacy of the divide-and-conquer processing pattern is affected by the inherent heterogeneity among different modalities.\n\nInspired by these observations, we employ the shared and private encoders to learn the modality-invariant components and the modality-specific components, which are designed to capture commonality and specificity of individual modalities, respectively. We denote the shared encoder as E S (•; θ S ), and the private encoders as E Pm (•; θ Pm ), where m ∈ {V, A, T }. Then the representations are formulated as below:\n\nwith S m , P m ∈ R d k . The shared encoder E S (•; θ S ) shares the parameters θ S across all modalities, while the private encoders E Pm (•; θ Pm ) assign separate parameters θ Pm for each modality. Both the shared encoder and private encoders are implemented as simple linear network with the activation function of GeLU  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modeling Uninformativeness.",
      "text": "While the integration of modality-invariant and modality-specific components facilitates a comprehensive representation of multi-modal inputs in former works  [8, 9] , we argue that this kind of approaches has not yet reached its full potential. The main problem lies in the persistence of meaningless information within the desired representations, for example, modality-specific unknown noise may be categorized as complementary components. This may compromise information purity and limit the model's expressive capacity. It is infeasible to directly isolate the noise from the modality-specific components as there lacks ground-truth nor any reference signal. Instead, drawing inspiration from adversarial training, we adopted an indirect approach for such separation. Our novel idea is that the private encoders should distinguish the informative input signals from the uninformativeness, mapping them into distinct areas. To this end, we first generate Gaussian noise vectors which are subsequently aligned into feature subspace using the same private encoders, namely:\n\nwhere m ∈ {V, A, T }. We further require the outputs of a private encoder for the normal input Z m and the noise to be different through constraints, as described in next subsection. This strengthens the robustness of the learned representations and aids in the extraction of more refined, purer information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Representation Objectives",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Information Constraint.",
      "text": "A conventional approach to discover useful representations involves maximizing the mutual information (MI) between the input and output of models. However, MI is notoriously difficult to compute, especially in continuous and highdimensional contexts. Recent solution  [23]  leverage mutual information constraint that estimate and maximize the MI between input data and learned high-level representations simultaneously. Specifically, we employ the objective function based on the Jensen-Shannon divergence here, due to its proven stability and alignment with our primary aim of maximizing MI rather than obtaining an precise value. The estimator is shown below:\n\nwhere E θ (Z) is the encoder parameterized by θ, P(•) is the empirical probability distribution, sp(z ) = log(1 + e z ) is the softplus function, and T ω : X × Y → R is a discriminator function modeled by a neural network with parameters ω called the statistics network.\n\nSince the modality-invariant components are expected to capture cross-modal commonality, we calculate the MI between the outputs and the combination of inputs. We also maximize the MI between the noise outputs and the generated Gaussian noise, encouraging N m to remain as less informative as the noise inputs after alignment through private encoders. We denote the above procedure as following:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Consistency Constraint.",
      "text": "Inspired by  [24] , we introduce the Barlow Twins loss (BT loss) to be the consistency constraint. BT loss is originally designed for learning embedding which are invariant to distortions of the input sample, it forces two embedding vectors to be similar by making the cross-correlation matrix as close to the identity matrix as possible, which minimizes the redundancy between the components of these vectors. Concretely, each representation pair S A , S B is normalized to be mean-centered along the batch dimension as S A,nor , S B,nor , such that each unit has mean output 0 over the batch. The normalized matrices can be then utilized to depict the cross-correlation matrix:\n\nwhere b indexes batch samples and i, j index the vector dimension of the networks' outputs. The BT loss is expressed as:\n\nii\n\nIn the purpose of exploring shared information and commonality across modalities, we transfer the concept into our case by treating different modalities as different views. Following the observations in  [25] , we set λ BT to be the dimension of the embedding, and calculate the BT loss between the modality-invariant components of each modalities pair:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Difference Constraint.",
      "text": "Since both the modality-invariant components and the modality-specific components are learned from the same highlevel features Z m , it may result in the redundancy of information. Moreover, as explained in former subsections, private encoders should effectively differentiate between informative and uninformative inputs. For this sake, we employ the Hilbert-Schmidt Independence Criterion (HSIC)  [26]  to measure independence. Formally, the HSIC constraint between any two representations R 1 , R 2 is defined as:\n\nwhere K 1 and K 2 are the Gram matrices with k 1,ij = k 1 (r i 1 , r j 1 ) and k 2,ij = k 2 (r i 2 , r j 2 ). U = I -(1/n)ee T , where I is an identity matrix and e is an all-one column vector. In our setting, we use the inner product kernel function for K 1 and K 2 . To augment the distinction among individual components, the overall difference constraint is expressed as:\n\nwhere (R 1 , R 2 ) is the pair from (S m , P m ), (S m , N m ), (P m1 , P m2 ), and (P m , N m ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reconstruction Constraint.",
      "text": "We adopt a vanilla reconstruction constraint, which aims to help the combination of representations capture more comprehensive information of their respective modality. Note that we include N m during the reconstruction, as in our assumption that the modality-invariant components and the modality-specific components contain no meaningless information compared to the original signals. By employing a decoder function\n\nthe reconstruction constraint is then designed as the mean squared error between Z m and Ẑm :\n\nwhere ∥ • ∥ 2 2 is the squared L 2 -norm. We further minimize the MI between the informative and uninformative vectors through cyclic-reconstruction and gradient-reversal layers  [27] . Let F m be the concatenation of S m and P m . D C (•; θ Fm ), D C (•; θ Nm ) be the decoders for reconstruction from F m to N m and from N m to F m , respectively. The objective is then formulated as below:\n\nwhere GRL(•) is a gradient reversal layer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Prediction",
      "text": "Until now, the information disentanglement has been conducted in the unsupervised manner. We now complete our final objective function with the downstream task. The learned modality-invariant and modality-specific components are first fused by a simple linear layer with dimension reduction, and subsequently trained via shallow MLPs: G(•; θ G ) with several hidden layers and GeLU activation to get the prediction denoted as {ŷ i } or Ŷ :",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ŷ = G(H).",
      "text": "(16) Thanks to the explicit modeling of noise, the proposed disentangled decomposition allows for a fusion process that is simpler than alternative methods and results in improved performance. Specifically, to further reduce the task-related information inside the trained noise (thus the trained noise can be more meaningless in the sense of both informatics and task), we devise a noise-prediction loss with another shallow MLPs: G N (•; θ G N ), for the prediction {ŷ N,i } or ŶN from noise:\n\nThe final objective function is computed as:\n\nThe seven loss terms are necessary and serve for different purposes, yet the final loss is controlled by only four hyper-parameters. Here, α, β, γ, λ determine the contribution of each corresponding constraint to the overall loss. And L T ask is the prediction loss, where we employ the standard cross-entropy loss for the classification task, and the mean error loss for the regression task.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Datasets And Evaluation Criteria",
      "text": "In this paper, we choose three multi-modal dataset for evaluation, namely CMU-MOSI and CMU-MOSEI for emotion recognition, and UR-FUNNY for humor detection.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cmu-Mosi.",
      "text": "CMU-MOSI  [28]  is a widely-utilized dataset for MSA. The dataset is collected from 2199 opinion video clips from YouTube, which is splited to 1284 samples for training set, 229 samples for validation set, and 686 samples for testing set, with sentiment score ranges from -3 to 3 for each sample. Same as previous works, we adopt the 7-class accuracy (Acc-7), the binary accuracy (Acc-2), mean absolute error (MAE), the Pearson Correlation (Corr), and the F1 score for evaluation.   1 : Performance compared with the SOTA approaches in CMU-MOSI, CMU-MOSEI and UR-FUNNY, with MInD's results highlighted in bold. According to the comparison, while some baseline methods may stand out when evaluated by specific metric on one dataset, only MInD exhibits consistently competitive performance across all datasets under all metrics.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cmu-Mosei.",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cmu-Mosei",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Urfunny.",
      "text": "UR-FUNNY  [30]  dataset contains 16,514 samples of multi-modal punchlines labeled with a binary label for humor/nonhumor instance from TED talks, which is partitioned into 10,598 samples in the training set, 2,626 in the validation set, and 3,290 in the testing set. We report the binary accuracy (Acc-2) for this binary classification task.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Implementation Details",
      "text": "Following recent works, we utilize the pretrained BERT-base-uncased model to obtain a 768-dimension embedding for textual features. Specifically, since the original transcripts are not available for our considered UR-FUNNY version, we follow the same procedure as  [8]  to retrieve the raw texts from Glove  [31] . The acoustic features are extracted from COVAREP  [32] , where the dimensions are 74 for MOSI/MOSEI and 81 for UR-FUNNY. Moreover, we use Facet 2  to extract facial expression features for both MOSI and MOSEI, and OpenFace  [33]  for UR-FUNNY. The final visual feature dimensions are 47 for MOSI, 35 for MOSEI, and 75 for UR-FUNNY.\n\nOur model is built on the Pytorch 2.0.1 with one single Nvidia 3090 GPU. The number of transformer encoder layers for visual and audio are both 3. For the MOSI, MOSEI and UR-FUNNY benchmarks, the batch sizes and epochs are 32 and 100, respectively. We compare our model with many baselines, including pure learning based models such as TFN  [2] , LMF  [34] , MFM  [35] , and MulT  [14] . Besides, we also compare our model with feature space manipulation approaches like ICCN  [36] , MISA  [8] , Self-MM  [37] , HyCon  [38] , BBFN  [39] , FDMER  [9]  and CubeMLP  [40] . Moreover, the more recent and competitive methods, Liu et al.  [41] , AOBERT  [42] , SURGM  [43] , ConFEDE  [44] , AcFormer  [45] , TCHFN  [46]  and Self-HCL  [47]  are also taken into our consideration. Results are directly taken from their corresponding paper.",
      "page_start": 7,
      "page_end": 10
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multi-Modal Emotion Recognition.",
      "text": "As shown in Tab.1, MInD outperforms each baseline on most or even all evaluation metrics. While some baseline methods may stand out when evaluated by specific metric on one dataset, only MInD exhibits consistently competitive performance across all datasets under all metrics. Specifically, on the MOSI dataset, our approach shows the best results on Acc-7 and MAE, while on the MOSEI dataset, MInd surpasses all the SOTA Acc-2 and the F1 scores. Although on MOSEI, Acc-7 of our approach is relatively lower than SOTA, it could be attributed to the fact that MInd only adopts simple concatenation and shallow linear network for fusion and prediction, which limits fine-grained sentiment calculation on larger dataset. However, we still achieve overall satisfactory results without sophisticated fusion strategy, which reveals that our approach is able to capture sufficiently distinct information to form a comprehensive view of multi-modal inputs. Notably, MInD significantly improves the performance on both datasets compared to MISA  [8]  and FDMER  [9] , which are also disentanglement-based methods. This is attributed to our introduction of trained noise that aids in the extraction of more refined, purer information through adversarial learning.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multi-Modal Humor Detection.",
      "text": "Further experiments are conducted on the UR-FUNNY dataset to verify the applicability of MInD. Since humor detection is sensitive to heterogeneous representations of different modalities, the best result achieved by MInD demonstrate the efficacy of our proposed multi-modal framework in learning distinct representations and capturing reliable information. In Tab.2, we remove each modality separately to explore the performance of the bi-modal MInD, which performs consistently worse compared to the tri-modal MInD, suggesting that distinct modalities provides indispensable information. Specifically, we observe a significant drop in performance when we remove the text modality, yet similar drops are not observed in the other two cases. This shows the dominance of the text modality over the visual and audio modalities, probably due to the reason that the text modality contains manual transcriptions which could be inherently better, while on the contrary, the visual and audio modalities contain unfiltered raw signals with more noisy and redundant information.   To empirically validate the effectiveness of the proposed disentanglement scheme, we carry out ablation studies on the modality-invariant components and the modality-specific components. As shown in Tab.2, muting any one of the components leads to a degraded performance, indicating that each set of components capture different aspects of the information and is hence essential and meaningful. In addition, we provide a non-disentangled version where the backbone features are directly utilized for fusion and prediction. This situation shows even worse results on MOSI and UR-FUNNY, which further demonstrates the effectiveness of our approach.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Role Of Constraint.",
      "text": "As shown in Tab.2, all the constraints show non-trivial contribution to the performance of MInD. When there is no L Inf o , information extracted from the high-level features may be insufficient due to the adoption of simple shared and private encoders. This in turn demonstrates that with the help of well designed constraints, neural network models can be simple yet effective. When we remove L Cons or L Dif f , model fails to capture the shared information or specific information of distinct modalities. In our model, L Recon and L CyR ensure the completeness and refinement of learned information, respectively. Removing them also brings worse performance. The removal of L N P leads to a slight degradation of performance on MOSEI and UR-FUNNY, and it is worth noting that the result on UR-FUNNY in this case still surpasses the baselines in Tab.2. Finally, we present the results trained only with L T ask . The largest performance drop on most of the metrics demonstrates the necessity of all the constraints in our model.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization",
      "text": "We visualize the noise vectors N m , which are generated from Gaussian noise and subsequently aligned within the feature subspace by private encoders, along with the feature distributions of modality-invariant components S m and modality-specific components P m from individual modalities before and after training through T-SNE  [48] , using data from the testing set of UR-FUNNY.\n\nAs shown below, before training, there is no clear boundary between the modality-specific components and the synthetic noise. After training, the representations of the modality-specific components and the synthetic noise become more separated in the feature subspace, which helps a better exploitation of useful information. This illustrates the potential of MInD in isolating the meaningless information, thus facilitating better representations.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiment On Multi-Modal Intent Recognition",
      "text": "In order to investigate the generality of our proposed framework on other affective computing tasks, we carry out further experiment on MIntRec  [49]  dataset for multi-modal intent recognition. MIntRec includes 2,224 high-quality samples, divided into 1,334 for training, 445 for validation, and 445 for testing. This dataset is designed to categorize intents into two levels: coarse-grained and fine-grained. The coarse-grained level features binary intent labels, differentiating between expressing emotions or attitudes and pursuing goals. The fine-grained level provides a more detailed classification, with 20 intent labels: 11 related to expressing emotions or attitudes and 9 focused on goal achievement.\n\nWe compare MInD's performance on the more difficult fine-grained level with the state-of-the-art multi-modal intent recognition methods, including MulT  [14] , MISA  [8] , MAG-BERT  [50] , SPECTRA  [51]  and CAGC  [52] . The results are directly taken from their corresponding paper. As shown below, our model has achieved SOTA performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the Multi-modal Information Disentanglement (MInD) method to overcome the challenges caused by the inherent heterogeneity of distinct modalities through the decomposition of multi-modal inputs into modality-invariant and modality-specific components. We obtain the refined representations via the well-designed constraints and improve the quality of disentanglement with the help of explicitly training the generated noise in an adversarial manner, which provides a new insight to pay attention to the meaningless information during the learning of different representation subspace. Experimental results demonstrate the superiority of our method. In the future, we plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed model. Each embedding Zm from the backbones is fed into a shared encoder and multiple",
      "page": 3
    },
    {
      "caption": "Figure 2: We present the visualization results of noise vectors, as well as the modality-invariant and modality-specific",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "impeding the full exploitation of multi-modal"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "acceptance of the paper.": "1\nINTRODUCTION"
        },
        {
          "acceptance of the paper.": "Recently, there has been a growing interest in Multi-modal Sentiment Analysis (MSA). The comprehension of sentiment"
        },
        {
          "acceptance of the paper.": "is often enhanced by cross-modal input, such as visual, audio, and textual information. Consequently, researchers have"
        },
        {
          "acceptance of the paper.": "focused on developing effective joint representations that integrate all relevant information from the collected data [1],"
        },
        {
          "acceptance of the paper.": "while most of the models rely on designing sophisticated fusion techniques [2] for the exploration of the intra-modal"
        },
        {
          "acceptance of the paper.": "and inter-modal dynamics. Although multi-modal\nlearning has been theoretically shown to outperform uni-modal"
        },
        {
          "acceptance of the paper.": "learning [3], in practice, the modality gap resulting from the inherent heterogeneity of distinct modalities hampers the"
        },
        {
          "acceptance of the paper.": "full exploitation of the inter-modal information for effective multi-modal representations. This phenomenon persists"
        },
        {
          "acceptance of the paper.": "across a broad range of multi-modal models, covering texts, natural images, videos, medical images, and amino-acid"
        },
        {
          "acceptance of the paper.": "sequences [4]. Therefore, prior approaches that address the representations of each modality through a comprehensive"
        },
        {
          "acceptance of the paper.": "learning framework may lead to insufficiently refined and potentially redundant multi-modal representations."
        },
        {
          "acceptance of the paper.": "Recent studies have initiated an exploration into the learning of distinct multi-modal representations. Pham et al. [5]"
        },
        {
          "acceptance of the paper.": "translates a source modality to a target modality for joint representations using cyclic reconstruction. Mai et al. [6] also"
        },
        {
          "acceptance of the paper.": "provides a adversarial encoder-decoder classifier framework to learn a modality-invariant embedding space through"
        },
        {
          "acceptance of the paper.": "translating the distributions. But\nthese methods do not explicitly learn the modality-specific representations which"
        },
        {
          "acceptance of the paper.": "reveal\nthe unique characteristic of emotions from different perspectives. By adopting the shared-private learning"
        },
        {
          "acceptance of the paper.": "frameworks [7], Hazarika et al. [8] and Yang et al. [9] attempt to incorporate a diverse set of information by learning"
        },
        {
          "acceptance of the paper.": "different factorized subspace for each modality in order to obtain better representations for fusion. However,\ntheir"
        },
        {
          "acceptance of the paper.": "approaches either utilizes simple constraints that fail to guarantee a perfect factorization, or relies on a complex fusion"
        },
        {
          "acceptance of the paper.": "module which may indicate that the extracted information may be unrefined. Moreover, they both neglect the control in"
        },
        {
          "acceptance of the paper.": "the information flow, which could result in the loss of practical information."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "deal with the insufficient exploitation of information from heterogeneous modalities. The main strategy is to decompose"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "features of each modality with information optimization. Specifically, the first component is the modality-invariant"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "component, which can effectively capture the underlying commonalities and explore the shared information across"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "modalities. Secondly, we train the modality-specific component to capture the distinctive information and characteristic"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "features. Furthermore, as unknown noise of each modality may be categorized as complementary components, we"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "explicitly train the generated noise in an adversarial manner to enhance the refinement of the learned information and"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "mitigate the impact of uninformativeness on the quality of the representations. The combination of the modality-invariant"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "components and the modality-specific components thus enables a more straightforward fusion process compared to"
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "alternative methods, resulting in enhanced performance."
        },
        {
          "Motivated by the above observations, we propose the Multi-modal Information Disentanglement (MInD) approach to": "The contributions of this paper can be summarized as:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "alternative methods, resulting in enhanced performance.": "The contributions of this paper can be summarized as:"
        },
        {
          "alternative methods, resulting in enhanced performance.": ""
        },
        {
          "alternative methods, resulting in enhanced performance.": ""
        },
        {
          "alternative methods, resulting in enhanced performance.": "and modality-specific representations, thus aiding the subsequent fusion for prediction tasks."
        },
        {
          "alternative methods, resulting in enhanced performance.": "• We explicitly train the generated noise in a novel way to improve the quality of"
        },
        {
          "alternative methods, resulting in enhanced performance.": ""
        },
        {
          "alternative methods, resulting in enhanced performance.": "disentanglement in MSA."
        },
        {
          "alternative methods, resulting in enhanced performance.": ""
        },
        {
          "alternative methods, resulting in enhanced performance.": ""
        },
        {
          "alternative methods, resulting in enhanced performance.": "information."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Text": "Hi there, today I have a very"
        },
        {
          "Text": "special announcement"
        },
        {
          "Text": "namely I will be attending"
        },
        {
          "Text": "the … convention this year …"
        },
        {
          "Text": "Figure 1: Our proposed model. Each embedding Zm from the backbones is fed into a shared encoder and multiple"
        },
        {
          "Text": "private encoders to generate modality-invariant components Sm and modality-specific components Pm, respectively."
        },
        {
          "Text": "Meanwhile, by passing Gaussian noise Gm to the private encoders, we align the uninformativeness within the feature"
        },
        {
          "Text": "subspace. This process is guided by mutual information maximization, the consistency loss and difference loss. After"
        },
        {
          "Text": "that, we proposed the vanilla reconstruction module, the cyclic reconstruction module and the noise prediction module"
        },
        {
          "Text": "to further improve the representations."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "heterogeneity among different modalities."
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "Inspired by these observations, we employ the shared and private encoders to learn the modality-invariant components"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "and the modality-specific components, which are designed to capture commonality and specificity of\nindividual"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "modalities, respectively. We denote the shared encoder as ES(·; θS), and the private encoders as EPm(·; θPm), where"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "m ∈ {V, A, T }. Then the representations are formulated as below:"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "(2)\nSm = ES(Zm; θS),"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "(3)\nPm = EPm(Zm; θPm ),"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "with Sm, Pm ∈ Rdk . The shared encoder ES(·; θS) shares the parameters θS across all modalities, while the private"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "encoders EPm (·; θPm ) assign separate parameters θPm for each modality. Both the shared encoder and private encoders"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "are implemented as simple linear network with the activation function of GeLU [22]."
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "3.2.2\nModeling Uninformativeness."
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "While the integration of modality-invariant and modality-specific components facilitates a comprehensive representation"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "of multi-modal inputs in former works [8, 9], we argue that this kind of approaches has not yet reached its full potential."
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "The main problem lies in the persistence of meaningless information within the desired representations, for example,"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "modality-specific unknown noise may be categorized as complementary components. This may compromise information"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "purity and limit the model’s expressive capacity. It is infeasible to directly isolate the noise from the modality-specific"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "components as there lacks ground-truth nor any reference signal. Instead, drawing inspiration from adversarial training,"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "we adopted an indirect approach for such separation. Our novel idea is that the private encoders should distinguish the"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "informative input signals from the uninformativeness, mapping them into distinct areas. To this end, we first generate"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "Gaussian noise vectors which are subsequently aligned into feature subspace using the same private encoders, namely:"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "(4)\nGm ∼ N (0, 1),\nNm = EPm(Gm; θPm),"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "where m ∈ {V, A, T }. We further require the outputs of a private encoder for the normal\ninput Zm and the noise"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "to be different\nthrough constraints, as described in next subsection. This strengthens the robustness of the learned"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "representations and aids in the extraction of more refined, purer information."
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "3.3\nRepresentation Objectives"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "3.3.1\nInformation Constraint."
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "A conventional approach to discover useful representations involves maximizing the mutual information (MI) between"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "the input and output of models. However, MI is notoriously difficult to compute, especially in continuous and high-"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "dimensional contexts. Recent solution [23] leverage mutual information constraint that estimate and maximize the"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "MI between input data and learned high-level representations simultaneously. Specifically, we employ the objective"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "function based on the Jensen-Shannon divergence here, due to its proven stability and alignment with our primary aim"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "of maximizing MI rather than obtaining an precise value. The estimator is shown below:"
        },
        {
          "modalities [21]. Furthermore,\nthe efficacy of the divide-and-conquer processing pattern is affected by the inherent": "I (JSD)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3.2": "Inspired by [24], we introduce the Barlow Twins loss (BT loss) to be the consistency constraint. BT loss is originally",
          "Consistency Constraint.": ""
        },
        {
          "3.3.2": "designed for learning embedding which are invariant to distortions of the input sample, it forces two embedding vectors",
          "Consistency Constraint.": ""
        },
        {
          "3.3.2": "to be similar by making the cross-correlation matrix as close to the identity matrix as possible, which minimizes the",
          "Consistency Constraint.": ""
        },
        {
          "3.3.2": "redundancy between the components of these vectors. Concretely, each representation pair SA, SB is normalized to be",
          "Consistency Constraint.": ""
        },
        {
          "3.3.2": "mean-centered along the batch dimension as SA,nor, SB,nor, such that each unit has mean output 0 over the batch. The",
          "Consistency Constraint.": ""
        },
        {
          "3.3.2": "normalized matrices can be then utilized to depict the cross-correlation matrix:",
          "Consistency Constraint.": ""
        },
        {
          "3.3.2": "",
          "Consistency Constraint.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where GRL(·) is a gradient reversal layer.": "3.4"
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        },
        {
          "where GRL(·) is a gradient reversal layer.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMU-MOSI": "",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        },
        {
          "CMU-MOSI": "F1↑",
          "CMU-MOSEI": "F1↑",
          "UR-FUNNY": "Acc2↑"
        },
        {
          "CMU-MOSI": "80.7",
          "CMU-MOSEI": "82.1",
          "UR-FUNNY": "68.57"
        },
        {
          "CMU-MOSI": "82.4",
          "CMU-MOSEI": "82.1",
          "UR-FUNNY": "67.53"
        },
        {
          "CMU-MOSI": "81.6",
          "CMU-MOSEI": "84.3",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "83.0",
          "CMU-MOSEI": "84.2",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "82.8",
          "CMU-MOSEI": "82.3",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "85.9",
          "CMU-MOSEI": "85.3",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "85.1",
          "CMU-MOSEI": "85.6",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "84.3",
          "CMU-MOSEI": "86.1",
          "UR-FUNNY": "71.68"
        },
        {
          "CMU-MOSI": "85.5",
          "CMU-MOSEI": "84.5",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "84.2",
          "CMU-MOSEI": "85.0",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "86.4",
          "CMU-MOSEI": "85.9",
          "UR-FUNNY": "70.82"
        },
        {
          "CMU-MOSI": "84.5",
          "CMU-MOSEI": "85.1",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "85.5",
          "CMU-MOSEI": "85.8",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "85.2",
          "CMU-MOSEI": "85.8",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "86.3",
          "CMU-MOSEI": "86.5",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "85.0",
          "CMU-MOSEI": "85.9",
          "UR-FUNNY": "-"
        },
        {
          "CMU-MOSI": "83.6",
          "CMU-MOSEI": "85.3",
          "UR-FUNNY": "70.61"
        },
        {
          "CMU-MOSI": "84.7",
          "CMU-MOSEI": "85.8",
          "UR-FUNNY": "71.87"
        },
        {
          "CMU-MOSI": "86.0",
          "CMU-MOSEI": "86.7",
          "UR-FUNNY": "72.55"
        },
        {
          "CMU-MOSI": "",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "MISA\n42.3\n83.4\n83.6\n0.783\n0.761\n52.2\n85.5\n85.3\n0.555\n0.756\n70.61"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "FDMER\n44.1\n84.6\n84.7\n0.724\n0.788\n54.1\n86.1\n85.8\n0.536\n0.773\n71.87"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "MInD(ours)\n46.6\n86.0\n86.0\n0.711\n0.791\n53.9\n86.6\n86.7\n0.529\n0.772\n72.55"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "Table 1: Performance compared with the SOTA approaches in CMU-MOSI, CMU-MOSEI and UR-FUNNY, with"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "MInD’s results highlighted in bold. According to the comparison, while some baseline methods may stand out when"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "evaluated by specific metric on one dataset, only MInD exhibits consistently competitive performance across all datasets"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "under all metrics."
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "4.1.3\nURFUNNY."
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "UR-FUNNY [30] dataset contains 16,514 samples of multi-modal punchlines labeled with a binary label for humor/non-"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "humor instance from TED talks, which is partitioned into 10,598 samples in the training set, 2,626 in the validation set,"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "and 3,290 in the testing set. We report the binary accuracy (Acc-2) for this binary classification task."
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "4.2\nImplementation Details"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "Following recent works, we utilize the pretrained BERT-base-uncased model to obtain a 768-dimension embedding for"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "textual features. Specifically, since the original transcripts are not available for our considered UR-FUNNY version, we"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "follow the same procedure as [8] to retrieve the raw texts from Glove [31]. The acoustic features are extracted from"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "COVAREP [32], where the dimensions are 74 for MOSI/MOSEI and 81 for UR-FUNNY. Moreover, we use Facet2 to"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "extract facial expression features for both MOSI and MOSEI, and OpenFace [33] for UR-FUNNY. The final visual"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "feature dimensions are 47 for MOSI, 35 for MOSEI, and 75 for UR-FUNNY."
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "Our model is built on the Pytorch 2.0.1 with one single Nvidia 3090 GPU. The number of transformer encoder layers"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "for visual and audio are both 3. For the MOSI, MOSEI and UR-FUNNY benchmarks, the batch sizes and epochs are 32"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "and 100, respectively."
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "4.3\nComparison With SOTA Models"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "4.3.1\nBaselines."
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "We compare our model with many baselines,\nincluding pure learning based models such as TFN [2], LMF [34],"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "MFM [35], and MulT [14]. Besides, we also compare our model with feature space manipulation approaches like"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "ICCN [36], MISA [8], Self-MM [37], HyCon [38], BBFN [39], FDMER [9] and CubeMLP [40]. Moreover,\nthe"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "more recent and competitive methods, Liu et al. [41], AOBERT [42], SURGM [43], ConFEDE [44], AcFormer [45],"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "TCHFN [46] and Self-HCL [47] are also taken into our consideration. Results are directly taken from their corresponding"
        },
        {
          "Self-HCL\n-\n84.9\n85.0\n0.711\n0.788\n-\n85.9\n85.9\n0.531\n0.775\n-": "paper."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CMU-MOSI": "",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        },
        {
          "CMU-MOSI": "MAE↓",
          "CMU-MOSEI": "MAE↓",
          "UR-FUNNY": "Acc2↑"
        },
        {
          "CMU-MOSI": "0.711",
          "CMU-MOSEI": "0.529",
          "UR-FUNNY": "72.55"
        },
        {
          "CMU-MOSI": "",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        },
        {
          "CMU-MOSI": "0.857",
          "CMU-MOSEI": "0.541",
          "UR-FUNNY": "71.12"
        },
        {
          "CMU-MOSI": "0.759",
          "CMU-MOSEI": "0.547",
          "UR-FUNNY": "70.79"
        },
        {
          "CMU-MOSI": "1.452",
          "CMU-MOSEI": "0.841",
          "UR-FUNNY": "49.67"
        },
        {
          "CMU-MOSI": "",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        },
        {
          "CMU-MOSI": "0.793",
          "CMU-MOSEI": "0.546",
          "UR-FUNNY": "70.30"
        },
        {
          "CMU-MOSI": "0.777",
          "CMU-MOSEI": "0.550",
          "UR-FUNNY": "70.91"
        },
        {
          "CMU-MOSI": "0.925",
          "CMU-MOSEI": "0.576",
          "UR-FUNNY": "68.97"
        },
        {
          "CMU-MOSI": "",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        },
        {
          "CMU-MOSI": "0.755",
          "CMU-MOSEI": "0.542",
          "UR-FUNNY": "71.22"
        },
        {
          "CMU-MOSI": "0.789",
          "CMU-MOSEI": "0.551",
          "UR-FUNNY": "72.28"
        },
        {
          "CMU-MOSI": "0.768",
          "CMU-MOSEI": "0.556",
          "UR-FUNNY": "71.70"
        },
        {
          "CMU-MOSI": "0.727",
          "CMU-MOSEI": "0.558",
          "UR-FUNNY": "72.01"
        },
        {
          "CMU-MOSI": "0.787",
          "CMU-MOSEI": "0.539",
          "UR-FUNNY": "72.28"
        },
        {
          "CMU-MOSI": "0.732",
          "CMU-MOSEI": "0.532",
          "UR-FUNNY": "72.46"
        },
        {
          "CMU-MOSI": "0.788",
          "CMU-MOSEI": "0.546",
          "UR-FUNNY": "71.64"
        },
        {
          "CMU-MOSI": "Table 2: Results of ablation studies.",
          "CMU-MOSEI": "",
          "UR-FUNNY": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Results of ablation studies.": "4.3.2\nMulti-modal Emotion Recognition."
        },
        {
          "Table 2: Results of ablation studies.": "As shown in Tab.1, MInD outperforms each baseline on most or even all evaluation metrics. While some baseline"
        },
        {
          "Table 2: Results of ablation studies.": "methods may stand out when evaluated by specific metric on one dataset, only MInD exhibits consistently competitive"
        },
        {
          "Table 2: Results of ablation studies.": "performance across all datasets under all metrics. Specifically, on the MOSI dataset, our approach shows the best results"
        },
        {
          "Table 2: Results of ablation studies.": "on Acc-7 and MAE, while on the MOSEI dataset, MInd surpasses all the SOTA Acc-2 and the F1 scores. Although"
        },
        {
          "Table 2: Results of ablation studies.": "on MOSEI, Acc-7 of our approach is relatively lower than SOTA,"
        },
        {
          "Table 2: Results of ablation studies.": "adopts simple concatenation and shallow linear network for fusion and prediction, which limits fine-grained sentiment"
        },
        {
          "Table 2: Results of ablation studies.": "calculation on larger dataset. However, we still achieve overall satisfactory results without sophisticated fusion strategy,"
        },
        {
          "Table 2: Results of ablation studies.": "which reveals that our approach is able to capture sufficiently distinct information to form a comprehensive view of"
        },
        {
          "Table 2: Results of ablation studies.": "multi-modal inputs. Notably, MInD significantly improves the performance on both datasets compared to MISA [8] and"
        },
        {
          "Table 2: Results of ablation studies.": "FDMER [9], which are also disentanglement-based methods. This is attributed to our introduction of trained noise that"
        },
        {
          "Table 2: Results of ablation studies.": "aids in the extraction of more refined, purer information through adversarial learning."
        },
        {
          "Table 2: Results of ablation studies.": "4.3.3\nMulti-modal Humor Detection."
        },
        {
          "Table 2: Results of ablation studies.": "Further experiments are conducted on the UR-FUNNY dataset"
        },
        {
          "Table 2: Results of ablation studies.": "detection is sensitive to heterogeneous representations of different modalities,"
        },
        {
          "Table 2: Results of ablation studies.": "demonstrate the efficacy of our proposed multi-modal framework in learning distinct representations and capturing"
        },
        {
          "Table 2: Results of ablation studies.": "reliable information."
        },
        {
          "Table 2: Results of ablation studies.": "4.4\nAblation Studies"
        },
        {
          "Table 2: Results of ablation studies.": "4.4.1\nRole of Modality."
        },
        {
          "Table 2: Results of ablation studies.": ""
        },
        {
          "Table 2: Results of ablation studies.": ""
        },
        {
          "Table 2: Results of ablation studies.": "Specifically, we observe a significant drop in performance when we remove the text modality, yet similar drops are not"
        },
        {
          "Table 2: Results of ablation studies.": "observed in the other two cases. This shows the dominance of the text modality over the visual and audio modalities,"
        },
        {
          "Table 2: Results of ablation studies.": "probably due to the reason that\nthe text modality contains manual"
        },
        {
          "Table 2: Results of ablation studies.": "while on the contrary, the visual and audio modalities contain unfiltered raw signals with more noisy and redundant"
        },
        {
          "Table 2: Results of ablation studies.": "information."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "Figure 2: We present the visualization results of noise vectors, as well as the modality-invariant and modality-specific"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "components from distinct modalities, taking the testing set of UR-FUNNY as example. Blue: Invariant; Red: Specific;"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "Green: Noise. MInD is able to depict different aspects of information."
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "4.4.2\nRole of Disentanglement."
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "To empirically validate the effectiveness of the proposed disentanglement scheme, we carry out ablation studies on"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "the modality-invariant components and the modality-specific components. As shown in Tab.2, muting any one of the"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "components leads to a degraded performance, indicating that each set of components capture different aspects of the"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "information and is hence essential and meaningful.\nIn addition, we provide a non-disentangled version where the"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "backbone features are directly utilized for fusion and prediction. This situation shows even worse results on MOSI and"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "UR-FUNNY, which further demonstrates the effectiveness of our approach."
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "4.4.3\nRole of Constraint."
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "As shown in Tab.2, all the constraints show non-trivial contribution to the performance of MInD. When there is no"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "LInf o, information extracted from the high-level features may be insufficient due to the adoption of simple shared and"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "private encoders. This in turn demonstrates that with the help of well designed constraints, neural network models can"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "be simple yet effective. When we remove LCons or LDif f , model fails to capture the shared information or specific"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "information of distinct modalities. In our model, LRecon and LCyR ensure the completeness and refinement of learned"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "information, respectively. Removing them also brings worse performance. The removal of LN P leads to a slight"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "degradation of performance on MOSEI and UR-FUNNY, and it\nis worth noting that\nthe result on UR-FUNNY in"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "this case still surpasses the baselines in Tab.2. Finally, we present\nthe results trained only with LT ask. The largest"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "performance drop on most of the metrics demonstrates the necessity of all the constraints in our model."
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "4.5\nVisualization"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "We visualize the noise vectors Nm, which are generated from Gaussian noise and subsequently aligned within the"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "feature subspace by private encoders, along with the feature distributions of modality-invariant components Sm and"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "modality-specific components Pm from individual modalities before and after training through T-SNE [48], using data"
        },
        {
          "(b) Visual, audio and text vectors after training, from left to right, respectively.": "from the testing set of UR-FUNNY."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "noise. After training, the representations of the modality-specific components and the synthetic noise become more"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "separated in the feature subspace, which helps a better exploitation of useful information. This illustrates the potential"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "of MInD in isolating the meaningless information, thus facilitating better representations."
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "4.6\nExperiment On Multi-modal Intent Recognition"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "In order to investigate the generality of our proposed framework on other affective computing tasks, we carry out further"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "experiment on MIntRec [49] dataset for multi-modal intent recognition."
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "MIntRec includes 2,224 high-quality samples, divided into 1,334 for training, 445 for validation, and 445 for testing."
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "This dataset\nis designed to categorize intents into two levels: coarse-grained and fine-grained. The coarse-grained"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "level features binary intent labels, differentiating between expressing emotions or attitudes and pursuing goals. The"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "fine-grained level provides a more detailed classification, with 20 intent labels: 11 related to expressing emotions or"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "attitudes and 9 focused on goal achievement."
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "We compare MInD’s performance on the more difficult fine-grained level with the state-of-the-art multi-modal intent"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results"
        },
        {
          "As shown below, before training, there is no clear boundary between the modality-specific components and the synthetic": "are directly taken from their corresponding paper. As shown below, our model has achieved SOTA performance."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "are directly taken from their corresponding paper. As shown below, our model has achieved SOTA performance."
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "Methods"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "MulT"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "MISA"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "MAG-BERT"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "SPECTRA"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "CAGC"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": "MInD(Ours)"
        },
        {
          "recognition methods, including MulT [14], MISA [8], MAG-BERT [50], SPECTRA [51] and CAGC [52]. The results": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "References"
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "Current challenges and new directions in sentiment analysis research."
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "2020."
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "for multimodal sentiment analysis. arXiv preprint arXiv:1707.07250, 2017."
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "2021."
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "Processing Systems, 35:17612–17625, 2022."
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": ""
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "Learning robust\njoint representations by cyclic translations between modalities."
        },
        {
          "plan to broaden the application spectrum of our method, deploying it across a diverse array of multi-modal scenarios.": "Conference on Artificial Intelligence, volume 33, pages 6892–6899, 2019."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "learning and graph fusion network for multimodal fusion.\nIn Proceedings of the AAAI Conference on Artificial"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Intelligence, volume 34, pages 164–172, 2020."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[7] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "separation networks. Advances in neural information processing systems, 29, 2016."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[8] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. Misa: Modality-invariant and-specific representa-"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "tions for multimodal sentiment analysis.\nIn Proceedings of the 28th ACM international conference on multimedia,"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "pages 1122–1131, 2020."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[9] Dingkang Yang, Shuai Huang, Haopeng Kuang, Yangtao Du, and Lihua Zhang. Disentangled representation"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "the 30th ACM International Conference on\nlearning for multimodal emotion recognition.\nIn Proceedings of"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Multimedia, pages 1642–1651, 2022."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[11] Aman Shenoy and Ashish Sardana. Multilogue-net: A context aware rnn for multi-modal emotion detection and"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "sentiment analysis in conversation. arXiv preprint arXiv:2002.08267, 2020."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[12] Md Shad Akhtar, Dushyant Singh Chauhan, Deepanway Ghosal, Soujanya Poria, Asif Ekbal, and Pushpak"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Bhattacharyya. Multi-task learning for multi-modal emotion recognition and sentiment analysis. arXiv preprint"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "arXiv:1905.05812, 2019."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[13]\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for visual"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "question answering. Advances in neural information processing systems, 29, 2016."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[14] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhut-"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "dinov. Multimodal transformer for unaligned multimodal language sequences.\nIn Proceedings of the conference."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Association for Computational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[15]\nJean-Benoit Delbrouck, Noé Tits, Mathilde Brousmiche, and Stéphane Dupont. A transformer-based joint-"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "encoding for emotion recognition and sentiment analysis. arXiv preprint arXiv:2006.15955, 2020."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[16] Hyunjik Kim and Andriy Mnih. Disentangling by factorising.\nIn International Conference on Machine Learning,"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "pages 2649–2658. PMLR, 2018."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[17] Mathieu Salzmann, Carl Henrik Ek, Raquel Urtasun, and Trevor Darrell. Factorized orthogonal latent spaces."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 701–708."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "JMLR Workshop and Conference Proceedings, 2010."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[18] Weikuo Guo, Huaibo Huang, Xiangwei Kong, and Ran He. Learning disentangled representation for cross-modal"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "retrieval with deep mutual information estimation.\nIn Proceedings of the 27th ACM International Conference on"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Multimedia, pages 1712–1720, 2019."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[19] Wenyi Tang, Bei Hui, Ling Tian, Guangchun Luo, Zaobo He, and Zhipeng Cai. Learning disentangled user"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "representation with multi-view information fusion on social networks.\nInformation Fusion, 74:77–86, 2021."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[20]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[21] Yi Zhang, Mingyuan Chen, Jundong Shen, and Chongjun Wang.\nTailor versatile multi-modal\nlearning for"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "multi-label emotion recognition.\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume 36,"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "pages 9100–9108, 2022."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[22] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[23] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "arXiv:1808.06670, 2018."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[24]\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "redundancy reduction.\nIn International Conference on Machine Learning, pages 12310–12320. PMLR, 2021."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[25] Yao-Hung Hubert Tsai, Shaojie Bai, Louis-Philippe Morency, and Ruslan Salakhutdinov. A note on connecting"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "barlow twins with negative-sample-free contrastive learning. arXiv preprint arXiv:2104.13712, 2021."
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "[26] Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. Supervised feature selection via"
        },
        {
          "[6] Sijie Mai, Haifeng Hu, and Songlong Xing. Modality to modality translation: An adversarial representation": "dependence estimation.\nIn Proceedings of the 24th international conference on Machine learning, pages 823–830,"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Journal of machine learning\nMarch, and Victor Lempitsky. Domain-adversarial\ntraining of neural networks."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "research, 17(59):1–35, 2016."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[28] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Multimodal sentiment intensity analysis in"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "videos: Facial gestures and verbal messages.\nIEEE Intelligent Systems, 31(6):82–88, 2016."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[29] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.\nIn Proceedings of the"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2236–2246,"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "2018."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[30] Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency,"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "et al. Ur-funny: A multimodal language dataset for understanding humor. arXiv preprint arXiv:1904.06618, 2019."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[31]\nJeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "1532–1543, 2014."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[32] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. Covarep—a collaborative"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "voice analysis repository for speech technologies.\nIn 2014 ieee international conference on acoustics, speech and"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "signal processing (icassp), pages 960–964. IEEE, 2014."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[33] Tadas Baltrušaitis, Peter Robinson, and Louis-Philippe Morency. Openface:\nan open source facial behavior"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "analysis toolkit.\nIn 2016 IEEE winter conference on applications of computer vision (WACV), pages 1–10. IEEE,"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "2016."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[34] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-Philippe"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Morency. Efficient low-rank multimodal fusion with modality-specific factors. arXiv preprint arXiv:1806.00064,"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "2018."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[35] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov. Learning"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "factorized multimodal representations. arXiv preprint arXiv:1806.06176, 2018."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[36] Zhongkai Sun, Prathusha Sarma, William Sethares, and Yingyu Liang. Learning relationships between text,"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "audio, and video via deep canonical correlation for multimodal language analysis.\nIn Proceedings of the AAAI"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Conference on Artificial Intelligence, volume 34, pages 8992–8999, 2020."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[37] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. Learning modality-specific representations with self-supervised"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "the AAAI conference on artificial\nmulti-task learning for multimodal sentiment analysis.\nIn Proceedings of"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "intelligence, volume 35, pages 10790–10797, 2021."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[38] Sijie Mai, Ying Zeng, Shuangjia Zheng, and Haifeng Hu. Hybrid contrastive learning of tri-modal representation"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "for multimodal sentiment analysis.\nIEEE Transactions on Affective Computing, 2022."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[39] Wei Han, Hui Chen, Alexander Gelbukh, Amir Zadeh, Louis-philippe Morency, and Soujanya Poria. Bi-bimodal"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "modality fusion for correlation-controlled multimodal sentiment analysis. In Proceedings of the 2021 International"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Conference on Multimodal Interaction, pages 6–15, 2021."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[40] Hao Sun, Hongyi Wang, Jiaqing Liu, Yen-Wei Chen, and Lanfen Lin. Cubemlp: An mlp-based model for multi-"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "modal sentiment analysis and depression estimation.\nIn Proceedings of the 30th ACM International Conference"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "on Multimedia, pages 3722–3729, 2022."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Improving the modality\n[41] Peipei Liu, Xin Zheng, Hong Li, Jie Liu, Yimo Ren, Hongsong Zhu, and Limin Sun."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "representation with multi-view contrastive learning for multimodal sentiment analysis.\nIn ICASSP 2023-2023"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE, 2023."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[42] Kyeonghun Kim and Sanghyun Park. Aobert: All-modalities-in-one bert for multimodal sentiment analysis."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Information Fusion, 92:37–45, 2023."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[43] Yewon Hwang and Jong-Hwan Kim.\nSelf-supervised unimodal\nlabel generation strategy using recalibrated"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "modality representations for multimodal sentiment analysis.\nIn Findings of the Association for Computational"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Linguistics: EACL 2023, pages 35–46, 2023."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[44]\nJiuding Yang, Yakun Yu, Di Niu, Weidong Guo, and Yu Xu. Confede: Contrastive feature decomposition for"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "multimodal sentiment analysis.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "Linguistics (Volume 1: Long Papers), pages 7617–7630, 2023."
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "[45] Daoming Zong, Chaoyue Ding, Baoxiang Li, Jiakui Li, Ken Zheng, and Qunyan Zhou. Acformer: An aligned and"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "compact transformer for multimodal sentiment analysis. In Proceedings of the 31st ACM International Conference"
        },
        {
          "[27] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario": "on Multimedia, pages 833–842, 2023."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "based on text-centric hierarchical fusion network. Knowledge-Based Systems, 300:112220, 2024."
        },
        {
          "[46]": "[47] Youjia Fu, Junsong Fu, Huixia Xue, and Zihao Xu. Self-hcl: Self-supervised multitask learning with hybrid",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": ""
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "contrastive learning strategy for multimodal sentiment analysis. Electronics, 13(14):2835, 2024."
        },
        {
          "[46]": "[48] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research,",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": ""
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "9(11), 2008."
        },
        {
          "[46]": "[49] Hanlei Zhang, Hua Xu, Xin Wang, Qianrui Zhou, Shaojie Zhao, and Jiayan Teng. Mintrec: A new dataset for",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": ""
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "multimodal intent recognition.\nIn Proceedings of the 30th ACM International Conference on Multimedia, pages"
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "1688–1697, 2022."
        },
        {
          "[46]": "[50] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency,",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": ""
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "and Ehsan Hoque.\nIntegrating multimodal information in large pretrained transformers.\nIn Proceedings of the"
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "conference. Association for Computational Linguistics. Meeting, volume 2020, page 2359. NIH Public Access,"
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "2020."
        },
        {
          "[46]": "[51] Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, and Yongbin",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": ""
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "Li. Speech-text pre-training for spoken dialog understanding with explicit cross-modal alignment.\nIn Proceedings"
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages"
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "7900–7913, 2023."
        },
        {
          "[46]": "[52] Kaili Sun, Zhiwen Xie, Mang Ye, and Huyin Zhang. Contextual augmented global contrast for multimodal intent",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": ""
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "recognition.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages"
        },
        {
          "[46]": "",
          "Jingming Hou, Nazlia Omar, Sabrina Tiun, Saidah Saad, and Qian He. Tchfn: Multimodal sentiment analysis": "26963–26973, 2024."
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Rada Mihalcea"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "3",
      "title": "What makes multi-modal learning better than single (provably)",
      "authors": [
        "Yu Huang",
        "Chenzhuang Du",
        "Zihui Xue",
        "Xuanyao Chen",
        "Hang Zhao",
        "Longbo Huang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning",
      "authors": [
        "Yuhui Victor Weixin Liang",
        "Yongchan Zhang",
        "Serena Kwon",
        "James Yeung",
        "Zou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Domain separation networks",
      "authors": [
        "Konstantinos Bousmalis",
        "George Trigeorgis",
        "Nathan Silberman",
        "Dilip Krishnan",
        "Dumitru Erhan"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "9",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "11",
      "title": "Multilogue-net: A context aware rnn for multi-modal emotion detection and sentiment analysis in conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Multilogue-net: A context aware rnn for multi-modal emotion detection and sentiment analysis in conversation",
      "arxiv": "arXiv:2002.08267"
    },
    {
      "citation_id": "12",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "year": "2019",
      "venue": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "arxiv": "arXiv:1905.05812"
    },
    {
      "citation_id": "13",
      "title": "Hierarchical question-image co-attention for visual question answering",
      "authors": [
        "Jiasen Lu",
        "Jianwei Yang",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "15",
      "title": "A transformer-based jointencoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "A transformer-based jointencoding for emotion recognition and sentiment analysis",
      "arxiv": "arXiv:2006.15955"
    },
    {
      "citation_id": "16",
      "title": "Disentangling by factorising",
      "authors": [
        "Hyunjik Kim",
        "Andriy Mnih"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "17",
      "title": "Factorized orthogonal latent spaces",
      "authors": [
        "Carl Mathieu Salzmann",
        "Raquel Henrik Ek",
        "Trevor Urtasun",
        "Darrell"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "18",
      "title": "Learning disentangled representation for cross-modal retrieval with deep mutual information estimation",
      "authors": [
        "Weikuo Guo",
        "Huaibo Huang",
        "Xiangwei Kong",
        "Ran He"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Learning disentangled user representation with multi-view information fusion on social networks",
      "authors": [
        "Wenyi Tang",
        "Bei Hui",
        "Ling Tian",
        "Guangchun Luo",
        "Zaobo He",
        "Zhipeng Cai"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "21",
      "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
      "authors": [
        "Yi Zhang",
        "Mingyuan Chen",
        "Jundong Shen",
        "Chongjun Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "23",
      "title": "Learning deep representations by mutual information estimation and maximization",
      "authors": [
        "Devon Hjelm",
        "Alex Fedorov",
        "Samuel Lavoie-Marchildon",
        "Karan Grewal",
        "Phil Bachman",
        "Adam Trischler",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "Learning deep representations by mutual information estimation and maximization",
      "arxiv": "arXiv:1808.06670"
    },
    {
      "citation_id": "24",
      "title": "Barlow twins: Self-supervised learning via redundancy reduction",
      "authors": [
        "Jure Zbontar",
        "Li Jing",
        "Ishan Misra",
        "Yann Lecun",
        "Stéphane Deny"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "A note on connecting barlow twins with negative-sample-free contrastive learning",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2021",
      "venue": "A note on connecting barlow twins with negative-sample-free contrastive learning",
      "arxiv": "arXiv:2104.13712"
    },
    {
      "citation_id": "26",
      "title": "Supervised feature selection via dependence estimation",
      "authors": [
        "Le Song",
        "Alex Smola",
        "Arthur Gretton",
        "Karsten Borgwardt",
        "Justin Bedo"
      ],
      "year": "2007",
      "venue": "Proceedings of the 24th international conference on Machine learning"
    },
    {
      "citation_id": "27",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "François Laviolette",
        "Mario March",
        "Victor Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "28",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "29",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "Md Kamrul Hasan",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Jianyuan Zhong",
        "Md Iftekhar Tanveer",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Ur-funny: A multimodal language dataset for understanding humor",
      "arxiv": "arXiv:1904.06618"
    },
    {
      "citation_id": "31",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "32",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "33",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrušaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE winter conference on applications of computer vision (WACV)"
    },
    {
      "citation_id": "34",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "35",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "36",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "38",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "Sijie Mai",
        "Ying Zeng",
        "Shuangjia Zheng",
        "Haifeng Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Alexander Gelbukh",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "40",
      "title": "Cubemlp: An mlp-based model for multimodal sentiment analysis and depression estimation",
      "authors": [
        "Hongyi Hao Sun",
        "Jiaqing Wang",
        "Yen-Wei Liu",
        "Lanfen Chen",
        "Lin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Improving the modality representation with multi-view contrastive learning for multimodal sentiment analysis",
      "authors": [
        "Peipei Liu",
        "Xin Zheng",
        "Hong Li",
        "Jie Liu",
        "Yimo Ren",
        "Hongsong Zhu",
        "Limin Sun"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "42",
      "title": "Aobert: All-modalities-in-one bert for multimodal sentiment analysis",
      "authors": [
        "Kyeonghun Kim",
        "Sanghyun Park"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "43",
      "title": "Self-supervised unimodal label generation strategy using recalibrated modality representations for multimodal sentiment analysis",
      "authors": [
        "Yewon Hwang",
        "Jong-Hwan Kim"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EACL 2023"
    },
    {
      "citation_id": "44",
      "title": "Confede: Contrastive feature decomposition for multimodal sentiment analysis",
      "authors": [
        "Jiuding Yang",
        "Yakun Yu",
        "Di Niu",
        "Weidong Guo",
        "Yu Xu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "Acformer: An aligned and compact transformer for multimodal sentiment analysis",
      "authors": [
        "Daoming Zong",
        "Chaoyue Ding",
        "Baoxiang Li",
        "Jiakui Li",
        "Ken Zheng",
        "Qunyan Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Tchfn: Multimodal sentiment analysis based on text-centric hierarchical fusion network",
      "authors": [
        "Jingming Hou",
        "Nazlia Omar",
        "Sabrina Tiun",
        "Saidah Saad",
        "Qian He"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "47",
      "title": "Self-hcl: Self-supervised multitask learning with hybrid contrastive learning strategy for multimodal sentiment analysis",
      "authors": [
        "Youjia Fu",
        "Junsong Fu",
        "Huixia Xue",
        "Zihao Xu"
      ],
      "year": "2024",
      "venue": "Electronics"
    },
    {
      "citation_id": "48",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "49",
      "title": "Mintrec: A new dataset for multimodal intent recognition",
      "authors": [
        "Hanlei Zhang",
        "Hua Xu",
        "Xin Wang",
        "Qianrui Zhou",
        "Shaojie Zhao",
        "Jiayan Teng"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "50",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amir Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "51",
      "title": "Speech-text pre-training for spoken dialog understanding with explicit cross-modal alignment",
      "authors": [
        "Tianshu Yu",
        "Haoyu Gao",
        "Ting-En",
        "Min Lin",
        "Yuchuan Yang",
        "Wentao Wu",
        "Chao Ma",
        "Fei Wang",
        "Yongbin Huang",
        "Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "Contextual augmented global contrast for multimodal intent recognition",
      "authors": [
        "Kaili Sun",
        "Zhiwen Xie",
        "Mang Ye",
        "Huyin Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}