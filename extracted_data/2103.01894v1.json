{
  "paper_id": "2103.01894v1",
  "title": "Investigations On Audiovisual Emotion Recognition In Noisy Conditions",
  "published": "2021-03-02T17:45:16Z",
  "authors": [
    "Michael Neumann",
    "Ngoc Thang Vu"
  ],
  "keywords": [
    "speech emotion recognition",
    "audiovisual",
    "noisy conditions",
    "multimodal",
    "data augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper we explore audiovisual emotion recognition under noisy acoustic conditions with a focus on speech features. We attempt to answer the following research questions: (i) How does speech emotion recognition perform on noisy data? and (ii) To what extend does a multimodal approach improve the accuracy and compensate for potential performance degradation at different noise levels? We present an analytical investigation on two emotion datasets with superimposed noise at different signal-to-noise ratios, comparing three types of acoustic features. Visual features are incorporated with a hybrid fusion approach: The first neural network layers are separate modality-specific ones, followed by at least one shared layer before the final prediction. The results show a significant performance decrease when a model trained on clean audio is applied to noisy data and that the addition of visual features alleviates this effect.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Related Work",
      "text": "Automatic recognition of human affect has become a fastgrowing research field in recent years, finding its way into a variety of application areas, such as health care, call centers, voice assistants, or automotive applications. While the research focus has long been on one modality individually (e.g. speech or facial expressions), multimodal emotion recognition -and multimodal machine learning in general -has gained increasing attention recently. For a comprehensive survey on multimodal machine learning the reader can refer to  [1] . Regarding emotion recognition, Sebe et al. presented a survey on multimodal approaches  [2]  and proposed probabilistic graphical models for fusing modalities. In another early work, Busso et al. compared early and late fusion and showed that acoustic and visual features contain complementary information about expressed emotions  [3] . More recently, deep learning and end-to-end learning gained traction in the field, partially because of the availability of larger amounts of training data  [4, 5, 6, 7, 8] .\n\nAnother aspect of increasing interest is the performance of systems outside of clean laboratory conditions, which is for example addressed by the 'Emotion Recognition in the Wild Challenge'  [9] . The effect of noisy data has been investigated for speech in several studies  [10, 11, 12, 13]  and speech enhancement methods are one promising direction for better SER quality  [14, 15, 16, 17] . While we are aware of the different methods to attenuate noise effects in speech data, we focus specifically on a multimodal approach because only few studies addressed the problem of noisy data in audiovisual experiments and we want to investigate the complementary effects of both modalities. Banda et al. focused on the effect of corrupted videos and showed that a multimodal system retains a reasonably high performance compared to video-only  [18] . Lin et al. added noise to both audio and video and presented a semi-coupled Hidden Markow Model to diminish the negative impact of noise  [19] . In contrast, we focus on noisy acoustics, leaving the videos untouched. While most of the abovementioned studies present results for matched train and test data (either clean or with added noise) and for positive signalto-noise ratios (SNR), we also analyze the performance in the unmatched condition (train on clean and test on noisy data), including negative SNRs, as it was done by Triantafyllopoulos et al.  [17]  with deep learning based speech enhancement.\n\nThe main research questions we attempt to answer are: (i) How does speech emotion recognition perform on noisy data (at different noise intensities)? (ii) To what extend does a multimodal approach improve the accuracy in clean and noisy conditions? Our findings show strong performance degradation when speech-only models are applied to noisy audio, throughout different features, noise types and datasets. Adding visual features and data augmentation both significantly improve the performance. The analysis of error patterns in predictions reveals a strong bias towards the class happy on noisy test data for audio-only; adding visual features weakens this effect. Further, we found that multimodal fusion helps to distinguish the high-arousal emotion classes angry and happy better from each other in clean audio conditions. Comparing three acoustic feature sets, one main finding is that a convolutional neural network (CNN) with log Mel filterbanks as input performs most stable under noisy conditions, com-pared to feed-forward networks on the other feature sets, which exhibit strong biases towards single classes. However, the overall best performance is achieved with eGeMAPS features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acoustic Features",
      "text": "We examine three types of acoustic features, including handcrafted (eGeMAPS) and model-based representations (Deep-Spectrum). Noisy audio is generated as in  [20] : We use the acoustic simulator presented in  [21]  to superimpose different types of noises from the Freesound database  [22]  (babble, music and transportation noise) to the clean audio at {-10, -5, 0, 5} dB SNR. A sample rate of 16kHz is used for all data.\n\nThe extended Geneva minimalistic acoustic parameter set (eGeMAPS)  [23]  is a feature set recommendation for affective computing. It consists of 25 low level descriptors (frequency related, energy related and spectral parameters) to which several functionals are applied. This results in an 88-dimensional utterance-level representation. We use openSMILE  [24]  to extract eGeMAPS features.\n\nFor the third feature set we use DeepSpectrum  [25] , a Python toolkit for acoustic feature extraction based on pretrained image CNNs. Spectrograms are generated from the acoustic signal and fed into a pre-trained CNN. The activations of a specific layer form the feature vectors. We use the DeepSpectrum default settings, i.e. extract the activations of the 'fc2' layer from AlexNet, resulting in a 4,096-dimensional feature vector for one utterance. 1",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Visual Features",
      "text": "Visual representations are obtained as follows: Each video frame is converted to gray scale and Contrast-limited adaptive histogram equalization  [26]  is applied to enhance contrast, followed by face recognition using dlib's  [27]  frontal face detector. The detected face region is cropped and resized to 100x100 pixels to feed it into the VGG 11-layer model (configuration \"A\" from  [28] ) trained on ImageNet  [29] . We take the activations of the first fully connected layer as framelevel intermediate features and apply average pooling across all frames of the same utterance to obtain a 4,096-dimensional utterance-level representation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Neural Network Structures",
      "text": "For both unimodal and multimodal experiments we train fully connected feed-forward (FF) neural networks (except for log Mel filterbanks, for which a CNN is applied). All models are implemented with PyTorch  [30] . The FF networks are  composed of a stack of fully connected layers with tanh nonlinearity, each followed by dropout regularization, as shown in Fig.  1(a) . For filterbanks, which are a time-preserving 3dimensional matrix, we train a strided CNN composed of two convolutional layers with ReLU activation, each followed by a dropout layer. There is no pooling layer, but pooling is implicitly controlled by tuning the stride size of the convolution. The kernels of the first convolutional layer span the entire input feature dimension (23 filterbanks), i.e. the subsequent layer is a 1-D convolution over time. Because a CNN requires a fixed input size, we set the sample length to 7.5s for MSP-IMPROV and 3s for CREMA-D (based on mean duration + standard deviation of each corpus) and apply zero-padding for shorter utterances.\n\nFor multimodal fusion we use a hybrid approach, shown in Fig.  1(b) : audio and video input is fed into separate subnetworks (at least one layer), whose outputs are concatenated and fed into a joint network. For filterbanks, the sub-network is a CNN (as described above) whose output is flattened and fed into the joint network.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "The MSP-IMPROV dataset  [31]  contains English dyadic interactions between actors (six pairs of one female and one male each). Video recordings show the speakers in front of a green screen; audio is provided at 44.1 kHz sample rate.  2 Emotion annotations have been obtained through crowdsourcing (at least five annotations per sample). We use the four discrete emotion classes angry, happy, neutral, sad, resulting in a total of 7,798 samples. Due to the imbalanced class distribution  3  we use a weighted cross entropy loss function in all experiments. The class weights are calculated as class weight = N/(C * N c ), where N is the total number of samples, C the number of classes, and N c the number of samples in class c. As there are no default train and test partitions, we apply 6-fold cross validation (leave-one-session-out) to ensure speaker-independent evaluation (10% of the train set are randomly selected as development set).\n\nThe CREMA-D dataset  [32]  consists of English read speech; 91 speakers read 12 target sentences in six different emotions (happy, sad, angry, fearful, disgusted, and neutral). Video recordings show the speakers in front of a green screen; audio is provided at 16 kHz sample rate. Annotations have also been obtained through crowdsourcing (at least six per sample). There are individual annotations for audio, video and audiovisual data from which we use the latter. To facilitate comparisons between datasets, we use the same four classes as for MSP-IMPROV, resulting in 4,799 samples. We apply the same weighted loss as described above because the data are also imbalanced. 4 For evaluation we split the data by speaker IDs to avoid speaker overlap: speakers 1-63 as train, speakers 64-77 as development, and speakers 78-91 as test set. We have verified that these partitions are balanced regarding age and gender distributions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hyper-Parameters And Training",
      "text": "To establish baselines, we assessed the accuracy on clean audio on the development set (average result across 6 folds for MSP-IMPROV) in a grid search for these hyper-parameters: number of layers, number of neurons per layer, dropout rate (additionally for CNN: number and size of feature maps, stride size). This was done for each feature type (unimodal and multimodal) and dataset individually. Table  1  shows the number and size of layers for each input option. The CNN hyper-parameters are: 128 feature maps of width 10 and stride 7 for MSP-IMPROV CREMA-D unimodal, and 128 feature maps of width 15 and stride 3 for CREMA-D multimodal. For MSP-IMPROV, we applied dropout at a rate of 0.5 for all except the DeepSpectrum features, for which we selected 0.7 to prevent overfitting. For CREMA-D, overfitting appears more prevalent, presumably because of the artificial nature of the data. We obtained a dropout rate of 0.7 for all but the unimodal eGeMAPS features, for which 0.5 was applied. Model training was done with a batch size of 32 for 100 epochs for unimodal and 50 epochs for multimodal input. We ran all experiments (except hyper-parameter tuning) three times and report mean and standard deviation in terms of unweighted average recall (UAR).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models Trained On Clean And Applied To Noisy Audio",
      "text": "In the first experiment we apply trained models to data with different noise levels and compare the results with the clean 4  reference data. The results are shown in Fig.  2 . Comparing different noise types, we observed the same tendencies across features and datasets. Overall, the recognition performance is slightly higher for transportation noise and lowest for babble noise. Fig.  3  shows results for all three noise types on the MSP-IMPROV data with eGeMAPS features. The remainder of this paper presents results for babble noise only, due to space limitations. In general, we observed a large decline in performance from clean to noisy audio on both datasets, with decreasing UAR for higher noise levels. Adding visual features consistently improves the performance by a large margin.\n\nFor MSP-IMPROV (Fig.  2a ), the best audio-only result is 48.59% ±0.23% (clean audio, eGeMAPS), the best multimodal result is 53.50% ±0.14% (clean audio, DeepSpectrum). The video-only UAR is 44.24% ±0.59%.\n\nTo gain more insights on the results, we analyzed the models' predictions. Exemplary confusion matrices are shown in Fig.  4 . For clean audio, the individual class recalls with eGeMAPS and DeepSpectrum features are well balanced (cf. Fig.  4a ). For filterbanks we observed a bias towards the class neutral (Fig.  4e ). Adding visual features improves the recall for angry and happy notably, which can partially be explained by the high recall for happy in the video-only case. When applied to noisy audio, we observed that happy is predominantly predicted for all three feature types. With eGeMAPS (audio-only), this effect is most pronounced (cf. Fig.  4b ). Adding the visual modality improves recall for the other three classes significantly (cf. Fig.  4d ). With filterbanks and DeepSpectrum features (audio-only), the majority of samples is predicted as either happy or neutral at low noise levels and the bias towards happy increases with higher noise levels. Adding visual features improves the recall for angry and sad considerably (cf. Fig.  4e-4h ), but a bias towards the high-arousal classes happy and angry remains at higher noise levels.\n\nOne main finding of the analysis is that the performance decline on noisy data is smallest for the model with filterbank features. However, the reference performance on clean data is lowest in this case. The confusion matrices show that the model with filterbank features is more stable on noisy audio data with respect to the balance between classes and the bias  towards one single class. Fig.  4b  and 4f illustrate this comparison: While with GeMAPS features, the number of samples wrongly predicted as happy is very high and almost all angry samples are predicted as happy, these effects are less pronounced with filterbank features. This difference between the models and features becomes even larger at high noise levels.\n\nFor CREMA-D (Fig.  2b ), the results show the same patterns as for MSP-IMPROV: performance decline proportional to the noise level (even more pronounced at higher noise levels). An exception is the combination of visual and eGeMAPS features, for which the UAR at 5dB SNR is higher than on clean audio. We found that the benefit of adding visual features is differently distributed across emotion classes. In general, the largest improvement is observed for the class happy, which also appears to be the class with the highest recall for video-only. As a result, it can happen that the total UAR is slightly higher on noisy data than on clean data because the imbalance between classes increases and certain biases are even more emphasized. This effect is not as strong for MSP-IMPROV because overall the differences in recall for individual classes are not as large.\n\nThe best audio-only result is 63.76% ±1.35% (clean audio, eGeMAPS), the best multimodal result is obtained with eGeMAPS features (71.38% ±0.28% at 5dB SNR and 71.17% ±0.29% on clean audio). The video-only UAR is 59.63% ±0.71%.\n\nThe inspection of confusion matrices showed high recall for the class angry on clean audio throughout all feature sets. With eGeMAPS the class recalls are most balanced, while with filterbanks a high proportion of samples is wrongly predicted as neutral. On noisy audio we observed a strong bias towards happy with eGeMAPS, a strong bias towards angry with filterbanks and high confusion between sad and neutral with DeepSpectrum features. With higher noise levels the biases towards the high-arousal classes angry and happy become stronger. The addition of visual features improves generally the recall for happy and neutral.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Models Trained On Single Noise Levels",
      "text": "In the second experiment, we train and evaluate the models at the same noise level (matched condition). Fig.  5  shows the results for MSP-IMPROV. The results for CREMA-D exhibit similar characteristics. In contrast to the first experiment the performance remains much more stable for noisy data when the model is trained on this kind of data. The results within one feature set are at a similar level with a tendency of slightly lower UAR for higher noise levels. This decrease is most pronounced for eGeMAPS features (audio-only). These observations paired with the findings of the first experiment emphasize the severe consequences that a mismatch between train and test data can cause.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Augmentation: Training On All Noise Levels",
      "text": "In the third experiment all models are trained on the union of data from all noise levels (including clean audio) and evaluated on the different noise levels separately (similar setup as in section 4.1). The results for MSP-IMPROV are shown in     2a , the performance on clean audio decreases throughout all features and multimodal combinations, while it improves remarkably on noisy data, especially for audio-only. The addition of visual features is especially useful for filterbank and DeepSpectrum features. These results show that data augmentation in the form of added noise is beneficial in noisy conditions. However, a trade-off between lower accuracy on clean and higher accuracy on noisy data needs to be accepted.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this investigation on audiovisual emotion recognition in noisy acoustic conditions, we have shown that the performance decreases significantly when training and test data do not match (clean vs. noisy), and that this effect is dampened with audiovisual models. We showed that data augmentation by adding noise to the training set increases the accuracy on noisy audio significantly, but can affect results on clean data negatively.\n\nOne limitation of the present study is that the Lombard effect -the phenomenon that people speak differently than usual in noisy environments -is not taken into account. To consider this, future work needs to be based on real-world noisy speech data instead of superimposed noise. The comparison between feature sets showed that the eGeMAPS parameter set yields the best overall results. However, the inspection of error patterns revealed that the CNN with log Mel filterbank features yields more stable predictions under noisy conditions with respect to the magnitude of accuracy decline and the class balance in predictions.",
      "page_start": 5,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Neural network architectures.",
      "page": 2
    },
    {
      "caption": "Figure 1: (a). For ﬁlterbanks, which are a time-preserving 3-",
      "page": 2
    },
    {
      "caption": "Figure 1: (b): audio and video input is fed into separate sub-",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparing",
      "page": 3
    },
    {
      "caption": "Figure 3: shows results for all three noise types on the",
      "page": 3
    },
    {
      "caption": "Figure 2: a), the best audio-only result",
      "page": 3
    },
    {
      "caption": "Figure 4: For clean audio, the individual class recalls with",
      "page": 3
    },
    {
      "caption": "Figure 4: e). Adding visual features improves",
      "page": 3
    },
    {
      "caption": "Figure 4: b). Adding the visual modality improves recall for the",
      "page": 3
    },
    {
      "caption": "Figure 4: d). With ﬁlter-",
      "page": 3
    },
    {
      "caption": "Figure 4: e-4h), but a bias towards the",
      "page": 3
    },
    {
      "caption": "Figure 2: Results for training on clean audio only and applying the models to noisy audio data (babble noise).",
      "page": 4
    },
    {
      "caption": "Figure 3: Results on MSP-IMPROV for training with clean",
      "page": 4
    },
    {
      "caption": "Figure 4: b and 4f illustrate this compar-",
      "page": 4
    },
    {
      "caption": "Figure 2: b), the results show the same",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the",
      "page": 4
    },
    {
      "caption": "Figure 4: Results (recall in %) for MSP-IMPROV from uni- and multimodal models trained on clean audio only and tested on",
      "page": 5
    },
    {
      "caption": "Figure 5: Results on MSP-IMPROV for training and evaluation",
      "page": 5
    },
    {
      "caption": "Figure 6: Again, CREMA-D results exhibit similar character-",
      "page": 5
    },
    {
      "caption": "Figure 2: a, the performance on clean audio",
      "page": 5
    },
    {
      "caption": "Figure 6: Results on MSP-IMPROV for training on all noise",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Hyper-parameters. ’x+y’ means: x layers in each",
      "data": [
        {
          "2\n2": "3",
          "128\n128": "256",
          "3\n2": "3"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI"
    },
    {
      "citation_id": "4",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso"
      ],
      "year": "2004",
      "venue": "Intl. conference on Multimodal interfaces"
    },
    {
      "citation_id": "5",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "authors": [
        "J Han"
      ],
      "year": "2019",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "7",
      "title": "Multimodal fusion based on information gain for emotion recognition in the wild",
      "authors": [
        "E Ghaleb"
      ],
      "year": "2017",
      "venue": "2017 Intelligent Systems Conference (IntelliSys)"
    },
    {
      "citation_id": "8",
      "title": "Lstm-modeling of continuous emotions in an audiovisual affect recognition framework",
      "authors": [
        "M Wöllmer"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "9",
      "title": "Performance analysis of unimodal and multimodal models in valence-based empathy recognition",
      "authors": [
        "A Mallol-Ragolta"
      ],
      "year": "2019",
      "venue": "IEEE FG"
    },
    {
      "citation_id": "10",
      "title": "Emotiw 2019: Automatic emotion, engagement and cohesion prediction tasks",
      "authors": [
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition in the noise applying large acoustic feature sets",
      "authors": [
        "B Schuller"
      ],
      "year": "2006",
      "venue": "Speech Prosody"
    },
    {
      "citation_id": "12",
      "title": "Towards more reality in the recognition of emotional speech",
      "authors": [
        "B Schuller"
      ],
      "year": "2007",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition from noisy speech",
      "authors": [
        "M You"
      ],
      "year": "2006",
      "venue": "ICME"
    },
    {
      "citation_id": "14",
      "title": "Robust emotion recognition in noisy speech via sparse representation",
      "authors": [
        "X Zhao"
      ],
      "year": "2014",
      "venue": "Robust emotion recognition in noisy speech via sparse representation"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition in noisy environment",
      "authors": [
        "F Chenchah",
        "Z Lachiri"
      ],
      "year": "2016",
      "venue": "ATSIP"
    },
    {
      "citation_id": "16",
      "title": "Investigating speech enhancement and perceptual quality for speech emotion recognition",
      "authors": [
        "A Avila"
      ],
      "year": "2018",
      "venue": "Investigating speech enhancement and perceptual quality for speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with lstm neural networks",
      "authors": [
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with lstm neural networks"
    },
    {
      "citation_id": "18",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos"
      ],
      "year": "2019",
      "venue": "Towards robust speech emotion recognition using deep residual networks for speech enhancement"
    },
    {
      "citation_id": "19",
      "title": "Noise analysis in audiovisual emotion recognition",
      "authors": [
        "N Banda",
        "P Robinson"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "A probabilistic fusion strategy for audiovisual emotion recognition of sparse and noisy data",
      "authors": [
        "J.-C Lin"
      ],
      "year": "2013",
      "venue": "Intl. Conference on Orange Technologies"
    },
    {
      "citation_id": "21",
      "title": "Investigations on end-to-end audiovisual fusion",
      "authors": [
        "M Wand"
      ],
      "year": "2018",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "22",
      "title": "A large-scale open-source acoustic simulator for speaker recognition",
      "authors": [
        "M Ferras"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "23",
      "title": "Freesound technical demo",
      "authors": [
        "F Font"
      ],
      "year": "2013",
      "venue": "ACM international conference on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben"
      ],
      "year": "2013",
      "venue": "Intl. Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian"
      ],
      "year": "2017",
      "venue": "Snore sound classification using image-based deep spectrum features"
    },
    {
      "citation_id": "27",
      "title": "Adaptive histogram equalization and its variations",
      "authors": [
        "S Pizer"
      ],
      "year": "1987",
      "venue": "Computer vision, graphics, and image processing"
    },
    {
      "citation_id": "28",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "29",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "ICLR"
    },
    {
      "citation_id": "30",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky"
      ],
      "year": "2015",
      "venue": "IJCV"
    },
    {
      "citation_id": "31",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "A Paszke"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}