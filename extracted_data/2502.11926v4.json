{
  "paper_id": "2502.11926v4",
  "title": "Brighter: Bridging The Gap In Human-Annotated Textual Emotion Recognition Datasets For 28 Languages",
  "published": "2025-02-17T15:39:50Z",
  "authors": [
    "Shamsuddeen Hassan Muhammad",
    "Nedjma Ousidhoum",
    "Idris Abdulmumin",
    "Jan Philip Wahle",
    "Terry Ruas",
    "Meriem Beloucif",
    "Christine de Kock",
    "Nirmal Surange",
    "Daniela Teodorescu",
    "Ibrahim Said Ahmad",
    "David Ifeoluwa Adelani",
    "Alham Fikri Aji",
    "Felermino D. M. A. Ali",
    "Ilseyar Alimova",
    "Vladimir Araujo",
    "Nikolay Babakov",
    "Naomi Baes",
    "Ana-Maria Bucur",
    "Andiswa Bukula",
    "Guanqun Cao",
    "Rodrigo Tufino Cardenas",
    "Rendi Chevi",
    "Chiamaka Ijeoma Chukwuneke",
    "Alexandra Ciobotaru",
    "Daryna Dementieva",
    "Murja Sani Gadanya",
    "Robert Geislinger",
    "Bela Gipp",
    "Oumaima Hourrane",
    "Oana Ignat",
    "Falalu Ibrahim Lawan",
    "Rooweither Mabuya",
    "Rahmad Mahendra",
    "Vukosi Marivate",
    "Alexander Panchenko",
    "Andrew Piper",
    "Charles Henrique Porto Ferreira",
    "Vitaly Protasov",
    "Samuel Rutunda",
    "Manish Shrivastava",
    "Aura Cristina Udrea",
    "Lilian Diana Awuor Wanzare",
    "Sophie Wu",
    "Florian Valentin Wunderlich",
    "Hanif Muhammad Zhafran",
    "Tianhui Zhang",
    "Yi Zhou",
    "Saif M. Mohammad"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "People worldwide use language in subtle and complex ways to express emotions. Although emotion recognition-an umbrella term for several NLP tasks-impacts various applications within NLP and beyond, most work in this area has focused on high-resource languages. This has led to significant disparities in research efforts and proposed solutions, particularly for under-resourced languages, which often lack high-quality annotated datasets. In this paper, we present BRIGHTER-a collection of multilabeled, emotion-annotated datasets in 28 different languages and across several domains. BRIGHTER primarily covers low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances labeled by fluent speakers. We highlight the challenges related to the data collection and annotation processes, and then report experimental results for monolingual and crosslingual multi-label emotion identification, as well as emotion intensity recognition. We analyse the variability in performance across languages and text domains, * Equal contribution both with and without the use of LLMs, and show that the BRIGHTER datasets represent a meaningful step towards addressing the gap in text-based emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "While emotions are expressed and managed daily, they are complex, nuanced, and sometimes hard to articulate and interpret. That is, people use language in subtle and complex ways to express emotions across languages and cultures  (Wiebe et al., 2005; Mohammad and Kiritchenko, 2018; Mohammad et al., 2018a)  and perceive them subjectively, even within the same culture or social group. Emotion recognition is at the core of several NLP applications in healthcare, dialogue systems, computational social science, digital humanities, narrative analysis, and many others  (Mohammad et al., 2018b; Saffar et al., 2023) . It is an umbrella term for multiple NLP tasks, such as detecting the possible emotions of the speaker, identifying what emotion a piece of text is conveying, and detecting the emotions evoked in a reader  (Mohammad, 2022) . In this paper, we use emotion recognition to refer to perceived emotions, i.e., what emotion most people think the speaker might have felt given a sentence or a short text snippet uttered by them.\n\nMost work on emotion recognition has focused on high-resource languages such as English, Spanish, German, and Arabic  (Strapparava and Mihalcea, 2007; Seyeditabari et al., 2018; Chatterjee et al., 2019; Kumar et al., 2022) . This is partly due to the unavailability of datasets in under-served languages, which has led to a major research gap in the area, which is particularly noticeable in lowresource languages. That is, despite the linguistic diversity present in different parts of the world, such as Africa and Asia, which are home to more than 4,000 languages 1  , few emotion recognition resources are available in these languages. To bridge this gap, we introduce BRIGHTER-a collection of manually annotated emotion datasets for 28 languages containing nearly 100,000 instances from diverse data sources: speeches, social media, news, literature, and reviews. The languages belong to 7 language families (see Figure  1 ) and are predominantly low-resource, mainly spoken in Africa, Asia, Eastern Europe, Latin America, along with mid-to high-resource languages such as English. Each instance in BRIGHTER is curated and annotated by fluent speakers based on six emotion classes: joy, sadness, anger, fear, surprise, disgust, and none for neutral. The instances are multilabeled and include 4 levels of intensity that vary from 0 to 3 (examples in Figure  2 ). We describe the collection, annotation, and quality control steps used to construct BRIGHTER. We then test various baseline experiments and observe that LLMs still struggle with recognising perceived emotions in text. We further report on the observed discrepancies across languages such as the fact that, for lowresource languages, LLMs perform significantly better when prompted in English. We make our datasets public 2  , which presents an important step towards work on emotion recognition and related tasks as we involve local communities in the collection and annotation. Our insights into languagespecific characteristics of emotions in text, nuances, and challenges may enable the creation of more inclusive digital tools.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Brighter Dataset Collection",
      "text": "As our BRIGHTER collection includes datasets in 28 different languages, curated and annotated by fluent speakers, we use several data sources, collection, and annotation strategies depending on 1) the availability of the textual data potentially rich in emotions and 2) access to annotators. We detail the choices made when selecting and balancing sources, annotating the instances, and controlling for data quality in the following section.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Sources",
      "text": "Selecting appropriate data can be challenging when resources are scarce. Therefore, we typically combine multiple sources, as shown in Table  1 . Below, we outline the main textual domain inputs used to construct BRIGHTER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Social Media Posts",
      "text": "We use social media data collected from various platforms, including  Reddit (e.g., eng, deu), YouTube (e.g., esp, ind, jav, sun), Twitter (e.g., hau, ukr), and Weibo (e.g., chn) . For some languages, we re-annotate existing sentiment datasets for emotions (e.g., the sentiment analysis benchmark AfriSenti  (Muhammad et al., 2023a)  for ary, hau, kin; the Twitter dataset by  Bobrovnyk (2019)  for ukr; the RED-v2 dataset  (Ciobotaru et al., 2022)  for ron).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Personal Narratives, Talks, And Speeches",
      "text": "Anonymised sentences from personal diary posts are ideal for extracting sentences where the speaker is centering their own emotions as opposed to the emotions of someone else. Hence, we use these in eng, deu, and ptbr, mainly from subreddits such as, e.g., IAmI.\n\nSimilarly, the afr dataset includes sentences from speeches and talks which constitute a good source for potentially emotive text.\n\nLiterary texts We manually translated the novel \"La Grande Maison\" (The Big House) by the Algerian author Mohammed Dib 3  from French to Algerian Arabic and further post-processed the translation to generate sentences to be annotated by native speakers. Note that the translator is bilingual and a native Algerian Arabic speaker. Such a source is typically rich in emotions as it includes interactions between various characters. Moreover, Algerian Arabic is mainly spoken due to the Arabic diglossia, which makes this resource valuable since it highly differs from social media datasets in arq.\n\nNews data Although we prefer emotionally rich social media data from different platforms, such data is not always available. Therefore, when data sources are limited, to collect a larger number of instances, we annotate news data and headlines in some African languages (e.g., yor, hau, and vmw).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Human-Written And Machine Generated Data",
      "text": "We create a dataset from scratch for Hindi (hin) and Marathi (mar). We ask annotators to generate emotive sentences on a given topic (e.g., family). In addition, we automatically translate a small section of the Hindi dataset to Marathi, and native speakers manually fix the translation errors. Finally, we augment both datasets with a few hundred qualityapproved instances generated by ChatGPT.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Processing And Quality Control",
      "text": "Prior to annotation, we preprocess the data by removing duplicates, invisible characters, garbled encoding, and incorrectly rendered emoticons. We anonymise all texts and exclude content with excessive expletives or dehumanising language.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Annotating Brighter",
      "text": "As a text snippet can elicit multiple emotions simultaneously, we ask the annotators to select all the emotions that apply to a given text rather than choosing a single dominant emotion class. (low intensity), 2 (moderate intensity level), and 3 (high intensity). We provide the definitions of the categories and annotation guide in Appendix D.\n\nWe use Amazon Mechanical Turk to annotate the English dataset, and Toloka 4 to label the Russian, Ukrainian, and Tatar instances. However, as traditional crowdsourcing platforms do not have a large pool of annotators who speak various low-resource languages, we directly recruit fluent speakers to annotate the data and use the academic version of LabelStudio  (Tkachenko et al., 2020 (Tkachenko et al., -2025) )  and Potato  (Pei et al., 2022)  to set up our annotation platform.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Annotators' Reliability",
      "text": "While both inter-annotator agreement (IAA) and reliability scores evaluate annotation quality, they capture different aspects. IAA evaluates the extent to which annotators agree with one another, whereas reliability scores measure the consistency of aggregated labels across repeated annotation trials  (Kiritchenko and Mohammad, 2016) . Conse-4 https://toloka.ai quently, reliability scores tend to increase with a larger number of annotations, while IAA scores do not depend on the number of annotations per instance.\n\nWe report the annotation reliability using Split-Half Class Match Percentage (SHCMP;  Mohammad, 2024) . SHCMP extends the concept of Split-Half Reliability (SHR), traditionally applied to continuous scores  (Kiritchenko and Mohammad, 2016) , to discrete categories, such as our emotion intensity labels. SHCMP measures the extent to which n bins (i.e., random subsets) classify items consistently. The dataset is randomly split into n bins (corresponding to halves when n = 2) 1, 000 times, and the proportion of instances receiving the same class label across bins is averaged to return the final SHCMP score. A higher SHCMP indicates greater reliability, meaning that repeated annotations would likely result in similar class labels. Additional details are provided in Appendix D. Figure  3  shows a heatmap of SHCMP scores for the BRIGHTER datasets. Overall, the SHCMP scores are high (greater than 60% for n = 2), indi- cating that our annotations are reliable.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Determining The Final Labels",
      "text": "We expected a level of disagreement as emotions are complex, subtle, and perceived differently even from people within the same culture. In addition, text-based communication is limited as it lacks cues such as tone, relevant context, and information about the speaker. Our approach for aggregating the per-annotator emotion and intensity labels is detailed below. We also publicly share the individual (non-aggregated) annotations, recognising that annotator disagreement can provide useful signals in itself  (Plank, 2022) .\n\nAggregating the emotion labels The final emotion labels are determined based on the emotions and associated intensity values selected by the annotators. That is, the given emotion is considered present if:\n\n1. At least two annotators select a label with an intensity value of 1, 2, or 3 (low, medium, or high, respectively).\n\n2. The average score exceeds a predefined threshold T . We set T to 0.5.\n\nAggregating the intensity labels Once the labels for perceived emotions are assigned, we determine the final intensity score for each instance by averaging the selected intensity scores and rounding them up to the nearest integer. We assign intensity scores only for datasets in which the majority of instances are annotated by ≥ 5 annotators, to ensure robustness. Therefore, BRIGHTER includes emotion labels for 28 languages and intensity labels for 10 languages.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Final Data Statistics",
      "text": "Figure  4  shows the distribution of the annotated emotions in the BRIGHTER datasets. The neutral class contains instances that do not belong to any of the six predefined categories (i.e., anger, disgust, fear, sadness, joy, and surprise). Although most languages include all six categories, the English dataset does not include disgust, and the Afrikaans one does not include surprise due to an insufficient class representation. Furthermore, class distributions show substantial variation as we chose various data sources as shown in Table  1 .\n\n3 Experiments",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Setup",
      "text": "We report the data split sizes in Table  1 . The test sets are relatively large, ranging from approximately 1, 000 to nearly 3, 000 instances. Datasets without training data are excluded from training and are used solely for testing in cross-lingual settings.\n\nFor our baseline experiments, we evaluate multilabel emotion classification and emotion intensity prediction using both Multilingual Language Models (MLMs) and Large Language Models (LLMs).\n\nMulti-label emotion classification in few-shot settings We report emotion classification performance using five LLMs-Qwen2.5-72B  (Yang et al., 2024) , Dolly-v2-12B  (Conover et al., 2023) , LlaMA-3.3-70B  (Touvron et al., 2023) ,  Mixtral-8x7B (Jiang et al., 2024) , and DeepSeek-R1-70B  (DeepSeek-AI et al., 2025) . We prompt the LLMs using Chain-of-Thought (CoT) reasoning to predict the presence of each emotion from the predefined set. We set the number of few-shot examples to 8 and consider only the first generated answer (i.e., top-1). We",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Surprise",
      "text": "Figure  4 : Emotion label distribution across the BRIGHTER datasets. Each bar represents the number of labeled instances per emotion (i.e., anger, disgust, fear, joy, sadness, surprise, and neutral) and its percentage.\n\nreport macro F1 scores across 28 languages.\n\nIn Appendix D, we also provide monolingual classification results for the 24 languages with training data (see Table  5 ).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Multi-Label Emotion Classification In Crosslingual Settings",
      "text": "We report the macro F-score results for systems trained without using any data in the 28 target languages when testing on each. Hence, we train MLMs on all languages in one family (see Figure  1 ) except for one held-out target language, which we test on and report the results for each test set. For families with only one language (i.e., Sino-Tibetan, Creole, and Turkic), we train on Slavic languages (rus and ukr) and test on tat; two Niger-Congo languages (swa and yor) and test on pcm; and on rus and test on chn.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Emotion Intensity Prediction",
      "text": "We report Pearson correlation scores for systems trained on the intensity-labeled training sets in 10 languages.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experimental Results",
      "text": "Table  2  reports the results of few-shot and crosslingual experiments for multi-label emotion classification and Table  3  reports those for emotion intensity classification. Our results corroborate how challenging emotion classification is for LLMs, even for high-resource languages such as eng and deu. The performance is worse for low-resource languages, for which Dolly-v2-12B performs the worst, and Qwen2.5-72B performs the best on average.\n\nWe observe the largest performance for yor with a maximum of 27.44. hin, mar, and tat have the best performance among all languages, which is unsurprising since the tat dataset is single-labeled, and close to 70% and 80% of the test data for mar and hin respectively are single-labeled.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Multi-Label Emotion Recognition Results",
      "text": "The crosslingual experiments demonstrate that model performance depends on both the languages used for transfer learning and those included in the pretraining of the models. For instance, in some cases, training on languages from the same family improves performance and even surpasses few-shot settings, e.g., swe benefits when RemBERT is finetuned on other Germanic languages. However, all Niger-Congo languages, particularly vmw, benefit the least from crosslingual transfer across all models, with RemBERT performing the worst. This is largely due to the severe under-resourcedness of these languages, even when data is combined. Notably, XLM-R performs exceptionally well on languages such as deu, chn, hin, and ptbr, but struggles significantly with others (e.g., swe, ptmz). In contrast, mDeBERTa yields the most consistent results across most languages, even though it shows low performance on ibo, vmw, and yor, which are not part of the CC-100 corpus (Conneau Table  2 : Average F1-Macro for multi-label emotion classification. In the few-shot setting, we predict the emotion class on test set in 28 languages. In the crosslingual setting, we train on all languages within a language family except the target language, and evaluate on the test set of the target language. The best performance scores in few-shot and crosslingual settings are highlighted in blue and orange, respectively.\n\net al., 2020) used in its training. While mDeBERTa was also not trained on arq, the inclusion of Modern Standard Arabic (MSA) in its pretraining data might have positively influenced its performance. Overall, our results indicate that multilingual models transfer more effectively to languages seen during pretraining, while often producing random or unreliable outputs for languages absent from their training data.\n\nEmotion intensity prediction For intensity detection, a more challenging task, Dolly-v2-12B performs the worst, whereas DeepSeek-R1-70B shows promising results, outperforming other models in most languages. Llama-3.3-70B and Qwen2.5-72B achieve the highest scores in English. Interestingly, MLMs tend to perform better on high-resource languages-RemBERT, in particular, achieves strong results for deu, eng, esp, and rus, with chn being the only exception. In contrast, for primarily spoken, low-resource vernaculars (e.g., arq), LLMs demonstrate striking improvements -DeepSeek-R1-70B, for instance, achieves improvements exceeding 36 points.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Analysis",
      "text": "The results in Figure  5a  suggest that LLM performance is highly dependent on the prompt wording when asking for the presence of emotion on the English test set using different paraphrases of the same text. Further, Figure  5b  shows that, when testing the effect of n-shot settings on the English test set, we observe a significant improvement in performance with more shots, with Mixtral-8x7B and Llama-3.3-70B outperforming other models. However, the scores tend to reach a plateau at 4 shots for all LLMs except for Qwen2.5-72B, which suggests that 4 to 8 shots may be sufficient to obtain stable results. In addition, when testing how likely we can get the correct answer when prompting LLMs to generate tokens based on a selection of k generations, the results shown in Figure  5c  suggest that increasing the value of k results",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Multilingual Language Models (Mlms)",
      "text": "Large Language Models (LLMs) Lang. LaBSE RemBERT XLM-R mBERT mDeBERTa Qwen2.5-72B Dolly-v2-12B Llama-3.3-70B Mixtral-8x7B DeepSeek-R1-70B    consistently in better performance, particularly when using DeepSeekR1-70B, which achieves an F-score > 90 when k = 8 whereas Mixtral-8x7B shows a smaller change in performance followed by Llama-3.3-70B and Qwen2.5-72B. The ranking of the models for k = 8 remains consistent with the one achieved for k = 1.\n\nWhen comparing the performance of models prompted in English versus the target language, Figure  6  shows that LLMs generally perform better with English prompts except for arq, where Qwen2.5-72B achieves better results when prompted in Modern Standard Arabic (MSA). The improvement from using English prompts is particularly evident in low-resource languages  (e.g., hau, mar, vmw) , where models like Dolly-v2-12B and Llama-3.3-70B perform poorly with prompts in the target language. Appraisal theories of emotion propose that emotions arise from our evaluation of events based on personal experiences, leading to different emotional responses among individuals  (Arnold, 1960; Frijda, 1986; Lazarus, 1991; Scherer, 2009; Ellsworth, 2013; Moors et al., 2013; Roseman, 2013; Ortony et al., 2022) . The theory of constructed emotion claims that emotions are not hardwired or universal, but rather conceptual constructs formed by the brain  (Barrett, 2016 (Barrett, , 2017)) .\n\nEarly work in NLP primarily focused on sentiment analysis-identifying whether a text conveys positive, negative, or neutral valence  (Mohammad, 2016; Muhammad et al., 2023b) . More recent research has shifted toward a broader goal: detecting specific emotions in text, such as anger, fear, joy, and sadness. This shift aligns with discrete models of emotion, including Paul Ekman's six basic emotions  (Ekman, 1992 ) and Plutchik's Wheel of Emotions  (Plutchik, 1980) , which includes anger, disgust, fear, happiness, sadness, surprise, anticipation, and trust.\n\nSeveral initiatives have created emotion classification datasets for languages other than English such as Italian  (Bianchi et al., 2021) , Romanian  (Ciobotaru et al., 2022) ,  Indonesian (Saputri et al., 2018), and Bengali (Iqbal et al., 2022) . However, the field remains predominantly Western-centric. Although multilingual datasets such as XED  (Öhman et al., 2020)  and XLM-EMO  (Bianchi et al., 2022)  exist, the latter's reliance on translated data for over ten languages may not adequately reflect cultural nuances in emotional expression. Emotions are culture-sensitive and highly contextual, shaped by different norms and values  (Hershcovich et al., 2022; Havaldar et al., 2023; Mohamed et al., 2024; Plaza-del Arco et al., 2024) .\n\nFurthermore, although emotions can co-occur  (Vishnubhotla et al., 2024) , most existing datasets assume a single-label classification framework. While GoEmotions  (Demszky et al., 2020)  addresses multi-label emotion classification, to our knowledge, no multilingual resources capture simultaneous emotions and intensity across languages. This work aims to advance the field by introducing emotion-labeled data for 28 languages. Given the lack of consensus around what constitutes a low-resource language, approximately 15 to 17 among these could reasonably be considered as such.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "We presented BRIGHTER, a collection of emotion recognition datasets in 28 languages spoken across various continents. The instances in BRIGHTER are multi-labeled, collected, and annotated by fluent speakers, with 10 datasets annotated for emotion intensity. When testing LLMs on our dataset collection, the results show that they still struggle with predicting perceived emotions and their intensity levels, especially for under-resourced languages. Further, our results show that LLM performance is highly dependent on the wording of the prompt, its language, and the number of shots in few-shot settings. We publicly release BRIGHTER, our annotation guidelines, and individual labels to the research community.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Limitations",
      "text": "Emotions are subjective, subtle, expressed, and perceived differently. We do not claim that BRIGHTER covers the true emotions of the speakers, is fully representative of the language use of the 28 languages, or covers all possible emotions. We discuss this extensively in the Ethics Section.\n\nWe are aware of the limited data sources in some low-resource languages. Therefore, our datasets cannot be used for tasks that require a large amount of data from a given language. However, they remain a good starting point for research in the area.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Ethical Considerations",
      "text": "Emotion perception and expression are inherently subjective and nuanced, as they are closely tied to a myriad of factors (e.g., cultural background, social group, personal experiences, and social context). As such, it is impossible to determine with absolute certainty how someone is feeling based solely on short text snippets. Therefore, we explicitly state that our datasets focus on perceived emotions-that is, the emotions most people believe the speaker may have felt. Accordingly, we do not claim to annotate the true emotion of the speaker, as this cannot be definitively inferred from short texts alone. We recognise the importance of this distinction, as perceived emotions may differ from actual emotions.\n\nWe also acknowledge potential biases in our data. Text-based communication inherently carries biases, and our data sources may reflect such tendencies. Similarly, annotators may come with their own subtle, internalised biases. Moreover, although many of our datasets focus on low-resource languages, we do not claim they fully capture the usage of these languages. While we took care to exclude inappropriate content, some instances may have been inadvertently overlooked.\n\nWe strongly encourage careful ethical reflection before using our datasets. Use of the data for commercial purposes or by state actors in high-risk applications is strictly prohibited unless explicitly approved by the dataset creators. Systems developed using our datasets may not be reliable at the individual instance level and are sensitive to domain shifts. They should not be used to make critical decisions about individuals, such as in health-related applications, without appropriate expert oversight. See  Mohammad (2022 Mohammad ( , 2023) )  for a comprehensive discussion on these issues.\n\nFinally, all annotators involved in the study were compensated at rates exceeding the local minimum wage.\n\n1. Random Splitting with Tie-Breaking The dataset of N annotated items is randomly divided into two equal subsets, A 1 and A 2 . For datasets with an odd number of annotations, probabilistic tie-breaking is applied to ensure balanced splits.\n\n2. Class Assignment For each item x i (i = 1, 2, . . . , N ):\n\n• Assign x i a score based on its annotations in A 1 and A 2 .\n\n• Let C 1 (x i ) and C 2 (x i ) denote the class of x i derived from A 1 and A 2 , respectively. Scores from A 1 and A 2 are then assigned to their respective bins, denoted as c 1 and c 2 .",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Class Binning",
      "text": "4. Match Calculation Define a match indicator M (x i ) to evaluate consistency for each item:\n\nThis ensures that items are considered consistent if their scores fall into the same bin or adjacent bins.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Proportion Of Matches",
      "text": "Compute the total number of matches, N match , across all items:\n\nM (x i ).",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Shcmp Computation",
      "text": "The SHCMP score is calculated as the proportion of matches, expressed as a percentage: SHCMP (%) = N match N × 100.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Averaging",
      "text": "We repeat the process k times with different random splits and compute the average SHCMP score:\n\nwhere SHCMP j is the SHCMP score from the j-th split.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "E Experimental Settings",
      "text": "For LLMs, we used the default parameters from HuggingFace except for temperature which we set to 0 for deterministic output and top-k is set to 1. Only for the top-k ablations in which top-k > 1 in Figure  5c , we set temperature to 0.7. We ask all LLMs to perform CoT. We trained on the train set for 2 epochs with a learning rate of 1e-5 and and evaluated on test set. For MLMs experiments, we trained on the training set for 2 epochs with a learning rate of 1e-5 and evaluated on the test set.",
      "page_start": 22,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Languages included in BRIGHTER and their",
      "page": 1
    },
    {
      "caption": "Figure 2: Examples from the BRIGHTER dataset collection in 6 different languages with their translations and",
      "page": 2
    },
    {
      "caption": "Figure 2: ). We describe",
      "page": 2
    },
    {
      "caption": "Figure 3: shows a heatmap of SHCMP scores",
      "page": 4
    },
    {
      "caption": "Figure 3: Split-Half Class Match Percentage,",
      "page": 5
    },
    {
      "caption": "Figure 4: shows the distribution of the annotated",
      "page": 5
    },
    {
      "caption": "Figure 4: Emotion label distribution across the BRIGHTER datasets. Each bar represents the number of labeled",
      "page": 6
    },
    {
      "caption": "Figure 1: ) except for one held-out target language,",
      "page": 6
    },
    {
      "caption": "Figure 5: a suggest that LLM perfor-",
      "page": 7
    },
    {
      "caption": "Figure 5: b shows that, when",
      "page": 7
    },
    {
      "caption": "Figure 5: c suggest that increasing the value of k results",
      "page": 7
    },
    {
      "caption": "Figure 5: Ablation studies on the effect of prompt wording variation, few-shot examples, and pass@k predictions",
      "page": 8
    },
    {
      "caption": "Figure 6: shows that LLMs generally per-",
      "page": 8
    },
    {
      "caption": "Figure 6: Comparing models’ performance across",
      "page": 8
    },
    {
      "caption": "Figure 5: c, we set temperature to 0.7. We ask",
      "page": 17
    },
    {
      "caption": "Figure 7: Example of the few-shot prompt template for assessing anger in Track A.",
      "page": 21
    },
    {
      "caption": "Figure 8: Example of the few-shot prompt template for assessing anger in Track B.",
      "page": 22
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "73.29\n51.91"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "76.91\n32.85\n49.51\n43.05\n32.52"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "76.68"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "45.00\n51.49\n39.58\n65.02"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "76.97"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "44.61"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "33.27"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "44.60"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "53.86"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "51.19\n19.09\n29.08"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "27.44"
        },
        {
          "43.66\n50.87\n47.21\n53.45\n54.26\n56.99": "20.38"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Train Set (%)\nLanguage\nSingle\nMulti\nNeutral": "chn\n54.00\n23.74\n22.26\nsun\n58.94\n36.18\n4.88\nafr\n47.79\n6.69\n45.52\nswe\n43.16\n16.60\n40.24\nswa\n41.67\n3.33\n55.00\nesp\n61.02\n38.98\n0.00\narq\n28.53\n50.05\n9.42\nptbr\n52.11\n13.80\n34.09\nptmz\n52.00\n0.44\n47.56\nukr\n44.77\n2.24\n52.99\nmar\n67.69\n8.56\n23.75\nrus\n64.63\n11.08\n24.29\nibo\n72.44\n3.63\n23.93\namh\n50.82\n27.68\n21.50\ndeu\n41.78\n34.05\n24.17\nvmw\n52.80\n0.45\n46.75\npcm\n55.00\n40.46\n4.54\neng\n38.64\n47.02\n14.34\nhin\n66.35\n10.80\n22.85\ntat\n81.48\n0.00\n18.52",
          "Development Set (%)\nSingle\nMulti\nNeutral": "53.60\n23.58\n22.82\n59.09\n36.26\n4.65\n56.14\n7.86\n36.01\n46.30\n20.37\n33.33\n45.78\n3.56\n50.66\n65.22\n34.78\n0.00\n28.57\n50.00\n10.71\n61.06\n11.82\n27.12\n50.92\n0.37\n48.71\n47.24\n2.36\n50.39\n68.57\n7.62\n23.81\n66.35\n12.23\n21.42\n61.12\n10.91\n27.97\n56.13\n30.31\n16.56\n41.84\n35.19\n22.97\n53.49\n0.39\n46.12\n50.00\n36.63\n4.37\n34.07\n42.22\n9.70\n60.40\n7.92\n31.68\n84.00\n0.00\n16.00",
          "Test Set (%)\nSingle\nMulti\nNeutral": "53.90\n24.30\n21.80\n59.40\n36.07\n4.54\n37.39\n10.35\n52.26\n42.76\n18.81\n38.43\n46.26\n3.81\n49.93\n65.14\n34.86\n0.00\n27.95\n44.76\n8.35\n52.68\n13.59\n33.73\n53.03\n0.51\n46.45\n45.23\n1.79\n52.98\n68.94\n9.33\n21.73\n66.91\n12.89\n20.20\n73.61\n3.97\n22.42\n48.50\n24.67\n26.83\n41.23\n32.10\n26.66\n53.46\n0.52\n46.32\n51.57\n38.08\n4.35\n38.58\n48.76\n10.34\n77.31\n5.66\n13.92\n85.71\n0.00\n14.29"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: Average F1-Macro for monolingual multi-label emotion classification. Each model is trained and",
      "data": [
        {
          "50.64": "80.76\n51.30\n42.60",
          "46.29": "82.20\n55.50"
        },
        {
          "50.64": "",
          "46.29": "42.57"
        },
        {
          "50.64": "36.95\n69.79\n75.62\n36.93",
          "46.29": "45.91\n76.23\n83.77\n37.31"
        },
        {
          "50.64": "27.53",
          "46.29": "22.65"
        },
        {
          "50.64": "49.23\n57.71",
          "46.29": "51.98"
        },
        {
          "50.64": "",
          "46.29": "53.94"
        },
        {
          "50.64": "50.07",
          "46.29": "53.45"
        },
        {
          "50.64": "21.13",
          "46.29": "12.14"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Building resources for emakhuwa: machine translation and news classification benchmarks",
      "authors": [
        "Felermino Ali",
        "Henrique Lopes Cardoso",
        "Rui Sousa"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Emotion and personality",
      "authors": [
        "Magda B Arnold"
      ],
      "year": "1960",
      "venue": "Emotion and personality"
    },
    {
      "citation_id": "3",
      "title": "The nchlt speech corpus of the south african languages. Workshop Spoken Language Technologies for Underresourced Languages",
      "authors": [
        "Etienne Barnard",
        "H Marelie",
        "Charl Davel",
        "Febe Van Heerden",
        "Jaco Wet",
        "Badenhorst"
      ],
      "year": "2014",
      "venue": "The nchlt speech corpus of the south african languages. Workshop Spoken Language Technologies for Underresourced Languages"
    },
    {
      "citation_id": "4",
      "title": "How Emotions are Made: The Secret Life of the Brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "Expert Thinking Series"
    },
    {
      "citation_id": "5",
      "title": "The theory of constructed emotion: an active inference account of interoception and categorization",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2016",
      "venue": "Social Cognitive and Affective Neuroscience",
      "doi": "10.1093/scan/nsw154"
    },
    {
      "citation_id": "6",
      "title": "FEEL-IT: Emotion and sentiment classification for the Italian language",
      "authors": [
        "Federico Bianchi",
        "Debora Nozza",
        "Dirk Hovy"
      ],
      "year": "2021",
      "venue": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "7",
      "title": "Xlm-emo: Multilingual emotion prediction in social media text",
      "authors": [
        "Federico Bianchi",
        "Debora Nozza",
        "Dirk Hovy"
      ],
      "year": "2022",
      "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis"
    },
    {
      "citation_id": "8",
      "title": "Automated building and analysis of ukrainian twitter corpus for toxic text detection",
      "authors": [
        "Kateryna Bobrovnyk"
      ],
      "year": "2019",
      "venue": "COLINS 2019"
    },
    {
      "citation_id": "9",
      "title": "SemEval-2019 task 3: EmoContext contextual emotion detection in text",
      "authors": [
        "Ankush Chatterjee",
        "Kedhar Nath Narahari",
        "Meghana Joshi",
        "Puneet Agrawal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S19-2005"
    },
    {
      "citation_id": "10",
      "title": "RED v2: Enhancing RED dataset for multi-label emotion detection",
      "authors": [
        "Alexandra Ciobotaru",
        "Mihai Vlad Constantinescu",
        "Liviu Dinu",
        "Stefan Dumitrescu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.747"
    },
    {
      "citation_id": "12",
      "title": "Free dolly: Introducing the world's first truly open instructiontuned llm",
      "authors": [
        "Mike Conover",
        "Matt Hayes",
        "Ankit Mathur",
        "Jianwei Xie",
        "Jun Wan",
        "Sam Shah",
        "Ali Ghodsi",
        "Patrick Wendell",
        "Matei Zaharia",
        "Reynold Xin"
      ],
      "year": "2023",
      "venue": "Free dolly: Introducing the world's first truly open instructiontuned llm"
    },
    {
      "citation_id": "13",
      "title": "others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Junxiao Song",
        "Ruoyu Zhang",
        "Runxin Xu",
        "Qihao Zhu",
        "Shirong Ma",
        "Peiyi Wang",
        "Xiao Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao"
      ],
      "venue": "others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "14",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.372"
    },
    {
      "citation_id": "15",
      "title": "Appraisal theory: old and new questions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "emot. rev"
    },
    {
      "citation_id": "16",
      "title": "The emotions. Studies in Emotion and Social Interaction",
      "authors": [
        "H Nico",
        "Frijda"
      ],
      "year": "1986",
      "venue": "The emotions. Studies in Emotion and Social Interaction"
    },
    {
      "citation_id": "17",
      "title": "Multilingual language models are not multicultural: A case study in emotion",
      "authors": [
        "Shreya Havaldar",
        "Bhumika Singhal",
        "Sunny Rai",
        "Langchen Liu",
        "Sharath Chandra Guntuku",
        "Lyle Ungar"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis",
      "doi": "10.18653/v1/2023.wassa-1.19"
    },
    {
      "citation_id": "18",
      "title": "Challenges and strategies in crosscultural NLP",
      "authors": [
        "Daniel Hershcovich",
        "Stella Frank",
        "Heather Lent",
        "Mostafa Miryam De Lhoneux",
        "Stephanie Abdou",
        "Emanuele Brandl",
        "Laura Bugliarello",
        "Ilias Piqueras",
        "Ruixiang Chalkidis",
        "Constanza Cui",
        "Katerina Fierro",
        "Phillip Margatina",
        "Anders Rust",
        "Søgaard"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.482"
    },
    {
      "citation_id": "19",
      "title": "Bemoc: A corpus for identifying emotion in bengali texts",
      "authors": [
        "Avishek Md Asif Iqbal",
        "Omar Das",
        "Mohammed Sharif",
        "Hoque",
        "Sarker"
      ],
      "year": "2022",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "20",
      "title": "Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 others",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Antoine Sablayrolles",
        "Arthur Roux",
        "Blanche Mensch",
        "Savary"
      ],
      "year": "2024",
      "venue": "Mixtral of experts",
      "arxiv": "arXiv:2401.04088"
    },
    {
      "citation_id": "21",
      "title": "Capturing reliable fine-grained sentiment associations by crowdsourcing and best-worst scaling",
      "authors": [
        "Svetlana Kiritchenko",
        "Saif Mohammad"
      ],
      "year": "2016",
      "venue": "Proceedings of The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)"
    },
    {
      "citation_id": "22",
      "title": "Languages of russia: Using social networks to collect texts",
      "authors": [
        "Irina Krylova",
        "Boris Orekhov",
        "Ekaterina Stepanova",
        "Lyudmila Zaydelman"
      ],
      "year": "2015",
      "venue": "Information Retrieval: 9th Russian Summer School"
    },
    {
      "citation_id": "23",
      "title": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer",
      "authors": [
        "Shivani Kumar",
        "Anubhav Shrimal",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2022",
      "venue": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer"
    },
    {
      "citation_id": "24",
      "title": "Emotion and adaptation",
      "authors": [
        "Lazarus Richard"
      ],
      "year": "1991",
      "venue": "Emotion and adaptation"
    },
    {
      "citation_id": "25",
      "title": "No culture left behind: ArtELingo-28, a benchmark of WikiArt with captions in 28 languages",
      "authors": [
        "Youssef Mohamed",
        "Runjia Li",
        "Ibrahim Ahmad",
        "Kilichbek Haydarov",
        "Philip Torr",
        "Kenneth Church",
        "Mohamed Elhoseiny"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.1165"
    },
    {
      "citation_id": "26",
      "title": "Best practices in the creation and use of emotion lexicons",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EACL 2023",
      "doi": "10.18653/v1/2023.findings-eacl.136"
    },
    {
      "citation_id": "27",
      "title": "2018a. Semeval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez",
        "Mohammad Salameh",
        "Svetlana Kiritchenko"
      ],
      "venue": "Proceedings of the 12th international workshop on semantic evaluation"
    },
    {
      "citation_id": "28",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1001"
    },
    {
      "citation_id": "29",
      "title": "Understanding emotions: A dataset of tweets to study interactions between affect categories",
      "authors": [
        "Saif Mohammad",
        "Svetlana Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "30",
      "title": "9 -sentiment analysis: Detecting valence, emotions, and other affectual states from text",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2016",
      "venue": "Emotion Measurement",
      "doi": "10.1016/B978-0-08-100508-8.00009-6"
    },
    {
      "citation_id": "31",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2022",
      "venue": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "arxiv": "arXiv:2109.08256"
    },
    {
      "citation_id": "32",
      "title": "WorryWords: Norms of anxiety association for over 44k English words",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.910"
    },
    {
      "citation_id": "33",
      "title": "Appraisal theories of emotion: State of the art and future development",
      "authors": [
        "Agnes Moors",
        "Phoebe Ellsworth",
        "Klaus Scherer",
        "Nico Frijda"
      ],
      "year": "2013",
      "venue": "Emotion review"
    },
    {
      "citation_id": "34",
      "title": "Tajuddeen Gwadabe, and 8 others. 2023a. AfriSenti: A Twitter sentiment analysis benchmark for African languages",
      "authors": [
        "Shamsuddeen Muhammad",
        "Idris Abdulmumin",
        "Abinew Ayele",
        "Nedjma Ousidhoum",
        "David Adelani",
        "Seid Yimam",
        "Ibrahim Ahmad",
        "Meriem Beloucif",
        "Saif Mohammad",
        "Sebastian Ruder",
        "Oumaima Hourrane",
        "Alipio Jorge",
        "Pavel Brazdil",
        "Felermino Ali",
        "Davis David",
        "Salomey Osei",
        "Bello Shehu-Bello",
        "Falalu Lawan"
      ],
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.862"
    },
    {
      "citation_id": "35",
      "title": "2023b. SemEval-2023 task 12: Sentiment analysis for african languages (AfriSenti-SemEval)",
      "authors": [
        "Shamsuddeen Hassan"
      ],
      "venue": "Proceedings of the 17th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "36",
      "title": "Abinew Ali Ayele, Oana Ignat, Alexander Panchenko, and 2 others. 2025. Semeval-2025 task 11: Bridging the gap in text-based emotion detection",
      "authors": [
        "Shamsuddeen Hassan",
        "Nedjma Ousidhoum",
        "Idris Abdulmumin",
        "Muhie Seid",
        "Jan Yimam",
        "Terry Philip Wahle",
        "Meriem Ruas",
        "Christine Beloucif",
        "Daniela De Kock ; Nirmal Surange",
        "David Teodorescu",
        "Alham Ifeoluwa Adelani",
        "Felermino Fikri Aji",
        "Vladimir Ali",
        "Araujo"
      ],
      "venue": "Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025)"
    },
    {
      "citation_id": "37",
      "title": "Xed: A multilingual dataset for sentiment analysis and emotion detection",
      "authors": [
        "Emily Öhman",
        "Marc Pàmies",
        "Kaisla Kajava",
        "Jörg Tiedemann"
      ],
      "year": "2020",
      "venue": "Xed: A multilingual dataset for sentiment analysis and emotion detection",
      "arxiv": "arXiv:2011.01612"
    },
    {
      "citation_id": "38",
      "title": "The cognitive structure of emotions",
      "authors": [
        "Andrew Ortony",
        "Gerald Clore",
        "Allan Collins"
      ],
      "year": "2022",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "39",
      "title": "Modeling reportable events as turning points in narrative",
      "authors": [
        "Jessica Ouyang",
        "Kathleen Mckeown"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Potato: The portable text annotation tool",
      "authors": [
        "Jiaxin Pei",
        "Aparna Ananthasubramaniam",
        "Xingyao Wang",
        "Naitian Zhou",
        "Apostolos Dedeloudis",
        "Jackson Sargent",
        "David Jurgens"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "41",
      "title": "The \"problem\" of human label variation: On ground truth in data, modeling and evaluation",
      "authors": [
        "Barbara Plank"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "42",
      "title": "Emotion analysis in NLP: Trends, gaps and roadmap for future directions",
      "authors": [
        "Flor Miriam",
        "Plaza-Del Arco",
        "Alba Curry",
        "Amanda Curry",
        "Dirk Hovy"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "43",
      "title": "Chapter 1 -a general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of Emotion",
      "doi": "10.1016/B978-0-12-558701-3.50007-7"
    },
    {
      "citation_id": "44",
      "title": "Appraisal in the emotion system: Coherence in strategies for coping",
      "authors": [
        "Ira Roseman"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "45",
      "title": "Textual emotion detection in health: Advances and applications",
      "authors": [
        "Alieh Hajizadeh Saffar",
        "Tiffany Mann",
        "Bahadorreza Ofoghi"
      ],
      "year": "2023",
      "venue": "Journal of Biomedical Informatics"
    },
    {
      "citation_id": "46",
      "title": "Emotion classification on indonesian twitter dataset",
      "authors": [
        "Mei Silviana Saputri",
        "Rahmad Mahendra",
        "Mirna Adriani"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Asian Language Processing (IALP)",
      "doi": "10.1109/IALP.2018.8629262"
    },
    {
      "citation_id": "47",
      "title": "The dynamic architecture of emotion: Evidence for the component process model",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2009",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "48",
      "title": "Emotion detection in text: a review",
      "authors": [
        "Armin Seyeditabari",
        "Narges Tabari",
        "Wlodek Zadrozny"
      ],
      "year": "2018",
      "venue": "Emotion detection in text: a review",
      "arxiv": "arXiv:1806.00674"
    },
    {
      "citation_id": "49",
      "title": "Svensk absabank",
      "authors": [
        "Språkbanken Text"
      ],
      "year": "2024",
      "venue": "Svensk absabank",
      "doi": "10.23695/2B74-0515"
    },
    {
      "citation_id": "50",
      "title": "SemEval-2007 task 14: Affective text",
      "authors": [
        "Carlo Strapparava",
        "Rada Mihalcea"
      ],
      "year": "2007",
      "venue": "Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)"
    },
    {
      "citation_id": "51",
      "title": "Label Studio: Data labeling software",
      "authors": [
        "Maxim Tkachenko",
        "Mikhail Malyuk",
        "Andrey Holmanyuk",
        "Nikolai Liubimov"
      ],
      "year": "2020",
      "venue": "Open source software available from"
    },
    {
      "citation_id": "52",
      "title": "Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "venue": "Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "53",
      "title": "Emotion granularity from text: An aggregate-level indicator of mental health",
      "authors": [
        "Krishnapriya Vishnubhotla",
        "Daniela Teodorescu",
        "Mallory Feldman",
        "Kristen Lindquist",
        "Saif Mohammad"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.1069"
    },
    {
      "citation_id": "54",
      "title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation",
      "authors": [
        "Janyce Wiebe",
        "Theresa Wilson",
        "Claire Cardie"
      ],
      "year": "2005",
      "venue": "Annotating expressions of opinions and emotions in language. Language resources and evaluation"
    },
    {
      "citation_id": "55",
      "title": "Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang"
      ],
      "venue": "Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report",
      "arxiv": "arXiv:2412.15115"
    },
    {
      "citation_id": "56",
      "title": "My heart skipped a beat! recognizing expressions of embodied emotion in natural language",
      "authors": [
        "Yuan Zhuang",
        "Tianyu Jiang",
        "Ellen Riloff"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "57",
      "title": "Table 4: Percentage distribution of SingleLabel, MultiLabel, and NeutralLabel for the Train, Development, and Test Sets",
      "venue": "Table 4: Percentage distribution of SingleLabel, MultiLabel, and NeutralLabel for the Train, Development, and Test Sets"
    }
  ]
}