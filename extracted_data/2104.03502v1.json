{
  "paper_id": "2104.03502v1",
  "title": "Emotion Recognition From Speech Using Wav2Vec 2.0 Embeddings",
  "published": "2021-04-08T04:31:58Z",
  "authors": [
    "Leonardo Pepino",
    "Pablo Riera",
    "Luciana Ferrer"
  ],
  "keywords": [
    "speech emotion recognition",
    "transfer learning",
    "wav2vec 2.0"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition datasets are relatively small, making the use of the more sophisticated deep learning approaches challenging. In this work, we propose a transfer learning method for speech emotion recognition where features extracted from pre-trained wav2vec 2.0 models are modeled using simple neural networks. We propose to combine the output of several layers from the pre-trained model using trainable weights which are learned jointly with the downstream model. Further, we compare performance using two different wav2vec 2.0 models, with and without finetuning for speech recognition. We evaluate our proposed approaches on two standard emotion databases IEMOCAP and RAVDESS, showing superior performance compared to results in the literature.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, progressively more people are interacting with virtual voice assistants, such as Siri, Alexa, Cortana and Google Assistant. Interfaces that ignore a user's emotional state or fail to manifest the appropriate emotion can dramatically impede performance and risks being perceived as cold, socially inept, untrustworthy, and incompetent  [1] . Because of this, speech emotion recognition is becoming an increasingly relevant task.\n\nSeveral datasets have been created over the years for training and evaluating emotion recognition models, including SAVEE  [2] , RAVDESS  [3] , EMODB  [4] , IEMOCAP  [5] , and MSP-Podcast  [6] . With the exception of MSP-Podcast, these datasets are relatively small in size, usually including only a few dozen speakers. In terms of modeling techniques, many different traditional approaches have been proposed, using hidden Markov models (HMMs)  [7, 8] , support vector machines (SVMs)  [9, 10] , decision trees  [11, 12] , and, more recently, deep neural networks (DNNs)  [13, 14, 15 ]. Yet, while DNNs have shown large gains over traditional approaches on tasks like automatic speech recognition (ASR)  [16]  and speaker identification  [17] , the gains observed on emotion recognition are limited, likely due to the small size of the datasets.\n\nA common strategy when dealing with small training datasets is to apply transfer learning techniques. One approach for transfer learning is to use a model learned for a certain auxiliary task for which large datasets are available for training to improve robustness for the task of interest for which data is scarce. The model learned on the auxiliary task can be used as feature extractor or fine-tuned, after replacing some of its final layers, to the task of interest. Recently, transfer learning approaches have been explored in the field of speech emotion recognition. In  [18] , a deep neural network based on transformers  [19]  is pretrained on LibriSpeech  [20]  using multiple self-supervised objectives at different time scales. Then, the model is used as a feature extractor or fine-tuned for speech emotion recognition, among other downstream tasks. Similarly, in  [21]  and  [22] , a deep encoder is pretrained in a contrastive predictive coding task, and the resulting embeddings are tested in speech emotion datasets. Recently, several models for automatic speech recognition (ASR) which use self-supervised pretraining have been released, including wav2vec  [23]  and VQ-wav2vec  [24] . A few recent studies  [25, 26, 27]  have successfully applied representations from these models as features for emotion recognition.\n\nIn line with those works, in this paper, we explore the use of the wav2vec 2.0 model  [28] , an improved version of the original wav2vec model, as a feature extractor for speech emotion recognition. The main contributions of our paper are (1) the use of wav2vec 2.0 representations for speech emotion recognition which, to our knowledge, had never been done for this task, (2) a novel approach for the downstream model which leverages information from multiple layers of the wav2vec 2.0 model and leads to significant improvements over previous approaches, and (3) an analysis of the importance of the different layers in wav2vec 2.0 for the emotion recognition task. Our results are superior to others in the literature for models based only on acoustic information for IEMOCAP and RAVDESS. The code to replicate the results of this paper will soon be released at https://github.com/habla-liaa/ser-with-w2v2.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "In our study, we extracted features from two released wav2vec 2.0 models and used them for speech emotion recognition. In this section, we describe the wav2vec 2.0 model, the datasets used for training and evaluation, and the downstream models.\n\n2.1. Wav2vec 2.0 model architecture Wav2vec 2.0  [28]  is a framework for self-supervised learning of representations from raw audio. The model consists of three stages. The first stage is a local encoder, which contains several convolutional blocks and encodes the raw audio into a sequence of embeddings with a stride of 20 ms and a receptive field of 25 ms. Two models have been released for public use, a large one and a base one where the embeddings are 1024-and 768-dimensional, respectively. The second stage is a contextualized encoder, which takes the local encoder representations as input. Its architecture consists of several transformer encoder blocks  [19] . The base model uses 12 transformer blocks with 8 attention heads each, while the large model uses 24 transformer blocks with 16 attention heads each. Finally, a quantization module, takes the local encoder representations as input and consists of 2 codebooks with 320 entries each. A linear map is used to turn the local encoder representations into logits. Given the logits, Gumbel-Softmax  [29]  is applied to sample from each codebook. The selected codes are concatenated and a linear transformation is applied to the resulting vector leading to a quantized representation of the local encoder output, which is used in the objective function, as explained below. arXiv:2104.03502v1 [cs.SD] 8 Apr 2021 2.2. Wav2Vec 2.0 pretraining and finetuning The wav2vec 2.0 model is pretrained in a self-supervised setting, similar to the masked language modelling used in BERT  [30]  for NLP. Contiguous time steps from the local encoder representations are randomly masked and the model is trained to reproduce the quantized local encoder representations for those masked frames at the output of the contextualized encoder.\n\nThe training objective is composed by terms of the form\n\nwhere sim(ct, qt) is the cosine distance between the contextualized encoder outputs ct and the quantized local encoder representations qt. t is the time step, κ is the temperature and Q is the union of a set of K distractors and qt. The distractors are outputs of the local encoder sampled from masked frames belonging to the same utterance as qt. The contrastive loss is then given by Lm summed over all masked frames. Finally, terms to encourage diversity of the codebooks and L2 regularization are added to the contrastive loss.\n\nThe main goal of the wav2vec 2.0 paper was to use the learned representations to improve ASR performance, requiring less data for training and enabling its use for low resource languages. To this end, the model trained as described above is finetuned for ASR using a labelled speech corpus like Lib-riSpeech. A randomly initialized linear projection is added at the output of the contextual encoder and the connectionist temporal classification (CTC) loss  [31]  is minimized. The models finetuned in 960 hours of LibriSpeech reach state of the art results in automatic speech recognition when evaluated in different subsets of LibriSpeech. Even when finetuning using considerably less hours, wav2vec 2.0 models reach a performance comparable to the state of the art.\n\nIn this paper we compared the performance in speech emotion recognition when using both the wav2vec 2.0 base model pretrained in Librispeech without finetuning 1 (we will call this model Wav2vec2-PT), and a model finetuned for ASR using a 960-hour subset of Librispeech 2 (Wav2vec2-FT). In both cases, we used the base model as we did not see significant performance improvements when using the large one, and it allowed us to reduce computational requirements.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Features",
      "text": "We compare results when using the output of the local and the contextualized encoders as input to the downstream models. We also propose to use a weighted average of the outputs of the 12 transformer blocks, along with those of the local encoder. The weights of this average are trained along with the rest of the downstream model, as explained in the next subsection.\n\nAs baseline features, we also calculated magnitude spectrograms with a hanning window of 25 ms and a hop size of 10 ms, and eGeMAPS  [32]  low level descriptors (LLD), which are commonly used in the emotion recognition literature. The eGeMAPS features were extracted using opensmilepython  [33] . In order to match the sequence lengths of the eGeMAPS features, which use a stride of 10 ms, with the lengths of the wav2vec 2.0 features, which have a stride of 20 ms, we downsampled the eGeMAPS LLDs by averaging every 2 consecutive frames.\n\nAll features were normalized by subtracting the mean and dividing by the standard deviation of that feature over all the 1 https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec small.pt 2 https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec small 960h.pt",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Downstream Models",
      "text": "Our downstream model is inspired by  [27]  due to its simplicity, which should reduce the chance of overfitting. The model we will refer to as Dense, consists of 2 dense layers with 128 neurons, ReLU activation and dropout with a probability of 0.2, applied independently to each frame of the input features sequence f . This is equivalent to using 1D pointwise convolutional layers (with a kernel size of 1) and 128 filters. The outputs are averaged over time resulting in a vector of size 128. Finally, another dense layer with softmax activation returns probabilities for each of the emotion classes. During the training of the downstream model, the weights of the wav2vec 2.0 model remain unaltered, so it serves as a feature extractor. For computational reasons, we used a maximum sequence length of 400 for IEMOCAP and 250 for RAVDESS, as input to the network. This is equivalent to 8 seconds and 5 seconds, respectively. Note that the output of the contextualized encoder can contain information from the full input waveform which might be longer than the sequence seen by the network. This is because each output of the contextual encoder has a receptive field given by the whole input waveform, since this encoder is a transformer. On the other hand, the local encoder has a limited receptive field, so its outputs can only capture local information.\n\nFor the case in which we take as features the activations from both the wav2vec 2.0 local encoder and the transformer blocks, we incorporate a trainable weighted average as the first layer. This layer learns the weights αi for each of the wav2vec 2.0 layer activations, fi, where f0 corresponds to the local encoder output, f1 through f11 are the outputs of the internal blocks in the transformer and f12 is the output of the contex-tualized encoder. Then, the activations are combined as follows\n\nThe weights αi are initialized with 1.0. Note that the layer activations can be combined in this way because they are all the same size. This way of extracting features from a pretrained model is similar to the approach used in ELMO  [34]  for NLP.\n\nIn the results section, we compare performance of the Dense model with the LSTM model, in which the second dense layer is replaced by an LSTM layer. Finally, we also evaluate a third model, called Fusion model, which incorporates a branch taking as input eGeMAPS features. In this last model, the outputs of the first dense layer are concatenated before applying the second dense layer. The described downstream models can be seen in Figure  1 .\n\nThe downstream models were trained using batches of 32 utterances, Adam optimizer with a learning rate of 0.001, and early stopping with a patience of 4 epochs monitoring the validation loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [3]  is a multi-modal database of emotional speech and song. It features 24 different actors (12 males and 12 females) enacting 2 statements: \"Kids are talking by the door\" and \"Dogs are sitting by the door.\" with 8 different emotions: happy, sad, angry, fearful, surprise, disgust, calm, and neutral. These emotions are expressed in two different intensities: normal and strong, except for neutral (normal only). Each of the combinations was spoken and sung, and repeated 2 times, leading to 104 unique vocalizations per actor. Following  [35] , we merged the neutral and calm emotions, resulting in 7 emotions, and used the first 20 actors for training, actors 20-22 for validation to do early stopping, and actors 22-24 for test.\n\nThe Interactive Emotional Dyadic Motion Capture (IEMO-CAP) dataset  [5]  has a length of approximately 12 hours and consists of scripted and improvised dialogues by 10 speakers. It is composed of 5 sessions, each including speech from an actor and an actress. Annotators were asked to label each sample choosing one or more labels from a pool of emotions. In this work, we used 4 emotional classes: anger, happiness, sadness and neutral, and following the work in  [36] , we relabeled excitement samples as happiness. Instances from other classes and with no majority label across the annotations were discarded.  3  We also trimmed the waveforms to 15 seconds to reduce the computational requirements when extracting wav2vec 2.0 features. Only 2% of the waveforms were longer than 15 seconds. To evaluate the models in IEMOCAP, we performed 5-fold cross-validation, leaving one session out for each of the folds, as it is a standard practice with this dataset. We used one of the 4 training sessions to perform early stopping for each cross-validation model.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "We calculated the performance of our models by training them 5 times with different seeds. Table  1  shows the average recall over all emotion classes obtained using the different features extracted from wav2vec 2.0 models. We also show two systems based on eGeMAPS and spectrogram features, which can be considered as baselines. In all cases, features are normalized by speaker and the Dense model architecture in Figure  1  is used as downstream model. We can see that features for both wav2vec 2.0 models, the one finetuned in 960 hours of Librispeech (wav2vec2-FT) and the one that is not finetuned (wav2vec2-PT), the local encoder representations lead to better results than both of the baseline features. It is worth noting that eGeMAPS, spectrograms and the wav2vec 2.0 local encoder representations contain information restricted only to a local window around each frame, of 60 ms for eGeMAPS, and 25 ms for the others. Also, the downstream model combines information from consecutive frames using just global average, which is a very simple approach that might be suboptimal because it cannot take into account temporal patterns in the features. In spite of that, the local encoder representations, particularly the ones obtained from the PT model, reach a performance comparable to much more complex models like the one proposed in  [14] , which is a CNN with Bi-LSTM layers trained in a fully supervised setting with spectrograms as input.\n\nWe can also see that the wav2vec2-PT features perform better than wav2vec2-FT features in all cases. In particular, the model using only the contextualized encoder outputs for the wav2vec2-FT model has the worst results in the table. We hypothesize that this is because when the model is finetuned for an ASR task, information that is not relevant for that task but might be relevant for speech emotion recognition is lost from the embeddings. For example, information about the pitch might not be important for speech recognition, while it is essential for speech emotion recognition.\n\nFurther, Table  1  shows that using a weighted average of the transformer blocks outputs along with the local encoder outputs (rows labelled \"All layers\") results in better performance than using only the local or the contextual encoder outputs. Using only the local encoder representations might not give information about events occurring at time scales larger than the receptive field (25 ms). Further, the output of the contextual encoder might be similar to the quantized local encoder representations, since the contrastive loss objective is designed to achieve this goal. Hence, using both of these layers, along with all intermediate layers in the transformer provides additional information that is valuable to the model.\n\nFigure  2  shows the weights αi from the weighted average layer once the downstream models are trained. We can see that the middle layers are given larger weights. This could be because these layers have already contextualized enough informa- tion from the local encoder, but they are not yet too specific to the pretraining or finetuning task as the last layers. Also, in the case of the feature extracted from the wav2vec2-PT model, the weights tend to be larger in the layers closer to the output when compared with the weights for the features from the wav2vec2-FT model. This again suggests that the layers of the wav2vec2-FT model closer to the output are less useful for emotion recognition than those of the wav2vec2-PT model. Finally, note that in both datasets, the different training seeds lead to very similar weights, as observed from the error bars in Figure  2 , from which we can conclude that these weights are not too sensitive to the neural network initialization. Finally, we experimented with several variations of the best performing model in Table  1 . The first line in Table  2  shows the results for that system, corresponding to the fifth line in Table 1. The rest of the lines in Table  2  show results for that model and the others in Figure  1  applying global normalization instead of speaker-dependent normalization. This is the scenario that is most commonly used in papers as is the one with less assumptions about the available data, treating all samples from a speaker as independent from each other, not assuming that additional samples from a speaker are available at test time.\n\nComparing the first and the second line in Table  2 , we can see that results significantly degrade when doing global normalization. This is expected since normalization by speaker helps the model focus on the emotion characteristics by eliminating part of the speaker information. The degradation is larger in RAVDESS probably because, in this dataset, audios from different emotions do not have much variation in lexical content. Hence, by reducing the effect of speaker in the features all that is left is the variation due to the emotions. On the other hand, in IEMOCAP data, the variation due to lexical content is still in the samples after speaker normalization.\n\nThe third and fourth lines in Table  2  show the results obtained using the LSTM model, and the Fusion model. The latter fuses eGeMAPS with the wav2vec 2.0 features. We can see that using an LSTM layer before the global pooling, does not seem to bring improvements over using a simple dense layer. This might be because wav2vec 2.0 features are already contextualized and have global information about the full utterance. Also, LSTMs might be more prone to overfitting or optimization problems than a simpler dense layer. Finally, the table shows some modest improvements from the addition of eGeMAPS features, suggesting that wav2vec 2.0 features may be lacking some of the information present in eGeMAPS. BiLSTM w. attention  [13]  58.8 -CNN-BiLSTM  [14]  59.4 -TDNN-LSTM w. attention  [15]  60.7 -Wav2Vec  [27]  64.3 -Table  2  also shows some of the results obtained in other works on the IEMOCAP dataset, using the same experimental setup we are using. For a fair comparison with our work, we restrict the comparison to models in the literature that do not use automatic or manual transcriptions. We can see that all of our models perform better than the state of the art.\n\nFinally, we also compare our results with those in  [38] , which consists of a deep convolutional neural network using acoustic features as input. For this comparison, we used the dense downstream model with global normalization and imitated their experimental setup both for IEMOCAP and RAVDESS. For IEMOCAP, they only use the improvised sessions and full agreement utterances. For RAVDESS, they use 5 fold cross-validation, dividing the data randomly. Moreover, they do not merge calm and neutral emotions, so the total number of emotions to be predicted is 8. We outperformed the models in  [38]  for both datasets obtaining an average recall of 84.1 ± 1.2 % in RAVDESS, and 72.1 ± 0.9% in IEMOCAP, compared to the results in the paper which are 64.3% and 71.6%, respectively.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we explored different ways of extracting and modeling features from pretrained wav2vec 2.0 models for speech emotion recognition. We proposed to combine the different layers in the wav2vec 2.0 model using trainable weights and model the resulting features with a simple DNN with a time-wise pooling layer. We evaluated our models on two standard emotion datasets, IEMOCAP and RAVDESS, and showed superior results on both cases, compared to those in recent literature. We found that the combination of information from different layers in the wav2vec 2.0 model led to improved results over using only the encoder outputs, as in previous works. Further, we found that the combination of the wav2vec 2.0 features with a set of prosodic features gave additional gains, suggesting that the wav2vec 2.0 model does not contain all the prosodic information needed for emotion recognition. Finally, we showed that a wav2vec 2.0 model finetuned for the task of ASR worked worse than the one trained only with the self-supervised task, indicating that the finetuning eliminates information from the embeddings that is useful for emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Downstream models used for this work. The green",
      "page": 2
    },
    {
      "caption": "Figure 1: The downstream models were trained using batches of 32",
      "page": 3
    },
    {
      "caption": "Figure 1: is used as downstream model.",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the weights αi from the weighted average",
      "page": 3
    },
    {
      "caption": "Figure 2: Weights of the trainable weighted average layer of",
      "page": 4
    },
    {
      "caption": "Figure 1: applying global normalization",
      "page": 4
    },
    {
      "caption": "Figure 1: using the wav2vec2-PT features from all layers, with two types",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "lpepino@dc.uba.ar,\npriera@dc.uba.ar,lferrer@dc.uba.ar"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "among other downstream tasks. Similarly,\nin [21] and [22], a\nAbstract"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "deep encoder\nis pretrained in a contrastive predictive coding"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "Emotion recognition datasets are relatively small, making the"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "task, and the resulting embeddings are tested in speech emotion"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "use of\nthe more sophisticated deep learning approaches chal-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "datasets. Recently, several models for automatic speech recog-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "lenging.\nIn this work, we propose a transfer\nlearning method"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "nition (ASR) which use self-supervised pretraining have been"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "for speech emotion recognition where features extracted from"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "released, including wav2vec [23] and VQ-wav2vec [24]. A few"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "pre-trained wav2vec 2.0 models are modeled using simple neu-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "recent studies [25, 26, 27] have successfully applied represen-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "ral networks. We propose to combine the output of several lay-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "tations from these models as features for emotion recognition."
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "ers from the pre-trained model using trainable weights which"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "In line with those works,\nin this paper, we explore the use"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "are learned jointly with the downstream model.\nFurther, we"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "of the wav2vec 2.0 model [28], an improved version of the orig-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "compare performance using two different wav2vec 2.0 mod-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "inal wav2vec model, as a feature extractor for speech emotion"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "els, with and without ﬁnetuning for\nspeech recognition. We"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "recognition.\nThe main contributions of our paper are (1)\nthe"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "evaluate our proposed approaches on two standard emotion"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "use of wav2vec 2.0 representations for speech emotion recog-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "databases IEMOCAP and RAVDESS, showing superior perfor-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "nition which,\nto our knowledge, had never been done for\nthis"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "mance compared to results in the literature."
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "task,\n(2) a novel approach for\nthe downstream model which"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "Index Terms:\nspeech emotion recognition,\ntransfer\nlearning,"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "leverages information from multiple layers of the wav2vec 2.0"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "wav2vec 2.0"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "model and leads to signiﬁcant improvements over previous ap-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "proaches, and (3) an analysis of the importance of the different"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "1.\nIntroduction"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "layers in wav2vec 2.0 for\nthe emotion recognition task. Our"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "results are superior to others in the literature for models based\nIn recent years, progressively more people are interacting with"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "only on acoustic information for\nIEMOCAP and RAVDESS.\nvirtual voice assistants, such as Siri, Alexa, Cortana and Google"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "The code to replicate the results of this paper will soon be re-\nAssistant.\nInterfaces that\nignore a user’s emotional state or fail"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "leased at https://github.com/habla-liaa/ser-with-w2v2.\nto manifest\nthe appropriate emotion can dramatically impede"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "performance and risks being perceived as cold, socially inept,"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "untrustworthy, and incompetent\n[1].\nBecause of\nthis,\nspeech"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "2. Methods"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "emotion recognition is becoming an increasingly relevant task."
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "In our study, we extracted features from two released wav2vec\nSeveral\ndatasets\nhave\nbeen\ncreated\nover\nthe\nyears\nfor"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "2.0 models and used them for speech emotion recognition.\nIn\ntraining and evaluating emotion recognition models,\nincluding"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "this section, we describe the wav2vec 2.0 model,\nthe datasets\nSAVEE [2], RAVDESS [3], EMODB [4], IEMOCAP [5], and"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "used for training and evaluation, and the downstream models.\nMSP-Podcast\n[6]. With the exception of MSP-Podcast,\nthese"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "datasets are relatively small\nin size, usually including only a"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "2.1. Wav2vec 2.0 model architecture"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "few dozen speakers.\nIn terms of modeling techniques, many"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "Wav2vec 2.0 [28]\nis a framework for self-supervised learning\ndifferent traditional approaches have been proposed, using hid-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "of representations from raw audio. The model consists of three\nden Markov models (HMMs)\n[7, 8], support vector machines"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "stages. The ﬁrst stage is a local encoder, which contains sev-\n(SVMs)\n[9, 10], decision trees\n[11, 12],\nand, more recently,"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "eral convolutional blocks and encodes the raw audio into a se-\ndeep neural networks (DNNs)\n[13, 14, 15]. Yet, while DNNs"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "quence of embeddings with a stride of 20 ms and a receptive\nhave shown large gains over traditional approaches on tasks like"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "ﬁeld of 25 ms. Two models have been released for public use,\nautomatic speech recognition (ASR) [16] and speaker identiﬁ-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "a large one and a base one where the embeddings are 1024- and\ncation [17], the gains observed on emotion recognition are lim-"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "768-dimensional, respectively. The second stage is a contextu-\nited, likely due to the small size of the datasets."
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "alized encoder, which takes the local encoder representations\nA common\nstrategy when\ndealing with\nsmall\ntraining"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "datasets is to apply transfer learning techniques. One approach\nas input. Its architecture consists of several transformer encoder"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "for transfer learning is to use a model learned for a certain auxil-\nblocks [19]. The base model uses 12 transformer blocks with 8"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "iary task for which large datasets are available for training to im-\nattention heads each, while the large model uses 24 transformer"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "prove robustness for the task of interest for which data is scarce.\nblocks with 16 attention heads each.\nFinally, a quantization"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "module,\ntakes the local encoder\nrepresentations as input and\nThe model\nlearned on the auxiliary task can be used as feature"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "consists of 2 codebooks with 320 entries each. A linear map is\nextractor or ﬁne-tuned, after replacing some of its ﬁnal\nlayers,"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "used to turn the local encoder representations into logits. Given\nto the task of\ninterest. Recently,\ntransfer\nlearning approaches"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "the logits, Gumbel-Softmax [29] is applied to sample from each\nhave been explored in the ﬁeld of speech emotion recognition."
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "codebook.\nThe selected codes are concatenated and a linear\nIn [18], a deep neural network based on transformers [19]\nis"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "transformation is applied to the resulting vector\nleading to a\npretrained on LibriSpeech [20] using multiple self-supervised"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "quantized representation of the local encoder output, which is\nobjectives at different time scales. Then, the model is used as a"
        },
        {
          "2Departamento de Computaci´on, FCEyN, Universidad de Buenos Aires (UBA), Argentina": "used in the objective function, as explained below.\nfeature extractor or ﬁne-tuned for speech emotion recognition,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "eGeMAPS"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "The wav2vec 2.0 model\nis pretrained in a self-supervised set-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "...\n...\n..."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "ting, similar to the masked language modelling used in BERT",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "[30] for NLP. Contiguous time steps from the local encoder rep-",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Weighted Average\nWeighted Average\nPointwise Conv1D"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "resentations are randomly masked and the model\nis trained to",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "...\n...\n..."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "reproduce the quantized local encoder representations for those",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "masked frames at the output of the contextualized encoder.",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Pointwise Conv1D\nPointwise Conv1D"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "The training objective is composed by terms of the form",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Pointwise Conv1D\n...\n/LSTM"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "exp(sim(ct, qt)/κ)",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "(1)\nLm = − log",
          "b)\na)\nW2V2 Features\nW2V2 Features": "...\n..."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "(cid:80)",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "q∈ ˜Q exp(sim(ct, ˜q)/κ)",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "...\nGlobal Average"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "where sim(ct, qt) is the cosine distance between the contextu-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Pointwise Conv1D"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "alized encoder outputs ct and the quantized local encoder rep-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "resentations qt. t is the time step, κ is the temperature and ˜Q is",
          "b)\na)\nW2V2 Features\nW2V2 Features": "..."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Output Layer"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "the union of a set of K distractors and qt. The distractors are",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "outputs of the local encoder sampled from masked frames be-",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Global Average"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "longing to the same utterance as qt. The contrastive loss is then",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "given by Lm summed over all masked frames. Finally, terms to",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "encourage diversity of the codebooks and L2 regularization are",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Output Layer"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "added to the contrastive loss.",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "The green\nFigure 1: Downstream models used for this work."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "The main goal of\nthe wav2vec 2.0 paper was\nto use the",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "blocks represent trainable layers. We call the model in ﬁgure a)"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "learned representations to improve ASR performance,\nrequir-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Dense, when using a pointwise convolutional layer, and LSTM,"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "ing less data for training and enabling its use for low resource",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "when using an LSTM layer. We call the model in b) the Fusion"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "languages. To this end,\nthe model\ntrained as described above",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "model. Blocks for each time-step have a height dimension for"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "is ﬁnetuned for ASR using a labelled speech corpus like Lib-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "the features, and an optional depth dimension for the layers in"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "riSpeech. A randomly initialized linear projection is added at",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "the wav2vec 2.0 model."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "the output of the contextual encoder and the connectionist tem-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "poral classiﬁcation (CTC) loss [31] is minimized. The models",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "data for\nthe corresponding speaker. When disabling speaker"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "ﬁnetuned in 960 hours of LibriSpeech reach state of the art re-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "normalization for comparison of\nresults, we replaced it with"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "sults in automatic speech recognition when evaluated in differ-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "global normalization per feature.\nIn this case,\nthe statistics are"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "ent subsets of LibriSpeech. Even when ﬁnetuning using con-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "computed over the training data only, excluding the test data."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "siderably less hours, wav2vec 2.0 models reach a performance",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "comparable to the state of the art.",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "In this paper we compared the performance in speech emo-",
          "b)\na)\nW2V2 Features\nW2V2 Features": "2.4. Downstream models"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "tion recognition when using both the wav2vec 2.0 base model",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "Our downstream model\nis inspired by [27] due to its simplic-"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "pretrained in Librispeech without ﬁnetuning1\n(we will call\nthis",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "ity, which should reduce the chance of overﬁtting. The model"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "model Wav2vec2-PT), and a model ﬁnetuned for ASR using a",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "we will refer to as Dense, consists of 2 dense layers with 128"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "960-hour subset of Librispeech2 (Wav2vec2-FT). In both cases,",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "neurons, ReLU activation and dropout with a probability of 0.2,"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "we used the base model as we did not see signiﬁcant perfor-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "applied independently to each frame of\nthe input\nfeatures se-"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "mance improvements when using the large one, and it allowed",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "quence f .\nThis is equivalent\nto using 1D pointwise convolu-"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "us to reduce computational requirements.",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "tional layers (with a kernel size of 1) and 128 ﬁlters. The outputs"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "are averaged over time resulting in a vector of size 128. Finally,"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "2.3. Features",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "another dense layer with softmax activation returns probabili-"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "We compare results when using the output of the local and the",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "ties for each of the emotion classes. During the training of the"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "contextualized encoders as input to the downstream models. We",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "downstream model,\nthe weights of the wav2vec 2.0 model re-"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "also propose to use a weighted average of the outputs of the 12",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "main unaltered, so it serves as a feature extractor."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "transformer blocks, along with those of the local encoder. The",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "For computational reasons, we used a maximum sequence"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "weights of\nthis average are trained along with the rest of\nthe",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "length of 400 for IEMOCAP and 250 for RAVDESS, as input"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "downstream model, as explained in the next subsection.",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "to the network. This is equivalent\nto 8 seconds and 5 seconds,"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "As baseline features, we also calculated magnitude spec-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "respectively. Note that the output of the contextualized encoder"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "trograms with a hanning window of 25 ms\nand a hop size",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "can contain information from the full\ninput waveform which"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "of 10 ms,\nand eGeMAPS [32]\nlow level descriptors\n(LLD),",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "might be longer than the sequence seen by the network. This is"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "which are commonly used in the emotion recognition litera-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "because each output of the contextual encoder has a receptive"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "ture. The eGeMAPS features were extracted using opensmile-",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "ﬁeld given by the whole input waveform, since this encoder is a"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "python [33].\nIn order\nto match the sequence lengths of\nthe",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "transformer. On the other hand, the local encoder has a limited"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "eGeMAPS features, which use\na\nstride of 10 ms, with the",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "receptive ﬁeld, so its outputs can only capture local information."
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "lengths of the wav2vec 2.0 features, which have a stride of 20",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "For\nthe case in which we take as features the activations"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "ms, we downsampled the eGeMAPS LLDs by averaging every",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "from both the wav2vec 2.0 local encoder and the transformer"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "2 consecutive frames.",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "blocks, we incorporate a trainable weighted average as the ﬁrst"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "All features were normalized by subtracting the mean and",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "layer. This layer learns the weights αi for each of the wav2vec"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "dividing by the standard deviation of\nthat\nfeature over all\nthe",
          "b)\na)\nW2V2 Features\nW2V2 Features": ""
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "",
          "b)\na)\nW2V2 Features\nW2V2 Features": "2.0 layer activations, fi, where f0 corresponds to the local en-"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "1https://dl.fbaipublicﬁles.com/fairseq/wav2vec/wav2vec small.pt",
          "b)\na)\nW2V2 Features\nW2V2 Features": "are the outputs of\nthe internal\ncoder output, f1\nthrough f11"
        },
        {
          "2.2. Wav2Vec 2.0 pretraining and ﬁnetuning": "2https://dl.fbaipublicﬁles.com/fairseq/wav2vec/wav2vec small 960h.pt",
          "b)\na)\nW2V2 Features\nW2V2 Features": "the contex-\nblocks in the transformer and f12 is the output of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "and models explored in IEMOCAP and RAVDESS. All results"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "are obtained with the Dense downstream model."
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Pretrained\nFeatures\nDataset"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "model\nIEMOCAP\nRAVDESS"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "None\neGeMAPS\n52.4 ± 0.1\n57.0 ± 2.4"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Spectrogram\n49.8 ± 1.0\n44.5 ± 0.8"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Wav2vec2-PT\nLocal enc.\n60.3 ± 0.7\n65.4 ± 1.7"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Cont. enc.\n58.5 ± 0.6\n69.0 ± 0.2"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "67.2 ± 0.7\n84.3 ± 1.7\nAll layers"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Wav2vec2-FT\nLocal enc.\n57.3 ± 1.0\n58.8 ± 2.7"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Cont. enc.\n44.6 ± 1.0\n37.5 ± 3.0"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "All layers\n63.8 ± 0.3\n68.7 ± 0.9"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "extracted from wav2vec 2.0 models. We also show two sys-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "tems based on eGeMAPS and spectrogram features, which can"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "be considered as baselines.\nIn all cases,\nfeatures are normal-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "ized by speaker and the Dense model architecture in Figure 1"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "is used as downstream model. We can see that\nfeatures\nfor"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "both wav2vec 2.0 models,\nthe one ﬁnetuned in 960 hours of"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Librispeech (wav2vec2-FT) and the one that\nis not ﬁnetuned"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "(wav2vec2-PT), the local encoder representations lead to better"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "results than both of the baseline features. It is worth noting that"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "eGeMAPS,\nspectrograms and the wav2vec 2.0 local encoder"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "representations contain information restricted only to a local"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "window around each frame, of 60 ms for eGeMAPS, and 25 ms"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "for the others. Also, the downstream model combines informa-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "tion from consecutive frames using just global average, which"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "is a very simple approach that might be suboptimal because it"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "cannot\ntake into account\ntemporal patterns in the features.\nIn"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "spite of that,\nthe local encoder representations, particularly the"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "ones obtained from the PT model, reach a performance compa-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "rable to much more complex models like the one proposed in"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "[14], which is a CNN with Bi-LSTM layers trained in a fully"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "supervised setting with spectrograms as input."
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "We can also see that the wav2vec2-PT features perform bet-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "ter\nthan wav2vec2-FT features in all cases.\nIn particular,\nthe"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "model using only the contextualized encoder outputs\nfor\nthe"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "wav2vec2-FT model has the worst results in the table. We hy-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "pothesize that this is because when the model is ﬁnetuned for an"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "ASR task, information that is not relevant for that task but might"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "be relevant for speech emotion recognition is lost from the em-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "beddings. For example,\ninformation about\nthe pitch might not"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "be important\nfor\nspeech recognition, while it\nis essential\nfor"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "speech emotion recognition."
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "Further, Table 1 shows that using a weighted average of the"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "transformer blocks outputs along with the local encoder outputs"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "(rows labelled ”All\nlayers”) results in better performance than"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "using only the local or\nthe contextual encoder outputs. Using"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "only the local encoder representations might not give informa-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "tion about events occurring at time scales larger than the recep-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "tive ﬁeld (25 ms). Further, the output of the contextual encoder"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "might be similar to the quantized local encoder representations,"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "since the contrastive loss objective is designed to achieve this"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "goal. Hence, using both of these layers, along with all interme-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "diate layers in the transformer provides additional\ninformation"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "that is valuable to the model."
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "from the weighted average\nFigure 2 shows the weights αi"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "layer once the downstream models are trained. We can see that"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": ""
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "the middle layers are given larger weights. This could be be-"
        },
        {
          "features\nTable 1: Average recall (%) obtained for the different": "cause these layers have already contextualized enough informa-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Average recall from the model variants in Figure 1",
      "data": [
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "using the wav2vec2-PT features from all layers, with two types"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "of normalization (top block) compared with results from other"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "works, which all use global normalization (bottom block).\nIn"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "bold are the best results using global normalization."
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Model - Norm\nIEMOCAP\nRAVDESS"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Dense - Speaker\n67.2 ± 0.7\n84.3 ± 1.7"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Dense - Global\n65.8 ± 0.3\n75.7 ± 2.3"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "LSTM - Global\n64.8 ± 1.9\n74.6 ± 3.7"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "66.3 ± 0.7\n77.5 ± 1.0\nFusion - Global"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "BiLSTM w. attention [13]\n58.8\n-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "CNN-BiLSTM [14]\n59.4\n-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "TDNN-LSTM w. attention [15]\n60.7\n-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Wav2Vec [27]\n64.3\n-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Table 2 also shows some of\nthe results obtained in other"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "works on the IEMOCAP dataset, using the same experimental"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "setup we are using.\nFor a fair comparison with our work, we"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "restrict\nthe comparison to models in the literature that do not"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "use automatic or manual\ntranscriptions. We can see that all of"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "our models perform better than the state of the art."
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Finally, we also compare our\nresults with those in [38],"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "which\nconsists\nof\na\ndeep\nconvolutional\nneural\nnetwork\nus-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "ing acoustic features as input.\nFor\nthis comparison, we used"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "the dense downstream model with global normalization and"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "imitated\ntheir\nexperimental\nsetup\nboth\nfor\nIEMOCAP\nand"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "RAVDESS. For IEMOCAP,\nthey only use the improvised ses-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "sions and full agreement utterances. For RAVDESS,\nthey use"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "5 fold cross-validation, dividing the data randomly. Moreover,"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "they do not merge calm and neutral emotions, so the total num-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "ber of emotions to be predicted is 8. We outperformed the mod-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "els in [38] for both datasets obtaining an average recall of 84.1"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "± 1.2 % in RAVDESS, and 72.1 ± 0.9% in IEMOCAP, com-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "pared to the results in the paper which are 64.3% and 71.6%,"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "respectively."
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "4. Conclusions"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "In this work, we explored different ways of extracting and mod-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "eling features from pretrained wav2vec 2.0 models for speech"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "emotion recognition. We proposed to combine the different lay-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "ers in the wav2vec 2.0 model using trainable weights and model"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "the resulting features with a simple DNN with a time-wise pool-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "ing layer. We evaluated our models on two standard emotion"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "datasets,\nIEMOCAP and RAVDESS, and showed superior\nre-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "sults on both cases, compared to those in recent\nliterature. We"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "found that the combination of information from different layers"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "in the wav2vec 2.0 model\nled to improved results over using"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "only the encoder outputs, as in previous works.\nFurther, we"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "found that\nthe combination of the wav2vec 2.0 features with a"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "set of prosodic features gave additional gains, suggesting that"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "the wav2vec 2.0 model does not contain all\nthe prosodic in-"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "formation needed for emotion recognition. Finally, we showed"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "that a wav2vec 2.0 model ﬁnetuned for the task of ASR worked"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "worse than the one trained only with the self-supervised task,"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "indicating that\nthe ﬁnetuning eliminates information from the"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "embeddings that is useful for emotion recognition."
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "5. Acknowledgements"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": ""
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "This material is based upon work supported by a Google Faculty"
        },
        {
          "from the model variants in Figure 1\nTable 2: Average recall": "Research Award, 2019, and an Amazon Research Award, 2019."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "rispeech: An asr corpus based on public domain audio books,”"
        },
        {
          "6. References": "[1]\nS. Brave and C. Nass, “Emotion in human–computer interaction,”",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "in 2015 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "6. References": "in Human-computer interaction fundamentals.\nCRC Press Boca",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "Signal Processing (ICASSP), 2015."
        },
        {
          "6. References": "Raton, FL, USA, 2009, vol. 20094635.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[21] M. Li,\nB. Yang,\nJ. Levy, A.\nStolcke, V. Rozgic,\nS. Mat-"
        },
        {
          "6. References": "[2]\nS. Haq and P.\nJackson,\n“Machine Audition:\nPrinciples, Algo-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "soukas,\nC.\nPapayiannis,\nD.\nBone,\nand\nC. Wang,\n“Con-"
        },
        {
          "6. References": "rithms and Systems,” W. Wang, Ed.\nIGI Global, 2010.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "trastive Unsupervised Learning for Speech Emotion Recogni-"
        },
        {
          "6. References": "[3]\nS. R. Livingstone and F. A. Russo, “The Ryerson Audio-Visual",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "tion,” arXiv:2102.06357, 2021."
        },
        {
          "6. References": "Database of Emotional Speech and Song (RAVDESS): A dy-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[22] R. Zhang, H. Wu, W. Li, D. Jiang, W. Zou, and X. Li, “Trans-"
        },
        {
          "6. References": "namic, multimodal set of\nfacial and vocal expressions in North",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "former based unsupervised pre-training for acoustic representa-"
        },
        {
          "6. References": "American English,” PLOS ONE, vol. 13, no. 5, 2018.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "tion learning,” arXiv:2007.14602, 2021."
        },
        {
          "6. References": "[4]\nF. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier,\nand",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[23]\nS.\nSchneider,\nA.\nBaevski,\nR.\nCollobert,\nand M.\nAuli,"
        },
        {
          "6. References": "B. Weiss, “A database of german emotional speech,” in Ninth Eu-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "“wav2vec: Unsupervised Pre-training for Speech Recognition,”"
        },
        {
          "6. References": "ropean Conference on Speech Communication and Technology,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "arXiv:1904.05862, 2019."
        },
        {
          "6. References": "2005.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[24] A. Baevski,\nS. Schneider,\nand M. Auli,\n“vq-wav2vec:\nSelf-"
        },
        {
          "6. References": "[5] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "supervised learning of discrete speech representations,” in Inter-"
        },
        {
          "6. References": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "national Conference on Learning Representations, 2020."
        },
        {
          "6. References": "Interactive emotional dyadic motion capture database,” Language",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "resources and evaluation, vol. 42, no. 4, 2008.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[25]\nS. Siriwardhana, A. Reis, R. Weerasekera, and S. Nanayakkara,"
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "“Jointly\nFine-Tuning\n”BERT-like”\nSelf\nSupervised\nMod-"
        },
        {
          "6. References": "[6] R. Lotﬁan and C. Busso, “Building naturalistic emotionally bal-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "els\nto\nImprove Multimodal\nSpeech\nEmotion\nRecognition,”"
        },
        {
          "6. References": "anced speech corpus by retrieving emotional speech from existing",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "arXiv:2008.06682, 2020."
        },
        {
          "6. References": "podcast recordings,” IEEE Transactions on Affective Computing,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "vol. 10, no. 4, 2019.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[26] M. Macary, M. Tahon, Y. Est`eve, and A. Rousseau, “On the use"
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "of Self-supervised Pre-trained Acoustic and Linguistic Features"
        },
        {
          "6. References": "[7] B. Schuller, G. Rigoll, and M. Lang, Hidden Markov Model-based",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "for Continuous Speech Emotion Recognition,” arXiv:2011.09212,"
        },
        {
          "6. References": "Speech Emotion Recognition, 2003, vol. 2.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "2020."
        },
        {
          "6. References": "[8] N.\nSato\nand Y. Obuchi,\n“Emotion\nrecognition\nusing mel-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "¨\n[27]\nJ. Boigne, B. Liyanage, and T.\nOstrem, “Recognizing More Emo-"
        },
        {
          "6. References": "Information and Media Tech-\nfrequency cepstral\ncoefﬁcients,”",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "tions with Less Data Using Self-supervised Transfer Learning,”"
        },
        {
          "6. References": "nologies, vol. 2, no. 3, 2007.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "arXiv:2011.05585, 2020."
        },
        {
          "6. References": "[9]\nP. Shen, Z. Changjun,\nand X. Chen,\n“Automatic speech emo-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[28] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:"
        },
        {
          "6. References": "tion recognition using support vector machine,” in Proceedings of",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "A Framework for Self-Supervised Learning of Speech Represen-"
        },
        {
          "6. References": "2011 International Conference on Electronic & Mechanical En-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "tations,” arXiv:2006.11477, 2020."
        },
        {
          "6. References": "gineering and Information Technology, vol. 2.\nIEEE, 2011.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[29]\nE.\nJang, S. Gu,\nand B. Poole,\n“Categorical\nreparameterization"
        },
        {
          "6. References": "[10] K. S. Rao, S. G. Koolagudi, and R. R. Vempada, “Emotion recog-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "with gumbel-softmax,” arXiv preprint arXiv:1611.01144, 2016."
        },
        {
          "6. References": "nition from speech using global and local prosodic features,” In-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "ternational journal of speech technology, vol. 16, no. 2, 2013.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[30]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-"
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "training of deep bidirectional\ntransformers\nfor\nlanguage under-"
        },
        {
          "6. References": "[11] M. Borchert and A. Dusterhoft, “Emotions in speech-experiments",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "standing,” arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "6. References": "with prosody and quality features in speech for use in categorical",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "and dimensional emotion recognition environments,” in Interna-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[31] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Con-"
        },
        {
          "6. References": "tional Conference on Natural Language Processing and Knowl-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "nectionist\ntemporal\nclassiﬁcation:\nlabelling\nunsegmented\nse-"
        },
        {
          "6. References": "edge Engineering.\nIEEE, 2005.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "quence data with recurrent neural networks,” in ICML, 2006."
        },
        {
          "6. References": "[12] C.-C. Lee, E. Mower, C. Busso,\nS. Lee,\nand S. Narayanan,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[32]\nF. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr´e,"
        },
        {
          "6. References": "“Emotion recognition using a hierarchical binary decision tree ap-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "C. Busso, L. Y. Devillers,\nJ. Epps, P. Laukka, S. S. Narayanan"
        },
        {
          "6. References": "proach,” Speech Communication, vol. 53, no. 9-10, 2011.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "et al., “The geneva minimalistic acoustic parameter set (gemaps)"
        },
        {
          "6. References": "[13]\nS. Mirsamadi, E. Barsoum,\nand C. Zhang,\n“Automatic speech",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "for voice research and affective computing,” IEEE transactions on"
        },
        {
          "6. References": "emotion recognition using recurrent neural networks with local",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "affective computing, vol. 7, no. 2, 2015."
        },
        {
          "6. References": "attention,” in 2017 IEEE International Conference on Acoustics,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[33]\nF. Eyben, M. W¨ollmer, and B. Schuller, “Opensmile:\nthe mu-"
        },
        {
          "6. References": "Speech and Signal Processing (ICASSP).\nIEEE, 2017.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "nich versatile and fast open-source audio feature extractor,”\nin"
        },
        {
          "6. References": "[14] A. Satt, S. Rozenberg, and R. Hoory, “Efﬁcient emotion recogni-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "Proceedings of\nthe 18th ACM international conference on Mul-"
        },
        {
          "6. References": "tion from speech using deep learning on spectrograms.” in Inter-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "timedia, 2010."
        },
        {
          "6. References": "speech, 2017.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[34] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,"
        },
        {
          "6. References": "[15] M. Sarma, P. Ghahremani, D. Povey, N. K. Goel, K. K. Sarma,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "and L. Zettlemoyer, “Deep contextualized word representations,”"
        },
        {
          "6. References": "and N. Dehak, “Emotion identiﬁcation from raw speech signals",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "in NAACL, 2018."
        },
        {
          "6. References": "using dnns.” in Interspeech, 2018.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[35] K. Venkataramanan and H. R. Rajamohan, “Emotion recognition"
        },
        {
          "6. References": "[16] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "from speech,” arXiv preprint arXiv:1912.10458, 2019."
        },
        {
          "6. References": "S. Wang, Z. Zhang, Y. Wu et al.,\n“Conformer:\nConvolution-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[36] H. M. Fayek, M. Lech, and L. Cavedon, “Evaluating deep learning"
        },
        {
          "6. References": "augmented transformer\nfor\nspeech recognition,” arXiv preprint",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "architectures for Speech Emotion Recognition,” Neural Networks,"
        },
        {
          "6. References": "arXiv:2005.08100, 2020.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "vol. 92, 2017."
        },
        {
          "6. References": "[17]\nP. Matˇejka, O. Glembek, O. Novotn`y, O. Plchot, F. Gr´ezl, L. Bur-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[37]\nP. Riera, L. Ferrer, A. Gravano, and L. Gauder, “No sample left"
        },
        {
          "6. References": "get, and J. H. Cernock`y, “Analysis of dnn approaches to speaker",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "behind: Towards a comprehensive evaluation of speech emotion"
        },
        {
          "6. References": "identiﬁcation,” in ICASSP.\nIEEE, 2016.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "recognition system,” in Proc. Workshop on Speech, Music and"
        },
        {
          "6. References": "[18] Y.\nZhao,\nD. Yin,\nC.\nLuo,\nZ.\nZhao,\nC.\nTang, W.\nZeng,",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "Mind 2019, 2019."
        },
        {
          "6. References": "and Z.-J. Zha, “General-Purpose Speech Representation Learn-",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "[38] D. Issa, M. Fatih Demirci, and A. Yazici, “Speech emotion recog-"
        },
        {
          "6. References": "ing\nthrough\na Self-Supervised Multi-Granularity Framework,”",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "nition with deep convolutional neural networks,” Biomedical Sig-"
        },
        {
          "6. References": "arXiv:2102.01930, 2021.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": "nal Processing and Control, vol. 59, 2020."
        },
        {
          "6. References": "[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        },
        {
          "6. References": "arXiv preprint arXiv:1706.03762, 2017.",
          "[20] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Lib-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion in human-computer interaction",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2009",
      "venue": "Human-computer interaction fundamentals"
    },
    {
      "citation_id": "3",
      "title": "Machine Audition: Principles, Algorithms and Systems",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "4",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "5",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Hidden Markov Model-based Speech Emotion Recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "Hidden Markov Model-based Speech Emotion Recognition"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using melfrequency cepstral coefficients",
      "authors": [
        "N Sato",
        "Y Obuchi"
      ],
      "year": "2007",
      "venue": "Information and Media Technologies"
    },
    {
      "citation_id": "10",
      "title": "Automatic speech emotion recognition using support vector machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2013",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "12",
      "title": "Emotions in speech-experiments with prosody and quality features in speech for use in categorical and dimensional emotion recognition environments",
      "authors": [
        "M Borchert",
        "A Dusterhoft"
      ],
      "year": "2005",
      "venue": "International Conference on Natural Language Processing and Knowledge Engineering"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee",
        "E Mower",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "14",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Emotion identification from raw speech signals using dnns"
    },
    {
      "citation_id": "17",
      "title": "Conformer: Convolutionaugmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolutionaugmented transformer for speech recognition",
      "arxiv": "arXiv:2005.08100"
    },
    {
      "citation_id": "18",
      "title": "Analysis of dnn approaches to speaker identification",
      "authors": [
        "P Matějka",
        "O Glembek",
        "O Novotnỳ",
        "O Plchot",
        "F Grézl",
        "L Burget",
        "J Cernockỳ"
      ],
      "year": "2016",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "General-Purpose Speech Representation Learning through a Self-Supervised Multi-Granularity Framework",
      "authors": [
        "Y Zhao",
        "D Yin",
        "C Luo",
        "Z Zhao",
        "C Tang",
        "W Zeng",
        "Z.-J Zha"
      ],
      "year": "2021",
      "venue": "General-Purpose Speech Representation Learning through a Self-Supervised Multi-Granularity Framework",
      "arxiv": "arXiv:2102.01930"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "21",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Contrastive Unsupervised Learning for Speech Emotion Recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Contrastive Unsupervised Learning for Speech Emotion Recognition",
      "arxiv": "arXiv:2102.06357"
    },
    {
      "citation_id": "23",
      "title": "Transformer based unsupervised pre-training for acoustic representation learning",
      "authors": [
        "R Zhang",
        "H Wu",
        "W Li",
        "D Jiang",
        "W Zou",
        "X Li"
      ],
      "year": "2021",
      "venue": "Transformer based unsupervised pre-training for acoustic representation learning",
      "arxiv": "arXiv:2007.14602"
    },
    {
      "citation_id": "24",
      "title": "wav2vec: Unsupervised Pre-training for Speech Recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised Pre-training for Speech Recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "25",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "26",
      "title": "Jointly Fine-Tuning \"BERT-like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Jointly Fine-Tuning \"BERT-like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "27",
      "title": "On the use of Self-supervised Pre-trained Acoustic and Linguistic Features for Continuous Speech Emotion Recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2020",
      "venue": "On the use of Self-supervised Pre-trained Acoustic and Linguistic Features for Continuous Speech Emotion Recognition",
      "arxiv": "arXiv:2011.09212"
    },
    {
      "citation_id": "28",
      "title": "Recognizing More Emotions with Less Data Using Self-supervised Transfer Learning",
      "authors": [
        "J Boigne",
        "B Liyanage",
        "T Östrem"
      ],
      "year": "2020",
      "venue": "Recognizing More Emotions with Less Data Using Self-supervised Transfer Learning",
      "arxiv": "arXiv:2011.05585"
    },
    {
      "citation_id": "29",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "30",
      "title": "Categorical reparameterization with gumbel-softmax",
      "authors": [
        "E Jang",
        "S Gu",
        "B Poole"
      ],
      "year": "2016",
      "venue": "Categorical reparameterization with gumbel-softmax",
      "arxiv": "arXiv:1611.01144"
    },
    {
      "citation_id": "31",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "32",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "ICML"
    },
    {
      "citation_id": "33",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "34",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Deep contextualized word representations",
      "authors": [
        "M Peters",
        "M Neumann",
        "M Iyyer",
        "M Gardner",
        "C Clark",
        "K Lee",
        "L Zettlemoyer"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech",
      "arxiv": "arXiv:1912.10458"
    },
    {
      "citation_id": "37",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "38",
      "title": "No sample left behind: Towards a comprehensive evaluation of speech emotion recognition system",
      "authors": [
        "P Riera",
        "L Ferrer",
        "A Gravano",
        "L Gauder"
      ],
      "year": "2019",
      "venue": "Proc. Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "39",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}