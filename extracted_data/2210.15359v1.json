{
  "paper_id": "2210.15359v1",
  "title": "Exploiting Modality-Invariant Feature For Robust Multimodal Emotion Recognition With Missing Modalities",
  "published": "2022-10-27T12:16:25Z",
  "authors": [
    "Haolin Zuo",
    "Rui Liu",
    "Jinming Zhao",
    "Guanglai Gao",
    "Haizhou Li"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "Missing modality imagination",
    "Central moment discrepancy (CMD)",
    "Invariant feature"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition leverages complementary information across modalities to gain performance. However, we cannot guarantee that the data of all modalities are always present in practice. In the studies to predict the missing data across modalities, the inherent difference between heterogeneous modalities, namely the modality gap, presents a challenge. To address this, we propose to use invariant features for a missing modality imagination network (IF-MMIN) which includes two novel mechanisms: 1) an invariant feature learning strategy that is based on the central moment discrepancy (CMD) distance under the full-modality scenario; 2) an invariant feature based imagination module (IF-IM) to alleviate the modality gap during the missing modalities prediction, thus improving the robustness of multimodal joint representation. Comprehensive experiments on the benchmark dataset IEMOCAP demonstrate that the proposed model outperforms all baselines and invariantly improves the overall emotion recognition performance under uncertain missing-modality conditions. We release the code at: https://github.com/ZhuoYulang/IF-MMIN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The study of multimodal emotion recognition with missing modalities seeks to perform emotion recognition in realistic environments  [1, 2] , where some data could be missing due to obscured cameras, damaged microphones, etc. The mainstream solution for the missing modality problem can be summarized in two categories: 1) missing data generation  [3] [4] [5] , 2) multimodal joint representation learning  [6, 7] . In  [3] , an encoder-decoder network was proposed to generate high-quality missing modality images according to the available modality, In  [7] , a translation-based method with cycle consistency loss was studied to learn joint representations between modalities. In  [1] , a Missing Modality Imagination Network, or MMIN for short, was studied to learn joint representations by predicting missing modalities, which combines the above two methods.\n\nThe modality gap between heterogeneous modalities  [8] [9] [10]  remains an issue, which adversely affects emotion recognition accuracy. The question is how to alleviate such a modality gap. While the modalities have their unique characteristics, they share the same information in the semantic space. The modality-invariant feature was introduced to multimodal emotion recognition with full modality data, which shows remarkable performance. Hazarika et al.  [8]  proposed the shared subspace to learn potential commonalities between modalities to reduce the influence of the modality gap. Liu et al.  [11]  proposed discrete shared spaces for capturing fine-grained representations to improve cross-modal retrieval accuracy. All the studies suggest that the modality-invariant feature effectively bridges the modality gap. We note that there has been no related work for emotion recognition under the missing-modality conditions.\n\nIn this work, we propose a missing modality imagination network with the invariant feature (IF-MMIN). Specifically, we first learn the modality-invariant feature among various modalities by using a central moment discrepancy (CMD) distance  [12]  based constraint training strategy. We then design the IF-MMIN neural architecture to predict the invariant features of the missing modality from the available modality. In this way, we fully explore the available modality to alleviate the modality gap problem in cross-modal imagination, thus, improving the robustness of multimodal joint representation. The experimental results, on the benchmark dataset IEMOCAP, show that the proposed method outperforms the state-of-the-art baseline models under all missing-modality conditions.\n\nThe main contributions of this work are, 1) we propose a CMD-based distance constraint training to learn the modality-arXiv:2210.15359v1 [cs.CV] 27 Oct 2022 invariant feature among full modalities; 2) we introduce invariant features into the cross-modality imagination process to reduce the impact of the modality gap and enhance the robustness of multimodal joint representation; and 3) experimental results on various missing modalities conditions demonstrate that the proposed IF-MMIN can perform accurate emotion recognition performance in the scenario of missing modalities.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "If-Mmin: Methodology",
      "text": "The proposed IF-MMIN scheme first employs a central moment discrepancy (CMD) distance based invariant feature learning strategy under full-modality signals to learn the modality-specific and modality-invariant features. During IF-MMIN training, the IF-IM reads these two features to learn the robust joint representation through missing modality imagination.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cmd Distance-Based Invariant Feature Learning",
      "text": "As shown in Fig.  1 , the pipeline of invariant feature learning includes three modules: specificity encoder, invariance encoder, and classifier. Specificity aims to extract the high-level features (h a , h v , h t ) from the raw features (x a , x v , x t ) of each modality to represent the modalityspecific features. The invariance encoder takes the modalityspecific features as input to extract the modality-invariant features H, which is concatenation by the high-level features (H a , H v , H t ) among all modalities. At last, the fullconnected layer-based classifier input the concatenation of h and H to predict the emotion category. After pertaining, we will adopt pretrained specificity and invariance encoders along with the proposed IF-IM module to build the IF-MMIN architecture.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Specificity & Invariance Encoders",
      "text": "As the blue blocks in Fig.  1 , the specificity encoder is composed of three modules: acoustic, visual, and textual encoder, Enc a , Enc v , and Enc t for short respectively. Specifically, Enc a employs the LSTM  [13]  and max-pooling layer to extract the utterance-level acoustic feature h a from the raw feature x a . Enc v shares a similar structure with Enc a to read the raw features x v and output the utterance-level visual features h v . Enc t adopts the TextCNN  [14] , which is a power text representation model in NLP filed, to extract the utterance-level textual features h t from the raw feature x t .\n\nThe invariance encoder, Enc , is shown as the green blocks in Fig.  1 , which consists of the full-connected layer, the activation function, and the dropout layer. It aims to map modality-specific features (h a , h v , h t ) into a shared subspace with CMD-based distance constraint strategy (as shown by the red arrow in Fig.  1 ) to obtain high-level features (H a , H v , H t ). Then, we concatenate the three high-level features into modality-invariant features H.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cmd-Based Distance Constraint",
      "text": "The CMD-based distance constraint aims to reduce the discrepancy between the high-level features (H a , H v , H t ) of modalities. Note that CMD  [12]  is a state-of-the-art distance metric that measures the discrepancy between the distribution of two features by matching their order-wise moment differences. We ensure that modality-invariant representation can be learned by minimizing the L cmd :\n\nwhere E(H) is the empirical expectation vector of the input sample H, and C k (H) = E((H -E(H)) k ) is the vector of all k th order sample central moments of the coordinates of H.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "If-Mmin Training",
      "text": "The overall architecture of IF-MMIN is illustrated in Fig.  2 (a), which includes 1) Specificity Encoder; 2) Invariance Encoder; 3) Modality-invariant Feature aware Imagination Module, IF-IM for short; and 4) Classifier.\n\nAssume that the full-modalities input is x = (x a , x v , x t ). Specificity encoder takes (x a , x v miss , x t ), where miss indicates the specific missing modality, as input to extract the modality-specific features (h a , h v , h t ), which are then concatenated as final output h. Invariance encoder reads (h a , h v , h t ) to predict the modality-invariant feature H , which is a concatenation of high-level features",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Invariant Feature Aware Imagination Module (If-Im)",
      "text": "As shown in Fig.  2 (b), IF-IM is built with the cascaded autoencoder which includes M autoencoders. Different from  [1] , IF-IM reads the h and H simultaneously. In addition, H is a cascaded input given to each autoencoder to assist the missing modality imagination and alleviate the modality gap problem.\n\nEach autoencoder denoted as ω i , i = 1,2,...,M . Then the calculation of each autoencoder can be defined as:\n\nwhere ∆z i is the output of the i th autoencoder. The imagined missing modality h of IF-IM can be defined as: h = ∆z M .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss Functions",
      "text": "During\n\nThe total loss function is the sum of the three functions: L = L cls + λ 1 L img + λ 2 L inv , where λ 1 and λ 2 are the balance factors.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "We validate the IF-MMIN on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [15] . Following  [1] , we process IEMOCAP emotional labels into four categories: happy, angry, sad, and neutral. The splitting ratio of training/validation/testing sets is 8:1:1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Similar to those in  [1] , the raw features x a , x v and x t are 130-dim OpenSMILE  [16]  features with the configuration of \"IS13 ComParE\", 342-dim \"Denseface\" features extracted by a pretrained DenseNet model  [17]  and 1024-dim BERT word embeddings, respectively.\n\nThe hidden size of specifically encoders Enc a and Enc v is set to 128, Enc t contains 3 convolution blocks with kernel sizes of 3,4,5 and the output size of 128. The size of invariance encoder Enc output, H, is 128. IF-IM consists of 5 autoencoders in size 384-256-128-64-128-256-384, where the hidden-vector size is 64. The classifier includes 3 fullyconnected layers of size {128,128,4}. Since the value of L inv is quite smaller (about 1%) than L img , we set λ 1 is 1 and λ 2 is 100 to balance the numerical difference and elevate the importance of L inv in the total loss. The batch size is 128 and the dropout rate is 0.5. We adopt the Adam optimizer  [18]  which with the dynamic learning rate and the initial rate is 0.0002, and use the Lambda LR  [19]  to update the learning rate.\n\nWe conduct all experiments, including invariant feature learning and IF-MMIN training, with 10-fold crossvalidation, where each fold contains 40 epochs.\n\nTo demonstrate the robustness of our models, we run each model three times to alleviate the influences of random initialization of parameters. We select the best model on the validation set and report its performance on the testing set. All models are implemented with Pytorch deep learning toolkit and run on a single NVIDIA Tesla P100 graphic card.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparative Study",
      "text": "We develop three multimodal emotion recognition systems for a comparative study. 1) MCTN  [7]  learns the joint representation via a cyclic translation between missing and available modalities; 2) MMIN  [1]  is the state-of-the-art model for the missing modality problem which learns the joint representation through cross-modality imagination via the autoencoder and cycle consistency learning; 3) MMIN w/o cycle  [1]  removes the cycle consistency learning part of MMIN and just retains the forward missing modality imagination process, which is a fair counterpart for our IF-MMIN.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Main Results For Uncertain Missing-Modality",
      "text": "To validate our IF-MMIN under different missing modality testing conditions  [1] , We report all results in terms of Weighted Accuracy (WA)  [20]  and Unweighted Accuracy (UA)  [21] . As shown in rows 2 through 5 of Table  1 , our IF-MMIN achieves the highest average values under all missingmodality testing conditions. For each condition, IF-MMIN also outperforms all baselines except for conditions {a} and {v}, where it is comparable to the optimal baseline. The possible reason is that the textual modality contains more semantic information than the audio and visual modalities  [22] . In a nutshell, all the results show that IF-MMIN can learn robust multimodal joint representation, alleviate the modality gap by introducing modality-invariant features, and thus enable remarkable performance under different missingmodality testing conditions. As shown in rows 5 through 7 of Table  1 , IF-MMIN also outperforms the IF-MMIN w/o L inv and IF-MMIN w/o cascaded input in most cases, which confirms that 1) the invariance encoder of IF-MMIN can predict accurate invariant feature under the constraints of L inv , so as to better serve the IF-IM; 2) the cascaded input can provide prior knowledge when each layer of autoencoder works and indeed strengthen the imagination ability of IF-IM.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visualization Analysis",
      "text": "The accuracy of invariant feature learning is the premise for IF-MMIN to work well. Therefore, to verify the role of invariant feature learning related modules, including L cmd , L inv , H and H, we conduct the following visualization experiments for IF-MMIN.\n\nWe visualize the H under six missing conditions using t-SNE algorithm in a two dimensional plane  [23] , as shown in 3(a). We randomly select 600 sentences from the testing set, 100 sentences for each condition, and extract 600 invariant features H . Therefore, there are 600 points in",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The pipeline of the central moment discrepancy",
      "page": 2
    },
    {
      "caption": "Figure 2: The diagrams for the proposed IF-MMIN. (a) shows",
      "page": 3
    },
    {
      "caption": "Figure 3: (a). We randomly select 600 sentences from the",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualization analysis about the invariant feature",
      "page": 4
    },
    {
      "caption": "Figure 3: (a), 100 points in each color. It’s observed that all H′",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The experimental results of our IF-MMIN, three baselines, and two ablation systems under six missing-modality",
      "data": [
        {
          "System": "",
          "Testing Conditions": "{a}\n{v}\n{t}\n{a,v}\n{a,t}\n{v,t}\nAverage"
        },
        {
          "System": "",
          "Testing Conditions": "WA\nUA"
        },
        {
          "System": "MCTN [7]",
          "Testing Conditions": "0.4975\n0.5162"
        },
        {
          "System": "MMIN [1]",
          "Testing Conditions": "0.5511\n0.5726"
        },
        {
          "System": "MMIN w/o cycle [1]",
          "Testing Conditions": "0.5503\n0.5821"
        },
        {
          "System": "IF-MMIN (ours)",
          "Testing Conditions": "0.5620 ↑\n0.5813 ∗\n⇑\n⇑"
        },
        {
          "System": "w/o Linv",
          "Testing Conditions": "0.5513\n0.5767"
        },
        {
          "System": "w/o cascaded input",
          "Testing Conditions": "0.5552\n0.5768"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "3",
      "title": "CTFN: Hierarchical learning for multimodal sentiment analysis using coupled-translation fusion network",
      "authors": [
        "Jiajia Tang",
        "Kang Li",
        "Xuanyu Jin",
        "Andrzej Cichocki",
        "Qibin Zhao",
        "Wanzeng Kong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Deep adversarial learning for multi-modality missing data completion",
      "authors": [
        "Lei Cai",
        "Zhengyang Wang",
        "Hongyang Gao",
        "Dinggang Shen",
        "Shuiwang Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "5",
      "title": "Metric learning on healthcare data with incomplete modalities",
      "authors": [
        "Qiuling Suo",
        "Weida Zhong",
        "Fenglong Ma",
        "Ye Yuan",
        "Jing Gao",
        "Aidong Zhang"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Semi-supervised deep generative modelling of incomplete multi-modality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "2018 ACM Multimedia Conference on Multimedia Conference, MM 2018"
    },
    {
      "citation_id": "7",
      "title": "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Zhao Ren",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Crossmodal retrieval and synthesis (X-MRS): closing the modality gap in shared subspace learning",
      "authors": [
        "Ricardo Guerrero",
        "Xuan Pham",
        "Vladimir Pavlovic"
      ],
      "year": "2021",
      "venue": "MM '21: ACM Multimedia Conference, Virtual Event"
    },
    {
      "citation_id": "11",
      "title": "Domain and modality gaps for lidar-based person detection on mobile robots",
      "authors": [
        "Dan Jia",
        "Alexander Hermans",
        "Bastian Leibe"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "12",
      "title": "Cross-modal discrete representation learning",
      "authors": [
        "Alexander Liu",
        "Souyoung Jin",
        "Cheng-I Lai",
        "Andrew Rouditchenko",
        "Aude Oliva",
        "James Glass"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "13",
      "title": "Central moment discrepancy (CMD) for domain-invariant representation learning",
      "authors": [
        "Werner Zellinger",
        "Thomas Grubinger",
        "Edwin Lughofer",
        "Thomas Natschläger",
        "Susanne Saminger-Platz"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "14",
      "title": "Long shortterm memory based recurrent neural network architectures for large vocabulary speech recognition",
      "authors": [
        "Hasim Sak",
        "Andrew Senior",
        "Franc",
        "Beaufays"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "15",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "17",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th International Conference on Multimedia 2010"
    },
    {
      "citation_id": "18",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "20",
      "title": "Deep transformer models for time series forecasting: The influenza prevalence case",
      "authors": [
        "Neo Wu",
        "Bradley Green",
        "Xue Ben",
        "Shawn O' Banion"
      ],
      "year": "2020",
      "venue": "Deep transformer models for time series forecasting: The influenza prevalence case",
      "arxiv": "arXiv:2001.08317"
    },
    {
      "citation_id": "21",
      "title": "Accuracy weighted diversitybased online boosting",
      "authors": [
        "Ishwar Baidari",
        "Nagaraj Honnikoll"
      ],
      "year": "2020",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "22",
      "title": "Pitchsynchronous single frequency filtering spectrogram for speech emotion recognition",
      "authors": [
        "Md Shruti Gupta",
        "Akshay Shah Fahad",
        "Deepak"
      ],
      "year": "2020",
      "venue": "Multim. Tools Appl"
    },
    {
      "citation_id": "23",
      "title": "NHFNET: A non-homogeneous fusion network for multimodal sentiment analysis",
      "authors": [
        "Ziwang Fu",
        "Feng Liu",
        "Qing Xu",
        "Jiayin Qi",
        "Xiangling Fu",
        "Aimin Zhou",
        "Zhibin Li"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Multimedia and Expo, ICME 2022"
    },
    {
      "citation_id": "24",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}