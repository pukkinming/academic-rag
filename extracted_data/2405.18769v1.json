{
  "paper_id": "2405.18769v1",
  "title": "Ous: Scene-Guided Dynamic Facial Expression Recognition",
  "published": "2024-05-29T05:12:16Z",
  "authors": [
    "Xinji Mai",
    "Haoran Wang",
    "Zeng Tao",
    "Junxiong Lin",
    "Shaoqi Yan",
    "Yan Wang",
    "Jing Liu",
    "Jiawen Yu",
    "Xuan Tong",
    "Yating Li",
    "Wenqiang Zhang"
  ],
  "keywords": [
    "Dynamic Facial Expression Recognition",
    "Affective Computing",
    "Overall Understanding of the Scene",
    "Type Fusion {xjmai23",
    "hrwang23",
    "ztao19",
    "linjx23",
    "sqyan19",
    "yanwang19",
    "jingliu19",
    "jwyu23",
    "xtong23}@fudan.edu.cn"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic Facial Expression Recognition (DFER) is crucial for affective computing but often overlooks the impact of scene context. We have identified a significant issue in current DFER tasks: human annotators typically integrate emotions from various angles, including environmental cues and body language, whereas existing DFER methods tend to consider the scene as noise that needs to be filtered out, focusing solely on facial information. We refer to this as the Rigid Cognitive Problem. The Rigid Cognitive Problem can lead to discrepancies between the cognition of annotators and models in some samples. To align more closely with the human cognitive paradigm of emotions, we propose an Overall Understanding of the Scene DFER method (OUS). OUS effectively integrates scene and facial features, combining scene-specific emotional knowledge for DFER. Extensive experiments on the two largest datasets in the DFER field, DFEW and FERV39k, demonstrate that OUS significantly outperforms existing methods. By analyzing the Rigid Cognitive Problem, OUS successfully understands the complex relationship between scene context and emotional expression, closely aligning with human emotional understanding in real-world scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "D YNAMIC Facial Expression Recognition is a crucial subfield of affective computing, facing a persistent challenge of ambiguous emotion classification. Typically, dynamic facial expressions that cannot be clearly classified by human annotators are removed during dataset construction. However, DFER methods still encounter a significant amount of ambiguous emotion classification. Through analyzing the performance of multiple DFER methods across different datasets, we found that these ambiguous classifications persist due to the discrepancy between annotators' and models' cognition, rather than actual annotation errors.\n\nIn DFER methods, almost all approaches unanimously consider scene information as noise and retain only the facial inputs. However, human annotators evaluate videos that include complete scene information when annotating facial expressions  [1] [2] . This discrepancy in the amount of information leads to what we call the Rigid Cognitive Problem. As illustrated in Fig.  2 , an example of the Rigid Cognitive Problem is presented. When viewing the face alone, the expression might convey sadness, fear, or other ambiguous emotions. However, with the scene context, human annotators can clearly identify it as a feeling of happiness and comfort in the figure above, and understand that the expression in the figure below is not joy but fear and sadness. In this instance, the positive scene polarity helps human annotators determine the ambiguous human expression more accurately  [3]  [4]  [5] . Therefore, we can define the Rigid Cognitive Problem as this, as shown in Fig.  1 . The Rigid Cognitive Problem refers to the human annotator instinctively combining scene information and facial information when annotating, and classifying unclearly faces are completed and preserved through scene information. For the DFER method, when using the data set, only face information is used for emotion classification, which leads to a decrease in performance. As shown in the Fig.  1  above, when analyzing emotions, human beings comprehensively consider the face, scene polarity, flowers, and sun and other objects. It is concluded that the conclusion of Happiness, and the model only obtains face information when analyzing emotions, so it is difficult to distinguish the unclear expression.\n\nIn psychology, some theories explain why the Rigid Cognitive PROBLEM occurs. Ekman  [6]  pointed out that while cer-arXiv:2405.18769v1 [cs.CV] 29 May 2024 tain basic facial expressions are universally recognized across cultures, accurately interpreting these expressions requires understanding the specific context. For example, a smile can signify happiness, embarrassment, or even anger, depending on the context. Schachter and Singer's Two-Factor Theory  [7]  [8] further explained that emotion results from the interaction between physiological arousal and its cognitive interpretation, influenced by the immediate environment and context. These theories demonstrate that when interpreting emotions, people consider various factors, including facial expressions, body language, and scene context, as do human annotators  [9] [10] . Further interpretation of the Rigid Cognitive Problem reveals how scene information influences our judgment of ambiguous expressions. Environmental psychologist Roger Barker's Behavior Settings Theory  [11]  emphasized that individuals' emotions and behaviors are significantly influenced by their surroundings. The emotional atmosphere of a lively amusement park versus a bloodshed refugee camp could lead to vastly different emotional responses. Scene polarity itself guides our judgment of ambiguous expressions, suggesting that positive scenes are more likely to be associated with positive emotions, and negative scenes with negative emotions  [12]  [13]  [14] . Therefore, incorporating scene polarity into the variables considered by methods is an intuitive inference.\n\nHowever, existing DFER methods typically focus on analyzing facial features in images while discarding other parts as noise, overlooking the subtle emotional and atmospheric nuances conveyed by these images, including scene polarity. For humans, integrating cues from scene polarity, scene objects, and facial expressions helps infer emotions, combining ambiguous facial expressions with broader scene context  [15] [16] . DFER methods lack this capability, with most focusing solely on facial classification, which is insufficient.\n\nRecognizing the Rigid Cognitive Problem, we propose a new framework called Overall Understanding of the Scene to address this issue. Specifically, we aim to: First, isolate scene polarity classification from scene information to guide model classification of ambiguous expressions, employing polarity loss in OUS to separate scene polarity. Second, align scene information itself with facial information, considering the strong connection between non-facial information (e.g., body movements, object information) and emotions. OUS uses similarity loss for this alignment. Third, design a novel fusion module to merge scene and facial features, as they lack temporal relationships and are more about spatial perspectives. Existing fusion methods mostly target temporal sequences. OUS's Type Fusion Encoder (TFE) accomplishes this. Finally, develop an effective feature classification method and loss function to handle noisy information due to scene inclusion, as simple module classification is no longer suitable. OUS utilizes contrastive loss and flexible prompt design for this purpose.\n\nOur research contributions are summarized as follows:\n\n• We identify and analyze the Rigid Cognitive Problem, designing a dynamic facial expression recognition method called Overall Understanding of the Scene, which effectively aligns and integrates scene information to extract emotional knowledge from complex features. Extensive experiments on the two largest DFER datasets, DFEW and FERV39k, demonstrate that OUS significantly outperforms existing methods. • We design three loss functions-similarity loss, polarity loss, and contrastive loss-to constrain the training process, helping OUS align scene information, extract scene polarity, and classify emotional information. These multi-loss constraints effectively reduce the latent space distance between scene and human features, aiding in emotion classification through polarity guidance and contrastive loss. • We have designed a Type fusion encoder (TFE) that mainly uses cross-type attention mechanism to effectively integrate scenarios and facial information to integrate the characteristics related to emotion.\n\nFig.  2 . Scene polarity helps determine mood. The left side shows facial expressions. When look at the face alone, the upper and lower expressions may be surprise, happiness, fear, sadness, etc., but by injecting the polarity of the scene, you can judge that these are two completely different expressions (happiness and sadness).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Dfer Methods",
      "text": "Compared to static expression recognition, DFER is often more reliable due to the temporal correlations between different frames in facial sequences  [17]    [18] . In recent years, a series of advanced techniques and datasets have driven the development of this field  [19]    [20] . These advancements provide tools for deeper understanding and analysis of facial expressions, paving the way for future applications  [21] . In terms of data processing in the early stage, the work of Tao  [22]  pointed out that we can obtain high dynamic emotional fragments through the frequency based on a complete or complex videos, which lays the foundation for subsequent work. Recently, Vision Transformer (ViT)  [23]  based on the Transformer architecture  [24]   Similarity Loss L similarity aligns the latent spaces of facial and scene features, reducing the distance between them using convolutional and fully connected layers. We also consider that the first four encoding blocks capture information like texture, color, and atmosphere, which are abstractly related to emotions. Polarity Loss constrains the Polarity Encoder to extract this polarity information, which is then concatenated with V f t and Vst and fed into the Type Fusion Encoder. This mechanism uses Learnable Queries (Q learn ) as the Query, with V f t and Vst as the Key and Value. Learnable Queries, initialized with a Gaussian distribution, are shared across all cross-type attention blocks and refined during training to capture emotional entities in the features. Finally, OUS incorporates substantial scene information, making fixed prompt methods unsuitable. Instead, it uses updatable prompts to compute contrastive loss with the output features.\n\nporal sequence data and capturing long-range temporal dependencies. Originally designed for natural language processing, Transformers' ability to handle sequence data makes them perform exceptionally in DFER research. Besides traditional deep learning methods, approaches like CLIP  [25] , CLIPER  [26]  and A 3 lign-DFER  [27]  have also gained significant attention in this field. CLIP, through a contrastive learning approach on large-scale image and text data, achieves powerful crossmodal representation capabilities. These methods significantly improve emotion recognition accuracy in natural and uncontrolled environments by integrating visual and textual information. Our method differs from the aforementioned approaches that focus solely on extracting facial information from videos for emotion recognition. We emphasize recognizing complex features of the entire scene and the face within the video, treating scene information not as noise but as a valuable supplement for emotion understanding, aligning more closely with human emotional perception patterns.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Scene Information In Dfer Datasets",
      "text": "The development and application of a series of datasets in the DFER field have greatly promoted its growth. CK+  [28]  is a classic facial expression dataset providing rich expression sequences, serving as a benchmark for evaluating various DFER algorithms. RAF-DB  [29]  includes real-world facial expressions collected from the internet, highlighting the importance of emotion recognition in natural environments. AFEW  [30] , sourced from movies and TV shows, focuses on recognizing facial expressions in dynamic and realistic contexts. MMI  [31] , a multimodal facial expression database with controlled and spontaneous expressions, is particularly suited for analyzing subtle facial expression changes. DFEW  [32] , as a more challenging dataset, contains facial expressions captured in natural environments, suitable for testing algorithms in real-world scenarios. FERV39k  [33]  and other datasets extend the exploration range in the DFER field by providing a large number of facial expression samples with various attributes  [34] . Among these datasets, DFEW and FERV39k are the largest unconstrained DFER datasets, with sample sizes and difficulty levels far surpassing others. It is noteworthy that regardless of the dataset used in DFER tasks, they typically contain complete scene information rather than just facial details. The DFEW dataset contains a large number of real-life scenes, FERV39k contains data of up to 22 scenes, and other data sets contain a large number of scenes except human faces. However, most methods discard all scene information when recognizing facial information from these datasets. We argue that this approach is unrealistic, as scene information can provide valuable context for emotion understanding.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this section, we explore the complexity of the proposed OUS framework. The overall structure of OUS is shown in Fig.  3 . OUS mainly includes spatial encoding, temporal encoding, multi-Loss constraints and TFE fusion composition. Vision Encoder is used for spatial feature extraction. Frames Encoder is used to extract time characteristics. Similarity Loss is used to align the hidden space, and the Contrast Loss is used to classify emotional classification. TFE is used to integrate scene characteristics and facial features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Overview Of Training Strategy For Different Losses",
      "text": "Our framework primarily relies on a training strategy reinforced by three distinct loss functions to guide the optimization strategy. We posit that the introduction of scene information comprises three parts: the guidance of ambiance and texture information, the alignment with facial information in the latent space, and a loss that can flexibly match environmental entities. Specifically, color, textural, and luminance information, as well as related visual entities in the scene (such as carousels, artillery fire, etc.), can preliminarily determine the emotional polarity (positive, neutral, negative) of humans in that scene. For instance, in bright, flowery environments like amusement parks, the emotional polarity is likely positive, whereas in dark, bloody environments with ongoing artillery fire, it is more likely negative. We believe that this emotional polarity can guide the extraction of concepts related to emotions from the scene.\n\nWe use the polarity loss to optimize the polarity encoder so it can extract emotion-related polarity information from the output of the first four layers of the vision encoder to guide the cross-type attention mechanism. The polarity loss is defined as the cross-entropy loss:\n\nwhere y i is the true polarity label and ŷi is the predicted polarity.\n\nTo fuse scene features with facial features, we need to align them in the same latent space. The similarity loss is used to optimize the frame encoder, linear, and convolutional layers, helping align scene features V s and facial features V f in the latent space and narrowing the relationship between scene features and emotion classification. The similarity loss is defined as the cosine similarity:\n\nwhere V fi and V si represent the facial and scene features, respectively. Finally, Due to the introduction of complex scene information, fixed prompts such as \"a photo of\" are no longer suitable for complex environmental scenes. Based on this, we use variable prompts that update during the training process. We compare the output features with the prompts using a contrastive loss function L cont to optimize the overall model, defined as:\n\nwhere sim denotes the similarity function, τ is a temperature scaling parameter, and N is the batch size.\n\nWhen the loss value exceeds α, we employ a global loss, which is the sum of similarity loss, polarity loss, and contrast loss:\n\nand when it is less than α, we only use the contrastive loss. This design helps to reduce the potential spatial distance between the scene and human features, and helps the model classify emotions effectively through polarity guidance and contrast loss. Besides, It helps our model converge quickly and find an appropriate solution space.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Type Fusion Encoder",
      "text": "In the fusion of facial and scene features, we designed a cross-type attention mechanism based on the attention mechanism. Unlike the block structure in transformers where attention is followed by normalization and then a feed-forward network, we first perform layer normalization on the facial and scene features used as keys and values. This is because we believe these features need to be normalized before extraction.\n\nFollowing this, when inputting into the multi-head attention mechanism, we use a shared Learnable Query (Qlearn), initialized with a Gaussian distribution, as the Query, with facial features V norm f and scene features V norm s as the Key and Value respectively. Qlearn is shared across all crosstype attention blocks. During training, as backpropagation progresses, Qlearn gradually denoises and learns to capture entities related to emotions from the facial and scene features, and filters the features that need attention from the Value matrix.\n\nLearnable Queries are crucial for capturing the specific emotional entities within the context of the scene and facial expressions. A learnable query as it gradually denoises and learns, it pays attention to the parts related to emotion, and filters out the parts that do not need to be noticed in the process of multiplying with the value matrix, leaving only the features that need attention. These are initialized with a Gaussian distribution N (µ, σ 2 ) to introduce variability and flexibility in capturing diverse features. The parameters of the Gaussian distribution are chosen to reflect the expected distribution of the features:\n\nwhere µ and σ 2 are the mean and variance of the Gaussian distribution, respectively. This initialization ensures that the model starts with a diverse set of queries that can adapt to various features during training. The output of the attention mechanism is then subjected to layer normalization:\n\nNext, the normalized output is passed through a Multi-Layer Perceptron (MLP) as the feed-forward network:\n\nThe MLP can be defined as:\n\nFinally, the output of the MLP is normalized again:\n\nThis design helps OUS effectively integrate facial and scene features. The layer normalization before the multi-head attention ensures that both types of features are on a similar scale, allowing the attention mechanism to more effectively learn the important features related to emotions. The use of a shared Learnable Query enables the model to focus on relevant entities and improve the emotional understanding of the scene and facial expressions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Prompt Engineering",
      "text": "The application of prompt engineering in our OUS is inspired by the methodologies of the CLIP and CoOp papers. In CLIP, a series of text prompts are trained to assist the model in understanding and categorizing image content, while CoOp further learns a series of continuous vectors (i.e., learnable prompts) for zero-shot or few-shot learning on a pre-trained CLIP model. In our model, learnable prompts are used to guide the model in associating image features with emotional states. Specifically, we define a set of learnable vectors as prompts that are combined with image features to compute the final emotion classification probabilities. These prompts undergo optimization along with image features during training, capturing important semantic connections in the emotion recognition task. Specifically, the prompts given to the text encoder are designed as follows:\n\nWhere each [V ] m (m ∈ {1, . . . , M }) is a vector of the same dimension as the word embeddings (i.e., 512 for CLIP ViT-B/32, 768 for CLIP ViT-L/14), and M is a hyperparameter specifying the number of context tokens  [35] .\n\nIn sum, prompt engineering provides our model with a flexible and powerful approach to understanding the complex relationships between images and textual prompts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Other Detail Of Network Architecture",
      "text": "Video segments {V | V ∈ R B×T ×C×H×W } are initially processed by a preprocessing block B p (•), resulting in the facial video sequence and the scene video sequence. Here, B, T , C, H, and W represent the batch size, number of frames, channels, height, and width of the video sequence, respectively. The preprocessing module includes a crucial component: the facial recognition module, which separates facial and environmental information within the video. The facial recognition module is set to a frozen state, meaning its weights remain unchanged during the training process, ensuring stability and consistency from video input to feature extraction.\n\nSubsequently, we encode the facial and environmental information into latent space facial features V f and scene features V s using a shared Vision Encoder. The weights we use are from ViT-L/14. Each input frame of the dual-stream facial features and scene features is processed independently. Frames are divided into N patches and then flattened into a Ddimensional latent space as follows:\n\nwhere v i p is the flattened patch vector, E is the embedding matrix, and E pos is the positional embedding matrix.\n\nThe temporal (T ) and batch (B) dimensions are merged into one dimension to form the facial features V f and the scene features V s , such that V ∈ R B•T ×N ×D and V ∈ R B•T ×N ×F , with F denoting the feature dimension.\n\nThe Vision Encoder remains frozen throughout. Next, we input the facial features V f and scene features V s into an LSTM for early feature fusion. The LSTM processes the input features as follows:\n\nwhere h represents the hidden state, h prev is the previous hidden state, x is the input feature vector (either V f or V s ), and σ is the activation function.\n\nThe facial temporal features are then processed to be V f t by a trainable Frames Encoder f f (•), designed to capture the dynamic nature of facial expressions over time. We first retransform the facial features back into tensor shape B × T × F and then input them into the Frames Encoder to be V st . Scene information, having less temporal variation, is extracted by computing the frames means:\n\nproviding a stable representation of the scene. We use convolutional layers and fully connected layers to align the latent spaces of facial V f t and scene features V st , reducing the distance between them. Similarity Loss L sim is used to update the Frame Encoder, convolutional layers, and fully connected layers.\n\nAdditionally, the features output from the initial attention blocks during encoding contain information related to color, texture, and overall ambiance of the environment, which are abstractly linked to emotions. We use a Polarity Encoder to encode these features to be V pol and Polarity Loss L pol to update the Polarity Encoder.\n\nThe output polarity features V pol are concatenated with the processed facial V f t and scene features V st and fed into the cross-type attention mechanism. The cross-type attention mechanism uses Learnable Queries Q learn as the Query, with facial and scene features as the Key and Value. The Learnable Queries Q learn are initially randomly initialized to a Gaussian distribution and shared across all cross-type attention blocks. During training, as backpropagation updates proceed, the Learnable Queries Q learn gradually denoise and learn to capture entities related to emotions in the V pol , V st , and V f t . The attention mechanism is defined as:\n\nwhere Q, K, and V represent the Query, Key, and Value matrices, respectively, and d k is the scaling factor. Finally, we believe that OUS introduces substantial scene information, making fixed prompt methods unsuitable. Instead, we use updatable prompts to compute contrastive loss L contrast with the output features.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiment",
      "text": "We aim to meticulously evaluate the performance of OUS in DFER tasks, conducted across two mainstream datasets in DFER: FERV39k and DFEW, which cover a wide range of real-world scenarios. Experiments on these datasets not only demonstrate our method's superior performance but also highlight the contributions of each module through ablation studies.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Training Details",
      "text": "OUS was trained in a computing environment with 4 NVIDIA GeForce RTX 3090 GPUs and an Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz. The training, based on CLIP's open-source code, utilized the Adam optimizer. With an initial learning rate of 0.002 and a batch size of 16, the model was trained for 60 epochs. The learning rate was reduced to a third of its value whenever the loss on the validation set didn't decrease for five consecutive epochs, and training was considered converged when the rate fell below 1e-7. The model was deemed overfitting if the training accuracy exceeded 80%. The final model saved was the one with the lowest loss on the validation set. The entire training process followed strict data preprocessing and augmentation protocols for reliability and reproducibility.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Performance Evaluation",
      "text": "OUS's performance was assessed on FERV39k and DFEW datasets, using weighted average recall (WAR) and unweighted  average recall (UAR) as the primary metrics. Compared to existing advanced methods, our results outlined in Tables I and II show OUS's superior performance. OUS has increased by 2.85% on the DFEW dataset than the current SOTA method, and an increase of 1.65% on the FERV39k dataset. Observing Fig.  5 , it is apparent that across the two datasets, the categories of Happiness, Sad, Surprise and Fear exhibit the highest recognition accuracy. Obviously, Disgust and Fear are samples with the lowest accuracy of classification. This is also a characteristic of the DFER field. Due to the serious long tail distribution effect of data sets, there are fewer expressions of dislikes and fear. Furthermore, as per the results in Table  I , these categories show the most significant improvement compared to other methods. This aligns with our initial hypothesis that considering contextual scene information can enhance recognition accuracy. The ease of acquiring and recognizing contextual cues for Happiness, Sad, Surprise and Fear, such as closely related body movements and either comfortable or tense environments, supports this assertion. Happiness, Sad, Surprise and Fear are easily confused expressions, which are difficult to classify in separate face recognition, but the classification accuracy increases greatly after injection of scene information, which also proves the existence of Rigid Cognitive Problem and the effective solution in OUS method.\n\nIn our comparative analysis, we meticulously reviewed DFER methods from the last decade, with a focus on the prevailing state-of-the-art methods. These include 3D Convolutional Neural Networks (C3D)  [36] , Inflated 3D ConvNet (I3D-RGB)  [37] , Pseudo-3D Residual Networks (P3D)  [38] , and various configurations of 3D ResNet18  [39] . We also evaluated methods based on the ResNet architecture, such as ResNet18 combined with LSTM networks  [40] , ResNet18 with Gated Recurrent Units (GRU), and ResNet18-ViT  [23] . Additionally, our review covered CLIP-based methods, notably CLIPER  [26] , DFER-CLIP  [41] , EmoCLIP  [42] , and other approaches like Former-DFER  [32] , LOGO-Former  [43] , NR-DFERNet  [44] , T-ESFL  [45] , DPCNet  [46] , IAL  [47] , M3DFEL  [48] , AEN  [49] , SW-FSCL  [50] , EC-STFL  [32] , and others. Cumulatively, our method demonstrated the highest effectiveness on the FERV39k and DFEW datasets, showcasing its robustness in the dynamic facial expression recognition domain.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Ablation Study",
      "text": "Module ablation. To ensure the fairness and rigor of our experiments, we performed ablation studies by selectively removing the multi-loss constraints and replacing the Type Fusion Encoder with average pooling of the scene and facial features. We compared three settings: using only the cross loss, using multi-loss constraints (similarity loss, polarity loss, and contrastive loss), and replacing the TFE with average pooling. We first examined the impact of the multi-loss constraints. The results, shown in Table IV and V, indicate that both the WAR and UAR significantly decreased when only the contrastive loss was used. This suggests that the interaction between the three losses is a crucial component of the model's effectiveness. When using only the cross loss, we observed a notable increase in convergence cycles and a decrease in performance. This may be because the solution space constrained by the three losses is easier to converge, highlighting the importance of the multi-loss strategy.\n\nWe investigated the impact of the TFE by replacing it with average pooling of the scene and facial features. The results, shown in Table  IV  and V, reveal a significant decrease in accuracy when the TFE is removed. This underscores the TFE's role in capturing the emotional context from facial and scene features for emotion prediction. The absence of the TFE module resulted in a marked reduction in both UAR and WAR, indicating that the TFE is essential for extracting emotionrelated connections between facial and scene features.\n\nWe also analyzed the convergence behavior of the model under different settings. Table  IV  and V illustrates the convergence cycles for the cross loss and multi-loss settings. The model using only the cross loss exhibited significantly slower convergence and lower final performance, as evidenced by the increased number of epochs required to achieve optimal performance. This further emphasizes the effectiveness of the multi-loss strategy in facilitating faster convergence and better performance.\n\nThe ablation studies highlight the importance of the multiloss constraints and the TFE in our OUS model. The significant drop in UAR and WAR when using only the cross loss or replacing the TFE with average pooling demonstrates that these components are integral to the model's ability to capture and predict emotions accurately. The multi-loss constraints provide a solution space that is easier to converge, thereby improving the model's performance. Similarly, the TFE plays a critical role in extracting emotion-related connections between facial and scene features, which are essential for accurate emotion prediction. Hyperparametric ablation. We performed extensive ablation studies to verify the superiority of our selected hyperparameters. The experiments are divided into two main parts: the investigation of prompt length and the combined ablation of TFE block numbers and prompt length. We explored different prompt lengths to identify the optimal setting. Our findings indicate a positive correlation between prompt length and method performance. Longer prompts provide more flexibility and better capacity to model the additional scene information. The detailed results are presented in    improving the model's accuracy and robustness. We also conducted combined ablation studies on the number of TFE blocks and the prompt length. The purpose was to determine the best combination that maximizes model performance. The results, detailed in Table  III , demonstrate that the model performs optimally when the TFE block number is set to 12 and the prompt length is 64. This configuration effectively captures the nuanced interactions between facial and scene features, enhancing the overall emotion recognition accuracy.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "D. Discussion",
      "text": "We compare the performance of OUS with other stateof-the-art (SOTA) methods on the DFEW dataset for sevenclass classification. The results are presented in Table  I . The performance of OUS on the DFEW and FERV39k datasets is summarized in Table  II . We compare our method against several baseline models, highlighting the superior performance of OUS. The results in Table I reveal that the inclusion of scene information significantly improves the classification of happiness, sadness, and fear. This aligns with our initial hypothesis that these emotions have a strong correlation with the surrounding scene context and body movements. Tables I and II provide evidence that our method outperforms existing state-of-the-art DFER approaches over the past decade. In the Happy classification, OUS is 4.05% higher than the SOTA method, 6.71% higher than the current SOTA method in Sad classification, 5.05% higher than the current SOTA method than the current SOTA method in Neutral, and 1.73% higher than the current SOTA method in Surprise. 12.35% of the current SOTA method on Fear. All in all, OUS leads by 2.85% on the DFEW dataset and 1.65% on the FERV39k dataset compared to current SOTA methods. As illustrated in Table  IV  and Table IV-C, there is a positive correlation between the length of learnable prompts and model accuracy. It is noteworthy that models using only the cross loss function without the TFE module performed the worst. In contrast, our model with three loss constraints and the TFE module showed a performance improvement of 7.99%, and the convergence period reduced from 15 epochs to 6 epochs. As Shown in Fig.  6 , whether on DFEW or FERV39k dataset, the Loss and Accurate of the train and test set are completely converged within 5K steps, that is, in the 6 epochs can reach approximate convergence, and the score all reach SOTA level, surpassing the current SOTA method. Furthermore, as shown in Fig.  4 , first look at the (a) figure. This is the feature distribution of the images of the DFEW and FERV39k data sets in the Vision Encoder. Because the Vision Encoder is frozen, the features are evenly distributed. The preliminary gathering phenomenon of features after entering Frame Encoder is preliminary in (b) figure. This is due to the optimization of the Similarity Loss, which makes the face characteristics gather. The features in the figure Due to the optimization of Multi-loss, the overall gathering is more obvious in (c) figure. (d) Show the powerful gathering effect of the Type Fusion Encoder we designed. The features are significantly clustered after passing through the TFE module, substantiating the substantial contribution of our three loss constraints and the TFE module to the effectiveness of OUS. This also confirms our hypothesis that the injection of polarity helps to classify emotions.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we identified and analyzed the Rigid Cognitive Problem, a prevalent issue and methodological bias in DFER tasks. To address this problem, we designed a dynamic facial expression recognition method called Overall Understanding of the Scene (OUS), which effectively aligns and integrates scene information to extract emotional knowledge from complex features. The method primarily utilizes multiple loss constraints to reduce the latent space distance between scene and human features and efficiently classify emotions. Additionally, the Type Fusion Encoder (TFE) with a crosstype attention mechanism and learnable queries effectively integrates scene and facial information. Extensive experiments conducted on the DFEW and FERV39k datasets demonstrated significant improvements over existing methods, validating the robustness and effectiveness of our approach. Overall, We have discovered and analyzed the Rigid Cognitive Problem and proposed the OUS method to address this issue. The superior performance of OUS illustrates the importance of utilizing scene information in the DFER task, and we hope our work can inspire other researchers.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Human annotators instinctively combine scene information and faces",
      "page": 1
    },
    {
      "caption": "Figure 2: , an example of the Rigid",
      "page": 1
    },
    {
      "caption": "Figure 1: The Rigid Cognitive",
      "page": 1
    },
    {
      "caption": "Figure 1: above, when analyzing emotions, human beings",
      "page": 1
    },
    {
      "caption": "Figure 2: Scene polarity helps determine mood. The left side shows facial",
      "page": 2
    },
    {
      "caption": "Figure 3: OUS Overall Architecture Diagram. OUS employs a dual-stream structure, separating images into facial and scene images through preprocessing.",
      "page": 3
    },
    {
      "caption": "Figure 3: OUS mainly includes spatial encoding, temporal encoding,",
      "page": 4
    },
    {
      "caption": "Figure 4: Changes in Feature Clustering During the Inference Process. Global layout visualization of the feature space. In the legend, 0 to 6 represent",
      "page": 6
    },
    {
      "caption": "Figure 5: , it is apparent that across the two datasets,",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion Matrices on the DFEW and FERV39k Dataset.",
      "page": 7
    },
    {
      "caption": "Figure 6: Loss and Accurate in the process of OUS training In DFEW and FERV39k Dataset.",
      "page": 8
    },
    {
      "caption": "Figure 6: , whether on DFEW or FERV39k dataset, the Loss and",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "N\no": "mr"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "mr L\nP",
          "M": "L\nP"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6(Ours)": "",
          "12": "16 16\n16 32\n16 64",
          "64": "",
          "0.60937": "",
          "0.74103": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The perception of emotion in body expressions",
      "authors": [
        "B Gelder",
        "A De Borst",
        "R Watson"
      ],
      "year": "2015",
      "venue": "Wiley Interdisciplinary Reviews: Cognitive Science"
    },
    {
      "citation_id": "2",
      "title": "Body language: Embodied perception of emotion",
      "authors": [
        "C Sinke",
        "M Kret",
        "B De Gelder"
      ],
      "year": "2013",
      "venue": "Measurement With Persons"
    },
    {
      "citation_id": "3",
      "title": "Recognition of facial expressions is influenced by emotional scene gist",
      "authors": [
        "R Righart",
        "B Gelder"
      ],
      "year": "2008",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience"
    },
    {
      "citation_id": "4",
      "title": "Blinded by emotion? effect of the emotionality of a scene on susceptibility to false memories",
      "authors": [
        "S Porter",
        "L Spencer",
        "A Birt"
      ],
      "year": "2003",
      "venue": "Canadian Journal of Behavioural Science/Revue canadienne des sciences du comportement"
    },
    {
      "citation_id": "5",
      "title": "Emotional perception: meta-analyses of face and natural scene processing",
      "authors": [
        "D Sabatinelli",
        "E Fortune",
        "Q Li",
        "A Siddiqui",
        "C Krafft",
        "W Oliver",
        "S Beck",
        "J Jeffries"
      ],
      "year": "2011",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "6",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1971",
      "venue": "Nebraska symposium on motivation"
    },
    {
      "citation_id": "7",
      "title": "Cognitive, social, and physiological determinants of emotional state",
      "authors": [
        "S Schachter",
        "J Singer"
      ],
      "year": "1962",
      "venue": "Psychological review"
    },
    {
      "citation_id": "8",
      "title": "Body-Language-Communication",
      "authors": [
        "C Müller",
        "A Cienki",
        "E Fricke",
        "S Ladewig",
        "D Mcneill",
        "S Tessendorf"
      ],
      "year": "2013",
      "venue": "Body-Language-Communication"
    },
    {
      "citation_id": "9",
      "title": "Social interaction context shapes emotion recognition through body language, not facial expressions",
      "authors": [
        "L Abramson",
        "R Petranker",
        "I Marom",
        "H Aviezer"
      ],
      "year": "2021",
      "venue": "Emotion"
    },
    {
      "citation_id": "10",
      "title": "Tracking changes in continuous emotion states using body language and prosodic cues",
      "authors": [
        "A Metallinou",
        "A Katsamanis",
        "Y Wang",
        "S Narayanan"
      ],
      "year": "2011",
      "venue": "2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Ecological psychology: Concepts and methods for studying the environment of human behavior",
      "authors": [
        "E Hall"
      ],
      "year": "1969",
      "venue": "Ecological psychology: Concepts and methods for studying the environment of human behavior"
    },
    {
      "citation_id": "12",
      "title": "Nonverbal communication",
      "authors": [
        "S Duncan"
      ],
      "year": "1969",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "13",
      "title": "Nonverbal behavior and nonverbal communication",
      "authors": [
        "M Wiener",
        "S Devoe",
        "S Rubinow",
        "J Geller"
      ],
      "year": "1972",
      "venue": "Psychological review"
    },
    {
      "citation_id": "14",
      "title": "Cultural aspects of nonverbal communication",
      "authors": [
        "M Lafrance",
        "C Mayo"
      ],
      "year": "1978",
      "venue": "International Journal of Intercultural Relations"
    },
    {
      "citation_id": "15",
      "title": "Body, language, and mind",
      "authors": [
        "T Ziemke",
        "J Zlatev",
        "R Frank"
      ],
      "year": "2007",
      "venue": "Body, language, and mind"
    },
    {
      "citation_id": "16",
      "title": "Towards an affect space for robots to display emotional body language",
      "authors": [
        "A Beck",
        "L Cañamero",
        "K Bard"
      ],
      "year": "2010",
      "venue": "19th International symposium in robot and human interactive communication"
    },
    {
      "citation_id": "17",
      "title": "Real-life dynamic facial expression recognition: a review",
      "authors": [
        "S Saleem",
        "S Zeebaree",
        "M Abdulrazzaq"
      ],
      "year": "2021",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "18",
      "title": "Dynamic facial expression recognition using longitudinal facial expression atlases",
      "authors": [
        "Y Guo",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2012",
      "venue": "Computer Vision-ECCV 2012: 12th European Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Facial expression recognition in dynamic sequences: An integrated approach",
      "authors": [
        "H Fang",
        "N Mac Parthaláin",
        "A Aubrey",
        "G Tam",
        "R Borgo",
        "P Rosin",
        "P Grant",
        "D Marshall",
        "M Chen"
      ],
      "year": "2014",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "The development of dynamic facial expression recognition at different intensities in 4-to 18-year-olds",
      "authors": [
        "R Montirosso",
        "M Peverelli",
        "E Frigerio",
        "M Crespi",
        "R Borgatti"
      ],
      "year": "2010",
      "venue": "Social Development"
    },
    {
      "citation_id": "21",
      "title": "Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition",
      "authors": [
        "M Liu",
        "S Shan",
        "R Wang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "Freq-hd: An interpretable frequency-based high-dynamics affective clip selection method for in-the-wild facial expression recognition in videos",
      "authors": [
        "Z Tao",
        "Y Wang",
        "Z Chen",
        "B Wang",
        "S Yan",
        "K Jiang",
        "S Gao",
        "W Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, ser. MM '23",
      "doi": "10.1145/3581783.3611972"
    },
    {
      "citation_id": "23",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "24",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "26",
      "title": "Cliper: A unified vision-language framework for in-the-wild facial expression recognition",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Cliper: A unified vision-language framework for in-the-wild facial expression recognition",
      "arxiv": "arXiv:2303.00193"
    },
    {
      "citation_id": "27",
      "title": "A3lign-dfer: Pioneering comprehensive dynamic affective alignment for dynamic facial expression recognition with clip",
      "authors": [
        "Z Tao",
        "Y Wang",
        "J Lin",
        "H Wang",
        "X Mai",
        "J Yu",
        "X Tong",
        "Z Zhou",
        "S Yan",
        "Q Zhao"
      ],
      "year": "2024",
      "venue": "A3lign-dfer: Pioneering comprehensive dynamic affective alignment for dynamic facial expression recognition with clip",
      "arxiv": "arXiv:2403.04294"
    },
    {
      "citation_id": "28",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "29",
      "title": "Landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition",
      "authors": [
        "D Gera",
        "S Balasubramanian"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "30",
      "title": "Benchmarking facial image analysis technologies (befit)",
      "authors": [
        "H Ekenel"
      ],
      "year": "2012",
      "venue": "2012 3rd International Conference on Image Processing Theory, Tools and Applications (IPTA)"
    },
    {
      "citation_id": "31",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC"
    },
    {
      "citation_id": "32",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "33",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Dynamic facial expression recognition with atlas construction and sparse representation",
      "authors": [
        "Y Guo",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "35",
      "title": "Conditional prompt learning for vision-language models",
      "authors": [
        "K Zhou",
        "J Yang",
        "C Loy",
        "Z Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "37",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Learning spatio-temporal representation with pseudo-3d residual networks",
      "authors": [
        "Z Qiu",
        "T Yao",
        "T Mei"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "39",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "41",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Z Zhao",
        "I Patras"
      ],
      "year": "2023",
      "venue": "Prompting visual-language models for dynamic facial expression recognition",
      "arxiv": "arXiv:2308.13382"
    },
    {
      "citation_id": "42",
      "title": "Emoclip: A vision-language method for zero-shot video facial expression recognition",
      "authors": [
        "N Foteinopoulou",
        "I Patras"
      ],
      "year": "2023",
      "venue": "Emoclip: A vision-language method for zero-shot video facial expression recognition",
      "arxiv": "arXiv:2310.16640"
    },
    {
      "citation_id": "43",
      "title": "Logo-former: Local-global spatio-temporal transformer for dynamic facial expression recognition",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "authors": [
        "H Li",
        "M Sui",
        "Z Zhu"
      ],
      "year": "2022",
      "venue": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "arxiv": "arXiv:2206.04975"
    },
    {
      "citation_id": "45",
      "title": "Scaling languageimage pre-training via masking",
      "authors": [
        "Y Li",
        "H Fan",
        "R Hu",
        "C Feichtenhofer",
        "K He"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Dpcnet: Dual path multi-excitation collaborative network for facial expression representation learning in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "W Song",
        "S Gao",
        "Y Huang",
        "Z Chen",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "47",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "48",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "49",
      "title": "Frame level emotion guided dynamic facial expression recognition with emotion grouping",
      "authors": [
        "B Lee",
        "H Shin",
        "B Ku",
        "H Ko"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Empower smart cities with sampling-wise dynamic facial expression recognition via frame-sequence contrastive learning",
      "authors": [
        "S Yan",
        "Y Wang",
        "X Mai",
        "Q Zhao",
        "W Song",
        "J Huang",
        "Z Tao",
        "H Wang",
        "S Gao",
        "W Zhang"
      ],
      "year": "2023",
      "venue": "Computer Communications"
    }
  ]
}