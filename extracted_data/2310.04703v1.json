{
  "paper_id": "2310.04703v1",
  "title": "Integrating Contrastive Learning Into A Multitask Transformer Model For Effective Domain Adaptation",
  "published": "2023-10-07T06:41:29Z",
  "authors": [
    "Chung-Soo Ahn",
    "Jagath C. Rajapakse",
    "Rajib Rana"
  ],
  "keywords": [
    "Contrastive learning",
    "cross-corpus adaptation",
    "domain adaptation",
    "speech emotion recognition",
    "transformers"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While speech emotion recognition (SER) research has made significant progress, achieving generalization across various corpora continues to pose a problem. We propose a novel domain adaptation technique that embodies a multitask framework with SER as the primary task, and contrastive learning and information maximisation loss as auxiliary tasks, underpinned by fine-tuning of transformers pre-trained on large language models. Empirical results obtained through experiments on well-established datasets like IEMOCAP and MSP-IMPROV, illustrate that our proposed model achieves stateof-the-art performance in SER within cross-corpus scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion recognition (SER) has been an active research area for decades, but generalization across multiple corpora has not been fully addressed. Existing SER methods perform poorly when tested on data from different sources, especially when dataset sizes are different. This motivates us to propose a generalized approach for domain adaptation for SER, overcoming the limitations of data constraints.\n\nA substantial amount of progress has been made through pre-trained transformer models in speech recognition and audio representation learning  [1] . Moreover, it is verified that information in pre-trained transformers on language models is easily transferable to emotion recognition  [2] . In this paper, we intend to tackle cross-corpus SER by exploiting rich knowledge embedded in pre-trained transformers to enhance the generalizability of the models. Here, we address the crosscorpus SER problem where the emotion recognition model is trained on one corpus and tested on a different corpus.\n\nContrastive learning is one of the recent successful paradigms in self-supervised learning that can learn structures between data when sample sizes are small  [3] . Its * correspondence to asjagath@ntu.edu.sg applicability to domain adaptation has started to gain popularity as well  [4] . Yet, contrastive learning is under-explored in the cross-corpus SER problem. In particular, how it can be effectively used for domain adaption in SER needs further exploration.\n\nInformation Maximization (IM) loss follows cluster assumption  [5] , that a better classifier will learn to find class boundaries to have large margins. IM loss is optimized without labels, simply optimizing entropy computed from only logits. It has strong potential to be used in domain adaptation in SER.\n\nIn this paper, we address all the above challenges. The contributions of this paper are as follows:\n\n1. We propose a multitask framework for domain adaptation in SER, underpinned by a pre-trained transformer, where SER is the primary task and contrastive learning for learning structure of the source and target corpus data are used as secondary task.\n\n2. We add Information Maximization (IM) loss for clustering another secondary task as it can explicitly learn the cluster structure when used together with contrastive learning.\n\n3. We test our model on the cross-corpus experiment, where the model trained on IEMOCAP  [6]  and tested on MSP-IMPROV  [7]  and achieve state-of-the-art performance with 10% improvement.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In this section, we discuss the literature while clustering them into three groups. In the first group, we present the studies that introduce multitask learning for domain adaptation in SER. Semi-supervised learning which makes use of unsupervised learning objectives along with emotion classification objective has been proposed  [11] . Such is ladder network that uses layer-wise reconstruction loss that allows usage of additional unlabelled data. Auto-encoder type reconstruction loss was also attempted  [12, 13] . Another alternative that requires reconstruction loss is GAN-based approaches which also is implemented together with synthetic data generation to aid training  [14, 15] . Additional learning objective can be achieved by other labels, such as languages, which relatively easier to obtain and can be used as auxiliary task  [16] .\n\nIn the second group, we present studies using pre-trained transformer for cross-corpus SER. Thanks to recent advancement in self-supervised learning with pre-text tasks, such as Wav2Vec2  [1] , its knowledge transfer to emotion has become more popular  [2] . Following the trend, pre-trained transformer was combined with domain adversarial learning  [8]  to achieve cross-corpus SER. VGGish transformer with spectrogram input was used to cross-corpus SER  [9] , where the model was pre-trained on speaker recogition task and its transferred knowledge helped domain adaptation. It is also common to only take resulting embedding from pre-trained transformer as input and build new transformer and train from scratch  [10] .\n\nSummarising the existing studies, we note that none of the existing studies have used a pre-trained transformer within a multitask learning framework while using contrastive learning as a secondary task to improve the accuracy of the primary SER tasks in a cross-corpus setting. This confirms the novelty of our approach in contrast to the existing literature.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "Wav2Vec2  [1]  is adopted and two parallel stream is implemented, while sharing same weights. For each speech sample, two augmented speech is generated and fed into those two parallel transformers, as shown in Fig.  1 . Our method also adopts multitask learning framework, that emotion classification layer is on top of transformer and other layers for other auxiliary tasks. Both source data and target data for training will be used to compute loss and train the model through back propagation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Wav2Vec2 For Ser",
      "text": "Wav2Vec2  [1]  is self-supervised learning model that takes input from raw waveform data of speech. It is consisted of convolution layers for feature extraction and transformer layers. Transformer layers are consisted of 12 layers of transformer modules. Inspired from Yang et al.  [2] , we take representations from every transformer layer (total 13 representations) and compute their weighted sum which is learned during training. The output of a transformer is average-pooled and fed to followed by a softmax layer.\n\nThe emotion class label c given the weighted output h of the pre-trained transformer is given by\n\nThe weights W and biases b of the fully connected layers are learned during training by minimizing the cross-entropy loss L emo :\n\nwhere y depicts the target emotion label, 1 c the indicater function and E computes expectation over the samples. .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Contrastive learning is aimed at learning to attract positive pairs of inputs and repel negative pairs. Most common and popular variants now uses InfoNCE  [3]  loss to achieve this. We follow the framework from Chen et al.  [17] , generating latent variable z by feeding h through single hidden layer feedforward network,\n\nwhere σ is ReLU activation. It is known that contrastive learning is more effective with z than h  [17] . With this setting, we compute InfoNCE loss between representations z and z ′ as follows:\n\nz i refers to i th sample of latent variable z in the mini batch and z ′ i refers to the z from the forward pass of other augment sample. Hyperparameter τ is temperature  [18]  which controls the smoothness of softmax function. cosine sim refers to cosine similarity score 1  Attracting similar data points and repelling dissimlar data points can reveal structure of the dataset. We intend to achieve clustering effect on both source and target data at the same time.\n\n1 cosine sim(a, b) = ab ∥a∥∥b∥",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Information Maximization Loss",
      "text": "Additional auxilliary task is introduced in our work, infomation maximization loss. This is inspired from Krause et al.  [5]  and Liang et al.  [19] . Information Maximization (IM) loss follows cluster assumption  [5] , that better classifier will learn to find class boundaries to have large margins. IM loss is optimized without labels, simply optimizing entropy computed from only logits. However, this might lead to trivial solution where every data samples collapse into single class.  [5]  proposed uniform distribution contraint for each cluster (or class).\n\nFinally, we compute IM loss by computing expected entropy subtracted by empirical label's entropy.\n\nwhere empirical label distribution (simply average of softmax output from all data points) as\n\nWe use IM loss to aid contrastive learning in forming clusters. IM loss explicitly learns to form wide separation margin, thus capable for our needs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation And Augmentation Classification Loss",
      "text": "Data augmentation is important component in our model as it increases data size for transformers to learn avoiding overfitting. Also data augmentation is essential for contrastive learning as it requires different views of the same sample to learn similarities between two augmentations. We exploited augmentation tool (https://github.com/asteroid-team/torchaudiomentations) to augment raw audio waveforms. The augmentation function that we adopted are: Gain, Polority-Inversion, Shift, TimeInversion, BandStopFilter, PeakNormalization and AddColoredNoise. All these functions add perturbation to the waveforms and we used mixtures of them to create five pipelines of augmentation.\n\nWhen implementing data augmentation pipeline, we also assign labels to the data stating which pipeline was this input is perturbed. With this additional label provided, we add another classification task as auxiliary task and compute its loss as:\n\nThe label y a represents from which augmentation pipeline this input came from. This classification layer is separate from emotion classification layer, thus learnable weights W ′ and b ′ are not same with W and b.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multitask Learning For Domain Adaptation",
      "text": "Finally, all the above mentioned losses are summed up for final loss function for training. L total = L emo + λ 1 L aug + λ 2 L cont + λ 3 L IM where λ 1 ,λ 2 , and λ 3 are hyperparamter constants to control importance of each loss and to be determined empirically. This loss is computed same regardless of input coming from source or target. Our work is assuming that small fraction of target data with label is available. If label is not available L emo can be dropped during training with target data. Detailed procedure of our domain adaptation can be referred to subsection 4.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "In this subsection, we briefly describe the two datasets used for cross-corpus SER experiments. As transformers take raw waveform as inputs, there is no feature extraction.\n\n1. IEMOCAP  [6] : IEMOCAP dataset consists of 12 hours conversation between two actors. Total 10 actors were recruited to record five sessions. In this work, we focused on binary emotions: neutral, happy, and excited are labelled as positive; and sad and angry as negative.\n\nRecordings with other emotion labels are excluded.\n\n2. MSP-IMPROV  [7] : MSP-IMPROV dataset is constructed similar to IEMOCAP dataset but with 12 actors and six sessions, which has a relatively larger size than IEMOCAP. This dataset only have four emotion labels: neutral, happy, angry and sad. This labels are converted to binary similar to IEMOCAP dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Domain Adaptation Experiment",
      "text": "Our work is aimed at situations where only a small fraction of target data is available with labels to access. IEMOCAP dataset was used as source data and 90% of the data was used as training set (denoted by S tr ) and remaining 10% was reserved as validation (denoted by S va ) which was used for early stopping. From target dataset MSP-IMPROV, 5% of Table  1 . Performance comparison with existing methods",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Uar",
      "text": "CNN-BLSTM  [20]  59.52% CNN-BLSTM+DANN+CenterLoss  [20]  57.26% CNN-LSTM  [8]  55.73% DoGAT  [8]  59.42% Ours (without labels) 59.39% Ours 69.25%\n\ndata was taken with labels as the training set (denoted by T tr ) and remaining 95% was reserved for testing set (denoted by T te ). And we also experimented with a situation where labels was considered unavailable for all target data. In this case, we set 30% and 70% ratio of the MSP-IMPROV dataset for T tr and T te , respectively. Domain adaptation was tested by training on S tr and T tr for one epoch alternatively. After each epoch, emotion classification accuracy was measured on S va . When the accuracy did not improve from previous epoch, training was halted and UAR (Unweighted Average Recall) on T te was recorded.\n\nWe empirically set the hyperparameters of the loss: λ 1 = 0.1, λ 2 = 0.5 and λ 3 = 0.5. Pre-trained transformers with name 'facebook/wav2vec2-base-960h' (downloaded from https://huggingface.co/facebook/wav2vec2-base-960h) which was trained in Wav2Vec2 with hidden layer size of 768; and we set size for U and V as 256 and 128, respectively. Learning rate was 1 × 10 -4 and decayed on plateau by factor of 0.1 with patience of 5. Validation accuracy was measured for learning rate decay and the epoch (separate from early stopping validation) was set as 100 batches for source dataset and 25 batches for target dataset with labels. When target data was without labels, epoch was set as same with the source and 100 batches were evaluated to decide on learning rate decay.\n\nOur experiment results is presented in Table .1. Our method outperforms the state-of-the-art methods by about 10%. Even more, our model performs on par with state-ofthe-art methods when trained on target data without labels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Atudy",
      "text": "Our ablation study focused on cases where no labels are available, to clearly examine generalizability of our model. We also tested contribution of component losses in the cost function. Contribution of L cont was the largest and L aug was the least. Especially, L aug had negligible impact (UAR decreased by 0.2% when removed). We believe that L cont learns to attract representation from similar data samples and repel representation from dissimilar data samples, thus learning the overall structure of the dataset. And L IM enforces representation to have larger separation margin, resulting in localizing representations from the same cluster in to a dense region.  We also visualized the data in the latent space in Figure .2 to test the effect of each loss component. Each marker represents h computed for data from T te without L aug and differentiated per class. It is notifiable that representations are well dispersed thanks to contrastive learning. The edge region of scatter plot is denser than center and each class is more concentrated either left-end or right-end, thanks to information maximization loss. Finally, contrastive learning and information maximization proves to be effective in domain adaptation as it did not exploit labels of target data to achieve this.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed to incorporate contrastive learning via multitask learning for domain adaptation in SER. Earlier, contrastive learning was rarely studied on domain adaption problem in context of SER. We proposed to train a transformer model with contrastive learning and other tasks as auxiliary tasks on top of emotion classification. Our experiments demonstrated that multitask learning with contrastive learning was able to learn the structure from data without labels by attracting similar and repelling dissimilar data. Furthermore, information maximization turned out to be capable of pushing representations of different classes to the edge region of different directions for separation. Our experiments showed that our model is able to achieve on par with state-of-the-art in domain adaption without labels. Moreover, our model improved 10% above state-of-the-art with only 5% of the target dataset with labels.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overall architecture of contrastive learning on speech",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualizing h computed from Tte from model without",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Computer Science and Engineering": "Nanyang Technological University, Singapore",
          "School of Mathematics, Physics and Computing": "University of Southern Queensland, Australia"
        },
        {
          "School of Computer Science and Engineering": "ABSTRACT",
          "School of Mathematics, Physics and Computing": "applicability to domain adaptation has started to gain popu-"
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "larity as well [4]. Yet, contrastive learning is under-explored"
        },
        {
          "School of Computer Science and Engineering": "While speech emotion recognition (SER) research has made",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "in the cross-corpus SER problem.\nIn particular, how it can"
        },
        {
          "School of Computer Science and Engineering": "significant progress, achieving generalization across various",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "be effectively used for domain adaption in SER needs further"
        },
        {
          "School of Computer Science and Engineering": "corpora continues to pose a problem. We propose a novel do-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "exploration."
        },
        {
          "School of Computer Science and Engineering": "main adaptation technique that embodies a multitask frame-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "Information Maximization (IM)\nloss\nfollows cluster as-"
        },
        {
          "School of Computer Science and Engineering": "work with SER as\nthe primary task, and contrastive learn-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "sumption [5],\nthat a better classifier will\nlearn to find class"
        },
        {
          "School of Computer Science and Engineering": "ing and information maximisation loss as auxiliary tasks, un-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "boundaries to have large margins.\nIM loss is optimized with-"
        },
        {
          "School of Computer Science and Engineering": "derpinned by fine-tuning of transformers pre-trained on large",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "out\nlabels,\nsimply optimizing entropy computed from only"
        },
        {
          "School of Computer Science and Engineering": "language models. Empirical results obtained through experi-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "logits.\nIt has strong potential\nto be used in domain adapta-"
        },
        {
          "School of Computer Science and Engineering": "ments on well-established datasets like IEMOCAP and MSP-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "tion in SER."
        },
        {
          "School of Computer Science and Engineering": "IMPROV,\nillustrate that our proposed model achieves state-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "In this paper, we address all\nthe above challenges.\nThe"
        },
        {
          "School of Computer Science and Engineering": "of-the-art performance in SER within cross-corpus scenarios.",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "contributions of this paper are as follows:"
        },
        {
          "School of Computer Science and Engineering": "Index Terms— Contrastive learning, cross-corpus adap-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "1. We propose a multitask framework for domain adapta-"
        },
        {
          "School of Computer Science and Engineering": "tation, domain adaptation, speech emotion recognition, trans-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "tion in SER, underpinned by a pre-trained transformer,"
        },
        {
          "School of Computer Science and Engineering": "formers",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "where SER is the primary task and contrastive learning"
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "for\nlearning structure of\nthe source and target corpus"
        },
        {
          "School of Computer Science and Engineering": "1.\nINTRODUCTION",
          "School of Mathematics, Physics and Computing": "data are used as secondary task."
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "2. We add Information Maximization (IM) loss for clus-"
        },
        {
          "School of Computer Science and Engineering": "Speech Emotion recognition (SER) has been an active re-",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "tering another secondary task as it can explicitly learn"
        },
        {
          "School of Computer Science and Engineering": "search area for decades, but generalization across multiple",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "the\ncluster\nstructure when\nused\ntogether with\ncon-"
        },
        {
          "School of Computer Science and Engineering": "corpora has not been fully addressed. Existing SER methods",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "trastive learning."
        },
        {
          "School of Computer Science and Engineering": "perform poorly when tested on data from different sources,",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "especially when dataset sizes are different. This motivates us",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "3. We\ntest our model on the\ncross-corpus\nexperiment,"
        },
        {
          "School of Computer Science and Engineering": "to propose a generalized approach for domain adaptation for",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "where the model\ntrained on IEMOCAP[6] and tested"
        },
        {
          "School of Computer Science and Engineering": "SER, overcoming the limitations of data constraints.",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "on MSP-IMPROV[7] and achieve state-of-the-art per-"
        },
        {
          "School of Computer Science and Engineering": "A substantial amount of progress has been made through",
          "School of Mathematics, Physics and Computing": ""
        },
        {
          "School of Computer Science and Engineering": "",
          "School of Mathematics, Physics and Computing": "formance with 10% improvement."
        },
        {
          "School of Computer Science and Engineering": "pre-trained transformer models in speech recognition and au-",
          "School of Mathematics, Physics and Computing": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.1. Wav2Vec2 for SER": "Wav2Vec2 [1] is self-supervised learning model that takes in-"
        },
        {
          "3.1. Wav2Vec2 for SER": "put\nfrom raw waveform data of\nspeech.\nIt\nis consisted of"
        },
        {
          "3.1. Wav2Vec2 for SER": "convolution layers for feature extraction and transformer lay-"
        },
        {
          "3.1. Wav2Vec2 for SER": "ers.\nTransformer\nlayers are consisted of 12 layers of\ntrans-"
        },
        {
          "3.1. Wav2Vec2 for SER": "former modules. Inspired from Yang et al. [2], we take repre-"
        },
        {
          "3.1. Wav2Vec2 for SER": "sentations from every transformer layer (total 13 representa-"
        },
        {
          "3.1. Wav2Vec2 for SER": "tions) and compute their weighted sum which is learned dur-"
        },
        {
          "3.1. Wav2Vec2 for SER": "ing training. The output of a transformer\nis average-pooled"
        },
        {
          "3.1. Wav2Vec2 for SER": "and fed to followed by a softmax layer."
        },
        {
          "3.1. Wav2Vec2 for SER": "The emotion class label c given the weighted output h of"
        },
        {
          "3.1. Wav2Vec2 for SER": "the pre-trained transformer is given by"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "p (c|h) = softmax(W h + b),\n(1)"
        },
        {
          "3.1. Wav2Vec2 for SER": "The weights W and biases b of the fully connected layers are"
        },
        {
          "3.1. Wav2Vec2 for SER": "learned during training by minimizing the cross-entropy loss"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "Lemo:"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "(cid:88) c\n(2)\nLemo = −E{\n1c(y)log(p(c|h))}."
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "where y depicts the target emotion label, 1c the indicater func-"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "tion and E computes expectation over the samples.\n."
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "3.2. Contrastive learning"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "Contrastive learning is aimed at\nlearning to attract positive"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "pairs of\ninputs and repel negative pairs. Most common and"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "popular variants now uses InfoNCE [3] loss to achieve this."
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "We follow the framework from Chen et al.[17], generating la-"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "tent variable z by feeding h through single hidden layer feed-"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "forward network,"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "z = V σ(U h),\n(3)"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        },
        {
          "3.1. Wav2Vec2 for SER": "where σ is ReLU activation. It is known that contrastive learn-"
        },
        {
          "3.1. Wav2Vec2 for SER": "ing is more effective with z than h[17]. With this setting, we"
        },
        {
          "3.1. Wav2Vec2 for SER": "compute InfoNCE loss between representations z and z′ as"
        },
        {
          "3.1. Wav2Vec2 for SER": "follows:"
        },
        {
          "3.1. Wav2Vec2 for SER": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.3.\nInformation Maximization loss": "Additional auxilliary task is introduced in our work,\ninfoma-",
          "3.5. Multitask Learning for domain adaptation": "Finally, all the above mentioned losses are summed up for fi-"
        },
        {
          "3.3.\nInformation Maximization loss": "tion maximization loss.\nThis is inspired from Krause et al.",
          "3.5. Multitask Learning for domain adaptation": "nal\nloss function for\ntraining. Ltotal = Lemo + λ1Laug +"
        },
        {
          "3.3.\nInformation Maximization loss": "[5] and Liang et al.[19]. Information Maximization (IM) loss",
          "3.5. Multitask Learning for domain adaptation": "λ2Lcont + λ3LIM where λ1,λ2, and λ3 are hyperparamter"
        },
        {
          "3.3.\nInformation Maximization loss": "follows cluster assumption [5], that better classifier will learn",
          "3.5. Multitask Learning for domain adaptation": "constants to control\nimportance of each loss and to be deter-"
        },
        {
          "3.3.\nInformation Maximization loss": "to find class boundaries to have large margins.\nIM loss is op-",
          "3.5. Multitask Learning for domain adaptation": "mined empirically."
        },
        {
          "3.3.\nInformation Maximization loss": "timized without\nlabels, simply optimizing entropy computed",
          "3.5. Multitask Learning for domain adaptation": "This loss is computed same regardless of\ninput coming"
        },
        {
          "3.3.\nInformation Maximization loss": "from only logits.\nHowever,\nthis might\nlead to trivial\nsolu-",
          "3.5. Multitask Learning for domain adaptation": "from source or target. Our work is assuming that small frac-"
        },
        {
          "3.3.\nInformation Maximization loss": "tion where every data samples collapse into single class.\n[5]",
          "3.5. Multitask Learning for domain adaptation": "tion of target data with label is available.\nIf label is not avail-"
        },
        {
          "3.3.\nInformation Maximization loss": "proposed uniform distribution contraint\nfor each cluster\n(or",
          "3.5. Multitask Learning for domain adaptation": "able Lemo can be dropped during training with target data."
        },
        {
          "3.3.\nInformation Maximization loss": "class).",
          "3.5. Multitask Learning for domain adaptation": "Detailed procedure of our domain adaptation can be referred"
        },
        {
          "3.3.\nInformation Maximization loss": "",
          "3.5. Multitask Learning for domain adaptation": "to subsection 4.2."
        },
        {
          "3.3.\nInformation Maximization loss": "Finally, we compute IM loss by computing expected en-",
          "3.5. Multitask Learning for domain adaptation": ""
        },
        {
          "3.3.\nInformation Maximization loss": "tropy subtracted by empirical label’s entropy.",
          "3.5. Multitask Learning for domain adaptation": ""
        },
        {
          "3.3.\nInformation Maximization loss": "",
          "3.5. Multitask Learning for domain adaptation": "4. EXPERIMENT"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "p(c)log(ˆp(c))}.\n+E{": "(cid:88) c"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "where empirical label distribution (simply average of soft-"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "(cid:80)N"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "max output from all data points) as ˆp (c) = 1"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "i=1 p(ci|hi).\nN"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "We use IM loss to aid contrastive learning in forming clus-"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "ters. IM loss explicitly learns to form wide separation margin,"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "thus capable for our needs."
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "3.4. Data augmentation and Augmentation Classification"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "loss"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "Data augmentation is important component\nin our model as"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "it\nincreases data size for transformers to learn avoiding over-"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "fitting.\nAlso data augmentation is essential\nfor contrastive"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "learning as it requires different views of the same sample to"
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": ""
        },
        {
          "p(c)log(ˆp(c))}.\n+E{": "learn similarities between two augmentations. We exploited"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is perturbed. With this additional": "other classification task as auxiliary task and compute its loss",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "as:",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "Laug = −E{",
          "label provided, we add an-": "(6)\n1ca (ya)log(p(ca|h))}."
        },
        {
          "is perturbed. With this additional": "(cid:88) c",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "a",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "represents\nThe label ya",
          "label provided, we add an-": "from which augmentation pipeline"
        },
        {
          "is perturbed. With this additional": "",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "this\ninput came from.",
          "label provided, we add an-": "This classification layer\nis\nseparate"
        },
        {
          "is perturbed. With this additional": "",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "from emotion classification layer,",
          "label provided, we add an-": "thus learnable weights W ′"
        },
        {
          "is perturbed. With this additional": "",
          "label provided, we add an-": ""
        },
        {
          "is perturbed. With this additional": "and b′ are not same with W and b.",
          "label provided, we add an-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "strated that multitask learning with contrastive learning was": "able to learn the structure from data without labels by attract-"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": "Furthermore,\nin-"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": "to be capable of pushing"
        },
        {
          "strated that multitask learning with contrastive learning was": "representations of different classes to the edge region of dif-"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": "ferent directions for separation. Our experiments showed that"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": "our model is able to achieve on par with state-of-the-art in do-"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": "main adaption without labels. Moreover, our model improved"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": "10% above state-of-the-art with only 5% of the target dataset"
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        },
        {
          "strated that multitask learning with contrastive learning was": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data was taken with labels as the training set (denoted by Ttr)": "and remaining 95% was reserved for testing set (denoted by"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Tte). And we also experimented with a situation where labels"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "was considered unavailable for all target data. In this case, we"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "set 30% and 70% ratio of the MSP-IMPROV dataset for Ttr"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "and Tte, respectively."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Domain adaptation was tested by training on Str and Ttr"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "for one epoch alternatively. After each epoch, emotion clas-"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "sification accuracy was measured on Sva. When the accuracy"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "did not improve from previous epoch, training was halted and"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "UAR (Unweighted Average Recall) on Tte was recorded."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "We\nempirically\nset\nthe\nhyperparameters\nof\nthe\nloss:"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Pre-trained transform-\nλ1 = 0.1, λ2 = 0.5 and λ3 = 0.5."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "ers with name ’facebook/wav2vec2-base-960h’ (downloaded"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "from https://huggingface.co/facebook/wav2vec2-base-960h)"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "which was\ntrained in Wav2Vec2 with hidden layer\nsize of"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "768; and we set size for U and V\nas 256 and 128,\nrespec-"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "tively. Learning rate was 1 × 10−4 and decayed on plateau"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "by factor of 0.1 with patience of 5. Validation accuracy was"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "measured for\nlearning rate decay and the\nepoch (separate"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "from early stopping validation) was\nset as 100 batches\nfor"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "source dataset and 25 batches for\ntarget dataset with labels."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "When target data was without\nlabels, epoch was set as same"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "with the source and 100 batches were evaluated to decide on"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "learning rate decay."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Our\nexperiment\nresults\nis\npresented\nin Table.1.\nOur"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "method outperforms\nthe\nstate-of-the-art methods by about"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "10%. Even more, our model performs on par with state-of-"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "the-art methods when trained on target data without labels."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "4.3. Ablation atudy"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Our ablation study focused on cases where no labels are avail-"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "able,\nto clearly examine generalizability of our model. We"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "also tested contribution of component losses in the cost func-"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "tion.\nContribution of Lcont was\nthe largest and Laug was"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "(UAR de-\nthe least. Especially, Laug had negligible impact"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "creased by 0.2% when removed). We believe that Lcont learns"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "to attract representation from similar data samples and repel"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "representation from dissimilar data samples, thus learning the"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "overall structure of the dataset. And LIM enforces represen-"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "tation to have larger separation margin, resulting in localizing"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "representations from the same cluster in to a dense region."
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Table 2. Ablation Study"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "Our Model\nUAR"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "69.25%\ncomplete model"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "59.39%\nwithout labels"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "59.10%\nwithout Laug"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "58.68%\nwithout LIM & Laug"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": "57.37%\nwithout Lcont & Laug"
        },
        {
          "data was taken with labels as the training set (denoted by Ttr)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "2022, pp. 120–129."
        },
        {
          "6. REFERENCES": "[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[11] Srinivas\nParthasarathy\nand Carlos Busso,\n“Semi-"
        },
        {
          "6. REFERENCES": "and Michael Auli,\n“wav2vec 2.0: A framework for",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "supervised speech emotion recognition with ladder net-"
        },
        {
          "6. REFERENCES": "self-supervised learning of speech representations,” Ad-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "works,” IEEE/ACM transactions on audio, speech, and"
        },
        {
          "6. REFERENCES": "vances\nin neural\ninformation processing systems, vol.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "language processing, vol. 28, pp. 2697–2709, 2020."
        },
        {
          "6. REFERENCES": "33, pp. 12449–12460, 2020.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[12] Michael Neumann and Ngoc Thang Vu,\n“Improving"
        },
        {
          "6. REFERENCES": "[2] Shu-wen Yang,\nPo-Han\nChi,\nYung-Sung\nChuang,",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "speech emotion recognition with unsupervised repre-"
        },
        {
          "6. REFERENCES": "Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Andy T",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "sentation learning on unlabeled speech,”\nin ICASSP"
        },
        {
          "6. REFERENCES": "Liu,\nJiatong\nShi, Xuankai Chang, Guan-Ting\nLin,",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "2019-2019 IEEE International Conference on Acous-"
        },
        {
          "6. REFERENCES": "et al.,\n“Superb:\nSpeech processing universal perfor-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "tics,\nSpeech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "6. REFERENCES": "mance benchmark,”\narXiv preprint arXiv:2105.01051,",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "2019, pp. 7390–7394."
        },
        {
          "6. REFERENCES": "2021.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[13] Vipula Dissanayake, Haimo Zhang, Mark Billinghurst,"
        },
        {
          "6. REFERENCES": "[3] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "and Suranga Nanayakkara,\n“Speech emotion recogni-"
        },
        {
          "6. REFERENCES": "resentation learning with contrastive predictive coding,”",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "Interspeech\ntion ‘in the wild’using an autoencoder,”"
        },
        {
          "6. REFERENCES": "arXiv preprint arXiv:1807.03748, 2018.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "2020, 2020."
        },
        {
          "6. REFERENCES": "[4] Rui Wang, Zuxuan Wu, Zejia Weng,\nJingjing Chen,",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[14] Bo-Hao Su and Chi-Chun Lee,\n“Unsupervised cross-"
        },
        {
          "6. REFERENCES": "Guo-Jun Qi, and Yu-Gang Jiang,\n“Cross-domain con-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "corpus speech emotion recognition using a multi-source"
        },
        {
          "6. REFERENCES": "trastive learning for unsupervised domain adaptation,”",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "cycle-gan,” IEEE Transactions on Affective Computing,"
        },
        {
          "6. REFERENCES": "IEEE Transactions on Multimedia, 2022.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "2022."
        },
        {
          "6. REFERENCES": "[5] Andreas Krause, Pietro Perona, and Ryan Gomes, “Dis-",
          "the 30th ACM International Conference on Multimedia,": "[15] Siddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak,"
        },
        {
          "6. REFERENCES": "criminative clustering by regularized information max-",
          "the 30th ACM International Conference on Multimedia,": "and Bjorn Wolfgang Schuller,\n“Self\nsupervised ad-"
        },
        {
          "6. REFERENCES": "information processing\nimization,” Advances in neural",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "versarial domain adaptation for cross-corpus and cross-"
        },
        {
          "6. REFERENCES": "systems, vol. 23, 2010.",
          "the 30th ACM International Conference on Multimedia,": "IEEE Transac-\nlanguage speech emotion recognition,”"
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "tions on Affective Computing, 2022."
        },
        {
          "6. REFERENCES": "[6] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N",
          "the 30th ACM International Conference on Multimedia,": "[16] Siddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak,"
        },
        {
          "6. REFERENCES": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,",
          "the 30th ACM International Conference on Multimedia,": "and Bj¨orn W Schuller,\n“Multitask learning from aug-"
        },
        {
          "6. REFERENCES": "“Iemocap:\nInteractive emotional dyadic motion capture",
          "the 30th ACM International Conference on Multimedia,": "mented auxiliary data\nfor\nimproving speech emotion"
        },
        {
          "6. REFERENCES": "database,” Language resources and evaluation, vol. 42,",
          "the 30th ACM International Conference on Multimedia,": "IEEE Transactions on Affective Comput-\nrecognition,”"
        },
        {
          "6. REFERENCES": "pp. 335–359, 2008.",
          "the 30th ACM International Conference on Multimedia,": "ing, 2022."
        },
        {
          "6. REFERENCES": "[7] Carlos Busso,\nSrinivas\nParthasarathy, Alec Burma-",
          "the 30th ACM International Conference on Multimedia,": "[17] Ting Chen, Simon Kornblith, Mohammad Norouzi, and"
        },
        {
          "6. REFERENCES": "nia, Mohammed AbdelWahab, Najmeh Sadoughi, and",
          "the 30th ACM International Conference on Multimedia,": "Geoffrey Hinton,\n“A simple framework for contrastive"
        },
        {
          "6. REFERENCES": "Emily Mower Provost,\n“Msp-improv: An acted cor-",
          "the 30th ACM International Conference on Multimedia,": "learning of visual representations,” in International con-"
        },
        {
          "6. REFERENCES": "pus of dyadic interactions to study emotion perception,”",
          "the 30th ACM International Conference on Multimedia,": "ference on machine learning. PMLR, 2020, pp. 1597–"
        },
        {
          "6. REFERENCES": "IEEE Transactions on Affective Computing, vol. 8, no.",
          "the 30th ACM International Conference on Multimedia,": "1607."
        },
        {
          "6. REFERENCES": "1, pp. 67–80, 2016.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, “Distill-"
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "ing the knowledge in a neural network,” arXiv preprint"
        },
        {
          "6. REFERENCES": "[8] Yuan Gao, Longbiao Wang, Jiaxing Liu, Jianwu Dang,",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "arXiv:1503.02531, 2015."
        },
        {
          "6. REFERENCES": "and Shogo Okada,\n“Adversarial domain generalized",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "transformer\nfor cross-corpus\nspeech emotion recogni-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[19]\nJian Liang, Dapeng Hu, and Jiashi Feng,\n“Do we re-"
        },
        {
          "6. REFERENCES": "tion,” IEEE Transactions on Affective Computing, 2023.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "ally need to access\nthe source data?\nsource hypothe-"
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "sis transfer for unsupervised domain adaptation,” in In-"
        },
        {
          "6. REFERENCES": "[9] Alessandro Arezzo\nand Stefano Berretti,\n“Speaker",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "ternational Conference on Machine Learning (ICML),"
        },
        {
          "6. REFERENCES": "vgg cct: Cross-corpus speech emotion recognition with",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "2020, pp. 6028–6039."
        },
        {
          "6. REFERENCES": "speaker embedding and vision transformers,”\nin Pro-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "ceedings of\nthe 4th ACM International Conference on",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "[20] Yuan Gao, Shogo Okada, Longbiao Wang, Jiaxing Liu,"
        },
        {
          "6. REFERENCES": "Multimedia in Asia, 2022, pp. 1–7.",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "and Jianwu Dang,\n“Domain-invariant\nfeature\nlearn-"
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "ing for cross corpus\nspeech emotion recognition,”\nin"
        },
        {
          "6. REFERENCES": "[10] Shiqing Zhang, Ruixin Liu, Yijiao Yang, Xiaoming",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "ICASSP 2022-2022 IEEE International Conference on"
        },
        {
          "6. REFERENCES": "Zhao, and Jun Yu, “Unsupervised domain adaptation in-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "Acoustics,\nSpeech\nand\nSignal Processing\n(ICASSP)."
        },
        {
          "6. REFERENCES": "tegrating transformer and mutual information for cross-",
          "the 30th ACM International Conference on Multimedia,": ""
        },
        {
          "6. REFERENCES": "",
          "the 30th ACM International Conference on Multimedia,": "IEEE, 2022, pp. 6427–6431."
        },
        {
          "6. REFERENCES": "corpus speech emotion recognition,”\nin Proceedings of",
          "the 30th ACM International Conference on Multimedia,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "3",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "4",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "5",
      "title": "Cross-domain contrastive learning for unsupervised domain adaptation",
      "authors": [
        "Rui Wang",
        "Zuxuan Wu",
        "Zejia Weng",
        "Jingjing Chen",
        "Guo-Jun Qi",
        "Yu-Gang Jiang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Discriminative clustering by regularized information maximization",
      "authors": [
        "Andreas Krause",
        "Pietro Perona",
        "Ryan Gomes"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Adversarial domain generalized transformer for cross-corpus speech emotion recognition",
      "authors": [
        "Yuan Gao",
        "Longbiao Wang",
        "Jiaxing Liu",
        "Jianwu Dang",
        "Shogo Okada"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Speaker vgg cct: Cross-corpus speech emotion recognition with speaker embedding and vision transformers",
      "authors": [
        "Alessandro Arezzo",
        "Stefano Berretti"
      ],
      "year": "2022",
      "venue": "Proceedings of the 4th ACM International Conference on Multimedia in Asia"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised domain adaptation integrating transformer and mutual information for crosscorpus speech emotion recognition",
      "authors": [
        "Shiqing Zhang",
        "Ruixin Liu",
        "Yijiao Yang",
        "Xiaoming Zhao",
        "Jun Yu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Semisupervised speech emotion recognition with ladder networks",
      "authors": [
        "Srinivas Parthasarathy",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "13",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition 'in the wild'using an autoencoder",
      "authors": [
        "Haimo Vipula Dissanayake",
        "Mark Zhang",
        "Suranga Billinghurst",
        "Nanayakkara"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition 'in the wild'using an autoencoder"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised crosscorpus speech emotion recognition using a multi-source cycle-gan",
      "authors": [
        "Bo-Hao Su",
        "Chi-Chun Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and crosslanguage speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Bjorn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "19",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "20",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "Jian Liang",
        "Dapeng Hu",
        "Jiashi Feng"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "21",
      "title": "Domain-invariant feature learning for cross corpus speech emotion recognition",
      "authors": [
        "Yuan Gao",
        "Shogo Okada",
        "Longbiao Wang",
        "Jiaxing Liu",
        "Jianwu Dang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}