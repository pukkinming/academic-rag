{
  "paper_id": "2503.23450v1",
  "title": "Au-Ttt: Vision Test-Time Training Model For Facial Action Unit Detection",
  "published": "2025-03-30T14:09:13Z",
  "authors": [
    "Bohao Xing",
    "Kaishen Yuan",
    "Zitong Yu",
    "Xin Liu",
    "Heikki Kälviäinen"
  ],
  "keywords": [
    "-Facial AU detection",
    "test-time training."
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Action Units (AUs) detection is a cornerstone of objective facial expression analysis and a critical focus in affective computing. Despite its importance, AU detection faces significant challenges, such as the high cost of AU annotation and the limited availability of datasets. These constraints often lead to overfitting in existing methods, resulting in substantial performance degradation when applied across diverse datasets. Addressing these issues is essential for improving the reliability and generalizability of AU detection methods. Moreover, many current approaches leverage Transformers for their effectiveness in long-context modeling, but they are hindered by the quadratic complexity of self-attention. Recently, Test-Time Training (TTT) layers have emerged as a promising solution for long-sequence modeling. Additionally, TTT applies self-supervised learning for iterative updates during both training and inference, offering a potential pathway to mitigate the generalization challenges inherent in AU detection tasks. In this paper, we propose a novel vision backbone tailored for AU detection, incorporating bidirectional TTT blocks, named AU-TTT. Our approach introduces TTT Linear to the AU detection task and optimizes image scanning mechanisms for enhanced performance. Additionally, we design an AU-specific Region of Interest (RoI) scanning mechanism to capture fine-grained facial features critical for AU detection. Experimental results demonstrate that our method achieves competitive performance in both within-domain and cross-domain scenarios. Index Terms-Facial AU detection, test-time training.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial action units, as defined by the Facial Action Coding System, represent specific movements or deformations of facial muscles, providing a framework for analyzing complex facial expressions  [1] -  [5] . Consequently, AU detection has become a highly active area of research in recent years, spurring the development of numerous advanced methods. Traditionally, most approaches have relied on Convolutional Neural Networks (CNNs)  [6]  or Graph Neural Networks (GNNs)  [7] , which limit the model's focus to localized facial regions. Recently, however, innovative methods  [8] -  [14]  have Fig.  1 . Performance (F1 score) gap between the within-and cross-domain AU detection for DRML  [16] , J ÂA-Net  [17] , MEGraphAU  [10] , FG-Net  [18] , AUFormer  [13] , and AU-TTT(Ours). The within-domain performance is averaged between DISFA and BP4D, while the cross-domain performance is averaged between BP4D to DISFA and DISFA to BP4D.\n\nintroduced Transformers  [15] , leveraging their exceptional capability to model long-range dependencies and extract richer global context information from facial data. Specifically, early AU detection methods relied heavily on hand-crafted features, which struggled to capture the complexity and subtlety of facial muscle activations, resulting in limited performance. The advent of deep learning, particularly CNNs, marked a significant breakthrough by enabling automatic feature extraction. Methods such as J ÂA-Net  [17]  employed multi-scale hierarchical designs to capture features at varying resolutions, thereby improving the detection of AUs across diverse scales of muscle activation.\n\nTo address the intricate relationships among AUs, researchers explored GNNs  [10] ,  [19] , which effectively model the inherent dependencies between AUs using graph-based structures. For example, ME-GraphAU  [10]  utilized a graph with multi-dimensional edge features and Gated Graph Convolutional Networks  [20]  to capture correlations between AU pairs, leading to enhanced detection accuracy. However, despite their effectiveness, GNN-based methods primarily emphasize local features and fail to fully exploit the global dependencies present in the data.\n\nThe recent integration of Transformers into AU detection has significantly enhanced the field by enabling the modeling of long-range dependencies  [8] ,  [9] ,  [11] -  [14] . For instance, methods like FAUDT  [8]  leveraged Transformer blocks to capture global correlations between AUs, thereby providing richer contextual information for more accurate detection. However, despite these advancements, the substantial number of learnable parameters in Transformer-based models poses a challenge, increasing the risk of overfitting, particularly when training on datasets with limited AU annotations. Furthermore, the quadratic complexity of the self-attention mechanism significantly increases computational costs.\n\nIn addition, manual annotations for AUs are cumbersome and costly, as they require trained coders to label each frame individually. Common AU datasets, i.e., DISFA  [22]  and BP4D  [23] , only contain a limited number of subjects (27 and 41 subjects respectively). Current AU detection methods are predominantly evaluated using within-domain validation, where both training and testing data are sourced from the same dataset. While these methods often demonstrate strong withindomain performance, such results can obscure overfitting as shown in Figure  1 , as their ability to generalize to unseen domains remains insufficiently explored. Cross-domain evaluations frequently reveal significant performance degradation when models are applied to datasets with variations in camera settings, environmental conditions, and subject demographics  [24] ,  [25] .\n\nAlthough some studies  [13] ,  [18] ,  [24] ,  [25]  have made efforts to improve cross-domain generalization, their scope remains limited. Existing approaches to enhance generalization include identity-aware training on diverse datasets, as demonstrated by IdenNet  [26] , and domain adaptation techniques  [27] , but these methods typically require access to the target domain. DeepFN  [28]  addresses this issue by normalizing facial expressions onto a common template, but significant challenges persist in achieving robust cross-domain performance. AUFormer  [13]  leverages large-scale pretraining  [29]  and parameter-efficient transfer learning to improve cross-domain robustness but does not incorporate targeted strategies to address domain shifts. Similarly, FG-Net  [18]  utilizes generative model features to mitigate cross-domain challenges. Moreover, both methods rely on pretrained weights obtained from extensive additional data.\n\nApart from recent architectures such as Transformer and Mamba  [30] ,  [31] , the concept of learning at test time has a long history in machine learning  [32] ,  [33] . More recently, Test-Time Training (TTT) extended this idea to Recurrent Neural Networks (RNNs)  [34]  and has emerged as an effective strategy for addressing out-of-distribution (OOD) problem  [21] ,  [35] ,  [36] . The core idea of TTT is that each test instance defines its own learning problem, where the test instance itself serves as the target for generalization. In traditional settings, a predictor f is trained on all training instances and used to directly predict f (x) for a given test instance x . In contrast, TTT formulates a specific learning problem centered on x. It updates a model f x using x (with f as its initialization) and then predicts f x (x). Since the test instance lacks labels, the learning problem is defined using a self-supervised task. Previous studies demonstrated that TTT with reconstruction tasks significantly enhances performance, particularly for out-lier instances  [35] .\n\nIn this paper, we introduce AU-TTT, a novel facial action unit detection method designed to enhance cross-domain generalization. AU-TTT leverages Test-Time Training to improve the model's generalization ability. Additionally, the integration of bidirectional scanning and AU RoI scanning enables AU-TTT to effectively capture both general image information and fine-grained AU-related facial features, further enhancing its robustness and performance.\n\nOur main contributions can be summarized as follows:\n\n• We propose AU-TTT, a novel framework that integrates Test-Time Training into facial AU detection to address the challenge of out-of-distribution generalization.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Au-Ttt",
      "text": "In this section, we first outline the fundamentals of the TTT Layer  [21] . We then present AU-TTT, our proposed adaptation of the Test-Time Training framework, specifically designed for facial AU detection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Preliminaries",
      "text": "We briefly introduce the original TTT Block and TTT Layer proposed in  [21] , as illustrated in Figure  2 . The TTT Block replaces the self-attention in the basic building block of Transformers with the TTT Layer. The TTT Layer leverages self-supervised reconstruction to encode information into the model's weights. This process is demonstrated using input sequences X = [x t ], t ∈ {1, . . . , N }. It begins with an initial hidden state W 0 , which is iteratively refined through self-supervised learning. For each input token x t , the model computes an output as follows\n\nwhere W t represents the current hidden state, and z t is the updated embedding. The hidden state W t is refined using a gradient descent step on a self-supervised loss ℓ, which minimizes the difference between the model's predicted embedding and the actual embedding. The update rule, illustrated in Figure  2  (C), can also be expressed as\n\nwhere η denotes the learning rate. The TTT Layer further incorporates a multi-view reconstruction mechanism to enhance the optimization of the reconstruction process. The updated self-supervised loss is described as      where θ K and θ V are the projections for the training view and label view, respectively. The corresponding output rule is as follows\n\nwhere θ Q is the projection for the test view.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Au-Ttt",
      "text": "The original TTT Layer was designed for NLP tasks and is not directly applicable to image-based AU detection. To address this limitation, we made several improvements in this subsection. The first issue to consider is that the mini-batch size within the original TTT Layer is suboptimal for CV tasks. Through experimentation, we discovered that adjusting the mini-batch size based on the number of tokens corresponding to the image's feature rows significantly improves performance. We refer to this modified version of TTT as forward TTT. Next, we incorporate bidirectional scan TTT (Bi-TTT) into the AU-TTT block to mitigate the impact of sequential modeling on image tasks. Additionally, we introduce an AU RoI scanning mechanism (AU RoI TTT) specifically designed for AU detection tasks.\n\nAn overview of the proposed AU-TTT framework is shown in Figure  3 . The standard TTT Layer is originally designed for 1D sequences. To adapt it for vision tasks, we first transform the input image I ∈ R H×W ×C into flattened patches X ∈ R J×(P 2 •C) , where (H, W ) denotes the height and width of the image, C represents the number of channels, and P is the size of each image patch. Next, we linearly project X into a vector of size D and add learnable positional embeddings E pos ∈ R (J+1)×D , as follows\n\nwhere X j represents the j-th patch of X, W ∈ R (P 2 •C)×D is the learnable projection. Inspired by ViT  [15] , we also incorporate a class token (CLS) to represent the entire patch sequence. The token sequence Z l-1 is then passed through the l-th layer of the AU-TTT encoder, producing the output Z l . Specifically, the input token sequence Z l-1 is first normalized using a normalization layer. Then, the token sequence is processed in both forward and backward directions. The bidirectional TTT process is formulated as follows (for simplicity, the layer l notation is omitted in the equations below):\n\nBi-TTT(Z) = Bi-TTT([z 1 ; z 2 ; . . . ; z J ; CLS]) = Linear(Cat[TTT([z 1 ; z 2 ; . . . ; z J ; CLS]);\n\nReverse(TTT([z J ; z J-1 ; . . . ; z 1 ; CLS]))]). (  6 ) where Z represents the input sequence, z j denotes the j-th token, and CLS is the class token. The operation Reverse(•) reverses the sequence order, excluding the CLS token. TTT represents the foward TTT Layer, Cat[•; •] denotes the concatenation of results along the channel dimension, and Linear is a linear projection used to reduce the dimension.\n\nAdditionally, we leverage facial landmarks to generate a heatmap for extracting local features of AU RoI regions  [18] . Specifically, we first perform an element-wise multiplication between the facial features and the generated mask:\n\nwhere\n\nHere, H ′ and W ′ represent the height and width of the features, respectively. N AU represents the number of AUs in the dataset. Through broadcasting, the element-wise multiplication results in Z AU ∈ R NAU×H ′ ×W ′ ×D . Next, we compress Z AU into tokens Z ′ AU ∈ R NAU×D using mean pooling, which are then modeled using the TTT layer to capture the relationships between different AUs.\n\nAt the same time, we employ a set of dilated convolutions with varying dilation rates as a Multi-Scale Perception (MSP) module to extract multi-scale facial features. Finally, the outputs of the AU RoI TTT, bidirectional TTT, and MSP modules are combined through addition and passed through a MLP to produce the output token sequence Z l .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Loss Function",
      "text": "AU detection is typically formulated as a multi-label binary classification problem. To generate the final predictions, we transform the CLS token and the heatmap obtained from the last block of AU-TTT. For the classification loss, we adopt the Margin-Truncated Difficulty-Aware Weighted Asymmetric Loss (MDWA-Loss) proposed by AUFormer  [13] . This loss function emphasizes activated AUs, adaptively evaluates the difficulty of unactivated AUs, and discards potentially mislabeled samples. The MDWA-Loss can be expressed as:\n\nwhere y i and p i represent the ground truth and predictions for the i-th AU, respectively. ω i is the weight for addressing class imbalance, and it can be formulated as\n\n, where r i is the occurrence rate of the i-th AU. p m,i is used to discard potentially mislabeled samples and can be formulated as p m,i = max(p im, 0), where m ∈ [0, 1] is the truncation margin. γ i is used to distinguish the different difficulty levels of unactivated AUs and can be formulated as γ i = B L + (B R -B L ) × r i , where B L and B R are left and right boundaries of γ i respectively.\n\nAdditionally, referring to  [17] , we also introduce a weighted multi-label dice loss (WDI-Loss) to alleviate the issue of AU prediction bias towards non-occurrence, as follow: where ϵ is a smooth term and the meanings of y i , p i , and ω i remain consistent with those defined in Equation (  8 ).\n\nIn addition, we utilize Mean Squared Error (MSE) loss to enhance the network's capability in capturing AU RoI location-related information which can be expressed as:\n\nwhere m represents the ground-truth heatmap and m represents the predicted heatmap.\n\nThe overall loss of AUFormer can be expressed as:\n\nwhere λ MDWA , λ WDI , and λ MSE are hyperparameters.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Settings",
      "text": "Datasets. We select two publicly available datasets, DISFA  [22]  and BP4D  [23] , which are commonly used for AU detection. These datasets were collected from different subjects under varying backgrounds and lighting conditions. Follow  [6] ,  [10] ,  [13] ,  [17] ,  [18] , we employ subject-exclusive 3-fold cross-validation.\n\nImplementation Details. We pretrain AU-TTT on ImageNet-1K  [44] , following the configuration of ViT-S/16  [15] . Specifically, we set the patch size to 16, the embedding dimension to 384, the depth to 12, and the number of heads to 6. The dilated rates r 1 , r 2 , and r 3 in Multi-Scale Perception module are set to 1, 3, and 5. The left and right boundaries B L and B R of γ are set to 1 and 2. The truncation margin m for BP4D and DISFA are set to 0.1 and 0.15. The smooth term ϵ is set to 1. The number of AUs N AU for DISFA and BP4D is set to 8 and 12, following  [16] .\n\nEvaluation Metric. Following prior works  [13] ,  [16] -  [18] ,  [43] , we use the F1-score as the evaluation metric for AU detection performance. The F1-score represents the harmonic mean of precision and recall.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Performance (F1 score) gap between the within- and cross-domain",
      "page": 1
    },
    {
      "caption": "Figure 1: , as their ability to generalize to unseen",
      "page": 2
    },
    {
      "caption": "Figure 2: (C), can also be expressed as",
      "page": 2
    },
    {
      "caption": "Figure 2: A: Original TTT Block. The basic building block of Transformers, originally based on self-attention, is replaced with the TTT Layer in the Transformer",
      "page": 3
    },
    {
      "caption": "Figure 3: Top Left: The overall AU-TTT framework. Top Right: The architecture of the AU-TTT Block. Bottom Left: The original TTT Layer scanning",
      "page": 3
    },
    {
      "caption": "Figure 3: The standard TTT Layer is originally designed for",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Weight": "",
          "Column_3": "Q",
          "Column_4": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 2": "Flatt",
          "3 4 5 6": "en & Linear Projectio"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "4",
          "2 3": "5 6"
        },
        {
          "1": "7",
          "2 3": "8 9"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "4"
        },
        {
          "1": "7"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "Brais Martinez",
        "Michel Valstar",
        "Bihan Jiang",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "2",
      "title": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "authors": [
        "Bohao Xing",
        "Zitong Yu"
      ],
      "year": "2024",
      "venue": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "arxiv": "arXiv:2408.11424"
    },
    {
      "citation_id": "3",
      "title": "Enhancing micro gesture recognition for emotion understanding via context-aware visual-text contrastive learning",
      "authors": [
        "Deng Li",
        "Bohao Xing",
        "Xin Liu"
      ],
      "year": "2024",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "4",
      "title": "Counterfactual discriminative microexpression recognition",
      "authors": [
        "Yong Li",
        "Menglin Liu"
      ],
      "year": "2024",
      "venue": "Visual Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Eald-mllm: Emotion analysis in long-sequential and de-identity videos with multi-modal large language model",
      "authors": [
        "Deng Li",
        "Xin Liu",
        "Bohao Xing",
        "Baiqiang Xia",
        "Yuan Zong",
        "Bihan Wen",
        "Heikki Kälviäinen"
      ],
      "year": "2024",
      "venue": "Eald-mllm: Emotion analysis in long-sequential and de-identity videos with multi-modal large language model",
      "arxiv": "arXiv:2405.00574"
    },
    {
      "citation_id": "6",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "7",
      "title": "The graph neural network model",
      "authors": [
        "Franco Scarselli",
        "Marco Gori"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "8",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Bjorn Jacob",
        "Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "9",
      "title": "Multi-scale promoted self-adjusting correlation learning for facial action unit detection",
      "authors": [
        "Xin Liu",
        "Kaishen Yuan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "authors": [
        "Cheng Luo",
        "Siyang Song"
      ],
      "year": "2022",
      "venue": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "arxiv": "arXiv:2205.01782"
    },
    {
      "citation_id": "11",
      "title": "Semantic learning for facial action unit detection",
      "authors": [
        "Xuehan Wang",
        "C Philip Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "12",
      "title": "Fan-trans: Online knowledge distillation for facial action unit detection",
      "authors": [
        "Jing Yang",
        "Jie Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF WACV"
    },
    {
      "citation_id": "13",
      "title": "Auformer: Vision transformers are parameter-efficient facial action unit detectors",
      "authors": [
        "Kaishen Yuan",
        "Zitong Yu"
      ],
      "year": "2025",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Multi-scale dynamic and hierarchical relationship modeling for facial action units recognition",
      "authors": [
        "Zihan Wang",
        "Siyang Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "15",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "16",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "Kaili Zhao",
        "Wen-Sheng Chu"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "17",
      "title": "Jaa-net: joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu"
      ],
      "year": "2021",
      "venue": "IJCV"
    },
    {
      "citation_id": "18",
      "title": "Fg-net: Facial action unit detection with generalizable pyramidal features",
      "authors": [
        "Yufeng Yin",
        "Di Chang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF WACV"
    },
    {
      "citation_id": "19",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "Xuesong Niu",
        "Hu Han"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "20",
      "title": "Residual gated graph convnets",
      "authors": [
        "Xavier Bresson",
        "Thomas Laurent"
      ],
      "year": "2017",
      "venue": "Residual gated graph convnets",
      "arxiv": "arXiv:1711.07553"
    },
    {
      "citation_id": "21",
      "title": "Learning to (learn at test time): Rnns with expressive hidden states",
      "authors": [
        "Yu Sun",
        "Xinhao Li"
      ],
      "year": "2024",
      "venue": "Learning to (learn at test time): Rnns with expressive hidden states",
      "arxiv": "arXiv:2407.04620"
    },
    {
      "citation_id": "22",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Mahoor"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "24",
      "title": "Cross-domain au detection: Domains, learning approaches, and measures",
      "authors": [
        "Itir Onal Ertugrul",
        "Jeffrey Cohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "25",
      "title": "Crossing domains for au coding: Perspectives, approaches, and measures",
      "authors": [
        "Itir Onal Ertugrul",
        "Jeffrey Cohn"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "26",
      "title": "Idennet: Identity-aware facial action unit detection",
      "authors": [
        "Cheng-Hao Tu",
        "Chih-Yuan Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "27",
      "title": "Self-supervised patch localization for cross-domain facial action unit detection",
      "authors": [
        "Yufeng Yin",
        "Liupei Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "28",
      "title": "Deepfn: towards generalizable facial action unit recognition with deep face normalization",
      "authors": [
        "Javier Hernandez",
        "Daniel Mcduff"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "29",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "Albert Gu",
        "Tri Dao"
      ],
      "year": "2023",
      "venue": "Mamba: Linear-time sequence modeling with selective state spaces",
      "arxiv": "arXiv:2312.00752"
    },
    {
      "citation_id": "31",
      "title": "Fusionmamba: Dynamic feature enhancement for multimodal image fusion with mamba",
      "authors": [
        "Xinyu Xie",
        "Yawen Cui"
      ],
      "year": "2024",
      "venue": "Visual Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Local learning algorithms",
      "authors": [
        "Léon Bottou",
        "Vladimir Vapnik"
      ],
      "year": "1992",
      "venue": "Neural computation"
    },
    {
      "citation_id": "33",
      "title": "Learning by transduction",
      "authors": [
        "Alex Gammerman",
        "Volodya Vovk"
      ],
      "year": "2013",
      "venue": "Learning by transduction",
      "arxiv": "arXiv:1301.7375"
    },
    {
      "citation_id": "34",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Test-time training with masked autoencoders",
      "authors": [
        "Yossi Gandelsman",
        "Yu Sun"
      ],
      "year": "2022",
      "venue": "Advances in NeurIPS"
    },
    {
      "citation_id": "36",
      "title": "scfusionttt: Single-cell transcriptomics and proteomics fusion with test-time training layers",
      "authors": [
        "Dian Meng",
        "Bohao Xing",
        "Xinlei Huang",
        "Yanran Liu",
        "Yijun Zhou",
        "Zitong Yu",
        "Xubin Zheng"
      ],
      "year": "2024",
      "venue": "scfusionttt: Single-cell transcriptomics and proteomics fusion with test-time training layers",
      "arxiv": "arXiv:2410.13257"
    },
    {
      "citation_id": "37",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "Guanbin Li",
        "Xin Zhu"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Hybrid message passing with performance-driven structures for facial action unit detection",
      "authors": [
        "Tengfei Song",
        "Zijun Cui"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "39",
      "title": "Exploiting semantic embedding and visual feature for facial action unit detection",
      "authors": [
        "Huiyuan Yang",
        "Lijun Yin",
        "Yi Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "40",
      "title": "Piap-df: Pixel-interested and anti person-specific facial action unit detection net with discrete feedback learning",
      "authors": [
        "Yang Tang",
        "Wangding Zeng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF ICCV"
    },
    {
      "citation_id": "41",
      "title": "Knowledge-driven self-supervised representation learning for facial action unit recognition",
      "authors": [
        "Yanan Chang",
        "Shangfei Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "42",
      "title": "Biomechanics-guided facial action unit detection through force modeling",
      "authors": [
        "Zijun Cui",
        "Chenyi Kuang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "43",
      "title": "Knowledge-spreader: Learning semi-supervised facial action dynamics by consistifying knowledge granularity",
      "authors": [
        "Xiaotian Li",
        "Xiang Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF ICCV"
    },
    {
      "citation_id": "44",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong"
      ],
      "year": "2009",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "45",
      "title": "Analyzing and improving the image quality of stylegan",
      "authors": [
        "Tero Karras",
        "Samuli Laine"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on CVPR"
    },
    {
      "citation_id": "46",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "Tero Karras"
      ],
      "year": "2019",
      "venue": "A style-based generator architecture for generative adversarial networks",
      "arxiv": "arXiv:1812.04948"
    }
  ]
}