{
  "paper_id": "2003.04241v1",
  "title": "Deep Neural Networks For Automatic Speech Processing: A Survey From Large Corpora To Limited Data",
  "published": "2020-03-09T16:26:30Z",
  "authors": [
    "Vincent Roger",
    "Jérôme Farinas",
    "Julien Pinquier"
  ],
  "keywords": [
    "Audio Processing",
    "Deep Learning Techniques",
    "Deep Neural Networks",
    "Few-Shot Learning",
    "Speech Analysis",
    "Under-Resourced Languages"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Most state-of-the-art speech systems are using Deep Neural Networks (DNNs). Those systems require a large amount of data to be learned. Hence, learning state-of-the-art frameworks on under-resourced speech languages/problems is a difficult task. Problems could be the limited amount of data for impaired speech. Furthermore, acquiring more data and/or expertise is time-consuming and expensive. In this paper we position ourselves for the following speech processing tasks: Automatic Speech Recognition, speaker identification and emotion recognition. To assess the problem of limited data, we firstly investigate state-of-the-art Automatic Speech Recognition systems as it represents the hardest tasks (due to the large variability in each language). Next, we provide an overview of techniques and tasks requiring fewer data. In the last section we investigate few-shot techniques as we interpret under-resourced speech as a few-shot problem. In that sense we propose an overview of few-shot techniques and perspectives of using such techniques for the focused speech problems in this survey. It occurs that the reviewed techniques are not well adapted for large datasets. Nevertheless, some promising results from the literature encourage the usage of such techniques for speech processing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A UTOMATIC speech processing systems drastically im- proved the past few years, especially Automatic Speech Recognition (ASR) systems. It is also the case for other speech processing tasks such as speaker identification, emotion classification, etc. This success was made possible by the large amount of annotated data available combined with the extensive use of deep learning techniques and the capacity of modern Graphics Processing Units. Some models are already deployed for everyday usage such as your personal assistants on your smartphones, your connected speakers and so on.\n\nNevertheless, challenges remain for automatic speech processing systems. They lack robustness against large vocabulary in real-world environment: this includes noises, distance from the speaker, reverberations and other alterations. Some challenges, such as CHIME  [1] , provide data to let the community try to handle some of these problems. It is being investigated to improve the generalization of modern models by avoiding the inclusion of other annotated data for every possible environment.\n\nState-Of-The-Art (SOTA) techniques for most speech tasks require large datasets. Indeed, with modern DNN speech processing systems, having more data usually imply better performances. The TED-LIUM 3 from  [2]  (with 452 hours) provide more than twice the data of the TED-LIUM 2 dataset. Doing so, they obtain better results by training their model on TED-LIUM 3 than training their model over TED-LIUM 2 data. This improvement in performance for ASR systems is also observed with the LibriSpeech dataset (from  [3] ). V.  Panayotov     [3] .\n\nThis phenomenon, of having more data imply better performances, is also observable with the VoxCeleb 2 dataset compare to the VoxCeleb dataset:  [4]  increase the number of sentences from 100,000 utterances to one million utterances and increase the number of identities from 1251 to 6112 compared to the previous version of VoxCeleb. Doing so, they obtain better performances compare to training their model with the previous VoxCeleb dataset.\n\nWith under-resourced languages (such as  [5] ) and/or tasks (pathological detection with speech signals), we lack large datasets. By under-resourced, we mean limited digital resources (limited acoustic and text corpora) and/or a lack of linguistic expertise. For a more precise definition and details of the problem you may look  [6] . Non-conventional speech tasks such as disease detection (such as Parkinson, gravity of ENT cancer and others) using audio are examples of tasks under resourced. Train Deep Neural Network models in such context is a challenge for these under-resourced speech datasets. This is especially the case for large vocabulary tasks. M. Moore et al. showed that recent ASR systems are not well adapted for impaired speech  [7]  and M. B.  Mustafa et al.  showed the difficulties to adapt such models with limited amount of data  [8] . Few-shot learning consists of training a model using kshot (where shot means an example per class), where k ≥ 1 and k is a low number. Training an ASR system on a new language, adapting an ASR system on pathological speech or doing a speaker identification with few examples are still complicated tasks. We think that few-shot techniques may be useful to tackle these problems. This survey will be focused on how to learn Deep Neural Network (DNN) models under low resources for speech data with non-overlapping mono signals. Therefore, we will first review SOTA ASR techniques that use a large amount of data (section II). Then we will review techniques and speech tasks (speaker identification, emotion recognition) requiring fewer data than SOTA techniques (section III). We will also look into pathological speech processing for ASR using adaptation techniques (subsection III-B). Finally, we will review few-shot techniques for audio (section IV) which is the focus of this survey.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Automatic Speech Recognition Systems",
      "text": "In this section, we will review SOTA ASR systems using multi-models and end-to-end models. Here, we are focused on mono speech sequences x = [x 1 , x 2 , . . . , x n ] where x i can be speech features or audio samples. ASR systems consist in matching x into a sequence of words y = [y 1 , y 2 , . . . , y u ] (where u ≤ n). The systems reviewed were evaluated using Word Error Rate (WER) measure.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multi-Models",
      "text": "A multi-model approach consists in solving a problem using multiple models. Those models are designed to solve either sub-tasks (related to the problem) and the targeted task. The minimum configuration is with two models (let say f and g) to solve a given task. Classically for the ASR task we can first learn an acoustic model (a phoneme classifier or equivalent sound units), then learn on top of it a language model that output the desired sequence of words. Hence, we have:\n\nwith f being the language model and g being the acoustic model. Both can be learned separately or conjointly. Usually, hybrid models are used as acoustic models. Hybrid models consist in using probabilistic models with deterministic ones. Probabilistic models involve randomness using random variables combined with trained parameters. Hence, every prediction is sightly different on a given example x. Gaussian Mixture Models (GMMs) are an example of such models. Deterministic models do not involve randomness and every prediction are the same given an input x. DNNs are an example of such models. A popular and efficient hybrid model is the DNN-Hidden Markov Model (DNN-HMM). DNN-HMM consists in replacing the GMMs that estimate the probability density functions by DNNs. The DNNs can be learned as phone classifiers. They form the acoustic model. This acoustic model is combined with a Language Model (LM) that maps the phonemes into a sequence of words. C. Lüscher et al. used DNN-HMMs combined with a Language Model to obtain SOTA on LibriSpeech test-other set (official augmented test set)  [9] . This model process MFCC computed on the audio signals. Their best LM approach consisted in the use of Transformer from  [10] . Transformers are autoregressive models (depending on the previous outputs of the models) using soft attention mechanisms. Soft attention consists in determining a glimpse g over all possible glimpses such as:\n\nwith x being the input data and a the attention parameters. Their best hybrid model got a Word Error Rate (WER) of 5.7% for the test-other set and a WER of 2.7% for test-clean set.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. End-To-End Systems",
      "text": "In end-to-end approaches, the goal is to determine a model f that can do the mapping:\n\nIt will be learned straightforward from the x to the desired y. Only supervised methods can be end-to-end to solve the speech tasks we are focused on.\n\nIn ASR systems,  [11]  got SOTA on LibriSpeech test-clean official set. Compared to  [9]  they used Vocal Tract Length Perturbation as the input of their end-to-end model. C. Kim et al. model is based on the Encoder-Decoder architecture using stacked LSTM for the encoder and LSTM combined with soft attention for the decoder  [11] . They obtain a WER of 2.44% on test-clean and a WER of 8.29% on test-other. Those results are close to  [9]  (best hybrid model results) and show that endto-end approaches are competitive compared to multi-model approaches.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Techniques And Tasks Requiring Fewer Data",
      "text": "Some techniques require fewer data than the techniques of the previous section. In this section we will enumerate the principal ways to leverage (to our best knowledge) the lack of large datasets like unimpaired speech. We will also look into tasks requiring fewer data (speaker identification and emotion recognition). We will not talk of semi-supervised techniques that use a large amount of unsupervised data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Augmentation",
      "text": "The first way to leverage the lack of data is to artificially augment the number of data. To do so, classic approach consists for example in adding noise or deformation. Such as in  [12] . They obtain near SOTA on Librispeech (1000 hours from  [3] ) with an end-to-end models. Nevertheless, they obtain SOTA results on SwitchBoard (300 hours from  [13] ) with a WER of 6.8%/14.1% on the Switchboard/CallHome portion using shallow fusion and their data augmentation. But theses are handcrafted augmentations and some of them require additional audios (like adding noise). Some other approaches use generative models to have new samples such as in  [14] ,  [15] . A. Chatziagapi et al. used conditional Generative Adversarial Networks (GAN) to generate new samples  [14] . Conditioned GAN are GAN where we can control the mode of the generated samples. Doing so, they balanced their initial dataset and obtain better results. Y. Jiao et al. used Deep Convolutional GANs to generate dysarthric speech and improve their results  [15] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Domain Transposition",
      "text": "Another way to leverage the lack of data is to use domain transposition to avoid complex domain, here is some recent examples on speech:\n\n• K. Wang et al. used GAN to dereverberate speech signal  [16] . In their work, the generator is used as a mapping function of reverberated signals into dereverberated speech signals.\n\n• L.-W. Chen et al. do vocal conversion using GAN with a controller mapping impaired speech to a representation space z  [17] . z is then the input of the generator that is used as a mapping function to have unimpaired speech signals.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "• S. Zhao Et Al. Used Cycle Gan (Framework Designed",
      "text": "for domain transfer) as an audio enhancer  [18] . Their resulting model is SOTA on Chime-4 dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Models Requiring Fewer Parameters",
      "text": "Having fewer data disallow the use of many parameters for Neural Network models to avoid overfitting. This is why some techniques tried to have models requiring fewer parameters.\n\nHere, we highlight some recent techniques that we find interesting:\n\n• The use of SincNet, from  [19] , layers to replace classic 1D convolutions over raw audio. Here, instead of requiring window size parameters (with window size being the window size of the 1D convolution) per filter, we only need two parameters per filter for every window size.\n\nTheses two parameters represent in a way (not directly) the values of the bandwidth at high and low energy. • The use of LightGRU (LiGRU), from  [20] , based on the Gated Recurrent Unit (GRU) framework. LiGRU is a simplification of the GRU framework given some assumption in audio. They removed the reset gate of the GRU and used the ReLU activation function (combined with the Batch Normalization) instead of the tanh activation function.\n\n• The use of quaternions Neural Networks, from  [21] , for speech processing. The quaternion formulation allows the fuse of 4 dimensions into one inducing a drastic reduction of required parameters in their experiments (near 4 times).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Multi-Task Approach",
      "text": "Multi-task models can be viewed as an extension of the Encoder-Decoder architecture where you have a decoder per task with a shared encoder (like in Figure  1 ). Then those tasks are trained conjointly with classic feed-forward algorithms. The goal of a multi-task learning is to have an encoder outputting sufficient information for every task. Doing so, it can potentially improve the performances of each task compared to mono task architectures. It is a way to have a more representative encoder given the same amount of data.\n\nIn emotion recognition,  [22]  got SOTA results over a modified version of the IEMOCAP database to have a fourclass problem. Those emotions are: angry, happy, neutral and sad. Y. Li et al. used an end-to-end multi-task system with only supervised tasks: gender identification and emotion identification  [22] . The resulting model achieve an overall accuracy for the emotion task (which is the main target) of 81.6% and an average accuracy of each emotion category of 82.8%. Using such approach allows them to achieve balanced results over unbalanced data.\n\nNevertheless, using only supervised tasks requires multiple ground-truth for the targeted dataset. S.  Pascual   a combination of self-supervised tasks combined with unsupervised tasks to tackle this problem and used the resulting encoder for transfer learning  [23] . They recently improved this work in  [24]  where they use more tasks, a recurrent unit on top of the encoder and denoising mechanisms using multiple data augmentation on their system.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "E. Transfer Learning",
      "text": "Transfer learning techniques consist of using a pre-trained model and transfer its knowledge to solve a related problem/task. Usually we use the encoding part of the pre-trained model to initialize the model for the new problem/task.\n\nContrastive Predictive Coding (CPC from  [25] ) is an architecture to learn unsupervised audio representation using a 2-level architecture combined with a self-supervised loss. They achieved good results by transferring the obtained model for speaker identification and phone classification (on LibriSpeech dataset) compared to MFCC features. This work inspired  [23] . They developed an unsupervised multi-task model (with certain losses being self-supervised) to obtain better encoders for transfer learning. They applied it on multiple tasks and obtain decent results on speaker identification (using VTCK), emotion recognition (using INTERFACE) and ASR (using TIMIT).\n\nThe benefit of pre-trained network for transfer learning decrease as the target task diverges from the original task of the pre-trained network  [26] . To tackle this,  [25] ,  [23]  attempt to have generic tasks with their unsupervised approach, and they obtained promising results. Also, the benefit of transfer learning decrease when the dissimilarity between the datasets increase  [26] . This problem can discourage the use of transfer learning for some pathological speech. Whereas, Dysarthric and Accented Speech seems similar to speech in librispeech dataset according to  [27] . Where they successfully used transfer learning to improve their results over a 36.7 hours dataset.\n\nNevertheless,  [8]  showed that acoustic characteristics of unimpaired and impaired speech are very different. In the case of having few data such problems can be critical. It is why looking into few-shot techniques could be helpful.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Few-Shot Learning And Speech",
      "text": "In the previous sections, we reviewed models that require a large amount of data. This among of data is not always available such as for pathological speech. Google is trying to acquire more data of that nature 1 . But acquiring such data can be quite expensive and time consuming. M. B. Mustafa et al. recommend the use of adaptive techniques to tackle limited amount of data problem in such case  [8] . But we think few-shot technique can be an other solution to this problem. Nevertheless, some non-common tasks such as pathological or dialect identification with few examples are still hard to train with SOTA techniques based on large speech datasets. This is why we investigate the following few-shot techniques and see the adaptations required for using them on speech datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Few-Shot Notations",
      "text": "Let consider a distribution P from which we draw Independent Identically Distributed (iid) episodes E, where E is composed of a support set S, unlabeled data x and a query set Q. Support set correspond to the supervised samples the model has access to:\n\nwith x i being samples and y i being the corresponding labels such as y i ∈ {1, 2, . . . , K}. K being the number of classes appearing in P . The query set is composed of samples to classify x with ŷ being the corresponding ground truth.\n\nTo summarize, episodes drawn from P have the following form: E = {S = {(x 1 , y 1 ), . . . , (x s , y s )}, x = (x 1 , . . . , xr ),\n\nwith s, r and t fixed values that respectively represent the number of supervised samples for the support set, the number of unsupervised samples and the number of supervised samples for the query set.\n\nIn this survey, we will focus on Few-Shot Learning techniques where r = 0, t ≥ 1 and s = kn, with n being the number of times each label appears for the support set and k the number of classes selected from P , such as k ≤ K. Hence, we have a n-shot with k ways (or classes) for each episode. One-shot learning is just a special case of few-shot learning where n = 1. In some few-shot framework, we only sample one episode from P and it represents our task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Few-Shot Learning Techniques",
      "text": "In this section we will review frameworks that impacted the few-shot learning field in image processing, frameworks with a formulation that seems adapted for speech processing and frameworks already successfully used by the speech community.\n\n1 https://blog.google/outreach-initiatives/accessibility/impaired-speech-recognition/",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Siamese Technique",
      "text": "Siamese Neural Networks are designed to be used per episode  [28] . They consist of measuring the distance between two samples and tell if they are similar or not. Hence, Siamese network uses the samples from the support set S as references for each class. It is then trained using all the combinations of samples from S Q which represent much more training than having only s+t samples in classical feedforward frameworks. Siamese Networks take two samples (x 1 and x 2 ) as input and compute a distance between them, as follows:\n\nwith Enc being a DNN encoder that represents the signal input, σ being the sigmoid function, α learnable parameters that weight the importance of each component of the encoder and x 1 and x 2 sampled from either the support set nor the queries set.\n\nTo define the class of a new sample from Q or any new data, we have to compute the distance between each reference from S and the new sample. An example of comparison between a reference and a new example is shown in Figure  2 . Then, the class of the reference with the lowest distance become the prediction of the model. To learn such model,  [28]  used this loss function:\n\nwith x = [x 1 , . . . , x s , x1 , . . . , xt ] from S and Q. y(x) is a function that returns the label corresponding to the example x. Also, φ last layer should be a softmax.\n\nx i",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Siamese Model",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Enc Φ Xj Enc",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Same Or Different",
      "text": "Figure  2 . Example of comparison between a reference (x i ) and a new example (x j ) from the query set. Where Enc is the same network applied to both x i and xj . The model output the distance between x i and xj class.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "R. Eloff Et Al. Used A Modified Version Of This Framework For Multimodal",
      "text": "Learning (framework that is out of scope for this survey) between speech and image signal  [29] . The speech signals used consist of 11-digit number (zero to nine and oh) with the corresponding 10 images (oh and zero give the same images). The problem is to associate speech signals with the corresponding image. In their experiment, the model shows some invariances to speakers (accuracy of 70.12% ± 0.68) using only a one-shot configuration, which is promising results.\n\nSiamese Neural Networks are not well adapted when the number of classes K or the number of shots q become too high. It increases the number of references to compare and the computation time to forward the model. It is mostly a problem for learning the model. After the model is learned, we can pre-calculate all representations for the support set to reduce this effect. Also, it drastically increases the number of combinations to do for training, this can be viewed as a positive point as we can truncate the number of combinations to use for training the model. This framework seems not adapted for end-to-end ASR with large vocabulary such as in the English speech (around 470,000 words). Maybe it will be sufficient for languages such as Esperanto language (around 16,780 words). The other way to use such a framework in ASR systems is to use it in hybrid models as an acoustic model.\n\nWhere we can learn it on every phoneme (for example 44 phonemes/sounds in English) or more refined sound units.\n\nSiamese framework seems interesting for tasks such as speaker identification. Indeed, this framework allows adding new speaker without retraining the model (supposing the model had generalized) or change the architecture of the model. We have to at least add one example of the new speaker to the references. Furthermore, Siamese formulation seems well adapted for speaker verification. Indeed, by replacing the pair (x, speaker id) by the pair (x, S top5 ) we can do speaker verification with such technique. Where S top5 is a support set composed of signals from the 5 top predictions of the identification sub-task.\n\nNevertheless, this framework will be limited if the number of speakers to identify become too high. Even so, it is possible to use such techniques in an end-to-end ASR system when the vocabulary is limited, such as in  [29]  experiment.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "2) Matching Network",
      "text": "Matching Networks from  [30]  is a few-shot framework designed to be trained on multiple episodes. This framework is composed of one model ϕ. This model is trained over a set of training episodes (with typically 5 to 25 ways). This model evaluates new examples given the support set S like in the Siamese framework:\n\nIn matching learning, ϕ is as follows:\n\nwith, a being the attention kernel.\n\nIn  [30]  this attention kernel is as follows:\n\nwhere c is the cosine distance, f and g are embedding functions.\n\nO. Vinyals et al. used a recurrent architecture to modulate the representation of f using the support set S  [30] . The goal is to have f following the same type of representation of g. To do this, g function is as follows:\n\nwhere -→ h i and ←h i represent a bi-LSTM output over g ′ (x i ) which is a DNN.\n\nf function is as follows:\n\nwith, attLST M being an LSTM with a fixed number of recurrences to do (here m), g(S) represents the application of g to each x i from the S set. f ′ is a DNN with the same architecture as g ′ , but not necessarily share the parameter values. Hence, training this framework consists in the maximization of the log likelihood of ϕ given the parameters of g and f .\n\nFigure  3  illustrates forward time of the Matching Network model. For forward time on new samples g(S) can be pre-calculated to gain computation time. Nevertheless, as for Siamese networks, Matching networks have the same disadvantages when q and/or K become too high. Furthermore, adding new classes to a trained Matching Network model is not as easy as for Siamese Network models. Indeed, it requires retraining the Matching Network model to add an element to the support set. Whereas, Matching learning showed better results than the Siamese framework on image datasets from  [30]  experiments. It is why it should be investigated in speech processing to see if it is still the case.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "S",
      "text": "Matching Network Model",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "3) Prototypical Networks",
      "text": "Prototypical Networks  [31]  are designed to work with multiple episodes. In the prototypical framework, the model ϕ does its predictions given the support set S of an episode such as the previously seen frameworks. This framework uses training episodes as mini-batches to obtain the final model. This model is formulated as follows:\n\nwhere c k is the prototype of the class k, d being a Bregman divergence (for their useful properties in optimization, see  [31]  for more details) that also follow this property:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "J. Snell Et Al. Used The Euclidean Distance For D Instead Of The Cosine Distance Used In Meta Learning And Matching",
      "text": "Learning papers  [31] . Doing so, they obtain better results in their experiments. Next, they go further by reducing the Euclidean to a linear function.\n\nIn the prototypical framework, there is only one prototype for each class k as illustred in Figure . 4. It is computed such as:\n\nwith f being a mapping function such as R D → R M and S k being the samples with k of the support set.\n\nCompared to Siamese and Matching Learning Networks, prototypical networks require only one comparison per class and not q per class for q-shot learning like in Siamese and Matching Learning Networks. It is why this framework is less subject to the high computation problem for prediction of new samples as it is only influenced by high K. It will certainly be insufficient for end to end ASR systems on English language but it is a step forward to it.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "S",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4) Meta-Learning",
      "text": "Meta-learning  [32]  are designed to be learned on multiple episodes (also called datasets). In this framework a trainee model (T ) with parameters θ T is trained for every episode from the start of every episode. It usually has a classic DNN architecture. The support set and the query set in the episodes are considered as the training set and the test set for the trainee model.\n\nAlong with this trainee model, a second model is learned: the meta model (M) with parameters θ M . This meta model is the key of meta learning, it consists in monitoring the trainee model by updating θ T parameters. To learn this meta model, sampling iid episodes from P to form the meta-dataset (D) is suggested in  [32] . This meta-dataset is composed of a training set (D train ), a validation set (D valid ) and a testing set (D test ).\n\nWhile the trainee model is training on an episode E j , the meta model is charged to update its parameters:\n\nwith L Tj being the loss function of the trainee model learned over the episode E j and θ Tj t-1 are the parameters of the trainee model at step t-1. Also, M has to guess initial weights of the trainee models at step t = 0 (θ Tj 0 ). The learning curve (loss) of the trainee model over E j is viewed in  [32]  as a sequence that can be the input of the meta model M. For simplicity, we will use the notation of T instead of T j for the next paragraphs. Figure  5  illustrate the learning steps of the trainee using the meta model. a) Trainee parameters update: S. Ravi and H. Larochelle identify the learning process of T using classic feedforward update on episode E j to be similar with the c t update gate of the LSTM framework  [32] . In the meta learning framework, the update gate c t of the LSTM framework is then used as the θ T t estimator, such as:\n\nwith θT t = -α t ∇ θ T t-1 L T t being the update term of the parameters θ T t-1 , f t being the forget gate and i t the update gate.\n\nb) Parameters of the meta model: Both i t and f t are part of the Meta learner. In the meta-learning framework, the update gate is formulated as follows:\n\nwith the W I and b I being parameters of M. The update gate is used to control update term in 16 like the learning rate in classic feedforward approach.\n\nNext, the forget gate in the meta-learning framework is formulated as follows:\n\nwith W F and b F parameters of M. This gate is here to decide whether the learning of the trainee should restart or not. This can be useful to get out of a sub-optimal local minimum. Note that this gate is not present in classic feedforward approaches (where this gate is equal to one). The trainee model (T ) of this framework can be any kind of model such as a Siamese Neural Network. Hence, it can have the advantages of this framework. It also can avoid the Siamese neural network disadvantages as it can use any other framework (usually classic DNN). This framework is interesting for speech processing to learn efficient models (in terms of learning speed) when we have multiple ASR tasks with different vocabulary. For example, let say we have these kinds of speech episodes: dialing numbers, commands to a robot A and commands to a robot B. The model can initialize good filters for the first layers (as it is still speech processing). Another example could be learning acoustic models for multiple languages (with each episode corresponding to a language).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "5) Graph Neural Network",
      "text": "The use of Graph Neural Network (GNN) is used by V. Garcia and J. Bruna introduce the use of Graph Neural Network (GNN) in their few-shot framework  [33] . This framework is designed to be used with multiple episodes they called tasks. In this framework, one model is used over a complete graph G. G = (V, E) where every node corresponds to an example. GNN for few-shot learning consists in applying Graph Convolutions Layers over the graph G.\n\nInitial vertices construction to guess the ground truth of a query xi from the query set Q: V (0) = ((Enc(x 1 ), h(y 1 )), . . . , (Enc(x s ), h(y s )), (Enc( x1 ), u), . . . , (Enc( xr ), u)\n\nwhere Enc is an embedding extraction function (a Neural Network or any classic feature extraction technique), h the one-hot encoding function and u = K -1 1 K an uniform distribution for examples with unknown labels (the unsupervised ones from x and/or from the query set Q).\n\nFrom now the vertices at each layer l (with 0 being the initial vertices) will be denoted:\n\nwhere n = s + r + 1 and\n\nEvery layers in GNN are computed as follows:\n\nwith A (l) being the adjacency operators constructed from V (l)  and Gc being the graph convolution.\n\na) The adjacency operators construction: The adjacency operator us a set:\n\nwith Ã(l) being the adjacency matrix of V (l) .\n\nFor every (i, j) ∈ E (recall we have complete graphs), we compute the values of the adjacency matrix such as:\n\nwhere:\n\nwith f being a multi-layer perceptron with its parameter denoted θ f . Ã(l) is then normalized using the softmax function over each line. b) Graph convolution: The graph convolution requires the construction of the adjacency operators set and is computed as follows:\n\nwith B being an adjacency operator from A, θ\n\nB,l ∈ R d l-1 ,d l learnable parameters and ρ being a point wise linearity (usually leaky ReLU).\n\nc) Training the model: The output of the resulting GNN model is a mapping of the vertices to a K-simplex that give the probability of xi being in class k. V. Garcia and J. Bruna used the cross-entropy to learn the model other all examples in the query set Q  [33] . Hence, the GNN few-shot framework consists in learning θ f and θ 1,l . . . θ card(A),l parameters over all episodes.\n\nd) Few-shot GNN on audio: This framework was used by  [34]  on 5-way audio classification problems. The 5 ways episodes are randomly selected from the initial dataset: Au-dioSet  [35]  for creating the 5-ways training episodes and  [36]  data to create the 5-ways test episodes.\n\nS. Zhang et al. compare the use of per class attention (or intra-class) and global attention which gave the best results  [34] . They applied it for each layer. Their experiments were done for 1-shot, 5-shots and 10-shots with the respective accuracy of 69.4%±0.66, 78.3%±0.46 and 83.6%±0.98. Such results really motivate us in the path of few-shot learning for speech signals. Nevertheless, this framework does not allow the use of many classes and shots per episode which increase the number of nodes and thus the computations in forward time. Hence, it is not suited for large vocabulary problems.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Summary And Future Directions",
      "text": "In this survey, we investigated few-shot techniques for speech usage. In order to do so, we started with state-of-theart speech processing systems. These systems require a large amount of data and are not suited for under-resourced speech problems. We also looked into techniques requiring fewer data using data augmentation, domain transposition, models requiring fewer parameters, multi-task approach and transfer learning. Nevertheless, these techniques are less efficient in a data-limited context. Next, we studied few-shot techniques and how well the different frameworks are adapted for classical speech tasks.\n\nThe main drawback of the reviewed techniques is the amount of computation required for large datasets (like Lib-riSpeech from  [3] ) compared to SOTA models we reviewed in section II. Nevertheless, we considered some recent works already using few-shot techniques on speech with promising results. Such techniques seem useful for classical speech tasks on impaired speakers. Moreover, we think it can be useful for unconventional speech tasks like measuring the intelligibility of a person (with impaired or unimpaired speakers) to help the re-education process (by identifying the problems faster). Acquiring a large amount of data is painful for some patients (with severe pathologies). We believe that few-shot techniques may help the community to tackle this problem. To see the interest of such techniques we will work on a benchmark for different speech tasks. We will do some adaptations when necessary, but we think that we can use the different frameworks straightforward. After that, we plan to use the technique with the best results on this benchmark as a base for learning the concept of intelligibility.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Then those tasks",
      "page": 3
    },
    {
      "caption": "Figure 1: Multi-task architecture illustration. The output of the encoder is",
      "page": 3
    },
    {
      "caption": "Figure 2: Example of comparison between a reference (xi) and a new example",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates forward time of the Matching Network",
      "page": 5
    },
    {
      "caption": "Figure 3: Illustration of the Matching Network model to predict class of a",
      "page": 5
    },
    {
      "caption": "Figure 4: Illustration of the Prototypical Network model to predict class of",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrate the learning steps of the trainee using the",
      "page": 6
    },
    {
      "caption": "Figure 5: Meta-Learning illustration for training over episode Ej at step t.",
      "page": 6
    },
    {
      "caption": "Figure 6: Illustration of the input of the ﬁrst layer (or Graph Convolution)",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "IRIT, Universit´e de Toulouse, CNRS, Toulouse, France"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Most state-of-the-art speech systems are using Deep Neural Networks (DNNs). Those systems require a large amount of data to"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "be learned. Hence,\nlearning state-of-the-art\nframeworks on under-resourced speech languages/problems is a difﬁcult task. Problems"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "could be the limited amount of data for impaired speech. Furthermore, acquiring more data and/or expertise is\ntime-consuming"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "and expensive. In this paper we position ourselves for the following speech processing tasks: Automatic Speech Recognition, speaker"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "identiﬁcation and emotion recognition. To\nassess\nthe problem of\nlimited data, we ﬁrstly\ninvestigate\nstate-of-the-art Automatic"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Speech Recognition systems as\nit\nrepresents\nthe hardest\ntasks\n(due\nto the\nlarge variability in each language). Next, we provide"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "an overview of\ntechniques and tasks requiring fewer data.\nIn the last\nsection we investigate few-shot\ntechniques as we interpret"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "under-resourced speech as a few-shot problem. In that sense we propose an overview of few-shot techniques and perspectives of using"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "such techniques for the focused speech problems in this survey. It occurs that the reviewed techniques are not well adapted for large"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "datasets. Nevertheless,\nsome promising results from the literature encourage the usage of such techniques for speech processing."
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Index Terms—Audio Processing, Deep Learning Techniques, Deep Neural Networks, Few-Shot Learning, Speech Analysis, Under-"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Resourced Languages."
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "compare to the VoxCeleb dataset:\n[4]\nincrease the number of\nI.\nINTRODUCTION"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "sentences\nfrom 100,000 utterances\nto one million utterances"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "and\nincrease\nthe\nnumber\nof\nidentities\nfrom 1251\nto\n6112\ncompared\nto\nthe\nprevious\nversion\nof VoxCeleb. Doing\nso,\nA UTOMATIC speech processing systems drastically im-"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Recognition\n(ASR)\nsystems.\nIt\nis\nalso\nthe\ncase\nfor\nother"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "they\nobtain\nbetter\nperformances\ncompare\nto\ntraining\ntheir"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "speech processing tasks such as speaker identiﬁcation, emotion"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "model with the previous VoxCeleb dataset."
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "classiﬁcation,\netc. This\nsuccess was made\npossible\nby\nthe"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "large amount of annotated data available combined with the"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "With under-resourced languages (such as [5]) and/or\ntasks"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "extensive use of deep learning techniques and the capacity of"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "(pathological detection with\nspeech signals), we\nlack\nlarge"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "modern Graphics Processing Units. Some models are already"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "datasets. By\nunder-resourced, we mean\nlimited\ndigital\nre-"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "deployed for everyday usage such as your personal assistants"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "sources\n(limited acoustic and text corpora) and/or a lack of"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "on your smartphones, your connected speakers and so on."
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "linguistic expertise. For a more precise deﬁnition and details of"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Nevertheless, challenges remain for automatic speech pro-\nthe problem you may look [6]. Non-conventional speech tasks"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "cessing systems. They lack robustness against\nlarge vocabu-\nsuch as disease detection (such as Parkinson, gravity of ENT"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "lary in real-world environment:\nthis includes noises, distance"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "cancer and others) using audio are examples of\ntasks under"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "from the speaker,\nreverberations and other alterations. Some\nresourced. Train Deep Neural Network models in such context"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "challenges,\nsuch\nas CHIME\n[1],\nprovide\ndata\nto\nlet\nthe"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "is a challenge for these under-resourced speech datasets. This"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "community try to handle some of\nthese problems.\nIt\nis being\nis especially the case\nfor\nlarge vocabulary tasks. M. Moore"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "investigated to improve the generalization of modern models\net al.\nshowed that\nrecent ASR systems are not well adapted"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "by avoiding the inclusion of other annotated data\nfor every"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "for\nimpaired speech [7] and M. B. Mustafa et al. showed the"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "possible environment.\ndifﬁculties to adapt such models with limited amount of data"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "State-Of-The-Art (SOTA) techniques for most speech tasks\n[8]. Few-shot\nlearning consists of\ntraining a model using k-"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "require\nlarge\ndatasets.\nIndeed, with modern DNN speech\nshot\n(where shot means an example per class), where k ≥ 1"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "processing systems,\nhaving more data\nusually imply better\nand k is a low number. Training an ASR system on a new"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "performances. The TED-LIUM 3 from [2]\n(with 452 hours)\nlanguage,\nadapting an ASR system on\npathological\nspeech"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "provide more than twice the data of the TED-LIUM 2 dataset.\nor doing a speaker\nidentiﬁcation with few examples are still"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Doing so,\nthey obtain better\nresults by training their model\ncomplicated tasks. We think that\nfew-shot\ntechniques may be"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "on TED-LIUM 3 than training their model over TED-LIUM\nuseful\nto tackle these problems."
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "2 data. This\nimprovement\nin performance for ASR systems\nThis survey will be focused on how to learn Deep Neural"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "is also observed with the LibriSpeech dataset\n(from [3]). V.\nNetwork (DNN) models under\nlow resources for speech data"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "Panayotov et al. obtain better results on the Wall Street Journal\nwith non-overlapping mono signals. Therefore, we will ﬁrst"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "(WSJ)\ntest\nset by training a model over LibriSpeech dataset\nreview SOTA ASR techniques that use a large amount of data"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "(1000 hours) than training a model over the WSJ training set\n(section II). Then we will review techniques and speech tasks"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "(82 hours) [3].\n(speaker\nidentiﬁcation, emotion recognition)\nrequiring fewer"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "This\nphenomenon,\nof\nhaving more\ndata\nimply\nbetter\ndata\nthan SOTA techniques\n(section III). We will\nalso look"
        },
        {
          "Vincent ROGER, J´erˆome FARINAS and Julien PINQUIER": "performances,\nis also observable with the VoxCeleb 2 dataset\ninto pathological speech processing for ASR using adaptation"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "techniques\nfor audio (section IV) which is\nthe focus of\nthis",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "In end-to-end approaches,\nthe goal\nis to determine a model"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "survey.",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "f that can do the mapping:"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "(3)\ny = f (x)"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "II. AUTOMATIC SPEECH RECOGNITION SYSTEMS",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "It will be learned straightforward from the x to the desired"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "In this\nsection, we will\nreview SOTA ASR systems using",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "y. Only supervised methods can be end-to-end to solve the"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "multi-models and end-to-end models. Here, we\nare\nfocused",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "speech tasks we are focused on."
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "on mono speech sequences x = [x1, x2, . . . , xn] where xi can",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "In ASR systems,\n[11] got SOTA on LibriSpeech test-clean"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "be speech features or audio samples. ASR systems consist\nin",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "ofﬁcial\nset. Compared to [9]\nthey used Vocal Tract Length"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "matching x into a\nsequence of words y = [y1, y2, . . . , yu]",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "Perturbation as the input of their end-to-end model. C. Kim et"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "reviewed were evaluated using\n(where u ≤ n). The systems",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "al. model\nis based on the Encoder-Decoder architecture using"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "Word Error Rate (WER) measure.",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "stacked LSTM for the encoder and LSTM combined with soft"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "attention for\nthe decoder\n[11]. They obtain a WER of 2.44%"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "A. Multi-models",
          "B. End-to-end systems": ""
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "",
          "B. End-to-end systems": "on test-clean and a WER of 8.29% on test-other. Those results"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "A multi-model approach consists in solving a problem using",
          "B. End-to-end systems": "are close to [9] (best hybrid model results) and show that end-"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "multiple models. Those models are designed to solve either",
          "B. End-to-end systems": "to-end approaches are competitive compared to multi-model"
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "sub-tasks (related to the problem) and the targeted task. The",
          "B. End-to-end systems": "approaches."
        },
        {
          "techniques (subsection III-B). Finally, we will review few-shot": "minimum conﬁguration is with two models (let say f and g) to",
          "B. End-to-end systems": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "a controller mapping impaired speech to a representation",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "space z [17]. z is then the input of\nthe generator that",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "used as a mapping function to have unimpaired speech",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "",
          ". . .": ". . ."
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "signals.",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "S. Zhao et\nal.\nused Cycle GAN (framework designed",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "for domain transfer)\nas\nan audio enhancer",
          ". . .": ""
        },
        {
          "• L.-W. Chen et al. do vocal conversion using GAN with": "resulting model\nis SOTA on Chime-4 dataset.",
          ". . .": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "C. Models requiring fewer parameters": ""
        },
        {
          "C. Models requiring fewer parameters": "Having fewer data disallow the use of many parameters for"
        },
        {
          "C. Models requiring fewer parameters": "Neural Network models to avoid overﬁtting. This is why some"
        },
        {
          "C. Models requiring fewer parameters": ""
        },
        {
          "C. Models requiring fewer parameters": "techniques\ntried to have models\nrequiring fewer parameters."
        },
        {
          "C. Models requiring fewer parameters": "Here, we highlight some recent\ntechniques that we ﬁnd inter-"
        },
        {
          "C. Models requiring fewer parameters": "esting:"
        },
        {
          "C. Models requiring fewer parameters": "• The use of SincNet,\nfrom [19],\nlayers to replace classic"
        },
        {
          "C. Models requiring fewer parameters": "1D convolutions over raw audio. Here,\ninstead of requir-"
        },
        {
          "C. Models requiring fewer parameters": "ing window size parameters (with window size being"
        },
        {
          "C. Models requiring fewer parameters": "the window size of the 1D convolution) per ﬁlter, we only"
        },
        {
          "C. Models requiring fewer parameters": "need two parameters per ﬁlter\nfor\nevery window size."
        },
        {
          "C. Models requiring fewer parameters": "Theses two parameters represent\nin a way (not directly)"
        },
        {
          "C. Models requiring fewer parameters": "the values of\nthe bandwidth at high and low energy."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "1) Siamese technique"
        },
        {
          "4": "Siamese Neural Networks\nare\ndesigned\nto\nbe\nused\nper"
        },
        {
          "4": ""
        },
        {
          "4": "episode [28]. They consist of measuring the distance between"
        },
        {
          "4": ""
        },
        {
          "4": "two samples and tell if they are similar or not. Hence, Siamese"
        },
        {
          "4": ""
        },
        {
          "4": "network uses the samples from the support set S as references"
        },
        {
          "4": ""
        },
        {
          "4": "for each class. It\nis then trained using all\nthe combinations of"
        },
        {
          "4": ""
        },
        {
          "4": "samples from S S Q which represent much more training than"
        },
        {
          "4": "having only s+t samples in classical feedforward frameworks."
        },
        {
          "4": ""
        },
        {
          "4": "Siamese Networks take two samples (x1 and x2) as input and"
        },
        {
          "4": "compute a distance between them, as follows:"
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "(6)\nφ(x1, x2) = σ(X α|Enc(x1) − Enc(x2)|)"
        },
        {
          "4": ""
        },
        {
          "4": "with Enc being a DNN encoder\nthat\nrepresents\nthe\nsignal"
        },
        {
          "4": "input, σ being the sigmoid function, α learnable parameters"
        },
        {
          "4": "that weight\nthe importance of each component of the encoder"
        },
        {
          "4": "sampled from either\nthe support\nset nor\nthe\nand x1\nand x2"
        },
        {
          "4": ""
        },
        {
          "4": "queries set."
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "To deﬁne the class of a new sample from Q or any new data,"
        },
        {
          "4": ""
        },
        {
          "4": "we have to compute the distance between each reference from"
        },
        {
          "4": ""
        },
        {
          "4": "S and the new sample. An example of comparison between"
        },
        {
          "4": ""
        },
        {
          "4": "a reference and a new example is\nshown in Figure 2. Then,"
        },
        {
          "4": "the class of the reference with the lowest distance become the"
        },
        {
          "4": ""
        },
        {
          "4": "prediction of\nthe model. To learn such model,\n[28] used this"
        },
        {
          "4": ""
        },
        {
          "4": "loss function:"
        },
        {
          "4": ""
        },
        {
          "4": "L =Ey(xi)=y(˜xj) log(φ(xi, ˜xj ))+"
        },
        {
          "4": ""
        },
        {
          "4": "(7)"
        },
        {
          "4": "Ey(xi)6=y(˜xj) log(1 − φ(xi, ˜xj ))"
        },
        {
          "4": "is a\nfrom S and Q. y(x)\nwith ˜x = [x1, . . . , xs, ˆx1, . . . , ˆxt]"
        },
        {
          "4": ""
        },
        {
          "4": "function that\nreturns\nthe label corresponding to the example"
        },
        {
          "4": ""
        },
        {
          "4": "x. Also, φ last\nlayer should be a softmax."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "f function is as follows:"
        },
        {
          "5": ""
        },
        {
          "5": "(12)\nf (ˆx) = attLST M (f ′(ˆx), g(S), m)"
        },
        {
          "5": ""
        },
        {
          "5": "with,\nattLST M being\nan LSTM with\na ﬁxed\nnumber\nof"
        },
        {
          "5": "recurrences\nrepresents\nthe\napplication\nto do (here m), g(S)"
        },
        {
          "5": "from the S set. f ′\nis a DNN with the same\nof g to each xi"
        },
        {
          "5": "architecture\nas\ng′,\nbut\nnot\nnecessarily\nshare\nthe\nparameter"
        },
        {
          "5": "values.\nHence,\ntraining\nthis\nframework\nconsists\nin\nthe"
        },
        {
          "5": "maximization of the log likelihood of ϕ given the parameters"
        },
        {
          "5": "of g and f ."
        },
        {
          "5": ""
        },
        {
          "5": "Figure 3 illustrates forward time of\nthe Matching Network"
        },
        {
          "5": "model.\nFor\nforward\ntime\non\nnew samples\ncan\nbe\ng(S)"
        },
        {
          "5": "pre-calculated\nto\ngain\ncomputation\ntime.\nNevertheless,"
        },
        {
          "5": "as\nfor\nSiamese\nnetworks, Matching\nnetworks\nhave\nthe"
        },
        {
          "5": "same\ndisadvantages when\nq\nand/or K become\ntoo\nhigh."
        },
        {
          "5": "Furthermore,\nadding\nnew classes\nto\na\ntrained Matching"
        },
        {
          "5": "Network model\nis\nnot\nas\neasy\nas\nfor\nSiamese Network"
        },
        {
          "5": "models.\nIndeed,\nit\nrequires\nretraining the Matching Network"
        },
        {
          "5": "model to add an element to the support set. Whereas, Matching"
        },
        {
          "5": "learning showed better\nresults\nthan the Siamese\nframework"
        },
        {
          "5": "on image datasets from [30] experiments. It\nis why it should"
        },
        {
          "5": "be investigated in speech processing to see if it is still the case."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Matching Network Model"
        },
        {
          "5": ""
        },
        {
          "5": "gθ(x1)"
        },
        {
          "5": ". . .\ngθ"
        },
        {
          "5": "S"
        },
        {
          "5": "ϕ\nMost probable class"
        },
        {
          "5": "gθ(xs)"
        },
        {
          "5": ""
        },
        {
          "5": "fθ\nxi"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "Figure 3.\nIllustration of\nthe Matching Network model\nto predict class of a"
        },
        {
          "5": ""
        },
        {
          "5": "new example ˆxi."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "3) Prototypical Networks"
        },
        {
          "5": "Prototypical Networks\n[31]\nare\ndesigned\nto work with"
        },
        {
          "5": ""
        },
        {
          "5": "multiple episodes.\nIn the prototypical\nframework,\nthe model"
        },
        {
          "5": ""
        },
        {
          "5": "ϕ does its predictions given the support set S of an episode"
        },
        {
          "5": "such as the previously seen frameworks. This framework uses"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "training episodes as mini-batches\nto obtain the ﬁnal model."
        },
        {
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "with\n˜θT\nbeing\nthe\nupdate\nterm of\nthe"
        },
        {
          "6": "t\n= −αt∇θT\nt−1LT"
        },
        {
          "6": "parameters θT"
        },
        {
          "6": "t−1, ft being the forget gate and it"
        },
        {
          "6": "gate."
        },
        {
          "6": ""
        },
        {
          "6": "b) Parameters of\nare\nthe meta model: Both it\nand ft"
        },
        {
          "6": ""
        },
        {
          "6": "part of\nthe Meta learner.\nIn the meta-learning framework,\nthe"
        },
        {
          "6": ""
        },
        {
          "6": "update gate is formulated as follows:"
        },
        {
          "6": ""
        },
        {
          "6": ", θT\n, LT"
        },
        {
          "6": "(17)\nit = σ(WI .[∇θT\nt\nt−1, it−1] + bI )\nt−1LT"
        },
        {
          "6": ""
        },
        {
          "6": "with the WI and bI being parameters of M. The update gate"
        },
        {
          "6": ""
        },
        {
          "6": "is used to control update term in 16 like the learning rate in"
        },
        {
          "6": ""
        },
        {
          "6": "classic feedforward approach."
        },
        {
          "6": "Next,\nthe\nforget gate\nin\nthe meta-learning framework is"
        },
        {
          "6": "formulated as follows:"
        },
        {
          "6": ""
        },
        {
          "6": ", θT\n(18)\n, LT"
        },
        {
          "6": "ft = σ(WF .[∇θT\nt−1, ft−1] + bF )\nt−1 LT"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "is\nhere\nto\nparameters of M. This gate\nwith WF\nand bF"
        },
        {
          "6": "decide whether\nthe\nlearning\nof\nthe\ntrainee\nshould\nrestart"
        },
        {
          "6": "or\nnot.\nThis\ncan\nbe\nuseful\nto\nget\nout\nof\na\nsub-optimal"
        },
        {
          "6": "local minimum. Note that\nthis gate is not present\nin classic"
        },
        {
          "6": "feedforward approaches (where this gate is equal\nto one)."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "b) Graph convolution: The graph convolution requires"
        },
        {
          "7": "the construction of the adjacency operators set and is computed"
        },
        {
          "7": "as follows:"
        },
        {
          "7": ""
        },
        {
          "7": "BV (l)θ(k)"
        },
        {
          "7": "(25)\nB,l)\nGc(V (l), A(l)) = ρ( X"
        },
        {
          "7": ""
        },
        {
          "7": "B∈A"
        },
        {
          "7": ""
        },
        {
          "7": "with B being an adjacency operator from A, θ(k)"
        },
        {
          "7": "B,l ∈ Rdl−1,dl"
        },
        {
          "7": ""
        },
        {
          "7": "learnable parameters and ρ being a point wise linearity (usually"
        },
        {
          "7": "leaky ReLU)."
        },
        {
          "7": "c) Training the model: The output of the resulting GNN"
        },
        {
          "7": ""
        },
        {
          "7": "model\nis a mapping of\nthe vertices to a K-simplex that give"
        },
        {
          "7": ""
        },
        {
          "7": "the probability of\nxi being in class k. V. Garcia and J. Bruna"
        },
        {
          "7": ""
        },
        {
          "7": "used the cross-entropy to learn the model other all examples"
        },
        {
          "7": ""
        },
        {
          "7": "the GNN few-shot framework\nin the query set Q [33]. Hence,"
        },
        {
          "7": ""
        },
        {
          "7": "consists in learning θf\nand θ1,l . . . θcard(A),l parameters over"
        },
        {
          "7": "all episodes."
        },
        {
          "7": ""
        },
        {
          "7": "d) Few-shot GNN on audio: This\nframework was used"
        },
        {
          "7": ""
        },
        {
          "7": "by [34] on 5-way audio classiﬁcation problems. The 5 ways"
        },
        {
          "7": "episodes are randomly selected from the initial dataset: Au-"
        },
        {
          "7": ""
        },
        {
          "7": "dioSet [35] for creating the 5-ways training episodes and [36]"
        },
        {
          "7": "data to create the 5-ways test episodes."
        },
        {
          "7": "S. Zhang et al. compare the use of per class attention (or"
        },
        {
          "7": "intra-class) and global attention which gave the best\nresults"
        },
        {
          "7": "[34]. They applied it\nfor each layer. Their experiments were"
        },
        {
          "7": ""
        },
        {
          "7": "done\nfor\n1-shot,\n5-shots\nand\n10-shots with\nthe\nrespective"
        },
        {
          "7": "accuracy of 69.4%±0.66, 78.3%±0.46 and 83.6%±0.98. Such"
        },
        {
          "7": ""
        },
        {
          "7": "results really motivate us in the path of\nfew-shot\nlearning for"
        },
        {
          "7": ""
        },
        {
          "7": "speech signals. Nevertheless,\nthis\nframework does not allow"
        },
        {
          "7": ""
        },
        {
          "7": "the use of many classes and shots per episode which increase"
        },
        {
          "7": ""
        },
        {
          "7": "the number of nodes and thus\nthe\ncomputations\nin forward"
        },
        {
          "7": ""
        },
        {
          "7": "time. Hence,\nit\nis not suited for large vocabulary problems."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "[19] M. Ravanelli\nand Y. Bengio, “Interpretable Convolutional Filters with"
        },
        {
          "8": "SincNet,” in NIPS 2018 Workshop IRASL, Nov. 2018."
        },
        {
          "8": ""
        },
        {
          "8": "[20] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio, “Light Gated Re-"
        },
        {
          "8": ""
        },
        {
          "8": "current Units for Speech Recognition,” IEEE Transactions on Emerging"
        },
        {
          "8": "Topics\nin Computational\nIntelligence,\nvol. 2, no. 2, pp. 92–102, Apr."
        },
        {
          "8": "2018."
        },
        {
          "8": ""
        },
        {
          "8": "[21]\nT. Parcollet, M. Ravanelli, M. Morchid, G. Linar`es, and R. De Mori,"
        },
        {
          "8": ""
        },
        {
          "8": "“Speech recognition with quaternion neural networks,” in NeurIPS 2018"
        },
        {
          "8": "-\nIRASL, Nov. 2018."
        },
        {
          "8": "[22] Y. Li, T. Zhao, and T. Kawahara, “Improved End-to-End Speech Emotion"
        },
        {
          "8": ""
        },
        {
          "8": "Recognition Using Self Attention Mechanism and Multitask Learning,”"
        },
        {
          "8": "in Interspeech 2019.\nISCA, Sep. 2019, pp. 2803–2807."
        },
        {
          "8": "[23]\nS.\nPascual, M. Ravanelli,\nJ.\nSerr`a, A. Bonafonte,\nand Y. Bengio,"
        },
        {
          "8": "“Learning\nProblem-Agnostic\nSpeech Representations\nfrom Multiple"
        },
        {
          "8": "Self-Supervised Tasks,”\nin Interspeech 2019.\nISCA, Sep. 2019, pp."
        },
        {
          "8": "161–165."
        },
        {
          "8": "[24] M. Ravanelli,\nJ.\nZhong,\nS.\nPascual,\nP.\nSwietojanski,\nJ. Monteiro,"
        },
        {
          "8": "J. Trmal, and Y. Bengio, “Multi-task self-supervised learning for Robust"
        },
        {
          "8": "Speech Recognition,” arXiv:2001.09239 [cs, eess], Jan. 2020."
        },
        {
          "8": "[25] A. van den Oord, Y. Li, and O. Vinyals, “Representation Learning with"
        },
        {
          "8": "Contrastive Predictive Coding,” CoRR, Aug. 2018."
        },
        {
          "8": "[26]\nJ. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are"
        },
        {
          "8": "in Neural\nInformation\nfeatures\nin deep neural networks?” in Advances"
        },
        {
          "8": "Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D."
        },
        {
          "8": "Lawrence, and K. Q. Weinberger, Eds.\nCurran Associates,\nInc., 2014,"
        },
        {
          "8": "pp. 3320–3328."
        },
        {
          "8": "[27]\nJ. Shor, D. Emanuel, O. Lang, O. Tuval, M. Brenner, J. Cattiau, F. Vieira,"
        },
        {
          "8": "M. McNally, T. Charbonneau, M. Nollstadt, A. Hassidim, and Y. Matias,"
        },
        {
          "8": "“Personalizing ASR for Dysarthric and Accented Speech with Limited"
        },
        {
          "8": "Data,” in Interspeech 2019.\nISCA, Sep. 2019, pp. 784–788."
        },
        {
          "8": "[28] G. Koch, R. Zemel, and R. Salakhutdinov, “Siamese Neural Networks"
        },
        {
          "8": "for One-shot Image Recognition,” ICML Deep Learning Workshop, p. 8,"
        },
        {
          "8": "2015."
        },
        {
          "8": "[29] R. Eloff, H. A. Engelbrecht,\nand H. Kamper,\n“Multimodal One-shot"
        },
        {
          "8": "ICASSP\n2019\n-\n2019\nIEEE\nLearning\nof\nSpeech\nand\nImages,”\nin"
        },
        {
          "8": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "8": "(ICASSP), May 2019, pp. 8623–8627."
        },
        {
          "8": "[30] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu, and D. Wierstra,"
        },
        {
          "8": "in Neural\n“Matching Networks\nfor One Shot Learning,”\nin Advances"
        },
        {
          "8": "Information Processing Systems 29, D. D. Lee, M. Sugiyama, U. V."
        },
        {
          "8": "Luxburg, I. Guyon, and R. Garnett, Eds.\nCurran Associates, Inc., 2016,"
        },
        {
          "8": "pp. 3630–3638."
        },
        {
          "8": "[31]\nJ. Snell, K. Swersky, and R. Zemel, “Prototypical Networks\nfor Few-"
        },
        {
          "8": "Information Processing Systems\nshot Learning,” in Advances in Neural"
        },
        {
          "8": "30,\nI. Guyon, U. V.\nLuxburg,\nS. Bengio, H. Wallach, R.\nFergus,"
        },
        {
          "8": "S. Vishwanathan, and R. Garnett, Eds.\nCurran Associates,\nInc., 2017,"
        },
        {
          "8": "pp. 4077–4087."
        },
        {
          "8": "[32]\nS. Ravi\nand H. Larochelle,\n“Optimization\nas\na Model\nfor Few-Shot"
        },
        {
          "8": "Learning,” in ICLR 2017, 2017, p. 11."
        },
        {
          "8": "[33] V. Garcia\nand J. Bruna, “Few-Shot Learning with Graph Neural Net-"
        },
        {
          "8": "works,” in ICLR 2018, 2018, p. 13."
        },
        {
          "8": "[34]\nS. Zhang, Y. Qin, K. Sun, and Y. Lin, “Few-Shot Audio Classiﬁcation"
        },
        {
          "8": "with Attentional Graph Neural Networks,” in Interspeech 2019.\nISCA,"
        },
        {
          "8": "Sep. 2019, pp. 3649–3653."
        },
        {
          "8": "[35]\nJ. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence,"
        },
        {
          "8": "R. C. Moore, M. Plakal, and M. Ritter, “Audio Set: An ontology and"
        },
        {
          "8": "human-labeled\ndataset\nfor\naudio events,”\nin 2017 IEEE International"
        },
        {
          "8": "Conference on Acoustics, Speech and Signal Processing (ICASSP). New"
        },
        {
          "8": "Orleans, LA:\nIEEE, Mar. 2017, pp. 776–780."
        },
        {
          "8": "[36]\nS. Zhang, H.\nJiang, S. Zhang, and B. Xu, “Fast SVM Training Based"
        },
        {
          "8": "INTER-\non the Choice of Effective Samples\nfor Audio Classiﬁcation,”"
        },
        {
          "8": "SPEECH 2006 -\nICSLP, p. 4, 2006."
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "ACKNOWLEDGMENT"
        },
        {
          "8": ""
        },
        {
          "8": "Vincent Roger doctorate is\nfounded by Federal University"
        },
        {
          "8": ""
        },
        {
          "8": "of Toulouse\nand Occitanie Region no2018-1290 (ALDOCT"
        },
        {
          "8": "no500). This work is part of the ANR-18-CE45-0008 RUGBI"
        },
        {
          "8": ""
        },
        {
          "8": "project\nfounded by French National Research Agency."
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines",
      "authors": [
        "J Barker",
        "S Watanabe",
        "E Vincent"
      ],
      "year": "2018",
      "venue": "ISCA"
    },
    {
      "citation_id": "2",
      "title": "TED-LIUM 3: Twice as much data and corpus repartition for experiments on speaker adaptation",
      "authors": [
        "F Hernandez",
        "V Nguyen",
        "S Ghannay",
        "N Tomashenko",
        "Y Estève"
      ],
      "year": "2018",
      "venue": "Speech and Computer -20th International Conference"
    },
    {
      "citation_id": "3",
      "title": "Librispeech: An ASR corpus based on public domain audio in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Librispeech: An ASR corpus based on public domain audio in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "VoxCeleb2: Deep Speaker Recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Interspeech 2018. ISCA"
    },
    {
      "citation_id": "5",
      "title": "Speech corpora of under resourced languages of northeast india",
      "authors": [
        "B Deka",
        "J Chakraborty",
        "A Dey",
        "S Nath",
        "P Sarmah",
        "S Nirmala",
        "S Vijaya"
      ],
      "year": "2018",
      "venue": "2018 Oriental COCOSDA -International Conference on Speech Database and Assessments"
    },
    {
      "citation_id": "6",
      "title": "Automatic speech recognition for under-resourced languages: A survey",
      "authors": [
        "L Besacier",
        "E Barnard",
        "A Karpov",
        "T Schultz"
      ],
      "year": "2014",
      "venue": "Automatic speech recognition for under-resourced languages: A survey"
    },
    {
      "citation_id": "7",
      "title": "Whistle-blowing ASRs: Evaluating the Need for More Inclusive Speech Recognition Systems",
      "authors": [
        "M Moore",
        "H Venkateswara",
        "S Panchanathan"
      ],
      "year": "2018",
      "venue": "Interspeech 2018. ISCA"
    },
    {
      "citation_id": "8",
      "title": "Severity-Based Adaptation with Limited Data for ASR to Aid Dysarthric Speakers",
      "authors": [
        "M Mustafa",
        "S Salim",
        "N Mohamed",
        "B Al-Qatab",
        "C Siong"
      ],
      "year": "2014",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "9",
      "title": "RWTH ASR Systems for LibriSpeech: Hybrid vs Attention",
      "authors": [
        "C Lüscher",
        "E Beck",
        "K Irie",
        "M Kitza",
        "W Michel",
        "A Zeyer",
        "R Schlüter",
        "H Ney"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "10",
      "title": "Attention is All you Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg"
    },
    {
      "citation_id": "11",
      "title": "Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System",
      "authors": [
        "C Kim",
        "M Shin",
        "A Garg",
        "D Gowda"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "12",
      "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition"
    },
    {
      "citation_id": "13",
      "title": "SWITCHBOARD: Telephone speech corpus for research and development",
      "authors": [
        "J Godfrey",
        "E Holliman",
        "J Mcdaniel"
      ],
      "year": "1992",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "15",
      "title": "Simulating dysarthric speech for training data augmentation in clinical speech applications",
      "authors": [
        "Y Jiao",
        "M Tu",
        "V Berisha",
        "J Liss"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Investigating Generative Adversarial Networks based Speech Dereverberation for Robust Speech Recognition",
      "authors": [
        "K Wang",
        "J Zhang",
        "S Sun",
        "Y Wang",
        "F Xiang",
        "L Xie"
      ],
      "year": "2018",
      "venue": "Investigating Generative Adversarial Networks based Speech Dereverberation for Robust Speech Recognition"
    },
    {
      "citation_id": "17",
      "title": "Generative Adversarial Networks for Unpaired Voice Transformation on Impaired Speech",
      "authors": [
        "L.-W Chen",
        "H.-Y Lee",
        "Y Tsao"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "18",
      "title": "Multi-Task Multi-Network Joint-Learning of Deep Residual Networks and Cycle-Consistency Generative Adversarial Networks for Robust Speech Recognition",
      "authors": [
        "S Zhao",
        "C Ni",
        "R Tong",
        "B Ma"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "19",
      "title": "Interpretable Convolutional Filters with SincNet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "NIPS 2018 Workshop IRASL"
    },
    {
      "citation_id": "20",
      "title": "Light Gated Recurrent Units for Speech Recognition",
      "authors": [
        "M Ravanelli",
        "P Brakel",
        "M Omologo",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Speech recognition with quaternion neural networks",
      "authors": [
        "T Parcollet",
        "M Ravanelli",
        "M Morchid",
        "G Linarès",
        "R Mori"
      ],
      "year": "2018",
      "venue": "NeurIPS 2018 -IRASL"
    },
    {
      "citation_id": "22",
      "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "23",
      "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
      "authors": [
        "S Pascual",
        "M Ravanelli",
        "J Serrà",
        "A Bonafonte",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "24",
      "title": "Multi-task self-supervised learning for Robust Speech Recognition",
      "authors": [
        "M Ravanelli",
        "J Zhong",
        "S Pascual",
        "P Swietojanski",
        "J Monteiro",
        "J Trmal",
        "Y Bengio"
      ],
      "year": "2020",
      "venue": "Multi-task self-supervised learning for Robust Speech Recognition",
      "arxiv": "arXiv:2001.09239"
    },
    {
      "citation_id": "25",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "A Van Den Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "How transferable are features in deep neural networks?",
      "authors": [
        "J Yosinski",
        "J Clune",
        "Y Bengio",
        "H Lipson"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
      "authors": [
        "J Shor",
        "D Emanuel",
        "O Lang",
        "O Tuval",
        "M Brenner",
        "J Cattiau",
        "F Vieira",
        "M Mcnally",
        "T Charbonneau",
        "M Nollstadt",
        "A Hassidim",
        "Y Matias"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "28",
      "title": "Siamese Neural Networks for One-shot Image Recognition",
      "authors": [
        "G Koch",
        "R Zemel",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "ICML Deep Learning Workshop"
    },
    {
      "citation_id": "29",
      "title": "Multimodal One-shot Learning of Speech and Images",
      "authors": [
        "R Eloff",
        "H Engelbrecht",
        "H Kamper"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "30",
      "title": "Matching Networks for One Shot Learning",
      "authors": [
        "O Vinyals",
        "C Blundell",
        "T Lillicrap",
        "D Wierstra"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Prototypical Networks for Fewshot Learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Optimization as a Model for Few-Shot Learning",
      "authors": [
        "S Ravi",
        "H Larochelle"
      ],
      "year": "2017",
      "venue": "Optimization as a Model for Few-Shot Learning"
    },
    {
      "citation_id": "33",
      "title": "Few-Shot Learning with Graph Neural Networks",
      "authors": [
        "V Garcia",
        "J Bruna"
      ],
      "year": "2018",
      "venue": "ICLR 2018"
    },
    {
      "citation_id": "34",
      "title": "Few-Shot Audio Classification with Attentional Graph Neural Networks",
      "authors": [
        "S Zhang",
        "Y Qin",
        "K Sun",
        "Y Lin"
      ],
      "year": "2019",
      "venue": "ISCA"
    },
    {
      "citation_id": "35",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Fast SVM Training Based on the Choice of Effective Samples for Audio Classification",
      "authors": [
        "S Zhang",
        "H Jiang",
        "S Zhang",
        "B Xu"
      ],
      "year": "2006",
      "venue": "ICSLP"
    }
  ]
}