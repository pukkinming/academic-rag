{
  "paper_id": "2309.06928v1",
  "title": "Dynamic Causal Disentanglement Model For Dialogue Emotion Detection",
  "published": "2023-09-13T12:58:09Z",
  "authors": [
    "Yuting Su",
    "Yichen Wei",
    "Weizhi Nie",
    "Sicheng Zhao",
    "Anan Liu"
  ],
  "keywords": [
    "in preceding utterances can exert influence on the emotion detection of subsequent utterances. Thus",
    "\"stars in a drag show\" may mean a joyful and lively performance scene",
    "but in fact",
    "the emotion label of the utterance is S"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion detection is a critical technology extensively employed in diverse fields. While the incorporation of commonsense knowledge has proven beneficial for existing emotion detection methods, dialogue-based emotion detection encounters numerous difficulties and challenges due to human agency and the variability of dialogue content. In dialogues, human emotions tend to accumulate in bursts. However, they are often implicitly expressed. This implies that many genuine emotions remain concealed within a plethora of unrelated words and dialogues. In this paper, we propose a Dynamic Causal Disentanglement Model based on hidden variable separation, which is founded on the separation of hidden variables. This model effectively decomposes the content of dialogues and investigates the temporal accumulation of emotions, thereby enabling more precise emotion recognition. First, we introduce a novel Causal Directed Acyclic Graph (DAG) to establish the correlation between hidden emotional information and other observed elements. Subsequently, our approach utilizes pre-extracted personal attributes and utterance topics as guiding factors for the distribution of hidden variables, aiming to separate irrelevant ones. Specifically, we propose a dynamic temporal disentanglement model to infer the propagation of utterances and hidden variables, enabling the accumulation of emotion-related information throughout the conversation. To guide this disentanglement process, we leverage the ChatGPT-4.0 and LSTM networks to extract utterance topics and personal attributes as observed information. Finally, we test our approach on two popular datasets in dialogue emotion detection and relevant experimental results verified the model's superiority.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "With the ongoing advancements in computer technology and increasing technological sophistication, human-computer communication has spread to a wide range of applications. It is essential for robots to sense users' emotions and provide coherent, empathetic responses. This underscores the significant potential value of dialogue emotion detection across diverse fields. Emotion detection can be used effectively in recommendation systems, medical contexts, education, and beyond. With the proliferation of interactive machines, the importance of emotion detection is steadily increasing. Due to the wide spectrum of emotional expressions, particularly within varying topics and contexts, a single utterance may contain different emotional states, thus presenting a challenge for emotion detection.\n\nYuting Su, Yichen Wei, Weizhi Nie, and Anan Liu are with the School of Electrical and Information Engineering, Tianjin University. Sicheng Zhao is with the School of Software, Tsinghua University.\n\nWeizhi Nie is the Corresponding author, Email: weizhinie@tju.edu.cn. With the exploration of NLP technology, researchers proposed some methods to solve emotional detection problems. Khare et al.  [1]  utilized CNN networks and proposed an emotion detection method based on time-frequency representation, using various convolutional neural networks for automatic feature extraction and classification. Wang et al.  [2]  extracted emotion features using wavelet packet analysis from speech signals for speaker-independent emotion detection. Jiao et al.  [3]  proposed an attention gated hierarchical memory network for real-time emotion detection processing. Huang et al.  [4]  proposed a Bi-LSTM network that can efficiently utilize past and future input features to classify. Majumder et al.  [5]  proposed a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and utilizes this information for emotion classification. Metallinou et al.  [6]  focused on temporary emotional context and demonstrated the effectiveness of context-sensitive in emotion detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. Motivation",
      "text": "We find that prevailing techniques in dialogue emotion detection directly utilize the text of dialogue utterances for feature extraction and modeling. This approach, however, results in words unrelated to emotion in dialogues impeding accurate emotion detection. We view these unrelated words as spurious correlation information. Furthermore, we recognize that keywords in preceding utterances can exert influence on the emotion detection of subsequent utterances. Thus, acquiring contextual information from prior dialogues plays a pivotal role in guiding emotion detection. As is shown in Fig.  1 , it is a dialogue between two friends from the MELD dataset. We marked the interfering words in the dialogue utterances in red color. For example, \"stars in a drag show\" may mean a joyful and lively performance scene, but in fact, the emotion label of the utterance is Sadness. We find the distant relationship mentioned in the previous dialogue can better explain the speaker's sadness in this utterance. We posit that the utilization of the previous context and the exclusion of spurious correlation information in utterances can significantly improve the accuracy of emotion detection.\n\nIn our work, to avoid the impact of spurious correlation information, we propose a Causal Disentanglement Model. Our objective is to model hidden variables which propagate to generate the utterances, and to separate hidden variables unrelated to emotions from other variables. The causal relationship is illustrated in Fig.  2  Among the hidden variables constituting the utterance U , namely s, v, and z, we designate s as the hidden variable associated with emotions and contributing to utterance composition, v as the hidden variable linked to dialogue topics related to emotions, and z as the hidden variable unrelated to emotions. As depicted in the figure, both s and v have a causal relationship with the emotion E, while v possesses a causal relationship with the dialogue topic F . We apply ChatGPT to complete the text feature extraction task, i.e., extraction of the dialogue topic F . Subsequently, we augment the dataset with the extracted topic text and topic feature vectors for future research. Notably, throughout each time stage, the evolution and generation of all hidden variables are influenced by the auxiliary variable P , which represents personal attribute information derived from previous conversations using an LSTM network. In addition, we propose a sequential VAE framework  [7]  to reconstruct dialogue utterance U and dialogue topic F as part of our model optimization process.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Contributions",
      "text": "This paper makes the following main contributions:\n\n• We propose a Dynamic Causal Disentanglement Model, which aims to separate hidden variables unrelated to emotions from dialogue utterances. Our model significantly improves the robustness of emotional features. • We apply ChatGPT to complete the feature extraction task and add the topic text and feature vectors to the existing dataset. • We evaluate the performance of our method on the IEMOCAP and MELD datasets. The experimental results indicate that our method has significant improvements compared to the most advanced methods. In Section II, we present the relevant work. In Section III, we briefly introduce the background of the causal model. In Section IV, we introduce the specific content of the model and the calculation formula of the methodology in detail. In Section V, we present key experiments. In Section VI, we present the conclusions of our work and discuss future research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Detection",
      "text": "Emotion detection represents a prominent and actively researched subfield within natural language processing, finding widespread application in diverse domains, including the medical field, intelligent devices, and the Internet of Things  [8] -  [11] . The initial research on dialogue emotion detection is mainly based on audio features and video features  [12] . Recently, the development of deep learning drives the research on dialogue emotion detection in text  [13]  and speech  [14] .\n\nLong Short Term Memory (LSTM) networks  [15]  is a temporal recurrent neural network. It is one of the most common and effective methods for dealing with sequential tasks. Poria et al.  [16]  proposed a detection model-based LSTM that enables discourse to capture contextual information from the surrounding environment within the same video. Li et al.  [17]  proposed a multimodal attention-based BLSTM network framework for detection. They propose Attentionbased Bidirectional Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to automatically learn the best temporal features that are useful for detection. Huang et al.  [18]  propose a novel Hierarchical LSTMs for Contextual Emotion Detection (HRLCE) model.\n\nIn recent studies, the incorporation of common sense knowledge has been increasingly prevalent. In tasks involving inference, the integration of common sense knowledge facilitates the deduction of implicit information from textual content. Ghosal et al.  [19]  proposed COSMIC, a new framework that includes different commonsense elements to assist in detection. Zhong et al.  [20]  leveraged commonsense knowledge to enrich the transformer encoder. Li et al.  [21]  proposed a conversation modeling module to accumulate information from the conversation and proposed a knowledge integration strategy to integrate the conversation-related commonsense knowledge generated from the event-based knowledge graph. Bosselut et al.  [22]  proposed commonsense transformers (COMET) which are used to learn to generate rich and diverse commonsense descriptions in natural language.\n\nGraph models are widely applied to process and analyze semantics and relations in the texts. The latest progress in graph model research has also been introduced into the field of emotion detection. RGAT  [23]  can capture the speaker dependency and the sequential information with the relational position encodings which provide the RGAT model with sequential information reflecting the relational graph structure. IGCN  [24]  applied the incremental graph structure to imitate the process of dynamic conversation. Choi et al.  [25]  utilized the residual network (ResNet)-based, intrautterance feature extractor and the GCN-based, interutterance feature extractor to fully exploit the intra-inter informative features.\n\nNevertheless, these approaches fail to disentangle features causally related to emotions from dialogue utterances, which results in spurious correlations and diminished robustness in feature extraction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Causal Hidden Markov Model",
      "text": "The Causal Hidden Markov Model is a common statistical model, which is widely utilized in many time series problems, including image processing, natural language processing, bioinformatics, and other fields. The Causal Hidden Markov Model is an extension of the Markov process that integrates hidden states. In contrast to the standard Markov Model, the Causal Hidden Markov Model excels in capturing the causal relationship between observation sequences and hidden state sequences by acquiring knowledge of the transition probabilities and probability distributions of observations and hidden states. This enhanced capability contributes to a more effective modeling of time series data. Many methods are proposed based on Causal Hidden Markov Models. Khiatani et al.  [26]  constructed the Hidden Markov Model to learn the relationship between weather states and hidden state sequences, achieving more accurate weather prediction. ZHANG et al.  [27]  utilized the Factorial Hidden Markov Model Based on the Gaussian Mixture Model for non-invasive load detection. Guo et al.  [28]  attempted to accurately predict the chloride ion concentration that causes bridge degradation, and in the face of random detection time intervals, proposed a predictive Hidden semi-Markov Model to achieve prediction. Zhao et al.  [29]  proposed a Causal Conditional Hidden Markov Model to predict multimodal traffic flow, which respectively designs a prior network and a posterior network to mine the causal relation in the hidden variables inference stage. Mak et al.  [30]  proposed a Causal Topology Design Hidden Markov Model to solve viewpoint variation issue, which can help view independent multiple silhouette posture recognition. Suphalakshmi et al.  [31]  utilized a Full Causal Two Dimensional Hidden Markov Model with a novel 2D Viterbi algorithm for image segmentation and classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Preliminaries",
      "text": "We commence with an introduction to Structural Causal Models (SCMs). The SCM refers to a causal graph represented by a directed acyclic graph associated with structural equations. The causal graph is denoted as G := (V, E), where V and E represent node set and edge set respectively. In the edge set, each arrow x → y (x, y ∈ E) indicates that x has a direct influence on y and changes the distribution of y. The structural equation contains the production relation of each variable in the point set. In the node-set, for V := {v 1 , . . . , v k }, we define p(v i ) to represent the set of parent nodes of v i , and the set of the parent nodes will affect it. Through the production relation, we define the relation as\n\nwhere f i denotes causal mechanisms.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Methodology",
      "text": "Inspired by the Causal Hidden Markov Model  [32] , we propose the Dynamic Causal Disentanglement Model to model dialogues as a novel approach for modeling dialogues. Our modifications and enhancements to the model are tailored to facilitate the detection of emotional content within dialogue utterances. In our Dynamic Causal Disentanglement Model, we denote observed variable f t ∈ F , u t ∈ U , p t ∈ P as the dialogue topics, dialogue utterances, and personal attributes at time stage t. The e t ∈ E is the predicted emotion label at time stage t. We disentangle the utterance into hidden variables s t , v t , z t . To observe the progress of dialogues clearly, we design an LSTM network to learn the features of the speaker's attributes at the current time stage. Due to the recent popularity of ChatGPT, we apply ChatGPT-4.0 to extract dialogue topic features.\n\nIn this section, we introduce the Dynamic Causal Disentanglement Model in detail and elucidate the methods used for learning hidden variables within the model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dynamic Causal Disentanglement Model",
      "text": "To describe the dialogue progresses, we introduce the causal directed acyclic graph (CDAG) shown in Fig.  2 . In the CDAG, the causal relation of variable m is formulated as In causal learning, we propose prior and posterior networks to learn the distribution of variables to achieve disentanglement. In the prior network, we input the concatenated vectors into the GRU and subsequently design two independent Fully Connected (FC) layers to obtain the mean and the log variance vectors. Within the posterior network, we input concatenated vectors into the FC layer, similar to the prior network, to obtain mean and log variance vectors. The hidden variable data obtained from the posterior network is then utilized for the reconstruction of observed variables and the prediction of labels.\n\nm and ε m is independent exogenous variables. All variables form a Hidden Markov model. We define the model as follows:\n\nTo enhance detection accuracy, our objective is to disentangle emotion-related information. We introduce hidden variables s t , v t , and z t to construct the observation variables U t , F t , and E t at respective time stages. Concurrently, the hidden variables are influenced by the auxiliary variable P t , which undergoes continuous updates throughout the ongoing dialogue. s t represents the hidden variable associated with emotions and contributing to utterance composition, v t represents the hidden variable linked to dialogue topics related to emotions and z t represents the hidden variable unrelated to emotions. Collectively, these hidden variables form a comprehensive utterance representation. The hidden variables s t and v t , which impact emotional assessment, are related to the emotion label E t , whereas the hidden variable v t pertaining to the dialogue topic solely influences the dialogue topic F t . Improved predictive accuracy can be achieved by isolating the hidden variables contributing to the utterance and eliminating extraneous variables. For simplicity, we define h := {s, v, z} ; o := {P, F, U }. According to Causal Markov Condition  [33] , we calculate the factorization of joint distribution as:\n\nWe aim to analyze the distribution of hidden variables from the prior distribution p ψ (h t |h t-1 , P t ). Due to the difficulty of calculation, we choose to utilize posterior distribution q ϕ (h ≤T |o ≤T , E ≤T ) for approximation  [34] . p(E t |s t , v t ), p(U t |h t ), p(F t |v t ) are the processing of learned hidden variables in the generation network. After defining the joint probability distribution, we propose a sequential VAE framework to learn our causal model, as shown in Fig.  3 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "1)",
      "text": "Prior Network: For establishing prior distributions, we introduce a prior network into our model. This prior network takes both the observed variable P and the hidden variables from the preceding time step as inputs. We utilize Gated Recurrent Units (GRUs)  [35]  to propagate hidden variables, enhancing the model's capacity to capture long-term dependencies. After the GRUs unit, we design independent Fully Connected layers (FCs) to obtain the mean and the log variance vectors of the hidden variables. We utilize the hidden variable h ∈ {s, v, z} to illustrate the disentanglement propagation process of GRUs as follows:\n\n(2) r p t and z p t represent the status of the reset gate and update gate. Hidden update status h p t is determined by candidate hidden status hp t and update gate status z p t . In the above formula, σ represents the sigmoid function, and ⊙ represents the elementwise multiplication operation. All hidden variables are learned in the same method.\n\n2) Posterior Network: In probability graph models, solving the posterior distribution of hidden variables through Bayesian formulas can be challenging. Consequently, we opt for using a known simple distribution to approximate the intricate distribution that requires inference. We utilize distinct units to learn posterior distributions as follows:\n\nSimilar to prior networks, we apply Fully Connected layers (FCs) to obtain the mean and the log variance vectors of the hidden variables. These vectors will be utilized for reconstruction in the following section.\n\n3) Generation Network: In the generative network, we employ the updated hidden variables to reconstruct the observed variables and predict emotion labels. To guarantee the representation ability of learned variables, we utilize s q t , v q t , z q t to obtain the reconstructed utterance Û , and v q t to obtain the reconstructed conversation topic F . We optimize our model by approximating the reconstructed variables Û and F to the observed variables U and F .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Learning Method",
      "text": "The optimization target is the Evidence Lower Bound (ELBO). ELBO consists of evidence and KL divergence and maximizing ELBO is tantamount to minimizing the KL divergence, intending to approximate the posterior distribution as closely as possible to the true posterior distribution. ELBO is represented as:\n\nwhere D(• ∥ •) denotes KL divergence. After simplifying the evidence and KL divergence, we obtain:\n\n(5) According to Eq.1, we know the prior distribution p ψ . We consider the mean and the log variance vectors in the prior network as the learned results, p ψ (α t |α t-1 , P t ) for each t is distributed as N (µ ψ (α t-1 , P t ), Σ ψ (α t-1 , P t )). In the posterior distribution, the posterior is given by:\n\n.\n\n(6) Under the similar reparameterization, q ϕ (h t |h t-1 , o t ) for each t is distributed as N (µ(h t-1 , o t ), Σ(h t-1 , o t )). Substituting Eq.1 and Eq.6 in Eq.5, we reformulate the ELBO as:\n\nIn Eq.7, following the derivation based on probability theory, we know that q ϕ (E ≤T |o ≤T ): and L q ϕ ,p ψ denotes as follows:\n\nwhere the L 1 , L 2 , L 3 are respectively defined as:\n\nWe parameterize the p ψ (E t |s t , v t ) as q ϕ (E t |s t , v t ), making L 2 degenerate to 0. At each time stage t, the L 1 denotes the reconstruction loss, while the L 3 denotes the KL divergence of the hidden variables.\n\nC. Feature extraction based on ChatGPT-4.0\n\nChatGPT is a large-scale natural language processing model developed by OpenAI. Following extensive training, ChatGPT exhibits exceptional competence in both comprehending and generating natural language text, making it versatile for diverse natural language processing tasks. For the task of dialogue emotion detection, we rely on ChatGPT for pertinent text feature extraction.\n\nIn our approach, obtaining the dialogue utterance's topic as the observation variable is essential. As the conversation unfolds, the topic continuously evolves, necessitating the acquisition of topic features at each time stage. We have selected ChatGPT-4.0 for this task due to its superior contextual understanding in text processing tasks. Specifically, it excels in providing more precise topic information by comprehending preceding dialogues. A visual representation of topic extraction is depicted in Fig.  4 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Experiment",
      "text": "We apply our method to IEMOCAP and MELD datasets. Subsequently, we conducted ablation experiments and robustness analyses. Initially, we present the data conditions and implementation details. A. Datasets IEMOCAP  [36]  is a multimodal conversation dataset collected by the SAIL laboratory at the University of Southern California. This dataset comprises 151 dialogues, totaling 7433 sentences. The dataset is annotated with six emotion categories: neutral, happy, sad, angry, frustrated, and excited. Non-neutral emotions constitute a majority at 77%. IEMOCAP stands as the most widely utilized dataset in the field of dialogue emotion detection, characterized by its high-quality annotations. However, it is worth noting that the dataset has a relatively limited data size.\n\nMELD  [37]  is an extension of the EmotionLines dataset  [38] , augmented with additional audio and video information. The MELD dataset comprises 1433 multi-party dialogues and 13708 utterances extracted from the television series \"Friends\". This dataset provides annotations for utterances, categorizing them into seven distinct emotion categories: neutral, joy, surprise, sadness, anger, disgust, and fear. Non-neutral emotions constitute a majority at 53%. MELD is distinguished by its high-quality annotations and the incorporation of multimodal data. Statistics of splits in two datasets are shown in Table .I.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Baseline And State-Of-The-Arts",
      "text": "BC-LSTM  [16]  proposes a bidirectional LSTM network to capture contextual content, generate context aware discourse representations, and utilize them for sentiment classification. However, this model only considers context dependencies and ignores speaker dependencies.\n\nCMN  [39]  utilizes the dependency relationships of speakers to identify emotions. It designs different GRU networks for two speakers to model utterance context from dialogue history and inputs current utterances into different memory networks to learn utterance representation. However, this model is limited to conversations with only two participants.\n\nICON  [40]  is an extension of CMN that utilizes another GRU to connect the GRU network outputs of two speakers for explicit speaker modeling. Similarly, it can only model conversations between two speakers.\n\nConGCN  [41]  constructs a heterogeneous graph convolutional neural network. The utterances and speakers of a conversation are represented by nodes. The graph contains edges between utterance nodes and edges between utterance and speaker nodes. This method models context and speakersensitive dependencies for emotion detection.\n\nKET  [20]  is a knowledge-enriched transformer for emotion detection. It dynamically leverages common sense knowledge by using a context aware graph attention mechanism.\n\nDialogueRNN  [5]  is a benchmark model for dialogue emotion detection. It is a recurrent network that utilizes three GRU networks to model the speaker, the context from the preceding utterances, and the emotion of the preceding utterance to track the emotion of each speaker.\n\nDialogueGCN  [42]  represents the structure and interaction of a conversation by constructing a graph structure between participants and utilizes graph convolutional neural networks for message passing and feature learning of the graph structure.\n\nKES  [43]  utilizes a self-attention layer specialized for enhanced semantic text features with external commonsense knowledge and two networks based on LSTM for tracking individual internal state and context external state to learn interactions between interlocutors participating in a conversation.\n\nTL-ERC  [44]  utilizes a generative conversational model to transfer emotional knowledge. It transfers the parameters of a trained hierarchical model to an emotion detection classifier. SS-HiT  [45]  is a novel semantic and sentiment hierarchical transformer for emotion detection. Each utterance token is represented as matrices with both semantic and sentiment word embeddings. Then, fuse utterance tokens as token features to further capture the long dependent utterance-level information through transformer encoders for prediction.\n\nSentic GAT  [46]  is a context-and sentiment-aware framework, which designs a dialogue transformer (DT) network with hierarchical multihead attention to capture the intra-and inter-dependency relationship in context. In addition, commonsense knowledge is dynamically represented by the contextand sentiment-aware graph attention mechanism based on sentimental consistency.\n\nEmpaGen  [47]  proposes an auxiliary empathic multiturn dialogue generation to enhance dialogue emotion understanding, which is the first attempt to utilize empathy-based dialogue generation for the emotion detection task.\n\nGGCN  [48]  is a growing graph convolution network that proposes a prior knowledge extraction framework to get auxiliary information. The method considers utterance connections and emotion accumulation for emotional detection.\n\nIEIN  [49]  is an iterative emotional interaction model that utilizes iteratively predicted emotional labels instead of real emotional labels to continuously correct predictions and provide feedback on inputs during the iteration process.\n\nSDTN  [50]  dynamically tracks the local and global speaker states as the conversation progresses to capture implicit stimulation of emotional shift. It mainly contains the speaker interaction tracker based on GRU and the emotion state decoder module based on the conditional random field.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Implementation Details",
      "text": "In this section, we will furnish a comprehensive overview of the implementation of our proposed method. We preextract topic features and personal attribute features from the prior dialogue content. For the extraction of personal attribute features, we design an LSTM network and specify the feature vector dimension as 64.\n\nWe utilize ChatGPT-4.0 API with Python 3 on a single Nvidia RTX 3060 GPU and an Intel i7 CPU. We configure ChatGPT-4.0 as a text work assistant for the task of text extraction. Following the role assignment, we provide the prompt: \"I will give you a set of daily dialogue, understand the meaning of the entire dialogue, and extract the topic from each utterance in conjunction with the previous context. Use concise vocabulary to accurately express the topic.\" Additionally, we specify detailed format requirements to facilitate future text collection and dataset construction. To ensure a comprehensive understanding of dialogue content and context, we input entire dialogues into the model, instructing it to extract topic information for each utterance. However, in the case of lengthy dialogues with numerous utterances, the model occasionally miscounted them, leading to a mismatch between the number of generated topics and utterances. To address this, we segment long dialogues into batches, each containing 20 utterances. This approach allows ChatGPT-4.0 to better grasp the context and accurately determine the number of utterances. We apply the bert-base-uncased model for processing English text to convert topic text information into 768-dimension feature vectors for subsequent experiments, which is one of the pre-trained BERT models  [51] . Then we utilize two linear layers to reduce the dimensionality of the feature vectors to 64 dimensions.\n\nThen, we construct the Dynamic Causal Disentanglement Model by combining all observed variables. The parameter matrices of the observation variable extraction model and the model are randomly initialized and continuously optimized during training. Throughout the training process, we utilize Adam as the optimizer with the weight attenuation set to 0.00005 and the learning rate set to 0.001. The model is trained for 80 epochs. We develop the evaluations with Python 3 and PyTorch 1.13.0 on a single Nvidia RTX 3060 GPU and an Intel i7 CPU.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Comparison With State-Of-The-Arts",
      "text": "We compare the performance of our proposed model with the current state-of-the-art methods. The experimental results show that our model performs better than other state-of-the-art models on two benchmark datasets.\n\nIEMOCAP: IEMOCAP is a dyadic interactive dialogues dataset consisting of long conversations and it contains many non-neutral emotions. We evaluate our model on the IEMO-CAP dataset, and the experiment results are shown in Table .II. Our model achieves a new state-of-the-art F1-score of 68.9% and accuracy of 68.8% for the emotion detection task on the IEMOCAP dataset. We compare the results of our proposed model with the state-of-the-art methods. Although our model still needs to improve the performance in some emotional categories, it is worth noting that our model has made significant improvements in both Happy and Frustrated emotions. Compared with the IEIN method, our model demonstrates a critical improvement of 1.2% in the average F1-score of Happy emotion. Compared with the state-of-the-art methods, our model achieves improvements in the accuracy of 0.7% and the average F1-score of 0.8% for the Neutral emotion. Compared with the GGCN method, our model achieves a significant improvement of 0.5% and 1.9% in the average F1score of the Excited and Frustrated emotions. In general, our approach outperforms the state-of-the-art methods.\n\nMELD: The MELD dataset comprises multiparty conversations extracted from television series. Notably, many MELD dialogues involve more than five participants, with a maximum of nine, resulting in limited utterances per participant and posing challenges for contextual dependency modeling. Furthermore, the dataset exhibits a substantial presence of nonneutral emotions. In contrast to the IEMOCAP dataset, MELD  dialogues are notably shorter, with an average length of 10 utterances, intensifying the challenges in emotion detection. Importantly, we find that the CMN and ICON methods, designed for dyadic conversations, are not suitable for MELD. These emotion detection models achieve poor results on this dataset.\n\nOur model achieves a new state-of-the-art F1-score of 67.5%, and our model obtains the best results among the compared methods, the experiment results are shown in Table.III. Our model has made significant progress in Anger, Joy, and Surprise emotions. Although the F1-score of our model is not as good as IEIN and GGCN, compared to most methods, we still make progress in Disgust and Fear emotions. We observe that the topic features extracted by ChatGPT-4.0 substantially enhance our model's performance, enabling a deeper understanding of utterance meanings and the speakers' psychology in brief conversations involving multiple participants. Consequently, our model demonstrates exceptional performance on the MELD dataset.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Ablation Study",
      "text": "To underscore the effectiveness of hidden variables disentanglement and the significance of external variables in our approach, we undertake an ablation study. The comparison results are in Table .IV, we conclude that leveraging relevant hidden variables derived from disentanglement aids in emotion detection. Our approach demonstrates significant improvements in accuracy and F1-score when compared to models lacking disentanglement. The result validates our idea and also proves the effectiveness of disentanglement.\n\nWe observe that when employing LSTM-extracted topic features and personal attribute features separately, the models incorporating personal attribute features outperformed those relying on topic features by 1.7% and 1.3% in terms of F1scores for both datasets, respectively. Our analysis suggests that personal attribute features, serving as auxiliary variables in the propagation of hidden variables, elucidate the distribution of hidden variables, thereby exerting a more pronounced influence on result enhancement. We find that in comparison to the model using theme features extracted by LSTM, the models incorporating theme features extracted by ChatGPT-4.0 and BERT exhibited improvements of 0.9% and 0.8% in F1score on both datasets, respectively, demonstrating the superior ability of ChatGPT-4.0 in text processing.\n\nThe model combining two observation variables and disentanglement achieves the best results, which verifies the effectiveness of our proposed method and all observed variables. Overall, the introduction of observed variables and the disentanglement of hidden variables collectively enhance the accuracy of emotion detection.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Discussion On Prediction Accuracy Of Long Dialogue Utterances",
      "text": "To further assess the stability of our proposed model, we calculate the accuracy of emotion detection within each period, as illustrated in Fig.  5 . Specifically, for the IEMOCAP dataset, we divide every five utterances into a time batch, and due to the extended length of dialogues in this dataset, we analyze the first forty utterances. In the case of MELD, a dataset featuring shorter dialogues, we segment each utterance into a time batch and analyze the first eight utterances.\n\nWe observe that the emotion detection methods BC-LSTM and DialogueRNN exhibit lower accuracy in the early stages of a dialogue, with accuracy gradually improving as the dialogue progresses. The results suggest that these contextual modeling methods require a greater number of utterances to comprehend ongoing dialogues due to the changing dialogue topics and contexts. In contrast, our method effectively mitigates this challenge. By disentangling hidden variables related to emotions at the outset of the dialogue, our model achieves high accuracy in the early time batch. Furthermore, our prediction accuracy remains stable and consistently high, facilitated by the incorporation of dialogue topics and personal attributes.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "G. Effectiveness Of Disentanglement",
      "text": "To demonstrate the reliability of the related variables after disentanglement, we implement a further validation method for s, v, and z. Specifically, we first train the Dynamic Causal Disentanglement Model and the observation variable extraction model to obtain the distribution of hidden variables. Then, we independently train six independent classifiers for prediction by different combinations of hidden variables, and the experiment results are shown in Table .V. We find that the results of the prediction containing z are lower than those without z, which proves the existence of spurious correlation information in the utterances. The experiments conducted with s and v both yield favorable results. The accuracy and F1 score obtained from variable s surpass those extracted from variable v on two datasets, suggesting that s harbors more salient feature information about emotional factors. The experimental results obtained by combining s and v are the best, affirming the validity and effectiveness of our approach.\n\nTo intuitively observe the influence of hidden variables on emotional features, we visualize emotional features acquired under varying conditions on the IEMOCAP dataset to examine emotion distribution within the feature space. Employing the t-SNE method  [52] , we project high-dimensional text features into a two-dimensional space, utilizing distinct colors for emotion labels, as depicted in Fig.  6 . In the two-dimensional space after dimensionality reduction, the inter-point distances serve to reflect high-dimensional data similarities. Observing the distribution of the text feature vectors obtained by preprocessing, we find most utterances with different emotions are bounded by each other, but a small number of utterances still exhibit a messy distribution. Simultaneously, we observe that utterances with non-neutral emotions and utterances with neutral emotions are close in two-dimensional space, implying their higher similarity in high-dimensional feature vectors. In contrast, the performance of the text feature vectors learned by the hidden variable s, v, and z demonstrates some enhancement. Utterances with neutral emotions tend to stay away from utterances with other emotions. However, the introduction of unrelated variable z constrains more precise boundary distinctions. Notably, we find that the text feature vectors learned by relevant variables s and v with the separation of the unrelated variable z achieve the best performance. The distinction between utterances with different emotions is more accurate, and the distance in the two-dimensional space is far, indicating the low similarity of high-dimensional features. The result further validates our perspective.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "H. Visualization Of Confusion Matrix And Analysis",
      "text": "We show the visualization to observe the comparison of emotion classification results between different methods. The confusion matrixes of different methods on the IEMOCAP and MELD datasets are shown in Fig.  7 .\n\nIn the case of the IEMOCAP dataset, experimental results reveal a notably high misprediction rate between the Neutral and Frustrated emotions. Our analysis indicates that, in some instances, humans may not express their frustration directly in their utterances, making it significantly different from sadness. This observation underscores that a speaker's emotion cannot be fully comprehended based solely on dialogue utterances. Our method makes up for the shortcomings of BC-LSTM, DialogueRNN, and other methods and we achieve a significant improvement of 0.8% and 1.9% in the average F1-score of the Neutral and Frustrated emotion, which demonstrates our model has a more comprehensive understanding of dialogue utterance. Furthermore, our method enhances F1-score accuracy across other emotional categories. We attribute this improvement to the separation of interfering words and the utilization of ChatGPT-4.0 for topic extraction. In the MELD dataset, the imbalance in emotion labels poses a challenge Fig.  8 . The importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase \"crush on you\" is a mention of the past rather than an expression of love. The reply \"oh\" is a response of sadness rather than a common surprise. Our method can comprehensively understand the meaning of dialogue utterances.\n\nto emotion detection, leading to many non-neutral emotions being incorrectly classified as Neutral in the validation set. Through a thorough examination of the confusion matrix, We observe that our model substantially enhances the accuracy of Fear and Disgust emotions detecting, in contrast to other models incapable of detecting these specific emotions. Our model achieves remarkable enhancements in accuracy and F1scores across multiple emotional categories.\n\nWe choose a dialogue from the MELD dataset to validate our viewpoint, as shown in Fig.  8 . The phrase \"crush on you\" mentioned in the first sentence does not convey an expression of love but rather refers to a concerning reference to events from the past, which may mislead our prediction. Within the dialogue, when Ross responds with \"oh\" upon being characterized as geeky, this response initially appears surprising. However, considering the context from the preceding text, we discern that Ross failed to make a favorable impression on Rachel, with whom he had a romantic interest. This understanding, combined with the extracted topic, indicates that his response carries an underlying sense of disappointment.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We propose a Dynamic Causal Disentanglement model for emotion detection in dialogues. In our model, we introduce hidden variables to disentangle hidden variables and learn the causal relationships between utterances and emotions. We optimize our model by maximizing the ELBO. Experimental results and analysis on two popular datasets demonstrate the high accuracy and robustness of our model. In future research, we intend to incorporate more precise representations of dialogue as observational variables to further enhance the analysis of dialogues, with the expectation of achieving even greater performance.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The motivation diagram from the MELD dataset. Red-marked words",
      "page": 1
    },
    {
      "caption": "Figure 2: Directed acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue",
      "page": 3
    },
    {
      "caption": "Figure 3: The network structure of Dynamic Causal Disentanglement Model. In causal learning, we propose prior and posterior networks to learn the distribution",
      "page": 4
    },
    {
      "caption": "Figure 4: The process of extracting conversation topics utilizing ChatGPT-4.0.",
      "page": 5
    },
    {
      "caption": "Figure 5: Prediction accuracy of long dialogue utterances on the IEMOCAP",
      "page": 8
    },
    {
      "caption": "Figure 6: The feature mappings of high-dimensional vectors obtained by t-SNE. Dots of various colors represent utterances with distinct emotion labels.",
      "page": 9
    },
    {
      "caption": "Figure 7: Confusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.",
      "page": 10
    },
    {
      "caption": "Figure 8: The importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of the past",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "Abstract—Emotion detection is a critical technology extensively"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "employed\nin\ndiverse\nfields. While\nthe\nincorporation\nof\ncom-"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "monsense knowledge has proven beneficial\nfor existing emotion"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "detection methods, dialogue-based emotion detection encounters"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "numerous difficulties and challenges due to human agency and"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "the variability of dialogue content. In dialogues, human emotions"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "tend to accumulate in bursts. However,\nthey are often implicitly"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "expressed. This\nimplies\nthat many\ngenuine\nemotions\nremain"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "concealed within a plethora of unrelated words and dialogues."
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "In this paper, we propose a Dynamic Causal Disentanglement"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "Model based on hidden variable\nseparation, which is\nfounded"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "on the\nseparation of hidden variables. This model\neffectively"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "decomposes the content of dialogues and investigates the temporal"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "accumulation of emotions, thereby enabling more precise emotion"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "recognition. First, we introduce a novel Causal Directed Acyclic"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "Graph (DAG)\nto establish the correlation between hidden emo-"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "tional\ninformation and other observed elements. Subsequently,"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "our\napproach\nutilizes\npre-extracted\npersonal\nattributes\nand"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "utterance topics as guiding factors for the distribution of hidden"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "variables,\naiming\nto\nseparate\nirrelevant\nones.\nSpecifically, we"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "propose\na\ndynamic\ntemporal\ndisentanglement model\nto\ninfer"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "the propagation of utterances\nand hidden variables,\nenabling"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "the accumulation of emotion-related information throughout\nthe"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "conversation. To guide this disentanglement process, we leverage"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "the ChatGPT-4.0 and LSTM networks to extract utterance topics"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "and\npersonal\nattributes\nas\nobserved\ninformation. Finally, we"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "test our approach on two popular datasets\nin dialogue emotion"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "detection and relevant experimental results verified the model’s"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "superiority."
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "Index Terms—Emotion Detection; Structural Causal Model,"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "Dynamic Causal Disentanglement Model"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "I.\nINTRODUCTION"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "With\nthe\nongoing\nadvancements\nin\ncomputer\ntechnology"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "and increasing technological\nsophistication, human-computer"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "communication has spread to a wide range of applications.\nIt"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "is essential\nfor\nrobots\nto sense users’ emotions and provide"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "coherent, empathetic responses. This underscores\nthe signif-"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "icant\npotential\nvalue\nof\ndialogue\nemotion\ndetection\nacross"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "diverse fields. Emotion detection can be used effectively in"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": "recommendation\nsystems, medical\ncontexts,\neducation,\nand"
        },
        {
          "Yuting Su, Yichen Wei, Weizhi Nie*, Sicheng Zhao, Anan Liu, Senior Member": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "knowledge generated from the event-based knowledge graph."
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "Bosselut\net\nal.\n[22]\nproposed\ncommonsense\ntransformers"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "(COMET) which are used to learn to generate rich and diverse"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "commonsense descriptions in natural\nlanguage."
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "Graph models\nare widely applied to process\nand analyze"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "semantics\nand relations\nin the\ntexts. The\nlatest progress\nin"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "graph model\nresearch has also been introduced into the field"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "of\nemotion\ndetection. RGAT [23]\ncan\ncapture\nthe\nspeaker"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "dependency and the sequential\ninformation with the relational"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "position\nencodings which\nprovide\nthe RGAT model with"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "sequential information reflecting the relational graph structure."
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "IGCN [24] applied the incremental graph structure to imitate"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "the process of dynamic conversation. Choi et al.\n[25] utilized"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "the\nresidual\nnetwork\n(ResNet)-based,\nintrautterance\nfeature"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "extractor and the GCN-based,\ninterutterance feature extractor"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "to fully exploit\nthe intra-inter\ninformative features."
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": ""
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "Nevertheless,\nthese approaches\nfail\nto disentangle features"
        },
        {
          "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense": "causally related to emotions from dialogue utterances, which"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on\nthe\nemotion\ndetection\nof\nsubsequent\nutterances. Thus,",
          "2": "II. RELATED WORK"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "acquiring contextual\ninformation from prior dialogues plays",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a pivotal\nrole in guiding emotion detection. As\nis\nshown in",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "A. Emotion detection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fig.1,\nit\nis a dialogue between two friends\nfrom the MELD",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dataset. We marked\nthe\ninterfering words\nin\nthe\ndialogue",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "utterances\nin red color. For example, “stars\nin a drag show”",
          "2": "Emotion detection represents a prominent and actively re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "may mean a joyful and lively performance scene, but\nin fact,",
          "2": "searched\nsubfield within\nnatural\nlanguage\nprocessing, find-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the\nemotion label of\nthe utterance\nis Sadness. We find the",
          "2": "ing widespread application in diverse domains,\nincluding the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "distant\nrelationship mentioned in the previous dialogue\ncan",
          "2": "medical field,\nintelligent devices, and the Internet of Things"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "better explain the speaker’s sadness in this utterance. We posit",
          "2": "[8]–[11]. The initial\nresearch on dialogue emotion detection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "that the utilization of the previous context and the exclusion of",
          "2": "is mainly based on audio features\nand video features\n[12]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spurious correlation information in utterances can significantly",
          "2": "Recently, the development of deep learning drives the research"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "improve the accuracy of emotion detection.",
          "2": "on dialogue emotion detection in text\n[13] and speech [14]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In our work,\nto avoid the\nimpact of\nspurious\ncorrelation",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Long\nShort Term Memory\n(LSTM)\nnetworks\n[15]\nis\na"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information, we\npropose\na Causal Disentanglement Model.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "temporal\nrecurrent\nneural\nnetwork.\nIt\nis\none\nof\nthe most"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Our objective is\nto model hidden variables which propagate",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "common and effective methods\nfor dealing with sequential"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to generate\nthe utterances,\nand to separate hidden variables",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "tasks.\nPoria\net\nal.\n[16]\nproposed\na\ndetection model-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "unrelated to emotions\nfrom other variables. The causal\nrela-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "LSTM that enables discourse to capture contextual information"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tionship is\nillustrated in Fig.2 Among the hidden variables",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "from the surrounding environment within the same video. Li"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "constituting\nthe\nutterance U ,\nnamely\ns,\nv,\nand\nz, we\ndes-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "et\nal.\n[17]\nproposed\na multimodal\nattention-based BLSTM"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "s\nignate\nas\nthe\nhidden\nvariable\nassociated with\nemotions",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "network\nframework\nfor\ndetection. They\npropose Attention-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "v\nand contributing to utterance\ncomposition,\nas\nthe hidden",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "based Bidirectional Long Short-Term Memory Recurrent Neu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "variable linked to dialogue topics\nrelated to emotions, and z",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "ral Networks\n(LSTM-RNNs)\nto automatically learn the best"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "as\nthe hidden variable unrelated to emotions. As depicted in",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "temporal\nfeatures\nthat are useful\nfor detection. Huang et al."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the figure, both s and v have a causal\nrelationship with the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "[18]\npropose\na\nnovel Hierarchical LSTMs\nfor Contextual"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion E, while v possesses a causal\nrelationship with the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Emotion Detection (HRLCE) model."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dialogue\ntopic F . We\napply ChatGPT to complete\nthe\ntext",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "In recent studies, the incorporation of common sense knowl-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "feature\nextraction task,\ni.e.,\nextraction of\nthe dialogue\ntopic",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "edge has been increasingly prevalent. In tasks involving infer-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "F . Subsequently, we augment\nthe dataset with the extracted",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "ence,\nthe integration of common sense knowledge facilitates"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "topic text and topic feature vectors for future research. Notably,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "the\ndeduction\nof\nimplicit\ninformation\nfrom textual\ncontent."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "throughout each time stage, the evolution and generation of all",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Ghosal et al.\n[19] proposed COSMIC, a new framework that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hidden variables are influenced by the auxiliary variable P ,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "includes different commonsense elements\nto assist\nin detec-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which represents personal attribute information derived from",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "tion. Zhong\net\nal.\n[20]\nleveraged\ncommonsense\nknowledge"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "previous conversations using an LSTM network.\nIn addition,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "to\nenrich\nthe\ntransformer\nencoder. Li\net\nal.\n[21]\nproposed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "we propose\na\nsequential VAE framework [7]\nto reconstruct",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "a\nconversation modeling module\nto accumulate\ninformation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dialogue utterance U and dialogue topic F as part of our model",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "from the conversation and proposed a knowledge integration"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "optimization process.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "strategy\nto\nintegrate\nthe\nconversation-related\ncommonsense"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "knowledge generated from the event-based knowledge graph."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Contributions",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Bosselut\net\nal.\n[22]\nproposed\ncommonsense\ntransformers"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "This paper makes the following main contributions:",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "(COMET) which are used to learn to generate rich and diverse"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• We propose a Dynamic Causal Disentanglement Model,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "commonsense descriptions in natural\nlanguage."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "which aims to separate hidden variables unrelated to emo-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "Graph models\nare widely applied to process\nand analyze"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tions\nfrom dialogue utterances. Our model\nsignificantly",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "semantics\nand relations\nin the\ntexts. The\nlatest progress\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "improves the robustness of emotional\nfeatures.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "graph model\nresearch has also been introduced into the field"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• We apply ChatGPT to complete the feature extraction task",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "of\nemotion\ndetection. RGAT [23]\ncan\ncapture\nthe\nspeaker"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and add the topic text and feature vectors to the existing",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "dependency and the sequential\ninformation with the relational"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dataset.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "position\nencodings which\nprovide\nthe RGAT model with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• We\nevaluate\nthe\nperformance\nof\nour method\non\nthe",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "sequential information reflecting the relational graph structure."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEMOCAP and MELD datasets. The experimental results",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "IGCN [24] applied the incremental graph structure to imitate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "indicate\nthat\nour method\nhas\nsignificant\nimprovements",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "the process of dynamic conversation. Choi et al.\n[25] utilized"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "compared to the most advanced methods.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "the\nresidual\nnetwork\n(ResNet)-based,\nintrautterance\nfeature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In Section II, we present\nthe relevant work.\nIn Section III,",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "extractor and the GCN-based,\ninterutterance feature extractor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "we briefly introduce the background of\nthe causal model.\nIn",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "to fully exploit\nthe intra-inter\ninformative features."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Section IV, we\nintroduce\nthe\nspecific\ncontent of\nthe model",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\nthe\ncalculation\nformula\nof\nthe methodology\nin\ndetail.",
          "2": "Nevertheless,\nthese approaches\nfail\nto disentangle features"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In Section V, we\npresent\nkey\nexperiments.\nIn Section VI,",
          "2": "causally related to emotions from dialogue utterances, which"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "we present\nthe\nconclusions of our work and discuss\nfuture",
          "2": "results\nin spurious correlations and diminished robustness\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "research.",
          "2": "feature extraction."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "topics related to emotions, and variables unrelated to emotions, respectively. Personal attributes Pt influence the distribution of the hidden variables st, vt, and"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "for optimization purposes.\nzt, which collectively form the utterances Ut. Following disentanglement, we reconstruct both the utterances Ut and the topics Ft"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "At each time step, st and vt are employed to derive the emotion label Et."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "B. Causal Hidden Markov Model\nand E represent node set and edge set respectively. In the edge"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "set, each arrow x → y (x, y ∈ E) indicates that x has a direct"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "The Causal Hidden Markov Model\nis a common statistical"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "influence on y and changes the distribution of y. The structural"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "model, which is widely utilized in many time\nseries prob-"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "equation contains\nthe production relation of each variable in"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "lems, including image processing, natural language processing,"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "the point set. In the node-set, for V := {v1, . . . , vk}, we define"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "bioinformatics, and other fields. The Causal Hidden Markov"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "p(vi) to represent\nthe set of parent nodes of vi, and the set of"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "Model\nis an extension of\nthe Markov process\nthat\nintegrates"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "the parent nodes will affect it. Through the production relation,"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "hidden\nstates.\nIn\ncontrast\nto\nthe\nstandard Markov Model,"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "we define\nthe\ndenotes\nrelation as vi ← fi(p(vi)) where fi"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "the Causal Hidden Markov Model\nexcels\nin\ncapturing\nthe"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "causal mechanisms."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "causal relationship between observation sequences and hidden"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "state\nsequences\nby\nacquiring\nknowledge\nof\nthe\ntransition"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "probabilities and probability distributions of observations and"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "hidden states. This enhanced capability contributes to a more\nIV. METHODOLOGY"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "effective modeling\nof\ntime\nseries\ndata. Many methods\nare"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "proposed based on Causal Hidden Markov Models. Khiatani et"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "Inspired\nby\nthe Causal Hidden Markov Model\n[32], we"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "al. [26] constructed the Hidden Markov Model\nto learn the re-"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "propose the Dynamic Causal Disentanglement Model to model"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "lationship between weather states and hidden state sequences,"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "dialogues\nas\na novel\napproach for modeling dialogues. Our"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "achieving more\naccurate weather prediction. ZHANG et\nal."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "modifications and enhancements to the model are tailored to"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "[27] utilized the Factorial Hidden Markov Model Based on"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "facilitate the detection of emotional content within dialogue"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "the Gaussian Mixture Model\nfor non-invasive load detection."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "utterances.\nIn our Dynamic Causal Disentanglement Model,"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "Guo et\nal.\n[28]\nattempted to accurately predict\nthe\nchloride"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "we denote observed variable ft ∈ F , ut ∈ U , pt ∈ P as the"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "ion concentration that causes bridge degradation, and in the"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "dialogue\ntopics, dialogue utterances,\nand personal\nattributes"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "face of random detection time intervals, proposed a predictive"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "at\ntime stage t. The et ∈ E is the predicted emotion label at"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "Hidden semi-Markov Model\nto achieve prediction. Zhao et al."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "time stage t. We disentangle the utterance into hidden variables"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "[29] proposed a Causal Conditional Hidden Markov Model"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "the progress of dialogues\nclearly, we\nst, vt, zt. To observe"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "to predict multimodal\ntraffic flow, which respectively designs"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "design an LSTM network to learn the features of the speaker’s"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "a prior network and a posterior network to mine the causal"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "attributes at the current time stage. Due to the recent popularity"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "relation in the hidden variables inference stage. Mak et al. [30]"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "of ChatGPT, we apply ChatGPT-4.0 to extract dialogue topic"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "proposed a Causal Topology Design Hidden Markov Model"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "features."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "to solve viewpoint variation issue, which can help view inde-"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "In this section, we introduce the Dynamic Causal Disentan-"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "pendent multiple silhouette posture recognition. Suphalakshmi"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "glement Model\nin detail and elucidate the methods used for"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "et\nal.\n[31] utilized a Full Causal Two Dimensional Hidden"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "learning hidden variables within the model."
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "Markov Model with a novel 2D Viterbi algorithm for\nimage"
        },
        {
          "Fig. 2.\nDirected acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue": "segmentation and classification."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "of variables\nto achieve disentanglement.\nIn the prior network, we input\nthe concatenated vectors\ninto the GRU and subsequently design two independent"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "Fully Connected (FC)\nlayers to obtain the mean and the log variance vectors. Within the posterior network, we input concatenated vectors into the FC layer,"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "similar\nto the prior network,\nto obtain mean and log variance vectors. The hidden variable data obtained from the posterior network is then utilized for\nthe"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "reconstruction of observed variables and the prediction of\nlabels."
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "learned hidden vari-\nindependent exogenous variables. All variables\nm and εm is\np(Ut|ht), p(Ft|vt) are the processing of"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "ables in the generation network. After defining the joint prob-\nform a Hidden Markov model. We define the model as follows:"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "ability distribution, we propose a sequential VAE framework"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "zt ← fz(Pt, εt\nz), vt ← fv(Pt, εt\nv), st ← fs(Pt, εt\ns),"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "to learn our causal model, as shown in Fig.3."
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "Pt ← fP (εt\nP ), Ft ← fF (vt, εt\nF ), Et ← fE(st, vt, εt\nE),"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "1) Prior Network: For establishing prior distributions, we"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "Ut ← fU (st, vt, zt, εt\nU )."
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "introduce\na\nprior\nnetwork\ninto\nour model. This\nprior\nnet-"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "To\nenhance\ndetection\naccuracy,\nour\nobjective\nis\nto\ndis-"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "work\ntakes\nboth\nthe\nobserved\nvariable P\nand\nthe\nhidden"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "entangle\nemotion-related\ninformation. We\nintroduce\nhidden"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "variables\nfrom the preceding time step as\ninputs. We utilize"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "to construct\nthe observation variables\nvariables st, vt, and zt"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "Gated Recurrent Units\n(GRUs)\n[35]\nto\npropagate\nhidden"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "at\nrespective\ntime\nstages. Concurrently,\nthe\nUt, Ft,\nand Et"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "variables, enhancing the model’s capacity to capture long-term"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "hidden variables are influenced by the auxiliary variable Pt,"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "dependencies. After\nthe GRUs unit, we design independent"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "which undergoes continuous updates throughout\nthe ongoing"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "Fully Connected\nlayers\n(FCs)\nto\nobtain\nthe mean\nand\nthe"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "dialogue.\nrepresents\nthe\nhidden\nvariable\nassociated with\nst"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "log variance vectors of\nthe hidden variables. We utilize\nthe"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "emotions\nand contributing to utterance\nrep-\ncomposition, vt"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "hidden variable h ∈ {s, v, z} to illustrate the disentanglement"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "resents\nthe hidden variable linked to dialogue topics\nrelated"
        },
        {
          "Fig. 3. The network structure of Dynamic Causal Disentanglement Model. In causal\nlearning, we propose prior and posterior networks to learn the distribution": "propagation process of GRUs as follows:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "learn posterior distributions as follows:": "sq\n· [st−1, Ut, Ft, Pt] + bq\nt = W q\ns,"
        },
        {
          "learn posterior distributions as follows:": "vq\n(3)\n· [vt−1, Ut, Ft, Pt] + bq\nt = W q\nv,"
        },
        {
          "learn posterior distributions as follows:": "zq\n· [st−1, Ut, Pt] + bq\nt = W q\ns."
        },
        {
          "learn posterior distributions as follows:": "Similar\nto prior networks, we apply Fully Connected layers"
        },
        {
          "learn posterior distributions as follows:": "(FCs)\nto obtain the mean and the log variance vectors of\nthe"
        },
        {
          "learn posterior distributions as follows:": "hidden variables. These vectors will be utilized for reconstruc-"
        },
        {
          "learn posterior distributions as follows:": "tion in the following section."
        },
        {
          "learn posterior distributions as follows:": "3) Generation Network:\nIn\nthe\ngenerative\nnetwork, we"
        },
        {
          "learn posterior distributions as follows:": "employ the updated hidden variables\nto reconstruct\nthe ob-"
        },
        {
          "learn posterior distributions as follows:": "served variables and predict emotion labels. To guarantee the"
        },
        {
          "learn posterior distributions as follows:": "representation ability of learned variables, we utilize sq\nt , vq\nt , zq"
        },
        {
          "learn posterior distributions as follows:": "to obtain the reconstructed utterance\nU , and vq\nto obtain the\nt"
        },
        {
          "learn posterior distributions as follows:": "ˆ"
        },
        {
          "learn posterior distributions as follows:": "reconstructed conversation topic\nF . We optimize our model"
        },
        {
          "learn posterior distributions as follows:": "ˆ\nˆ"
        },
        {
          "learn posterior distributions as follows:": "by approximating the reconstructed variables\nU and\nF to the"
        },
        {
          "learn posterior distributions as follows:": "observed variables U and F ."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "it\nexcels\nin\nproviding more\nprecise\ntopic\ninformation\nby",
          "6": "for\nexplicit\nspeaker modeling. Similarly,\nit\ncan only model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "comprehending preceding dialogues. A visual\nrepresentation",
          "6": "conversations between two speakers."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\ntopic extraction is depicted in Fig.4.",
          "6": "ConGCN [41]\nconstructs\na\nheterogeneous\ngraph\nconvo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "lutional\nneural\nnetwork. The\nutterances\nand\nspeakers\nof\na"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "conversation\nare\nrepresented\nby\nnodes. The\ngraph\ncontains"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "V. EXPERIMENT",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "edges between utterance nodes and edges between utterance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "We apply our method to IEMOCAP and MELD datasets.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "and speaker nodes. This method models context and speaker-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Subsequently, we conducted ablation experiments and robust-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "sensitive dependencies for emotion detection."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ness\nanalyses.\nInitially, we present\nthe data\nconditions\nand",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "KET [20] is a knowledge-enriched transformer for emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "implementation details.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "detection. It dynamically leverages common sense knowledge"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "by using a context aware graph attention mechanism."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TABLE I",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "DialogueRNN [5] is a benchmark model for dialogue emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Statistics of splits in two datasets.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "tion detection. It is a recurrent network that utilizes three GRU"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Dialogue\nUtterance",
          "6": "networks to model\nthe speaker,\nthe context from the preceding"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Methods\nCategory",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "utterances, and the emotion of the preceding utterance to track"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Train & Val\nTest\nTrain & Val\nTest",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "the emotion of each speaker."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEMOCAP\n120\n31\n5810\n1623\n6",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "DialogueGCN [42]\nrepresents the structure and interaction"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MELD\n1152\n280\n11098\n2610\n7",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "of a conversation by constructing a graph structure between"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "participants and utilizes graph convolutional neural networks"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "for message passing and feature learning of\nthe graph struc-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A. Datasets",
          "6": "ture."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "KES\n[43]\nutilizes\na\nself-attention\nlayer\nspecialized\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEMOCAP [36]\nis a multimodal conversation dataset col-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "enhanced semantic text\nfeatures with external commonsense"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lected by the SAIL laboratory at\nthe University of Southern",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "knowledge\nand two networks based on LSTM for\ntracking"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "California. This\ndataset\ncomprises\n151\ndialogues,\ntotaling",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "individual\ninternal\nstate\nand\ncontext\nexternal\nstate\nto\nlearn"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "7433\nsentences. The\ndataset\nis\nannotated with\nsix\nemotion",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "interactions between interlocutors participating in a\nconver-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "categories: neutral, happy, sad, angry,\nfrustrated, and excited.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "sation."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Non-neutral emotions constitute a majority at 77%. IEMOCAP",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "TL-ERC [44] utilizes a generative conversational model\nto"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "stands\nas\nthe most widely\nutilized\ndataset\nin\nthe\nfield\nof",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "transfer emotional knowledge. It\ntransfers the parameters of a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dialogue emotion detection, characterized by its high-quality",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "trained hierarchical model\nto an emotion detection classifier."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "annotations. However,\nit\nis worth noting that\nthe dataset has a",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "SS-HiT [45] is a novel semantic and sentiment hierarchical"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relatively limited data size.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "transformer\nfor\nemotion\ndetection. Each\nutterance\ntoken\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MELD [37]\nis\nan extension of\nthe EmotionLines dataset",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "represented as matrices with both semantic and sentiment word"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[38], augmented with additional audio and video information.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "embeddings. Then,\nfuse utterance tokens as token features to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The MELD dataset\ncomprises\n1433 multi-party\ndialogues",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "further capture the long dependent utterance-level\ninformation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\n13708\nutterances\nextracted\nfrom the\ntelevision\nseries",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "through transformer encoders for prediction."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Friends”. This\ndataset\nprovides\nannotations\nfor utterances,",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "Sentic GAT [46]\nis a context- and sentiment-aware frame-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "categorizing them into seven distinct emotion categories: neu-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "work, which\ndesigns\na\ndialogue\ntransformer\n(DT)\nnetwork"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tral, joy, surprise, sadness, anger, disgust, and fear. Non-neutral",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "with hierarchical multihead attention to capture the intra- and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotions constitute a majority at 53%. MELD is distinguished",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "inter-dependency relationship in context. In addition, common-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by its high-quality annotations and the incorporation of mul-",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "sense knowledge is dynamically represented by the context-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "timodal data. Statistics of splits in two datasets are shown in",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "and\nsentiment-aware\ngraph\nattention mechanism based\non"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Table.I.",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "sentimental consistency."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "EmpaGen [47] proposes an auxiliary empathic multiturn di-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B. Baseline and State-of-the-Arts",
          "6": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "6": "alogue generation to enhance dialogue emotion understanding,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "BC-LSTM [16] proposes a bidirectional LSTM network to",
          "6": "which is\nthe first attempt\nto utilize empathy-based dialogue"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "capture contextual content, generate context aware discourse",
          "6": "generation for\nthe emotion detection task."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representations, and utilize them for\nsentiment classification.",
          "6": "GGCN [48]\nis a growing graph convolution network that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "However,\nthis model only considers context dependencies and",
          "6": "proposes a prior knowledge extraction framework to get auxil-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ignores speaker dependencies.",
          "6": "iary information. The method considers utterance connections"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "CMN [39] utilizes the dependency relationships of speakers",
          "6": "and emotion accumulation for emotional detection."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to identify emotions.\nIt designs different GRU networks\nfor",
          "6": "IEIN [49]\nis an iterative emotional\ninteraction model\nthat"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "two speakers to model utterance context from dialogue history",
          "6": "utilizes\niteratively predicted emotional\nlabels\ninstead of\nreal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and inputs current utterances into different memory networks",
          "6": "emotional\nlabels to continuously correct predictions and pro-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "to\nlearn\nutterance\nrepresentation. However,\nthis model\nis",
          "6": "vide feedback on inputs during the iteration process."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "limited to conversations with only two participants.",
          "6": "SDTN [50] dynamically tracks the local and global speaker"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ICON [40]\nis\nan extension of CMN that utilizes\nanother",
          "6": "states as the conversation progresses to capture implicit stim-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "GRU to connect\nthe GRU network outputs of\ntwo speakers",
          "6": "ulation\nof\nemotional\nshift.\nIt mainly\ncontains\nthe\nspeaker"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Neutral"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "F1"
        },
        {
          "TABLE II": "54.0"
        },
        {
          "TABLE II": "52.4"
        },
        {
          "TABLE II": "57.4"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "59.2"
        },
        {
          "TABLE II": "63.5"
        },
        {
          "TABLE II": "64.3"
        },
        {
          "TABLE II": "61.3"
        },
        {
          "TABLE II": "63.8"
        },
        {
          "TABLE II": "-"
        },
        {
          "TABLE II": "65.1"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "54.4\n63.5\nOur\n53.5\n78.8\n78.2",
          "-": "65.1",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "75.7\n69.3\n68.8\n68.9\n66.0\n64.4\n76.3\n69.8"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "interaction\ntracker\nbased\non GRU and\nthe\nemotion\nstate",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "during training. Throughout\nthe\ntraining process, we utilize"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "decoder module based on the conditional\nrandom field.",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Adam as\nthe\noptimizer with\nthe weight\nattenuation\nset\nto"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "0.00005 and the learning rate set to 0.001. The model is trained"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "for 80 epochs. We develop the evaluations with Python 3 and"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "C.\nImplementation Details",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": ""
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "PyTorch 1.13.0 on a\nsingle Nvidia RTX 3060 GPU and an"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "In this\nsection, we will\nfurnish a comprehensive overview",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": ""
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Intel\ni7 CPU."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "of\nthe\nimplementation\nof\nour\nproposed method. We\npre-",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": ""
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "extract\ntopic features and personal attribute features from the",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": ""
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "D. Comparison with State-of-the-Arts"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "prior dialogue content. For the extraction of personal attribute",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": ""
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "features, we design an LSTM network and specify the feature",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "We compare the performance of our proposed model with"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "vector dimension as 64.",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "the current state-of-the-art methods. The experimental\nresults"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "We\nutilize ChatGPT-4.0 API with Python\n3\non\na\nsingle",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "show that our model performs better than other state-of-the-art"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "Nvidia RTX 3060 GPU and an Intel\ni7 CPU. We configure",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "models on two benchmark datasets."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "ChatGPT-4.0\nas\na\ntext work\nassistant\nfor\nthe\ntask\nof\ntext",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "IEMOCAP:\nIEMOCAP is\na\ndyadic\ninteractive\ndialogues"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "extraction.\nFollowing\nthe\nrole\nassignment, we\nprovide\nthe",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "dataset consisting of\nlong conversations and it contains many"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "prompt: “I will give you a set of daily dialogue, understand",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "non-neutral emotions. We evaluate our model on the IEMO-"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "the meaning of the entire dialogue, and extract\nthe topic from",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "CAP dataset, and the experiment results are shown in Table.II."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "each utterance in conjunction with the previous context. Use",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Our model achieves a new state-of-the-art F1-score of 68.9%"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "concise\nvocabulary\nto\naccurately\nexpress\nthe\ntopic.” Addi-",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "and accuracy of 68.8% for\nthe emotion detection task on the"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "tionally, we specify detailed format\nrequirements to facilitate",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "IEMOCAP dataset. We compare the results of our proposed"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "future\ntext\ncollection and dataset\nconstruction. To ensure\na",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "model with the state-of-the-art methods. Although our model"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "comprehensive understanding of dialogue content and context,",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "still\nneeds\nto\nimprove\nthe\nperformance\nin\nsome\nemotional"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "we\ninput\nentire\ndialogues\ninto\nthe model,\ninstructing\nit\nto",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "categories,\nit\nis worth noting that our model has made signifi-"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "extract\ntopic information for each utterance. However,\nin the",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "cant\nimprovements\nin both Happy and Frustrated emotions."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "case of lengthy dialogues with numerous utterances, the model",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Compared with\nthe\nIEIN method,\nour model\ndemonstrates"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "occasionally miscounted them,\nleading to a mismatch between",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "a\ncritical\nimprovement of 1.2% in the\naverage F1-score of"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "the number of generated topics and utterances. To address this,",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Happy emotion. Compared with the state-of-the-art methods,"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "we segment\nlong dialogues\ninto batches, each containing 20",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "our model\nachieves\nimprovements\nin the\naccuracy of 0.7%"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "utterances. This approach allows ChatGPT-4.0 to better grasp",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "and the\naverage F1-score of 0.8% for\nthe Neutral\nemotion."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "the context and accurately determine the number of utterances.",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Compared with\nthe GGCN method,\nour model\nachieves\na"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "We apply the bert-base-uncased model for processing English",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "significant\nimprovement of 0.5% and 1.9% in the average F1-"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "text\nto\nconvert\ntopic\ntext\ninformation\ninto\n768-dimension",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "score of\nthe Excited and Frustrated emotions.\nIn general, our"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "feature vectors\nfor\nsubsequent experiments, which is one of",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "approach outperforms the state-of-the-art methods."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "the pre-trained BERT models [51]. Then we utilize two linear",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "MELD: The MELD dataset comprises multiparty conversa-"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "layers\nto reduce the dimensionality of\nthe feature vectors\nto",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "tions extracted from television series. Notably, many MELD"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "64 dimensions.",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "dialogues\ninvolve more\nthan five participants, with a maxi-"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "Then, we\nconstruct\nthe Dynamic Causal Disentanglement",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "mum of nine,\nresulting in limited utterances per participant"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "Model by combining all observed variables. The parameter",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "and posing challenges\nfor\ncontextual dependency modeling."
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "matrices of\nthe observation variable extraction model and the",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "Furthermore, the dataset exhibits a substantial presence of non-"
        },
        {
          "SS-HiT [45]\n-\n-\n-\n-\n-": "model\nare\nrandomly\ninitialized\nand\ncontinuously\noptimized",
          "-": "",
          "-\n-\n-\n-\n-\n-\n-\n61.0": "neutral emotions. In contrast to the IEMOCAP dataset, MELD"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Joy"
        },
        {
          "TABLE III": "52.9"
        },
        {
          "TABLE III": "53.1"
        },
        {
          "TABLE III": "-"
        },
        {
          "TABLE III": "54.6"
        },
        {
          "TABLE III": "53.6"
        },
        {
          "TABLE III": "-"
        },
        {
          "TABLE III": "60.4"
        },
        {
          "TABLE III": "-"
        },
        {
          "TABLE III": "-"
        },
        {
          "TABLE III": "61.8"
        },
        {
          "TABLE III": "56.6"
        },
        {
          "TABLE III": "63.1"
        },
        {
          "TABLE III": "-"
        },
        {
          "TABLE III": "65.0"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": "results of ablation study."
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Disentanglement"
        },
        {
          "TABLE IV": "(cid:37)"
        },
        {
          "TABLE IV": "(cid:33)"
        },
        {
          "TABLE IV": "(cid:33)"
        },
        {
          "TABLE IV": "(cid:33)"
        },
        {
          "TABLE IV": "(cid:33)"
        },
        {
          "TABLE IV": "(cid:33)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": "ment\naids\nin emotion detection. Our"
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": "We\nobserve\nthat when"
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": "features and personal attribute features separately,"
        },
        {
          "DialogueRNN, and our model\nrespectively.": "incorporating personal\nattribute"
        },
        {
          "DialogueRNN, and our model\nrespectively.": ""
        },
        {
          "DialogueRNN, and our model\nrespectively.": "scores\nfor both datasets,\nrespectively. Our"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "Fig. 6.\nThe feature mappings of high-dimensional vectors obtained by t-SNE. Dots of various colors represent utterances with distinct emotion labels."
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "TABLE V\nthat personal attribute features, serving as auxiliary variables in"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "Prediction results of different hidden variables."
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "the propagation of hidden variables, elucidate the distribution"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "of\nhidden\nvariables,\nthereby\nexerting\na more\npronounced"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "Variables\nIEMOCAP\nMELD"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "influence on result enhancement. We find that\nin comparison"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "s\nv\nz\nAcc.\nF1\nF1"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "to the model using theme\nfeatures\nextracted by LSTM,\nthe"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "(cid:37)\n(cid:37)\n(cid:33)"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "59.8\n60.2\n59.8\nmodels incorporating theme features extracted by ChatGPT-4.0"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "(cid:37)\n(cid:33)\n(cid:37)\nand BERT exhibited improvements of 0.9% and 0.8% in F1-\n66.1\n66.4\n66.4"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "(cid:33)\n(cid:37)\n(cid:37)\nscore on both datasets, respectively, demonstrating the superior\n66.4\n66.5\n66.8"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "(cid:37)\n(cid:33)\n(cid:33)\nability of ChatGPT-4.0 in text processing.\n62.9\n63.2\n63.4"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "The model\ncombining two observation variables\nand dis-\n(cid:37)\n(cid:33)\n(cid:33)\n63.8\n63.8\n63.7"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "entanglement\nachieves\nthe\nbest\nresults, which\nverifies\nthe\n(cid:33)\n(cid:37)\n(cid:33)"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "68.8\n68.9\n67.5"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "effectiveness of our proposed method and all observed vari-"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "ables. Overall,\nthe introduction of observed variables and the"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "disentanglement of hidden variables collectively enhance the"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "G. Effectiveness of Disentanglement"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "accuracy of emotion detection."
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "To demonstrate the reliability of\nthe related variables after"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "disentanglement, we\nimplement\na\nfurther validation method"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "F\n. Discussion on Prediction Accuracy\nof Long Dialogue"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "for\ns,\nv,\nand\nz.\nSpecifically, we\nfirst\ntrain\nthe Dynamic"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "Utterances"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "Causal Disentanglement Model and the observation variable"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "extraction model\nto obtain the distribution of hidden variables.\nTo further assess\nthe stability of our proposed model, we"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "Then, we independently train six independent classifiers\nfor\ncalculate the accuracy of emotion detection within each period,"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "prediction by different combinations of hidden variables, and\nas illustrated in Fig.5. Specifically, for the IEMOCAP dataset,"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "the experiment results are shown in Table.V. We find that\nthe\nwe divide every five utterances into a time batch, and due to"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "results of\nthe prediction containing z\nare\nlower\nthan those\nthe extended length of dialogues in this dataset, we analyze the"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "without z, which proves the existence of spurious correlation\nfirst forty utterances. In the case of MELD, a dataset featuring"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "information in the utterances. The experiments conducted with\nshorter dialogues, we segment each utterance into a time batch"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "s and v both yield favorable results. The accuracy and F1 score\nand analyze the first eight utterances."
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "obtained from variable s surpass those extracted from variable\nWe observe that\nthe emotion detection methods BC-LSTM"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "v\ns\non\ntwo\ndatasets,\nsuggesting\nthat\nharbors more\nsalient\nand DialogueRNN exhibit lower accuracy in the early stages of"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "feature information about emotional factors. The experimental\na dialogue, with accuracy gradually improving as the dialogue"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "results obtained by combining s and v are the best, affirming\nprogresses. The results suggest\nthat\nthese contextual modeling"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "the validity and effectiveness of our approach.\nmethods require a greater number of utterances to comprehend"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "ongoing dialogues due\nto the\nchanging dialogue\ntopics\nand\nTo intuitively observe the influence of hidden variables on"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "contexts.\nIn\ncontrast,\nour method\neffectively mitigates\nthis\nemotional\nfeatures, we visualize emotional\nfeatures acquired"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "challenge. By disentangling hidden variables related to emo-\nunder varying conditions on the IEMOCAP dataset to examine"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "tions at\nthe outset of\nthe dialogue, our model achieves high\nemotion distribution within the feature space. Employing the"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "accuracy in the early time batch. Furthermore, our prediction\nt-SNE method [52], we project high-dimensional\ntext features"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "accuracy remains\nstable and consistently high,\nfacilitated by\ninto\na\ntwo-dimensional\nspace,\nutilizing\ndistinct\ncolors\nfor"
        },
        {
          "(a) Pre-processing data distribution\n(b) Data distribution learned by s, v, and z\n(c) Data distribution learned by s and v": "the incorporation of dialogue topics and personal attributes.\nemotion labels, as depicted in Fig.6.\nIn the two-dimensional"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "H. Visualization of Confusion Matrix and Analysis\nspace after dimensionality reduction,\nthe inter-point distances"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "serve to reflect high-dimensional data similarities. Observing"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "We\nshow the visualization to observe\nthe\ncomparison of"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "the distribution of\nthe\ntext\nfeature vectors obtained by pre-"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "emotion classification results between different methods. The"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "processing, we find most utterances with different emotions"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "confusion matrixes of different methods on the IEMOCAP and"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "are bounded by each other, but a small number of utterances"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "MELD datasets are shown in Fig.7."
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "still exhibit a messy distribution. Simultaneously, we observe"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "In the case of\nthe IEMOCAP dataset, experimental\nresults\nthat utterances with non-neutral emotions and utterances with"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "reveal a notably high misprediction rate between the Neutral\nneutral emotions are close in two-dimensional space, implying"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "and Frustrated emotions. Our analysis indicates that,\nin some\ntheir higher similarity in high-dimensional\nfeature vectors.\nIn"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "instances, humans may not express their frustration directly in\ncontrast, the performance of the text feature vectors learned by"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "their utterances, making it significantly different from sadness.\nthe hidden variable s, v, and z demonstrates\nsome enhance-"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "This observation underscores that a speaker’s emotion cannot\nment. Utterances with neutral emotions tend to stay away from"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "be fully comprehended based solely on dialogue utterances.\nutterances with\nother\nemotions. However,\nthe\nintroduction"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "Our method makes up for\nthe\nshortcomings of BC-LSTM,\nz\nof\nunrelated\nvariable\nconstrains more\nprecise\nboundary"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "DialogueRNN, and other methods and we achieve a significant\ndistinctions. Notably, we find\nthat\nthe\ntext\nfeature\nvectors"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "improvement of 0.8% and 1.9% in the\naverage F1-score of\nlearned by relevant variables s and v with the separation of"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "the Neutral and Frustrated emotion, which demonstrates our\nz\nthe unrelated variable\nachieve\nthe best performance. The"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "model has a more comprehensive understanding of dialogue\ndistinction between utterances with different emotions is more"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "utterance. Furthermore,\nour method\nenhances F1-score\nac-\naccurate, and the distance in the two-dimensional space is far,"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "curacy\nacross\nother\nemotional\ncategories. We\nattribute\nthis\nindicating the low similarity of high-dimensional features. The"
        },
        {
          "Fig. 7.\nConfusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.": "improvement\nto the\nseparation of\ninterfering words\nand the\nresult\nfurther validates our perspective."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "rather\nthan an expression of\nlove. The reply “oh” is a response of sadness rather\nthan a common surprise. Our method can comprehensively understand the"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "meaning of dialogue utterances."
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "to emotion detection,\nleading to many non-neutral emotions\noptimize our model by maximizing the ELBO. Experimental"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "being incorrectly classified as Neutral\nin the validation set.\nresults\nand\nanalysis\non\ntwo\npopular\ndatasets\ndemonstrate"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "Through a thorough examination of the confusion matrix, We\nthe\nhigh\naccuracy\nand\nrobustness\nof\nour model.\nIn\nfuture"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "observe\nthat our model\nsubstantially enhances\nthe\naccuracy\nresearch, we intend to incorporate more precise representations"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "of Fear and Disgust emotions detecting,\nin contrast\nto other\nof dialogue as observational variables\nto further enhance the"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "models\nincapable of detecting these\nspecific\nemotions. Our\nanalysis of dialogues, with the expectation of achieving even"
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "model achieves remarkable enhancements in accuracy and F1-\ngreater performance."
        },
        {
          "Fig. 8.\nThe importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase “crush on you” is a mention of\nthe past": "scores across multiple emotional categories."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model achieves remarkable enhancements in accuracy and F1-": "scores across multiple emotional categories.",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "We choose a dialogue from the MELD dataset\nto validate",
          "greater performance.": "REFERENCES"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "our viewpoint, as shown in Fig.8. The phrase “crush on you”",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "[1]\nS. K. Khare and V. Bajaj, “Time–frequency representation and convolu-"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "mentioned in the first sentence does not convey an expression",
          "greater performance.": "tional neural network-based emotion recognition,” IEEE transactions on"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "neural networks and learning systems, vol. 32, no. 7, pp. 2901–2909,"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "of\nlove but\nrather\nrefers\nto a concerning reference to events",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "2020."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "from the past, which may mislead our prediction. Within the",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "[2] K. Wang, G. Su, L. Liu,\nand S. Wang,\n“Wavelet packet\nanalysis\nfor"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "dialogue, when Ross\nresponds with “oh” upon being char-",
          "greater performance.": "speaker-independent\nemotion recognition,” Neurocomputing, vol. 398,"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "pp. 257–264, 2020."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "acterized as geeky,\nthis\nresponse initially appears\nsurprising.",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "[3] W.\nJiao, M. Lyu,\nand\nI. King,\n“Real-time\nemotion\nrecognition\nvia"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "However, considering the context from the preceding text, we",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "the\nattention gated hierarchical memory network,”\nin Proceedings of"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "discern that Ross\nfailed to make\na\nfavorable\nimpression on",
          "greater performance.": "AAAI\nconference on artificial\nintelligence, vol. 34, no. 05, 2020, pp."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "8002–8009."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "Rachel, with whom he had a romantic interest. This under-",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "[4]\nZ. Huang, W. Xu,\nand K. Yu,\n“Bidirectional\nlstm-crf models\nfor"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "standing, combined with the extracted topic,\nindicates that his",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "sequence tagging. arxiv 2015,” arXiv preprint arXiv:1508.01991, 2015."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "response carries an underlying sense of disappointment.",
          "greater performance.": "[5] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh,\nand"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "E. Cambria,\n“Dialoguernn: An attentive\nrnn for\nemotion detection in"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "of\nthe AAAI\nconference\non\nartificial\nconversations,”\nin Proceedings"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "VI. CONCLUSION",
          "greater performance.": "intelligence, vol. 33, no. 01, 2019, pp. 6818–6825."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "[6] A. Metallinou, M. Wollmer, A. Katsamanis, F. Eyben, B. Schuller,"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "We propose a Dynamic Causal Disentanglement model\nfor",
          "greater performance.": "and S. Narayanan, “Context-sensitive learning for enhanced audiovisual"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "IEEE Transactions\nemotion\nclassification,”\non Affective Computing,"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "emotion detection in dialogues.\nIn our model, we introduce",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "vol. 3, no. 2, pp. 184–198, 2012."
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "hidden\nvariables\nto\ndisentangle\nhidden\nvariables\nand\nlearn",
          "greater performance.": ""
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "",
          "greater performance.": "[7] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv"
        },
        {
          "model achieves remarkable enhancements in accuracy and F1-": "the causal relationships between utterances and emotions. We",
          "greater performance.": "preprint arXiv:1312.6114, 2013."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[8]\nJ. Zhu, F. Deng,\nJ. Zhao, D. Liu, and J. Chen, “Uaed: Unsupervised",
          "12": "[29] Y. Zhao, P. Deng,\nJ. Liu, X.\nJia,\nand M. Wang,\n“Causal\nconditional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "abnormal emotion detection network based on wearable mobile device,”",
          "12": "hidden markov model\nfor multimodal\ntraffic prediction,” arXiv preprint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on Network Science and Engineering, 2023.",
          "12": "arXiv:2301.08249, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[9] W.\nLi,\nJ. Xue, R.\nTan, C. Wang,\nZ. Deng,\nS.\nLi, G. Guo,\nand",
          "12": "[30] C. M. Mak, Y. Lee, and Y. H. Tay, “Causal hidden markov model\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D. Cao,\n“Global-local-feature-fused\ndriver\nspeech\nemotion\ndetection",
          "12": "view independent multiple silhouettes posture recognition,” in 2011 11th"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions\non\nfor\nintelligent\ncockpit\nin\nautomated\ndriving,”",
          "12": "International Conference on Hybrid Intelligent Systems (HIS).\nIEEE,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Intelligent Vehicles, 2023.",
          "12": "2011, pp. 78–84."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[10] H. Kim, J. Ben-Othman, L. Mokdad, and P. Bellavista, “A virtual emo-",
          "12": "[31] A. Suphalakshmi, S. Narendran, and P. Anandhakumar, “A full causal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tion detection architecture with two-way enabled delay bound toward",
          "12": "two dimensional hidden markov model for image segmentation,” in 2009"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "evolutional emotion-based iot\nservices,” IEEE Transactions on Mobile",
          "12": "IEEE International Advance Computing Conference.\nIEEE, 2009, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Computing, vol. 21, no. 4, pp. 1172–1181, 2020.",
          "12": "442–445."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[11] Y. Ding, N. Robinson, Q. Zeng, D. Chen, A. A. P. Wai, T.-S. Lee, and",
          "12": "[32]\nJ. Li, B. Wu, X. Sun,\nand Y. Wang,\n“Causal hidden markov model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "C. Guan, “Tsception: a deep learning framework for emotion detection",
          "12": "the IEEE/CVF\nfor\ntime series disease forecasting,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "joint conference on neural networks\nusing eeg,” in 2020 international",
          "12": "Conference\non Computer Vision\nand Pattern Recognition,\n2021,\npp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(IJCNN).\nIEEE, 2020, pp. 1–7.",
          "12": "12 105–12 114."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[12] D. Datcu and L.\nJ. Rothkrantz, “Semantic audiovisual data fusion for",
          "12": "[33]\nJ. Pearl, Causality.\nCambridge university press, 2009."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "automatic emotion recognition,” Emotion recognition: a pattern analysis",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[34] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe, “Variational\ninference:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "approach, pp. 411–435, 2015.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "the American statistical Associa-\nA review for statisticians,” Journal of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[13] V. Singh, M. Sharma, A. Shirode, and S. Mirchandani, “Text emotion",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "tion, vol. 112, no. 518, pp. 859–877, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2023\n8th\nInterna-\ndetection\nusing machine\nlearning\nalgorithms,”\nin",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[35]\nJ. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tional Conference on Communication and Electronics Systems (ICCES).",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "gated recurrent neural networks on sequence modeling,” arXiv preprint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE, 2023, pp. 1264–1268.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "arXiv:1412.3555, 2014."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[14]\nS. Bhattacharya, S. Borah, B. K. Mishra, and N. Das, “Deep analysis for",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[36] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "speech emotion recognization,” in 2022 Second International Conference",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on Computer Science, Engineering and Applications (ICCSEA).\nIEEE,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "dyadic motion capture database,” Language resources and evaluation,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2022, pp. 1–6.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "vol. 42, pp. 335–359, 2008."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[15]\nS. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[37]\nS. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihal-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "computation, vol. 9, no. 8, pp. 1735–1780, 1997.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "cea, “Meld: A multimodal multi-party dataset\nfor emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[16]\nS. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh,\nand L.-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "in conversations,” arXiv preprint arXiv:1810.02508, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "P. Morency,\n“Context-dependent\nsentiment\nanalysis\nin user-generated",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "et\n[38]\nS.-Y.\nChen,\nC.-C. Hsu,\nC.-C. Kuo,\nL.-W. Ku\nal.,\n“Emotion-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the 55th annual meeting of\nthe association\nvideos,” in Proceedings of",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "lines: An emotion corpus of multi-party conversations,” arXiv preprint"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "for computational\nlinguistics (volume 1: Long papers), 2017, pp. 873–",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "arXiv:1802.08379, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "883.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[39] D. Hazarika, S. Poria, A. Zadeh, E. Cambria, L.-P. Morency, and R. Zim-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[17] C. Li, Z. Bao, L. Li, and Z. Zhao, “Exploring temporal\nrepresentations",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "mermann, “Conversational memory network for emotion recognition in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by leveraging attention-based bidirectional\nlstm-rnns\nfor multi-modal",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "the conference. Association\ndyadic dialogue videos,” in Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion recognition,” Information Processing & Management, vol. 57,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "for Computational Linguistics. North American Chapter. Meeting, vol."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 3, p. 102185, 2020.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "2018.\nNIH Public Access, 2018, p. 2122."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[18] C. Huang, A. Trabelsi, and O. R. Za¨ıane, “Ana at\nsemeval-2019 task",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[40] D. Hazarika, S. Poria, R. Mihalcea, E. Cambria, and R. Zimmermann,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3: Contextual emotion detection in conversations\nthrough hierarchical",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "“Icon: Interactive conversational memory network for multimodal emo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lstms and bert,” arXiv preprint arXiv:1904.00132, 2019.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "the 2018 conference on empirical\ntion detection,”\nin Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] D. Ghosal, N. Majumder, A. Gelbukh, R. Mihalcea,\nand\nS.\nPoria,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "methods in natural\nlanguage processing, 2018, pp. 2594–2604."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Cosmic: Commonsense knowledge for emotion identification in con-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[41] D. Zhang, L. Wu, C. Sun, S. Li, Q. Zhu,\nand G. Zhou,\n“Modeling"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "versations,” arXiv preprint arXiv:2010.02795, 2020.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "both context-and speaker-sensitive dependence for emotion detection in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20]\nP.\nZhong,\nD. Wang,\nand\nC. Miao,\n“Knowledge-enriched\ntrans-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "multi-speaker conversations.” in IJCAI, 2019, pp. 5415–5421."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "former\nfor emotion detection in textual conversations,” arXiv preprint",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[42] D. Ghosal, N. Majumder, S. Poria, N. Chhaya, and A. Gelbukh, “Dia-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "arXiv:1909.10681, 2019.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "loguegcn: A graph convolutional neural network for emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21] D. Li, X. Zhu, Y. Li, S. Wang, D. Li, J. Liao, and J. Zheng, “Enhancing",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "in conversation,” arXiv preprint arXiv:1908.11540, 2019."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion\ninference\nin\nconversations with\ncommonsense\nknowledge,”",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[43]\nF. Ren and T. She, “Utilizing external knowledge to enhance semantics"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Knowledge-Based Systems, vol. 232, p. 107449, 2021.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "in emotion detection in conversation,” IEEE Access, vol. 9, pp. 154 947–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz,\nand",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "154 956, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Y\n. Choi, “Comet: Commonsense transformers for automatic knowledge",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[44] D. Hazarika, S. Poria, R. Zimmermann, and R. Mihalcea, “Conversa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "graph construction,” arXiv preprint arXiv:1906.05317, 2019.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "tional\ntransfer\nlearning for\nemotion recognition,”\nInformation Fusion,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23]\nT.\nIshiwatari, Y. Yasuda, T. Miyazaki,\nand\nJ. Goto,\n“Relation-aware",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "vol. 65, pp. 1–12, 2021."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "graph attention networks with relational position encodings for emotion",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "[45] H. Zhang, Y. Chen, Y. Cao, Y. Zhao, and P. Jiang, “Sentiment and seman-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition in conversations,” in Proceedings of the 2020 Conference on",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "tic hierarchical\ntransformer\nfor utterance-level emotion recognition,” in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Natural Language Processing (EMNLP), 2020,",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "2022 International Joint Conference on Information and Communication"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 7360–7370.",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "Engineering (JCICE).\nIEEE, 2022, pp. 130–136."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] W. Nie, R. Chang, M. Ren, Y. Su,\nand A. Liu,\n“I-gcn:\nincremental",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "graph convolution network for conversation emotion detection,” IEEE",
          "12": "[46] G.\nTu,\nJ. Wen, C.\nLiu, D.\nJiang,\nand\nE. Cambria,\n“Context-and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Transactions on Multimedia, vol. 24, pp. 4471–4481, 2021.",
          "12": "sentiment-aware\nnetworks\nfor\nemotion\nrecognition\nin\nconversation,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "IEEE Transactions on Artificial Intelligence, vol. 3, no. 5, pp. 699–708,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25] Y.-J. Choi, Y.-W. Lee, and B.-G. Kim, “Residual-based graph convolu-",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "12": "2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tional network for emotion recognition in conversation for smart internet",
          "12": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nthings,” Big Data, vol. 9, no. 4, pp. 279–288, 2021.",
          "12": "[47]\nL. Wang, R. Li, Y. Wu, and Z. Jiang, “A multiturn complementary gener-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[26] D. Khiatani and U. Ghose, “Weather\nforecasting using hidden markov",
          "12": "ative framework for conversational emotion recognition,” International"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "model,” in 2017 International Conference on Computing and Commu-",
          "12": "Journal of\nIntelligent Systems, vol. 37, no. 9, pp. 5643–5671, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nication Technologies\nfor Smart Nation (IC3TSN).\nIEEE, 2017, pp.",
          "12": "[48] W. Nie, Y. Bao, Y. Zhao, and A. Liu, “Long dialogue emotion detection"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "220–225.",
          "12": "based on commonsense knowledge graph guidance,” IEEE Transactions"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[27]\nL.\nZHANG and\nJ.\nZhaoxia,\n“Non-intrusive\nload monitoring\nusing",
          "12": "on Multimedia, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "factorial hidden markov model based on gaussian mixture model,” in",
          "12": "[49] X. Lu, Y. Zhao, Y. Wu, Y. Tian, H. Chen, and B. Qin, “An iterative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2020 IEEE Power & Energy Society General Meeting (PESGM).\nIEEE,",
          "12": "emotion interaction network for emotion recognition in conversations,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2020, pp. 1–5.",
          "12": "the 28th international conference on computational\nin Proceedings of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[28] C. Guo, Z. Liang,\nJ. Zeng, M.\nSong,\nand Z. Xue,\n“A predictive",
          "12": "linguistics, 2020, pp. 4078–4088."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "hidden\nsemi-markov model\nfor\nbridges\nsubject\nto\nchloride-induced",
          "12": "[50]\nJ. Chen, P. Huang, G. Huang, Q. Li, and Y. Xu, “Sdtn: Speaker dynamics"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "deterioration,” in 2021 IEEE 21st International Conference on Software",
          "12": "tracking network for emotion recognition in conversation,” in ICASSP"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Quality, Reliability and Security Companion (QRS-C).\nIEEE, 2021, pp.",
          "12": "2023-2023 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "751–756.",
          "12": "Signal Processing (ICASSP).\nIEEE, 2023, pp. 1–5."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[51]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[52]"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "2",
      "title": "Wavelet packet analysis for speaker-independent emotion recognition",
      "authors": [
        "K Wang",
        "G Su",
        "L Liu",
        "S Wang"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "4",
      "title": "Bidirectional lstm-crf models for sequence tagging",
      "authors": [
        "Z Huang",
        "W Xu",
        "K Yu"
      ],
      "year": "2015",
      "venue": "Bidirectional lstm-crf models for sequence tagging",
      "arxiv": "arXiv:1508.01991"
    },
    {
      "citation_id": "5",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "6",
      "title": "Context-sensitive learning for enhanced audiovisual emotion classification",
      "authors": [
        "A Metallinou",
        "M Wollmer",
        "A Katsamanis",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "8",
      "title": "Uaed: Unsupervised abnormal emotion detection network based on wearable mobile device",
      "authors": [
        "J Zhu",
        "F Deng",
        "J Zhao",
        "D Liu",
        "J Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Network Science and Engineering"
    },
    {
      "citation_id": "9",
      "title": "Global-local-feature-fused driver speech emotion detection for intelligent cockpit in automated driving",
      "authors": [
        "W Li",
        "J Xue",
        "R Tan",
        "C Wang",
        "Z Deng",
        "S Li",
        "G Guo",
        "D Cao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Intelligent Vehicles"
    },
    {
      "citation_id": "10",
      "title": "A virtual emotion detection architecture with two-way enabled delay bound toward evolutional emotion-based iot services",
      "authors": [
        "H Kim",
        "J Ben-Othman",
        "L Mokdad",
        "P Bellavista"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Mobile Computing"
    },
    {
      "citation_id": "11",
      "title": "Tsception: a deep learning framework for emotion detection using eeg",
      "authors": [
        "Y Ding",
        "N Robinson",
        "Q Zeng",
        "D Chen",
        "A Wai",
        "T.-S Lee",
        "C Guan"
      ],
      "year": "2020",
      "venue": "2020 international joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "12",
      "title": "Semantic audiovisual data fusion for automatic emotion recognition",
      "authors": [
        "D Datcu",
        "L Rothkrantz"
      ],
      "year": "2015",
      "venue": "Semantic audiovisual data fusion for automatic emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "Text emotion detection using machine learning algorithms",
      "authors": [
        "V Singh",
        "M Sharma",
        "A Shirode",
        "S Mirchandani"
      ],
      "year": "2023",
      "venue": "2023 8th International Conference on Communication and Electronics Systems (ICCES)"
    },
    {
      "citation_id": "14",
      "title": "Deep analysis for speech emotion recognization",
      "authors": [
        "S Bhattacharya",
        "S Borah",
        "B Mishra",
        "N Das"
      ],
      "year": "2022",
      "venue": "2022 Second International Conference on Computer Science, Engineering and Applications (ICCSEA)"
    },
    {
      "citation_id": "15",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "16",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "17",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "18",
      "title": "Ana at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical lstms and bert",
      "authors": [
        "C Huang",
        "A Trabelsi",
        "O Zaïane"
      ],
      "year": "2019",
      "venue": "Ana at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical lstms and bert",
      "arxiv": "arXiv:1904.00132"
    },
    {
      "citation_id": "19",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "20",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    },
    {
      "citation_id": "21",
      "title": "Enhancing emotion inference in conversations with commonsense knowledge",
      "authors": [
        "D Li",
        "X Zhu",
        "Y Li",
        "S Wang",
        "D Li",
        "J Liao",
        "J Zheng"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "22",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin",
        "M Sap",
        "C Malaviya",
        "A Celikyilmaz",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "arxiv": "arXiv:1906.05317"
    },
    {
      "citation_id": "23",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "I-gcn: incremental graph convolution network for conversation emotion detection",
      "authors": [
        "W Nie",
        "R Chang",
        "M Ren",
        "Y Su",
        "A Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Residual-based graph convolutional network for emotion recognition in conversation for smart internet of things",
      "authors": [
        "Y.-J Choi",
        "Y.-W Lee",
        "B.-G Kim"
      ],
      "year": "2021",
      "venue": "Big Data"
    },
    {
      "citation_id": "26",
      "title": "Weather forecasting using hidden markov model",
      "authors": [
        "D Khiatani",
        "U Ghose"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Computing and Communication Technologies for Smart Nation (IC3TSN"
    },
    {
      "citation_id": "27",
      "title": "Non-intrusive load monitoring using factorial hidden markov model based on gaussian mixture model",
      "authors": [
        "L Zhang",
        "J Zhaoxia"
      ],
      "year": "2020",
      "venue": "2020 IEEE Power & Energy Society General Meeting (PESGM)"
    },
    {
      "citation_id": "28",
      "title": "A predictive hidden semi-markov model for bridges subject to chloride-induced deterioration",
      "authors": [
        "C Guo",
        "Z Liang",
        "J Zeng",
        "M Song",
        "Z Xue"
      ],
      "year": "2021",
      "venue": "2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)"
    },
    {
      "citation_id": "29",
      "title": "Causal conditional hidden markov model for multimodal traffic prediction",
      "authors": [
        "Y Zhao",
        "P Deng",
        "J Liu",
        "X Jia",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Causal conditional hidden markov model for multimodal traffic prediction",
      "arxiv": "arXiv:2301.08249"
    },
    {
      "citation_id": "30",
      "title": "Causal hidden markov model for view independent multiple silhouettes posture recognition",
      "authors": [
        "C Mak",
        "Y Lee",
        "Y Tay"
      ],
      "year": "2011",
      "venue": "2011 11th International Conference on Hybrid Intelligent Systems (HIS)"
    },
    {
      "citation_id": "31",
      "title": "A full causal two dimensional hidden markov model for image segmentation",
      "authors": [
        "A Suphalakshmi",
        "S Narendran",
        "P Anandhakumar"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Advance Computing Conference"
    },
    {
      "citation_id": "32",
      "title": "Causal hidden markov model for time series disease forecasting",
      "authors": [
        "J Li",
        "B Wu",
        "X Sun",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Causality",
      "authors": [
        "J Pearl"
      ],
      "year": "2009",
      "venue": "Causality"
    },
    {
      "citation_id": "34",
      "title": "Variational inference: A review for statisticians",
      "authors": [
        "D Blei",
        "A Kucukelbir",
        "J Mcauliffe"
      ],
      "year": "2017",
      "venue": "Journal of the American statistical Association"
    },
    {
      "citation_id": "35",
      "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "authors": [
        "J Chung",
        "C Gulcehre",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "36",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "38",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "S.-Y Chen",
        "C.-C Hsu",
        "C.-C Kuo",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "39",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "40",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "41",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "42",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "43",
      "title": "Utilizing external knowledge to enhance semantics in emotion detection in conversation",
      "authors": [
        "F Ren",
        "T She"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "44",
      "title": "Conversational transfer learning for emotion recognition",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Zimmermann",
        "R Mihalcea"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "45",
      "title": "Sentiment and semantic hierarchical transformer for utterance-level emotion recognition",
      "authors": [
        "H Zhang",
        "Y Chen",
        "Y Cao",
        "Y Zhao",
        "P Jiang"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Information and Communication Engineering (JCICE)"
    },
    {
      "citation_id": "46",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu",
        "D Jiang",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "A multiturn complementary generative framework for conversational emotion recognition",
      "authors": [
        "L Wang",
        "R Li",
        "Y Wu",
        "Z Jiang"
      ],
      "year": "2022",
      "venue": "International Journal of Intelligent Systems"
    },
    {
      "citation_id": "48",
      "title": "Long dialogue emotion detection based on commonsense knowledge graph guidance",
      "authors": [
        "W Nie",
        "Y Bao",
        "Y Zhao",
        "A Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "49",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "X Lu",
        "Y Zhao",
        "Y Wu",
        "Y Tian",
        "H Chen",
        "B Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "50",
      "title": "Sdtn: Speaker dynamics tracking network for emotion recognition in conversation",
      "authors": [
        "J Chen",
        "P Huang",
        "G Huang",
        "Q Li",
        "Y Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "52",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}