{
  "paper_id": "2508.20796v1",
  "title": "Speech Emotion Recognition Via Entropy-Aware Score Selection",
  "published": "2025-08-28T13:58:09Z",
  "authors": [
    "ChenYi Chua",
    "JunKai Wong",
    "Chengxin Chen",
    "Xiaoxiao Miao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a multimodal framework for speech emotion recognition that leverages entropy-aware score selection to combine speech and textual predictions. The proposed method integrates a primary pipeline that consists of an acoustic model based on wav2vec2.0 and a secondary pipeline that consists of a sentiment analysis model using RoBERTa-XLM, with transcriptions generated via Whisper-large-v3. We propose a late score fusion approach based on entropy and varentropy thresholds to overcome the confidence constraints of primary pipeline predictions. A sentiment mapping strategy translates three sentiment categories into four target emotion classes, enabling coherent integration of multimodal predictions. The results on the IEMOCAP and MSP-IMPROV datasets show that the proposed method offers a practical and reliable enhancement over traditional single-modality systems 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech Emotion Recognition (SER), which aims to recognise emotions directly from voice inputs as discrete emotion classes  [1] , has become a crucial area of study in humancomputer interaction, enhancing the emotional intelligence of virtual assistants, interactive robots, and mental health monitoring systems  [2] . The rapid development of deep SER models, such as Convolutional Neural Networks (CNNs)  [3] , Recurrent Neural Networks (RNNs)  [4] , and Transformerbased architectures  [5] ,  [6] ,  [7] , has substantially improved recognition accuracy by capturing complex temporal and contextual patterns in speech. Despite these advances, SER remains challenging due to the subtlety and complexity of emotional expression, limited data availability, and ambiguous labeling, which often lead to misclassification  [8] ,  [9] .\n\nTo address these issues, multimodal approaches that combine speech with textual or visual information have been explored to improve robustness  [10] ,  [11] . Among these, integrating speech with text is particularly practical, as textual data can be obtained through automatic speech recognition (ASR) even when authentic transcripts are unavailable  [12] . These transcripts are then processed using pretrained Transformerbased text models, such as BERT  [13]  or RoBERTa  [14] , to extract rich textual features, while speech models are employed to extract acoustic features.\n\nThe key challenge lies in effectively fusing these two modalities. Fusion approaches can be broadly categorized into three types: Early fusion merges raw or low-level features from each modality but often struggles with alignment and dimensionality mismatches  [10] ,  [15] . Intermediate fusion learns joint representations from both modalities, offering deeper integration but increasing training complexity  [11] ,  [16] . Late fusion, or decision-level fusion, enhances flexibility by allowing each modality to operate independently before merging its outputs, reducing the impact of modality-specific errors  [17] . Techniques such as score averaging or rule-based merging further leverage the complementary strengths of different models while supporting independent updates to each component  [18] .\n\nThis work focuses on multimodal emotion recognition and proposes an entropy-aware late score selection strategy. A speech utterance is processed through two branches and obtains two scores. The primary speech branch utilizes a selfsupervised learning model as a feature extractor, followed by a classifier that generates emotion predictions covering the full range of emotion classes, four classes in our experiments. The secondary textual branch processes the speech using an ASR model to obtain transcriptions, which are then fed into pretrained sentiment models applied off-the-shelf, without fine-tuning on emotion datasets to produce scores for three sentiment categories: Positive, Neutral, and Negative.\n\nThe proposed entropy-aware score selection strategy guides the fusion of speech and sentiment scores. The first step is to evaluate the confidence of the speech score to determine whether intervention from the secondary model is necessary. This decision is based on whether the entropy and varentropy values of the speech score exceed delicately determined thresholds. If the speech score exhibits low confidence and low stability, the system refers to the secondary model for assistance. To address the mismatch between the four emotion classes and three sentiment categories, we introduce a sentiment mapping strategy: Positive and Neutral map directly to Happy and Neutral, while Negative is classified as Angry or Sad based on the primary model's confidence.\n\nWe evaluate these strategies on the IEMOCAP  [1]  and MSP-IMPROV  [19]  datasets to assess their impact on emotion classification performance. By incorporating the secondary model as a fallback mechanism for low-confidence predictions, we observed consistent improvements across both datasets, demonstrating enhanced classification precision and robustness, particularly in emotionally ambiguous cases. These results suggest that our late fusion framework offers a practical and reliable enhancement over a single-modality framework.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Proposed Method",
      "text": "This section details the proposed multimodal speech recognition approach, illustrated in Figure  1 , which comprises two independent pipelines processing the same audio input 2  to obtain predictions from two models, followed by an entropyaware score selection strategy to boost performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Primary Speech Modality",
      "text": "The primary pipeline follows a conventional speech emotion recognition approach. A speech utterance is fed into a selfsupervised learning-based wav2vec2  [5]  as a feature extractor to obtain emotion-discriminative features, which are then passed through a classifier consisting of two linear projection layers to generate emotion prediction. Specifically, we finetune wav2vec2 on the emotion datasets, with its parameters updated during the fine-tuning stage. The primary score can be represented as: p s = {p s c | c ∈ {Ang, Sad, Hap, Neu}}, where p s c represents the predicted probability for class c among the four possible emotion classes in the primary branch. Since this branch is trained directly on speech data with labels, we consider it the primary/more reliable source for emotion prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Secondary Text Modality",
      "text": "The secondary pipeline consists of a Speech-To-Text (S2T) model that generates transcribed text, which is then cleaned and preprocessed through several steps, including expanding contractions, removing punctuation, lemmatizing, lowercasing, and removing numbers, before being passed to a text-based sentiment analysis model  [20] . This process does not involve any training on the specific speech datasets, using pretrained S2T and sentiment models applied off-the-shelf provides a second opinion on the expressed sentiment of the input speech.\n\n1) Whisper Series Speech-to-text Model: Whisper is a robust multilingual transformer-based ASR model developed by OpenAI  [21] . It is composed of an encoder-decoder architecture based on the Transformer framework. The encoder ingests log-Mel spectrogram features and consists of multiple stacked Transformer layers with multi-head self-attention, positional encodings, and layer normalization. The decoder generates the text output autoregressively, conditioned on both audio features and previously generated tokens.\n\n2) RoBERTa Series Sentiment Model: We employ the RoBERTa series model for sentiment analysis  [22] . The model architecture consists of Transformer blocks, each with multihead self-attention, feed-forward layers, residual connections, and layer normalization. The final hidden state is passed through a linear projection layer followed by softmax function to output probabilities across the three sentiment classes: Negative, Neutral, and Positive, denoted as p t = {p t i | i ∈ {Negative, Neutral, Positive}}, where p t i represents the predicted probability for class i among the three possible sentiment classes in the secondary branch. This model is directly adopted from an open-source checkpoint trained on large-scale sentiment data. It is used without any adaptation or finetuning to provide a secondary opinion in score fusion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Entropy-Aware Score Selection Strategy",
      "text": "After obtaining speech emotion p s and sentiment p t scores, an entropy-aware score selection strategy is introduced to determine when the primary model's predictions should be supplemented by those from the secondary model. The principle is that if the primary model's prediction has higher certainty, it is retained, otherwise, the system relies on the secondary model.\n\nWe utilize two metrics to quantify the certainty of the primary model's prediction. The first metric is entropy H , directly measures the degree of uncertainty:\n\nA lower H is preferred, indicating high prediction certainty.\n\nThe second metric is varentropy V, which quantifies the\n\nVarentropy provides an additional measure of confidence by assessing how sharply peaked or flat the probability distribution is, thereby quantifying the stability of entropy H. A higher varentropy V is preferred, indicating stable uncertainty estimates of the primary model, demonstrating robustness to minor input variations. If a primary score p s has high entropy H and low varentropy V, measured against two thresholds τ H and τ V , it is deemed unreliable. In such cases, we defer the final decision to the secondary sentiment model. This decision is implemented through the merge function described in Algorithm 1, which maps the sentiment prediction into one of the target emotion classes. If the thresholds are not met, the original prediction from the primary model is used.\n\nTo derive the optimal thresholds τ e and τ v , we perform a grid search over all possible pairs of entropy and varentropy values within empirically determined ranges on the training data. Note that the search for optimal thresholds is conducted separately for each emotion class c, resulting in four sets of thresholds {τ c e , τ   3  To do this, we group the training samples based on their emotion labels and determine the thresholds for each group independently. Taking one emotion class c as an example, suppose we have N training samples belonging to class c. Let P k (H) and P k (V) denote the k-th percentiles of entropy and varentropy values for the N samples, respectively. The search ranges for the thresholds are defined as:\n\nThese ranges are motivated by the observation that incorrect predictions tend to exhibit higher entropy and lower varentropy, with optimal threshold values typically located near the 75th percentile for entropy and the 25th percentile for varentropy. The search is conducted using a fixed step size of ∆ = ±10 percentile points. Each candidate threshold pair is given by:\n\nfor k = 0, 1, . . . , K and l = 0, 1, . . . , L, forming a grid of candidate thresholds. The optimal thresholds (τ c e , τ c v ) for each emotion class c are determined by maximizing a detection accuracy metric M, defined as:\n\nwhere the objective metric M is computed as:\n\nwhere D denoting the number of misclassified samples successfully identified by the current thresholding rule, and T representing the total number of samples that satisfy both the entropy and varentropy threshold conditions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Sentiment Mapping Strategy",
      "text": "This component addresses the challenge of mapping threeclass sentiment outputs (Positive, Neutral, Negative) into the required four emotional classes (Happy, Neutral, Sad, Angry). While positive and neutral sentiments map straightforwardly to Happy and Neutral, respectively, negative sentiment needs further discrimination between Sad and Angry emotions. We propose two methods for this mapping, both of which are evaluated and the one yielding the higher accuracy is selected automatically.\n\n1) Refer to Primary Model Mapping: In cases of negative sentiment, this method consults the primary model's confidence scores between Angry and Sad emotions, assigning the sentiment accordingly based on the higher confidence score from the primary model.\n\n2) Simple or Flip Mapping: We establish a threshold (ranging between 0 and 1) τ c m for each emotion class c where sentiment scores below the threshold map to Sad and those above to Angry. This mapping is flexible and can be reversed through a \"flip\" flag since both emotions can validly represent a negative sentiment. In a similar process to obtaining the optimal entropy and varentropy threshold values, a grid search on the training dataset is performed on a range of discreet values set at constant intervals using the same detection accuracy metric previously mentioned.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Revert Change Strategy",
      "text": "Once all optimal threshold pairs {τ c e , τ c v , τ c m } are determined, we adopt an additional strategy to enhance the prediction: we aim to construct an exclusion list E using the training set, where all detrimental emotion changes are stored. Since these changes cause performance drops on the training set, during the inference stage, if such a change occurs, it will be skipped, and the primary prediction will be retained.\n\nRegarding the creation of E, for each training sample, if, for example, a sample's primary prediction is Angry, and after applying the entropy and valentropy thresholds, the prediction changes to Sad (indicated as AngSad), while the accuracy calculated using the same performance metric defined in Equation 7 drops, this is considered a harmful change and will be added to E. Such changes should be avoided during the reference stage.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments",
      "text": "In this section, we first select the speech-to-text model and sentiment model from various publicly available options, and then verify the effectiveness of the proposed entropy-aware score selection method on two commonly-used speech emotion recognition datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets",
      "text": "Two widely recognized datasets are used to verify the effectiveness of the proposed method. The first is IEMOCAP  [1] , which comprises 5 sessions, each containing one male and one female actor performing scripted and improvised scenarios. We employ 10-fold cross-validation, where 4 sessions are used for training, and the utterances from the remaining session-containing two speakers-are used for validation and testing, respectively.\n\nThe second dataset, MSP-IMPROV  [19] , consists of 6 sessions, each with one male and one female actor. We adopt 6fold cross-validation, where each fold designates one complete session as the test set, while the remaining five sessions are split into training (80%) and validation (20%) subsets.\n\nTable  I  lists the number of utterances for each emotion in each session for both datasets.\n\n1) Settings: For each cross-validation fold, we dynamically determine the optimal entropy, varentropy, and sentiment thresholds based on the training data of that fold, and then apply them to the corresponding test set. This approach ensures adaptive yet consistent prediction merging across all folds for both the IEMOCAP and MSP-IMPROV datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Results On Various Text Modality Models",
      "text": "The left side of Table  II  compares three S2T ASR models: w2v-CTC  [5] , Whisper-tiny-en  [23] , and Whisper-large-v3  [23] . Each model was used to transcribe audio segments from the IEMOCAP dataset. Among them, Whisper-large-v3 achieved the lowest word error rate (WER) of 14.71% and was selected as the primary S2T model for the secondary branch.\n\nThe right side of Table  II  compares three sentiment analysis models used to classify authentic IEMOCAP transcripts into three sentiment categories: Positive, Neutral, and Negative, where the Negative class encompasses both angry and sad emotional categories. To evaluate sentiment prediction performance, we tested three advanced Transformer-based models: DistilBERT  [24] , RoBERTa  [25] , and RoBERTa-XLM  [26] . Cleaned transcripts were passed through each model to generate sentiment predictions. While RoBERTa achieved the highest overall F1 score (46.12%), RoBERTa-XLM showed the most consistent and balanced classification performance across all sentiment categories, particularly excelling in the Negative class with an F1 score of 50.74%. Given the importance of accurately detecting negative sentiment in emotion recognition, RoBERTa-XLM was selected as the sentiment analysis model for our secondary pipeline 10 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Results On The Entropy-Aware Score Selection",
      "text": "In this section, we first present the averaged results across different folds on the IEMOCAP dataset to compare the effectiveness of using entropy and varentropy thresholds for guiding dynamic switching between the primary and secondary 10 We also experimented with other sentiment models, such as Gemini-1.5-Flash and GPT-4o Mini, but did not observe improved performance. Since these models have not been adapted to IEMOCAP, MSP-IMPROV, or similar content-specific datasets, their performance remains suboptimal. We believe that a sentiment model better tuned to such datasets would yield improved results in the merged pipeline. pipeline models, versus not using any score selection. We then report the results for three metrics, Unweighted Accuracy (UA), Weighted Accuracy (WA), and F1 Score, for each fold on both IEMOCAP and MSP-IMPROV. 1) Comparison With and Without Score Merging Methods: Table  III  lists the results on IEMOCAP with and without score selection strategies. Apparently, applying any score selection strategy, whether based on entropy, varentropy, or both of them, leads to improvements across all evaluation metrics compared to the speech-modality-only method (w/o score selection), except when using entropy alone, where the WA result (63.85%) falls slightly short. Among the strategies, the combined Entropy + Varentropy approach achieves the highest overall performance, with UA (65.81%), WA (65.41%), and F1 score (64.55%), surpassing both individual-threshold methods and significantly outperforming the baseline. Therefore, we adopt the combined approach for its enhanced robustness and reliability for the following experiments.  2) IEMOCAP Dataset Results: Table  IV  shows fold-wise performance of the proposed score selection method on IEMO-CAP. Results are compared before and after applying the entropy + varentropy-based score selection. Seven out of ten folds (Folds 3-7, 9, and 10) exhibit consistent improvements across all three evaluation metrics, demonstrating that the merge strategy, through confidence-driven switching, leveraged textual sentiment cues to correct misclassifications. Even in folds like Fold 6 with strong baseline accuracy, the algorithm yielded marginal improvements or stable results, demonstrating non-destructive behavior.\n\n3) MSP-IMPROV Dataset Results: To further validate the generalizability of our approach, we applied the proposed score selection method to the MSP-IMPROV dataset. The speech scores were obtained from the wav2vec2 model fine-tuned on the MSP-IMPROV dataset, while the text scores were obtained using the same text model as used for IEMOCAP. Results from Table V similarly show improvements. Folds 2-4, and 6 in particular saw significant uplifts, suggesting that the entropy + varentropy-based merge is not overfit to IEMOCAP's structure and retains robustness under distributional shift. The merge mechanism was able to dynamically defer to the secondary sentiment pipeline when confidence was low, preserving precision without introducing instability.\n\nThe final assessment shows consistent performance gains using our entropy-aware score selection strategy across both datasets. Although some folds experienced slight performance drops, this may be due to low-confidence predictions or threshold misalignment in certain emotion classes. On average, improvements in F1 score range from 0.5% to 1.2%, indicating the effectiveness of the merging framework while highlighting room for further enhancement. Future work could explore more powerful text-based models to strengthen the utility of the secondary sentiment signal. Additionally, we observe that entropy and varentropy thresholds differ across emotion classes, suggesting that class-specific or dynamically adaptive thresholding could further refine fusion decisions.\n\n4) Further Analysis: As explained in Section II-C, lower entropy and higher varentropy are indicative of more accurate predictions, and the search space of these metrics is dependent on the emotion class. Therefore, it is insightful to visualize how their distributions vary across classes. Figure  2  plots the entropy (solid color-filled bars) and varentropy (hatched colorfilled bars) for each predicted class on Fold 3 of the IEMOCAP dataset. For each class, the values are shown separately for correctly predicted (green) and incorrectly predicted (red) samples. It is obvious that lower entropy and higher varentropy correlate with more accurate predictions. For entropy, the green bars (correct predictions) are consistently lower than the red bars (incorrect predictions) across all four classes, suggesting that lower entropy values are associated with greater confidence and correctness. In the case of varentropy, the green hatched bars (correct predictions) are generally higher or more centrally distributed than the red hatched bars (incorrect predictions), indicating that higher varentropy values likewise correspond to greater prediction confidence and accuracy.\n\nMoreover, each emotion class demonstrates a distinct range of entropy and varentropy values, reinforcing the need for a per-class thresholding strategy. A single global threshold would fail to capture these class-specific patterns effectively. Additionally, the merging process can occasionally turn a correct prediction into an incorrect one. Therefore, choosing an appropriate threshold becomes a trade-off: it must balance maximizing the detection of incorrect predictions while minimizing the erroneous rejection of correct ones. This trade-off motivates the use of accuracy as the evaluation metric during parameter selection.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we proposed a multimodal score selection methodology that combines a primary wav2vec2-based speech branch with a secondary Whisper + RoBERTa-XLM sentiment branch for SER. For score selection between the two branches, we leveraged entropy and varentropy thresholds to identify uncertain predictions and dynamically switched to the secondary pipeline for improved reliability. Experiments on both the IEMOCAP and MSP-IMPROV datasets demonstrated clear improvements in accuracy, robustness, and stability across emotional classes, particularly in challenging cases involving conflicting emotional cues. Overall, the proposed late fusion strategy offers a computationally efficient, flexible, and reliable pipeline, outperforming single-modality systems and fixed fusion strategies.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed multimodal emotion recognition framework.",
      "page": 2
    },
    {
      "caption": "Figure 1: , which comprises two",
      "page": 2
    },
    {
      "caption": "Figure 2: Distributions of entropy (solid color-filled bars) and varentropy",
      "page": 5
    },
    {
      "caption": "Figure 2: plots the",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "‡ Duke Kunshan University, China"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "extract rich textual features, while speech models are employed"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "to extract acoustic features."
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "The key challenge lies in effectively fusing these two modal-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "ities. Fusion approaches can be broadly categorized into three"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "types: Early fusion merges raw or low-level features from each"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "modality but often struggles with alignment and dimensionality"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "mismatches [10],\n[15].\nIntermediate fusion learns joint\nrepre-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "sentations\nfrom both modalities, offering deeper\nintegration"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "but\nincreasing training complexity [11],\n[16]. Late fusion, or"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "decision-level\nfusion,\nenhances flexibility\nby\nallowing\neach"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "modality to operate independently before merging its outputs,"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "reducing\nthe\nimpact\nof modality-specific\nerrors\n[17]. Tech-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "niques such as score averaging or\nrule-based merging further"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "leverage\nthe\ncomplementary\nstrengths\nof\ndifferent models"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "while supporting independent updates to each component [18]."
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "This work focuses on multimodal emotion recognition and"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "proposes\nan\nentropy-aware\nlate\nscore\nselection\nstrategy. A"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "speech utterance is processed through two branches and ob-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "tains\ntwo scores. The primary speech branch utilizes a self-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "supervised learning model as a feature extractor,\nfollowed by"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "a classifier that generates emotion predictions covering the full"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "range\nof\nemotion\nclasses,\nfour\nclasses\nin\nour\nexperiments."
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "The\nsecondary\ntextual\nbranch\nprocesses\nthe\nspeech\nusing"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "an ASR model\nto obtain transcriptions, which are\nthen fed"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "into pretrained sentiment models applied off-the-shelf, without"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "fine-tuning on emotion datasets\nto produce\nscores\nfor\nthree"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "sentiment categories: Positive, Neutral, and Negative."
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "The proposed entropy-aware score selection strategy guides"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "the\nfusion of\nspeech and sentiment\nscores. The first\nstep is"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "to evaluate\nthe\nconfidence of\nthe\nspeech score\nto determine"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "whether\nintervention from the secondary model\nis necessary."
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "This\ndecision\nis\nbased\non whether\nthe\nentropy\nand\nvaren-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "tropy values of the speech score exceed delicately determined"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "thresholds.\nIf\nthe\nspeech score\nexhibits\nlow confidence\nand"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "low stability,\nthe\nsystem refers\nto the\nsecondary model\nfor"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "assistance. To address the mismatch between the four emotion"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "classes and three sentiment categories, we introduce a senti-"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "ment mapping strategy: Positive and Neutral map directly to"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "Happy and Neutral, while Negative is classified as Angry or"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "Sad based on the primary model’s confidence."
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "We\nevaluate\nthese\nstrategies\non\nthe\nIEMOCAP\n[1]\nand"
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": ""
        },
        {
          "Institute Of Acoustics, Chinese Academy Of Sciences, China": "MSP-IMPROV [19] datasets to assess their impact on emotion"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "we consider\nit\nthe primary/more reliable source for emotion"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "prediction."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "B.\nSecondary Text Modality"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "The secondary pipeline consists of a Speech-To-Text\n(S2T)"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "model\nthat generates\ntranscribed text, which is\nthen cleaned"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "and preprocessed through several\nsteps,\nincluding expanding"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "contractions, removing punctuation, lemmatizing, lowercasing,"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "and removing numbers, before being passed to a\ntext-based"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "sentiment analysis model\n[20]. This process does not\ninvolve"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "any training on the specific speech datasets, using pretrained"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "S2T and\nsentiment models\napplied\noff-the-shelf\nprovides\na"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "second opinion on the expressed sentiment of the input speech."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "1) Whisper Series Speech-to-text Model: Whisper\nis a ro-"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "bust multilingual\ntransformer-based ASR model developed by"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "OpenAI\n[21].\nIt\nis composed of an encoder-decoder architec-"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "ture based on the Transformer framework. The encoder ingests"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "log-Mel spectrogram features and consists of multiple stacked"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "Transformer\nlayers with multi-head self-attention, positional"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "encodings, and layer normalization. The decoder generates the"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "text output autoregressively, conditioned on both audio features"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "and previously generated tokens."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "2) RoBERTa\nSeries\nSentiment Model: We\nemploy\nthe"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "RoBERTa series model for sentiment analysis [22]. The model"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "architecture consists of Transformer blocks, each with multi-"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "head self-attention,\nfeed-forward layers,\nresidual connections,"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "and\nlayer\nnormalization. The\nfinal\nhidden\nstate\nis\npassed"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "through a linear projection layer followed by softmax function"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "to\noutput\nprobabilities\nacross\nthe\nthree\nsentiment\nclasses:"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "= {pt\n|\nNegative, Neutral,\nand\nPositive,\ndenoted\nas pt"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "i"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "i ∈ {Negative, Neutral, Positive}}, where pt\nrepresents\nthe\ni"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "i\npredicted\nprobability\nfor\nclass\namong\nthe\nthree\npossible"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "sentiment\nclasses\nin\nthe\nsecondary\nbranch. This model\nis"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "directly adopted from an open-source\ncheckpoint\ntrained on"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "large-scale sentiment data.\nIt\nis used without any adaptation"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "or finetuning to provide a secondary opinion in score fusion."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "C. Entropy-Aware Score Selection Strategy"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "After obtaining speech emotion ps and sentiment pt scores,"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "an\nentropy-aware\nscore\nselection\nstrategy\nis\nintroduced\nto"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "determine when\nthe\nprimary model’s\npredictions\nshould\nbe"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "supplemented by those from the secondary model. The prin-"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "ciple\nis\nthat\nif\nthe\nprimary model’s\nprediction\nhas\nhigher"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "certainty,\nit\nis\nretained, otherwise,\nthe\nsystem relies on the"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "secondary model."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "We\nutilize\ntwo metrics\nto\nquantify\nthe\ncertainty\nof\nthe"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "primary model’s prediction. The first metric\nis\nentropy H ,"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "directly measures the degree of uncertainty:"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "N(cid:88) i\nH = H(ps) = −\nps\nlog(ps\n(1)\nc\nc)."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "=1"
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "A lower H is preferred,\nindicating high prediction certainty."
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": ""
        },
        {
          "this\nbranch\nis\ntrained\ndirectly\non\nspeech\ndata with\nlabels,": "The\nsecond metric\nis varentropy V, which quantifies\nthe"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "5:",
          "emotion ← Neu": "else if r.sentiment = \"positive\" then"
        },
        {
          "4:": "6:",
          "emotion ← Neu": "emotion ← Hap"
        },
        {
          "4:": "7:",
          "emotion ← Neu": "else"
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "8:",
          "emotion ← Neu": "▷ Refer\nif fm = \"refer\" then"
        },
        {
          "4:": "9:",
          "emotion ← Neu": "emotion ← Ang if r.ps"
        },
        {
          "4:": "",
          "emotion ← Neu": "Ang ≥ r.ps\nSad else Sad"
        },
        {
          "4:": "10:",
          "emotion ← Neu": "else if fm = \"simple\" then"
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "",
          "emotion ← Neu": "emotion ← Ang if (r.pt"
        },
        {
          "4:": "11:",
          "emotion ← Neu": "r.sentiment ≤ τm) ⊕ fi"
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "12:",
          "emotion ← Neu": "end if"
        },
        {
          "4:": "13:",
          "emotion ← Neu": "end if"
        },
        {
          "4:": "14:",
          "emotion ← Neu": "if (r.prediction, emotion) ∈ E then"
        },
        {
          "4:": "15:",
          "emotion ← Neu": "emotion ← r.prediction"
        },
        {
          "4:": "16:",
          "emotion ← Neu": "end if"
        },
        {
          "4:": "17:",
          "emotion ← Neu": "else"
        },
        {
          "4:": "18:",
          "emotion ← Neu": "emotion ← r.prediction"
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "19:",
          "emotion ← Neu": "end if"
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "20: Efinal ← emotion",
          "emotion ← Neu": ""
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "21:",
          "emotion ← Neu": "return Efinal"
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        },
        {
          "4:": "",
          "emotion ← Neu": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "e\nv\ne\nv"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "Require:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "this, we group the\ntraining samples based on their\nemotion"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "(i) A single sample r with:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "labels and determine the thresholds\nfor each group indepen-"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "– r.prediction: predicted emotion by the speech branch",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "– r.sentiment: predicted sentiment by the text branch",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "dently. Taking one emotion class c as an example, suppose we"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "– r.ps: predicted class probabilities from the speech branch,",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "have N training samples belonging to class c. Let Pk(H) and"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "Ang, ps\nHap, ps\nNeu}\nSad, ps",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "Pk(V) denote the k-th percentiles of entropy and varentropy"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "– r.pt: predicted class probabilities from the text branch,",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "values for\nthe N samples,\nrespectively. The search ranges for"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "Neg, pt\nPos, pt\nNeu}",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "– r.H(ps): entropy of\nthe speech prediction score",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "the thresholds are defined as:"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "– r.V(ps): varentropy of\nthe speech prediction score",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(3)\nτe ∈ [P75(H) − ∆,\nP75(H) + ∆] ,"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "(ii) {Entropy, Valentropy, Mapping} threshold sets {τ c\nm} for each\nv τ c\ne , τ c",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "class, where c ∈ {Ang, Sad, Hap, Neu}",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(4)\nτv ∈ [P25(V) − ∆,\nP25(V) + ∆] ."
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "(iii) A set E of disallowed emotion changes (Eg. ”AngSad”, ”NeuHap”)",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "(iv) fm : a string flag indicating the sentiment-to-emotion mapping strategy",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "These ranges are motivated by the observation that\nincorrect"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "(iv)\nthe final mapping\nfi : a string flag indicating to invert",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "predictions\ntend to exhibit higher\nentropy and lower varen-"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "Ensure: Efinal: Final merged emotion",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "tropy, with\noptimal\nthreshold\nvalues\ntypically\nlocated\nnear"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": ", τ r.prediction\n, τ r.prediction\n1:\nτe, τv, τm ← τ r.prediction",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "2:\nif r.H(ps) ≥ τe and r.V(ps) ≤ τv then",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "the\n75th\npercentile\nfor\nentropy\nand\nthe\n25th\npercentile\nfor"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "3:\nif r.sentiment = \"neutral\" then",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "varentropy. The\nsearch is\nconducted using a fixed step size"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "4:\nemotion ← Neu",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "of ∆ = ±10 percentile points. Each candidate threshold pair"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "5:\nelse if r.sentiment = \"positive\" then",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "6:\nemotion ← Hap",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "is given by:"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "else\n7:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(5)\nτe = P75(H) − ∆ + kδ,\nτv = P25(V) − ∆ + lδ,"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "8:\n▷ Refer\nto primary model\nif fm = \"refer\" then",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "9:\nemotion ← Ang if r.ps",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "Ang ≥ r.ps\nSad else Sad",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "for k = 0, 1, . . . , K and l = 0, 1, . . . , L,\nforming a grid of"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "10:\n▷ Simple or flip mapping\nelse if fm = \"simple\" then",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "candidate thresholds. The optimal\nthresholds (τ c\ne , τ c\nv ) for each"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "emotion ← Ang if (r.pt",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "11:\nelse Sad\nr.sentiment ≤ τm) ⊕ fi",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "c\nemotion\nclass\nare\ndetermined\nby maximizing\na\ndetection"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "end if\n12:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "end if\n13:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "accuracy metric M, defined as:"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "14:\nif (r.prediction, emotion) ∈ E then",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "15:\nemotion ← r.prediction\n▷ Revert change",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(τ c"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "end if\n16:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(6)\nM(τe, τv),\ne , τ c\nv ) = arg max"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "else\n17:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(τe,τv)"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "18:\nemotion ← r.prediction",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "where the objective metric M is computed as:"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "end if\n19:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(cid:19)"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(cid:18) D"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "20: Efinal ← emotion",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "(cid:12)(cid:12)\n× 100,\n(7)\nM(τe, τv) = Accuracy ="
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "21:\nreturn Efinal",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "τe,τv"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "T"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "where D denoting the number of misclassified samples\nsuc-"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "cessfully identified by the\ncurrent\nthresholding rule,\nand T"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "representing the total number of samples that satisfy both the"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "dispersion of\nthe class probabilities relative to the entropy:",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "entropy and varentropy threshold conditions."
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "N(cid:88) i\nV = V(ps) =\nps\n(2)\nc (log(ps\nc) + H)2 .",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "D.\nSentiment Mapping Strategy"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "=1",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "This component addresses the challenge of mapping three-"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "Varentropy provides an additional measure of confidence by",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "class\nsentiment outputs\n(Positive, Neutral, Negative)\ninto the"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "assessing\nhow sharply\npeaked\nor flat\nthe\nprobability\ndistri-",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "required four emotional classes (Happy, Neutral, Sad, Angry)."
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "bution is,\nthereby quantifying the\nstability of\nentropy H. A",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "While positive and neutral\nsentiments map straightforwardly"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "higher varentropy V is preferred,\nindicating stable uncertainty",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "to Happy and Neutral,\nrespectively, negative sentiment needs"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "estimates of\nthe primary model, demonstrating robustness\nto",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "further discrimination between Sad and Angry emotions. We"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "minor\ninput variations.",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "propose\ntwo methods\nfor\nthis mapping,\nboth\nof which\nare"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "If a primary score ps has high entropy H and low varentropy",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "evaluated and the one yielding the higher accuracy is selected"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "V, measured against\nit\nis deemed\ntwo thresholds τH and τV ,",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "automatically."
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "unreliable.\nIn such cases, we defer\nthe final decision to the",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "1) Refer to Primary Model Mapping:\nIn cases of negative"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "secondary\nsentiment model. This\ndecision\nis\nimplemented",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "sentiment,\nthis method\nconsults\nthe\nprimary model’s\nconfi-"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "through the merge function described in Algorithm 1, which",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "dence scores between Angry and Sad emotions, assigning the"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "maps the sentiment prediction into one of\nthe target emotion",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "sentiment\naccordingly based on the higher\nconfidence\nscore"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "classes.\nIf\nthe thresholds are not met,\nthe original prediction",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "from the primary model."
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "from the primary model\nis used.",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "2)\nSimple or Flip Mapping: We establish a threshold (rang-"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "To\nderive\nthe\noptimal\nthresholds\nand\nper-\nτe\nτv, we",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "τ c\ning\nbetween\n0\nand\n1)\neach\nemotion\nclass\nc where\nm for"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "form a\ngrid\nsearch\nover\nall\npossible\npairs\nof\nentropy",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "and varentropy values within empirically determined ranges",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "3We initially experimented with a single set of\nthresholds searched across"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "on\nthe\ntraining\ndata.\nNote\nthat\nthe\nsearch\nfor\nopti-",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "the entire dataset, but\nthe results were unsatisfactory. Upon analysis, we found"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "mal\nthresholds\nis\nconducted\nseparately\nfor\neach\nemotion",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "that each class exhibits distinct entropy and varentropy distributions, which"
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "{τ c",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": ""
        },
        {
          "Algorithm 1 Efinal = Merge(r, τ c\ne , τ c\nv , τ c\nm, E, fm , fi )": "∈\nclass\nc,\nresulting\nin\nfour\nsets\nof\nthresholds\ne , τ c\nv }",
          "[{τ Ang\n, τ Ang\n}, {τ Sad\n, τ Sad\n}, {τ Hap\n, τ Hap\n}, {τ Neu\n, τ Neu\n}].3 To do": "motivated the use of class-wise thresholds."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "above to Angry. This mapping is flexible and can be reversed",
          "session as\nthe test\nset, while the remaining five sessions are": "split\ninto training (80%) and validation (20%) subsets."
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "through a ”flip” flag since both emotions can validly represent",
          "session as\nthe test\nset, while the remaining five sessions are": "Table I\nlists\nthe number of utterances\nfor each emotion in"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "a\nnegative\nsentiment.\nIn\na\nsimilar\nprocess\nto\nobtaining\nthe",
          "session as\nthe test\nset, while the remaining five sessions are": "each session for both datasets."
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "optimal entropy and varentropy threshold values, a grid search",
          "session as\nthe test\nset, while the remaining five sessions are": "1)\nSettings:\nFor\neach\ncross-validation\nfold, we\ndynami-"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "on the\ntraining dataset\nis performed on a\nrange of discreet",
          "session as\nthe test\nset, while the remaining five sessions are": "cally determine the optimal entropy, varentropy, and sentiment"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "values\nset\nat\nconstant\nintervals\nusing\nthe\nsame\ndetection",
          "session as\nthe test\nset, while the remaining five sessions are": "thresholds based on the\ntraining data of\nthat\nfold,\nand then"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "accuracy metric previously mentioned.",
          "session as\nthe test\nset, while the remaining five sessions are": "apply them to the corresponding test set. This approach ensures"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "adaptive yet consistent prediction merging across all\nfolds for"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "E. Revert Change Strategy",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "both the IEMOCAP and MSP-IMPROV datasets."
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "{τ c\nOnce\nall\noptimal\nthreshold\npairs\nare\ndeter-\ne , τ c\nv , τ c\nm}",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "TABLE II"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "mined, we adopt an additional strategy to enhance the predic-",
          "session as\nthe test\nset, while the remaining five sessions are": "WER (%) FOR DIFFERENT SPEECH-TO-TEXT MODELS (LEFT) AND F1 (%)"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "FOR DIFFERENT TEXT-BASED SENTIMENT MODELS USING GROUND"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "tion: we aim to construct an exclusion list E using the training",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "TRUTH TRANSCRIPTS (RIGHT)."
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "set, where all detrimental emotion changes are stored. Since",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "these\nchanges\ncause performance drops on the\ntraining set,",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "S2T\nWER\nSentiment\nF1 Score"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "during the inference stage,\nif such a change occurs,\nit will be",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "Neg.\nNeu.\nPos.\nOverall"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "skipped, and the primary prediction will be retained.",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "76.39\nw2v-CTC4\n34.26\nDistilBERT5\n11.62\n2.75\n17.79"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "Regarding the creation of E,\nfor each training sample,\nif,",
          "session as\nthe test\nset, while the remaining five sessions are": "46.12\nwhisper-tiny6\n22.96\nRoBERTa7\n48.09\n46.60\n43.66"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "for example, a sample’s primary prediction is Angry, and after",
          "session as\nthe test\nset, while the remaining five sessions are": "14.71\n50.74\n48.42\nwhisper-large8\nRoBERTa-XLM9\n39.10\n46.09"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "applying the entropy and valentropy thresholds,\nthe prediction",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "changes\nto Sad\n(indicated\nas AngSad), while\nthe\naccuracy",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "B. Results on Various Text Modality Models"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "calculated\nusing\nthe\nsame\nperformance metric\ndefined\nin",
          "session as\nthe test\nset, while the remaining five sessions are": ""
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "Equation 7 drops,\nthis\nis\nconsidered a harmful\nchange\nand",
          "session as\nthe test\nset, while the remaining five sessions are": "The left side of Table II compares three S2T ASR models:"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "will be added to E. Such changes\nshould be avoided during",
          "session as\nthe test\nset, while the remaining five sessions are": "w2v-CTC [5], Whisper-tiny-en\n[23],\nand Whisper-large-v3"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "the reference stage.",
          "session as\nthe test\nset, while the remaining five sessions are": "[23]. Each model was\nused\nto\ntranscribe\naudio\nsegments"
        },
        {
          "sentiment\nscores below the\nthreshold map to Sad and those": "",
          "session as\nthe test\nset, while the remaining five sessions are": "from the IEMOCAP dataset. Among them, Whisper-large-v3"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "5"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "170"
        },
        {
          "TABLE I": "442"
        },
        {
          "TABLE I": "384"
        },
        {
          "TABLE I": "245"
        },
        {
          "TABLE I": "1241"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "METHOD ON IEMOCAP DATASET"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Before"
        },
        {
          "TABLE IV": "68.18"
        },
        {
          "TABLE IV": "69.30"
        },
        {
          "TABLE IV": "68.81"
        },
        {
          "TABLE IV": "67.34"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "63.03"
        },
        {
          "TABLE IV": "62.32"
        },
        {
          "TABLE IV": "62.12"
        },
        {
          "TABLE IV": "64.02"
        },
        {
          "TABLE IV": "66.27"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "54.99"
        },
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "64.64"
        },
        {
          "TABLE IV": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "S+T\nVarentropy\n65.56\n64.81\n64.24"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "65.05\n64.55\nS+T\nEntropy + Varentropy\n65.81"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "pipeline models,\nversus\nnot\nusing\nany\nscore\nselection. We"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "then report\nthe results for three metrics, Unweighted Accuracy"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "(UA), Weighted Accuracy (WA), and F1 Score,\nfor each fold"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "on both IEMOCAP and MSP-IMPROV."
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "1) Comparison With and Without Score Merging Methods:"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "Table III lists the results on IEMOCAP with and without score"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "selection strategies. Apparently, applying any score selection"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "strategy, whether\nbased\non\nentropy,\nvarentropy,\nor\nboth\nof"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "them,\nleads\nto\nimprovements\nacross\nall\nevaluation metrics"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "compared\nto\nthe\nspeech-modality-only method\n(w/o\nscore"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "selection),\nexcept when using entropy alone, where\nthe WA"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "result\n(63.85%)\nfalls slightly short. Among the strategies,\nthe"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "combined Entropy + Varentropy approach achieves the highest"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "overall performance, with UA (65.81%), WA (65.41%), and F1"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "score (64.55%), surpassing both individual-threshold methods"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "and\nsignificantly\noutperforming\nthe\nbaseline. Therefore, we"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "adopt\nthe combined approach for\nits enhanced robustness and"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "reliability for\nthe following experiments."
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "TABLE IV"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "UA, WA, AND F1 SCORE RESULTS ACROSS FOLDS FOR OUR PROPOSED"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "METHOD ON IEMOCAP DATASET"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "Fold\nUA (%)\nWA (%)\nF1 Score (%)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "Before\nAfter\n(Change)\nBefore\nAfter\n(Change)\nBefore\nAfter\n(Change)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "1\n71.04\n70.53 (-0.51)\n68.18\n67.80 (-0.38)\n68.12\n67.72 (-0.40)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "70.86\n70.58 (-0.28)\n69.30\n69.12 (-0.18)\n69.91\n69.73 (-0.18)\n2"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "3\n67.89\n69.40 (1.51)\n68.81\n70.06 (1.25)\n69.95\n71.34 (1.39)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "4\n70.80\n71.34 (0.54)\n67.34\n67.71 (0.37)\n68.83\n69.10 (0.27)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "62.58\n64.09 (1.51)\n63.03\n64.37 (1.34)\n60.60\n62.50 (1.90)\n5"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "6\n62.41\n62.79 (0.38)\n62.32\n62.96 (0.64)\n61.98\n62.59 (0.61)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "7\n61.10\n61.54 (0.44)\n62.12\n62.88 (0.76)\n59.49\n60.25 (0.76)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "66.48\n65.31 (-1.17)\n64.02\n63.22 (-0.80)\n63.98\n63.18 (-0.80)\n8"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "9\n64.43\n64.98 (0.55)\n66.27\n65.93 (-0.32)\n67.28\n67.23 (-0.05)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "10\n56.02\n57.77 (1.55)\n54.99\n56.53 (1.54)\n50.00\n51.95 (1.95)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "AVG\n65.36\n65.81 (0.45)\n64.64\n65.06 (0.42)\n64.01\n64.56 (0.55)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "TABLE V"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "UA, WA, AND F1 SCORE RESULTS ACROSS FOLDS FOR OUR PROPOSED"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "METHOD ON MSP-IMPROV DATASET"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "Fold\nUA (%)\nWA (%)\nF1 Score (%)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "Before\nAfter\n(Change)\nBefore\nAfter\n(Change)\nBefore\nAfter\n(Change)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "1\n58.80\n58.41 (-0.39)\n65.02\n64.55 (-0.47)\n58.55\n58.08 (-0.47)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "2\n50.74\n52.88 (2.14)\n59.59\n61.07 (1.38)\n51.81\n54.30 (2.49)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "3\n50.16\n52.05 (1.89)\n65.33\n65.9 (0.57)\n50.55\n52.73 (2.18)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "4\n49.76\n50.95 (1.19)\n54.69\n55.84 (1.15)\n51.12\n52.5 (1.38)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "59.17\n59.06 (-0.11)\n63.1\n63.1 (0.00)\n60.72\n60.48 (-0.24)\n5"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "6\n44.71\n46.11 (1.40)\n50.17\n51.16 (0.99)\n43.99\n45.7 (1.71)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "AVG\n52.22\n53.24 (1.02)\n59.67\n60.27 (0.69)\n52.79\n53.97 (1.18)"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": ""
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "2)\nIEMOCAP Dataset Results: Table IV shows\nfold-wise"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "performance of the proposed score selection method on IEMO-"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "CAP. Results\nare\ncompared\nbefore\nand\nafter\napplying\nthe"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "entropy + varentropy-based score selection. Seven out of\nten"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "folds (Folds 3–7, 9, and 10) exhibit consistent\nimprovements"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "across\nall\nthree\nevaluation metrics,\ndemonstrating\nthat\nthe"
        },
        {
          "65.87\nS+T\nEntropy\n63.85\n64.46": "merge strategy, through confidence-driven switching, leveraged"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale self-supervised pre-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "green bars (correct predictions) are consistently lower than the",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "IEEE Journal\nof\nSelected\ntraining\nfor\nfull\nstack\nspeech\nprocessing,”"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "red bars\n(incorrect predictions)\nacross\nall\nfour\nclasses,\nsug-",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "gesting that\nlower entropy values are associated with greater",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[8]\nS. Poria, N. Majumder, R. Mihalcea, and E. Hovy, “Emotion recognition"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "in\nconversation: Research\nchallenges,\ndatasets,\nand\nrecent\nadvances,”"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "confidence\nand\ncorrectness.\nIn\nthe\ncase\nof\nvarentropy,\nthe",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "IEEE access, vol. 7, pp. 100 943–100 953, 2019."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "green hatched bars (correct predictions) are generally higher or",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[9]\nT. Baltruˇsaitis, C. Ahuja,\nand L.-P. Morency,\n“Multimodal machine"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "more centrally distributed than the red hatched bars (incorrect",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "learning: A survey and taxonomy,” IEEE transactions on pattern analysis"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "and machine intelligence, vol. 41, no. 2, pp. 423–443, 2018."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "predictions),\nindicating that higher varentropy values likewise",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[10]\nS. Poria, N. Majumder, D. Hazarika, E. Cambria, A. Gelbukh,\nand"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "correspond to greater prediction confidence and accuracy.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "A. Hussain, “Multimodal sentiment analysis: Addressing key issues and"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "Moreover, each emotion class demonstrates a distinct range",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "setting up the baselines,” IEEE Intelligent Systems, vol. 33, no. 6, pp."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "17–25, 2018."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "of\nentropy\nand\nvarentropy\nvalues,\nreinforcing\nthe\nneed\nfor",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[11] A. Zadeh, M. Chen, S. Poria, E. Cambria,\nand L.-P. Morency,\n“Ten-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "a\nper-class\nthresholding\nstrategy. A single\nglobal\nthreshold",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "sor\nfusion network for multimodal\nsentiment analysis,” arXiv preprint"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "would fail\nto capture these class-specific patterns effectively.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "arXiv:1707.07250, 2017."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[12]\nZ. Lu, L. Cao, Y. Zhang, C.-C. Chiu,\nand J. Fan,\n“Speech sentiment"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "Additionally,\nthe merging\nprocess\ncan\noccasionally\nturn\na",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "analysis via pre-trained features from end-to-end asr models,” in ICASSP"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "correct prediction into an incorrect one. Therefore, choosing",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "2020-2020\nIEEE International Conference\non Acoustics,\nSpeech\nand"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "an appropriate threshold becomes a trade-off:\nit must balance",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "Signal Processing (ICASSP).\nIEEE, 2020, pp. 7149–7153."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[13]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "maximizing the detection of\nincorrect predictions while mini-",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "of deep bidirectional\ntransformers for\nlanguage understanding,” in Pro-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "mizing the erroneous rejection of correct ones. This trade-off",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "ceedings of\nthe 2019 conference of\nthe North American chapter of\nthe"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "motivates the use of accuracy as the evaluation metric during",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "association for computational\nlinguistics: human language technologies,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "volume 1 (long and short papers), 2019, pp. 4171–4186."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "parameter selection.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[14] Y. Liu, “Roberta: A robustly optimized bert pretraining approach,” arXiv"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "preprint arXiv:1907.11692, vol. 364, 2019."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "CONCLUSION",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[15] A. K. Katsaggelos, S. Bahaadini, and R. Molina, “Audiovisual\nfusion:"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "In\nthis\nstudy, we\nproposed\na multimodal\nscore\nselection",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "the\nChallenges\nand new approaches,” Proceedings of\nIEEE, vol. 103,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "no. 9, pp. 1635–1653, 2015."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "methodology that combines a primary wav2vec2-based speech",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[16] C. Chen and P. Zhang, “Modality-collaborative transformer with hybrid"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "branch with a secondary Whisper + RoBERTa-XLM sentiment",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "feature\nreconstruction\nfor\nrobust\nemotion\nrecognition,” ACM Trans-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "branch for SER. For score selection between the two branches,",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "actions on Multimedia Computing, Communications and Applications,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "vol. 20, no. 5, pp. 1–23, 2024."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "we\nleveraged\nentropy\nand\nvarentropy\nthresholds\nto\nidentify",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[17] A.-L. Georgescu, G.-I. Chivu,\nand H. Cucu,\n“Exploring fusion tech-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "uncertain\npredictions\nand\ndynamically\nswitched\nto\nthe\nsec-",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "niques for multimodal emotion recognition,” in 2024 15th International"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "ondary pipeline for\nimproved reliability. Experiments on both",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "Conference on Communications (COMM).\nIEEE, 2024, pp. 1–6."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[18] K.-S. Song, Y.-H. Nho, J.-H. Seo, and D.-s. Kwon, “Decision-level fusion"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "the IEMOCAP and MSP-IMPROV datasets demonstrated clear",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "method for emotion recognition using multimodal emotion recognition"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "improvements\nin\naccuracy,\nrobustness,\nand\nstability\nacross",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "information,” in 2018 15th international conference on ubiquitous robots"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "emotional classes, particularly in challenging cases\ninvolving",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "(UR).\nIEEE, 2018, pp. 472–476."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[19] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "conflicting emotional cues. Overall,\nthe proposed late fusion",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "and E. M. Provost, “Msp-improv: An acted corpus of dyadic interactions"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "strategy offers a computationally efficient, flexible, and reliable",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "to study emotion perception,” IEEE Transactions on Affective Comput-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "pipeline,\noutperforming\nsingle-modality\nsystems\nand\nfixed",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "ing, vol. 8, no. 1, pp. 67–80, 2016."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[20]\nJ. Camacho-Collados and M. T. Pilehvar, “On the role of\ntext prepro-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "fusion strategies.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "cessing in neural network architectures: An evaluation study on text"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "categorization and sentiment analysis,” arXiv preprint arXiv:1707.01780,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "REFERENCES",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "2017."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "[1] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[21] A. Radford,\nJ. W. Kim, T. Xu, G. Brockman, C. McLeavey,\nand"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "I. Sutskever, “Robust\nspeech recognition via large-scale weak supervi-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "dyadic motion capture database,” Language resources and evaluation,",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "sion,” in International conference on machine learning.\nPMLR, 2023,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "vol. 42, pp. 335–359, 2008.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "pp. 28 492–28 518."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "[2] B. Maji, M. Swain, R. Guha,\nand A. Routray,\n“Multimodal\nemotion",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[22]\nF. Barbieri, L. E. Anke, and J. Camacho-Collados, “Xlm-t: Multilingual"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "recognition based on deep temporal\nfeatures using cross-modal\ntrans-",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "language models\nin twitter\nfor\nsentiment analysis and beyond,” arXiv"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "ICASSP 2023-2023\nIEEE International\nformer\nand\nself-attention,”\nin",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "preprint arXiv:2104.12250, 2021."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "Conference\non\nAcoustics,\nSpeech\nand\nSignal\nProcessing\n(ICASSP).",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[23] A.\nRadford,\nJ. W.\nKim,\nT.\nXu,\nG.\nBrockman,\nC. McLeavey,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "IEEE, 2023, pp. 1–5.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "and\nI.\nSutskever,\n“Robust\nspeech\nrecognition\nvia\nlarge-scale weak"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "[3] D. Bertero and P. Fung, “A first look into a convolutional neural network",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "supervision,” 2022. [Online]. Available: https://arxiv.org/abs/2212.04356"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "for\nspeech emotion detection,” in 2017 IEEE international conference",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[24] V. Sanh, L. Debut,\nJ. Chaumond, and T. Wolf, “Distilbert, a distilled"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "on acoustics, speech and signal processing (ICASSP).\nIEEE, 2017, pp.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "arXiv\npreprint\nversion\nof\nbert:\nsmaller,\nfaster,\ncheaper\nand\nlighter,”"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "5115–5119.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "arXiv:1910.01108, 2019."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "[4] R. A. Khalil, E. Jones, M. I. Babar, T. Jan, M. H. Zafar, and T. Alhussain,",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[25] A. F. Adoma, N.-M. Henry,\nand W. Chen,\n“Comparative\nanalyses of"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "“Speech emotion recognition using deep learning techniques: A review,”",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "bert, roberta, distilbert, and xlnet for text-based emotion recognition,” in"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "IEEE access, vol. 7, pp. 117 327–117 345, 2019.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "2020 17th international computer conference on wavelet active media"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "[5] A. Baevski, Y.\nZhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "technology and information processing (ICCWAMTIP).\nIEEE, 2020,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "A framework\nfor\nself-supervised\nlearning\nof\nspeech\nrepresentations,”",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "pp. 117–121."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "Advances in neural\ninformation processing systems, vol. 33, pp. 12 449–",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "[26] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek,"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "12 460, 2020.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "F. Guzm´an, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, “Un-"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "[6] W.-N. Hsu, Y.-H. H. Tsai, B. Bolte, R. Salakhutdinov, and A. Mohamed,",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "supervised cross-lingual representation learning at scale,” arXiv preprint"
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "“Hubert: How much\ncan\na\nbad\nteacher\nbenefit\nasr\npre-training?”\nin",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": "arXiv:1911.02116, 2019."
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        },
        {
          "correlate with more\naccurate\npredictions.\nFor\nentropy,\nthe": "and Signal Processing (ICASSP).\nIEEE, 2021, pp. 6533–6537.",
          "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "Multimodal emotion recognition based on deep temporal features using cross-modal transformer and self-attention",
      "authors": [
        "B Maji",
        "M Swain",
        "R Guha",
        "A Routray"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "A first look into a convolutional neural network for speech emotion detection",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Hubert: How much can a bad teacher benefit asr pre-training?",
      "authors": [
        "W.-N Hsu",
        "Y.-H Tsai",
        "B Bolte",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "9",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "11",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "12",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C.-C Chiu",
        "J Fan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "14",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "15",
      "title": "Audiovisual fusion: Challenges and new approaches",
      "authors": [
        "A Katsaggelos",
        "S Bahaadini",
        "R Molina"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "16",
      "title": "Modality-collaborative transformer with hybrid feature reconstruction for robust emotion recognition",
      "authors": [
        "C Chen",
        "P Zhang"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "17",
      "title": "Exploring fusion techniques for multimodal emotion recognition",
      "authors": [
        "A.-L Georgescu",
        "G.-I Chivu",
        "H Cucu"
      ],
      "year": "2024",
      "venue": "2024 15th International Conference on Communications (COMM)"
    },
    {
      "citation_id": "18",
      "title": "Decision-level fusion method for emotion recognition using multimodal emotion recognition information",
      "authors": [
        "K.-S Song",
        "Y.-H Nho",
        "J.-H Seo",
        "D.-S Kwon"
      ],
      "year": "2018",
      "venue": "2018 15th international conference on ubiquitous robots (UR)"
    },
    {
      "citation_id": "19",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "On the role of text preprocessing in neural network architectures: An evaluation study on text categorization and sentiment analysis",
      "authors": [
        "J Camacho-Collados",
        "M Pilehvar"
      ],
      "year": "2017",
      "venue": "On the role of text preprocessing in neural network architectures: An evaluation study on text categorization and sentiment analysis",
      "arxiv": "arXiv:1707.01780"
    },
    {
      "citation_id": "21",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond",
      "authors": [
        "F Barbieri",
        "L Anke",
        "J Camacho-Collados"
      ],
      "year": "2021",
      "venue": "Xlm-t: Multilingual language models in twitter for sentiment analysis and beyond",
      "arxiv": "arXiv:2104.12250"
    },
    {
      "citation_id": "23",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "24",
      "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "25",
      "title": "Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "2020 17th international computer conference on wavelet active media technology and information processing (ICCWAMTIP)"
    },
    {
      "citation_id": "26",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "E Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Unsupervised cross-lingual representation learning at scale",
      "arxiv": "arXiv:1911.02116"
    }
  ]
}