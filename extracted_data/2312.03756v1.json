{
  "paper_id": "2312.03756v1",
  "title": "Linecongraphs: Line Conversation Graphs For Effective Emotion Recognition Using Graph Neural Networks",
  "published": "2023-12-04T19:36:58Z",
  "authors": [
    "Gokul S Krishnan",
    "Sarala Padi",
    "Craig S. Greenberg",
    "Balaraman Ravindran",
    "Dinesh Manoch",
    "Ram D. Sriram"
  ],
  "keywords": [
    "Affective Computing",
    "Human-computer Interaction",
    "Line Conversation Graphs",
    "Emotion Recognition in Conversations",
    "Graph Neural Networks",
    "IEMOCAP",
    "MELD",
    "Sentiment Analysis",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is a critical aspect of affective computing, and it has many practical applications in healthcare, education, chatbots, and social media platforms. Earlier approaches for ERC analysis involved modeling both speaker and long-term contextual information using graph neural network architectures. However, it is ideal to deploy speaker-independent models for real-world applications. Additionally, long context windows can potentially create confusion in recognizing the emotion of an utterance in a conversation. To overcome these limitations, we propose novel line conversation graph convolutional network (LineConGCN) and graph attention (LineConGAT) models for ERC analysis. These models are speaker-independent and built using a graph construction strategy for conversations -line conversation graphs (LineConGraphs). The conversational context in LineConGraphs is short-term -limited to one previous and future utterance, and speaker information is not part of the graph. We evaluate the performance of our proposed models on two benchmark datasets, IEMOCAP and MELD, and show that our LineConGAT model outperforms the state-of-the-art methods with an F1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding sentiment shift information into line conversation graphs further enhances the ERC performance in the case of GCN models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Automatic Emotion recognition (ER) is an essential tool in human-computer interaction (HCI) as it allows people to interact with robots more naturally  [1, 2] . It also plays a significant role in various sectors, such as healthcare, education, surveillance, security, and automation  [3] . Emotion Recognition in Conversation (ERC) is a sub-field of ER that involves identifying the emotions of speakers in a given conversation as opposed to utterance level. Humans express emotions in various ways, resulting in different modalities of data such as video, audio, and text that can be potentially used for ERC. However, in the modern world, there has been a significant shift in the way textual content is created and used, enabling us to collect and mine textual data to analyze people's emotions. This shift has opened up new avenues for ERC analysis, and the analysis of textual data is now an important part of the ERC study. So, in this study, we focus on text utterances at a conversational level for ERC analysis.\n\nIn a conversation, speakers tend to maintain a stable emotional trend in line with their speaking logic, which is known as emotional inertia. This phenomenon has been discussed by Poria et al.  [4] . However, conversations have a unique nature, and speakers' emotions can be influenced by other speakers, resulting in emotion shifts  [5] . Previous models have primarily focused on emotional inertia and paid less attention to emotion shifts, making emotion recognition errors more likely to occur during emotion shifts  [4] . To tackle this challenge, researchers have proposed several models, including multi-talk learning models  [6, 7]  and crossmodal fusion networks  [8, 9]  with emotion-shift awareness  [10] . Gao et al.  [6]  proposed a multi-task learning model for emotion recognition that exploits emotion shift detection. However, the model's dependence on speaker information to generate emotion shift labels limited the effectiveness of the approach. On the other hand, Jiang Li et al.  [10]  proposed a Cross-modal Fusion Network with Emotion-Shift Awareness (CFN-ESA) for emotion recognition. The model concatenates utterance level and emotion shift features for audio, video, and text modalities and utilizes a multi-head attention recurrent neural network to optimize for ERC and emotion shift classification tasks. While both methods generated emotion shift labels from the actual emotional labels, it is not evident from the reported results that integrating emotion shift significantly improved the ERC performance. One potential reason for this could be that emotions and emotion shifts generated from emotion labels do not convey distinct information to each other.\n\nIndividuals express emotions in a unique way that can differ from person to person. To improve emotional recognition and understanding in conversations, it may be helpful to consider speaker-specific modeling based on preceding utterances. There are several proposed models that capture the emotional trends and dynamics of participants, including DialogXL  [5] , CauAIN  [11] , SAPBERT  [12] , DialogueEIN  [13] , and RBAGCN  [14] . Additionally, several graph-based models, such as MMGCN  [15] , RobERTa  [16] , Interactive Conversational memory Network (ICON)  [17] , conversational GCN  [18] , DialogRNN  [19] , DialogGCN  [20] , have utilized speaker information to model inter-intra speaker relationships and improve ERC performance. These models use various approaches to model inter and intra-speaker dependencies, causal-aware interaction networks, speaker identification, emotional interaction networks, static and dynamic states of speakers  [21] , and relational bilevel aggregation graph convolutional networks, to enhance emotional recognition accuracy in conversations. However, it's important to note that it's not always clear whether speaker modeling improves emotion recognition performance in ERC analysis. Results have shown that including speakers without any random speaker initializations, the performance is almost similar. This suggests that embedding or modeling speaker information into graph neural networks may not always improve model performance and may even create noise by feeding too much context, which could affect the ERC performance.\n\nContext is crucial in natural language processing (NLP) research domain, and contextual embeddings have been shown to significantly enhance the performance of NLP systems  [12] . In addition, context plays a significant role in recognizing the emotions of utterances in each conversation. The relevance of context varies depending on the problem and can be derived from local or distant conversational history. While local context is more apparent, distant context can also be useful when a speaker refers to earlier utterances spoken by anyone in the conversational history  [22] . In conversations, determining the emotion of an utterance at a specific time can be done by considering the preceding utterances at a time less than the current time as its context. Several methods have been developed to capture long-range global context, including contextual attention networks  [23] , context and sentiment-aware frameworks  [24] , Dual stream Recurrence attention networks  [25] , and conditional random fields  [22] , to improve ERC performance. However, computing this context and deciding how much of it to consider can be challenging due to the dynamic nature of emotions.\n\nAs mentioned earlier, prior methods of ERC analysis incorporated speaker information, emotion shift, and contextual information using graph neural network architectures. However, these approaches have limitations due to the unavailability of the speaker's identity or impracticality in building and deploying speaker-independent models for real-time applications. Moreover, recognizing the emotion of an utterance in a conversation can become confusing due to long-distance context. To address these limitations, we propose two novel models for ERC analysis: the line conversation graph convolutional network (LineConGCN) and the graph attention (LineConGAT) models, developed on line conversation graphs (LineConGraphs) by utilizing node feature representations extracted from a transformer model. These models are speaker-independent, and the context in LineConGraphs is limited to one previous and future utterance. We evaluated these models on two benchmark datasets, IEMOCAP and MELD, and demonstrated that our LineConGAT model outperforms the state-of-the-art methods. Furthermore, we showed that embedding sentiment shift information into LineConGCN can further enhance the ERC performance.\n\nThe main contributions of this paper are follows: 1) Propose Line Conversation Graphs (LineConGraphs), a speaker independent approach to represent conversations in the form of graphs, utilizing node feature representations extracted from transformer models. 2) Design LineConGCN and LinConGAT models to learn the nuances of the conversations represented by the proposed LineConGraph approach. 3) Embed sentiment shift information into LineConGraphs to capture the change in emotions for ERC analysis. 4) Compare the proposed approach to the state-of-the-art (SOTA) methods for ERC using two benchmark datasets, IEMOCAP and MELD. We also look at the proposed approach in greater detail. The rest of the paper is organized as follows: Section II delves into previous methods of constructing the graph representation for given conversations and developing graph neural network-based models for ERC analysis. Section III goes into detail about the proposed methodology for constructing the graph and applying GNNs for ERC. Sections IV and V provide a comprehensive discussion of the dataset utilized for the ERC analysis and provide a comprehensive analysis of the experiment results. Finally, Section VI concludes the paper with prospective future scope.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "When it comes to analyzing context-based utterances for ERC analysis, there are several successful methods that have been applied. Graph-based neural networks and transformer-based models have both been used to model long-term dependencies between words in utterances and relationships between utterances  [26] . However, prior methods have mostly focused on modeling the context extending the graph-based neural networks to model utterances, speakers, and relationships in a conversation  [3] . Emotional dynamics in conversations play a crucial role in ERC analysis, including inter-and intra-personal dependencies  [27] . To model these interactions, researchers have proposed several frameworks. For instance, Hazarika et al.  [17]  proposed the Interactive COnversational memory Network (ICON), which is a multimodal emotion detection framework that hierarchically models the self-and inter-speaker emotional influences into a global context. Similarly, Zhang et al.  [18]  proposed a conversational graph convolutional neural network (ConGCN) that models utterances and speakers in a given conversation. Another approach is DialogRNN  [19] , which builds recurrent neural network models by jointly encoding the preceding utterances and speaker states for context representation along with emotion representation from the state and preceding speaker states as context. Dialog graph convolutional networks (DialogGCN)  [20]  is another model that captures the long-distance context in given conversations.\n\nHu et al.  [15]  proposed a multimodal fused GCN (MMGCN) that effectively utilizes both multimodal and long-distance contextual information by leveraging speaker information to capture inter-and intra-speaker dependencies. Similarly, Kim and Vossen  [16]  proposed EmoBERTa approach which can learn intra-and inter-speaker states and context to predict the emotion of a current speaker by simply prepending speaker names to utterances and inserting separation tokens between the utterances in a dialogue. Choi et al.  [28]  proposed a residualbased graph convolution network (RGCN), to fully exploit the intra-inter informative features. This approach achieves superior performance compared with state-of-the-art methods, demonstrating the importance of explicit speaker modeling. Saxena et al.  [21]  recent study considers both conversational context and speaker personality by building GNN encoder to model the internal state of the speaker (personality) as a Static and Dynamic speaker state and reported state-of-the-art results. However, by embedding or modeling speaker information into GNN model does not always improve the model performance, and can instead create a considerable amount of noise by feeding too much context.\n\nLi et al.  [29]  introduced an ERC model that considers discourse relations and symbolic knowledge in multi-party conversations by generating the context-aware transformer representations and knowledge extraction to facilitate a better understanding of an emotion in a given conversations. Similarly, Yang et al.  [30]  proposed Supervised Cluster-level Contrastive Learning (SCCL) to enhance the encoder for extracting context-aware representations by utilizing pre-trained knowledge adapters for ERC. On the other hand,  [7, 31]  proposed a multimodal multitask learning models using intramodal and intermodal attention mechanisms for emotion recognition analysis. Furthermore, Li et al.  [32]  proposed a multimodal fusion-based approach called GraphMFT. The approach uses an improved Graph Attention (GAT) model to extract representations from three graphs consisting of text, vision, and audio modalities. Additionally, the approach utilizes speaker embeddings similar to MMGCN  [15]  to enhance the predictions of emotions, which makes it speaker-dependent.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Graph Construction Strategies",
      "text": "In this section, we focus on the graph construction strategies of some of the prior approaches for ERC. MMGCN  [15]  constructs a graph by considering utterances as nodes and connecting nodes from different modalities, such as audio, text, and video, based on the corresponding utterances. This allows for the modeling of contextual information across multiple modalities. Interestingly, the presence of speaker information in the MMGCN network did not significantly enhance emotion recognition performance in conversations, as indicated by the nearly identical F1 score with and without speaker embedding across various datasets. Other models, such as ConGCN  [18]  and GraphMFT  [32]  also use graph construction for emotion recognition analysis. The former constructs a graph by taking into account the utterances and speakers as nodes, while the latter considers the utterances as nodes and connects all past and future utterances in a conversation to the node. GraphMFT also considers two modalities at a time to represent the graph and includes speaker embeddings similar to MMGCN to slightly improve the ERC performance.\n\nSaxena et al.  [21]  created a graph using utterance and speaker information as nodes. The graph was fully connected but limited to a context of three utterances (previous and next) for GCN and five for GAT models. The results showed that speaker embedding did not significantly improve model performance, with a difference of only 1.2%. The authors only considered the past utterance context for emotion recognition analysis. Similarly, Ghosal et al.  [20]  hypothesized that each utterance in a conversation is contextually dependent on all other utterances, but constructing a fully connected graph is computationally expensive. Instead, the authors used a practical solution of constructing edges with a past and future context window size of ten. Choi et al.  [28]  also used a fully connected graph with a fixed window size of ten, without speaker information. Modeling the attention weights improved the F1 score for the IEMOCAP dataset, but not for the MELD dataset. Additionally, it is not clear how long-distance context or speaker information can help improve the emotional recognition of utterances. Further, extending the context window beyond a conversation could aggregate too much information and make it more challenging for the model to identify emotions. Overall, while graph-based approaches have shown promise in emotion recognition in conversations, the role of speaker information and long-distance context is still unclear. In this paper, we focus on building a speaker-independent model that considers a short context window of previous and next utterances for effectively modeling emotional categories for ERC.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The Figure  1  provides an overview of the proposed methodology for ERC analysis. The framework comprises two main components: graph construction and building the graph neural network models for ERC analysis. The building blocks of the proposed framework for ERC analysis are divided into three parts. The left block contains examples of conversations from the MELD dataset. The middle block describes the line graph construction strategy, while the final block is focused on modeling utterances at the conversation level for emotion recognition analysis. These building blocks form the core of the proposed methodology and are discussed in greater detail in subsequent sections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Linecongraphs: Line Conversation Graphs",
      "text": "Graphs are an immensely powerful tool for describing complex systems. They consist of nodes or objects, connected by edges or interactions. The real strength of graph formalism is its ability to emphasize the relationships between nodes while maintaining a remarkable level of versatility  [33] . Recently, graph-based neural networks have been proposed for ERC analysis, demonstrating the immense potential of using  graphs for data analysis  [34, 35, 36] . The graph representation that is fed to the graph-based neural network models plays a critical role in the overall ERC pipeline and its analyses and therefore, we aim to improve the ERC performance by improving the effectiveness of the graph representation.\n\nC: Conversations, u: Utterances, x: Node features u 4",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Linecongraphs: Line Conversation Graphs",
      "text": "Fig.  2 : The construction of a line conversation graph for a corpus of conversations. Abbreviations: C i : i th conversation; u i : i th utterance; x i : feature extracted for i th utterance node; NOTE: self-loops are not shown in the figure for readability\n\nIn a conversation, current utterance emotion mostly depends on the emotions of other utterances, especially the previous ones  [20, 37] . This phenomenon is called \"emotion shift\", and we can capture this change in emotion using graphs that consider specific contexts of a conversation. Our proposed approach, called LineConGraphs, leverages line graphs to identify emotions accurately. Each node in the graph represents a single utterance, and each graph represents an entire conversation. By focusing on the previous and next utterances, we gain invaluable insight into the nuances of \"emotion shift\", allowing our model to effectively identify the emotion of a given utterance in a conversation. What sets LineConGraphs apart is its ability to consider each conversation in a corpus as a separate entity, ensuring that emotion recognition is performed based solely on the short context of that particular conversation. This approach eliminates the risk of unnecessary context or information creeping in from other conversations in the corpus. We can represent a corpus with many conversations as a single graph of connected utterance nodes, enabling GNN models to generalize learning from patterns and accurately classify similar emotions across different conversations.\n\nFigure  2  illustrates how the proposed LineConGraphs works for given utterances in a conversation. For example, consider a corpus with three conversations-C 1 , C 2 and C 3 , and nine utterances in total where C 1 consists of {u 1 , u 2 , u 3 }, C 2 contains {u 4 , u 5 , u 6 , u 7 } and {u 8 , u 9 } are part of C 3 . As we can see in the graph depicted in Figure  2 , each utterance is represented by a node, and these nodes are linked by connecting each utterance node to its previous and subsequent nodes. Additionally, we also have self-loops for each node to ensure that the current node becomes a part of the aggregation process in GNNs. In this particular line graph, every node has a degree of 2 (3, including the self-loop)  1  , with the exception of the first and last nodes in a conversation, which has a degree of 1 (2 including the self-loop). We generate a graph for every conversation within the specified dataset. As each node represents an utterance, we perform a node classification task on top of the graph to obtain the emotional labels for each utterance. This can be described as:\n\nGiven a conversation C with t utterances, C = (u 1 , u 2 , u 3 , ...u t ), spoken by m speakers S p = (sp 1 , sp 2 , sp 3 , ...sp m ), with n emotion labels, E = (e 1 , e 2 , e 3 , ...e t ), where each utterance u ∈ C, associated with a speaker s ∈ S, has a labeled emotion e ∈ E. The goal is to design a model to learn a function f (u, C) → E, that can predict the emotion of an utterance u in a given conversation.\n\n1) Node Feature Extraction: The graph neural network model has a significant advantage in its ability to incorporate node features into the learning process. It has been found that GNNs perform well when there is a strong correlation between node features and node labels  [38] . Different ways of initializing node features include a centrality-based approach and a learning-based approach. The former method constructs a node feature based on its local neighborhood, while the latter considers a node feature as the node embedding. Recently, EmoBERTa  [16]  has been proposed for the task of",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gcn Layers",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Classification",
      "text": "Fig.  3 : Overview of the proposed framework using the LineConGraphs with GCN layers for ERC analysis. We use sentiment shift as edge weights where \"s\" represents sentiment shift, and \"ns\" indicates there is no sentiment shift. [ u i : i th utterance ; X i : i th node feature extracted for an utterance; X k i indicates feature representation of i th utterance after k GCN layers] ERC, which can be used as a pre-trained model for generating effective feature representations of utterances. The proposed LineConGraphs approach uses EmoBERTa-base 2 for extracting the feature representation of the utterances, which is used as node level feature that the utterance is associated with. Formally, for each utterance, say u, we extract features from the pretrained model associated with the utterance, say u v ∈ R [1×n] ; where u v is a vector representation of utterance u and R [1×n] indicates a vector of real numbers of size n, where n is the dimension of the vector extracted for u from the transformer model.\n\n2) Edge Attributes: To better detect emotion shifts in utterances, we can embed sentiment information as edge weights or edge features in our line graph construction process and we refer to this phenomenon as \"sentiment shift\". For GCN models, we use edge weights to represent sentiment shifts, while for GAT models, we directly provide sentiment information as edge features. We assume that this can help the model effectively recognize the emotions of utterances. For example, in the case of GCN, u i and u j are two connected nodes with an edge weight \"s\" if there is a change in the sentiment, otherwise with an edge weight \"ns\" (s and ns indicate 'shift' and 'no shift' respectively). Similarly, in the case of GAT, the sentiment labels of nodes of an edge is embed as a vector: [s i , s j ] (s i and s j indicate the sentiment labels of nodes i and j respectively). Figures  3  and 4  demonstrate the embedded sentiment information in the graphs and the overall learning of GCN and GAT layers for ERC analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Learning Approaches For Erc Task",
      "text": "GNNs are powerful tools for representing data relationships in various scientific and engineering fields, such as computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining  [39, 40] . GNNs combine the strengths of recursive neural networks and random walk models to handle a broad range of graphs, including cyclic, directed, and undirected graphs without any preprocessing  [33, 41] . The key to GNNs is the generation of node representations, which depend on the graph's structure 2 https://huggingface.co/tae898/emoberta-base and feature information encoded at the graph, edge, and node levels. This flexible framework allows for the development of deep neural networks on graph data, overcoming the challenges of complex encoders for graph-structured data. In this paper, we use GNNs to effectively identify emotions in utterances through our LineConGraphs approach. We designed two different GNNs, Graph Convolution Network and Graph Attention Network, to achieve this goal. The following sections will elaborate on these models in detail.\n\n1) Graph Convolution Networks: Graph convolutional networks (GCNs)  [42]  have become an increasingly popular deep learning approach for graph-structured data. They offer a powerful tool for learning graph representations, with the \"graph convolution\" operation applying the same linear transformation to all the neighbors of a node followed by a nonlinear activation function. The GCN model is often used as a baseline GNN architecture  [43] , employing the symmetric-normalized aggregation and self-loop update approach. Overall, GCNs and their variants have proven to be a highly effective and versatile tool for analyzing graph-structured data.\n\nIn order to thoroughly examine the graphs created through the LineConGraphs approach, we make use of a GCN-based model that consists of two graph convolution layers and a Rectified Linear Unit (ReLU) activation layer situated in between. Let us consider a corpus with containing \"m\" utterances, each represented by an n-dimensional vector. Using the LineConGraphs methodology, we create a graph G = (V, E) where its node feature matrix X ∈ R [m×n] , we define the adjacency matrix as A and the degree matrix as D. The first layer of GCN learns from the n-dimensional feature representation X 0 to create a new representation X 1 of dimension \"k\". This can be represented as:\n\nwhere Â = D 1/2 AD -1/2 is a noramlized adjacency matrix and \"D\" indicates normalized degree matrix, and W 0 ∈ R [n×k] is a weight matrix. The newly extracted representation, X u 1",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Emotion Classification",
      "text": "Fig.  4 : Overview of the proposed framework using the LineConGraphs with GAT layers for ERC analysis. We use sentiment shift as edge feature where 0: negative, 1: neutral, 2: positive). [ u i : i th utterance ; X i : i th node feature extracted for an utterance; X k i feature representation of i th utterance after k GAT layers] represented as:\n\nwhere W 1 ∈ R [k×t] is a weight matrix for the second GCN layer and \"t\" is the number of labels, i.e., the number of emotions in this case. We then use argmax to select the maximum value among the t values in the final layer as the predicted emotion. Figure  3  gives an overview of the proposed framework which utilizes the LineConGraphs with GCN layers for ERC analysis. The GCN model takes the line graph as input and extracts the pretrained model features from EmoBERTa model as node features. Each node represents an utterance, and the sentiment shift between two adjacent utterances is used as edge weight. We use \"s\" to represent sentiment shift and \"ns\" to indicate no sentiment shift. The feature representation of each utterance after each GCN layer is denoted by X k i where k represents the number of layers. The final layer embeddings are used to recognize the emotion of a given utterance. We evaluate the performance of GCN model for ERC task with and without using sentiment shift as edge weights.\n\n2) Graph Attention Networks: The Graph Attention Networks (GAT) model, proposed by Velivckovic et al.  [44] , assigns importance or attention weights to each neighboring nodes in a graph. This allows for a neighbor's influence to be considered during the aggregation operation performed over each node's feature representations. However, the original GAT model suffers from the problem of static attention weights. This means that the same attention weight is selected for any set of node representations, limiting the effectiveness of the GAT layer. To solve this issue, a modified version of the GAT layer called GATv2 was proposed  [45] , which calculates dynamic attention for any set of node representations. We used the GATv2 based model on the constructed conversational graphs using the proposed LineConGraphs strategies with node features extracted from transformer based model EmoBERTa to predict the emotion labels associated with each node/utterance.\n\nFor example, a single layer GAT model learns the new representation X 1 i for \"i th \" node with an initial node feature X 0 i . This can be formalized as:\n\nwhere N (i) refers to the set of all neighbour nodes of node i; W 0 ∈ R [n×k] refers to shared learnable weight matrix for the first layer and α i,j is the shared learnable attention weight for the node connection from node i to its neighbour node j.\n\nThe attention weights for a node i to its neighbor nodes j are calculated as:\n\nwhere sf is a scoring function that computes a score for every edge (i, j) and is given as:\n\nwhere a is a learnable one dimensional vector, ⊕ indicates a concatenation operation and σ is LeakyReLU activation function.\n\nThe softmax function normalizes the scores so that the sum of all attention weights for a node and its neighbors adds up to 1.\n\nThe GAT learning model used for ERC analysis is given as follows:\n\nwhere σ is ReLU activation function; W 1 ∈ R [k×t] is a weight matrix for the second GAT layer and t is the number of labels, i.e., the number of emotions in this case; α 1 i,j and α 0 i,j are attention weights for edge connections between two nodes i and j. We then use argmax to select the maximum value among the t final layer values as the predicted emotion label.\n\nFigure  4  gives an overview of the proposed framework using the LineConGraphs with GAT layers for ERC analysis. There are two GAT layers used for ERC analysis and as part of the experiments, we embed sentiment shift as edge feature vectors where we directly consider the sentiment labels of connected nodes (0: negative, 1: neutral, 2: positive). Fro example, for conversation C 3 in Figure  4 , we have sentiment labels of utterances u 8 and u 9 as 'neutral' and 'negative' respectively. Therefore, the edge feature representation is indicated as  [1, 0] . The feature representation of each utterance after each GAT layer is denoted by X k i where k represents the number of layers. The final layer embeddings are used to recognize the emotion of an utterance in a given conversation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Datasets & Experimental Settings",
      "text": "In order to test the effectiveness of our proposed approach, we have utilized two well-known benchmark datasets for emotion recognition -Multi-modal Emotion Lines Dataset MELD  [46]  and interactive emotional dyadic motion capture IEMOCAP  [47] . Both datasets are multi-modal, meaning they include audio, video, and text components. In this paper, we only focused on the text modality for our experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Meld",
      "text": "It is an extension of the Emotion Lines dataset  [48] , which features conversations from the popular TV series \"Friends\". MELD includes 13708 utterances from 1433 conversations, featuring 304 unique characters from the show. As shown in Table  I , this dataset includes seven categorical emotions and sentiment scores for each utterance. The emotional categories and dataset splits used for our analysis are shown in Table  I . We utilized the training and development splits for fine-tuning our model and the test dataset to evaluate its performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Iemocap",
      "text": "It is a multimodal benchmark dataset for emotion recognition analysis. It includes 12 hours of data from ten different speakers, divided into 5 sessions with one male and one female speaker each. There are 151 conversations with 7433 utterances, covering nine categorical emotions and 3-dimensional labels. For our experiments, we only consider the utterances with at least two annotators agreeing on the emotion label. We focus on six emotions -happy, excited, neutral, sad, angry, and frustrated, resulting in a total of 5758 utterances for model training and 1622 utterances for model evaluations. Table II provides data statistics for the IEMOCAP dataset used for our experimental evaluations. We used the first four sessions of the dataset for training and the last session for model evaluations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Experimental Settings",
      "text": "In our experiments, we use PyTorch Geometric package  [49]  for implementing our proposed approach. We used a total of two GCN and two GAT layers, respectively, along with categorical cross-entropy as the loss function. Both models employed an AdamW optimizer with a learning rate of 10 - 3  and a weight decay parameter of 10 -4 . We employed sentiment shift information as edge weight in the case of GCN for MELD, where the value of s is \"1\" indicating no change in sentiment between connected nodes, and value of ns is \"-1\" which indicates a change in sentiment. Similarly, for the IEMOCAP dataset, we used \"1\" to indicate no change and \"2\" to indicate a change in sentiment labels 3 . Since the IEMOCAP dataset does not include sentiment scores, we used a RoBERTa-based sentiment classification model  [50]  to obtain sentiment labels and generate sentiment shift edge weights for the GCN model. In the GAT model, we encode the sentiment shift as an edge feature which contains the sentiment labels of connected nodes. We also construct a fully connected graph where every node is connected to every other node in a conversation and evaluated both GCN and GAT models on this graph using the same parameter settings as the line graph.\n\nWe used predefined train, development, and test datasets for experimental evaluations on the MELD dataset. As for the IEMOCAP dataset, we follow the state-of-the-art approaches, MMGCN  [15] , ConGCN  [18]  and DialogGCN  [20] , and trained the model on the first four sessions, while evaluating it on the fifth session. This evaluation ensures that our proposed method is evaluated in speaker-independent mode. To assess the performance of the proposed system, we report a weighted average F 1 score and compute confusion matrices to visualize how the system performed within and across different emotion categories.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Results And Discussions",
      "text": "In this section, we compare our proposed approach against the state-of-the-art approaches for ERC. We also perform a comparative analysis of speaker-dependent and independent models. Additionally, we analyze the performance of GCN and GAT models with and without sentiment shift. Finally, we visualize how emotions can often overlap and confuse with other categories of emotions highlighting the importance of accurately detecting and distinguishing between them.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Comparison With State Of The Art",
      "text": "We conducted extensive experimentation to assess the efficacy of the proposed LineConGraphs for ERC analysis and compared it with the current SOTA methods. In Table  III , we have presented a detailed performance comparison of our proposed line graph approach with GCN and GAT models against the current SOTA ERC task for text modality on the IEMOCAP and MELD benchmark datasets. Our line graph approach with GCN (LineConGCN) has surpassed all other methods with an impressive 68.18% on the MELD dataset, showcasing a significant 2.5% improvement over previous methods. Additionally, incorporating the sentiment shift information as edge weights (LineConGCNss) has led to an outstanding performance of 74.37%. This represents a significant 8.57% improvement over prior methods. The line graph approach with GAT (LineConGAT) further improved the ERC performance with an F1 score of 76.50%. This showcases a substantial 10.70% improvement over previous methods. Similar outcomes have been observed with sentiment shift as edge features. Overall, we can conclude that the GCN and GAT models built on top of LineConGraphs have achieved state-of-the-art results on the MELD dataset.\n\nOur line graph approach with GAT model (LineConGAT) demonstrated a comparable performance to previous methods on the IEMOCAP dataset achieving an impressive F1 score of 64.58%. However, we found that adding sentiment shifts as an edge feature did not lead to any significant improvement in performance. This could be due to the lack of ground truth sentiment scores available for the dataset, even though we used the RoBERTa based model to generate sentiment labels.\n\nAs indicated in the Table  III , our proposed approach is compared only to state-of-the-art methods that are consistent in experimental settings and test sets used for model evaluations. However, in the following section, we provide additional comparisons with other methods and discuss inconsistencies in model evaluations (Table  IV ).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Speaker Dependence In Erc",
      "text": "1) MELD: According to Table  IV , various multimodal and unimodal approaches incorporated speaker information for ERC analysis. However, these approaches may not be compelling in determining the emotion of the utterance in a specific context. It is important to note that speakers can express their emotions differently over time, making speaker-dependent models impractical for new speakers in real-time settings. Interestingly, except for ConGCN, all other methods achieved similar performance with and without speaker embedding. Moreover, speaker embedding did improve the model's performance for the IEMOCAP dataset, but it may be due to the limited number of speakers present in the dataset( only ten speakers). However, for the MELD dataset, there are 260 speakers, and there is no significant difference in performance with or without speaker embedding. Therefore, the proposed LineConGraphs approach outperformed prior methods that relied on speaker 2) IEMOCAP: Inconsistencies: Based on a thorough review of prior studies on the IEMOCAP dataset, it seems that there are inconsistencies in the experimental settings used to evaluate the performance of models in terms of ER analysis. However, Padi et al.  [53, 54]  have addressed these inconsistencies and have assessed the model's performance for both unimodal (text and audio) and multimodal scenarios for ER analysis. On the other hand, previous methods used to measure the performance of models for ERC analysis on the IEMOCAP dataset have Therefore, it is crucial to ensure that these settings are consistent to accurately evaluate the model's performance. In an effort to perform a fair comparison with the SOTA methods, we have compared our proposed approach with the current SOTA methods and their respective settings used for ERC analysis in Table  IV . Notably, MMGCN, GraphMFT, RGCN, and SCCL surpassed our model performance for the IEMOCAP dataset. However, GraphMFT and SCCL employed random data splits of 80% of data for training and 20% for testing their model performance for ERC analysis. On the other hand, EmoBERTa reported model performance on 10% of data, while 90% of data was used for model training. Moreover, MMGCN and GraphMFT models are multimodal, where they trained their models on audio, text, and video modalities. In contrast, our model is evaluated solely on text modality and in a speaker-independent manner. We trained our model in the first four sessions with eight speakers, four male, and four female, and evaluated it in the last session, which includes two speakers, one male and one female. This setup is consistent with DialogGCN, ConGCN, DialogRNN, and MMGCN models. While GraphMFT, RGCN, and EmoBERTa compared their model performances with DialogGCN, ConGCN, and DialogRNN models, it is not equitable to compare because the latter models were evaluated in session-wise, which is a speaker-independent way and represents around 20% of the IEMOCAP dataset, whereas, the former models measured their model performances on a random split which is the speaker-dependent way with either 20% or 10% data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Fully Connected Conversation Graphs",
      "text": "Section II, we provided an extensive literature review on various graph construction strategies for conversational modeling. Most previous methods focused on constructing fully connected graphs where every utterance in a conversation connects to every other in the given conversation.\n\nHowever, we believe this method could provide too much context, which may impact the model's ability to learn subtle nuances such as emotion shifting and mirroring. To assess the effectiveness of the proposed LineConGraphs approach with GCN and GAT models, a baseline called Fully Connected Conversation Graphs (FCConGraphs) was developed. This baseline approach will help us understand the effectiveness of our proposed method. We perform experiments by constructing a fully connected graph with GCN and GAT models. We also used sentiment information as edge weights for GCN and edge features for GAT. As shown in Table  V   with the GCN model (FullyConvGCN), and similarly, the line graph with the GAT model outperformed the FullyConvGAT model. This indicates that the line graph is the most effective approach for ERC analysis. Furthermore, the long-term context is aggregated by adding more GCN or GAT layers by allowing the information to be passed from past and future nodes in the graph without the need for explicit input. Therefore, the context is implicitly derived from the proposed LineConGraphs with the number of layers used in the model. 1) Sentiment Shift: Based on our analysis (Table  V ), we found that the addition of sentiment shift information in the form of edge weights and features does have a positive impact on the performance of the LineConGraphs strategy when applied to the MELD dataset. However, we do observe a decrease in the performance of the GCN model for the IEMOCAP dataset, which we suspect could be due to incorrect sentiment labeling. Despite this, we observed that the GAT model was able to compensate for this misclassification to some extent due to its ability to learn the attention weights dynamically. We conclude that while adding sentiment shift information can be beneficial in some cases, the GAT model's ability to learn the importance of connected edges is superior to the provision of edge features.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "D. Error Analysis",
      "text": "To illustrate the performance of the proposed line graph approach within and across different emotion categories, we report confusion matrices. Figure  5  shows the confusion matrices of the best performing models (LineConGAT) for the two benchmark datasets. The LineConGAT model confuses the \"disgust\", \"fear\", and \"sadness\" categories of emotions with the \"neutral\" class quite often while performing the best on the \"neutral\", \"joy\", and \"surprise\" emotions.\n\nSimilarly, in the case of IEMOCAP dataset, the LineConGAT model exhibits the highest performance in predicting the \"sadness\" and \"neutral\" categories of emotions. It should be noted that \"anger\" emotion is frequently confused with \"frustration\", \"excited\" emotion is often confused with \"neutral\" and \"happiness\" and the \"happy\" category of emotion is often confused with \"neutral\", and \"excited\". For the IEMOCAP dataset, the most similar emotions are getting confused, compared to the MELD dataset, where misclassifications are with the \"neutral\" emotion category.\n\nAs per the results presented in Table V, integrating sentiment shift as an edge feature to the LineConGCN model has improved the ERC performance for the MELD dataset. The confusion matrices were plotted to visually compare the LineConGCN and LineConGCN with sentiment edge weights (LineConGCN ss ). As shown in Figure  6 (a), the LineConGCN model confuses the \"Disgust\" category of emotion with \"neutral\" 33% of the time, and with \"angry\" 19% of the time. Similarly, \"surprised\" is confused with \"neutral\" 21% and with \"angry\" 10% of times. On average, the \"neutral\" emotion category got confused with the six categories of emotions for 27% of the time. In contrast, in LineConGCN ss (Figure  6  (b)), the \"Disgust\" category of emotion is mostly confused with \"anger\" 23% of the time and with \"neutral\" 17% of the time. Also, \"surprise\" is most confused with \"joy\" 17% of the time and with \"anger\" 12% of the time. The overall confusability of the \"neutral\" emotion category is reduced from 27% to 9% in LineConGCN ss compared to LineConGCN. It is worth noting that LineConGCN ss confuses between more similar emotions, while LineConGCN mostly confuses with the \"neutral\" emotion category. The study concluded that incorporating sentiment shift into graph modeling significantly improved the ERC performance by reducing the misclassification rate with the \"neutral\" emotion category. In the future, we intend to generate the sentiment labels for the IEMOCAP dataset using large language models like ChatGPT, and study the impact of the sentiment shift",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides an overview of the proposed",
      "page": 3
    },
    {
      "caption": "Figure 1: The building blocks of the proposed framework for ERC analysis.",
      "page": 4
    },
    {
      "caption": "Figure 2: The construction of a line conversation graph for a",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates how the proposed LineConGraphs",
      "page": 4
    },
    {
      "caption": "Figure 2: , each utterance",
      "page": 4
    },
    {
      "caption": "Figure 2: for readability",
      "page": 4
    },
    {
      "caption": "Figure 3: Overview of the proposed framework using the LineConGraphs with GCN layers for ERC analysis. We use sentiment",
      "page": 5
    },
    {
      "caption": "Figure 4: Overview of the proposed framework using the LineConGraphs with GAT layers for ERC analysis. We use sentiment",
      "page": 6
    },
    {
      "caption": "Figure 3: gives an overview of the proposed framework which",
      "page": 6
    },
    {
      "caption": "Figure 4: gives an overview of the proposed framework using",
      "page": 6
    },
    {
      "caption": "Figure 4: , we have sentiment labels of",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion matrices of LineConGAT model for MELD and IEMOCAP datasets. Abbreviations: Ang -Angry, Disg -",
      "page": 10
    },
    {
      "caption": "Figure 5: shows the confusion",
      "page": 10
    },
    {
      "caption": "Figure 6: (a), the LineConGCN",
      "page": 10
    },
    {
      "caption": "Figure 6: (b)), the “Disgust” category of emotion is mostly",
      "page": 10
    },
    {
      "caption": "Figure 6: Confusion matrices of LineConGCN models and LineConGCNss for MELD dataset. Abbreviations: Ang -Angry, Disg -",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "X3\nX2\nX4\nu2\nu3\nu4\nu1\nu5\nX5\nX1\nu: Utterances\nX: Node Features": "",
          "Sentiment Shift\nas Edge Weights": "Sentiment Shift\nas Edge Features"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "u1: You liked it? You really like it?\nEmotion: Surprise\nu2: Oh, yeah!\nEmotion: Joy\nu3: Which part exactly?\nEmotion: Neutral\nu4: The whole thing! Can we go?\nEmotion: Neutral\nu5: What about the scene with\nkangaroo?\nEmotion: Neutral": "",
          "Node\nFeatures\nFeature\nExtraction from\nTransformer\nConversation, Utterance\nEmotion": "Sentiment\nshift\nSentiment\nInformation"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "}\nC1: { u1, u2, u3\nS1: { positive, neutral, negative }\nE1: { excited, neutral, sad }\nC2: { u4, u5, u6, u7 }\nS2: { neutral, neutral, positive,\npositive }\nE2: { neutral, neutral, happy,\nhappy }\n }\nC3: { u8, u9\nS3: { neutral, negative }\nE3: { neutral, sad }": "",
          "Node\nFeatures\nFeature Extraction\nfrom\nTransformer Model\nConversation, Utterance\nEmotion": "Sentiment\nshift\nSentiment\nInformation"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "3",
      "title": "Being Human: Human-Computer Interaction in the Year",
      "authors": [
        "R Harper",
        "T Rodden",
        "Y Rogers",
        "A Sellen"
      ],
      "year": "2008",
      "venue": "Being Human: Human-Computer Interaction in the Year"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in conversations with emotion shift detection based on multi-task learning",
      "authors": [
        "Q Gao",
        "B Cao",
        "X Guan",
        "T Gu",
        "X Bao",
        "J Wu",
        "B Liu",
        "J Cao"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "7",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Domain invariant feature learning for speakerindependent speech emotion recognition",
      "authors": [
        "C Lu",
        "Y Zong",
        "W Zheng",
        "Y Li",
        "C Tang",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Emobed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Cfn-esa: A crossmodal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "J Li",
        "Y Liu",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Cfn-esa: A crossmodal fusion network with emotion-shift awareness for dialogue emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, L. D. Raedt",
      "doi": "10.24963/ijcai.2022/628"
    },
    {
      "citation_id": "12",
      "title": "Sapbert: Speaker-aware pretrained bert for emotion recognition in conversation",
      "authors": [
        "S Lim",
        "J Kim"
      ],
      "year": "2023",
      "venue": "Algorithms"
    },
    {
      "citation_id": "13",
      "title": "DialogueEIN: Emotion interaction network for dialogue affective analysis",
      "authors": [
        "Y Liu",
        "J Zhao",
        "J Hu",
        "R Li",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Rba-gcn: Relational bilevel aggregation graph convolutional network for emotion recognition",
      "authors": [
        "L Yuan",
        "G Huang",
        "F Li",
        "X Yuan",
        "C.-M Pun",
        "G Zhong"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "17",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19",
      "doi": "10.24963/ijcai.2019/752"
    },
    {
      "citation_id": "19",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "20",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Static and dynamic speaker modeling based on graph neural network for emotion recognition in conversation",
      "authors": [
        "P Saxena",
        "Y Huang",
        "S Kurohashi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop"
    },
    {
      "citation_id": "22",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Y Wang",
        "J Zhang",
        "J Ma",
        "S Wang",
        "J Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th annual meeting of the special interest group on discourse and dialogue"
    },
    {
      "citation_id": "23",
      "title": "A contextual attention network for multimodal emotion recognition in conversation",
      "authors": [
        "T Wang",
        "Y Hou",
        "D Zhou",
        "Q Zhang"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "24",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "G Tu",
        "J Wen",
        "C Liu",
        "D Jiang",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "A dual-stream recurrenceattention network with global-local awareness for emotion recognition in textual dialogue",
      "authors": [
        "J Li",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "A dual-stream recurrenceattention network with global-local awareness for emotion recognition in textual dialogue"
    },
    {
      "citation_id": "26",
      "title": "Graph neural networks for natural language processing: A survey",
      "authors": [
        "K Lingfei Wu",
        "Yu Chen",
        "B Long"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "How emotions work: The social functions of emotional expression in negotiations",
      "authors": [
        "M Morris",
        "D Keltner"
      ],
      "year": "2000",
      "venue": "Research in Organizational Behavior"
    },
    {
      "citation_id": "28",
      "title": "Residual-based graph convolutional network for emotion recognition in conversation for smart internet of things",
      "authors": [
        "Y.-J Choi",
        "Y.-W Lee",
        "B.-G Kim"
      ],
      "year": "2021",
      "venue": "Big Data"
    },
    {
      "citation_id": "29",
      "title": "Skier: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "W Li",
        "L Zhu",
        "R Mao",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "K Yang",
        "T Zhang",
        "H Alhuzali",
        "S Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "J Wang",
        "Y Liu",
        "L Rong",
        "Q Zheng",
        "D Song",
        "P Tiwari",
        "J Qin"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "32",
      "title": "Graphmft: A graph network based multimodal fusion technique for emotion recognition in conversation",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "33",
      "title": "Graph representation learning",
      "authors": [
        "W Hamilton"
      ],
      "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning"
    },
    {
      "citation_id": "34",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "J Zhou",
        "G Cui",
        "S Hu",
        "Z Zhang",
        "C Yang",
        "Z Liu",
        "L Wang",
        "C Li",
        "M Sun"
      ],
      "year": "2020",
      "venue": "AI Open"
    },
    {
      "citation_id": "35",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Z Wu",
        "S Pan",
        "F Chen",
        "G Long",
        "C Zhang",
        "S Philip"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on neural networks and learning systems"
    },
    {
      "citation_id": "36",
      "title": "Deep learning on graphs: A survey",
      "authors": [
        "Z Zhang",
        "P Cui",
        "W Zhu"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "37",
      "title": "Mirroring facial expressions and emotions in dyadic conversations",
      "authors": [
        "C Navarretta"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16"
    },
    {
      "citation_id": "38",
      "title": "On node features for graph neural networks",
      "authors": [
        "C Duong",
        "T Hoang",
        "H Dang",
        "Q Nguyen",
        "K Aberer"
      ],
      "year": "2019",
      "venue": "On node features for graph neural networks"
    },
    {
      "citation_id": "39",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "J Zhou",
        "G Cui",
        "S Hu",
        "Z Zhang",
        "C Yang",
        "Z Liu",
        "L Wang",
        "C Li",
        "M Sun"
      ],
      "year": "2021",
      "venue": "Graph neural networks: A review of methods and applications"
    },
    {
      "citation_id": "40",
      "title": "Deep learning on graphs: A survey",
      "authors": [
        "Z Zhang",
        "P Cui",
        "W Zhu"
      ],
      "year": "2020",
      "venue": "Deep learning on graphs: A survey"
    },
    {
      "citation_id": "41",
      "title": "The graph neural network model",
      "authors": [
        "F Scarselli",
        "M Gori",
        "A Tsoi",
        "M Hagenbuchner",
        "G Monfardini"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "42",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-supervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "43",
      "title": "Simple and deep graph convolutional networks",
      "authors": [
        "M Chen",
        "Z Wei",
        "Z Huang",
        "B Ding",
        "Y Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "45",
      "title": "How attentive are graph attention networks?",
      "authors": [
        "S Brody",
        "U Alon",
        "E Yahav"
      ],
      "year": "2021",
      "venue": "How attentive are graph attention networks?",
      "arxiv": "arXiv:2105.14491"
    },
    {
      "citation_id": "46",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "47",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "48",
      "title": "Emotionlines: An emotion corpus of multiparty conversations",
      "authors": [
        "C.-C Hsu",
        "S.-Y Chen",
        "C.-C Kuo",
        "T.-H Huang",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "49",
      "title": "Fast graph representation learning with PyTorch Geometric",
      "authors": [
        "M Fey",
        "J Lenssen"
      ],
      "year": "2019",
      "venue": "ICLR Workshop on Representation Learning on Graphs and Manifolds"
    },
    {
      "citation_id": "50",
      "title": "Timelms: Diachronic language models from twitter",
      "authors": [
        "D Loureiro",
        "F Barbieri",
        "L Neves",
        "L Anke",
        "J Camacho-Collados"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "51",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "52",
      "title": "Dynamic interactive multiview memory network for emotion recognition in conversation",
      "authors": [
        "J Wen",
        "D Jiang",
        "G Tu",
        "C Liu",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "53",
      "title": "Improved speech emotion recognition using transfer learning and spectrogram augmentation",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "R Sriram",
        "D Manocha"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction, ser. ICMI '21"
    },
    {
      "citation_id": "54",
      "title": "Multimodal emotion recognition using transfer learning from speaker recognition and bert-based models",
      "authors": [
        "S Padi",
        "S Sadjadi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2022",
      "venue": "The Speaker and Language Recognition Workshop"
    }
  ]
}