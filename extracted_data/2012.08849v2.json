{
  "paper_id": "2012.08849v2",
  "title": "Affective Visualization In Virtual Reality",
  "published": "2020-12-16T10:42:40Z",
  "authors": [
    "Andres Pinilla",
    "Jaime Garcia",
    "William Raffe",
    "Jan-Niklas Voigt-Antons",
    "Robert Spang",
    "Sebastian Möller"
  ],
  "keywords": [
    "virtual reality",
    "affect",
    "emotion",
    "electrophysiology",
    "visual design"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A cluster of research in Affective Computing suggests that it is possible to infer some characteristics of users' affective states by analyzing their electrophysiological activity in realtime. However, it is not clear how to use the information extracted from electrophysiological signals to create visual representations of the affective states of Virtual Reality (VR) users. Visualization of users' affective states in VR can lead to biofeedback therapies for mental health care. Understanding how to visualize affective states in VR requires an interdisciplinary approach that integrates psychology, electrophysiology, and audio-visual design. Therefore, this review aims to integrate previous studies from these fields to understand how to develop virtual environments that can automatically create visual representations of users' affective states. The manuscript addresses this challenge in four sections: First, theories related to emotion and affect are summarized. Second, evidence suggesting that visual and sound cues tend to be associated with affective states are discussed. Third, some of the available methods for assessing affect are described. The fourth and final section contains five practical considerations for the development of virtual reality environments for affect visualization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Theoretical Models Of Emotion And Affect",
      "text": "The terms emotion and affect are often used interchangeably in the literature, but they are not exactly the same. There is not a general agreement about how to define these concepts. In this manuscript, emotions are defined as mental states that coordinate the operation of cognitive processes. This definition is based on the assumption that the human mind is designed as a computational system that consists of a series of information-processing programs  (Putnam, 1967) . Emotions are a particular type of program that coordinate other programs' operation  (Cosmides & Tooby, 1994) . Affect is defined as the cognitive representation of the bodily changes that come with emotions  (Barrett & Bliss-Moreau, 2009; Wundt, 1897) . Neither emotion nor affect can be directly observed or measured. However, affect is conceptually associated with the physiological changes of the body. Therefore, it is reasonable to use electrophysiological signals to infer affective states, which might allow to infer some characteristics of emotional states.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Theories",
      "text": "The Ortony, Clore & Collins (OCC) theory of emotions  (Ortony et al., 1988)  has been widely used in the field of computer science to model users' emotional responses (e.g.,  Conati & Zhou, 2002; Jaques & Vicari, 2007) . This theory describes emotions in terms of twenty-two categories and assumes a clear distinction between each category. This approach is compatible with existing emotion recognition algorithms because these are usually based on categorizing emotions (e.g.,  Harischandra & Perera, 2012; Mavridou et al., 2017) . According to the OCC theory  (Ortony et al., 1988) , the first step in an emotional response is the perception of the situation. Then the situation is evaluated (appraisal), and finally, the emotional response emerges. However, this theory does not consider the physiological changes associated with emotions.\n\nSimilarly, Robert Plutchnik proposed a structural model of emotions  (Plutchik, 1982) , commonly known as Plutchnik's wheel of emotions. This model consists of eight primary states: ecstasy, adoration, terror, amazement, grief, loathing, rage, and vigilance. According to Pluthnik's theory, any emotion can be described as a combination of a subset of those basic states. Here emotions are defined as a sequence of reactions towards a stimulus. This sequence includes a cognitive evaluation of the stimuli (appraisal), feelings (subjective experience of the emotion), autonomic neural activity, and behavioral responses.\n\nThere are at least three other major emotion theories: the James-Lang Theory (Lange & James, 1922), the Cannon-Bard Theory  (Bard, 1934; Cannon, 1927) , and the Schachter-Singer Theory  (Schachter & Singer, 1962) . According to  Shiota & Kalat (2012) , these theories have in common the assumption that emotional responses have four components but differ in the order those components take place during an emotional response. The components are:\n\n• Appraisal: The cognitive, rationalized evaluation of the context where the emotional response is produced. • Feeling: The subjective, momentary experience of the emotion.\n\n• Physiological change: The bodily changes produced by the emotional response.\n\n• Behavior: The observable conduct that comes with the emotion.\n\nAccording to the James-Lang Theory  (Lange & James, 1922) , the first step in an emotional response is the cognitive evaluation of the situation. Then, physiological changes are produced in the body, at the same time that a behavioral response is produced. Lastly, feelings take place.\n\nThe Cannon-Bard theory  (Bard, 1934; Cannon, 1927)  proposes that all the elements of an emotional response are independent of each other, and there is no particular order in which they occur. This theory is not compatible with the convincing amount of evidence indicating that emotional stimuli tend to trigger automatic changes in the body (e.g.,  Dimberg et al., 2000; Huster et al., 2009; Thayer et al., 2009) . Overall, these previous studies suggest interdependence between physiological changes and the other components of emotion.\n\nAccording to the Schachter-Singer theory  (Schachter & Singer, 1962) , physiological changes occur first. Then the user tries to find an explanation in the environment for those physiological changes. Depending on the explanation found, a label is assigned to the bodily changes perceived. Therefore, the physiological changes indicate the intensity of the emotional experience, but cognitive factors determine the emotion's valence (pleasant vs. unpleasant).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Theoretical Models Of Affect",
      "text": "Theoretical models of affect can be classified into two major groups: discrete and dimensional models. Discrete models are based on a categorical division of affective responses, while dimensional models represent affect as an array of continuous variables. Both types of models are commonly used in Affective Computing to build affect recognition models (e.g.,  Hernandez et al., 2014; Leslie et al., 2015; Sitaram et al., 2011) .\n\nIn broad terms, discrete models propose the existence of a few primary states, such as happiness, sadness, and anger. Affective responses are a combination of a subset of those fundamental states. Evidence obtained by  Ekman and Friesen (1971)  during an experiment conducted in New Guinea supports discrete models. In this experiment, stories with emotional content were told to 153 participants. One hundred thirty of them (84.97%) had no previous contact with the western culture. After each story had been told, participants saw a series of pictures of facial expressions and were asked to choose the more coherent face with the story. Interestingly, participants associated similar facial expressions to the same stories, regardless of their cultural background. Based on this evidence, it was proposed that there are at least six facial expressions that are universal (i.e., they are not affected by culture): happiness, anger, sadness, disgust, surprise, and fear. These results are consistent with earlier contributions from Charles Darwin, who pointed out the existence of activation patterns in facial muscles which are associated with affective states  (Darwin, 1872; Ekman, 2006) .\n\nDimensional models have their roots in the early contributions of Wilhelm Wundt, who proposed that affective responses have three dimensions: valence (pleasant -unpleasant), arousal (arousing -subduing), and intensity (strain -relaxation)  (Wundt, 1897) . On this basis, the Circumplex Model of Affect  (Russell, 1980)  was developed, representing affect in a twodimensional space, where valence and arousal are equivalent to the x-axis and y-axis, respectively.\n\nOther authors have proposed the Evaluative Space Model (ESM)  (Cacioppo et al., 1997) , which has three dimensions: Negativity in the x-axis, Positivity in the y-axis, and Net predisposition (to withdraw or approach a stimulus) in the z-axis. Unlike the Circumplex Model of Affect  (Russell, 1980) , the ESM  (Cacioppo et al., 1997)  contemplates the existence of affective responses with simultaneous negative and positive activation (\"bitter-sweet\" affective states). For example, while playing a terror video game, the user might feel fear, and at the same time, might feel excited because there is not a real danger. An analysis about dimensional models of affect can be found in  Mattek et al. (2017) .\n\nThe ESM proposes the existence of the negativity bias and the positivity offset. The negativity bias implies that negative activation produces more changes in the motivation to withdraw or approach stimuli than positive activation. Evidence supporting the existence of the negativity bias indicates that negative stimuli tend to produce more salient behaviors than positive stimuli  (Sutherland & Mather, 2012) , and negative stimuli tend to be associated with higher arousal than positive stimuli  (Lang et al., 2008) . The negativity bias suggests that terror video games should trigger higher arousal than video games associated with positive affective states. However, a recent study indicates that the arousal level triggered by terror video games is slightly lower than the arousal triggered by video games associated with positive affective states  (Martínez-Tejada et al., 2021) .\n\nThe positivity offset implies a slight positive motivation to approach unknown stimuli in a neutral environment. This mechanism has been associated with humans' natural tendency to explore new, unthreatening environments, even when that behavior is not associated with a reward  (Cacioppo et al., 1997, p. 12) . Further research about the positivity offset could help understand how to motivate VR users to explore virtual environments. For example, to stimulate engagement of players with VR games.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Visual And Sound Cues",
      "text": "Building a virtual environment for affective visualization requires content that any user can associate with a wide range of affective states, regardless of cultural differences or personal preferences. Therefore, this section presents recent studies suggesting an association between some characteristics of graphical elements and affective states. We do not intend to define a set of rules about how to communicate affect with audio-visual elements. Instead, we aim to provide guidelines about visual and sound features when creating visual representations of affective states.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Visual Cues",
      "text": "Rounded objects are associated with higher valence and lower arousal than sharp objects  (Bar & Neta, 2006) . And rounded lines are perceived as more attractive than straight or angular lines  (Aronoff, 2006; Aronoff et al., 1992) . Given that attractiveness is associated with positive affective states, rounded lines are likely associated with positive valence. Additional studies suggest that visual complexity plays a role in the likability of objects. People tend to prefer extremely simple or extremely complex objects  (Norman et al., 2010) . Given that likability tends to trigger positive valence  (Ryali et al., 2020) , an intermediate level of complexity is more likely associated with negative valence.\n\nA cross-cultural study showed that the most critical factors in the affective meaning of color are brightness and saturation, while hue has a secondary role  (Gao et al., 2007) . These results are consistent with evidence reported in  Valdez & Mehrabian (1994)  but contrast with recent studies indicating that hue has a significant role in the affective state associated with a color palette  (Bartram et al., 2017) . Additional evidence suggests that blue, green, and purple are among the most pleasant hues, while yellow is among the most unpleasant. Green-yellow, bluegreen, and green are the most arousing, while purple-blue and yellow-red are among the least arousing  (Palmer & Schloss, 2010) . Similarly, it has been found that the most pleasant colors are those with higher saturation and brightness  (Camgöz et al., 2002; Wilms & Oberfeld, 2018) . However, other studies suggest that there are not universal associations between colors and affective states. People tend to like colors associated with objects they like and dislike colors associated with objects they dislike  (Palmer & Schloss, 2010) . Additional evidence indicates that color associations change according to the context where colors are used  (Lipson-Smith et al., 2020) , supporting the hypothesis that there are not universal associations between colors and affective states. Yet, it is possible to establish color palettes that allow to communicate affective states. For example, bright, unsaturated colors are more suitable to communicate calm, while dark, red colors are more suitable to communicate disturbance  (Bartram et al., 2017) .\n\nTextures may influence the affective meaning of color  (Ebe & Umemuro, 2015; Lucassen et al., 2011) . This has been demonstrated by pairing colors with computer-generated textures and asking participants to rate the color-texture pairs using four scales: Warm-Cool, Masculine-Feminine, Hard-Soft, and Heavy-Light. Results suggest that texture significantly influences the evaluation on the Hard-Soft scale and has a minor impact on the other scales. However, this evidence does not allow to identify associations between particular texture patterns and affective responses.\n\nNon-static visual elements have other visual properties besides color, shape, and texture. Some of these additional properties are speed, motion shape, direction, and path curvature. Fastmoving objects are associated with higher arousal than slow-moving objects  (Feng et al., 2014; Piwek et al., 2015) . But there are contradictory findings regarding the type of valence associated with speed. One study suggest that fast movements are related to positive affective states  (Piwek et al., 2015) , while other study indicates the opposite  (Feng et al., 2014) .\n\nLinear motion with straight paths is associated with low arousal and positive valence  (Feng et al., 2014) . Jerky paths are associated with higher arousal than straight paths in linear motion  (Feng et al., 2014; Lockyer et al., 2011) . But the curvature of the path has no incidence in affective associations when applied to spherical or radial motion  (Feng et al., 2014) . Inward movements are related to more positive affective states than outward movements  (Feng et al., 2014) . Downwards-right motion tends to be linked to positive states, while upwards-left motion tends to be associated with negative states  (Lockyer et al., 2011) . In general, angular paths are related to more negative affective states than linear paths  (Lockyer et al., 2011) . And spherical motion patterns tend to be associated with higher arousal than linear motion patterns.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Sound Cues",
      "text": "Previous research indicates that the location of a sound source influences the affective states associated with that sound. When the user cannot see where the object is (outside of the field of view), it is often associated with more arousing affective states than when the user can see it (inside the field of view)  (Drossos et al., 2015; Tajadura-Jiménez, Larsson, et al., 2010) . Similarly, sounds located further away in the space are related to less arousing responses  (Tajadura-Jiménez et al., 2008) . The perception of an approaching sound is associated with more arousing responses than the perception of it moving away (Tajadura-Jiménez,  Väljamäe, et al., 2010) . These phenomena are likely to be linked to mechanisms enforced by evolution  (Cosmides & Tooby, 1994) . Our primitive ancestors had more chances to survive if they were aware of the most potentially dangerous objects, such as those they could not see, were closer to them, or were approaching them.\n\nThe reverberation of the sound, which is associated with space's size, can influence affective associations  (Tajadura-Jiménez, Larsson, et al., 2010) . Lower reverberation (smaller rooms) is linked to more pleasant states than higher reverberation (larger rooms). Perhaps, because the primitive human being was better protected from predators in closed spaces, leading to an evolutionary process that favors the activation of attentional resources when we are in open areas.\n\nOther studies indicate that asking people to rate pictures with affective content while listening to the sound of a heartbeat can influence their affective evaluations, as well as their heart rate  (Tajadura-Jiménez et al., 2008) . Here, the sound of a heart rate faster than the listener's one tends to increase their heart rate, while a slower sound seems to relax the listener's heart rate. Therefore, playing a fast heartbeat in the background might be an effective way of representing an increase in arousal.\n\nOn the other hand, music is pivotal for affective visualization because it can contribute to create more immersive experiences. However, it is a vast topic that will not be fully covered in this manuscript. Yet, it is important to mention that tempo influences music's affective perception  (Fernández-Sotos et al., 2016) . Faster tempo tends to be associated with higher arousal ratings, while slower tempo tends to be associated with lower arousal ratings. To the extent of our knowledge, there is no evidence suggesting that tempo influences valence ratings.\n\nMajor and minor chords are associated with positive and negative affective states, respectively  (Gerardi & Gerken, 1995) . Similarly, dissonant harmonies tend to be strongly associated with anger, and to a lesser extent, with fear  (Petri, 2009) . And it is possible to compose music based on people's affective states  (Williams et al., 2017) . However, it remains an open question whether it is feasible to do it in real-time, based on the user's electrophysiological signals.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Personalized Affective Visualizations",
      "text": "There might be individual differences in the affective states that each user associates with the same audio-visual stimuli. These individual differences could be amplified as a consequence of personal experiences. An ideal system for affective visualization should account for those individual differences, delivering personalized visual representations of affective states, similar to Bermudez i Badia (2019).\n\nSemertzidis et al. (  2020 ) developed an Augmented Reality (AR) system that automatically creates visual representations of the user's affective states. The visualizations consisted of fractals generated using Procedural Content Generation (PCG). The visual properties of the fractals varied according to the affective state detected in the user. However, the evidence reported by  Semertzidis et al. (2020)  does not allow to establish whether participants perceived that the fractals' graphical properties represented their affective states.\n\nAdditional studies indicate that it is possible to use PCG to create content dynamically, adjusting it to the preferences of the user. This approach is known as experience-driven procedural content generation (EDPCG)  (Raffe et al., 2015; Yannakakis & Togelius, 2011) . In broad terms, EDPCG consists of an iterative process where the content is constantly modified based on the user's feedback.\n\nThe general functioning of EDPCG is the same as an evolutionary algorithm (EA), which is an optimization process inspired by natural evolution. In a natural environment, the organisms that are better adapted to their habitat tend to have more reproductive success, hence more likely to pass their genes to the next generation. Similarly, objects can be created programmatically in a virtual environment and tested to identify the most successful ones. The criteria to identify which objects are more successful is based on a previously defined goal. This goal is defined by the developer based on the purpose of the application. During each iteration, the objects that are more successful at reaching the goal are identified. In the following iterations, new sets of objects are created, and the characteristics of the most successful objects tend to remain, whereas the characteristics of the least successful tend to disappear. It is assumed that repeating this process several times allows to reach the optimal parameters required to achieve the goal. For example, if the goal is to create personalized visual representations of positive affective states, and the EA detects that the user tends to associate red, rounded objects with positive affective states, the game would produce objects that would tend to be more red and more angular. An introduction to EA can be found in Eiben and Smith  (Eiben & Smith, 2015) .\n\nAdditional research indicates that it is possible to create automatically visual compositions in VR using Deep Convolutional Neural Networks (DCNN)  (Kitson et al., 2019) . Overall, the process consists of merging features from two images to create a third image. This approach could be combined with EDPCG  (Raffe et al., 2015; Yannakakis & Togelius, 2011)  to create personalized affective visualizations. The process would involve at least three steps: (1) Create a set of VR content that all users will observe and used that content as a baseline. This initial set of content could be developed following the guidelines described in Table  1;   (2) Capture user feedback about the visual stimuli to establish the affective state that each user associates to each piece of VR content; And (3) use DCNNs to merge features of the initial content onto new, personalized VR content.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Assessment Of Affective States",
      "text": "Users' feedback should be captured using methods that do not interrupt the VR experience, such as body movements (see Section 4.2) or electrophysiological signals, similar to  Georgiou and Demiris (2017) . Methods for assessing affective states can be grouped into three categories: self-report questionnaires, behavioral measures, and electrophysiological signals. Each method has advantages and disadvantages that will be discussed below.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Self-Reports",
      "text": "Self-reports allow participants to evaluate their affective state by answering a series of questions. They can be used to verify the accuracy of the acquired information through other methods, such as behavioral and electrophysiological signals. Data collected through selfreports are often used as a ground-truth in the field of HCI.\n\nIn general, self-report measures are relatively easy to implement because they only require to display a series of questions on a paper sheet or a screen. Unlike behavioral and electrophysiological methods, self-reports are considered a direct measure because they allow asking participants directly about their mental states  (Perkis et al., 2020) . However, they are susceptible to be biased by rational processes. For example, participants who believe that it is expected from them to respond in a certain way might adjust their responses to fulfill that expectation, causing a phenomenon known as experimenter bias  (Fisher, 1993) . Some available tools for the assessment of affective responses are the Positive and Negative Affect Schedule (PANAS)  (Watson et al., 1988) , Self-Assessment Manikin (SAM)  (Bradley & Lang, 1994) , and Pick a Mood (PAM)  (Desmet et al., 2016) . The PANAS consists of 20 words related to negative and positive feelings (ten negatives and ten positives). Participants use those words to report their affective state. Each word can receive a rating from 1 to 5.\n\nThe SAM  (Bradley & Lang, 1994 ) is an instrument that uses three scales: valence (pleasant / unpleasant), arousal (tension / relaxation) and dominance (inhibition / uninhibition). Each scale has five pictograms. Participants can select the blank spaces between each pictogram to indicate intermediate states. Therefore, answers to each scale can take values between 1 and 9 (see Figure  1 ). Given that this instrument is based on dimensions, it is compatible with dimensional models of affect. The SAM  (Bradley & Lang, 1994 ) is one of the most established instruments for assessing affect (over 7.000 citations) and has been used for the development of batteries of stimuli with emotional content, such as the International Affective Pictures System (IAPS)  (Lang et al., 2008)  and the DEAP dataset  (Koelstra et al., 2012) .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Fig 1",
      "text": "From top to bottom: valence, arousal, and dominance scales of the Self-Assessment Manikin (SAM). Taken from  (Bradley & Lang, 1994)  On the other hand, the PAM  (Desmet et al., 2016)  is based on discrete states. Therefore, it is compatible with discrete models of affect. This instrument also uses pictorial cues to assess participant's states. There are eight mood types plus a neutral one: excited, cheerful, relaxed, calm, bored, sad, irritated, and tense. There are three characters for each of these states: a man, a woman, and a robot (gender-neutral character). In comparison to the SAM  (Bradley & Lang, 1994 ), PAM's characters  (Desmet et al., 2016)  are more similar to a real human being (see Figure  2 ), which might be an advantage because it could be easier for participants to feel identified with the characters of the PAM  (Desmet et al., 2016) . The PAM has been used to understand how to design objects and experiences that could stimulate mood regulation  (Desmet, 2015) , analyze the effect of immersive virtual environments on gaming Quality of Experience (QoE)  (Hupont et al., 2015) , and analyze whether the effect of color on affective states varies across different VR rooms  (Lipson-Smith et al., 2020) . Using scales to analyze experiences in virtual environments might require to interrupt the VR experience. This limitation can potentially be counterbalanced by using subjective rating scales inside the virtual environment (Voigt-  Antons et al., 2020) .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Behavioral Measures",
      "text": "Behavioral measures allow inferring affective states from observable conducts, such as body movements  (Bull, 1978; Robitaille & McGuffin, 2019) , voice patterns  (Cordaro et al., 2016; Scherer & Oshinsky, 1977) , and facial expressions  (Ekman & Friesen, 1971) . During an experiment conducted by  Bull (1978) , participants listened to a series of audio recordings with emotional content while their body movements were videotaped. Results suggested that sadness is associated with dropping the head while boredom is related to leaning the face in one hand. Building on that, recent research indicates that it is possible to infer arousal from body movements in virtual reality users  (Kapur et al., 2005; Robitaille & McGuffin, 2019) . In general, faster body movements are associated with higher arousal.\n\nIt is possible to automatically analyze users' affective states based on their voice patterns  (Vogt et al., 2008) . Usually, a set of features are defined and used to build a classification model. Some of the features used for automatic speech emotion recognition are pitch, loudness, and tempo  (Polzehl et al., 2011; Vogt et al., 2008) . This approach is coherent with evidence suggesting that changes in vocalization patterns have an effect on the affective evaluation of speech, e.g.  (Banse & Scherer, 1996; Scherer & Oshinsky, 1977) .\n\nEye-tracking has been an essential measure of various individual states or even personality traits  (Hoppe et al., 2018) .  Greinacher & Voigt-Antons (2020)  demonstrated recently how this measure could be easily obtained from modern smartphones using built-in system libraries  (Greinacher & Voigt-Antons, 2020) . The accuracy of this approach is comparable to other webcam or selfie-cam-based systems. However, as the authors pointed out, having eye-tracking systems easily accessible in millions of devices opens up opportunities for remote or in-thefield studies with a much higher ecological validity than studies relying on heavy equipment traditionally used in laboratory investigations.\n\nAs mentioned in section 3.1, facial expressions are associated with affective states  (Ekman & Friesen, 1971) . These expressions can be analyzed visually and described in terms of the Facial Action Coding System (FACS)  (Ekman & Friesen, 1976) . The FACS is an instrument that describes all the possible movements of the facial muscles. Each movement is defined as an Action Unit (AU). Facial expressions can be described as a combination of a subset of all the Action Units defined in the FACS  (Ekman & Friesen, 1976) . In a study conducted by  Porcu et al. (2020) , AUs were used for real-time analysis of the facial expressions of video streaming users. Additional studies suggest that human facial expressions can be collected using crowdsourcing techniques  (D. McDuff et al., 2012) , and its analysis can be optimized using statistical models that adapt automatically to the characteristics of the data  (Feffer et al., 2018) . However, facial recognition might be challenging to implement in VR because the Head-Mounted Display (HMD) covers the user's face. Therefore, facial electromyography (fEMG), a technique introduced in the following section, might be more suitable for capturing VR users' facial expressions.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Electrophysiology",
      "text": "Electrophysiological methods allow measuring changes in the electrical potentials of the body. Usually, facial electromyography (fEMG), electrocardiography (ECG), and electroencephalography (EEG) are used to record facial muscle, heart, and brain activity, respectively. This section focuses on methods to infer emotions in terms of the Circumplex Model of Affect  (Russell, 1980)  (see Section 2.2.). Therefore, the focus is on techniques that can be used to infer valence and arousal. There are many approaches for affect detection using electrophysiological signals that are not based on the Circumplex Model of Affect  (Russell, 1980)  and are not included in this manuscript.\n\nArousal can be inferred from features extracted from ECG signals. The beat-to-beat intervals of the ECG signal (often referred to as RR-Intervals, RRI) are extracted, detecting its peaks and calculating the time lapse between each peak. These RRIs are used to analyze the heart rate variability (HRV). Prominent examples of time-domain features used to analyze HRV are the root mean square of successive differences (RMSSD) and the standard deviation of NN intervals (SDNN). It has been found that higher HRV is associated with higher emotional arousal  (Thayer et al., 2009) . It is possible to extract features from the ECG signal in the frequency domain by calculating the LF/HF ratio. The low-frequency component (LF) (0.04 to 0.15 Hz) is associated with parasympathetic activity, while the high-frequency component (HF) (0.15 to 0.4 Hz) is associated with sympathetic activity  (Malik et al., 1996) . The activation of the parasympathetic system is associated with relaxation, and activation of the sympathetic system is associated with arousal. Therefore, more activity in the HF component indicates higher arousal  (Pagani et al., 1984) . Further research has shown that it is possible to infer arousal from EEG signals in VR users employing long short-term memory (LSTM) recurrent neural networks (RNN)  (Hofmann et al., 2018) .\n\nA recent study compared the benefits of implementing HRV biofeedback in virtual reality with a traditional HRV biofeedback therapy  (Blum et al., 2019) , suggesting that the VR implementation produces more benefits for users in terms of relaxation self-efficacy, reduced mind wandering, and control of attentional resources. A similar approach was proposed in  Blum et al. (2020) , introducing a breathing biofeedback algorithm. This algorithm combines features extracted from electrocardiography activity with data inferred from diaphragm movements. The experiment was conducted using a chest band (Polar H10), which is a reliable, relatively inexpensive sensor. Results suggest that this approach can help to foster more regular and slower breathing in VR users.\n\nValence can be inferred from EMG and EEG signals. Previous evidence suggests that the Corrugator Supercilii muscle activity (located above the eyebrows) is associated with negative affective states. In contrast, the Zygomaticus Major muscle activity (located in the cheeks) is related to positive affective states  (Dimberg, 1982) . Changes in facial muscle activity can occur without conscious awareness of the participant  (Dimberg et al., 2000; Dimberg & Thunberg, 2012) . However, it might be challenging to implement EMG in a VR system because the pressure of the Head-Mounted Display (HDM) on the electrodes can create artifacts on the recorded signal.\n\nAsymmetry in the cortical activity of the frontal cortex is also associated to valence . It has been found that positive and negative emotions are processed in the left and right frontal cortex, respectively  (Huster et al., 2009; Ray & Cole, 1985; Antons, 2015) . Additionally, it has been found that cortical activity decreases as the alpha power (frequencies between 8 and 13 Hz) increases  (Pfurtscheller & Lopes da Silva, 1999) . Therefore, increased processing of positive stimuli is associated with decreased alpha power in the left frontal cortex (higher activity in the left side of the brain). Similarly, increased processing of negative stimuli is associated with decreased alpha power in the right frontal cortex (higher activity in the right side of the brain)  (Davidson, 1992; Huster et al., 2009; Pfurtscheller & Lopes da Silva, 1999) .\n\nThese findings are coherent with results obtained by  Reuderink et al. (2013)  in a study where the brain activity of video game players was recorded using EEG. Participants were asked to report their affective state using the SAM  (Bradley & Lang, 1994 ) after the game session ended. Results indicated a positive correlation between self-reported valence and alpha asymmetry. Likewise,  Koelstra et al. (2012)  analyzed the brain activity of 32 participants who watched forty musical videos and rated their emotional reactions to each video using the SAM  (Bradley & Lang, 1994) . A positive correlation was found between self-reported valence and alpha power in the right occipital region of the brain.\n\nEye-movements and eye-blinks cause artifacts in the EEG signals and are usually reflected in the activity of the frontal region of the brain. In non-stationary VR applications, it is particularly challenging to remove artifacts caused by muscle activity, head movements, or electrical activity from the VR headset  (Klug & Gramann, 2020) . It is possible to remove these artifacts using Independent Component Analysis (ICA). This technique allows to identify the components of an EEG signal that are not produced by brain activity  (Makeig et al., 1997) . The maximum number of independent components (ICs) that can be identified using ICA depends on the number of electrodes used. For example, a recording with 32 electrodes will allow to identify up to 32 ICs. Therefore, increasing the number of electrodes might help identify the artifacts in the signal with more precision. For a complete analysis about using ICA in nonstationary and stationary settings, see  Klug and Gramman (2020) .\n\nAn additional challenge is to process the EEG signals in real-time. ICA can be used in real-time  (Pion-Tonachini et al., 2015) , but it was not designed for that purpose. An alternative is Artifact Subspace Reconstruction (ASR) (S.  Blum et al., 2019; Mullen et al., 2015) , a technique designed for online artifact removal. ASR uses data recorded from the user as a baseline. Then, principal component analysis (PCP) is applied to identify the EEG channels that contain artifacts. The data of the corrupted channels are reconstructed using the baseline data as a reference. There is software available that can facilitate the implementation of ASR, such as BCILAB  (Kothe & Makeig, 2013) , OpenBiVE  (Renard et al., 2010)  and Neuropype (Intheon Labs, California).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Brain-Computer Interfaces",
      "text": "The implementation of electrophysiological signals in VR systems leads to the development of interfaces that allow interpreting users' brain activity as computer commands  (Wolpaw et al., 2002) . One of the basic assumptions underlying the development of Brain-Computer Interfaces (BCIs) is that mental processes originate in the brain. But there are BCIs that measure electrophysiological responses in other places of the body (e.g.,  Cassani et al., 2018) , such as the heart and facial muscles, because cognitive processes that originate in the brain can produce changes in the activity of other body parts.\n\nThere are different techniques for measuring brain activity that can be used for the development of BCIs. For example, electrocorticography (ECoG), Positron Emission Tomography (PET), and functional Magnetic Resonance Imaging (fMRI), among others. However, electroencephalography (EEG) is the method most frequently used in BCIs because (1) it provides high temporal resolution (i.e., relatively large amount of data points recorded per second); (2) does not create health risks for the user because the electrodes can be easily placed and removed from the scalp; (3) can be portable, which is important for applications where the user is moving; and (4) is less expensive than most of the other methods  (Zander & Kothe, 2011) .\n\nAccording to  Zander & Kothe (2011) , there are three types of BCIs: active, passive, and reactive. Active BCIs require the active participation of the user to generate an action. For example, patients who lack motor control can use mental commands to move a wheelchair  (Voznenko et al., 2018) . Passive BCIs do not require the conscious involvement of the user. They can be used, for example, to analyze the cognitive load of car drivers automatically  (Almahasneh et al., 2014) . Reactive BCIs use mental activity that occurs as a response to external stimuli. An example is a neurofeedback video game where threatening stimuli are presented, and players have to control their anxiety to obtain game score  (Schoneveld et al., 2016) . A VR application for affective visualization, would usually involve either a passive or a reactive BCI.\n\nThe typical workflow in a BCI involves at least four steps  (Antons et al., 2014; Zander & Kothe, 2011) :\n\n1. Preprocessing pipeline: Filter out the signal's noise and keep only the components that reflect brain activity. This process involves (but is not limited to) filtering frequency bands and removing artifacts caused by eye-movements or muscle activity. An introduction to signal processing can be found in Unpingco (2014). 2. Feature extraction: Isolate the information related to the psychological construct of interest based on previous neuroscience studies (see Section 4.3). 3. Classifier definition: A classification model is created using prerecorded data. The classifier is tested offline, and an estimate of the accuracy of the classification is calculated. In general, classifiers are trained using data that has been previously labeled by humans. Machine Learning algorithms are used to identify patterns in the data that tend to be associated with each label.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Classification Application:",
      "text": "The classification is implemented in the BCI to perform online analysis of the brain activity. The outputs of the classification are used as computer commands.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Practical Considerations",
      "text": "This section contains five practical considerations that might help during the development of a VR system for affective visualization.\n\n1. Which are the initial steps for designing a virtual environment? First, define who will use the virtual environment (target group) and what the user will do inside that environment. This will help to have a more clear idea about the interaction events that will occur during the experience. Look for other interactive experiences, such as games and art installations, that can serve as inspiration. This will trigger ideas and will help to understand how to implement them. Then, define the graphical layout of your environment (color palette, typographies, and textures The usable information for each type of signal is located in a different frequency range. Therefore, the maximum frequency of interest for each signal is different. For example, the usable information in an ECG signal is up to 100 Hz. Therefore, the sampling frequency for ECG signals should be at least 200 Hz. However, previous studies indicate that ECG recordings at 200 Hz contain noise in the high-frequency components  (Malik et al., 1996) . This noise can be reduced by recording at a higher sampling rate.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Discussion",
      "text": "This manuscript aims to understand how to develop VR systems for affective visualization. These systems would involve the development of at least two components: a virtual environment and an affect detection technique. The development of both components requires the understanding of theories related to emotion and affect. Therefore, the manuscript analyses previous research related to (1) theories of emotion and affect, (2) audio-visual cues associated with affective states, and (3) methods for assessment of affective states.\n\nStudies discussed in Section 3 suggest that specific visual and sound cues can represent users' emotions. However, most of these studies were conducted in experimental settings where the stimuli were carefully controlled. It is unclear whether the same psychological responses would occur if a combination of these cues were used simultaneously. For example, a particular combination of \"happy\" colors may result in an unbalanced visual composition that produces negative affective states. Or there might be motion patterns that are more prone to produce motion sickness in VR users, triggering negative states. Moreover, the novelty of a VR system in new users bias the emotions they associate with the audio-visual stimuli.\n\nOther studies mentioned in Section 3 suggest that leftwards linear motion tends to be associated with negative valence  (Feng et al., 2014; Lockyer et al., 2011) . This finding was obtained during experiments conducted in a western society, where time is represented as a progression to the right  (Fuhrman & Boroditsky, 2010) . Therefore, it is likely that western users associate leftward motion with negative affective states because that type of motion is culturally associated with regressing in time. However, in other cultures, such as the Hebrew culture, people represent time as a progression to the left  (Fuhrman & Boroditsky, 2010) . Therefore, it is possible that Hebrew users would associate leftward linear motion with positive valence. This hypothesis can be tested in future experiments.\n\nRecent studies have demonstrated that affective states can be elicited by triggering psychogenic shivering (PS)  (Haar et al., 2020; Schoeller, Haar, et al., 2019) , using a device that controls the temperature in the upper back of the participants. Additional research indicates that the ability to be empathetic with others' emotions can be influenced by delivering electrical stimulation in the vagus nerve  (Colzato et al., 2017) , and by inducing affective states in the observer through videos  (Pinilla et al., 2020) . It remains an open question how to use those findings to develop Mixed Reality (MR) technologies for empathy enhancement, as proposed by  Schoeller, Bertrand, et al. (2019) .\n\nMost of the existing techniques for inferring affective states from electrophysiological signals require the usage of previously annotated data to train a classifier, e.g.  (Harischandra & Perera, 2012; Mavridou et al., 2017) . But the amount of distinct affective states that can be detected using this approach is limited. Therefore, it might be convenient to formulate affect detection problems in terms of statistical regression. This approach would allow creating a model capable of describing affective states in terms of a continuum containing an infinite amount of distinct affective states. Previous studies suggest that it is possible to infer arousal from EEG signals  (Hofmann et al., 2018)  as a continuous variable. Future studies could investigate whether it is possible to use a similar approach to express valence in terms of a continuous variable.\n\nFinally, it is possible to use a programmatic approach to create virtual reality content in realtime, using procedural content generation (PCG)  (Bermudez i Badia et al., 2019; Raffe et al., 2015; Semertzidis et al., 2020; Yannakakis & Togelius, 2011) . PCG allows to create content dynamically that adjusts to user feedback. Electrophysiological signals could be used to capture user feedback without interrupting the VR experience. This approach would allow to create personalized virtual environments for emotion visualization, similar to  Kitson et al. (2019)  or Bermudez i Badia (2019).",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Declarations",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Conflict Of Interest",
      "text": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Author Contributions",
      "text": "All authors contributed to the study conception and analysis. Jaime Garcia and William Raffe contributed with analysis of data related to gaming and Virtual Reality. Jan-Niklas Voigt-Antons contributed to the analysis of data related to psychology and electrophysiology. Robert Philipp Greinacher contributed with the redaction of the manuscript and with data related to electrocardiography and eye-tracking. Sebastian Möller contributed to the analysis of data related to sound design and Machine Learning. Andres Pinilla performed the literature search, data analysis and wrote the first draft. All authors commented on previous versions of the manuscript.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Funding",
      "text": "We acknowledge the support of the German Research Foundation and the Open Access Publication Fund of TU Berlin.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Acknowledgments",
      "text": "This work was supported by the strategic partnership between the Technische Universität Berlin, Germany and the University of Technology Sydney, Australia.\n\nWe appreciate the generosity of Nick Busietta in giving us access to the Psydocs of LiminalVR (liminalvr.com). That documentation was crucial for writing Section 2 of this manuscript.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Data Availability Statement",
      "text": "Not applicable.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Given that this instrument is based on dimensions, it is compatible with dimensional",
      "page": 9
    },
    {
      "caption": "Figure 1: From top to bottom: valence, arousal, and dominance scales of the Self-Assessment Manikin",
      "page": 10
    },
    {
      "caption": "Figure 2: ), which might be an advantage because it could be easier for participants to feel",
      "page": 10
    },
    {
      "caption": "Figure 2: Female character of Pick A Mood (PAM), taken from Desmet et al. (2016). Eight discrete states",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Affective visualization in Virtual Reality: An integrative review"
        },
        {
          "Affective visualization in Virtual Reality": "Andres Pinilla1, 2 *, Jaime Garcia2, William Raffe2, Jan-Niklas Voigt-Antons1, 3,"
        },
        {
          "Affective visualization in Virtual Reality": "Robert P. Spang1, Sebastian Möller1, 3"
        },
        {
          "Affective visualization in Virtual Reality": "1 Quality and Usability Lab, Institute for Software Technology and Theoretical Computer"
        },
        {
          "Affective visualization in Virtual Reality": "Science, Faculty of Electrical Engineering and Computer Science, Technische Universität"
        },
        {
          "Affective visualization in Virtual Reality": "Berlin, Germany"
        },
        {
          "Affective visualization in Virtual Reality": "2 UTS Games Studio, Faculty of Engineering and IT, University of Technology Sydney UTS,"
        },
        {
          "Affective visualization in Virtual Reality": "Sydney, Australia"
        },
        {
          "Affective visualization in Virtual Reality": "3 German Research Center for Artificial Intelligence (DFKI), Berlin, Germany"
        },
        {
          "Affective visualization in Virtual Reality": "* Correspondence:"
        },
        {
          "Affective visualization in Virtual Reality": "Andres Pinilla"
        },
        {
          "Affective visualization in Virtual Reality": "andres.pinilla@qu.tu-berlin.de"
        },
        {
          "Affective visualization in Virtual Reality": "Keywords: virtual reality, affect, emotion, electrophysiology, visual design"
        },
        {
          "Affective visualization in Virtual Reality": "Abstract"
        },
        {
          "Affective visualization in Virtual Reality": "A  cluster  of  research  in  Affective  Computing  suggests  that  it  is  possible  to  infer  some"
        },
        {
          "Affective visualization in Virtual Reality": "characteristics of users’ affective states by analyzing their electrophysiological activity in real-"
        },
        {
          "Affective visualization in Virtual Reality": "time. However, it is not clear how to use the information extracted from electrophysiological"
        },
        {
          "Affective visualization in Virtual Reality": "signals  to  create  visual  representations  of  the  affective  states  of  Virtual  Reality  (VR)  users."
        },
        {
          "Affective visualization in Virtual Reality": "Visualization of users’ affective states in VR can lead to biofeedback therapies for mental health"
        },
        {
          "Affective visualization in Virtual Reality": "care.  Understanding  how  to  visualize  affective  states  in  VR  requires  an  interdisciplinary"
        },
        {
          "Affective visualization in Virtual Reality": "approach  that  integrates  psychology,  electrophysiology,  and  audio-visual  design.  Therefore,"
        },
        {
          "Affective visualization in Virtual Reality": "this review aims to integrate previous studies from these fields to understand how to develop"
        },
        {
          "Affective visualization in Virtual Reality": "virtual  environments  that  can  automatically  create  visual  representations  of  users’  affective"
        },
        {
          "Affective visualization in Virtual Reality": "states.  The  manuscript  addresses  this  challenge  in  four  sections:  First,  theories  related  to"
        },
        {
          "Affective visualization in Virtual Reality": "emotion and affect are summarized. Second, evidence suggesting that visual and sound cues"
        },
        {
          "Affective visualization in Virtual Reality": "tend to be associated with affective states are discussed. Third, some of the available methods"
        },
        {
          "Affective visualization in Virtual Reality": "for  assessing  affect  are  described.  The  fourth  and  final  section  contains  five  practical"
        },
        {
          "Affective visualization in Virtual Reality": "considerations for the development of virtual reality environments for affect visualization."
        },
        {
          "Affective visualization in Virtual Reality": "1"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "feedback"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "portable  method"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "in  VR  would  require  at \nleast \ntwo"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "to \ninfer  some"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "(Plutchik,  1982),"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Appraisal:  The  cognitive,  rationalized  evaluation  of  the  context  where  the  emotional"
        },
        {
          "Affective visualization in Virtual Reality": "response is produced."
        },
        {
          "Affective visualization in Virtual Reality": "Feeling: The subjective, momentary experience of the emotion."
        },
        {
          "Affective visualization in Virtual Reality": "Physiological change: The bodily changes produced by the emotional response."
        },
        {
          "Affective visualization in Virtual Reality": "Behavior: The observable conduct that comes with the emotion."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "response is produced.": "• \nFeeling: The subjective, momentary experience of the emotion."
        },
        {
          "response is produced.": "• \nPhysiological change: The bodily changes produced by the emotional response."
        },
        {
          "response is produced.": "• \nBehavior: The observable conduct that comes with the emotion."
        },
        {
          "response is produced.": "According to the James-Lang Theory (Lange & James, 1922), the first step in an emotional"
        },
        {
          "response is produced.": "response is the cognitive evaluation of the situation. Then, physiological changes are produced"
        },
        {
          "response is produced.": "in the body, at the same time that a behavioral response is produced. Lastly, feelings take place."
        },
        {
          "response is produced.": "The  Cannon-Bard  theory  (Bard,  1934;  Cannon, 1927)  proposes  that  all  the  elements  of  an"
        },
        {
          "response is produced.": "emotional response are independent of each other, and there is no particular order in which they"
        },
        {
          "response is produced.": "occur. This theory is not compatible with the convincing amount of evidence indicating that"
        },
        {
          "response is produced.": "emotional  stimuli  tend  to  trigger  automatic  changes  in  the  body  (e.g.,  Dimberg  et  al.,  2000;"
        },
        {
          "response is produced.": "Huster et al., 2009; Thayer et al., 2009). Overall, these previous studies suggest interdependence"
        },
        {
          "response is produced.": "between physiological changes and the other components of emotion."
        },
        {
          "response is produced.": "According to the Schachter-Singer theory (Schachter & Singer, 1962), physiological changes"
        },
        {
          "response is produced.": "occur first. Then the user tries to find an explanation in the environment for those physiological"
        },
        {
          "response is produced.": "changes.  Depending  on  the  explanation  found,  a  label  is  assigned  to  the  bodily  changes"
        },
        {
          "response is produced.": "perceived.  Therefore, \nthe  physiological  changes \nindicate \nthe \nintensity  of \nthe  emotional"
        },
        {
          "response is produced.": "experience, but cognitive factors determine the emotion’s valence (pleasant vs. unpleasant)."
        },
        {
          "response is produced.": "1.2 \nTheoretical models of affect"
        },
        {
          "response is produced.": "Theoretical models of affect can be classified into two major groups: discrete and dimensional"
        },
        {
          "response is produced.": "models.  Discrete  models  are  based  on  a  categorical  division  of  affective  responses,  while"
        },
        {
          "response is produced.": "dimensional models represent affect as an array of continuous variables. Both types of models"
        },
        {
          "response is produced.": "are commonly used in Affective Computing to build affect recognition models (e.g., Hernandez"
        },
        {
          "response is produced.": "et al., 2014; Leslie et al., 2015; Sitaram et al., 2011)."
        },
        {
          "response is produced.": "In broad terms, discrete models propose the existence of a few primary states, such as happiness,"
        },
        {
          "response is produced.": "sadness,  and  anger.  Affective  responses  are  a  combination  of  a  subset  of  those fundamental"
        },
        {
          "response is produced.": "states.  Evidence  obtained  by  Ekman  and  Friesen  (1971)  during  an  experiment  conducted  in"
        },
        {
          "response is produced.": "New Guinea supports discrete models. In this experiment, stories with emotional content were"
        },
        {
          "response is produced.": "told to 153 participants. One hundred thirty of them (84.97%) had no previous contact with the"
        },
        {
          "response is produced.": "western culture. After each story had been told, participants saw a series of pictures of facial"
        },
        {
          "response is produced.": "expressions  and  were  asked  to  choose  the  more  coherent  face  with  the  story.  Interestingly,"
        },
        {
          "response is produced.": "participants associated similar facial expressions to the same stories, regardless of their cultural"
        },
        {
          "response is produced.": "background. Based on this evidence, it was proposed that there are at least six facial expressions"
        },
        {
          "response is produced.": "that  are  universal  (i.e.,  they  are  not  affected  by  culture):  happiness,  anger,  sadness,  disgust,"
        },
        {
          "response is produced.": "surprise, and fear. These results are consistent with earlier contributions from Charles Darwin,"
        },
        {
          "response is produced.": "who pointed out the existence of activation patterns in facial muscles which are associated with"
        },
        {
          "response is produced.": "affective states (Darwin, 1872; Ekman, 2006)."
        },
        {
          "response is produced.": "Dimensional  models  have  their  roots  in  the  early  contributions  of  Wilhelm  Wundt,  who"
        },
        {
          "response is produced.": "proposed  that  affective  responses  have  three  dimensions:  valence  (pleasant  –  unpleasant),"
        },
        {
          "response is produced.": "arousal (arousing – subduing), and intensity (strain – relaxation) (Wundt, 1897). On this basis,"
        },
        {
          "response is produced.": "the Circumplex Model of Affect (Russell, 1980) was developed, representing affect in a two-"
        },
        {
          "response is produced.": "dimensional  space,  where  valence  and  arousal  are  equivalent \nto \nthe  x-axis  and  y-axis,"
        },
        {
          "response is produced.": "respectively."
        },
        {
          "response is produced.": "Other authors have proposed the Evaluative Space Model (ESM) (Cacioppo et al., 1997), which"
        },
        {
          "response is produced.": "has three dimensions: Negativity in the x-axis, Positivity in the y-axis, and Net predisposition"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "(to  withdraw  or  approach  a  stimulus)  in  the  z-axis.  Unlike  the  Circumplex  Model  of  Affect"
        },
        {
          "Affective visualization in Virtual Reality": "(Russell,  1980),  the  ESM  (Cacioppo  et  al.,  1997)  contemplates  the  existence  of  affective"
        },
        {
          "Affective visualization in Virtual Reality": "responses with simultaneous negative and positive activation (“bitter-sweet” affective states)."
        },
        {
          "Affective visualization in Virtual Reality": "For example, while playing a terror video game, the user might feel fear, and at the same time,"
        },
        {
          "Affective visualization in Virtual Reality": "might feel excited because there is not a real danger. An analysis about dimensional models of"
        },
        {
          "Affective visualization in Virtual Reality": "affect can be found in Mattek et al. (2017)."
        },
        {
          "Affective visualization in Virtual Reality": "The ESM proposes the existence of the negativity bias and the positivity offset. The negativity"
        },
        {
          "Affective visualization in Virtual Reality": "bias implies that negative activation produces more changes in the motivation to withdraw or"
        },
        {
          "Affective visualization in Virtual Reality": "approach stimuli than positive activation. Evidence supporting the existence of the negativity"
        },
        {
          "Affective visualization in Virtual Reality": "bias indicates that negative stimuli tend to produce more salient behaviors than positive stimuli"
        },
        {
          "Affective visualization in Virtual Reality": "(Sutherland & Mather, 2012), and negative stimuli tend to be associated with higher arousal"
        },
        {
          "Affective visualization in Virtual Reality": "than positive stimuli (Lang et al., 2008). The negativity bias suggests that terror video games"
        },
        {
          "Affective visualization in Virtual Reality": "should  trigger  higher  arousal  than  video  games  associated  with  positive  affective  states."
        },
        {
          "Affective visualization in Virtual Reality": "However,  a  recent  study  indicates  that  the  arousal  level  triggered  by  terror  video  games  is"
        },
        {
          "Affective visualization in Virtual Reality": "slightly lower than the arousal triggered by video games associated with positive affective states"
        },
        {
          "Affective visualization in Virtual Reality": "(Martínez-Tejada et al., 2021)."
        },
        {
          "Affective visualization in Virtual Reality": "The  positivity  offset  implies  a  slight  positive  motivation  to  approach  unknown  stimuli  in  a"
        },
        {
          "Affective visualization in Virtual Reality": "neutral  environment.  This  mechanism  has  been  associated  with  humans'  natural  tendency  to"
        },
        {
          "Affective visualization in Virtual Reality": "explore  new,  unthreatening  environments,  even  when  that  behavior  is  not  associated  with  a"
        },
        {
          "Affective visualization in Virtual Reality": "reward (Cacioppo et al., 1997, p. 12). Further research about the positivity offset could help"
        },
        {
          "Affective visualization in Virtual Reality": "understand  how  to  motivate  VR  users  to  explore  virtual  environments.  For  example,  to"
        },
        {
          "Affective visualization in Virtual Reality": "stimulate engagement of players with VR games."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Summary of audio-visual cues associated with affective states, according to previous",
      "data": [
        {
          "Affective visualization in Virtual Reality": "evolutionary  process  that  favors  the  activation  of attentional  resources  when  we  are  in  open"
        },
        {
          "Affective visualization in Virtual Reality": "areas."
        },
        {
          "Affective visualization in Virtual Reality": "Other studies indicate that asking people to rate pictures with affective content while listening"
        },
        {
          "Affective visualization in Virtual Reality": "to the sound of a heartbeat can influence their affective evaluations, as well as their heart rate"
        },
        {
          "Affective visualization in Virtual Reality": "(Tajadura-Jiménez et al., 2008). Here, the sound of a heart rate faster than the listener’s one"
        },
        {
          "Affective visualization in Virtual Reality": "tends to increase their heart rate, while a slower sound seems to relax the listener’s heart rate."
        },
        {
          "Affective visualization in Virtual Reality": "Therefore, playing a fast heartbeat in the background might be an effective way of representing"
        },
        {
          "Affective visualization in Virtual Reality": "an increase in arousal."
        },
        {
          "Affective visualization in Virtual Reality": "On the other hand, music is pivotal for affective visualization because it can contribute to create"
        },
        {
          "Affective visualization in Virtual Reality": "more immersive experiences. However, it is a vast topic that will not be fully covered in this"
        },
        {
          "Affective visualization in Virtual Reality": "manuscript. Yet, it is important to mention that tempo influences music’s affective perception"
        },
        {
          "Affective visualization in Virtual Reality": "(Fernández-Sotos et al., 2016). Faster tempo tends to be associated with higher arousal ratings,"
        },
        {
          "Affective visualization in Virtual Reality": "while  slower  tempo  tends  to  be  associated  with  lower  arousal  ratings.  To  the  extent  of  our"
        },
        {
          "Affective visualization in Virtual Reality": "knowledge, there is no evidence suggesting that tempo influences valence ratings."
        },
        {
          "Affective visualization in Virtual Reality": "Major and minor chords are associated with positive and negative affective states, respectively"
        },
        {
          "Affective visualization in Virtual Reality": "(Gerardi & Gerken, 1995). Similarly, dissonant harmonies tend to be strongly associated with"
        },
        {
          "Affective visualization in Virtual Reality": "anger, and to a lesser extent, with fear (Petri, 2009). And it is possible to compose music based"
        },
        {
          "Affective visualization in Virtual Reality": "on  people’s  affective  states  (Williams  et  al.,  2017).  However,  it  remains  an  open  question"
        },
        {
          "Affective visualization in Virtual Reality": "whether it is feasible to do it in real-time, based on the user’s electrophysiological signals."
        },
        {
          "Affective visualization in Virtual Reality": "Table 1. Summary of audio-visual cues associated with affective states, according to previous"
        },
        {
          "Affective visualization in Virtual Reality": "studies."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Summary of audio-visual cues associated with affective states, according to previous",
      "data": [
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": "studies."
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": "Static"
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": "visual cues"
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": "Non-static"
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": "visual cues"
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": "Sound cues"
        },
        {
          "Table 1. Summary of audio-visual cues associated with affective states, according to previous": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: ; (2) Capture",
      "data": [
        {
          "Affective visualization in Virtual Reality": "N/A"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "N/A"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "N/A"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "N/A"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "Major scale"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: ; (2) Capture",
      "data": [
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "Harmony",
          "Fast": "N/A",
          "Slow": "N/A",
          "N/A": "Minor scale"
        },
        {
          "Tempo": "Personalized affective visualizations",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        },
        {
          "Tempo": "",
          "Fast": "",
          "Slow": "",
          "N/A": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "screen.  Unlike  behavioral  and"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "9"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Fig 1 From top to bottom: valence, arousal, and dominance scales of the Self-Assessment Manikin"
        },
        {
          "Affective visualization in Virtual Reality": "(SAM). Taken from (Bradley & Lang, 1994)"
        },
        {
          "Affective visualization in Virtual Reality": "On the other hand, the PAM (Desmet et al., 2016) is based on discrete states. Therefore, it is"
        },
        {
          "Affective visualization in Virtual Reality": "compatible  with  discrete  models  of  affect.  This  instrument  also  uses  pictorial  cues  to  assess"
        },
        {
          "Affective visualization in Virtual Reality": "participant’s states. There are eight mood types plus a neutral one: excited, cheerful, relaxed,"
        },
        {
          "Affective visualization in Virtual Reality": "calm, bored, sad, irritated, and tense. There are three characters for each of these states: a man,"
        },
        {
          "Affective visualization in Virtual Reality": "a woman, and a robot (gender-neutral character). In comparison to the SAM (Bradley & Lang,"
        },
        {
          "Affective visualization in Virtual Reality": "1994), PAM’s  characters  (Desmet  et  al.,  2016)  are  more  similar  to  a  real  human  being  (see"
        },
        {
          "Affective visualization in Virtual Reality": "Figure  2),  which  might  be  an  advantage  because  it  could  be  easier  for  participants  to  feel"
        },
        {
          "Affective visualization in Virtual Reality": "identified with the characters of the PAM (Desmet et al., 2016)."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "that  human  facial  expressions  can  be  collected  using"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "(fEMG), \nelectrocardiography \n(ECG), \nand"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "(Blum  et  al.,  2019), \nsuggesting \nthat \nthe  VR"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "(ICA).  This \ntechnique  allows \nto \nidentify \nthe"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "(fMRI),"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "reactive BCI.": "The typical workflow in a BCI involves at least four steps (Antons et al., 2014; Zander & Kothe,"
        },
        {
          "reactive BCI.": "2011):"
        },
        {
          "reactive BCI.": "1.  Preprocessing pipeline: Filter out the signal's noise and keep only the components that"
        },
        {
          "reactive BCI.": "reflect brain activity. This process involves (but is not limited to) filtering frequency"
        },
        {
          "reactive BCI.": "bands  and  removing  artifacts  caused  by  eye-movements  or  muscle  activity.  An"
        },
        {
          "reactive BCI.": "introduction to signal processing can be found in Unpingco (2014)."
        },
        {
          "reactive BCI.": "2.  Feature extraction: Isolate the information related to the psychological construct of"
        },
        {
          "reactive BCI.": "interest based on previous neuroscience studies (see Section 4.3)."
        },
        {
          "reactive BCI.": "3.  Classifier  definition:  A  classification  model  is  created  using  prerecorded  data.  The"
        },
        {
          "reactive BCI.": "classifier  is  tested  offline,  and  an  estimate  of  the  accuracy  of  the  classification  is"
        },
        {
          "reactive BCI.": "calculated. In general, classifiers are trained using data that has been previously labeled"
        },
        {
          "reactive BCI.": "by humans. Machine Learning algorithms are used to identify patterns in the data that"
        },
        {
          "reactive BCI.": "tend to be associated with each label."
        },
        {
          "reactive BCI.": "4.  Classification  application:  The  classification  is  implemented  in  the  BCI  to  perform"
        },
        {
          "reactive BCI.": "online  analysis  of  the  brain  activity.  The  outputs  of  the  classification  are  used  as"
        },
        {
          "reactive BCI.": "computer commands."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Practical considerations": ""
        },
        {
          "Practical considerations": ""
        },
        {
          "Practical considerations": "1.  Which are the initial steps for designing a virtual environment? First, define who"
        },
        {
          "Practical considerations": "will  use  the  virtual  environment  (target  group)  and  what  the  user  will  do  inside  that"
        },
        {
          "Practical considerations": "environment. This will help to have a more clear idea about the interaction events that"
        },
        {
          "Practical considerations": "will occur during the experience. Look for other interactive experiences, such as games"
        },
        {
          "Practical considerations": "and art installations, that can serve as inspiration. This will trigger ideas and will help"
        },
        {
          "Practical considerations": "to  understand  how  to  implement  them.  Then,  define  the  graphical  layout  of  your"
        },
        {
          "Practical considerations": "environment (color palette, typographies, and textures)."
        },
        {
          "Practical considerations": "2.  Which software should I use for VR development? Unity is probably one of the best"
        },
        {
          "Practical considerations": "options. There are alternatives, such as Vizard, a virtual reality software for research."
        },
        {
          "Practical considerations": "However, to the extent of our knowledge, Unity is the only game engine compatible"
        },
        {
          "Practical considerations": "with open-source solutions, such as LSL and Excite-O-Meter. Therefore, it is relatively"
        },
        {
          "Practical considerations": "easy  and \ninexpensive \nto  develop  virtual  environments \nthat \nrely  on \nthe  user's"
        },
        {
          "Practical considerations": "physiological data using Unity."
        },
        {
          "Practical considerations": "3.  How to integrate Unity with electrophysiological equipment? One possibility is to"
        },
        {
          "Practical considerations": "use  LabStreamingLayer  (LSL),  a  tool for  collecting  time-series  data  in  experimental"
        },
        {
          "Practical considerations": "settings. Essentially, LSL allows to collect and synchronize the data and stream it into"
        },
        {
          "Practical considerations": "Unity. At the same time, it allows to send data from Unity (e.g., markers) to the signal"
        },
        {
          "Practical considerations": "processing software. Another option is to setup a UDP Broadcast to send information"
        },
        {
          "Practical considerations": "through your network."
        },
        {
          "Practical considerations": "Is  there  a  ready-to-use  solution  for  integrating  Unity  with  electrophysiological"
        },
        {
          "Practical considerations": "equipment?  Yes.  Excite-O-Meter  is  a  Unity  plugin  for  visualizing  cardiovascular"
        },
        {
          "Practical considerations": "activity, which is built on top of LSL. It can be used to visualize Heart Rate Variability"
        },
        {
          "Practical considerations": "(HRV) (see Section 4.3). By default, the Excite-O-Meter provides a time-series graph"
        },
        {
          "Practical considerations": "of the data. But you can customize it to build other types of visualizations."
        },
        {
          "Practical considerations": "5.  How  to  define  the  sampling  rate  for  recording  electrophysiological  signals?"
        },
        {
          "Practical considerations": "According the Nyquist-Shannon sampling theorem, the sampling rate should be twice"
        },
        {
          "Practical considerations": "the maximum frequency of interest. For example, if you are interested in frequencies of"
        },
        {
          "Practical considerations": "up to 128 Hz, you should use a sampling rate of at least 256 Hz. A sampling rate of 256"
        },
        {
          "Practical considerations": "Hz means that you are collecting 256 data points per second."
        },
        {
          "Practical considerations": "T\nhe usable information for each type of signal is located in a different frequency range."
        },
        {
          "Practical considerations": "Therefore, the maximum frequency of interest for each signal is different. For example,"
        },
        {
          "Practical considerations": "the  usable  information  in  an  ECG  signal  is  up  to  100  Hz.  Therefore,  the  sampling"
        },
        {
          "Practical considerations": "frequency for ECG signals should be at least 200 Hz. However, previous studies indicate"
        },
        {
          "Practical considerations": "that ECG recordings at 200 Hz contain noise in the high-frequency components (Malik"
        },
        {
          "Practical considerations": "et  al.,  1996).  This  noise  can  be  reduced  by  recording  at  a  higher  sampling  rate."
        },
        {
          "Practical considerations": "Therefore,  it  is  considered  a  good  practice  to record  ECG  signals  at  a  sampling  rate"
        },
        {
          "Practical considerations": "between 256 Hz and 512 Hz, EMG signals at a sampling rate between 512 Hz and 1024,"
        },
        {
          "Practical considerations": "and EEG signals at a sampling rate between 256 Hz and 512 Hz."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": "technologies  for  empathy  enhancement,  as  proposed  by  Schoeller,"
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        },
        {
          "Affective visualization in Virtual Reality": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "Antons, J.-N., Arndt, S., Schleicher, R., & Möller, S. (2014). Brain Activity Correlates of",
          "References": ""
        },
        {
          "7": "",
          "References": "Quality of Experience. In S. Möller & A. Raake (Eds.), Quality of Experience (pp."
        },
        {
          "7": "",
          "References": "109–119). Springer International Publishing. https://doi.org/10.1007/978-3-319-"
        },
        {
          "7": "",
          "References": "02681-7_8"
        },
        {
          "7": "Arndt, S., Perkis, A., & Voigt-Antons, J.-N. (2018). Using Virtual Reality and Head-Mounted",
          "References": ""
        },
        {
          "7": "",
          "References": "Displays to Increase Performance in Rowing Workouts. Proceedings of the 1st"
        },
        {
          "7": "",
          "References": "International Workshop on Multimedia Content Analysis in Sports  - MMSports’18,"
        },
        {
          "7": "",
          "References": "45–50. https://doi.org/10.1145/3265845.3265848"
        },
        {
          "7": "Aronoff, J. (2006). How We Recognize Angry and Happy Emotion in People, Places, and",
          "References": ""
        },
        {
          "7": "",
          "References": "Things. Cross-Cultural Research, 40(1), 83–105."
        },
        {
          "7": "",
          "References": "https://doi.org/10.1177/1069397105282597"
        },
        {
          "7": "Aronoff, J., Woike, B. A., & Hyman, L. M. (1992). Which are the stimuli in facial displays of",
          "References": ""
        },
        {
          "7": "",
          "References": "anger and happiness? Configurational bases of emotion recognition. Journal of"
        },
        {
          "7": "",
          "References": "Personality and Social Psychology, 62(6), 1050–1066. https://doi.org/10.1037/0022-"
        },
        {
          "7": "",
          "References": "3514.62.6.1050"
        },
        {
          "7": "Banse, R., & Scherer, K. R. (1996). Acoustic profiles in vocal emotion expression. Journal of",
          "References": ""
        },
        {
          "7": "",
          "References": "Personality and Social Psychology, 70(3), 614–636. https://doi.org/10.1037/0022-"
        },
        {
          "7": "",
          "References": "3514.70.3.614"
        },
        {
          "7": "Bar, M., & Neta, M. (2006). Humans Prefer Curved Visual Objects. Psychological Science,",
          "References": ""
        },
        {
          "7": "",
          "References": "17(8), 645–648. https://doi.org/10.1111/j.1467-9280.2006.01759.x"
        },
        {
          "7": "Bard, P. (1934). On emotional expression after decortication with some remarks on certain",
          "References": ""
        },
        {
          "7": "",
          "References": "theoretical views: Part I. Psychological Review, 41(4), 309–329."
        },
        {
          "7": "",
          "References": "https://doi.org/10.1037/h0070765"
        },
        {
          "7": "Barrett, L. F., & Bliss-Moreau, E. (2009). Chapter 4 Affect as a Psychological Primitive. In",
          "References": ""
        },
        {
          "7": "",
          "References": "Advances in Experimental Social Psychology (Vol. 41, pp. 167–218). Elsevier."
        },
        {
          "7": "",
          "References": "https://doi.org/10.1016/S0065-2601(08)00404-8"
        },
        {
          "7": "Bartram, L., Patra, A., & Stone, M. (2017). Affective Color in Visualization. Proceedings of",
          "References": ""
        },
        {
          "7": "",
          "References": "the 2017 CHI Conference on Human Factors in Computing Systems, 1364–1374."
        },
        {
          "7": "",
          "References": "https://doi.org/10.1145/3025453.3026041"
        },
        {
          "7": "Belger, J., Thone-Otto, A., Krohn, S., Finke, C., Tromp, J., Klotzsche, F., Villringer, A.,",
          "References": ""
        },
        {
          "7": "",
          "References": "Gaebler, M., Chojecki, P., & Quinque, E. (2019). Immersive Virtual Reality for the"
        },
        {
          "7": "",
          "References": "Assessment and Training of Spatial Memory: Feasibility in Individuals with Brain"
        },
        {
          "7": "",
          "References": "Injury. 2019 International Conference on Virtual Rehabilitation (ICVR), 1–2."
        },
        {
          "7": "",
          "References": "https://doi.org/10.1109/ICVR46560.2019.8994342"
        },
        {
          "7": "Bermudez i Badia, S., Quintero, L. V., Cameirao, M. S., Chirico, A., Triberti, S., Cipresso, P.,",
          "References": ""
        },
        {
          "7": "",
          "References": "& Gaggioli, A. (2019). Toward Emotionally Adaptive Virtual Reality for Mental"
        },
        {
          "7": "",
          "References": "Health Applications. IEEE Journal of Biomedical and Health Informatics, 23(5),"
        },
        {
          "7": "",
          "References": "1877–1887. https://doi.org/10.1109/JBHI.2018.2878846"
        },
        {
          "7": "Blandón, D. Z., Muñoz, J. E., Lopez, D. S., & Gallo, O. H. (2016). Influence of a BCI",
          "References": ""
        },
        {
          "7": "",
          "References": "neurofeedback videogame in children with ADHD. Quantifying the brain activity"
        },
        {
          "7": "",
          "References": "through an EEG signal processing dedicated toolbox. 2016 IEEE 11th Colombian"
        },
        {
          "7": "",
          "References": "Computing Conference (CCC), 1–8."
        },
        {
          "7": "",
          "References": "https://doi.org/10.1109/ColumbianCC.2016.7750788"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Blum, J., Rockstroh, C., & Göritz, A. S. (2019). Heart Rate Variability Biofeedback Based on"
        },
        {
          "Affective visualization in Virtual Reality": "Slow-Paced Breathing With Immersive Virtual Reality Nature Scenery. Frontiers in"
        },
        {
          "Affective visualization in Virtual Reality": "Psychology, 10, 2172. https://doi.org/10.3389/fpsyg.2019.02172"
        },
        {
          "Affective visualization in Virtual Reality": "Blum, J., Rockstroh, C., & Göritz, A. S. (2020). Development and Pilot Test of a Virtual"
        },
        {
          "Affective visualization in Virtual Reality": "Reality Respiratory Biofeedback Approach. Applied Psychophysiology and"
        },
        {
          "Affective visualization in Virtual Reality": "Biofeedback, 45(3), 153–163. https://doi.org/10.1007/s10484-020-09468-x"
        },
        {
          "Affective visualization in Virtual Reality": "Blum, S., Jacobsen, N. S. J., Bleichner, M. G., & Debener, S. (2019). A Riemannian"
        },
        {
          "Affective visualization in Virtual Reality": "Modification of Artifact Subspace Reconstruction for EEG Artifact Handling."
        },
        {
          "Affective visualization in Virtual Reality": "Frontiers in Human Neuroscience, 13. https://doi.org/10.3389/fnhum.2019.00141"
        },
        {
          "Affective visualization in Virtual Reality": "Bradley, M. M., & Lang, P. J. (1994). Measuring emotion: The self-assessment manikin and"
        },
        {
          "Affective visualization in Virtual Reality": "the semantic differential. Journal of Behavior Therapy and Experimental Psychiatry,"
        },
        {
          "Affective visualization in Virtual Reality": "25(1), 49–59. https://doi.org/10.1016/0005-7916(94)90063-9"
        },
        {
          "Affective visualization in Virtual Reality": "Bull, P. (1978). The interpretation of posture through an alternative methodology to role play."
        },
        {
          "Affective visualization in Virtual Reality": "British Journal of Social and Clinical Psychology, 17(1), 1–6."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1111/j.2044-8260.1978.tb00888.x"
        },
        {
          "Affective visualization in Virtual Reality": "Burdea, G. C., Cioi, D., Kale, A., Janes, W. E., Ross, S. A., & Engsberg, J. R. (2013)."
        },
        {
          "Affective visualization in Virtual Reality": "Robotics and Gaming to Improve Ankle Strength, Motor Control, and Function in"
        },
        {
          "Affective visualization in Virtual Reality": "Children With Cerebral Palsy—A Case Study Series. IEEE Transactions on Neural"
        },
        {
          "Affective visualization in Virtual Reality": "Systems and Rehabilitation Engineering, 21(2), 165–173."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/TNSRE.2012.2206055"
        },
        {
          "Affective visualization in Virtual Reality": "Cacioppo, J. T., Gardner, W. L., & Berntson, G. G. (1997). Beyond Bipolar"
        },
        {
          "Affective visualization in Virtual Reality": "Conceptualizations and Measures: The Case of Attitudes and Evaluative Space."
        },
        {
          "Affective visualization in Virtual Reality": "Personality and Social Psychology Review, 1(1), 3–25."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1207/s15327957pspr0101_2"
        },
        {
          "Affective visualization in Virtual Reality": "Camgöz, N., Yener, C., & Güvenç, D. (2002). Effects of hue, saturation, and brightness on"
        },
        {
          "Affective visualization in Virtual Reality": "preference. Color Research & Application, 27(3), 199–207."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1002/col.10051"
        },
        {
          "Affective visualization in Virtual Reality": "Cannon, W. B. (1927). The James-Lange Theory of Emotions: A Critical Examination and an"
        },
        {
          "Affective visualization in Virtual Reality": "Alternative Theory. The American Journal of Psychology, 39(1/4), 106."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.2307/1415404"
        },
        {
          "Affective visualization in Virtual Reality": "Cassani, R., Moinnereau, M.-A., & Falk, T. H. (2018). A Neurophysiological Sensor-"
        },
        {
          "Affective visualization in Virtual Reality": "Equipped Head-Mounted Display for Instrumental QoE Assessment of Immersive"
        },
        {
          "Affective visualization in Virtual Reality": "Multimedia. 2018 Tenth International Conference on Quality of Multimedia"
        },
        {
          "Affective visualization in Virtual Reality": "Experience (QoMEX), 1–6. https://doi.org/10.1109/QoMEX.2018.8463422"
        },
        {
          "Affective visualization in Virtual Reality": "Cavazza, M., Aranyi, G., Charles, F., Porteous, J., Gilroy, S., Klovatch, I., Jackont, G., Soreq,"
        },
        {
          "Affective visualization in Virtual Reality": "E., Keynan, N. J., Cohen, A., Raz, G., & Hendler, T. (2014). Towards Empathic"
        },
        {
          "Affective visualization in Virtual Reality": "Neurofeedback for Interactive Storytelling [Application/pdf]. 19 pages."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.4230/OASICS.CMN.2014.42"
        },
        {
          "Affective visualization in Virtual Reality": "Colzato, L. S., Sellaro, R., & Beste, C. (2017). Darwin revisited: The vagus nerve is a causal"
        },
        {
          "Affective visualization in Virtual Reality": "element in controlling recognition of other’s emotions. Cortex, 92, 95–102."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1016/j.cortex.2017.03.017"
        },
        {
          "Affective visualization in Virtual Reality": "Conati, C., & Zhou, X. (2002). Modeling Students’ Emotions from Cognitive Appraisal in"
        },
        {
          "Affective visualization in Virtual Reality": "Educational Games. In S. A. Cerri, G. Gouardères, & F. Paraguaçu (Eds.), Intelligent"
        },
        {
          "Affective visualization in Virtual Reality": "Tutoring Systems (pp. 944–954). Springer Berlin Heidelberg."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Cordaro, D. T., Keltner, D., Tshering, S., Wangchuk, D., & Flynn, L. M. (2016). The voice"
        },
        {
          "Affective visualization in Virtual Reality": "conveys emotion in ten globalized cultures and one remote village in Bhutan."
        },
        {
          "Affective visualization in Virtual Reality": "Emotion, 16(1), 117–128. https://doi.org/10.1037/emo0000100"
        },
        {
          "Affective visualization in Virtual Reality": "Cosmides, L., & Tooby, J. (1994). Origins of domain specificity: The evolution of functional"
        },
        {
          "Affective visualization in Virtual Reality": "organization. In L. A. Hirschfeld & S. A. Gelman (Eds.), Mapping the Mind (1st ed.,"
        },
        {
          "Affective visualization in Virtual Reality": "pp. 85–116). Cambridge University Press."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1017/CBO9780511752902.005"
        },
        {
          "Affective visualization in Virtual Reality": "D. McDuff, R. E. Kaliouby, & R. W. Picard. (2012). Crowdsourcing Facial Responses to"
        },
        {
          "Affective visualization in Virtual Reality": "Online Videos. IEEE Transactions on Affective Computing, 3(4), 456–468."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/T-AFFC.2012.19"
        },
        {
          "Affective visualization in Virtual Reality": "Darwin, C. (1872). The expression of the emotions in man and animals. John Murray."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1037/10001-000"
        },
        {
          "Affective visualization in Virtual Reality": "Davidson, R. J. (1992). Emotion and Affective Style: Hemispheric Substrates. Psychological"
        },
        {
          "Affective visualization in Virtual Reality": "Science, 3(1), 39–43. https://doi.org/10.1111/j.1467-9280.1992.tb00254.x"
        },
        {
          "Affective visualization in Virtual Reality": "Desmet. (2015). Design for mood: Twenty activity-based opportunities to design for mood"
        },
        {
          "Affective visualization in Virtual Reality": "regulation. International Journal of Design, 9 (2), 2015."
        },
        {
          "Affective visualization in Virtual Reality": "Desmet, Vastenburg, M. H., & Romero, N. (2016). Mood measurement with Pick-A-Mood:"
        },
        {
          "Affective visualization in Virtual Reality": "Review of current methods and design of a pictorial self-report scale. J. of Design"
        },
        {
          "Affective visualization in Virtual Reality": "Research, 14(3), 241. https://doi.org/10.1504/JDR.2016.079751"
        },
        {
          "Affective visualization in Virtual Reality": "Dimberg, U. (1982). Facial Reactions to Facial Expressions. Psychophysiology, 19(6), 643–"
        },
        {
          "Affective visualization in Virtual Reality": "647. https://doi.org/10.1111/j.1469-8986.1982.tb02516.x"
        },
        {
          "Affective visualization in Virtual Reality": "Dimberg, U., & Thunberg, M. (2012). Empathy, emotional contagion, and rapid facial"
        },
        {
          "Affective visualization in Virtual Reality": "reactions to angry and happy facial expressions: Empathy and rapid facial reactions."
        },
        {
          "Affective visualization in Virtual Reality": "PsyCh Journal, 1(2), 118–127. https://doi.org/10.1002/pchj.4"
        },
        {
          "Affective visualization in Virtual Reality": "Dimberg, U., Thunberg, M., & Elmehed, K. (2000). Unconscious Facial Reactions to"
        },
        {
          "Affective visualization in Virtual Reality": "Emotional Facial Expressions. Psychological Science, 11(1), 86–89."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1111/1467-9280.00221"
        },
        {
          "Affective visualization in Virtual Reality": "Drossos, K., Floros, A., Giannakoulopoulos, A., & Kanellopoulos, N. (2015). Investigating"
        },
        {
          "Affective visualization in Virtual Reality": "the Impact of Sound Angular Position on the Listener Affective State. IEEE"
        },
        {
          "Affective visualization in Virtual Reality": "Transactions on Affective Computing, 6(1), 27–42."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/TAFFC.2015.2392768"
        },
        {
          "Affective visualization in Virtual Reality": "Ebe, Y., & Umemuro, H. (2015). Emotion Evoked by Texture and Application to Emotional"
        },
        {
          "Affective visualization in Virtual Reality": "Communication. Proceedings of the 33rd Annual ACM Conference Extended"
        },
        {
          "Affective visualization in Virtual Reality": "Abstracts on Human Factors in Computing Systems, 1995–2000."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1145/2702613.2732768"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Feffer, M., Rudovic, O. (Oggi), & Picard, R. W. (2018). A Mixture of Personalized Experts"
        },
        {
          "Affective visualization in Virtual Reality": "for Human Affect Estimation. In P. Perner (Ed.), Machine Learning and Data Mining"
        },
        {
          "Affective visualization in Virtual Reality": "in Pattern Recognition (pp. 316–330). Springer International Publishing."
        },
        {
          "Affective visualization in Virtual Reality": "Feng, C., Bartram, L., & Riecke, B. E. (2014). Evaluating affective features of 3D"
        },
        {
          "Affective visualization in Virtual Reality": "motionscapes. Proceedings of the ACM Symposium on Applied Perception - SAP ’14,"
        },
        {
          "Affective visualization in Virtual Reality": "23–30. https://doi.org/10.1145/2628257.2628264"
        },
        {
          "Affective visualization in Virtual Reality": "Fernández-Sotos, A., Fernández-Caballero, A., & Latorre, J. M. (2016). Influence of Tempo"
        },
        {
          "Affective visualization in Virtual Reality": "and Rhythmic Unit in Musical Emotion Regulation. Frontiers in Computational"
        },
        {
          "Affective visualization in Virtual Reality": "Neuroscience, 10. https://doi.org/10.3389/fncom.2016.00080"
        },
        {
          "Affective visualization in Virtual Reality": "Fisher, R. J. (1993). Social Desirability Bias and the Validity of Indirect Questioning. Journal"
        },
        {
          "Affective visualization in Virtual Reality": "of Consumer Research, 20(2), 303. https://doi.org/10.1086/209351"
        },
        {
          "Affective visualization in Virtual Reality": "Fuhrman, O., & Boroditsky, L. (2010). Cross-Cultural Differences in Mental Representations"
        },
        {
          "Affective visualization in Virtual Reality": "of Time: Evidence From an Implicit Nonlinguistic Task. Cognitive Science, 34(8),"
        },
        {
          "Affective visualization in Virtual Reality": "1430–1451. https://doi.org/10.1111/j.1551-6709.2010.01105.x"
        },
        {
          "Affective visualization in Virtual Reality": "Gao, X.-P., Xin, J. H., Sato, T., Hansuebsai, A., Scalzo, M., Kajiwara, K., Guan, S.-S.,"
        },
        {
          "Affective visualization in Virtual Reality": "Valldeperas, J., Lis, M. J., & Billger, M. (2007). Analysis of cross-cultural color"
        },
        {
          "Affective visualization in Virtual Reality": "emotion. Color Research & Application, 32(3), 223–229."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1002/col.20321"
        },
        {
          "Affective visualization in Virtual Reality": "Garcia, J. A., & Navarro, K. F. (2014). The Mobile RehAppTM: An AR-based mobile game"
        },
        {
          "Affective visualization in Virtual Reality": "for ankle sprain rehabilitation. 2014 IEEE 3nd International Conference on Serious"
        },
        {
          "Affective visualization in Virtual Reality": "Games and Applications for Health (SeGAH), 1–6."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/SeGAH.2014.7067087"
        },
        {
          "Affective visualization in Virtual Reality": "Georgiou, T., & Demiris, Y. (2017). Adaptive user modelling in car racing games using"
        },
        {
          "Affective visualization in Virtual Reality": "behavioural and physiological data. User Modeling and User-Adapted Interaction,"
        },
        {
          "Affective visualization in Virtual Reality": "27(2), 267–311. https://doi.org/10.1007/s11257-017-9192-3"
        },
        {
          "Affective visualization in Virtual Reality": "Gerardi, G. M., & Gerken, L. (1995). The Development of Affective Responses to Modality"
        },
        {
          "Affective visualization in Virtual Reality": "and Melodic Contour. Music Perception: An Interdisciplinary Journal, 12(3), 279–"
        },
        {
          "Affective visualization in Virtual Reality": "290. https://doi.org/10.2307/40286184"
        },
        {
          "Affective visualization in Virtual Reality": "Greinacher, R., Kojic, T., Meier, L., Parameshappa, R. G., Moller, S., & Voigt-Antons, J.-N."
        },
        {
          "Affective visualization in Virtual Reality": "(2020). Impact of Tactile and Visual Feedback on Breathing Rhythm and User"
        },
        {
          "Affective visualization in Virtual Reality": "Experience in VR Exergaming. 2020 Twelfth International Conference on Quality of"
        },
        {
          "Affective visualization in Virtual Reality": "Multimedia Experience (QoMEX), 1–6."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/QoMEX48832.2020.9123141"
        },
        {
          "Affective visualization in Virtual Reality": "Greinacher, R., & Voigt-Antons, J.-N. (2020). Accuracy Assessment of ARKit 2 Based Gaze"
        },
        {
          "Affective visualization in Virtual Reality": "Estimation. In M. Kurosu (Ed.), Human-Computer Interaction. Design and User"
        },
        {
          "Affective visualization in Virtual Reality": "Experience (Vol. 12181, pp. 439–449). Springer International Publishing."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1007/978-3-030-49059-1_32"
        },
        {
          "Affective visualization in Virtual Reality": "Haar, A. J. H., Jain, A., Schoeller, F., & Maes, P. (2020). Augmenting aesthetic chills using a"
        },
        {
          "Affective visualization in Virtual Reality": "wearable prosthesis improves their downstream effects on reward and social"
        },
        {
          "Affective visualization in Virtual Reality": "cognition. Scientific Reports, 10(1), 21603. https://doi.org/10.1038/s41598-020-"
        },
        {
          "Affective visualization in Virtual Reality": "77951-w"
        },
        {
          "Affective visualization in Virtual Reality": "Harischandra, J., & Perera, M. U. S. (2012). Intelligent emotion recognition system using"
        },
        {
          "Affective visualization in Virtual Reality": "brain signals (EEG). 2012 IEEE-EMBS Conference on Biomedical Engineering and"
        },
        {
          "Affective visualization in Virtual Reality": "Sciences, 454–459. https://doi.org/10.1109/IECBES.2012.6498050"
        },
        {
          "Affective visualization in Virtual Reality": "Hernandez, J., Li, Y., Rehg, J. M., & Picard, R. W. (2014). BioGlass: Physiological parameter"
        },
        {
          "Affective visualization in Virtual Reality": "estimation using a head-mounted wearable device. 2014 4th International Conference"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "on Wireless Mobile Communication and Healthcare - Transforming Healthcare"
        },
        {
          "Affective visualization in Virtual Reality": "Through Innovations in Mobile and Wireless Technologies (MOBIHEALTH), 55–58."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/MOBIHEALTH.2014.7015908"
        },
        {
          "Affective visualization in Virtual Reality": "Hofmann, S. M., Klotzsche, F., Mariola, A., Nikulin, V. V., Villringer, A., & Gaebler, M."
        },
        {
          "Affective visualization in Virtual Reality": "(2018). Decoding Subjective Emotional Arousal during a Naturalistic VR Experience"
        },
        {
          "Affective visualization in Virtual Reality": "from EEG Using LSTMs. 2018 IEEE International Conference on Artificial"
        },
        {
          "Affective visualization in Virtual Reality": "Intelligence and Virtual Reality (AIVR), 128–131."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/AIVR.2018.00026"
        },
        {
          "Affective visualization in Virtual Reality": "Hoppe, S., Loetscher, T., Morey, S. A., & Bulling, A. (2018). Eye Movements During"
        },
        {
          "Affective visualization in Virtual Reality": "Everyday Behavior Predict Personality Traits. Frontiers in Human Neuroscience, 12,"
        },
        {
          "Affective visualization in Virtual Reality": "105. https://doi.org/10.3389/fnhum.2018.00105"
        },
        {
          "Affective visualization in Virtual Reality": "Hupont, I., Gracia, J., Sanagustin, L., & Gracia, M. A. (2015). How do new visual immersive"
        },
        {
          "Affective visualization in Virtual Reality": "systems influence gaming QoE? A use case of serious gaming with Oculus Rift. 2015"
        },
        {
          "Affective visualization in Virtual Reality": "Seventh International Workshop on Quality of Multimedia Experience (QoMEX), 1–6."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/QoMEX.2015.7148110"
        },
        {
          "Affective visualization in Virtual Reality": "Huster, R. J., Stevens, S., Gerlach, A. L., & Rist, F. (2009). A spectralanalytic approach to"
        },
        {
          "Affective visualization in Virtual Reality": "emotional responses evoked through picture presentation. International Journal of"
        },
        {
          "Affective visualization in Virtual Reality": "Psychophysiology, 72(2), 212–216. https://doi.org/10.1016/j.ijpsycho.2008.12.009"
        },
        {
          "Affective visualization in Virtual Reality": "Jaques, P. A., & Vicari, R. M. (2007). A BDI approach to infer student’s emotions in an"
        },
        {
          "Affective visualization in Virtual Reality": "intelligent learning environment. Computers & Education, 49(2), 360–384."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1016/j.compedu.2005.09.002"
        },
        {
          "Affective visualization in Virtual Reality": "Kapur, A., Kapur, A., Virji-Babul, N., Tzanetakis, G., & Driessen, P. F. (2005). Gesture-"
        },
        {
          "Affective visualization in Virtual Reality": "Based Affective Computing on Motion Capture Data. In J. Tao, T. Tan, & R. W."
        },
        {
          "Affective visualization in Virtual Reality": "Picard (Eds.), Affective Computing and Intelligent Interaction (pp. 1–7). Springer"
        },
        {
          "Affective visualization in Virtual Reality": "Berlin Heidelberg."
        },
        {
          "Affective visualization in Virtual Reality": "Kitson, A., DiPaola, S., & Riecke, B. E. (2019). Lucid Loop: A Virtual Deep Learning"
        },
        {
          "Affective visualization in Virtual Reality": "Biofeedback System for Lucid Dreaming Practice. Extended Abstracts of the 2019"
        },
        {
          "Affective visualization in Virtual Reality": "CHI Conference on Human Factors in Computing Systems, 1–6."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1145/3290607.3312952"
        },
        {
          "Affective visualization in Virtual Reality": "Klug, M., & Gramann, K. (2020). Identifying key factors for improving ICA-based"
        },
        {
          "Affective visualization in Virtual Reality": "decomposition of EEG data in mobile and stationary experiments. European Journal"
        },
        {
          "Affective visualization in Virtual Reality": "of Neuroscience. https://doi.org/10.1111/ejn.14992"
        },
        {
          "Affective visualization in Virtual Reality": "Koelstra, S., Muhl, C., Soleymani, M., Jong-Seok Lee, Yazdani, A., Ebrahimi, T., Pun, T.,"
        },
        {
          "Affective visualization in Virtual Reality": "Nijholt, A., & Patras, I. (2012). DEAP: A Database for Emotion Analysis ;Using"
        },
        {
          "Affective visualization in Virtual Reality": "Physiological Signals. IEEE Transactions on Affective Computing, 3(1), 18–31."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/T-AFFC.2011.15"
        },
        {
          "Affective visualization in Virtual Reality": "Koenig, S. T., Crucian, G. P., Duenser, A., Bartneck, C., & Dalrymple-Alford, J. C. (2011)."
        },
        {
          "Affective visualization in Virtual Reality": "Validity evaluation of a spatial memory task in virtual environments."
        },
        {
          "Affective visualization in Virtual Reality": "Kothe, C. A., & Makeig, S. (2013). BCILAB: A platform for brain–computer interface"
        },
        {
          "Affective visualization in Virtual Reality": "development. Journal of Neural Engineering, 10(5), 056014."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1088/1741-2560/10/5/056014"
        },
        {
          "Affective visualization in Virtual Reality": "Lang, P. J., Bradley, M. M., & Cuthbert, B. N. (2008). International affective picture system"
        },
        {
          "Affective visualization in Virtual Reality": "(IAPS): Affective ratings of pictures and instruction manual. Technical Report A-8."
        },
        {
          "Affective visualization in Virtual Reality": "University of Florida, Gainesville, FL."
        },
        {
          "Affective visualization in Virtual Reality": "Lange, C. G., & James, W. (Eds.). (1922). The emotions, Vol. 1. Williams & Wilkins Co."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1037/10735-000"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Leslie, G., Picard, R., & Lui, S. (2015). An EEG and Motion Capture Based Expressive Music"
        },
        {
          "Affective visualization in Virtual Reality": "Interface for Affective Neurofeedback. https://doi.org/10.13140/RG.2.1.4378.6081"
        },
        {
          "Affective visualization in Virtual Reality": "Li, Z., Tong, L., Wang, L., Li, Y., He, W., Guan, M., & Yan, B. (2016). Self-regulating"
        },
        {
          "Affective visualization in Virtual Reality": "positive emotion networks by feedback of multiple emotional brain states using real-"
        },
        {
          "Affective visualization in Virtual Reality": "time fMRI. Experimental Brain Research, 234(12), 3575–3586."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1007/s00221-016-4744-z"
        },
        {
          "Affective visualization in Virtual Reality": "Lipson-Smith, R., Bernhardt, J., Zamuner, E., Churilov, L., Busietta, N., & Moratti, D."
        },
        {
          "Affective visualization in Virtual Reality": "(2020). Exploring colour in context using Virtual Reality: Does a room change how"
        },
        {
          "Affective visualization in Virtual Reality": "you feel? Virtual Reality. https://doi.org/10.1007/s10055-020-00479-x"
        },
        {
          "Affective visualization in Virtual Reality": "Lockyer, M., Bartram, L., & Riecke, B. E. (2011). Simple Motion Textures for Ambient"
        },
        {
          "Affective visualization in Virtual Reality": "Affect. Computational Aesthetics in Graphics, Visualization, 8 pages."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.2312/COMPAESTH/COMPAESTH11/089-096"
        },
        {
          "Affective visualization in Virtual Reality": "Lucassen, M. P., Gevers, T., & Gijsenij, A. (2011). Texture affects color emotion. Color"
        },
        {
          "Affective visualization in Virtual Reality": "Research & Application, 36(6), 426–436. https://doi.org/10.1002/col.20647"
        },
        {
          "Affective visualization in Virtual Reality": "Makeig, S., Jung, T.-P., Bell, A. J., Ghahremani, D., & Sejnowski, T. J. (1997). Blind"
        },
        {
          "Affective visualization in Virtual Reality": "separation of auditory event-related brain responses into independent components."
        },
        {
          "Affective visualization in Virtual Reality": "Proceedings of the National Academy of Sciences, 94(20), 10979–10984."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1073/pnas.94.20.10979"
        },
        {
          "Affective visualization in Virtual Reality": "Malik, M., Bigger, J. T., Camm, A. J., Kleiger, R. E., Malliani, A., Moss, A. J., & Schwartz,"
        },
        {
          "Affective visualization in Virtual Reality": "P. J. (1996). Heart rate variability: Standards of measurement, physiological"
        },
        {
          "Affective visualization in Virtual Reality": "interpretation, and clinical use. European Heart Journal, 17(3), 354–381."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1093/oxfordjournals.eurheartj.a014868"
        },
        {
          "Affective visualization in Virtual Reality": "Martínez-Tejada, L. A., Puertas-González, A., Yoshimura, N., & Koike, Y. (2021). Exploring"
        },
        {
          "Affective visualization in Virtual Reality": "EEG Characteristics to Identify Emotional Reactions under Videogame Scenarios."
        },
        {
          "Affective visualization in Virtual Reality": "Brain Sciences, 11(3), 378. https://doi.org/10.3390/brainsci11030378"
        },
        {
          "Affective visualization in Virtual Reality": "Mattek, A. M., Wolford, G. L., & Whalen, P. J. (2017). A Mathematical Model Captures the"
        },
        {
          "Affective visualization in Virtual Reality": "Structure of Subjective Affect. Perspectives on Psychological Science, 12(3), 508–"
        },
        {
          "Affective visualization in Virtual Reality": "526. https://doi.org/10.1177/1745691616685863"
        },
        {
          "Affective visualization in Virtual Reality": "Mavridou, I., McGhee, J. T., Hamedi, M., Fatoorechi, M., Cleal, A., Ballaguer-Balester, E.,"
        },
        {
          "Affective visualization in Virtual Reality": "Seiss, E., Cox, G., & Nduka, C. (2017). FACETEQ interface demo for emotion"
        },
        {
          "Affective visualization in Virtual Reality": "expression in VR. 2017 IEEE Virtual Reality (VR), 441–442."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/VR.2017.7892369"
        },
        {
          "Affective visualization in Virtual Reality": "Mullen, T. R., Kothe, C. A. E., Chi, Y. M., Ojeda, A., Kerth, T., Makeig, S., Jung, T.-P., &"
        },
        {
          "Affective visualization in Virtual Reality": "Cauwenberghs, G. (2015). Real-time neuroimaging and cognitive monitoring using"
        },
        {
          "Affective visualization in Virtual Reality": "wearable dry EEG. IEEE Transactions on Biomedical Engineering, 62(11), 2553–"
        },
        {
          "Affective visualization in Virtual Reality": "2567. https://doi.org/10.1109/TBME.2015.2481482"
        },
        {
          "Affective visualization in Virtual Reality": "Norman, J. F., Beers, A., & Phillips, F. (2010). Fechner’s Aesthetics Revisited. Seeing and"
        },
        {
          "Affective visualization in Virtual Reality": "Perceiving, 23(3), 263–271. https://doi.org/10.1163/187847510X516412"
        },
        {
          "Affective visualization in Virtual Reality": "Ortony, A., Clore, G. L., & Collins, A. (1988). The cognitive structure of emotions."
        },
        {
          "Affective visualization in Virtual Reality": "Cambridge university press."
        },
        {
          "Affective visualization in Virtual Reality": "Pagani, M., Lombardi, F., Guzzetti, S., Sandrone, G., Rimoldi, O., Malfatto, G., Cerutti, S., &"
        },
        {
          "Affective visualization in Virtual Reality": "Malliani, A. (1984). Power spectral density of heart rate variability as an index of"
        },
        {
          "Affective visualization in Virtual Reality": "sympatho-vagal interaction in normal and hypertensive subjects. Journal of"
        },
        {
          "Affective visualization in Virtual Reality": "Hypertension. Supplement: Official Journal of the International Society of"
        },
        {
          "Affective visualization in Virtual Reality": "Hypertension, 2(3), S383-385."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Palmer, S. E., & Schloss, K. B. (2010). An ecological valence theory of human color"
        },
        {
          "Affective visualization in Virtual Reality": "preference. Proceedings of the National Academy of Sciences, 107(19), 8877–8882."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1073/pnas.0906172107"
        },
        {
          "Affective visualization in Virtual Reality": "Peperkorn, H. M., Alpers, G. W., & Mühlberger, A. (2014). Triggers of Fear: Perceptual Cues"
        },
        {
          "Affective visualization in Virtual Reality": "Versus Conceptual Information in Spider Phobia: Cues Versus Information in Spider"
        },
        {
          "Affective visualization in Virtual Reality": "Phobia. Journal of Clinical Psychology, 70(7), 704–714."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1002/jclp.22057"
        },
        {
          "Affective visualization in Virtual Reality": "Perkis, A., Timmerer, C., Baraković, S., Husić, J. B., Bech, S., Bosse, S., Botev, J.,"
        },
        {
          "Affective visualization in Virtual Reality": "Brunnström, K., Cruz, L., Moor, K. D., Saibanti, A. de P., Durnez, W., Egger-Lampl,"
        },
        {
          "Affective visualization in Virtual Reality": "S., Engelke, U., Falk, T. H., Hameed, A., Hines, A., Kojic, T., Kukolj, D., …"
        },
        {
          "Affective visualization in Virtual Reality": "Zadtootaghaj, S. (2020). QUALINET White Paper on Definitions of Immersive Media"
        },
        {
          "Affective visualization in Virtual Reality": "Experience (IMEx)."
        },
        {
          "Affective visualization in Virtual Reality": "Petri, T. (2009). Exploring relationships between audio features and emotion in music."
        },
        {
          "Affective visualization in Virtual Reality": "Frontiers in Human Neuroscience, 3."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.3389/conf.neuro.09.2009.02.033"
        },
        {
          "Affective visualization in Virtual Reality": "Pfurtscheller, G., & Lopes da Silva, F. H. (1999). Event-related EEG/MEG synchronization"
        },
        {
          "Affective visualization in Virtual Reality": "and desynchronization: Basic principles. Clinical Neurophysiology, 110(11), 1842–"
        },
        {
          "Affective visualization in Virtual Reality": "1857. https://doi.org/10.1016/S1388-2457(99)00141-8"
        },
        {
          "Affective visualization in Virtual Reality": "Picard, R. W., Vyzas, E., & Healey, J. (2001). Toward machine emotional intelligence:"
        },
        {
          "Affective visualization in Virtual Reality": "Analysis of affective physiological state. IEEE Transactions on Pattern Analysis and"
        },
        {
          "Affective visualization in Virtual Reality": "Machine Intelligence, 23(10), 1175–1191. https://doi.org/10.1109/34.954607"
        },
        {
          "Affective visualization in Virtual Reality": "Pinilla, A., Tamayo, R. M., & Neira, J. (2020). How Do Induced Affective States Bias"
        },
        {
          "Affective visualization in Virtual Reality": "Emotional Contagion to Faces? A Three-Dimensional Model. Frontiers in"
        },
        {
          "Affective visualization in Virtual Reality": "Psychology, 11. https://doi.org/10.3389/fpsyg.2020.00097"
        },
        {
          "Affective visualization in Virtual Reality": "Pion-Tonachini, L., Sheng-Hsiou Hsu, Makeig, S., Tzyy-Ping Jung, & Cauwenberghs, G."
        },
        {
          "Affective visualization in Virtual Reality": "(2015). Real-time EEG Source-mapping Toolbox (REST): Online ICA and source"
        },
        {
          "Affective visualization in Virtual Reality": "localization. 2015 37th Annual International Conference of the IEEE Engineering in"
        },
        {
          "Affective visualization in Virtual Reality": "Medicine and Biology Society (EMBC), 4114–4117."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/EMBC.2015.7319299"
        },
        {
          "Affective visualization in Virtual Reality": "Piwek, L., Pollick, F., & Petrini, K. (2015). Audiovisual integration of emotional signals from"
        },
        {
          "Affective visualization in Virtual Reality": "others’ social interactions. Frontiers in Psychology, 9."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.3389/fpsyg.2015.00611"
        },
        {
          "Affective visualization in Virtual Reality": "Plutchik, R. (1982). A psychoevolutionary theory of emotions. Social Science Information,"
        },
        {
          "Affective visualization in Virtual Reality": "21(4–5), 529–553. https://doi.org/10.1177/053901882021004003"
        },
        {
          "Affective visualization in Virtual Reality": "Polzehl, T., Schmitt, A., Metze, F., & Wagner, M. (2011). Anger recognition in speech using"
        },
        {
          "Affective visualization in Virtual Reality": "acoustic and linguistic cues. Speech Communication, 53(9–10), 1198–1209."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1016/j.specom.2011.05.002"
        },
        {
          "Affective visualization in Virtual Reality": "Porcu, S., Floris, A., Voigt-Antons, J.-N., Atzori, L., & Moller, S. (2020). Estimation of the"
        },
        {
          "Affective visualization in Virtual Reality": "Quality of Experience during Video Streaming from Facial Expression and Gaze"
        },
        {
          "Affective visualization in Virtual Reality": "Direction. IEEE Transactions on Network and Service Management, 1–1."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/TNSM.2020.3018303"
        },
        {
          "Affective visualization in Virtual Reality": "Putnam, H. (1967). The nature of mental states. In W. H. Capitan & D. D. Merrill (Eds.), Art,"
        },
        {
          "Affective visualization in Virtual Reality": "Mind, and Religion (pp. 1--223). Pittsburgh University Press."
        },
        {
          "Affective visualization in Virtual Reality": "Raffe, W. L., Zambetta, F., Li, X., & Stanley, K. O. (2015). Integrated Approach to"
        },
        {
          "Affective visualization in Virtual Reality": "Personalized Procedural Map Generation Using Evolutionary Algorithms. IEEE"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Transactions on Computational Intelligence and AI in Games, 7(2), 139–155."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1109/TCIAIG.2014.2341665"
        },
        {
          "Affective visualization in Virtual Reality": "Ray, W., & Cole, H. (1985). EEG alpha activity reflects attentional demands, and beta"
        },
        {
          "Affective visualization in Virtual Reality": "activity reflects emotional and cognitive processes. Science, 228(4700), 750–752."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1126/science.3992243"
        },
        {
          "Affective visualization in Virtual Reality": "Renard, Y., Lotte, F., Gibert, G., Congedo, M., Maby, E., Delannoy, V., Bertrand, O., &"
        },
        {
          "Affective visualization in Virtual Reality": "Lécuyer, A. (2010). OpenViBE: An Open-Source Software Platform to Design, Test,"
        },
        {
          "Affective visualization in Virtual Reality": "and Use Brain–Computer Interfaces in Real and Virtual Environments. Presence:"
        },
        {
          "Affective visualization in Virtual Reality": "Teleoperators and Virtual Environments, 19(1), 35–53."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1162/pres.19.1.35"
        },
        {
          "Affective visualization in Virtual Reality": "Reuderink, B., Mühl, C., & Poel, M. (2013). Valence, arousal and dominance in the EEG"
        },
        {
          "Affective visualization in Virtual Reality": "during game play. International Journal of Autonomous and Adaptive"
        },
        {
          "Affective visualization in Virtual Reality": "Communications Systems, 6(1), 45. https://doi.org/10.1504/IJAACS.2013.050691"
        },
        {
          "Affective visualization in Virtual Reality": "Robitaille, P., & McGuffin, M. J. (2019). Increased affect-arousal in VR can be detected from"
        },
        {
          "Affective visualization in Virtual Reality": "faster body motion with increased heart rate. Proceedings of the ACM SIGGRAPH"
        },
        {
          "Affective visualization in Virtual Reality": "Symposium on Interactive 3D Graphics and Games, 1–6."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1145/3306131.3317022"
        },
        {
          "Affective visualization in Virtual Reality": "Russell, J. A. (1980). A circumplex model of affect. Journal of Personality and Social"
        },
        {
          "Affective visualization in Virtual Reality": "Psychology, 39(6), 1161–1178. https://doi.org/10.1037/h0077714"
        },
        {
          "Affective visualization in Virtual Reality": "Ryali, C. K., Goffin, S., Winkielman, P., & Yu, A. J. (2020). From likely to likable: The role"
        },
        {
          "Affective visualization in Virtual Reality": "of statistical typicality in human social assessment of faces. Proceedings of the"
        },
        {
          "Affective visualization in Virtual Reality": "National Academy of Sciences, 117(47), 29371–29380."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1073/pnas.1912343117"
        },
        {
          "Affective visualization in Virtual Reality": "Schachter, S., & Singer, J. (1962). Cognitive, social, and physiological determinants of"
        },
        {
          "Affective visualization in Virtual Reality": "emotional state. Psychological Review, 69(5), 379–399."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1037/h0046234"
        },
        {
          "Affective visualization in Virtual Reality": "Scherer, K. R., & Oshinsky, J. S. (1977). Cue utilization in emotion attribution from auditory"
        },
        {
          "Affective visualization in Virtual Reality": "stimuli. Motivation and Emotion, 1(4), 331–346. https://doi.org/10.1007/BF00992539"
        },
        {
          "Affective visualization in Virtual Reality": "Schoeller, F., Bertrand, P., Gerry, L. J., Jain, A., Horowitz, A. H., & Zenasni, F. (2019)."
        },
        {
          "Affective visualization in Virtual Reality": "Combining Virtual Reality and Biofeedback to Foster Empathic Abilities in Humans."
        },
        {
          "Affective visualization in Virtual Reality": "Frontiers in Psychology, 9, 2741. https://doi.org/10.3389/fpsyg.2018.02741"
        },
        {
          "Affective visualization in Virtual Reality": "Schoeller, F., Haar, A. J. H., Jain, A., & Maes, P. (2019). Enhancing human emotions with"
        },
        {
          "Affective visualization in Virtual Reality": "interoceptive technologies. Physics of Life Reviews, 31, 310–319."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1016/j.plrev.2019.10.008"
        },
        {
          "Affective visualization in Virtual Reality": "Semertzidis, N., Scary, M., Andres, J., Dwivedi, B., Kulwe, Y. C., Zambetta, F., & Mueller,"
        },
        {
          "Affective visualization in Virtual Reality": "F. F. (2020). Neo-Noumena: Augmenting Emotion Communication. Proceedings of"
        },
        {
          "Affective visualization in Virtual Reality": "the 2020 CHI Conference on Human Factors in Computing Systems, 1–13."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1145/3313831.3376599"
        },
        {
          "Affective visualization in Virtual Reality": "Shiban, Y., Peperkorn, H., Alpers, G. W., Pauli, P., & Mühlberger, A. (2016). Influence of"
        },
        {
          "Affective visualization in Virtual Reality": "perceptual cues and conceptual information on the activation and reduction of"
        },
        {
          "Affective visualization in Virtual Reality": "claustrophobic fear. Journal of Behavior Therapy and Experimental Psychiatry, 51,"
        },
        {
          "Affective visualization in Virtual Reality": "19–26. https://doi.org/10.1016/j.jbtep.2015.11.002"
        },
        {
          "Affective visualization in Virtual Reality": "Shiban, Y., Reichenberger, J., Neumann, I. D., & Mühlberger, A. (2015). Social conditioning"
        },
        {
          "Affective visualization in Virtual Reality": "and extinction paradigm: A translational study in virtual reality. Frontiers in"
        },
        {
          "Affective visualization in Virtual Reality": "Psychology, 6. https://doi.org/10.3389/fpsyg.2015.00400"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Shiota, M. N., & Kalat, J. W. (2012). Emotion (Second Edition). Linda Schreiber-Ganster."
        },
        {
          "Affective visualization in Virtual Reality": "Sitaram, R., Lee, S., Ruiz, S., Rana, M., Veit, R., & Birbaumer, N. (2011). Real-time support"
        },
        {
          "Affective visualization in Virtual Reality": "vector classification and feedback of multiple emotional brain states. NeuroImage,"
        },
        {
          "Affective visualization in Virtual Reality": "56(2), 753–765. https://doi.org/10.1016/j.neuroimage.2010.08.007"
        },
        {
          "Affective visualization in Virtual Reality": "Sutherland, M. R., & Mather, M. (2012). Negative arousal amplifies the effects of saliency in"
        },
        {
          "Affective visualization in Virtual Reality": "short-term memory. Emotion, 12(6), 1367–1372. https://doi.org/10.1037/a0027860"
        },
        {
          "Affective visualization in Virtual Reality": "Tajadura-Jiménez, A., Larsson, P., Väljamäe, A., Västfjäll, D., & Kleiner, M. (2010). When"
        },
        {
          "Affective visualization in Virtual Reality": "room size matters: Acoustic influences on emotional responses to sounds. Emotion,"
        },
        {
          "Affective visualization in Virtual Reality": "10(3), 416–422. https://doi.org/10.1037/a0018423"
        },
        {
          "Affective visualization in Virtual Reality": "Tajadura-Jiménez, A., Väljamäe, A., Asutay, E., & Västfjäll, D. (2010). Embodied auditory"
        },
        {
          "Affective visualization in Virtual Reality": "perception: The emotional impact of approaching and receding sound sources."
        },
        {
          "Affective visualization in Virtual Reality": "Emotion, 10(2), 216–229. https://doi.org/10.1037/a0018422"
        },
        {
          "Affective visualization in Virtual Reality": "Tajadura-Jiménez, A., Väljamäe, A., & Västfjäll, D. (2008). Self-Representation in Mediated"
        },
        {
          "Affective visualization in Virtual Reality": "Environments: The Experience of Emotions Modulated by Auditory-Vibrotactile"
        },
        {
          "Affective visualization in Virtual Reality": "Heartbeat. CyberPsychology & Behavior, 11(1), 33–38."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1089/cpb.2007.0002"
        },
        {
          "Affective visualization in Virtual Reality": "Thayer, J. F., Hansen, A. L., Saus-Rose, E., & Johnsen, B. H. (2009). Heart Rate Variability,"
        },
        {
          "Affective visualization in Virtual Reality": "Prefrontal Neural Function, and Cognitive Performance: The Neurovisceral"
        },
        {
          "Affective visualization in Virtual Reality": "Integration Perspective on Self-regulation, Adaptation, and Health. Annals of"
        },
        {
          "Affective visualization in Virtual Reality": "Behavioral Medicine, 37(2), 141–153. https://doi.org/10.1007/s12160-009-9101-z"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "Wundt, W. (1897). Outlines of psychology. (C. H. Judd, Trans.). Williams and Norgate."
        },
        {
          "Affective visualization in Virtual Reality": "https://doi.org/10.1037/12908-000"
        },
        {
          "Affective visualization in Virtual Reality": "Yannakakis, G. N., & Togelius, J. (2011). Experience-Driven Procedural Content Generation."
        },
        {
          "Affective visualization in Virtual Reality": "IEEE Transactions on Affective Computing, 2(3), 147–161. https://doi.org/10.1109/T-"
        },
        {
          "Affective visualization in Virtual Reality": "AFFC.2011.6"
        },
        {
          "Affective visualization in Virtual Reality": "Zander, T. O., & Kothe, C. (2011). Towards passive brain–computer interfaces: Applying"
        },
        {
          "Affective visualization in Virtual Reality": "brain–computer interface technology to human–machine systems in general. Journal"
        },
        {
          "Affective visualization in Virtual Reality": "of Neural Engineering, 8(2), 025005. https://doi.org/10.1088/1741-2560/8/2/025005"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Affective visualization in Virtual Reality": "List of abbreviations"
        },
        {
          "Affective visualization in Virtual Reality": "Human Computer Interaction (HCI)"
        },
        {
          "Affective visualization in Virtual Reality": "Virtual Reality (VR)"
        },
        {
          "Affective visualization in Virtual Reality": "functional Magnetic Brain Imaging (fMRI)"
        },
        {
          "Affective visualization in Virtual Reality": "electroencephalography (EEG)"
        },
        {
          "Affective visualization in Virtual Reality": "Ortony, Clore & Collins (OCC) theory of emotions"
        },
        {
          "Affective visualization in Virtual Reality": "Evaluative Space Model (ESM)"
        },
        {
          "Affective visualization in Virtual Reality": "Positive and Negative Affect Schedule (PANAS)"
        },
        {
          "Affective visualization in Virtual Reality": "Self-Assessment Manikin (SAM)"
        },
        {
          "Affective visualization in Virtual Reality": "Pick a Mood (PAM)"
        },
        {
          "Affective visualization in Virtual Reality": "International Affective Pictures System (IAPS)"
        },
        {
          "Affective visualization in Virtual Reality": "Facial Action Coding System (FACS)"
        },
        {
          "Affective visualization in Virtual Reality": "Action Unit (AU)"
        },
        {
          "Affective visualization in Virtual Reality": "Head–Mounted Display (HMD)"
        },
        {
          "Affective visualization in Virtual Reality": "Electromyography (EMG)"
        },
        {
          "Affective visualization in Virtual Reality": "electrocardiography (ECG)"
        },
        {
          "Affective visualization in Virtual Reality": "electroencephalography (EEG)"
        },
        {
          "Affective visualization in Virtual Reality": "RR-Intervals (RRI)"
        },
        {
          "Affective visualization in Virtual Reality": "heart rate variability (HRV)"
        },
        {
          "Affective visualization in Virtual Reality": "root mean square of successive differences (RMSSD)"
        },
        {
          "Affective visualization in Virtual Reality": "standard deviation of NN intervals (SDNN)."
        },
        {
          "Affective visualization in Virtual Reality": "Low Frequency / High Frequency ratio (LF/HF ratio)"
        },
        {
          "Affective visualization in Virtual Reality": "long short-term memory recurrent neural networks (LSTMRNN)"
        },
        {
          "Affective visualization in Virtual Reality": "Independent Component Analysis (ICA)"
        },
        {
          "Affective visualization in Virtual Reality": "independent components (ICs)"
        },
        {
          "Affective visualization in Virtual Reality": "Artifact Subspace Reconstruction (ASR)"
        },
        {
          "Affective visualization in Virtual Reality": "principal component analysis (PCP)"
        },
        {
          "Affective visualization in Virtual Reality": "Brain–Computer Interfaces (BCIs)"
        },
        {
          "Affective visualization in Virtual Reality": "electrocorticography (ECoG),"
        },
        {
          "Affective visualization in Virtual Reality": "Positron Emission Tomography (PET)"
        }
      ],
      "page": 27
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Brain Activity Correlates of Quality of Experience",
      "authors": [
        "J.-N Antons",
        "S Arndt",
        "R Schleicher",
        "S Möller"
      ],
      "year": "2014",
      "venue": "Quality of Experience",
      "doi": "10.1007/978-3-319-02681-7_8"
    },
    {
      "citation_id": "2",
      "title": "Using Virtual Reality and Head-Mounted Displays to Increase Performance in Rowing Workouts",
      "authors": [
        "S Arndt",
        "A Perkis",
        "J.-N Voigt-Antons"
      ],
      "year": "2018",
      "venue": "Proceedings of the 1st International Workshop on Multimedia Content Analysis in Sports -MMSports",
      "doi": "10.1145/3265845.3265848"
    },
    {
      "citation_id": "3",
      "title": "How We Recognize Angry and Happy Emotion in People",
      "authors": [
        "J Aronoff"
      ],
      "year": "2006",
      "venue": "Places, and Things. Cross-Cultural Research",
      "doi": "10.1177/1069397105282597"
    },
    {
      "citation_id": "4",
      "title": "Which are the stimuli in facial displays of anger and happiness? Configurational bases of emotion recognition",
      "authors": [
        "J Aronoff",
        "B Woike",
        "L Hyman"
      ],
      "year": "1992",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.62.6.1050"
    },
    {
      "citation_id": "5",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.70.3.614"
    },
    {
      "citation_id": "6",
      "title": "Humans Prefer Curved Visual Objects",
      "authors": [
        "M Bar",
        "M Neta"
      ],
      "year": "2006",
      "venue": "Psychological Science",
      "doi": "10.1111/j.1467-9280.2006.01759.x"
    },
    {
      "citation_id": "7",
      "title": "On emotional expression after decortication with some remarks on certain theoretical views: Part I",
      "authors": [
        "P Bard"
      ],
      "year": "1934",
      "venue": "Psychological Review",
      "doi": "10.1037/h0070765"
    },
    {
      "citation_id": "8",
      "title": "Affect as a Psychological Primitive",
      "authors": [
        "L Barrett",
        "E Bliss-Moreau"
      ],
      "year": "2009",
      "venue": "Advances in Experimental Social Psychology",
      "doi": "10.1016/S0065-2601(08)00404-8"
    },
    {
      "citation_id": "9",
      "title": "Affective Color in Visualization",
      "authors": [
        "L Bartram",
        "A Patra",
        "M Stone"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3025453.3026041"
    },
    {
      "citation_id": "10",
      "title": "Immersive Virtual Reality for the Assessment and Training of Spatial Memory: Feasibility in Individuals with Brain Injury",
      "authors": [
        "J Belger",
        "A Thone-Otto",
        "S Krohn",
        "C Finke",
        "J Tromp",
        "F Klotzsche",
        "A Villringer",
        "M Gaebler",
        "P Chojecki",
        "E Quinque"
      ],
      "year": "2019",
      "venue": "International Conference on Virtual Rehabilitation (ICVR)",
      "doi": "10.1109/ICVR46560.2019.8994342"
    },
    {
      "citation_id": "11",
      "title": "Toward Emotionally Adaptive Virtual Reality for Mental Health Applications",
      "authors": [
        "S Bermudez I Badia",
        "L Quintero",
        "M Cameirao",
        "A Chirico",
        "S Triberti",
        "P Cipresso",
        "A Gaggioli"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2018.2878846"
    },
    {
      "citation_id": "12",
      "title": "Influence of a BCI neurofeedback videogame in children with ADHD. Quantifying the brain activity through an EEG signal processing dedicated toolbox",
      "authors": [
        "D Blandón",
        "J Muñoz",
        "D Lopez",
        "O Gallo"
      ],
      "year": "2016",
      "venue": "IEEE 11th Colombian Computing Conference (CCC)",
      "doi": "10.1109/ColumbianCC.2016.7750788"
    },
    {
      "citation_id": "13",
      "title": "Heart Rate Variability Biofeedback Based on Slow-Paced Breathing With Immersive Virtual Reality Nature Scenery",
      "authors": [
        "J Blum",
        "C Rockstroh",
        "A Göritz"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2019.02172"
    },
    {
      "citation_id": "14",
      "title": "Development and Pilot Test of a Virtual Reality Respiratory Biofeedback Approach",
      "authors": [
        "J Blum",
        "C Rockstroh",
        "A Göritz"
      ],
      "year": "2020",
      "venue": "Applied Psychophysiology and Biofeedback",
      "doi": "10.1007/s10484-020-09468-x"
    },
    {
      "citation_id": "15",
      "title": "A Riemannian Modification of Artifact Subspace Reconstruction for EEG Artifact Handling",
      "authors": [
        "S Blum",
        "N Jacobsen",
        "M Bleichner",
        "S Debener"
      ],
      "year": "2019",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2019.00141"
    },
    {
      "citation_id": "16",
      "title": "Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry",
      "doi": "10.1016/0005-7916(94)90063-9"
    },
    {
      "citation_id": "17",
      "title": "The interpretation of posture through an alternative methodology to role play",
      "authors": [
        "P Bull"
      ],
      "year": "1978",
      "venue": "British Journal of Social and Clinical Psychology",
      "doi": "10.1111/j.2044-8260.1978.tb00888.x"
    },
    {
      "citation_id": "18",
      "title": "Robotics and Gaming to Improve Ankle Strength, Motor Control, and Function in Children With Cerebral Palsy-A Case Study Series",
      "authors": [
        "G Burdea",
        "D Cioi",
        "A Kale",
        "W Janes",
        "S Ross",
        "J Engsberg"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2012.2206055"
    },
    {
      "citation_id": "19",
      "title": "Beyond Bipolar Conceptualizations and Measures: The Case of Attitudes and Evaluative Space",
      "authors": [
        "J Cacioppo",
        "W Gardner",
        "G Berntson"
      ],
      "year": "1997",
      "venue": "Personality and Social Psychology Review",
      "doi": "10.1207/s15327957pspr0101_2"
    },
    {
      "citation_id": "20",
      "title": "Effects of hue, saturation, and brightness on preference",
      "authors": [
        "N Camgöz",
        "C Yener",
        "D Güvenç"
      ],
      "year": "2002",
      "venue": "Color Research & Application",
      "doi": "10.1002/col.10051"
    },
    {
      "citation_id": "21",
      "title": "The James-Lange Theory of Emotions: A Critical Examination and an Alternative Theory",
      "authors": [
        "W Cannon"
      ],
      "year": "1927",
      "venue": "The American Journal of Psychology",
      "doi": "10.2307/1415404"
    },
    {
      "citation_id": "22",
      "title": "A Neurophysiological Sensor-Equipped Head-Mounted Display for Instrumental QoE Assessment of Immersive Multimedia",
      "authors": [
        "R Cassani",
        "M.-A Moinnereau",
        "T Falk"
      ],
      "year": "2018",
      "venue": "Tenth International Conference on Quality of Multimedia Experience (QoMEX)",
      "doi": "10.1109/QoMEX.2018.8463422"
    },
    {
      "citation_id": "23",
      "title": "Towards Empathic Neurofeedback for Interactive Storytelling",
      "authors": [
        "M Cavazza",
        "G Aranyi",
        "F Charles",
        "J Porteous",
        "S Gilroy",
        "I Klovatch",
        "G Jackont",
        "E Soreq",
        "N Keynan",
        "A Cohen",
        "G Raz",
        "T Hendler"
      ],
      "year": "2014",
      "venue": "Towards Empathic Neurofeedback for Interactive Storytelling",
      "doi": "10.4230/OASICS.CMN.2014.42"
    },
    {
      "citation_id": "24",
      "title": "Darwin revisited: The vagus nerve is a causal element in controlling recognition of other's emotions",
      "authors": [
        "L Colzato",
        "R Sellaro",
        "C Beste"
      ],
      "year": "2017",
      "venue": "Cortex",
      "doi": "10.1016/j.cortex.2017.03.017"
    },
    {
      "citation_id": "25",
      "title": "Modeling Students' Emotions from Cognitive Appraisal in Educational Games",
      "authors": [
        "C Conati",
        "X Zhou"
      ],
      "year": "2002",
      "venue": "Intelligent Tutoring Systems"
    },
    {
      "citation_id": "26",
      "title": "The voice conveys emotion in ten globalized cultures and one remote village in Bhutan",
      "authors": [
        "D Cordaro",
        "D Keltner",
        "S Tshering",
        "D Wangchuk",
        "L Flynn"
      ],
      "year": "2016",
      "venue": "Emotion",
      "doi": "10.1037/emo0000100"
    },
    {
      "citation_id": "27",
      "title": "Origins of domain specificity: The evolution of functional organization",
      "authors": [
        "L Cosmides",
        "J Tooby"
      ],
      "year": "1994",
      "venue": "Mapping the Mind",
      "doi": "10.1017/CBO9780511752902.005"
    },
    {
      "citation_id": "28",
      "title": "Crowdsourcing Facial Responses to Online Videos",
      "authors": [
        "D Mcduff",
        "R Kaliouby",
        "R Picard"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2012.19"
    },
    {
      "citation_id": "29",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin"
      ],
      "year": "1872",
      "venue": "The expression of the emotions in man and animals",
      "doi": "10.1037/10001-000"
    },
    {
      "citation_id": "30",
      "title": "Emotion and Affective Style: Hemispheric Substrates",
      "authors": [
        "R Davidson"
      ],
      "year": "1992",
      "venue": "Psychological Science",
      "doi": "10.1111/j.1467-9280.1992.tb00254.x"
    },
    {
      "citation_id": "31",
      "title": "Design for mood: Twenty activity-based opportunities to design for mood regulation",
      "authors": [
        "Desmet"
      ],
      "year": "2015",
      "venue": "International Journal of Design"
    },
    {
      "citation_id": "32",
      "title": "Mood measurement with Pick-A-Mood: Review of current methods and design of a pictorial self-report scale",
      "authors": [
        "Desmet",
        "M Vastenburg",
        "N Romero"
      ],
      "year": "2016",
      "venue": "J. of Design Research",
      "doi": "10.1504/JDR.2016.079751"
    },
    {
      "citation_id": "33",
      "title": "Facial Reactions to Facial Expressions",
      "authors": [
        "U Dimberg"
      ],
      "year": "1982",
      "venue": "Psychophysiology",
      "doi": "10.1111/j.1469-8986.1982.tb02516.x"
    },
    {
      "citation_id": "34",
      "title": "Empathy, emotional contagion, and rapid facial reactions to angry and happy facial expressions: Empathy and rapid facial reactions",
      "authors": [
        "U Dimberg",
        "M Thunberg"
      ],
      "year": "2012",
      "venue": "PsyCh Journal",
      "doi": "10.1002/pchj.4"
    },
    {
      "citation_id": "35",
      "title": "Unconscious Facial Reactions to Emotional Facial Expressions",
      "authors": [
        "U Dimberg",
        "M Thunberg",
        "K Elmehed"
      ],
      "year": "2000",
      "venue": "Psychological Science",
      "doi": "10.1111/1467-9280.00221"
    },
    {
      "citation_id": "36",
      "title": "Investigating the Impact of Sound Angular Position on the Listener Affective State",
      "authors": [
        "K Drossos",
        "A Floros",
        "A Giannakoulopoulos",
        "N Kanellopoulos"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2392768"
    },
    {
      "citation_id": "37",
      "title": "Emotion Evoked by Texture and Application to Emotional Communication",
      "authors": [
        "Y Ebe",
        "H Umemuro"
      ],
      "year": "1995",
      "venue": "Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems",
      "doi": "10.1145/2702613.2732768"
    },
    {
      "citation_id": "38",
      "title": "Introduction to evolutionary computing",
      "authors": [
        "A Eiben",
        "J Smith"
      ],
      "year": "2015",
      "venue": "Introduction to evolutionary computing"
    },
    {
      "citation_id": "39",
      "title": "Darwin and facial expression: A century of research in review",
      "authors": [
        "P Ekman"
      ],
      "year": "2006",
      "venue": "Darwin and facial expression: A century of research in review"
    },
    {
      "citation_id": "40",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0030377"
    },
    {
      "citation_id": "41",
      "title": "Measuring facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1976",
      "venue": "Environmental Psychology and Nonverbal Behavior",
      "doi": "10.1007/BF01115465"
    },
    {
      "citation_id": "42",
      "title": "A Mixture of Personalized Experts for Human Affect Estimation",
      "authors": [
        "M Feffer",
        "O Rudovic",
        ") Oggi",
        "R Picard"
      ],
      "year": "2018",
      "venue": "Machine Learning and Data Mining in Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Evaluating affective features of 3D motionscapes",
      "authors": [
        "C Feng",
        "L Bartram",
        "B Riecke"
      ],
      "year": "2014",
      "venue": "Proceedings of the ACM Symposium on Applied Perception -SAP '14",
      "doi": "10.1145/2628257.2628264"
    },
    {
      "citation_id": "44",
      "title": "Influence of Tempo and Rhythmic Unit in Musical Emotion Regulation",
      "authors": [
        "A Fernández-Sotos",
        "A Fernández-Caballero",
        "J Latorre"
      ],
      "year": "2016",
      "venue": "Frontiers in Computational Neuroscience",
      "doi": "10.3389/fncom.2016.00080"
    },
    {
      "citation_id": "45",
      "title": "Social Desirability Bias and the Validity of Indirect Questioning",
      "authors": [
        "R Fisher"
      ],
      "year": "1993",
      "venue": "Journal of Consumer Research",
      "doi": "10.1086/209351"
    },
    {
      "citation_id": "46",
      "title": "Cross-Cultural Differences in Mental Representations of Time: Evidence From an Implicit Nonlinguistic Task",
      "authors": [
        "O Fuhrman",
        "L Boroditsky"
      ],
      "year": "2010",
      "venue": "Cognitive Science",
      "doi": "10.1111/j.1551-6709.2010.01105.x"
    },
    {
      "citation_id": "47",
      "title": "Analysis of cross-cultural color emotion",
      "authors": [
        "X.-P Gao",
        "J Xin",
        "T Sato",
        "A Hansuebsai",
        "M Scalzo",
        "K Kajiwara",
        "S.-S Guan",
        "J Valldeperas",
        "M Lis",
        "M Billger"
      ],
      "year": "2007",
      "venue": "Color Research & Application",
      "doi": "10.1002/col.20321"
    },
    {
      "citation_id": "48",
      "title": "The Mobile RehApp TM : An AR-based mobile game for ankle sprain rehabilitation",
      "authors": [
        "J Garcia",
        "K Navarro"
      ],
      "year": "2014",
      "venue": "IEEE 3nd International Conference on Serious Games and Applications for Health (SeGAH)",
      "doi": "10.1109/SeGAH.2014.7067087"
    },
    {
      "citation_id": "49",
      "title": "Adaptive user modelling in car racing games using behavioural and physiological data",
      "authors": [
        "T Georgiou",
        "Y Demiris"
      ],
      "year": "2017",
      "venue": "User Modeling and User-Adapted Interaction",
      "doi": "10.1007/s11257-017-9192-3"
    },
    {
      "citation_id": "50",
      "title": "The Development of Affective Responses to Modality and Melodic Contour",
      "authors": [
        "G Gerardi",
        "L Gerken"
      ],
      "year": "1995",
      "venue": "An Interdisciplinary Journal",
      "doi": "10.2307/40286184"
    },
    {
      "citation_id": "51",
      "title": "Impact of Tactile and Visual Feedback on Breathing Rhythm and User Experience in VR Exergaming",
      "authors": [
        "R Greinacher",
        "T Kojic",
        "L Meier",
        "R Parameshappa",
        "S Moller",
        "J.-N Voigt-Antons"
      ],
      "year": "2020",
      "venue": "Twelfth International Conference on Quality of Multimedia Experience (QoMEX)",
      "doi": "10.1109/QoMEX48832.2020.9123141"
    },
    {
      "citation_id": "52",
      "title": "Accuracy Assessment of ARKit 2 Based Gaze Estimation",
      "authors": [
        "R Greinacher",
        "J.-N Voigt-Antons"
      ],
      "year": "2020",
      "venue": "Human-Computer Interaction. Design and User Experience",
      "doi": "10.1007/978-3-030-49059-1_32"
    },
    {
      "citation_id": "53",
      "title": "Augmenting aesthetic chills using a wearable prosthesis improves their downstream effects on reward and social cognition",
      "authors": [
        "A Haar",
        "A Jain",
        "F Schoeller",
        "P Maes"
      ],
      "year": "2020",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-020-77951-w"
    },
    {
      "citation_id": "54",
      "title": "Intelligent emotion recognition system using brain signals (EEG)",
      "authors": [
        "J Harischandra",
        "M Perera"
      ],
      "year": "2012",
      "venue": "Biomedical Engineering and Sciences",
      "doi": "10.1109/IECBES.2012.6498050"
    },
    {
      "citation_id": "55",
      "title": "BioGlass: Physiological parameter estimation using a head-mounted wearable device",
      "authors": [
        "J Hernandez",
        "Y Li",
        "J Rehg",
        "R Picard"
      ],
      "year": "2014",
      "venue": "th International Conference on Wireless Mobile Communication and Healthcare -Transforming Healthcare Through Innovations in Mobile and Wireless Technologies (MOBIHEALTH)",
      "doi": "10.1109/MOBIHEALTH.2014.7015908"
    },
    {
      "citation_id": "56",
      "title": "Decoding Subjective Emotional Arousal during a Naturalistic VR Experience from EEG Using LSTMs",
      "authors": [
        "S Hofmann",
        "F Klotzsche",
        "A Mariola",
        "V Nikulin",
        "A Villringer",
        "M Gaebler"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)",
      "doi": "10.1109/AIVR.2018.00026"
    },
    {
      "citation_id": "57",
      "title": "Eye Movements During Everyday Behavior Predict Personality Traits",
      "authors": [
        "S Hoppe",
        "T Loetscher",
        "S Morey",
        "A Bulling"
      ],
      "year": "2018",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2018.00105"
    },
    {
      "citation_id": "58",
      "title": "How do new visual immersive systems influence gaming QoE? A use case of serious gaming with Oculus Rift",
      "authors": [
        "I Hupont",
        "J Gracia",
        "L Sanagustin",
        "M Gracia"
      ],
      "year": "2015",
      "venue": "Seventh International Workshop on Quality of Multimedia Experience (QoMEX)",
      "doi": "10.1109/QoMEX.2015.7148110"
    },
    {
      "citation_id": "59",
      "title": "A spectralanalytic approach to emotional responses evoked through picture presentation",
      "authors": [
        "R Huster",
        "S Stevens",
        "A Gerlach",
        "F Rist"
      ],
      "year": "2009",
      "venue": "International Journal of Psychophysiology",
      "doi": "10.1016/j.ijpsycho.2008.12.009"
    },
    {
      "citation_id": "60",
      "title": "A BDI approach to infer student's emotions in an intelligent learning environment",
      "authors": [
        "P Jaques",
        "R Vicari"
      ],
      "year": "2007",
      "venue": "Computers & Education",
      "doi": "10.1016/j.compedu.2005.09.002"
    },
    {
      "citation_id": "61",
      "title": "Gesture-Based Affective Computing on Motion Capture Data",
      "authors": [
        "A Kapur",
        "A Kapur",
        "N Virji-Babul",
        "G Tzanetakis",
        "P Driessen"
      ],
      "year": "2005",
      "venue": "Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "62",
      "title": "Lucid Loop: A Virtual Deep Learning Biofeedback System for Lucid Dreaming Practice",
      "authors": [
        "A Kitson",
        "S Dipaola",
        "B Riecke"
      ],
      "year": "2019",
      "venue": "Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3290607.3312952"
    },
    {
      "citation_id": "63",
      "title": "Identifying key factors for improving ICA-based decomposition of EEG data in mobile and stationary experiments",
      "authors": [
        "M Klug",
        "K Gramann"
      ],
      "year": "2020",
      "venue": "European Journal of Neuroscience",
      "doi": "10.1111/ejn.14992"
    },
    {
      "citation_id": "64",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "Jong-Seok Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "65",
      "title": "Validity evaluation of a spatial memory task in virtual environments",
      "authors": [
        "S Koenig",
        "G Crucian",
        "A Duenser",
        "C Bartneck",
        "J Dalrymple-Alford"
      ],
      "year": "2011",
      "venue": "Validity evaluation of a spatial memory task in virtual environments"
    },
    {
      "citation_id": "66",
      "title": "BCILAB: A platform for brain-computer interface development",
      "authors": [
        "C Kothe",
        "S Makeig"
      ],
      "year": "2013",
      "venue": "Journal of Neural Engineering",
      "doi": "10.1088/1741-2560/10/5/056014"
    },
    {
      "citation_id": "67",
      "title": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2008",
      "venue": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual"
    },
    {
      "citation_id": "68",
      "title": "The emotions",
      "year": "1922",
      "venue": "The emotions",
      "doi": "10.1037/10735-000"
    },
    {
      "citation_id": "69",
      "title": "An EEG and Motion Capture Based Expressive Music Interface for Affective Neurofeedback",
      "authors": [
        "G Leslie",
        "R Picard",
        "S Lui"
      ],
      "year": "2015",
      "venue": "An EEG and Motion Capture Based Expressive Music Interface for Affective Neurofeedback",
      "doi": "10.13140/RG.2.1.4378.6081"
    },
    {
      "citation_id": "70",
      "title": "Self-regulating positive emotion networks by feedback of multiple emotional brain states using realtime fMRI",
      "authors": [
        "Z Li",
        "L Tong",
        "L Wang",
        "Y Li",
        "W He",
        "M Guan",
        "B Yan"
      ],
      "year": "2016",
      "venue": "Experimental Brain Research",
      "doi": "10.1007/s00221-016-4744-z"
    },
    {
      "citation_id": "71",
      "title": "Exploring colour in context using Virtual Reality: Does a room change how you feel?",
      "authors": [
        "R Lipson-Smith",
        "J Bernhardt",
        "E Zamuner",
        "L Churilov",
        "N Busietta",
        "D Moratti"
      ],
      "year": "2020",
      "venue": "Virtual Reality",
      "doi": "10.1007/s10055-020-00479-x"
    },
    {
      "citation_id": "72",
      "title": "Simple Motion Textures for Ambient Affect",
      "authors": [
        "M Lockyer",
        "L Bartram",
        "B Riecke"
      ],
      "year": "2011",
      "venue": "Computational Aesthetics in Graphics",
      "doi": "10.2312/COMPAESTH/COMPAESTH11/089-096"
    },
    {
      "citation_id": "73",
      "title": "Texture affects color emotion",
      "authors": [
        "M Lucassen",
        "T Gevers",
        "A Gijsenij"
      ],
      "year": "2011",
      "venue": "Color Research & Application",
      "doi": "10.1002/col.20647"
    },
    {
      "citation_id": "74",
      "title": "Blind separation of auditory event-related brain responses into independent components",
      "authors": [
        "S Makeig",
        "T.-P Jung",
        "A Bell",
        "D Ghahremani",
        "T Sejnowski"
      ],
      "year": "1997",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.94.20.10979"
    },
    {
      "citation_id": "75",
      "title": "Heart rate variability: Standards of measurement, physiological interpretation, and clinical use",
      "authors": [
        "M Malik",
        "J Bigger",
        "A Camm",
        "R Kleiger",
        "A Malliani",
        "A Moss",
        "P Schwartz"
      ],
      "year": "1996",
      "venue": "European Heart Journal",
      "doi": "10.1093/oxfordjournals.eurheartj.a014868"
    },
    {
      "citation_id": "76",
      "title": "Exploring EEG Characteristics to Identify Emotional Reactions under Videogame Scenarios",
      "authors": [
        "L Martínez-Tejada",
        "A Puertas-González",
        "N Yoshimura",
        "Y Koike"
      ],
      "year": "2021",
      "venue": "Brain Sciences",
      "doi": "10.3390/brainsci11030378"
    },
    {
      "citation_id": "77",
      "title": "A Mathematical Model Captures the Structure of Subjective Affect",
      "authors": [
        "A Mattek",
        "G Wolford",
        "P Whalen"
      ],
      "year": "2017",
      "venue": "A Mathematical Model Captures the Structure of Subjective Affect",
      "doi": "10.1177/1745691616685863"
    },
    {
      "citation_id": "78",
      "title": "FACETEQ interface demo for emotion expression in VR",
      "authors": [
        "I Mavridou",
        "J Mcghee",
        "M Hamedi",
        "M Fatoorechi",
        "A Cleal",
        "E Ballaguer-Balester",
        "E Seiss",
        "G Cox",
        "C Nduka"
      ],
      "year": "2017",
      "venue": "IEEE Virtual Reality",
      "doi": "10.1109/VR.2017.7892369"
    },
    {
      "citation_id": "79",
      "title": "Real-time neuroimaging and cognitive monitoring using wearable dry EEG",
      "authors": [
        "T Mullen",
        "C Kothe",
        "Y Chi",
        "A Ojeda",
        "T Kerth",
        "S Makeig",
        "T.-P Jung",
        "G Cauwenberghs"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/TBME.2015.2481482"
    },
    {
      "citation_id": "80",
      "title": "Fechner's Aesthetics Revisited",
      "authors": [
        "J Norman",
        "A Beers",
        "F Phillips"
      ],
      "year": "2010",
      "venue": "Seeing and Perceiving",
      "doi": "10.1163/187847510X516412"
    },
    {
      "citation_id": "81",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "1988",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "82",
      "title": "Power spectral density of heart rate variability as an index of sympatho-vagal interaction in normal and hypertensive subjects",
      "authors": [
        "M Pagani",
        "F Lombardi",
        "S Guzzetti",
        "G Sandrone",
        "O Rimoldi",
        "G Malfatto",
        "S Cerutti",
        "A Malliani"
      ],
      "year": "1984",
      "venue": "Journal of Hypertension. Supplement: Official Journal of the International Society of Hypertension"
    },
    {
      "citation_id": "83",
      "title": "An ecological valence theory of human color preference",
      "authors": [
        "S Palmer",
        "K Schloss"
      ],
      "year": "2010",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.0906172107"
    },
    {
      "citation_id": "84",
      "title": "Triggers of Fear: Perceptual Cues Versus Conceptual Information in Spider Phobia: Cues Versus Information in Spider Phobia",
      "authors": [
        "H Peperkorn",
        "G Alpers",
        "A Mühlberger"
      ],
      "year": "2014",
      "venue": "Journal of Clinical Psychology",
      "doi": "10.1002/jclp.22057"
    },
    {
      "citation_id": "85",
      "title": "",
      "authors": [
        "A Perkis",
        "C Timmerer",
        "S Baraković",
        "J Husić",
        "S Bech",
        "S Bosse",
        "J Botev",
        "K Brunnström",
        "L Cruz",
        "K Moor",
        "A Saibanti",
        "P De",
        "W Durnez",
        "S Egger-Lampl",
        "U Engelke",
        "T Falk",
        "A Hameed",
        "A Hines",
        "T Kojic",
        "D Kukolj",
        "S Zadtootaghaj"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "86",
      "title": "Exploring relationships between audio features and emotion in music",
      "authors": [
        "T Petri"
      ],
      "year": "2009",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/conf.neuro.09.2009.02.033"
    },
    {
      "citation_id": "87",
      "title": "Event-related EEG/MEG synchronization and desynchronization: Basic principles",
      "authors": [
        "G Pfurtscheller",
        "F Lopes Da Silva"
      ],
      "year": "1999",
      "venue": "Clinical Neurophysiology",
      "doi": "10.1016/S1388-2457(99)00141-8"
    },
    {
      "citation_id": "88",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/34.954607"
    },
    {
      "citation_id": "89",
      "title": "How Do Induced Affective States Bias Emotional Contagion to Faces? A Three-Dimensional Model",
      "authors": [
        "A Pinilla",
        "R Tamayo",
        "J Neira"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2020.00097"
    },
    {
      "citation_id": "90",
      "title": "Real-time EEG Source-mapping Toolbox (REST): Online ICA and source localization. 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
      "authors": [
        "L Pion-Tonachini",
        "Sheng-Hsiou",
        "Hsu",
        "S Makeig",
        "Tzyy-Ping",
        "Jung",
        "G Cauwenberghs"
      ],
      "year": "2015",
      "venue": "Real-time EEG Source-mapping Toolbox (REST): Online ICA and source localization. 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)",
      "doi": "10.1109/EMBC.2015.7319299"
    },
    {
      "citation_id": "91",
      "title": "Audiovisual integration of emotional signals from others' social interactions",
      "authors": [
        "L Piwek",
        "F Pollick",
        "K Petrini"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2015.00611"
    },
    {
      "citation_id": "92",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1982",
      "venue": "Social Science Information",
      "doi": "10.1177/053901882021004003"
    },
    {
      "citation_id": "93",
      "title": "Anger recognition in speech using acoustic and linguistic cues",
      "authors": [
        "T Polzehl",
        "A Schmitt",
        "F Metze",
        "M Wagner"
      ],
      "year": "2011",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2011.05.002"
    },
    {
      "citation_id": "94",
      "title": "Estimation of the Quality of Experience during Video Streaming from Facial Expression and Gaze Direction",
      "authors": [
        "S Porcu",
        "A Floris",
        "J.-N Voigt-Antons",
        "L Atzori",
        "S Moller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Network and Service Management",
      "doi": "10.1109/TNSM.2020.3018303"
    },
    {
      "citation_id": "95",
      "title": "The nature of mental states",
      "authors": [
        "H Putnam"
      ],
      "year": "1967",
      "venue": "Art, Mind, and Religion"
    },
    {
      "citation_id": "96",
      "title": "Integrated Approach to Personalized Procedural Map Generation Using Evolutionary Algorithms",
      "authors": [
        "W Raffe",
        "F Zambetta",
        "X Li",
        "K Stanley"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games",
      "doi": "10.1109/TCIAIG.2014.2341665"
    },
    {
      "citation_id": "97",
      "title": "EEG alpha activity reflects attentional demands, and beta activity reflects emotional and cognitive processes",
      "authors": [
        "W Ray",
        "H Cole"
      ],
      "year": "1985",
      "venue": "Science",
      "doi": "10.1126/science.3992243"
    },
    {
      "citation_id": "98",
      "title": "OpenViBE: An Open-Source Software Platform to Design, Test, and Use Brain-Computer Interfaces in Real and Virtual Environments",
      "authors": [
        "Y Renard",
        "F Lotte",
        "G Gibert",
        "M Congedo",
        "E Maby",
        "V Delannoy",
        "O Bertrand",
        "A Lécuyer"
      ],
      "year": "2010",
      "venue": "Presence: Teleoperators and Virtual Environments",
      "doi": "10.1162/pres.19.1.35"
    },
    {
      "citation_id": "99",
      "title": "Valence, arousal and dominance in the EEG during game play",
      "authors": [
        "B Reuderink",
        "C Mühl",
        "M Poel"
      ],
      "year": "2013",
      "venue": "International Journal of Autonomous and Adaptive Communications Systems",
      "doi": "10.1504/IJAACS.2013.050691"
    },
    {
      "citation_id": "100",
      "title": "Increased affect-arousal in VR can be detected from faster body motion with increased heart rate",
      "authors": [
        "P Robitaille",
        "M Mcguffin"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games",
      "doi": "10.1145/3306131.3317022"
    },
    {
      "citation_id": "101",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "102",
      "title": "From likely to likable: The role of statistical typicality in human social assessment of faces",
      "authors": [
        "C Ryali",
        "S Goffin",
        "P Winkielman",
        "A Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1912343117"
    },
    {
      "citation_id": "103",
      "title": "Cognitive, social, and physiological determinants of emotional state",
      "authors": [
        "S Schachter",
        "J Singer"
      ],
      "year": "1962",
      "venue": "Psychological Review",
      "doi": "10.1037/h0046234"
    },
    {
      "citation_id": "104",
      "title": "Cue utilization in emotion attribution from auditory stimuli",
      "authors": [
        "K Scherer",
        "J Oshinsky"
      ],
      "year": "1977",
      "venue": "Motivation and Emotion",
      "doi": "10.1007/BF00992539"
    },
    {
      "citation_id": "105",
      "title": "Combining Virtual Reality and Biofeedback to Foster Empathic Abilities in Humans",
      "authors": [
        "F Schoeller",
        "P Bertrand",
        "L Gerry",
        "A Jain",
        "A Horowitz",
        "F Zenasni"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2018.02741"
    },
    {
      "citation_id": "106",
      "title": "Enhancing human emotions with interoceptive technologies",
      "authors": [
        "F Schoeller",
        "A Haar",
        "A Jain",
        "P Maes"
      ],
      "year": "2019",
      "venue": "Physics of Life Reviews",
      "doi": "10.1016/j.plrev.2019.10.008"
    },
    {
      "citation_id": "107",
      "title": "Neo-Noumena: Augmenting Emotion Communication",
      "authors": [
        "N Semertzidis",
        "M Scary",
        "J Andres",
        "B Dwivedi",
        "Y Kulwe",
        "F Zambetta",
        "F Mueller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3313831.3376599"
    },
    {
      "citation_id": "108",
      "title": "Influence of perceptual cues and conceptual information on the activation and reduction of claustrophobic fear",
      "authors": [
        "Y Shiban",
        "H Peperkorn",
        "G Alpers",
        "P Pauli",
        "A Mühlberger"
      ],
      "year": "2016",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry",
      "doi": "10.1016/j.jbtep.2015.11.002"
    },
    {
      "citation_id": "109",
      "title": "Social conditioning and extinction paradigm: A translational study in virtual reality",
      "authors": [
        "Y Shiban",
        "J Reichenberger",
        "I Neumann",
        "A Mühlberger"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2015.00400"
    },
    {
      "citation_id": "110",
      "title": "Emotion",
      "authors": [
        "M Shiota",
        "J Kalat"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "111",
      "title": "Real-time support vector classification and feedback of multiple emotional brain states",
      "authors": [
        "R Sitaram",
        "S Lee",
        "S Ruiz",
        "M Rana",
        "R Veit",
        "N Birbaumer"
      ],
      "year": "2011",
      "venue": "NeuroImage",
      "doi": "10.1016/j.neuroimage.2010.08.007"
    },
    {
      "citation_id": "112",
      "title": "Negative arousal amplifies the effects of saliency in short-term memory",
      "authors": [
        "M Sutherland",
        "M Mather"
      ],
      "year": "2012",
      "venue": "Emotion",
      "doi": "10.1037/a0027860"
    },
    {
      "citation_id": "113",
      "title": "When room size matters: Acoustic influences on emotional responses to sounds",
      "authors": [
        "A Tajadura-Jiménez",
        "P Larsson",
        "A Väljamäe",
        "D Västfjäll",
        "M Kleiner"
      ],
      "year": "2010",
      "venue": "Emotion",
      "doi": "10.1037/a0018423"
    },
    {
      "citation_id": "114",
      "title": "Embodied auditory perception: The emotional impact of approaching and receding sound sources",
      "authors": [
        "A Tajadura-Jiménez",
        "A Väljamäe",
        "E Asutay",
        "D Västfjäll"
      ],
      "year": "2010",
      "venue": "Emotion",
      "doi": "10.1037/a0018422"
    },
    {
      "citation_id": "115",
      "title": "Self-Representation in Mediated Environments: The Experience of Emotions Modulated by Auditory-Vibrotactile Heartbeat",
      "authors": [
        "A Tajadura-Jiménez",
        "A Väljamäe",
        "D Västfjäll"
      ],
      "year": "2008",
      "venue": "CyberPsychology & Behavior",
      "doi": "10.1089/cpb.2007.0002"
    },
    {
      "citation_id": "116",
      "title": "Heart Rate Variability, Prefrontal Neural Function, and Cognitive Performance: The Neurovisceral Integration Perspective on Self-regulation, Adaptation, and Health",
      "authors": [
        "J Thayer",
        "A Hansen",
        "E Saus-Rose",
        "B Johnsen"
      ],
      "year": "2009",
      "venue": "Annals of Behavioral Medicine",
      "doi": "10.1007/s12160-009-9101-z"
    },
    {
      "citation_id": "117",
      "title": "Python for signal processing: Featuring IPython notebooks",
      "authors": [
        "J Unpingco"
      ],
      "year": "2014",
      "venue": "Python for signal processing: Featuring IPython notebooks"
    },
    {
      "citation_id": "118",
      "title": "Effects of color on emotions",
      "authors": [
        "P Valdez",
        "A Mehrabian"
      ],
      "year": "1994",
      "venue": "Journal of Experimental Psychology: General",
      "doi": "10.1037/0096-3445.123.4.394"
    },
    {
      "citation_id": "119",
      "title": "EmoVoice-A Framework for Online Recognition of Emotions from Voice",
      "authors": [
        "T Vogt",
        "E André",
        "N Bee"
      ],
      "year": "2008",
      "venue": "Perception in Multimodal Dialogue Systems"
    },
    {
      "citation_id": "120",
      "title": "Comparing Emotional States Induced by 360° Videos Via Head-Mounted Display and Computer Screen",
      "authors": [
        "J.-N Voigt-Antons",
        "E Lehtonen",
        "A Palacios",
        "D Ali",
        "T Kojic",
        "S Moller"
      ],
      "year": "2020",
      "venue": "Twelfth International Conference on Quality of Multimedia Experience (QoMEX)",
      "doi": "10.1109/QoMEX48832.2020.9123125"
    },
    {
      "citation_id": "121",
      "title": "Development and validation of brief measures of positive and negative affect: The PANAS scales",
      "authors": [
        "D Watson",
        "L Clark",
        "A Tellegen"
      ],
      "year": "1988",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.54.6.1063"
    },
    {
      "citation_id": "122",
      "title": "Affective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System",
      "authors": [
        "D Williams",
        "A Kirke",
        "E Miranda",
        "I Daly",
        "F Hwang",
        "J Weaver",
        "S Nasuto"
      ],
      "year": "2017",
      "venue": "ACM Transactions on Applied Perception",
      "doi": "10.1145/3059005"
    },
    {
      "citation_id": "123",
      "title": "Color and emotion: Effects of hue, saturation, and brightness",
      "authors": [
        "L Wilms",
        "D Oberfeld"
      ],
      "year": "2018",
      "venue": "Psychological Research",
      "doi": "10.1007/s00426-017-0880-8"
    },
    {
      "citation_id": "124",
      "title": "Brain-computer interfaces for communication and control",
      "authors": [
        "J Wolpaw",
        "N Birbaumer",
        "D Mcfarland",
        "G Pfurtscheller",
        "T Vaughan"
      ],
      "year": "2002",
      "venue": "Clinical Neurophysiology",
      "doi": "10.1016/S1388-2457(02)00057-3"
    },
    {
      "citation_id": "125",
      "title": "Outlines of psychology",
      "authors": [
        "W Wundt",
        "C Judd",
        "Trans"
      ],
      "year": "1897",
      "venue": "Williams and Norgate",
      "doi": "10.1037/12908-000"
    },
    {
      "citation_id": "126",
      "title": "Experience-Driven Procedural Content Generation",
      "authors": [
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.6"
    },
    {
      "citation_id": "127",
      "title": "Towards passive brain-computer interfaces: Applying brain-computer interface technology to human-machine systems in general",
      "authors": [
        "T Zander",
        "C Kothe"
      ],
      "year": "2011",
      "venue": "Journal of Neural Engineering",
      "doi": "10.1088/1741-2560/8/2/025005"
    }
  ]
}