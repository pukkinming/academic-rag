{
  "paper_id": "2509.01135v1",
  "title": "Matl-Dc: A Multi-Domain Aggregation Transfer Learning Framework For Eeg Emotion Recognition With Domain-Class Prototype Under Unseen Targets",
  "published": "2025-09-01T05:08:04Z",
  "authors": [
    "Guangli Li",
    "Canbiao Wu",
    "Zhehao Zhou",
    "Na Tian",
    "Zhen Liang"
  ],
  "keywords": [
    "EEG",
    "Emotion Recognition",
    "Prototype Representation",
    "Transfer Learning",
    "Unseen Target"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition based on electroencephalography (EEG) signals is increasingly becoming a key research hotspot in affective Brain-Computer Interfaces (aBCIs), and its core goal is accurate decoding and feature alignment of EEG signals. However, the current transfer learning model greatly depends on the source domain and target domain data, and emotion label noise hinder the practical application of emotion recognition. Therefore, we propose a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype under unseen targets (MATL-DC). We design the feature decoupling module to decouple class-invariant domain features from domain-invariant class features from shallow features. In the model training stage, the multi-domain aggregation mechanism based on the Maximum Mean Discrepancy (MMD) aggregates the domain feature space to form a superdomain, and extracts the domain prototype representation, which enhances the characteristics of emotional EEG signals. In each superdomain, we further extract the class prototype representation by class features. In addition, we adopt the pairwise learning strategy to transform the sample classification problem into the similarity problem between sample pairs, which effectively alleviates the influence of unavoidable label noise. It is worth noting that the target domain is completely unseen during the training process. In the inference stage, we use the trained domain prototypes and class prototypes for inference, and then realize emotion recognition. We rigorously validate it on the publicly available databases (SEED, SEED-IV and SEED-V). The results show that the accuracy of MATL-DC model is 84.70%, 68.11% and 61.08%, respectively. MATL-DC achieves comparable or even better performance than methods that rely on both source and target domains. Our work provides a promising solution for emotion recognition in unseen target domains. The source code is available at https://github.com/WuCB-BCI/MATL-DC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION is a mental state, which is a complex physio- logical and psychological reaction of human beings to external stimuli or internal activities  [1] . It not only affects people's emotions, thinking, and behavior, but also impacts their physical and mental health  [2] . Therefore, how to accurately describe and effectively recognize emotional states is an urgent problem to be solved. Affective computing is a rapidly developing interdisciplinary research field, and emotional state recognition is the key content of affective computing. In previous studies, emotion recognition has primarily relied on external cues, such as voice  [3] , facial expressions  [4] , and body movements  [5] , as well as internal physiological signals, including electrocardiography (ECG)  [6]  and electroencephalography (EEG)  [7] . Compared with external cues, emotion recognition based on physiological signals (EEG) has the characteristics of being difficult to disguise, having excellent real-time performance and strong objectivity. It has received increasing attention from researchers in different fields such as computer science, neuroscience, psychology  [8] [9] .\n\nCurrently, deep transfer learning methods have achieved great success in the field of emotion recognition  [10] . As shown in Fig.  1 , transfer learning strategy divide data into source domain (labeled data with known distribution) and target domain (unlabeled data with unknown distribution), and transfer the knowledge and features learned in the source domain to the learning process of the target domain. One of the key challenges of EEG-based deep transfer learning methods is to alleviate individual variability  [11] -  [15]  and improve feature-invariant representations  [16] -  [18] . For exam-ple：Yang et al.  [19]  proposed spectral-spatial attention alignment multi-source domain adaptation (S 2 A 2 -MSD), which constructs domain attention to represent affective cognition attributes in spatial and spectral domains and utilizes domain consistent loss to align them between domains, and further learning abundant domain-invariant features. Chen et al.  [20]  propose the multi-source marginal distribution adaptation (MS-MDA) for EEG emotion recognition, which takes both domain-invariant and domain-specific features into consideration. MS-MDA is used to solve the limitation that multiple EEG data as a single source domain fails to satisfy the assumption of domain adaptation that the source has a certain marginal distribution. Zhao et al.  [21]  proposed a plugand-play domain adaptation (PPDA) method for dealing with the inter-subject variability. PPDA method divide EEG representations into private components specific to each subject and shared emotional components that are universal to all subjects, which reduces the model's dependence on large amounts of EEG data.\n\nHowever, (1) the transfer learning paradigm is based on the assumption of model parameter transferability, which believes that there is potential reuse value of network architecture between different tasks, and cross-domain knowledge transfer is realized through parameter transfer or structure sharing. However, this needs to simultaneous rely on the data of the source domain and the target domain at the same time, which means that the model needs to be retrained when it is applied to a new subject, which greatly increases the cost of model training. In addition, the model is also prone to being affected by the data preferences of the target domain, resulting in inflated performance.  (2)  Previous studies have highlighted that EEG signals are highly subject dependent, with significant differences in Emotion perception and affective expression across individuals  [22]    [23] . These differences are further reflected in the neural mechanisms involved in emotion regulation, increasing the complexity of the emotion recognition task. Furthermore, we highlighting the need to develop innovative methods that adapt to diverse individual EEG patterns while maintaining stable performance. (3) Currently, EEG emotion experiments are basically induced by video. Individuals may have different subjective feelings for the same emotional stimulus, and their physiological arousal patterns often show low correlation with self-reports，which brings inevitable label noise to emotion labeling. Traditional pointwise learning strategies usually transform a single sample into a multi-class classification problem in the classification task. Compared with pointwise learning, the pairwise learning strategy can model the correlation between samples, which is less dependent on labels and has better robustness  [24]    [25] .\n\nTherefore, to address the aforementioned triple challenge of data dependence, individual differences and label noise interference, we proposed Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain and Class prototype (MATL-DC). First, we designed a feature decoupling module to separate domain features (representing individual differences in subjects) and class features (representing emotional commonalities across subjects). Then, Maximum Mean Difference (MMD) was introduced to quantify the discrepancy of feature distribution among subjects to realize the multi-domain aggregation mechanism and form superdomains, which fully exploits the potential distribution commonality among the subject groups while preserving the individual specificity, so that the MATL-DC model can enhance the generalization ability across subjects by using the shared features in the superdomains. Furthermore, we proposed a prototype representation adaptive update mechanism to adaptively fuse the domain class prototype information of historical training cycles. Finally, to reduce the effect of label noise, classification is described as a pairwise learning task. It is noteworthy that the target domain is completely unseen in the training process, which breaks through the dependence of traditional deep transfer learning on target domain data. Overall, the main contributions of this paper are summarized as follows:\n\n• We proposed a Multi-domain Aggregation Transfer\n\nLearning framework for EEG emotion recognition with",
      "page_start": 1,
      "page_end": 13
    },
    {
      "section_name": "Feature Extraction Sample Datas",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Source Domain",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Classification",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Feature & Parameter",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Targer Domain",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Knowledge Transfer",
      "text": "Feature & Parameter Clustering Clustering Fig.  1 : A simple framework for transfer learning strategies. Transfer learning aims to extract knowledge from one or more source domain and applies the knowledge to a target domain. Domain-Class prototype (MATL-DC), which separates domain and class features from shallow features, and the individual variability of EEG signals is conceptualized as a feature shift resulting from the interaction between these features.\n\n• We designed a Multi-Domain Aggregation mechanism to aggregate the domain feature space, and further adopt an adaptive prototype update method to compute domain prototype and class prototype representations, which represent the basic properties of class-invariant domain features and domain-invariant class features. • We use three publicly available databases (SEED, SEED-IV and SEED-V) for rigorous cross-subject validation.\n\nThe results show that even though the target domain is unseen, MATL-DC still achieves comparable or even better performance than deep transfer learning models that rely on target domain data. In addition, we thoroughly analyze the model's parameters and feature visualizations to deepen our understanding of the model and results. The rest of this article is arranged as follows: We briefly described the background to the model in Sec.II. Then in Sec.III, we introduce the concrete implementation of our proposed MATL-DC in detail. Our experimental results was described in Sec.IV. Finally, We discuss our model performance and conclusions in Sec.V.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A. Non-Deep-Learning-Based Emotion Recognition",
      "text": "In the early research on emotion recognition, researchers mainly relied on non-deep learning methods to extract emotional EEG features and recognize different emotions  [26] . These studies have yielded valuable insights and paved the way for the development of affective brain-computer interface (aBCI) systems. For example:\n\nWang et al.  [27]  proposed the minimum redundancy maximum relevance (MRMR) method to extract common critical features across subjects, and introduces an emotion recognition system based on EEG signals. The results show that the emotional state can be recognized by using frequency domain features and support vector machine (SVM). Duan et al.  [28]  proposed a new effective EEG feature named differential entropy (DE) to represent the characteristics associated with emotional states, and further confirmed that DE is more suitable for emotion recognition than the traditional feature. It is also confirmed that EEG signals on frequency band Gamma relates to emotional states more closely than other frequency bands. Mohammadi et al.  [29]  proposed a method for classifying human emotions using machine learning models and extracting discrete wavelet features from EEG signals, and considered a significant band of EEG with a reduced frontal electrode to get a better results. Liu et al.  [30]  proposed a novel emotional EEG feature extraction method: kernel Eigen-emotion pattern (KEEP), And an adaptive SVM is also proposed to deal with the learning problem in imbalanced emotional EEG datasets. Results show that KEEP gives much better classification results than the widely-used EEG frequency band power features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Deep Transfer Learning Based Emotion Recognition",
      "text": "In recent years, with the progress of deep learning theory and technology, a large number of transfer algorithms based on deep learning have been introduced to enhance the performance and generalization ability of the model, and have been widely used in the field of EEG-based aBCI  [31] .\n\nZhou et al.  [32]  proposed an EEG-based Emotion Style Transfer Network (E2STN) to obtain EEG representations that contain the content information of source domain and the style information of target domain, which is called stylized emotional EEG representations, and this representations are helpful for discriminative prediction. Li et al.  [33]  proposed a novel neural network model, called bi-hemisphere domain adversarial neural network (BiDANN) model, for EEG emotion recognition. It contains a global and two local domain discriminators that work adversarially with a classifier to learn discriminative emotional features for each hemisphere. He et al.  [34]  proposed a novel domain adaptation strategy called adversarial discriminative-temporal convolutional networks (AD-TCNs), which can ensure the invariance of the representation of feature graphs in different domains and fill in the differences between different domains. Li et al.  [35]  proposed a joint domain adaptation network, which are optimized by minimizing the classification error on the source while making the source and the target similar in their latent representations. Luo et al.  [36]  proposed a novel Wasserstein generative adversarial network domain adaptation (WGANDA) framework for building cross-subject EEG-based emotion recognition models. The proposed framework consists of GANs-like components and a two-step training procedure with pre-training and adversarial training.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Prototype Learning",
      "text": "The core concept of prototype learning is that each class is represented by a prototype representation (a feature vectors with representative features), and the task recognition is performed by evaluating the proximity or similarity between the sample features and the prototype representation  [37] .\n\nWang et al.  [38]  incorporated an efficient prototype-based data representation to learn a more discriminative embedded feature space for EEG-based emotion recognition, in which the Euclidean metric was adopted to assess the associations between the data samples and all the selected prototypes. Zhou et al.  [39]  proposed a novel transfer learning framework with Prototypical Representation based Pairwise Learning (PR-PL). The discriminative and generalized EEG features are learned for emotion revealing across individuals and the emotion recognition task is formulated as pairwise learning for improving the model tolerance to the noisy labels. Wang et al.  [40]  proposed a prototype-based domain adaptation SPD matrix network (daSPDnet) that can successfully capture an intrinsic emotional representation shared between different subjects, which jointly exploits feature adaptation with distribution confusion and sample adaptation with centroid alignment. Guo et al.  [41]  proposed and implement a novel neural network algorithm based on modifying the emotional neural network (EmNN) model to unify the prototype-and adaptive-learning theories, as well as apply the proposed model to two real-life challenging emotion tasks, static hand-gesture recognition and face recognition.\n\nHowever, the current most emotion recognition models need to rely on both source domain and target domain data, which greatly increases the practical application cost, and the robustness and generalization ability are limited. Therefore, the model framework based on unseen target domains emphasizes the importance of its development.  Suppose the source domain and the target domain are represented as (S, T). In the source domain, we define the sample distribution of each subject as an independent domain. Each subject has a domain space, so the source domain is composed of multiple domain Spaces. The source domain is defined as\n\n, where N d represents the number of subjects in the source domain. EEG samples for each of the subjects, defined as S n = {x i n , y i n } Ns i=1 , where x i n represents the i th sample data of the n th subject, y i n is the corresponding emotion label, and N s indicates the sample capacity of the n th subject in the source domain. Target domain sample defined as T = {x i t , y i t } Nt i=1 , where N t represents the target domain of EEG sample capacity. The target domain is completely unseen during training. For ease of inquiry, common notations and descriptions are displayed in TABLE I.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Feature Decoupling",
      "text": "As shown in Fig.  2 , assuming that the shallow feature extractor f g (•) is used to extract the shallow features of the sample, we introduce a domain feature decoupler f d (•) and a class feature decoupler f c (•) to decouple the semantic information in these shallow features, thereby obtaining the class-invariant domain features\n\n), respectively. To ensure that the feature decoupler can accurately separate two types of features, we utilize a domain discriminator D d (•) and a class discriminator D c (•) to identify the domain or category to which the features belong. Specifically, our goal is to enable the domain discriminator to accurately identify the domain to which the domain features belong, rather than using the domain features to identify the category to which they belong. On the contrary, the class discriminator can accurately identify the category to which the class feature belongs, but cannot use the class feature to identify the domain to which it belongs. By employing a decoupler and discriminator through adversarial training for successful decoupling of domain-specific and class-specific characteristics.\n\nTo achieve this goal, we perform adversarial training of the decoupler and discriminator. Before the class-specific features are fed into the domain discriminator and the domain-specific features are fed into the class discriminator, we pass through a Gradient Reversal Layer (GRL) to facilitate adversarial training. Specifically, the model maintains the input features unchanged during forward propagation, where data flows directly fed the network. During backpropagation, the GRL  c to which the sample belongs. Here, h(•) represent the bilinear transformation to capture the most relevant domain space (Eq.13). d cos (•) represents the cosine similarity calculation (Eq.15). multiplies the gradients by a negative coefficient, reversing their direction to enable adversarial training. We use the Binary Cross-Entropy (BCE) loss function to optimize the discriminator, converting the multi-class problem into several independent binary classification tasks. Suppose the GRL layer is represented by R(•), x d and x c represent domain features and class features, respectively, then the class discriminator loss function L cls is defined as:\n\nhere, ℓ BCE (•) represents the BCE loss function, y c represents the true label of the class feature. Similarly, the domain discriminator loss function L dom is defined as:\n\nTherefore, the objective function L F D of the feature decoupling module is defined as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Multi-Domain Aggregation",
      "text": "We defined the distribution of each subject sample as an independent domain. In the feature decoupling module, the domain discriminator identifies multiple domain (S 1:n ) based on the number of subjects. However, we argue that EEG samples from different subjects may not be exclusively associated with a single domain, but rather exhibit latent connections to multiple domains. Furthermore, as the scale of the subjects expands, too many domains will lead to a significant increase in computational costs, thereby reducing the operational and response efficiency of the model. Therefore, we need to aggregate the domains with potential connections to form a superdomain.\n\nSpecifically, we need to quantify the discrepancy between the domain feature distributions of different domains for multidomain aggregation. To achieve this goal, we adopt the Maximum Mean Discrepancy (MMD) strategy as the discrepancy measure. MMD is a non-parametric kernel method for measuring the discrepancy between two probability distributions, which can implicitly capture complex discrepancy in highdimensional feature Spaces. Suppose two independent sample sets, X and Y , follow the P and Q distribution respectively (X ∼ P, Y ∼ Q), the MMD is defined as:\n\nhere, E x and E y represent the Mathematical Expectation of X and Y , respectively. HK denote the Reproducing Kernel Hilbert Space (RKHS) mapped by the feature kernel κ, and ϕ(•) represents this mapping operation. The feature kernel κ can be interpreted as the distance function, which defines the distance measure between X and Y in the RKHS. This distance measure can be obtained by computing the inner-product of samples, assume domain samples are denoted as X = {x i } n i=1 and Y = {y j } m j=1 , denoted as κ(x, y) = ⟨ϕ(x), ϕ(y)⟩. Due to the non-stationary characteristics and high-dimensional spatio-temporal characteristics of EEG signals, we introduce Gaussian-Kernel function (GKF) to replace the traditional linear kernel function. GKF can map the data into an RKHS of infinite-dimensions to obtain the inner-product result without explicitly computing the mapping function ϕ(•). GKF is define as:\n\nhere, ∥•∥ 2 denotes the L2 norm. σ represent the bandwidth parameter. Therefore, Eq.4 can be transformed using GKF as:\n\nThe model is iteratively optimized, and obtained the MMD distance between each domain and all other domains, and form an MMD distance vector, which provides distance reference for multi-domain aggregation. In summary, the distance vector for the i th domain is defined as:\n\nhere, N represents the number of domains in the source domain. The coordinate positions of V i reflect the distribution characteristics of this domain. Next, we perform multi-domain aggregation based V to form a certain number of superdomains. K-means++ is a traditional clustering algorithm, which clusters by calculating the Euclidean distance between samples, and can avoid the extreme situation that the clustering centroid is too concentrated. Considering the characteristics of EEG signals and the aggregation performance of EEG data in the feature space, we innovated to use MMD distance to replace the traditional Euclidean distance in the K-means++, and formed a new K-means++ algorithm based on MMD. As shown in Fig.  2 , a superdomain is aggregated from multiple domains. Assuming the superdomain defined as S k , the clustering algorithm is defined as:\n\nhere, K denotes the number of superdomains. Eventually, we repartition the source domain into",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Adaptive Prototype Updating",
      "text": "For each superdomain, we assume the existence of a domain prototype representation, which embodies the essential characteristics of the domain features belonging to that superdomain. Samples within the same superdomain are distributed around prototype representation. From the perspective of probability distributions, the prototype representation can be regarded as the centroid of all domain features in superdomain. Similarly, within each superdomain, features corresponding to different emotional states can derive class prototype representations. The class prototype representation capture the fundamental attributes of each EEG emotion state in the superdomain. Therefore, extracting domain & class prototype representation can improve the robustness and generalization performance of the model. Assume that the domain feature and superdomain label in the superdomain S k are represented as {x i d , k}\n\n|X k d | denotes the number of domain features in the superdomain. Therefore, the domain prototype representation is defined as:\n\nSimilarly, in each superdomain space, the class features and labels are denoted as {x i c , m}\n\ni=1 , and |X m c | is denoted as the number of class features belonging to category m. Thus, the class prototype representation within each superdomain space are defined as:\n\nDuring the training phase, As shown in the Prototype Computing module of Fig.  2 , domain prototypes and class prototype representations are continuously updated through iterations to seek optimal parameter. However, in the process of model iteration, the obtained prototype representation may have large deviation even for two adjacent training moments. To avoid the possible instability of prototype calculation at different moments, we adopt an adaptive prototype update mechanism, and the prototype representation calculation at time t is affected by the time t -1, so as to improve the stability of model training. The update process is defined as：\n\nhere, α is the weight update parameter. In the early stage of training, the feature distribution changes drastically, and the prototype representation needs to be updated quickly to adapt to this dynamic adjustment. In the later stage of training, as the model tends to converge and the feature distribution tends to be stable, the update rate of the prototype should slow down. Thus, α is defined as：\n\nhere, α h and α l represent the upper and lower threshold of α, respectively. p is the hyperparameter that controls the rate of decay. When p > 1, α will decay rapidly at the beginning, and as the model continues to train, the decay rate will gradually slow down until we reach our desired goal.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Prototype Inference",
      "text": "In the training phase, MATL-DC model obtains the optimal domain prototype and the class prototype in each superdomain space by the adaptive update method. It is worth noting that the MATL-DC is completely unseen to the target domain samples during training. As shown in Eq.3, in the model inference stage, we first extract the corresponding domain features and class features in target domain. Next, we perform domain prototype inference to determine the superdomain space by trained domain prototype representation. Then, in the selected superdomain space, class prototype inference is performed through the trained class prototypes to determine the emotional state labels. Specifically, we introduce a bilinear transformation to quantify the interaction between domain features x d and the domain prototype µ k d . Assume that the bilinear transformation expressed as h(•), which is defined as:\n\nhere, (•) T denotes the transpose operation. θ is a trainable, randomly initialized bilinear transformation matrix that is not limited by positive definiteness or symmetry. θ is beneficial to enhance the feature representation ability. Overall, the interaction between x d and all superdomains µ k d (k = 1 : K) are represented as a vector, and the coordinate with the highest value corresponds to the superdomain to which it belongs, which is defined as:\n\nhere, sof tmax represent normalization function. Then, in the selected superdomain, we adopt cosine similarity strategy to calculate the similarity between class prototypes µ m c (m = 1 : M ) and class features x i c of samples. The µ m c (m = 1 : M ) and x i c form an interaction relationship to determine the class to which the sample belongs, which is defined as:\n\nhere, d cos (•) represents the cosine similarity calculation, and the coordinate with the highest similarity is the emotional state category.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Pairwise Learning",
      "text": "We adopt a pairwise learning strategy instead of the traditional pointwise learning to solve the problem of label noise and enhance the resistance of the model to label noise. Unlike pointwise learning, pairwise learning focuses on the latent relationship between samples and captures the relative association between them. Specifically, we transform the classification problem of samples into a similarity problem between samples. The pairwise learning loss function is defined as follows.\n\nhere, N b represents the number of samples in each batch during model training. r ij indicates whether the samples x i and x j belong to the same emotion category, and the value is 0 or 1. r ij = 1 indicates that the sample pairs belongs to the same emotion category, and r ij = 0 indicates that they do not belong to the same emotion category. G(•) represents the similarity measure between samples pair, which is also the probability that samples x i and x j belong to the same emotion category. The value of G(•) ranges from 0 to 1. The closer the value is to 1, the more likely the sample pair is to belong to the same emotion category, and vice versa. Overall, G is defined as:\n\nhere, P i and P j are obtained by Eq.15, and (•) denotes the inner product operation. || • || 2 denotes the L2 constraint. In addition, to avoid redundant feature extraction, we introduce soft regularization R with a weight parameter of β. Finally, the objective function of our proposed PL MDCP framework is defined as:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Databases And Data Processing",
      "text": "We rigorously validated the proposed MATL-DC framework using three established public databases: SEED  [45] , SEED-IV  [46]  and SEED-V  [47] , which are extensively utilized in affective computing research. The SEED dataset comprising 15 subjects, each subject completed 3 experimental sessions on separate dates. Each session contained 15 trials and include 3 emotions: positive, neutral and negative, induced by video clips and simultaneously recorded EEG signals. The SEED-IV dataset consists of 15 subjects, each of whom participated in 3 experimental sessions on different dates. Each session consists of 24 trials and contains four emotions: neutral, sad, fear and happy. In the SEED-V dataset, a total of 16 subjects participated, and each subject completed 3 sessions. Each session consisted of 15 trials with five emotions: happy, neutral, sad, disgust and fear.\n\nThe EEG signals in the database are preprocessed as follows: First, the EEG signals are downsampled to a 200 Hz sampling rate, and noise is manually removed, such as electromyography (EMG) and electrooculography (EOG). The denoised data is then filtered using a bandpass filter with a range of 0.3 Hz to 50 Hz. Second, for each experiment, the signals are segmented using a 1-s window, and differential entropy (DE  [48] ) features, representing the logarithmic energy spectrum of specific frequency bands, are extracted based on five frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-12Hz), Beta (14-30Hz), and Gamma (31-50Hz), resulting in 310 features for each EEG segment (5 frequency bands × 62 channels). Finally, a Linear Dynamic System (LDS) is applied to smooth all obtained features, leveraging the temporal dependency of emotional changes to filter out EEG components unrelated to emotions and those contaminated by noise.  [49]  The EEG processing procedure adheres to the same standards as previous studies to enable fair comparisons with models presented in previous literature.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Implementation Results",
      "text": "In the proposed model, the architecture of the shallow feature extractor f g (•) is designed as: input layer (310)hidden layer (64) -LeakyRelu activation (α = 0.01) -hidden layer (64) -nLeakyRelu activation (α = 0.01) -output layer  (64)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Experiment Protocols",
      "text": "To thoroughly evaluate the model's performance and enable a comprehensive comparison with existing methods, we adopt two different cross-validation protocols. It is worth noting that the target domain data is completely unseen during the training process of the model. (1) Cross-Subject Single-Session Leave-One-Subject-Out Cross-Validation, which is a widely used validation method in EEG-based emotion recognition tasks. In this protocols, one subject's signal sessions' data as the target domain, and the remaining subjects' signal sessions as the source domain. Keeping the same as the other research,  only data from the first session were used. We repeat training and validation until each subject is treated as the target domain, and the experimental results are averaged. (2) Cross-Subject Cross-Session Leave-One-Subject-Out Cross-Validation. To more closely simulate practical application scenarios, we use one subject's all sessions' data as the target and the remaining subjects' all sessions as the source. We repeat training and validation until each subject is treated as the target domain, and the experimental results are averaged. In the EEG-based emotion recognition task, this evaluation Protocol poses the greatest challenge to the validity of the model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Cross-Subject Single-Session Leave-One-Subject-Out Cross-Validation",
      "text": "We rigorously validate the proposed MATL-DC model and compare it with other state-of-the-art (SOTA) methods. The validation results for the SEED database are shown in TA-BLE II. The MATL-DC model shows significant performance advantages over traditional machine learning. The MATL-DC model achieves 84.70% accuracy, compared with the best machine learning model CORAL(71.48%), the performance is improved by 13.22%. In addition, MATL-DC still   It is worth noting that the target domain data is unseen during training of our proposed model. Other methods use the target domain data in the training process, but MATL-DC still achieves comparable or even slightly better performance than the methods that use the target domain data. These results prove that MATL-DC has strong robustness and generalization ability.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Cross-Subject",
      "text": "Cross-Session Leave-One-Subject-Out Cross-Validation Compared with cross-subject single-session, cross-subject cross-session not only considers the differences between subjects, but also integrates the differences between sessions.\n\nThe validation results for the SEED database are shown in TABLE V. The accuracy of MATL-DC model reaches 80.21%, which is is significantly better than traditional machine learning methods and deep learning methods. In the machine learning method, compared with the sub-optimal",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "F. Confusion Matrix",
      "text": "To qualitatively evaluate the recognition performance of MATL-DC for different emotion categories, we visualize the confusion matrices of MATL-DC on SEED, SEED-IV and SEED-V datasets. Here, the Y-axis represents the True label and the X-axis represents the predicted label.\n\nAs shown in Fig.  4 .(a)∼(d), on the SEED dataset, MATL-DC is similar to the other models, which has the best accuracy in recognizing positive emotions. In addition, MATL-DC has relatively excellent recognition performance, and the recognition accuracy of various emotions is all higher than 82%. In the DCORAL model, the performance discrepancy in identifying positive emotions and negative emotions is 10.4%, while the performance discrepancy in identifying different emotions in MATL-DC is only 4.03%. This indicates that the MATL-DC model has high stability and its performance will not fluctuate greatly, which further demonstrates that the MATL-DC has greatly robustness.\n\nThe confusion matrix on the SEED-IV dataset is shown in Fig.  4 .(e)∼(h). All the participating models performed poorly in recognizing the emotion of happy, while they all performed well in recognizing the other three emotions. When identifying neutral and sad emotions, compared to the suboptimal models DANN (65.96%) and DCORAL (59.56%), the performance of the MATL-DC model was improved by 4.36% and 11.59%, respectively. When identifying fear and happy emotions, the performance of MATL-DC was improved by 14.36% and 2.25% respectively compared with the suboptimal model DAN.\n\nAs shown in Fig.  5 .(a)∼(d), on the SEED-V dataset, the proposed MATL-DC model has weak performance in identifying happy emotions, and is easy to confuse happy emotions with neutral emotions. On the contrary, MATL-DC has the best performance in recognizing fear, sadness and nausea with 60.86%, 67.72% and 75.00%, respectively. Compared with the sub-optimal model, the performance of MATL-DC is improved by 4.79%, 7.46% and 11.45%, respectively.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "V. Discussion And Conclusion",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Visualization Of Domain And Class Features",
      "text": "To intuitively understand the influence of the domain and class features extracted by the MATL-DC model and the multidomain aggregation module, we performed T-SNE visualization of features based on the SEED database in order to clearly understand the evolution of features and prototypes during training.\n\nAs shown in Fig.  6  to fully explore the potential correlations between samples and subjects, as well as limited utilization of source domain data. There are no clear boundaries between domain features, and even a certain degree of overlap occurs. In addition, this also leads to an increase in computational cost.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "B. Ablation Experiment",
      "text": "In order to comprehensively evaluate the impact of each module in MATL-DC on the overall performance of the model, we conduct ablation experiments on MATL-DC based on the SEED dataset under the Cross-Subject Single-Session Leave-One-Subject-Out Cross-Validation strategy.\n\nThe results of Ablation experiments are shown in TABLE VIII. We remove domain prototype representation, and the model performance dropped by 5.75% when using only class",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Effect Of Noisy Labels",
      "text": "We effectively overcame the problem of label noise by pairwise learning strategy. In order to verify the robustness of the model under the pairwise learning and pointwise learning strategies and its dependence on the sample labels, we train the model using the source domain samples with manually added noise. Specifically, We replaced η% of the true labels in the source domain with random noise labels in a controlled manner, and evaluate the model on target domain samples. We set η to 5%, 10%, 20% and 30%. It is worth noting that the sample data of the target domain are completely unseen during training.\n\nAs shown in TABLE IX. In the pointwise learning strategy, the model's performance at different values η% is 76.73%, 75.89%, 73.45%, 71.84% and 70.04%, respectively. When the label noise rate η is 30%, the performance of the model decreases by 6.69%, with a significant decline in performance, which indicates that the pointwise learning strategy has limited robustness to label noise, and the model is susceptible to label noise. In the paired learning strategy, when the label noise rate η% gradually increases from 0% to 30%, the performance of the model decreases from 84.70% to 81.38%, and the overall performance decreases only by 3.32%. These results indicate that the pairwise learning strategy has a high tolerance to noisy labels. In addition, we observed that as η continues to increase, the discrepancy in performance between the two strategies of the model gradually widens. All these results show that the impact of label noise on model performance is limited, and our proposed MATL-DC model has excellent robustness and reliability.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "D. Number Of Superdomains",
      "text": "To evaluate the relationship between the value of the superdomain number K and the model performance in the multidomain aggregation mechanism, and to find the most appropriate K, we evaluated the changes in the model performance of MATL-DC under different K.\n\nAs shown in Fig.  8 , the blue vertical axis on the left represents the recognition accuracy of the model, and the red vertical axis on the right represents the standard deviation of the verification result. We gradually increased the value of K from 2 to 7, and the model performance changed significantly. When K = 4, the model performance reaches its peak at 84.7%±4.63%. Therefore, considering all factors, we set K to 4 to obtain the optimal model parameters. In addition, we observed that when K is too small, domain features with low similarity in feature distribution are forcibly clustered into the same superdomain, resulting in a decrease in model performance. When K is too large, although the sample features may be slightly discrepancy from the actual domain distribution, if clustering into a new superdomain, the boundary between the superdomains will be unclear or even overlapping, resulting model performance degradation, which is also illustrated by the feature visualization in Fig.  6 .(c)(f).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "E. Conclusion",
      "text": "This work proposes a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype (MATL-DC), which breaks through the dependence of traditional transfer learning frameworks on source and target domain. We designed a feature decoupling module to separate domain-class features. And a multi-domain aggregation mechanism was designed to adaptively obtain domain prototype and class prototype representations. In addition, MATL-DC addresses the negative impact of emotional label noise. MATL-DC has achieved advanced performance in several public databases, and provides a potential solution for emotion recognition under unseen target conditions.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A simple framework for transfer learning strategies.",
      "page": 2
    },
    {
      "caption": "Figure 2: The training phase of the MATL-DC framework. MATL-DC consists of three main modules, which are feature",
      "page": 4
    },
    {
      "caption": "Figure 3: The inferencding phase of the MATL-DC framework. The optimal domain prototype µd and class prototype µc obtained",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation.",
      "page": 9
    },
    {
      "caption": "Figure 5: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation.",
      "page": 10
    },
    {
      "caption": "Figure 6: T-SNE visualizations of domain features and domain prototype representations of the MATL-DC model without the",
      "page": 11
    },
    {
      "caption": "Figure 7: T-SNE visualizations of source and target class features of the MATL-DC model, showing the distribution of domain",
      "page": 12
    },
    {
      "caption": "Figure 8: Performance of the MATL-DC model under different",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "5Address Correspondence To:\njanezliang@szu.edu.cn."
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "Abstract—\nEmotion recognition based on electroencephalog-\nrecognition\nis\nthe\nkey\ncontent\nof\naffective\ncomputing.\nIn"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "raphy\n(EEG)\nsignals\nis\nincreasingly becoming\na key\nresearch"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "previous\nstudies,\nemotion\nrecognition\nhas\nprimarily\nrelied"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "hotspot\nin affective Brain-Computer Interfaces\n(aBCIs), and its"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "on external\ncues,\nsuch as voice\n[3],\nfacial\nexpressions\n[4],"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "core goal\nis accurate decoding and feature alignment of EEG"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "and body movements\n[5],\nas well\nas\ninternal physiological"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "signals. However, the current transfer learning model greatly de-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "signals,\nincluding\nelectrocardiography\n(ECG)\n[6]\nand\nelec-\npends on the source domain and target domain data, and emotion"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "label noise hinder the practical application of emotion recogni-\ntroencephalography (EEG)\n[7]. Compared with external cues,"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "tion. Therefore, we propose a Multi-domain Aggregation Transfer"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "emotion\nrecognition\nbased\non\nphysiological\nsignals\n(EEG)"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "Learning framework for EEG emotion recognition with Domain-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "has\nthe\ncharacteristics of being difficult\nto disguise, having"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "Class prototype under unseen targets (MATL-DC). We design the"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "excellent\nreal-time performance and strong objectivity.\nIt has"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "feature decoupling module\nto decouple\nclass-invariant domain"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "received\nincreasing\nattention\nfrom researchers\nin\ndifferent\nfeatures from domain-invariant class features from shallow fea-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "tures. In the model training stage, the multi-domain aggregation\nfields such as computer science, neuroscience, psychology [8]"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "mechanism based on the Maximum Mean Discrepancy (MMD)"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "[9]."
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "aggregates the domain feature space to form a superdomain, and"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "Currently,\ndeep\ntransfer\nlearning methods\nhave\nachieved"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "extracts the domain prototype representation, which enhances the"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "great\nsuccess\nin\nthe field\nof\nemotion\nrecognition\n[10]. As"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "characteristics of emotional EEG signals.\nIn each superdomain,"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "shown\nin Fig.1,\ntransfer\nlearning\nstrategy\ndivide\ndata\ninto\nwe\nfurther\nextract\nthe\nclass prototype\nrepresentation by\nclass"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "features. In addition, we adopt\nthe pairwise learning strategy to\nsource\ndomain\n(labeled\ndata with\nknown\ndistribution)\nand"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "transform the\nsample\nclassification problem into the\nsimilarity"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "target domain (unlabeled data with unknown distribution), and"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "problem between sample pairs, which effectively alleviates\nthe"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "transfer\nthe\nknowledge\nand\nfeatures\nlearned\nin\nthe\nsource"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "influence of unavoidable label noise.\nIt\nis worth noting that\nthe"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "domain\nto\nthe\nlearning\nprocess\nof\nthe\ntarget\ndomain. One"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "target domain is completely unseen during the training process."
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "of\nthe key challenges of EEG-based deep transfer\nlearning\nIn the\ninference\nstage, we use\nthe\ntrained domain prototypes"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "and\nclass\nprototypes\nfor\ninference,\nand\nthen\nrealize\nemotion\nmethods\nis\nto\nalleviate\nindividual\nvariability\n[11]–[15]\nand"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "recognition. We\nrigorously validate\nit on the publicly available"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "improve feature-invariant representations [16]–[18]. For exam-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "databases\n(SEED,\nSEED-IV and\nSEED-V). The\nresults\nshow"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "ple：Yang et al. [19] proposed spectral-spatial attention align-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "that\nthe accuracy of MATL-DC model\nis 84.70%, 68.11% and"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "ment multi-source\ndomain\nadaptation\n(S2A2-MSD), which"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "61.08%,\nrespectively. MATL-DC achieves\ncomparable\nor\neven"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "constructs domain attention to represent\naffective\ncognition\nbetter performance than methods\nthat rely on both source and"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "target\ndomains. Our work\nprovides\na\npromising\nsolution\nfor\nattributes\nin\nspatial\nand\nspectral\ndomains\nand\nutilizes\ndo-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "emotion recognition in unseen target domains. The source code"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "main\nconsistent\nloss\nto\nalign\nthem between\ndomains,\nand"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "is available at https://github.com/WuCB-BCI/MATL-DC."
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "further\nlearning\nabundant\ndomain-invariant\nfeatures. Chen"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "Index Terms—EEG; Emotion Recognition; Prototype Repre-\net\nal.\n[20]\npropose\nthe multi-source marginal\ndistribution"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "sentation; Transfer Learning; Unseen Target."
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "adaptation (MS-MDA)\nfor EEG emotion recognition, which"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "takes both domain-invariant and domain-specific features into"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "consideration. MS-MDA is used to solve the limitation that"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "I.\nINTRODUCTION"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "multiple EEG data as a single source domain fails to satisfy"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "the\nassumption of domain adaptation that\nthe\nsource has\na\nstate, which is a complex physio-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "certain marginal distribution. Zhao et al. [21] proposed a plug-\nand psychological\nreaction of human beings\nto\nE MOTION is a mental"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "and-play domain adaptation (PPDA) method for dealing with\nexternal\nstimuli or\ninternal\nactivities\n[1].\nIt not only affects"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "people’s\nemotions,\nthinking,\nand behavior, but\nalso impacts\nthe inter-subject variability. PPDA method divide EEG repre-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "sentations into private components specific to each subject and\ntheir physical and mental health [2]. Therefore, how to accu-"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "shared emotional components that are universal to all subjects,\nrately describe and effectively recognize emotional states is an"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "which reduces\nthe model’s dependence on large amounts of\nurgent problem to be solved. Affective computing is a rapidly"
        },
        {
          "Email: 1guangli010@hut.edu.cn, 2wucanbiao@m.scnu.edu.cn, 3463805331@qq.com, 4tiann7@mail2.sysu.edu.cn,": "EEG data.\ndeveloping interdisciplinary research field, and emotional state"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": ""
        },
        {
          "2": "Sample Datas\nFeature Extraction\nClassification"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Feature & \nClustering"
        },
        {
          "2": ""
        },
        {
          "2": "Parameter"
        },
        {
          "2": "Source Domain"
        },
        {
          "2": ""
        },
        {
          "2": "Knowledge Transfer"
        },
        {
          "2": "Targer Domain"
        },
        {
          "2": ""
        },
        {
          "2": "Feature & \nClustering"
        },
        {
          "2": "Parameter"
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Fig. 1: A simple\nframework for\ntransfer\nlearning strategies."
        },
        {
          "2": "Transfer learning aims to extract knowledge from one or more"
        },
        {
          "2": "source domain and applies the knowledge to a target domain."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "Domain-Class\nprototype\n(MATL-DC), which\nseparates"
        },
        {
          "2": ""
        },
        {
          "2": "domain and class features from shallow features, and the"
        },
        {
          "2": ""
        },
        {
          "2": "individual variability of EEG signals\nis\nconceptualized"
        },
        {
          "2": ""
        },
        {
          "2": "as a feature shift\nresulting from the interaction between"
        },
        {
          "2": ""
        },
        {
          "2": "these features."
        },
        {
          "2": ""
        },
        {
          "2": "• We designed a Multi-Domain Aggregation mechanism to"
        },
        {
          "2": ""
        },
        {
          "2": "aggregate\nthe domain feature\nspace,\nand further\nadopt"
        },
        {
          "2": ""
        },
        {
          "2": "an\nadaptive\nprototype\nupdate method\nto\ncompute\ndo-"
        },
        {
          "2": ""
        },
        {
          "2": "main prototype and class prototype representations, which"
        },
        {
          "2": ""
        },
        {
          "2": "represent\nthe basic properties of class-invariant domain"
        },
        {
          "2": ""
        },
        {
          "2": "features and domain-invariant class features."
        },
        {
          "2": ""
        },
        {
          "2": "• We use three publicly available databases (SEED, SEED-"
        },
        {
          "2": ""
        },
        {
          "2": "IV and SEED-V)\nfor\nrigorous\ncross-subject validation."
        },
        {
          "2": ""
        },
        {
          "2": "The results\nshow that even though the target domain is"
        },
        {
          "2": ""
        },
        {
          "2": "unseen, MATL-DC still achieves comparable or even bet-"
        },
        {
          "2": ""
        },
        {
          "2": "ter performance than deep transfer\nlearning models\nthat"
        },
        {
          "2": ""
        },
        {
          "2": "rely on target domain data.\nIn addition, we\nthoroughly"
        },
        {
          "2": ""
        },
        {
          "2": "analyze the model’s parameters and feature visualizations"
        },
        {
          "2": ""
        },
        {
          "2": "to deepen our understanding of\nthe model and results."
        },
        {
          "2": ""
        },
        {
          "2": "The rest of\nthis article is arranged as follows: We briefly de-"
        },
        {
          "2": "scribed the background to the model in Sec.II. Then in Sec.III,"
        },
        {
          "2": "we\nintroduce\nthe\nconcrete\nimplementation\nof\nour\nproposed"
        },
        {
          "2": "MATL-DC in detail. Our experimental\nresults was described"
        },
        {
          "2": "in Sec.IV. Finally, We discuss our model performance\nand"
        },
        {
          "2": "conclusions in Sec.V."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "II. RELATED WORK"
        },
        {
          "2": ""
        },
        {
          "2": "A. Non-Deep-Learning-Based Emotion Recognition"
        },
        {
          "2": ""
        },
        {
          "2": "In the\nearly research on emotion recognition,\nresearchers"
        },
        {
          "2": "mainly relied on non-deep learning methods\nto extract emo-"
        },
        {
          "2": "tional EEG features\nand recognize different\nemotions\n[26]."
        },
        {
          "2": "These\nstudies have yielded valuable\ninsights\nand paved the"
        },
        {
          "2": "way for the development of affective brain-computer interface"
        },
        {
          "2": "(aBCI) systems. For example:"
        },
        {
          "2": "Wang et al.\n[27] proposed the minimum redundancy max-"
        },
        {
          "2": "imum relevance (MRMR) method to extract common critical"
        },
        {
          "2": "features across subjects, and introduces an emotion recognition"
        },
        {
          "2": "system based\non EEG signals. The\nresults\nshow that\nthe"
        },
        {
          "2": "emotional state can be recognized by using frequency domain"
        },
        {
          "2": "et\nfeatures\nand\nsupport\nvector machine\n(SVM). Duan\nal."
        },
        {
          "2": "[28] proposed a new effective EEG feature named differential"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "feature\nspace\nfor EEG-based emotion recognition,\nin which"
        },
        {
          "3": "the Euclidean metric was adopted to assess\nthe associations"
        },
        {
          "3": "between the data samples and all the selected prototypes. Zhou"
        },
        {
          "3": "et al.\n[39] proposed a novel\ntransfer\nlearning framework with"
        },
        {
          "3": "Prototypical Representation based Pairwise Learning (PR-PL)."
        },
        {
          "3": "The discriminative and generalized EEG features are learned"
        },
        {
          "3": "for\nemotion\nrevealing\nacross\nindividuals\nand\nthe\nemotion"
        },
        {
          "3": "recognition task is formulated as pairwise learning for improv-"
        },
        {
          "3": "ing the model\ntolerance to the noisy labels. Wang et al.\n[40]"
        },
        {
          "3": "proposed\na\nprototype-based\ndomain\nadaptation SPD matrix"
        },
        {
          "3": "network (daSPDnet)\nthat can successfully capture an intrinsic"
        },
        {
          "3": "emotional\nrepresentation\nshared\nbetween\ndifferent\nsubjects,"
        },
        {
          "3": "which\njointly\nexploits\nfeature\nadaptation with\ndistribution"
        },
        {
          "3": "confusion and sample adaptation with centroid alignment. Guo"
        },
        {
          "3": "et al.\n[41] proposed and implement\na novel neural network"
        },
        {
          "3": "algorithm based on modifying the emotional neural network"
        },
        {
          "3": "(EmNN) model\nto unify the prototype- and adaptive-learning"
        },
        {
          "3": "theories, as well as apply the proposed model\nto two real-life"
        },
        {
          "3": ""
        },
        {
          "3": "challenging emotion tasks, static hand-gesture recognition and"
        },
        {
          "3": ""
        },
        {
          "3": "face recognition."
        },
        {
          "3": ""
        },
        {
          "3": "However,\nthe\ncurrent most\nemotion\nrecognition models"
        },
        {
          "3": ""
        },
        {
          "3": "need to rely on both source domain and target domain data,"
        },
        {
          "3": ""
        },
        {
          "3": "which greatly increases the practical application cost, and the"
        },
        {
          "3": ""
        },
        {
          "3": "robustness and generalization ability are limited. Therefore, the"
        },
        {
          "3": ""
        },
        {
          "3": "model framework based on unseen target domains emphasizes"
        },
        {
          "3": ""
        },
        {
          "3": "the importance of\nits development."
        },
        {
          "3": ""
        },
        {
          "3": "TABLE I: Frequently used notations and descriptions."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "Notation\nDescription"
        },
        {
          "3": ""
        },
        {
          "3": "S/T\nSource/Target Domain"
        },
        {
          "3": ""
        },
        {
          "3": "Domain/Class Feature\nxd/xc"
        },
        {
          "3": ""
        },
        {
          "3": "Domain/Class Label\nyd/yc"
        },
        {
          "3": ""
        },
        {
          "3": "Shallow Feature Extractor\nfg(·)"
        },
        {
          "3": ""
        },
        {
          "3": "Domain/Class Feature Decoupler\nfd(·)/fc(·)"
        },
        {
          "3": "Domain / Class Discriminator\nDd(·)/Dc(·)"
        },
        {
          "3": "Domain / Class Prototype Representation\nµd/µc"
        },
        {
          "3": ""
        },
        {
          "3": "R(·)\nGradient Reversal Layer"
        },
        {
          "3": ""
        },
        {
          "3": "HK\nReproducing Kernel Hillbert Space"
        },
        {
          "3": ""
        },
        {
          "3": "K\nNumber of Superdomains"
        },
        {
          "3": ""
        },
        {
          "3": "θ\nBilinear Transformation Matrix"
        },
        {
          "3": "ψ\nDomain Distribution"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "III. METHODOLOGY"
        },
        {
          "3": ""
        },
        {
          "3": "Inspired by previous\nresearch [42]–[44], we\nassume\nthat"
        },
        {
          "3": ""
        },
        {
          "3": "EEG features\ninvolve\ntwo\ntypes\nof\ndeep\nfeatures:\ndomain-"
        },
        {
          "3": ""
        },
        {
          "3": "invariant\nclass\nfeatures\nand class-invariant domain features."
        },
        {
          "3": ""
        },
        {
          "3": "Domain-invariant class features capture the semantic informa-"
        },
        {
          "3": ""
        },
        {
          "3": "tion of\nthe\nemotion category to which the\nsample belongs,"
        },
        {
          "3": "while\ndomain-invariant\nclass\nfeatures\nexpress\nthe\nspecific"
        },
        {
          "3": ""
        },
        {
          "3": "information\ngenerated\nby\nthe\nindividual\ndifferences\nof\nthe"
        },
        {
          "3": "subjects. The distribution differences of EEG signals among"
        },
        {
          "3": "different subjects can be attributed to the variation of domain"
        },
        {
          "3": "features, which leads to the dispersion of class feature distribu-"
        },
        {
          "3": "tion. Based on the above assumptions, We regard the original"
        },
        {
          "3": "EEG features as\nthe fusion of\nthese two features. Therefore,"
        },
        {
          "3": "we\nextracts EEG features while\nconsidering\nboth\ndomain-"
        },
        {
          "3": "specific and class-specific components to enhance robustness"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "...\n...": "...\n...\n...\n...\nLearning\nLearning"
        },
        {
          "...\n...": "1\n0.92\n0.67\n0.38\n1.00\n1\n0\n0"
        },
        {
          "...\n...": "1\n0\n0\n0.44\n0.24\n0.32"
        },
        {
          "...\n...": "Fig.\n2: The\ntraining\nphase\nof\nthe MATL-DC framework. MATL-DC consists\nof\nthree main modules, which\nare\nfeature"
        },
        {
          "...\n...": "decoupling module, prototype computing module and pairwise learning module. The goal of model training is to obtain optimal"
        },
        {
          "...\n...": "parameters\nrepresent Domain discriminator and\nfor domain prototype and class prototype representations. Here, Dd and Dc"
        },
        {
          "...\n...": "Class discriminator,\nrespectively. GRL represent Gradient Reversal Layer. S1 ∼ Sn represent\nthe domain space. S1 ∼ SK"
        },
        {
          "...\n...": "represent\nthe aggregated superdomain space. µd denotes the domain prototype representation of\nthe superdomain. µc denotes"
        },
        {
          "...\n...": "the class prototype representation within each superdomain space. dcos(·) represents the cosine similarity calculation, which"
        },
        {
          "...\n...": "is used to evaluate the relationship between class features and class prototypes."
        },
        {
          "...\n...": "and generalization capabilities across subjects.\nclass-invariant domain features xd = fd(fg(x)) and domain-"
        },
        {
          "...\n...": "Suppose\nthe\nsource\ndomain\nand\nthe\ntarget\ndomain\nare\ninvariant class features xc = fc(fg(x)), respectively. To ensure"
        },
        {
          "...\n...": "represented as\n(S, T).\nIn the\nsource domain, we define\nthe\nthat\nthe feature decoupler can accurately separate two types"
        },
        {
          "...\n...": "sample distribution of each subject as an independent domain.\nof\nfeatures, we utilize\nand a\na domain discriminator Dd(·)"
        },
        {
          "...\n...": "Each subject has\na domain space,\nso the\nsource domain is\nclass discriminator Dc(·) to identify the domain or category to"
        },
        {
          "...\n...": "composed of multiple domain Spaces. The source domain is\nwhich the features belong. Specifically, our goal\nis to enable"
        },
        {
          "...\n...": "the domain discriminator\nto accurately identify the domain\ndefined as S = {Sn}Nd\nn=1, where Nd represents the number of"
        },
        {
          "...\n...": "subjects\nin the source domain. EEG samples\nfor each of\nthe\nto which the domain features belong,\nrather\nthan using the"
        },
        {
          "...\n...": "domain features to identify the category to which they belong.\nsubjects, defined as Sn = {xi\nn, yi\nn}Ns\nn represents\ni=1, where xi"
        },
        {
          "...\n...": "On the contrary, the class discriminator can accurately identify\nthe ith sample data of the nth subject, yi\nn is the corresponding"
        },
        {
          "...\n...": "the category to which the class feature belongs, but cannot use\nemotion label, and Ns indicates the sample capacity of the nth"
        },
        {
          "...\n...": "subject\nin the source domain. Target domain sample defined\nthe class feature to identify the domain to which it belongs. By"
        },
        {
          "...\n...": "as T = {xi\nrepresents the target domain of\nemploying a decoupler and discriminator\nthrough adversarial\nt, yi\nt}Nt\ni=1, where Nt"
        },
        {
          "...\n...": "EEG sample capacity. The target domain is completely unseen\ntraining\nfor\nsuccessful\ndecoupling\nof\ndomain-specific\nand"
        },
        {
          "...\n...": "during training. For\nease of\ninquiry,\ncommon notations\nand\nclass-specific characteristics."
        },
        {
          "...\n...": "descriptions are displayed in TABLE I.\nTo achieve this goal, we perform adversarial\ntraining of the"
        },
        {
          "...\n...": "decoupler and discriminator. Before the class-specific features"
        },
        {
          "...\n...": "are fed into the domain discriminator and the domain-specific\nA. Feature Decoupling"
        },
        {
          "...\n...": "features are fed into the class discriminator, we pass through"
        },
        {
          "...\n...": "As\nshown\nin\nFig.2,\nassuming\nthat\nthe\nshallow feature"
        },
        {
          "...\n...": "a Gradient Reversal Layer\n(GRL)\nto\nfacilitate\nadversarial"
        },
        {
          "...\n...": "extractor\nis\nused\nto\nextract\nthe\nshallow features\nof\nfg(·)"
        },
        {
          "...\n...": "training. Specifically,\nthe model maintains\nthe input\nfeatures"
        },
        {
          "...\n...": "the\nsample, we\nintroduce\na domain feature decoupler fd(·)"
        },
        {
          "...\n...": "unchanged\nduring\nforward\npropagation, where\ndata\nflows"
        },
        {
          "...\n...": "and a class\nto decouple the semantic\nfeature decoupler fc(·)"
        },
        {
          "...\n...": "directly fed the network. During backpropagation,\nthe GRL"
        },
        {
          "...\n...": "information in these\nshallow features,\nthereby obtaining the"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "|X k"
        },
        {
          "6": "the number of domain features\nin the\nsuper-\nd | denotes"
        },
        {
          "6": "domain. Therefore,\nthe\ndomain\nprototype\nrepresentation\nis"
        },
        {
          "6": "defined as:"
        },
        {
          "6": ""
        },
        {
          "6": "1\n(cid:88)"
        },
        {
          "6": ""
        },
        {
          "6": "xi\nµk\nd ="
        },
        {
          "6": "d\n(9)"
        },
        {
          "6": "|X k"
        },
        {
          "6": "d |"
        },
        {
          "6": "xi\nd∈X k"
        },
        {
          "6": ""
        },
        {
          "6": "Similarly,\nin each superdomain space,\nthe class features and"
        },
        {
          "6": "|"
        },
        {
          "6": "labels are denoted as {xi\n, and |X m\n| is denoted as the\nc, m}|Xm\nc"
        },
        {
          "6": "number of class features belonging to category m. Thus,\nthe"
        },
        {
          "6": "class prototype representation within each superdomain space"
        },
        {
          "6": "are defined as:"
        },
        {
          "6": "1"
        },
        {
          "6": "(cid:88)"
        },
        {
          "6": "µm\nxi\nc ="
        },
        {
          "6": "c\n(10)"
        },
        {
          "6": "|\n|X m\nc"
        },
        {
          "6": "xi\nc∈Xm"
        },
        {
          "6": ""
        },
        {
          "6": "During\nthe\ntraining\nphase, As\nshown\nin\nthe\nPrototype"
        },
        {
          "6": "Computing module\nof\nFig.2,\ndomain\nprototypes\nand\nclass"
        },
        {
          "6": "prototype\nrepresentations\nare\ncontinuously\nupdated\nthrough"
        },
        {
          "6": "iterations to seek optimal parameter. However,\nin the process"
        },
        {
          "6": "of model\niteration,\nthe obtained prototype representation may"
        },
        {
          "6": "have large deviation even for\ntwo adjacent\ntraining moments."
        },
        {
          "6": "To avoid the possible\ninstability of prototype\ncalculation at"
        },
        {
          "6": "different moments, we\nadopt\nan\nadaptive\nprototype\nupdate"
        },
        {
          "6": "mechanism,\nand\nthe\nprototype\nrepresentation\ncalculation\nat"
        },
        {
          "6": "t\ntime\nis\naffected by the\ntime\nt − 1,\nso as\nto improve\nthe"
        },
        {
          "6": "stability of model\ntraining. The update process is defined as："
        },
        {
          "6": ""
        },
        {
          "6": "µ(t)\n= (1 − α)µ(t−1)\n+ αµ(t)"
        },
        {
          "6": "d\nd\nd"
        },
        {
          "6": "(11)"
        },
        {
          "6": "µ(t)\n= (1 − α)µ(t−1)\n+ αµ(t)"
        },
        {
          "6": "c\nc\nc"
        },
        {
          "6": ""
        },
        {
          "6": "here, α is the weight update parameter.\nIn the early stage of"
        },
        {
          "6": ""
        },
        {
          "6": "training,\nthe feature distribution changes drastically, and the"
        },
        {
          "6": ""
        },
        {
          "6": "prototype representation needs to be updated quickly to adapt"
        },
        {
          "6": ""
        },
        {
          "6": "to this dynamic adjustment. In the later stage of training, as the"
        },
        {
          "6": ""
        },
        {
          "6": "model\ntends to converge and the feature distribution tends to"
        },
        {
          "6": "be stable,\nthe update rate of\nthe prototype should slow down."
        },
        {
          "6": ""
        },
        {
          "6": "Thus, α is defined as："
        },
        {
          "6": ""
        },
        {
          "6": "(cid:19)p\n(cid:18)"
        },
        {
          "6": "t"
        },
        {
          "6": "(12)\n1 −\nα = αl + (αh − αl)"
        },
        {
          "6": "maxEpoch"
        },
        {
          "6": ""
        },
        {
          "6": "represent\nthe upper and lower threshold of α,\nhere, αh and αl"
        },
        {
          "6": "respectively. p is the hyperparameter\nthat controls the rate of"
        },
        {
          "6": "decay. When p > 1, α will decay rapidly at the beginning, and"
        },
        {
          "6": ""
        },
        {
          "6": "as the model continues to train,\nthe decay rate will gradually"
        },
        {
          "6": "slow down until we reach our desired goal."
        },
        {
          "6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "are obtained by Eq.15, and (·) denotes\nthe\nhere, Pi\nand Pj"
        },
        {
          "7": "inner product operation.\nthe L2 constraint.\nIn\n|| · ||2 denotes"
        },
        {
          "7": ""
        },
        {
          "7": "addition,\nto avoid redundant\nfeature extraction, we introduce"
        },
        {
          "7": "soft\nregularization R with a weight parameter of β. Finally,"
        },
        {
          "7": ""
        },
        {
          "7": "the objective function of our proposed PL MDCP framework"
        },
        {
          "7": "is defined as:"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "(18)\nLP L M DCP = LF D + Lpair + βR"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "IV. EXPERIMENTAL RESULTS"
        },
        {
          "7": ""
        },
        {
          "7": "A. Databases and Data Processing"
        },
        {
          "7": ""
        },
        {
          "7": "We rigorously validated the proposed MATL-DC framework"
        },
        {
          "7": "using three established public databases: SEED [45], SEED-"
        },
        {
          "7": "IV [46] and SEED-V [47], which are extensively utilized in"
        },
        {
          "7": ""
        },
        {
          "7": "affective computing research. The SEED dataset comprising"
        },
        {
          "7": "15 subjects, each subject completed 3 experimental\nsessions"
        },
        {
          "7": ""
        },
        {
          "7": "on separate dates. Each session contained 15 trials and include"
        },
        {
          "7": ""
        },
        {
          "7": "3 emotions: positive, neutral and negative,\ninduced by video"
        },
        {
          "7": ""
        },
        {
          "7": "clips and simultaneously recorded EEG signals. The SEED-"
        },
        {
          "7": ""
        },
        {
          "7": "IV dataset consists of 15 subjects, each of whom participated"
        },
        {
          "7": ""
        },
        {
          "7": "in 3 experimental\nsessions on different dates. Each session"
        },
        {
          "7": ""
        },
        {
          "7": "consists\nof\n24\ntrials\nand\ncontains\nfour\nemotions:\nneutral,"
        },
        {
          "7": "sad,\nfear\nand happy.\nIn the SEED-V dataset, a\ntotal of 16"
        },
        {
          "7": "subjects participated, and each subject completed 3 sessions."
        },
        {
          "7": ""
        },
        {
          "7": "Each session consisted of 15 trials with five emotions: happy,"
        },
        {
          "7": ""
        },
        {
          "7": "neutral, sad, disgust and fear."
        },
        {
          "7": ""
        },
        {
          "7": "The\nEEG signals\nin\nthe\ndatabase\nare\npreprocessed\nas"
        },
        {
          "7": "follows: First,\nthe EEG signals\nare downsampled to a 200"
        },
        {
          "7": "Hz\nsampling rate,\nand noise\nis manually removed,\nsuch as"
        },
        {
          "7": "electromyography (EMG) and electrooculography (EOG). The"
        },
        {
          "7": ""
        },
        {
          "7": "denoised data is\nthen filtered using a bandpass filter with a"
        },
        {
          "7": ""
        },
        {
          "7": "range of 0.3 Hz to 50 Hz. Second,\nfor each experiment,\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "signals are segmented using a 1-s window, and differential en-"
        },
        {
          "7": ""
        },
        {
          "7": "tropy (DE [48])\nfeatures,\nrepresenting the logarithmic energy"
        },
        {
          "7": ""
        },
        {
          "7": "spectrum of specific frequency bands, are extracted based on"
        },
        {
          "7": ""
        },
        {
          "7": "five frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha"
        },
        {
          "7": ""
        },
        {
          "7": "(8-12Hz), Beta (14-30Hz), and Gamma (31-50Hz),\nresulting"
        },
        {
          "7": ""
        },
        {
          "7": "in 310 features\nfor\neach EEG segment\n(5 frequency bands"
        },
        {
          "7": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "Accuracy%±Standard-Deviation%). Here,",
          "out cross-validation results on SEED-V dataset, expressed as": "(Mean-Accuracy%±Standard-Deviation%). Here, ’*’ indicates"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": "the results are obtained by our own implementation."
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "Pacc(%)",
          "out cross-validation results on SEED-V dataset, expressed as": "Pacc(%)"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": "Traditional machine learning methods"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "55.26 ± 12.43",
          "out cross-validation results on SEED-V dataset, expressed as": "35.73 ± 07.98"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "70.62 ± 09.02",
          "out cross-validation results on SEED-V dataset, expressed as": "53.14 ± 10.10"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "58.12 ± 09.52",
          "out cross-validation results on SEED-V dataset, expressed as": "37.57 ± 13.47"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "56.71 ± 12.29",
          "out cross-validation results on SEED-V dataset, expressed as": "38.32 ± 10.11"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": "Deep learning methods"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "82.54 ± 09.25",
          "out cross-validation results on SEED-V dataset, expressed as": "59.36 ± 16.83"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "75.42 ± 10.15",
          "out cross-validation results on SEED-V dataset, expressed as": "56.26 ± 14.56"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "79.95 ± 09.02",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "83.28 ± 09.60",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "78.40 ± 06.76",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": "TABLE V: Cross-subject cross-session leave-one-subject-out"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "",
          "out cross-validation results on SEED-V dataset, expressed as": "cross-validation results on SEED dataset, expressed as (Mean-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "indicates\nthe"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "Pacc(%)"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "72.56 ± 06.41"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "57.47 ± 10.01"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "55.18 ± 07.42"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "72.78 ± 06.60"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "78.42 ± 07.57"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "73.22 ± 05.48"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "78.80 ± 12.00"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "72.10 ± 16.80"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "68.40 ± 17.20"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": "80.21 ± 06.32"
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        },
        {
          "cross-validation results on SEED dataset, expressed as (Mean-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "Fig. 4: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation."
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "The SEED database contains three emotion categories: negative, neutral and positive. Among them, (a) MATL-DC; (b) DANN;"
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "(c) DAN;\n(d) DCORAL.\nthe Seed-IV database contains four emotion categories: happy, sad, calm and fear. Among them,\n(e)"
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "MATL-DC;\n(f) DANN;\n(g) DAN;\n(h) DCORAL. The horizontal axis\nrepresents\nthe predicted labels, while the vertical axis"
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "represents the true labels."
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "TABLE VII: Cross-subject\ncross-session\nleave-one-subject-\nTABLE VI: Cross-subject cross-session leave-one-subject-out"
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "cross-validation\nresults\non\nSEED-IV dataset,\nexpressed\nas\nout cross-validation results on SEED-V dataset, expressed as"
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "(Mean-Accuracy%±Standard-Deviation%). Here, ’*’ indicates\n(Mean-Accuracy%±Standard-Deviation%). Here, ’*’ indicates"
        },
        {
          "(e) MATL-DC\n(f) DANN\n(g) DAN\n(h) DCORAL": "the results are obtained by our own implementation.\nthe results are obtained by our own implementation."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cross-validation": "(Mean-Accuracy%±Standard-Deviation%). Here, ’*’ indicates",
          "results\non": "",
          "SEED-IV dataset,": "",
          "expressed": "",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "(Mean-Accuracy%±Standard-Deviation%). Here, ’*’ indicates"
        },
        {
          "cross-validation": "the results are obtained by our own implementation.",
          "results\non": "",
          "SEED-IV dataset,": "",
          "expressed": "",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "the results are obtained by our own implementation."
        },
        {
          "cross-validation": "Methods",
          "results\non": "Pacc(%)",
          "SEED-IV dataset,": "Methods",
          "expressed": "Pacc(%)",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "Pacc(%)"
        },
        {
          "cross-validation": "",
          "results\non": "Traditional machine learning methods",
          "SEED-IV dataset,": "",
          "expressed": "",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "Traditional machine learning methods"
        },
        {
          "cross-validation": "KNN* [50]",
          "results\non": "40.06 ± 04.98",
          "SEED-IV dataset,": "KPCA* [51]",
          "expressed": "47.79 ± 07.85",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "35.28 ± 07.57"
        },
        {
          "cross-validation": "SVM* [52]",
          "results\non": "48.36 ± 07.51",
          "SEED-IV dataset,": "SA* [53]",
          "expressed": "40.34 ± 05.85",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "41.20 ± 10.76"
        },
        {
          "cross-validation": "TCA* [54]",
          "results\non": "43.01 ± 07.13",
          "SEED-IV dataset,": "CORAL* [55]",
          "expressed": "50.01 ± 07.93",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "37.68 ± 08.40"
        },
        {
          "cross-validation": "GFK* [56]",
          "results\non": "43.48 ± 06.27",
          "SEED-IV dataset,": "RF* [57]",
          "expressed": "48.16 ± 09.43",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "37.89 ± 09.84"
        },
        {
          "cross-validation": "",
          "results\non": "Deep learning methods",
          "SEED-IV dataset,": "",
          "expressed": "",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "Deep learning methods"
        },
        {
          "cross-validation": "DAN* [58]",
          "results\non": "60.95 ± 09.34",
          "SEED-IV dataset,": "DANN* [59]",
          "expressed": "61.44 ± 11.66",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "40.34 ± 08.68"
        },
        {
          "cross-validation": "DCORAL* [61]",
          "results\non": "59.96 ± 09.03",
          "SEED-IV dataset,": "DDC* [60]",
          "expressed": "54.76 ± 09.02",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "52.23 ± 12.76"
        },
        {
          "cross-validation": "GCPL [71]",
          "results\non": "62.65 ± 09.79",
          "SEED-IV dataset,": "MS-MDA [20]",
          "expressed": "59.34 ± 05.48",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": "50.69 ± 14.01"
        },
        {
          "cross-validation": "A-LSTM [72]",
          "results\non": "55.03 ± 09.28",
          "SEED-IV dataset,": "IAG [68]",
          "expressed": "62.64 ± 10.25",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation": "",
          "results\non": "",
          "SEED-IV dataset,": "",
          "expressed": "",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation": "ADAST [73]",
          "results\non": "53.66 ± 13.63",
          "SEED-IV dataset,": "MFA-LR [74]",
          "expressed": "61.66 ± 11.53",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        },
        {
          "cross-validation": "MATL-DC",
          "results\non": "",
          "SEED-IV dataset,": "",
          "expressed": "64.51 ± 09.22",
          "as": "",
          "out cross-validation results on SEED-V dataset, expressed as": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(b) DANN\n(a) MATL-DC": "Fig. 5: Confusion matrices of different model settings under cross-subject single-session leave-one-subject-out cross-validation.",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "The Seed-V database contains five emotion categories: happiness, neutral, sadness, disgust and fear. Among them, (a) MATL-",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "DC; (b) DANN; (c) DAN; (d) DCORAL. The horizontal axis represents the predicted labels, while the vertical axis represents",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "the true labels.",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "model (RF: 72.78%), the performance of the MATL-DC model",
          "(c) DAN    \n(d) DCORAL": "performance discrepancy in identifying different emotions\nin"
        },
        {
          "(b) DANN\n(a) MATL-DC": "is\nimproved by 7.43%. Among the deep learning methods,",
          "(c) DAN    \n(d) DCORAL": "MATL-DC is only 4.03%. This indicates that\nthe MATL-DC"
        },
        {
          "(b) DANN\n(a) MATL-DC": "AdaMatch, DANN and Emt-B achieved similar performance",
          "(c) DAN    \n(d) DCORAL": "model has high stability and its performance will not fluctuate"
        },
        {
          "(b) DANN\n(a) MATL-DC": "with 78.14% and 78.42% and 78.80% , respectively. Compared",
          "(c) DAN    \n(d) DCORAL": "greatly, which further demonstrates\nthat\nthe MATL-DC has"
        },
        {
          "(b) DANN\n(a) MATL-DC": "with the suboptimal model Emt-B, our proposed MATL-DC",
          "(c) DAN    \n(d) DCORAL": "greatly robustness."
        },
        {
          "(b) DANN\n(a) MATL-DC": "is improved by 1.41%. Even though the target domain is not",
          "(c) DAN    \n(d) DCORAL": "The confusion matrix on the SEED-IV dataset\nis shown in"
        },
        {
          "(b) DANN\n(a) MATL-DC": "visible, MATL-DC still\nachieve\nslightly better performance",
          "(c) DAN    \n(d) DCORAL": "Fig.4.(e)∼(h). All\nthe participating models performed poorly"
        },
        {
          "(b) DANN\n(a) MATL-DC": "than the model\nthat depends on both the source domain and",
          "(c) DAN    \n(d) DCORAL": "in\nrecognizing\nthe\nemotion\nof\nhappy, while\nthey\nall\nper-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "the\ntarget domain. The\nresults on the SEED-IV dataset\nare",
          "(c) DAN    \n(d) DCORAL": "formed well\nin recognizing the other\nthree emotions. When"
        },
        {
          "(b) DANN\n(a) MATL-DC": "shown in TABLE VI. The MATL-DC model achieves the best",
          "(c) DAN    \n(d) DCORAL": "identifying neutral\nand sad emotions,\ncompared to the\nsub-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "emotion recognition performance\nin the SEED-IV database,",
          "(c) DAN    \n(d) DCORAL": "optimal models DANN (65.96%) and DCORAL (59.56%), the"
        },
        {
          "(b) DANN\n(a) MATL-DC": "with an accuracy of 64.51%, which is significantly improved",
          "(c) DAN    \n(d) DCORAL": "performance of the MATL-DC model was improved by 4.36%"
        },
        {
          "(b) DANN\n(a) MATL-DC": "compared with traditional machine learning methods and deep",
          "(c) DAN    \n(d) DCORAL": "and 11.59%,\nrespectively. When identifying fear\nand happy"
        },
        {
          "(b) DANN\n(a) MATL-DC": "learning methods. Compared with the sub-optimal deep learn-",
          "(c) DAN    \n(d) DCORAL": "emotions,\nthe performance of MATL-DC was\nimproved by"
        },
        {
          "(b) DANN\n(a) MATL-DC": "ing model (GCPL: 62.65%),\nthe performance of MATL-DC is",
          "(c) DAN    \n(d) DCORAL": "14.36% and 2.25% respectively compared with the suboptimal"
        },
        {
          "(b) DANN\n(a) MATL-DC": "improved by 1.72%. The results on the SEED-V dataset are",
          "(c) DAN    \n(d) DCORAL": "model DAN."
        },
        {
          "(b) DANN\n(a) MATL-DC": "shown in TABLE VII.\nIn the five EEG emotion categories of",
          "(c) DAN    \n(d) DCORAL": "As\nshown\nin Fig.5.(a)∼(d),\non\nthe SEED-V dataset,\nthe"
        },
        {
          "(b) DANN\n(a) MATL-DC": "SEED-V database, MATL-DC achieves 58.39%±08.63% clas-",
          "(c) DAN    \n(d) DCORAL": "proposed MATL-DC model has weak performance in identi-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "sification performance. DAN and CORAL have similar per-",
          "(c) DAN    \n(d) DCORAL": "fying happy emotions, and is easy to confuse happy emotions"
        },
        {
          "(b) DANN\n(a) MATL-DC": "formance with 54.27% and 54.68%,\nrespectively. Compared",
          "(c) DAN    \n(d) DCORAL": "with neutral\nemotions. On the\ncontrary, MATL-DC has\nthe"
        },
        {
          "(b) DANN\n(a) MATL-DC": "with the sub-optimal model CORAL, MATL-DC improved the",
          "(c) DAN    \n(d) DCORAL": "best performance in recognizing fear, sadness and nausea with"
        },
        {
          "(b) DANN\n(a) MATL-DC": "accuracy by 3.71%.",
          "(c) DAN    \n(d) DCORAL": "60.86%, 67.72% and 75.00%, respectively. Compared with the"
        },
        {
          "(b) DANN\n(a) MATL-DC": "All\nthese results suggest\nthat\nthe proposed MATL-DC can",
          "(c) DAN    \n(d) DCORAL": "sub-optimal model, the performance of MATL-DC is improved"
        },
        {
          "(b) DANN\n(a) MATL-DC": "maintain robust performance independently of\ntarget domain",
          "(c) DAN    \n(d) DCORAL": "by 4.79%, 7.46% and 11.45%,\nrespectively."
        },
        {
          "(b) DANN\n(a) MATL-DC": "data, effectively handling the challenges posed by inter-subject",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "and inter-session variability in EEG-based emotion recognition",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "",
          "(c) DAN    \n(d) DCORAL": "V. DISCUSSION AND CONCLUSION"
        },
        {
          "(b) DANN\n(a) MATL-DC": "tasks, which demonstrate the MATL-DC has\nstrong validity",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "",
          "(c) DAN    \n(d) DCORAL": "A. Visualization of Domain and Class Features"
        },
        {
          "(b) DANN\n(a) MATL-DC": "and application ability.",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "",
          "(c) DAN    \n(d) DCORAL": "To intuitively understand the influence of\nthe domain and"
        },
        {
          "(b) DANN\n(a) MATL-DC": "F\n. Confusion Matrix",
          "(c) DAN    \n(d) DCORAL": ""
        },
        {
          "(b) DANN\n(a) MATL-DC": "",
          "(c) DAN    \n(d) DCORAL": "class features extracted by the MATL-DC model and the multi-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "To\nqualitatively\nevaluate\nthe\nrecognition\nperformance\nof",
          "(c) DAN    \n(d) DCORAL": "domain aggregation module, we performed T-SNE visualiza-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "MATL-DC for different emotion categories, we visualize the",
          "(c) DAN    \n(d) DCORAL": "tion of features based on the SEED database in order to clearly"
        },
        {
          "(b) DANN\n(a) MATL-DC": "confusion matrices\nof MATL-DC on SEED, SEED-IV and",
          "(c) DAN    \n(d) DCORAL": "understand\nthe\nevolution\nof\nfeatures\nand\nprototypes\nduring"
        },
        {
          "(b) DANN\n(a) MATL-DC": "SEED-V datasets. Here,\nthe Y-axis\nrepresents\nthe True label",
          "(c) DAN    \n(d) DCORAL": "training."
        },
        {
          "(b) DANN\n(a) MATL-DC": "and the X-axis represents the predicted label.",
          "(c) DAN    \n(d) DCORAL": "As shown in Fig.6, we visualize the domain features and do-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "As shown in Fig.4.(a)∼(d), on the SEED dataset, MATL-DC",
          "(c) DAN    \n(d) DCORAL": "main prototype representation of the MATL-DC model without"
        },
        {
          "(b) DANN\n(a) MATL-DC": "is similar to the other models, which has the best accuracy in",
          "(c) DAN    \n(d) DCORAL": "the multi-domain aggregation module\n(a)∼(c)\nand with the"
        },
        {
          "(b) DANN\n(a) MATL-DC": "recognizing positive emotions. In addition, MATL-DC has rel-",
          "(c) DAN    \n(d) DCORAL": "multi-domain\naggregation module\n(d)∼(f). Different\ncolors"
        },
        {
          "(b) DANN\n(a) MATL-DC": "atively excellent recognition performance, and the recognition",
          "(c) DAN    \n(d) DCORAL": "represent different domain features,\n‘⋄’\nrepresent\nthe domain"
        },
        {
          "(b) DANN\n(a) MATL-DC": "accuracy of various emotions\nis all higher\nthan 82%.\nIn the",
          "(c) DAN    \n(d) DCORAL": "prototypes\nfor each domain, and the black ‘×’\nrepresent\nthe"
        },
        {
          "(b) DANN\n(a) MATL-DC": "DCORAL model,\nthe performance discrepancy in identifying",
          "(c) DAN    \n(d) DCORAL": "target domain features.\nIn (a)∼(c), When without\nthe multi-"
        },
        {
          "(b) DANN\n(a) MATL-DC": "positive emotions and negative emotions is 10.4%, while the",
          "(c) DAN    \n(d) DCORAL": "domain aggregation strategy,\ntoo many domain Spaces failed"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "Different Domain Features",
          "(f) End of Training": "Domain Features of the \nDomain Prototype"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "of the Source Domain",
          "(f) End of Training": "Target Domain\nRepresentation"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "Fig. 6: T-SNE visualizations of domain features and domain prototype representations of",
          "(f) End of Training": "the MATL-DC model without\nthe"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "Multi-Domain Aggregation module (a)∼(c) and with the Multi-Domain Aggregation module (d)∼(f), showing the distribution"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "of domain features at the beginning of training, after 50 training epoch and end of training, respectively. As the model continues",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "to train,\nthe domain features become more and more clustered.",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "TABLE\nVIII:\nResults\nof\nablation\nexperiments\nwith\nthe"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "to\nfully\nexplore\nthe\npotential\ncorrelations\nbetween\nsamples",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "MATL-DC model, expressed as (Mean-Accuracy%±Standard-"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "and subjects, as well as\nlimited utilization of\nsource domain",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "Deviation%)"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "data. There are no clear boundaries between domain features,",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "and\neven\na\ncertain\ndegree\nof\noverlap\noccurs.\nIn\naddition,",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "Ablation Strategy\nPacc(%)"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "this\nalso leads\nto an increase\nin computational\ncost. Based",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "w/o Domain Prototype\n78.95 ± 08.92"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "on the previous\nassumptions, we believe\nthat EEG samples",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "w/o Class Disc. Loss in Eq.1\n81.62 ± 07.16"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "from different\nsubjects\nhave\ncertain\npotential\nconnections",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "w/o Domain Disc. Loss in Eq.2\n79.34 ± 08.74"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "with multiple domains. Therefore,\nas\nshown in (d)∼(f), we",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "w/o Class & Domain Disc. Loss\n78.11 ± 09.24"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "aggregate multiple domains\ninto ascertainable superdomains.",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "w/o Multi-Domain Aggregation\n80.23 ± 05.12"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "End of training,\nthe domain features of each superdomain are",
          "(f) End of Training": "w/o Adaptive Parameters α in Eq.12\n81.54 ± 05.86"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "more compact\nin the feature space and have clear boundaries",
          "(f) End of Training": "w/o Pairwise Learning\n76.73 ± 06.62"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "with\neach\nother,\nand\nthe\ndomain\nprototype\nrepresentations",
          "(f) End of Training": "w/o The Bilinear Trans. Matrix θ in Eq.13\n82.94 ± 06.31"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "w/o Soft Regularization R in Eq.18\n83.65 ± 04.89"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "are located at\nthe center of each cluster. At\nthe same time,",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "we\ncan\nobserve\nthat\nthere\nis\na\ncertain\nimbalance\nin\nthe",
          "(f) End of Training": "MATL-DC\n84.70 ± 04.63"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "feature\ndistribution\nbetween\ndifferent\nsuperdomains, which",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "indicates\nthat\nthe commonality of\nfeature distribution among",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "different subjects is different, which leads to different sizes of",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "B. Ablation Experiment"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "superdomains.",
          "(f) End of Training": ""
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "Further, as\nshown in Fig.7, we visualize the class\nfeature",
          "(f) End of Training": "In order\nto comprehensively evaluate\nthe\nimpact of\neach"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "distribution. With the continuous training of the model, we can",
          "(f) End of Training": "module in MATL-DC on the overall performance of the model,"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "obviously observe\nthat\nthe\nintra-class\nfeatures\nare gradually",
          "(f) End of Training": "we conduct ablation experiments on MATL-DC based on the"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "closer, and inter-class features are gradually widened, forming",
          "(f) End of Training": "SEED dataset under\nthe Cross-Subject Single-Session Leave-"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "a\nvery\nclear\nclass\nfeature\ncluster, which\nshows\nthe\nclass",
          "(f) End of Training": "One-Subject-Out Cross-Validation strategy."
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "separability of\nthe model with different class features.",
          "(f) End of Training": "The results of Ablation experiments are shown in TABLE"
        },
        {
          "(d) Begin of Training\n(e) After 50 Training Epoch": "",
          "(f) End of Training": "VIII. We\nremove domain prototype\nrepresentation,\nand the"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "Class Features of the Target Domain\nDifferent Class Features of the Source Domain"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "Fig. 7: T-SNE visualizations of source and target class features of\nthe MATL-DC model, showing the distribution of domain"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "features at\nthe (a) beginning of training, (b) after 50 training epoch and (c) end of training, respectively. As the model continues"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "to train,\nthe class features become more and more clustered."
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "TABLE IX: Results of the MATL-DC model adding different"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "prototypes. This\nshows\nthat\nthe introduction of domain pro-"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "proportions(η%) of label noise to the source domain, expressed"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "totype makes\nthe model have stronger\nfeature representation"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "as\n(Mean-Accuracy%±Standard-Deviation%).\n↓ % denotes"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "ability, and then improves the performance of the model. After"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the difference between the performances."
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "removing the class discriminator loss,\nthe model performance"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "dropped from 84.70% to 81.60%, a decrease of 3.08%. After"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "Noisy\nPointwise\nPairwise\nDiscrepancy"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "removing the domain discriminator loss, we observed that\nthe"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "Ratio (η)\nLearning (%)\nLearning (%)\n(%)"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "model performance dropped by 5.36%. These two experimen-"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "0%\n76.73 ± 06.62\n84.70 ± 04.63\n↓ 07.97"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "tal\nresults\nindicate\nthat both domain/class discriminator\ncan"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "5%\n75.89 ± 06.75\n83.61 ± 05.28\n↓ 07.72"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "enhance\nthe\nfeature\ndecoupling\nability\nof\nthe model. After"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "10%\n73.45 ± 05.94\n83.02 ± 05.06\n↓ 09.57"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "simultaneously removing the domain discriminator & class\n20%\n71.84 ± 07.96\n82.31 ± 04.94\n↓ 10.47"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "30%\n70.04 ± 07.63\n81.38 ± 05.68\n↓ 11.34"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "loss,\nthe model performance decreased signifi-"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "cantly by 6.59%, which indicates\nthat\nthe domain and class\n(30%-0%)\n↓ 06.69\n↓ 03.32"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "discriminator\ntogether help to improve the feature extraction"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "ability and significantly improve the recognition performance."
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "After\nremoving multi-domain\naggregation mechanism,\nthe"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "C. Effect of Noisy Labels"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "performance of the model decreases by 4.47%, which indicates"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "that\nthe multi-domain aggregation mechanism fully utilized"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "We\neffectively\novercame\nthe\nproblem of\nlabel\nnoise\nby"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the\npotential\nfeature\nconnections\namong multiple\nsubjects"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "pairwise learning strategy. In order to verify the robustness of"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "and improves the performance of\nthe model. After\nremoving"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the model under\nthe pairwise learning and pointwise learning"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "adaptive parameter α, the performance of the model decreases"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "strategies and its dependence on the sample labels, we train"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "by 3.16%, which indicates that\nindicates that\nthe adoption of"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the model using the\nsource domain samples with manually"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "dynamic adaptive update algorithms\nin prototype computing"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "added noise. Specifically, We replaced η% of\nthe true labels"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "can enable the model\nto iterate more stably and avoid possible"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "in the source domain with random noise labels in a controlled"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "oscillations. We\nadopted\nthe\ntraditional\npointwise\nlearning"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "manner, and evaluate the model on target domain samples. We"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "pairwise\nstrategy\ninstead\nof\nlearning,\nand\nthe\nperformance"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "set η to 5%, 10%, 20% and 30%.\nIt\nis worth noting that\nthe"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "of\nthe model decreased by 7.97%, which indicates\nthat\nthe"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "sample data of the target domain are completely unseen during"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "pairwise learning strategy effectively improves the model per-"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "training."
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "formance by converting the classification problem of samples"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "As shown in TABLE IX. In the pointwise learning strategy,"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "into the similarity problem of\nsample pairs. After\nremoving"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the model’s performance\nat different values η% is 76.73%,"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the bilinear transformation matrix θ,\nthe model performance"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "75.89%,\n73.45%,\n71.84% and\n70.04%,\nrespectively. When"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "decreased by 1.76%, which further indicates that θ is beneficial"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the label noise rate η is 30%,\nthe performance of\nthe model"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "for enhancing the feature representation ability. The MATL-"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "decreases by 6.69%, with a significant decline in performance,"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "DC performance decreases after removing the soft regulariza-"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "which indicates that the pointwise learning strategy has limited"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "tion R, which indicates that the introduction of R is beneficial"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "robustness to label noise, and the model\nis susceptible to label"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "to avoid redundant\nfeature extraction."
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "noise. In the paired learning strategy, when the label noise rate"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "Overall, All\nthese results demonstrate the effectiveness of"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "η% gradually increases from 0% to 30%,\nthe performance of"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the individual components in the MATL-DC model and their"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "the model decreases from 84.70% to 81.38%, and the overall"
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "combined impact on the overall performance."
        },
        {
          "(c) End of Training\n(a) Begin of Training\n(b) After 50 Training Epoch": "performance decreases only by 3.32%. These results indicate"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": ""
        },
        {
          "13": "module to separate domain-class features. And a multi-domain"
        },
        {
          "13": ""
        },
        {
          "13": "aggregation mechanism was\ndesigned\nto\nadaptively\nobtain"
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "domain prototype and class prototype representations.\nIn ad-"
        },
        {
          "13": "dition, MATL-DC addresses the negative impact of emotional"
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "label noise. MATL-DC has achieved advanced performance in"
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "several public databases, and provides a potential solution for"
        },
        {
          "13": ""
        },
        {
          "13": "emotion recognition under unseen target conditions."
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "ACKNOWLEDGEMENTS"
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "This work was\nsupported\nin\npart\nby\nthe National Nat-"
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "ural\nScience\nFoundation\nof China\nunder Grant\n62176089,"
        },
        {
          "13": "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience"
        },
        {
          "13": ""
        },
        {
          "13": "Foundation\nof Hunan\nProvince\nunder Grant\n2023JJ20024,"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "74": "4",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Foundation\nof Hunan\nProvince\nunder Grant\n2023JJ20024,"
        },
        {
          "74": "2\n3\n4\n5\n6\n7",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "in\npart\nby\nthe Key Research\nand Development Project\nof"
        },
        {
          "74": "The Number of Superdomain K",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Hunan Province under Grant 2025QK3008,\nin part by the Key"
        },
        {
          "74": "Fig. 8: Performance of\nthe MATL-DC model under different",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Project of Xiangjiang Laboratory under Granted 23XJ02006,"
        },
        {
          "74": "number of aggregates K.",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "in part by the STI 2030-Major Projects 2021ZD0200500,\nin"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "part\nby\nthe Medical-Engineering\nInterdisciplinary Research"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Foundation of Shenzhen University under Grant 2024YG008,"
        },
        {
          "74": "that the pairwise learning strategy has a high tolerance to noisy",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "in part by the Shenzhen University-Lingnan University Joint"
        },
        {
          "74": "labels. In addition, we observed that as η continues to increase,",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Research Programme,\nand in part by Shenzhen-Hong Kong"
        },
        {
          "74": "the discrepancy in performance between the two strategies of",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Institute\nof Brain Science-Shenzhen Fundamental Research"
        },
        {
          "74": "the model gradually widens.",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Institutions (2023SHIBS0003)."
        },
        {
          "74": "All\nthese\nresults\nshow that\nthe\nimpact of\nlabel noise on",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "model performance is\nlimited, and our proposed MATL-DC",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "REFERENCES"
        },
        {
          "74": "model has excellent\nrobustness and reliability.",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[1] D. Schacter, D. Gilbert, and D. Wegner, Psychology (2nd ed.).\nAnnual"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Review of Neuroscience, Jan 2011."
        },
        {
          "74": "D. Number of Superdomains",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[2]\nS. PARADISO, “Affective neuroscience: The foundations of human and"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "animal emotions,” American Journal of Psychiatry, vol. 159, no. 10, pp."
        },
        {
          "74": "To evaluate the relationship between the value of the super-",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "1805–1805, 2002."
        },
        {
          "74": "domain number K and the model performance in the multi-",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[3]\nL. Tian,\nJ. D. Moore,\nand C. Lai,\n“Recognizing emotions\nin spoken"
        },
        {
          "74": "domain aggregation mechanism, and to find the most appro-",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "the 1st ACM\ndialogue with acoustic and lexical cues,” in Proceedings of"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "SIGCHI\nInternational Workshop\non\nInvestigating\nSocial\nInteractions"
        },
        {
          "74": "priate K, we evaluated the changes in the model performance",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "with Artificial Agents, Nov 2017, p. 45–46."
        },
        {
          "74": "of MATL-DC under different K.",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[4] B. Sun, L. Li, X. Wu, T. Zuo, Y. Chen, G. Zhou,\nJ. He, and X. Zhu,"
        },
        {
          "74": "As\nshown\nin\nFig.8,\nthe\nblue\nvertical\naxis\non\nthe\nleft",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "“Combining\nfeature-level\nand\ndecision-level\nfusion\nin\na\nhierarchical"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "classifier\nfor emotion recognition in the wild,” Journal on Multimodal"
        },
        {
          "74": "represents the recognition accuracy of\nthe model, and the red",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "User Interfaces, vol. 10, p. 125–137, Jun 2016."
        },
        {
          "74": "vertical\naxis\non\nthe\nright\nrepresents\nthe\nstandard\ndeviation",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[5] Y. Zhao, X. Wang, M. Goubran, T. Whalen, and E. M. Petriu, “Human"
        },
        {
          "74": "of\nthe verification result. We gradually increased the value",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "emotion\nand\ncognition\nrecognition\nfrom body\nlanguage\nof\nthe\nhead"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Intelligence and\nusing soft computing techniques,” Journal of Ambient"
        },
        {
          "74": "of K from 2\nto\n7,\nand\nthe model\nperformance\nchanged",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Humanized Computing, vol. 4, no. 1, p. 121–140, Feb 2013."
        },
        {
          "74": "significantly. When K = 4,\nthe model performance reaches",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[6]\nF. Agrafioti, D. Hatzinakos, and A. K. Anderson, “Ecg pattern analysis"
        },
        {
          "74": "its peak at 84.7%±4.63%. Therefore, considering all\nfactors,",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "IEEE Transactions\nfor\nemotion\ndetection,”\non Affective Computing,"
        },
        {
          "74": "we\nset K to 4 to obtain the optimal model parameters.\nIn",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "vol. 3, no. 1, pp. 102–115, 2012."
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[7] W. Ye, Z. Zhang, F. Teng, M. Zhang, J. Wang, D. Ni, F. Li, P. Xu, and"
        },
        {
          "74": "addition, we\nobserved\nthat when K is\ntoo\nsmall,\ndomain",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Z. Liang, “Semi-supervised dual-stream self-attentive adversarial graph"
        },
        {
          "74": "features with low similarity in feature distribution are forcibly",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "contrastive\nlearning for\ncross-subject\neeg-based emotion recognition,”"
        },
        {
          "74": "clustered into the same superdomain,\nresulting in a decrease",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "IEEE Transactions on Affective Computing, vol. 16, no. 1, pp. 290–305,"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "2025."
        },
        {
          "74": "in model performance. When K is\ntoo large,\nalthough the",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[8] W. Ye, J. Wang, L. Chen, L. Dai, Z. Sun, and Z. Liang, “Adaptive spa-"
        },
        {
          "74": "sample features may be slightly discrepancy from the actual",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "tial–temporal aware graph learning for eeg-based emotion recognition,”"
        },
        {
          "74": "domain distribution,\nif clustering into a new superdomain,\nthe",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Cyborg and Bionic Systems, vol. 6, p. 0088, 2025."
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[9]\nSiddharth, T.-P.\nJung,\nand T.\nJ. Sejnowski,\n“Utilizing\ndeep\nlearning"
        },
        {
          "74": "boundary between the superdomains will be unclear or even",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "towards multi-modal bio-sensing and vision-based affective computing,”"
        },
        {
          "74": "overlapping,\nresulting model performance degradation, which",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "IEEE Transactions on Affective Computing, vol. 13, no. 01, pp. 96–107,"
        },
        {
          "74": "is also illustrated by the feature visualization in Fig.6.(c)(f).",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Jan. 2022."
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[10] W. Li, W. Huan, B. Hou, Y. Tian, Z. Zhang, and A. Song, “Can emotion"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "be\ntransferred?—a\nreview on transfer\nlearning for\neeg-based emotion"
        },
        {
          "74": "E. Conclusion",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "recognition,” IEEE Transactions on Cognitive and Developmental Sys-"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "tems, vol. 14, no. 3, pp. 833–846, 2022."
        },
        {
          "74": "This work\nproposes\na Multi-domain Aggregation Trans-",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[11] V.\nJayaram, M. Alamgir, Y. Altun, B.\nScholkopf,\nand M. Grosse-"
        },
        {
          "74": "fer Learning framework for EEG emotion recognition with",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "Wentrup, “Transfer\nlearning in brain-computer\ninterfaces,” IEEE Com-"
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "putational\nIntelligence Magazine, vol. 11, no. 1, pp. 20–31, 2016."
        },
        {
          "74": "Domain-Class prototype (MATL-DC), which breaks\nthrough",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "[12]\nJ. Li, S. Qiu, Y.-Y. Shen, C.-L. Liu, and H. He, “Multisource transfer"
        },
        {
          "74": "the dependence of traditional\ntransfer learning frameworks on",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": ""
        },
        {
          "74": "",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "learning for cross-subject eeg emotion recognition,” IEEE Transactions"
        },
        {
          "74": "source and target domain. We designed a feature decoupling",
          "62276169\nand\n62201356,\nin\npart\nby\nthe Natural\nScience": "on Cybernetics, vol. 50, no. 7, pp. 3281–3293, 2020."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "[35]\nJ. Li, S. Qiu, C. Du, Y. Wang,\nand H. He,\n“Domain adaptation for"
        },
        {
          "14": "eeg emotion recognition based on latent representation similarity,” IEEE"
        },
        {
          "14": "Transactions on Cognitive and Developmental Systems, vol. 12, no. 2,"
        },
        {
          "14": "pp. 344–353, 2019."
        },
        {
          "14": "[36] Y. Luo,\nS.-Y. Zhang, W.-L. Zheng,\nand B.-L. Lu, WGAN Domain"
        },
        {
          "14": "Adaptation for EEG-Based Emotion Recognition.\nspringer, Jan 2018,"
        },
        {
          "14": "p. 275–286."
        },
        {
          "14": "[37] C.-L. Liu\nand M. Nakagawa,\n“Evaluation\nof\nprototype\nlearning\nal-"
        },
        {
          "14": "gorithms\nfor nearest-neighbor\nclassifier\nin application to handwritten"
        },
        {
          "14": "character\nrecognition,” Pattern Recognition, p. 601–615, Mar 2001."
        },
        {
          "14": "[38] Y. Wang, S. Qiu, C. Zhao, W. Yang,\nJ. Li, X. Ma, and H. He, “Eeg-"
        },
        {
          "14": "based emotion recognition with prototype-based data representation,” in"
        },
        {
          "14": "2019 41st Annual International Conference of\nthe IEEE Engineering in"
        },
        {
          "14": "Medicine and Biology Society (EMBC), Jul 2019."
        },
        {
          "14": "[39] R. Zhou, Z. Zhang, H. Fu, L. Zhang, L. Li, G. Huang, F. Li, X. Yang,"
        },
        {
          "14": "Y\n. Dong, Y.-T. Zhang, and Z. Liang, “Pr-pl: A novel prototypical\nrep-"
        },
        {
          "14": "resentation based pairwise learning framework for emotion recognition"
        },
        {
          "14": "using eeg signals,” IEEE Transactions on Affective Computing, vol. 15,"
        },
        {
          "14": "no. 2, pp. 657–670, 2024."
        },
        {
          "14": "[40] Y. Wang, S. Qiu, X. Ma,\nand H. He,\n“A prototype-based spd matrix"
        },
        {
          "14": "network for domain adaptation eeg emotion recognition,” Pattern Recog-"
        },
        {
          "14": "nition, vol. 110, p. 107626, 2021."
        },
        {
          "14": "[41] Y. Guo, C. Tang, H. Wu, and B. Chen, “Gnn-based multi-source domain"
        },
        {
          "14": "prototype\nrepresentation\nfor\ncross-subject\neeg\nemotion\nrecognition,”"
        },
        {
          "14": "Neurocomputing, vol. 609, p. 128445, 2024."
        },
        {
          "14": "[42] H. Zhang, Y.-F. Zhang, W. Liu, A. Weller, B. Sch¨olkopf,\nand E. P."
        },
        {
          "14": "Xing, “Towards principled disentanglement\nfor domain generalization,”"
        },
        {
          "14": "the IEEE/CVF Conference on Computer Vision and\nin Proceedings of"
        },
        {
          "14": "Pattern Recognition (CVPR), June 2022, pp. 8024–8034."
        },
        {
          "14": "[43] R. Cai, Z. Li,\nP. Wei,\nJ. Qiao, K. Zhang,\nand Z. Hao,\n“Learning"
        },
        {
          "14": "disentangled semantic representation for domain adaptation,” in IJCAI:"
        },
        {
          "14": "proceedings of\nthe conference, vol. 2019, 2019, p. 2060."
        },
        {
          "14": "[44] X. Peng, Z. Huang, X. Sun, and K. Saenko, “Domain agnostic learning"
        },
        {
          "14": "the 36th Interna-\nwith disentangled representations,” in Proceedings of"
        },
        {
          "14": "tional Conference on Machine Learning,\nser. Proceedings of Machine"
        },
        {
          "14": "Learning Research, vol. 97, 2019, pp. 5102–5112."
        },
        {
          "14": "[45] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and"
        },
        {
          "14": "channels for eeg-based emotion recognition with deep neural networks,”"
        },
        {
          "14": "IEEE Transactions on Autonomous Mental Development, vol. 7, no. 3,"
        },
        {
          "14": "pp. 162–175, 2015."
        },
        {
          "14": "[46] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “Emotionmeter:"
        },
        {
          "14": "IEEE\nA multimodal\nframework\nfor\nrecognizing\nhuman\nemotions,”"
        },
        {
          "14": "Transactions on Cybernetics, vol. 49, no. 3, pp. 1110–1122, 2019."
        },
        {
          "14": "[47] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition"
        },
        {
          "14": "performance\nand robustness of multimodal deep learning models\nfor"
        },
        {
          "14": "multimodal emotion recognition,” IEEE Transactions on Cognitive and"
        },
        {
          "14": "Developmental Systems, vol. 14, no. 2, pp. 715–729, 2021."
        },
        {
          "14": "[48] R.-N. Duan,\nJ.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for"
        },
        {
          "14": "eeg-based emotion classification,” in 2013 6th International IEEE/EMBS"
        },
        {
          "14": "Conference on Neural Engineering (NER), 2013, pp. 81–84."
        },
        {
          "14": "[49]\nL.-C. Shi and B.-L. Lu, “Off-line and on-line vigilance estimation based"
        },
        {
          "14": "on linear dynamical\nsystem and manifold learning,”\nin 2010 Annual"
        },
        {
          "14": "International Conference\nof\nthe\nIEEE Engineering\nin Medicine\nand"
        },
        {
          "14": "Biology, 2010, pp. 6587–6590."
        },
        {
          "14": "[50] D. Coomans and D. L. Massart, “Alternative k-nearest neighbour rules in"
        },
        {
          "14": "supervised pattern recognition: Part 1. k-nearest neighbour classification"
        },
        {
          "14": "by using alternative voting rules,” Analytica Chimica Acta, vol. 136, pp."
        },
        {
          "14": "15–27, 1982."
        },
        {
          "14": "[51]\nS. Mika, B.\nSch¨olkopf, A.\nSmola, K.-R. M¨uller, M.\nScholz,\nand"
        },
        {
          "14": "G. R¨atsch, “Kernel pca and de-noising in feature spaces,” in Proceedings"
        },
        {
          "14": "of\nthe 12th International Conference on Neural Information Processing"
        },
        {
          "14": "Systems,\nser. NIPS’98.\nCambridge, MA, USA: MIT Press, 1998, p."
        },
        {
          "14": "536–542."
        },
        {
          "14": "[52]\nJ. A. Suykens and J. Vandewalle, “Least squares support vector machine"
        },
        {
          "14": "classifiers,” Neural processing letters, vol. 9, no. 3, pp. 293–300, 1999."
        },
        {
          "14": "[53] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, “Unsupervised"
        },
        {
          "14": "2013\nIEEE\nvisual\ndomain\nadaptation\nusing\nsubspace\nalignment,”\nin"
        },
        {
          "14": "International Conference on Computer Vision, 2013, pp. 2960–2967."
        },
        {
          "14": "[54]\nS. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via"
        },
        {
          "14": "transfer component analysis,” IEEE Transactions on Neural Networks,"
        },
        {
          "14": "vol. 22, no. 2, pp. 199–210, 2011."
        },
        {
          "14": "[55] B. Sun,\nJ. Feng, and K. Saenko, “Return of\nfrustratingly easy domain"
        },
        {
          "14": "the Thirtieth AAAI Conference on Artifi-\nadaptation,” in Proceedings of"
        },
        {
          "14": "cial\nIntelligence.\nAAAI Press, 2016, p. 2058–2065."
        },
        {
          "14": "[56] B. Gong, Y. Shi, F. Sha, and K. Grauman, “Geodesic flow kernel\nfor"
        },
        {
          "14": "IEEE Conference\non Computer\nunsupervised\ndomain\nadaptation,”\nin"
        },
        {
          "14": "Vision and Pattern Recognition, 2012, pp. 2066–2073."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "2001."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[58] H. Li, Y.-M.\nJin, W.-L. Zheng, and B.-L. Lu, “Cross-subject emotion"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Information\nrecognition\nusing\ndeep\nadaptation\nnetworks,”\nin Neural"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Processing.\nCham: Springer\nInternational Publishing, 2018, pp. 403–"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "413."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[59] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "lette, M. Marchand, and V. Lempitsky, “Domain-adversarial\ntraining of"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "neural networks,” J. Mach. Learn. Res., vol. 17, no. 1, p. 2096–2030,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "2016."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[60]\nE. Tzeng,\nJ. Hoffman, N. Zhang, K. Saenko,\nand T. Darrell,\n“Deep"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "domain\nconfusion: Maximizing\nfor\ndomain\ninvariance,” ArXiv,\nvol."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "abs/1412.3474, 2014."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[61] B. Sun and K. Saenko,\n“Deep coral: Correlation alignment\nfor deep"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "domain\nadaptation,”\nin Computer Vision\n– ECCV 2016 Workshops,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "G. Hua and H. J´egou, Eds.\nCham: Springer\nInternational Publishing,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "2016, pp. 443–450."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[62]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "dynamical graph convolutional neural networks,” IEEE Transactions on"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[63] D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu, “Equiv-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "alence of distance-based and rkhs-based statistics in hypothesis testing,”"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Annals of Stats, vol. 41, no. 5, pp. 2263–2291, 2013."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[64] Y. Ding, N. Robinson, S. Zhang, Q. Zeng, and C. Guan, “Tsception:"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Capturing temporal dynamics and spatial asymmetry from eeg for emo-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "tion recognition,” IEEE Transactions on Affective Computing, vol. 14,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "no. 3, pp. 2238–2250, 2022."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[65] W. Zhang, F. Wang, Y.\nJiang, Z. Xu, S. Wu,\nand Y. Zhang,\n“Cross-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "subject eeg-based emotion recognition with deep domain confusion,” in"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Intelligent Robotics and Applications: 12th International Conference,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "ICIRA 2019, Shenyang, China, August 8–11, 2019, Proceedings, Part\nI"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "12.\nSpringer, 2019, pp. 558–570."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[66] D.\nBerthelot,\nR.\nRoelofs,\nK.\nSohn,\nN.\nCarlini,\nand"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "A.\nKurakin,\n“Adamatch:\nA\nunified\napproach\nto\nsemi-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "International\nsupervised\nlearning\nand\ndomain\nadaptation,”\nin"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Conference\non\nLearning Representations,\n2022.\n[Online]. Available:"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "https://openreview.net/forum?id=Q5uh1Nvv5dm"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[67] Y. Ding, C. Tong, S. Zhang, M. Jiang, Y. Li, K. J. Lim, and C. Guan,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "“Emt: A novel\ntransformer\nfor generalized cross-subject\neeg emotion"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "IEEE Transactions\non Neural Networks\nand\nLearning\nrecognition,”"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Systems, vol. 36, no. 6, pp. 10 381–10 393, 2025."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[68]\nT. Song, S. Liu, W. Zheng, Y. Zong, and Z. Cui, “Instance-adaptive graph"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "the AAAI Conference\nfor eeg emotion recognition,” in Proceedings of"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "on Artificial\nIntelligence, vol. 34, no. 03, 2020, pp. 2701–2708."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[69] Y. Xu, Y. Du, L. Li, H. Lai,\nJ. Zou, T. Zhou, L. Xiao, L. Liu,\nand"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "P. Ma,\n“Amdet: Attention based multiple dimensions\neeg transformer"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "for emotion recognition,” IEEE Transactions on Affective Computing,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "vol. 15, no. 3, pp. 1067–1077, 2023."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[70]\nT. Zhang, X. Wang, X. Xu,\nand C. L.\nP. Chen,\n“Gcb-net: Graph"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "convolutional broad network and its application in emotion recognition,”"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "IEEE Transactions on Affective Computing, vol. 13, no. 1, pp. 379–388,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "2022."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[71] W. Li, L. Fan, S. Shao, and A. Song, “Generalized contrastive partial"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "label\nlearning for cross-subject eeg-based emotion recognition,” IEEE"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Transactions on Instrumentation and Measurement, vol. 73, pp. 1–11,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "2024."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[72]\nT. Song, W. Zheng, C. Lu, Y. Zong, X. Zhang,\nand Z. Cui,\n“Mped:"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "A multi-modal\nphysiological\nemotion\ndatabase\nfor\ndiscrete\nemotion"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "recognition,” IEEE Access, vol. 7, pp. 12 177–12 191, 2019."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[73]\nE. Eldele, M. Ragab, Z. Chen, M. Wu, C.-K. Kwoh, X. Li, and C. Guan,"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "“Adast: Attentive cross-domain eeg-based sleep staging framework with"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "iterative self-training,” IEEE Transactions on Emerging Topics in Com-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "putational\nIntelligence, vol. 7, no. 1, pp. 210–221, 2022."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "[74] M. Jim´enez-Guarneros and G. Fuentes-Pineda, “Learning a robust uni-"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "fied domain adaptation framework for cross-subject eeg-based emotion"
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "Signal Processing\nrecognition,” Biomedical\nand Control,\nvol.\n86,\np."
        },
        {
          "[57]\nL. Breiman,\n“Random forests,” Machine\nlearning, vol. 45, pp. 5–32,": "105138, 2023."
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Annual Review of Neuroscience",
      "authors": [
        "D Schacter",
        "D Gilbert",
        "D Wegner"
      ],
      "year": "2011",
      "venue": "Annual Review of Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Affective neuroscience: The foundations of human and animal emotions",
      "authors": [
        "S Paradiso"
      ],
      "year": "2002",
      "venue": "American Journal of Psychiatry"
    },
    {
      "citation_id": "3",
      "title": "Recognizing emotions in spoken dialogue with acoustic and lexical cues",
      "authors": [
        "L Tian",
        "J Moore",
        "C Lai"
      ],
      "year": "2017",
      "venue": "Proceedings of the 1st ACM SIGCHI International Workshop on Investigating Social Interactions with Artificial Agents"
    },
    {
      "citation_id": "4",
      "title": "Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild",
      "authors": [
        "B Sun",
        "L Li",
        "X Wu",
        "T Zuo",
        "Y Chen",
        "G Zhou",
        "J He",
        "X Zhu"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "5",
      "title": "Human emotion and cognition recognition from body language of the head using soft computing techniques",
      "authors": [
        "Y Zhao",
        "X Wang",
        "M Goubran",
        "T Whalen",
        "E Petriu"
      ],
      "year": "2013",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "6",
      "title": "Ecg pattern analysis for emotion detection",
      "authors": [
        "F Agrafioti",
        "D Hatzinakos",
        "A Anderson"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Adaptive spatial-temporal aware graph learning for eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "J Wang",
        "L Chen",
        "L Dai",
        "Z Sun",
        "Z Liang"
      ],
      "year": "2025",
      "venue": "Cyborg and Bionic Systems"
    },
    {
      "citation_id": "9",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Siddharth",
        "T Jung",
        "Sejnowski"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Can emotion be transferred?-a review on transfer learning for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "W Huan",
        "B Hou",
        "Y Tian",
        "Z Zhang",
        "A Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "11",
      "title": "Transfer learning in brain-computer interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Scholkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2016",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "12",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Eegbased emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "14",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Eegbased brain-computer interfaces (bcis): A survey of recent studies on signal sensing technologies and computational intelligence approaches and their applications",
      "authors": [
        "X Gu",
        "Z Cao",
        "A Jolfaei",
        "P Xu",
        "D Wu",
        "T.-P Jung",
        "C.-T Lin"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
    },
    {
      "citation_id": "16",
      "title": "Adversarial deep learning in eeg biometrics",
      "authors": [
        "O Özdenizci",
        "Y Wang",
        "T Koike-Akino",
        "D Erdogmus"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "17",
      "title": "Domain-invariant representation learning from eeg with private encoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "R Mikut",
        "A Schmidt",
        "O Özdenizci"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Learning invariant representations from eeg via adversarial inference",
      "authors": [
        "O Özdenizci",
        "Y Wang",
        "T Koike-Akino",
        "D Erdogmus"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "Spectral-spatial attention alignment for multi-source domain adaptation in eeg-based emotion recognition",
      "authors": [
        "Y Yang",
        "Z Wang",
        "W Tao",
        "X Liu",
        "Z Jia",
        "B Wang",
        "F Wan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "21",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "The electroencephalogram as a biometric",
      "authors": [
        "R Paranjape",
        "J Mahovsky",
        "L Benedicenti",
        "Z Koles"
      ],
      "year": "2001",
      "venue": "Canadian Conference on Electrical and Computer Engineering"
    },
    {
      "citation_id": "23",
      "title": "Revealing feelings: facets of emotional expressivity in self-reports, peer ratings, and behavior",
      "authors": [
        "J Gross",
        "O John"
      ],
      "year": "1997",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "24",
      "title": "Similarity-based classification: Connecting similarity learning to binary classification",
      "authors": [
        "H Bao",
        "T Shimada",
        "L Xu",
        "I Sato",
        "M Sugiyama"
      ],
      "year": "2020",
      "venue": "Similarity-based classification: Connecting similarity learning to binary classification",
      "arxiv": "arXiv:2006.06207"
    },
    {
      "citation_id": "25",
      "title": "Deep fake image detection based on pairwise learning",
      "authors": [
        "C.-C Hsu",
        "Y.-X Zhuang",
        "C.-Y Lee"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "26",
      "title": "A review on the computational methods for emotional state estimation from the human eeg",
      "authors": [
        "M.-K Kim",
        "M Kim",
        "E Oh",
        "S.-P Kim"
      ],
      "year": "2013",
      "venue": "Computational and mathematical methods in medicine"
    },
    {
      "citation_id": "27",
      "title": "EEG-Based Emotion Recognition Using Frequency Domain Features and Support Vector Machines",
      "authors": [
        "X.-W Wang",
        "D Nie",
        "B.-L Lu"
      ],
      "year": "2011",
      "venue": "EEG-Based Emotion Recognition Using Frequency Domain Features and Support Vector Machines"
    },
    {
      "citation_id": "28",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "29",
      "title": "Wavelet-based emotion recognition system using eeg signal",
      "authors": [
        "Z Mohammadi",
        "J Frounchi",
        "M Amiri"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "30",
      "title": "Single-trial eeg-based emotion recognition using kernel eigen-emotion pattern and adaptive support vector machine",
      "authors": [
        "Y.-H Liu",
        "C.-T Wu",
        "Y.-H Kao",
        "Y.-T Chen"
      ],
      "year": "2013",
      "venue": "2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "31",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Eeg-based emotion style transfer network for cross-dataset emotion recognition",
      "authors": [
        "Y Zhou",
        "F Li",
        "Y Li",
        "Y Ji",
        "L Zhang",
        "Y Chen",
        "W Zheng",
        "G Shi"
      ],
      "year": "2023",
      "venue": "Eeg-based emotion style transfer network for cross-dataset emotion recognition",
      "arxiv": "arXiv:2308.05767"
    },
    {
      "citation_id": "33",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "35",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "36",
      "title": "WGAN Domain Adaptation for EEG-Based Emotion Recognition",
      "authors": [
        "Y Luo",
        "S.-Y Zhang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "WGAN Domain Adaptation for EEG-Based Emotion Recognition"
    },
    {
      "citation_id": "37",
      "title": "Evaluation of prototype learning algorithms for nearest-neighbor classifier in application to handwritten character recognition",
      "authors": [
        "C.-L Liu",
        "M Nakagawa"
      ],
      "year": "2001",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Eegbased emotion recognition with prototype-based data representation",
      "authors": [
        "Y Wang",
        "S Qiu",
        "C Zhao",
        "W Yang",
        "J Li",
        "X Ma",
        "H He"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "39",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "A prototype-based spd matrix network for domain adaptation eeg emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "X Ma",
        "H He"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Gnn-based multi-source domain prototype representation for cross-subject eeg emotion recognition",
      "authors": [
        "Y Guo",
        "C Tang",
        "H Wu",
        "B Chen"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "42",
      "title": "Towards principled disentanglement for domain generalization",
      "authors": [
        "H Zhang",
        "Y.-F Zhang",
        "W Liu",
        "A Weller",
        "B Schölkopf",
        "E Xing"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "43",
      "title": "Learning disentangled semantic representation for domain adaptation",
      "authors": [
        "R Cai",
        "Z Li",
        "P Wei",
        "J Qiao",
        "K Zhang",
        "Z Hao"
      ],
      "year": "2019",
      "venue": "IJCAI: proceedings of the conference"
    },
    {
      "citation_id": "44",
      "title": "Domain agnostic learning with disentangled representations",
      "authors": [
        "X Peng",
        "Z Huang",
        "X Sun",
        "K Saenko"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "45",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "46",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "47",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "48",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "49",
      "title": "Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning",
      "authors": [
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology"
    },
    {
      "citation_id": "50",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "D Coomans",
        "D Massart"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "51",
      "title": "Kernel pca and de-noising in feature spaces",
      "authors": [
        "S Mika",
        "B Schölkopf",
        "A Smola",
        "K.-R Müller",
        "M Scholz",
        "G Rätsch"
      ],
      "year": "1998",
      "venue": "Proceedings of the 12th International Conference on Neural Information Processing Systems, ser. NIPS'98"
    },
    {
      "citation_id": "52",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural processing letters"
    },
    {
      "citation_id": "53",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "54",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "55",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "57",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "58",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "59",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "60",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "ArXiv"
    },
    {
      "citation_id": "61",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "62",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Equivalence of distance-based and rkhs-based statistics in hypothesis testing",
      "authors": [
        "D Sejdinovic",
        "B Sriperumbudur",
        "A Gretton",
        "K Fukumizu"
      ],
      "year": "2013",
      "venue": "Annals of Stats"
    },
    {
      "citation_id": "64",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Crosssubject eeg-based emotion recognition with deep domain confusion",
      "authors": [
        "W Zhang",
        "F Wang",
        "Y Jiang",
        "Z Xu",
        "S Wu",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "Intelligent Robotics and Applications: 12th International Conference"
    },
    {
      "citation_id": "66",
      "title": "Adamatch: A unified approach to semisupervised learning and domain adaptation",
      "authors": [
        "D Berthelot",
        "R Roelofs",
        "K Sohn",
        "N Carlini",
        "A Kurakin"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "67",
      "title": "Emt: A novel transformer for generalized cross-subject eeg emotion recognition",
      "authors": [
        "Y Ding",
        "C Tong",
        "S Zhang",
        "M Jiang",
        "Y Li",
        "K Lim",
        "C Guan"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "68",
      "title": "Instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "69",
      "title": "Amdet: Attention based multiple dimensions eeg transformer for emotion recognition",
      "authors": [
        "Y Xu",
        "Y Du",
        "L Li",
        "H Lai",
        "J Zou",
        "T Zhou",
        "L Xiao",
        "L Liu",
        "P Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Gcb-net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Generalized contrastive partial label learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Li",
        "L Fan",
        "S Shao",
        "A Song"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "72",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "73",
      "title": "Adast: Attentive cross-domain eeg-based sleep staging framework with iterative self-training",
      "authors": [
        "E Eldele",
        "M Ragab",
        "Z Chen",
        "M Wu",
        "C.-K Kwoh",
        "X Li",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence"
    },
    {
      "citation_id": "74",
      "title": "Learning a robust unified domain adaptation framework for cross-subject eeg-based emotion recognition",
      "authors": [
        "M Jiménez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}