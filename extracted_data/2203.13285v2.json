{
  "paper_id": "2203.13285v2",
  "title": "Continuous-Time Audiovisual Fusion With Recurrence Vs. Attention For In-The-Wild Affect Recognition",
  "published": "2022-03-24T18:22:56Z",
  "authors": [
    "Vincent Karas",
    "Mani Kumar Tellamekala",
    "Adria Mallol-Ragolta",
    "Michel Valstar",
    "Björn W. Schuller"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present our submission to 3rd Affective Behavior Analysis in-the-wild (ABAW) challenge. Learning complex interactions among multimodal sequences is critical to recognise dimensional affect from in-the-wild audiovisual data. Recurrence and attention are the two widely used sequence modelling mechanisms in the literature. To clearly understand the performance differences between recurrent and attention models in audiovisual affect recognition, we present a comprehensive evaluation of fusion models based on LSTM-RNNs, self-attention and cross-modal attention, trained for valence and arousal estimation. Particularly, we study the impact of some key design choices: the modelling complexity of CNN backbones that provide features to the the temporal models, with and without endto-end learning. We trained the audiovisual affect recognition models on in-the-wild ABAW corpus by systematically tuning the hyper-parameters involved in the network architecture design and training optimisation. Our extensive evaluation of the audiovisual fusion models shows that LSTM-RNNs can outperform the attention models when coupled with low-complex CNN backbones and trained in an end-to-end fashion, implying that attention models may not necessarily be the optimal choice for continuous-time multimodal emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The growing market penetration of smart devices is radically increasing the number of scenarios where we interact with machines. Nowadays, such interactions take place in a wide range of environments, including the workplace, at home, or even inside our vehicles. If technology is going to accompany us in all aspect of our lives, powering machines with affective capabilities is a requirement to humanise technology towards a more natural Human-Computer Interaction (HCI). Creating more human-like technology is one of the objectives of Affective Computing  [31] .\n\nThis paper focuses on the automatic recognition of valence and arousal with the aim to develop Emotional Artificial Intelligence solutions that could allow machines to adapt to the users' affective states. For instance, in the vehicle environment, if the car detects that the driver has been showing high levels of arousal and negative levels of valence, the system could interpret that the driver is experiencing some sort of anger. Alternatively, low levels of arousal and negative valence may indicate sadness or fatigue  [10] . In this case, the car could suggest playing calm music or even pulling over to take some rest and relax for the safety of the own driver and those in the surroundings. Mood improvement and relaxation systems already exist on the market for some premium brands, but knowing when to suggest them and adapting them based on the detected emotions could greatly enhance the user experience  [2] .\n\nHowever, automatically detecting emotions and moods in a setting as described above, or any scenario in an uncontrolled environment, remains an open problem. It is commonly referred to as emotion recognition in the wild, and presents several challenges: Data is often noisy, e.g. for the visual modality, a person's face may be partially occluded, or there may be rapid changes in illumination. Audio from the voice may be indistinct due to background noise, or missing if the person is silent. Another issue lies in cross-cultural emotion recognition  [34] , i.e. automatic affect recognition systems needing to perform reliably for people of very diverse backgrounds, who may express their feelings differently.\n\nIn order to tackle this difficult problem, various methods have been proposed. These frequently involve fusing multiple modalities in order to better judge the emotional state from complementary information  [32] . Another common strategy is to make use of temporal information, since the emotional state fluctuates over time.\n\nFor the purpose of processing time series, recurrent neural networks (RNNs) continue to be popular. RNNs look at each element of the input sequentially and update their hidden state. Recent works in emotion recognition have also made use of networks based on self-attention and crossmodal attention  [5, 15] . While self-attention finds relations between the elements of one sequence, cross-modal attention relates two sequences from different modalities to each other  [41] . Compared to RNNs, attention-based network architectures have the advantage of allowing for parallel computation. However, adding recurrence may still improve an attention-based network  [15] .\n\nAlthough the recurrence and attention models widely applied to the multimodal fusion for affect recognition and sentiment analysis  [3, 29, 38, 39] , it is not very clear how their performance vary depending on the quality of input feature embeddings when modelling complex interactions among the modalities, particularly in end-to-end learning approaches. Specifically, to the best of our knowledge, not much attention is paid to comprehensively analysing the performance of RNNs and attention models based on the underlying CNN backbones' characteristics. To this end, we consider two commonly used CNN backbone models of two different complexity levels for extracting face image features: FaceNet based on InceptionResNetV1 architecture and MobileFaceNet based on MobileNetV2 architecture. Using the visual features extracted using these two CNN backbones and systematically tuning the hyperparameters of network design and optimisation, we comprehensively evaluate the performance of LSTM-RNNs, selfattention and cross-modal attention models on the task of audiovisual affect recognition.\n\nHerein, we present this comparative analysis of RNNs, self-attention and cross-modal attention as part of our entry to the third Affective Behavior in the Wild (ABAW) challenge. While similar comparisons have been performed, we focus our analysis specifically on the task of continuous emotion recognition in the wild. The Affwild2 dataset used in the challenge is the largest in the wild corpus annotated in terms of valence and arousal  [22] . Its data presents many of the difficulties listed above, including a high diversity of subjects, varying illumination and occlusions, and frequently noisy audio or silence. We believe that it is beneficial to benchmark the algorithms on such a dataset.\n\nOur main contributions are:\n\n1. We investigate the impact of CNN backbones with different complexities on the performance of LSTM-RNNs for audiovisual affect recognition in the wild, and show the effectiveness of end-to-end-learning.\n\n2. We contrast the performance of LSTM-RNNs with self-attention and cross-modal attention, and show that recurrent models can outperform attention models in combination with low-complexity CNN backbones.\n\nThe rest of the paper is structured as follows: We present our methodology in Sec. 3, and describe our experimental settings and results in Sec. 4. A discussion of the results follows in Sec. 5, and make suggestions for future work in Sec. 6. Finally, Sec. 7 concludes this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Recurrence vs. Attention for Sequence Modelling. To model the underlying temporal dynamics embedded in the continuous-time data, recurrent  [14]  and attention  [43]  mechanisms have been widely used. While the recurrence models rely on gated sequential propagation of temporal dynamics encoded into a latent state, the attention models bypass the sequential propagation of information and directly attend to the past inputs. Thus, the attention models can easily capture long-range temporal contingencies by circumventing the problem of vanishing gradients. Although, LSTM-RNNs  [14]  are designed to capture the long-range dependencies by controlling the information flow, they still fall short in practice due to their fixed dimensional latent state to hold the past information, unlike in the attention models. However, this advantage with attention models comes at the cost of poor (quadratic) scalability with the sequence length, which is not the case with RNNs. Furthermore, attention models can operate only within a fixed temporal context window whereas the RNNs can easily handle unbounded context  [12] . Some recent works  [16, 30]  made systematic efforts to understand the trade-offs between the recurrence and attention mechanisms. However, in the case of continuous-time multimodal fusion which requires modelling complex interactions among different modalities, not much is known about how their performance is influenced by some key design choices, for instance, the CNN backbone modelling complexity and the resultant input features quality. This observation motivates our attempt to study the impact of CNN backbones on the performance of LSTMs, self-attention and cross-modal attention models, by systematically tuning the hyper-parameters involved in the network architecture design and training optimisation. In-The-Wild Audiovisual Affect Recognition. The first affect in the wild challenge based on the Aff-wild dataset was introduced at CVPR 2017  [44] . In  [21] , the dataset and the challenge are described. Aff-wild has 298 videos sourced from YouTube. Shown in it are subjects reacting to a variety of stimuli, e.g. film trailers. Subsequently, the corpus was extended with additional videos, and renamed to Aff-wild2 dataset  [22] . Aff-wild2 has 548 videos, with a total of about 2, 78 M frames. The total number of subjects is 455, 277 of them male. The dataset is annotated with three sets of labels: continuous affect (valence and arousal), basic expressions (six emotions and neutral), and facial action units (FAUs). 545 videos have annotations for valence and arousal.\n\nThe first ABAW challenge was held as a workshop at FG2020  [18] . It consisted of three sub-challenges for estimating valence-arousal (VA track), classifying facial expressions (EXPR track), and detecting 8 facial action units (AU track). The winning team of the VA track  [7]  relied on a multi-task learning approach. To deal with the problem of incomplete labels in Aff-wild2 data used for the first ABAW competition, i.e. not all samples being annotated for each task, Deng et al.  [7]  proposed a teacher-student framework. An ensemble of deep models was trained with semisupervised learning, where the teacher predicted missing labels to guide the student.\n\nIn 2021, the second ABAW challenge took place in conjunction with ICCV 2021  [24] . Compared to the previous year, the database had been extended with more annotations. The challenge tracks were identical, but the AU track now included 12 AUs. The winner of the VA track  [8]  was the same team as in the previous year, again utilising a multi-task teacher-student framework. The approach also included the prediction uncertainty of an ensemble of student models to further improve performance.\n\nSeveral multi-task learning models  [7, 9, 26]  effectively leveraged the availability of Aff-wild2 data jointly annotated with the labels of dimensional affect, categorical expressions, and AUs. A holistic multi-task, multi-domain network for facial emotion analysis named FaceBehavior-Net was developed on Aff-wild2 and validated in a crosscorpus setting in  [19, 20, 23] .\n\nBuilding on the success of attention mechanism  [43]  in sequence data modelling in recent years, cross-modal attention based audiovisual fusion has been widely applied to the emotion recognition tasks  [11, 25, 33, 45, 46] . Unlike the aforementioned works that solely rely on the fusion of facial and vocal expressions for affect recognition, Antoniadis et al.  [1]  proposed to use the features of body and background visual context additionally.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "Since we want to compare fusion methods for timecontinuous emotion recognition, our method is based on deep neural networks operating on sequences of features extracted from the visual and audio modalities. We use the cropped and aligned faces from the videos as visual inputs and fixed-length clips as audio inputs. Our approach is illustrated in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Features",
      "text": "Visual features are extracted with the help of 2D-CNNs. We use pre-trained networks trained on facial recognition tasks. Specifically, we use FaceNet  [37]  based on the Inception-Resnetv1 architecture, and trained on VGGFace2  [4] . Alternatively, we employ MobileFaceNet  [6] , a lightweight architecture designed for facial recognition in embedded devices. MobileFaceNet is built upon residual blocks used in the MobileNetv2 network  [36]  Its usefulness as a feature extractor for emotion recognition was demonstrated in  [8] . Both CNNs return 512-dimensional feature embeddings. The FaceNet has approximately 27M parameters, while the MobileFaceNet has 0.99M parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Features",
      "text": "For audio feature extraction, we choose a 1D CNN network based on the architecture proposed by Zhao et al.  [47] . The CNN encoder has 4 local feature learning blocks consisting of 1D convolutions and maxpooling layers. The kernel sizes and output channels are  [3, 3, 3, 3]  and  [64, 64, 128, 128] . The choice of this architecture is motivated by its low parameter count (about 88k) and proven effectiveness for speech emotion recognition on a number of corpora.\n\nWe use the RECOLA dataset  [35] , a corpus of spontaneous affective interactions between French speakers, to pre-train the audio network. For this purpose, we combine the 1D-CNN with a 2-layer LSTM and a fully connected output layer and train the model end-to-end using the the End2You toolkit 1    [42] . Then, the LSTM and output layers were removed to obtain the convolutional feature extractor. We then added a global average pooling layer at the end so the network returns 128-dimensional embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sequence Modelling",
      "text": "Recurrence Models are widely used for sequential data modelling, whose fundamental strength lies in their ability to learn the underlying temporal context in the form of a hidden state i.e. h t = f (h t-1 , ..). This approach based on maintaining the hidden states is a natural solution to model the sequential data that is irregularly sampled from an underlying continuous-time series phenomenon  [12]  such as dimensional affect recognition. However, the limitations of recurrence models in terms of capturing cross-modal interactions in multimodal temporal data, which is critical for audiovisual emotion recognition, is not very clear. In this work, we consider the canonical Long-Short Term Memory (LSTM) RNNs  [14] , using both unidirectional and bidirectional models, for a comprehensive evaluation on valence and arousal estimation from face and speech data. Self-Attention (SA). Second, we use networks based on the Transformer architecture  [43] . Specifically, we use multiheaded scaled dot-product attention blocks with feedforward networks as employed in the transformer encoder. The scaled dot-product attention is defined as:\n\nMulti-head attention linearly projects the query, key and value pairs into different sub-spaces and performs attention on them in parallel, before recombining and projecting into the output dimension. It is defined as:\n\nIn order to fuse modalities within our models, we either use a simple concatenation of our feature embeddings, or a cross-modal fusion architecture. Cross-Modal Attention (CMA) Fusion is proposed in Tsai et al.  [41]  to implement the Multimodal Transformer network in which pair-wise attention modelling across different modalities is performed. On the task of discrete emotion recognition from multimodal signals, CMA demonstrate superior generalisation performance compared to LSTM-RNNs  [41] . However, when it comes to the continuous emotion recognition from multimodal data, the performance gains that CMA can achieve over the canonical RNNs is unclear. To delineate the trade-offs between the CMA and the other aforementioned sequence models, in this work we evaluate different CMA-based audiovisual fusion models. We implemented audiovisual CMA models by tailoring the multimodal transformer architecture 2  which was originally designed for text, audio and visual modalities.\n\nOur cross-modal architecture is based on the crossmodal attention blocks introduced by  [41] . In self-attention used in the transformer encoder, Q, K and V are identical. In the cross-modal attention however, the queries and the key, value pairs come from two different modalities, where Q is denoted as the target and K,V as the source respectively. It is similar to the transformer decoder, but does not involve self-attention. At each layer, the target modality is reinforced with the low-level information of the source modality  [41] .\n\nWhen employing concatenation of feature vectors, we pass the result through either a stack of recurrent layers or a self-attention stack. When using cross-modal fusion, we pass the features through two cross-modal blocks in paral-lel, one of them using the audio features to attend to the visual features and the other vice versa. We then concatenate the outputs of the cross-modal blocks before passing them to a self-attention stack.\n\nWe use fully connected and 1D convolutional layers to reduce the dimension features returned by our extractor networks before passing them to our sequence models. When using 1D-CNNs with kernel size larger than 1, this also serves to encode the local temporal context. For the transformer networks, we add additional position embedding layers with fixed sinusoidal patterns, since they would otherwise not be able to distinguish the order of the sequence passed to them  [43] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss Functions",
      "text": "We use fully connected layers to return the outputs of our. Each model has two output heads. The first head has size 2 and is used for prediction of valence and arousal scores.\n\nWe use two losses for the regression head. The first is based on the concordance correlation coefficient (CCC)  [27] , which is defined as in Eq. (  3 ). It measures the correlation between two sequences, and ranges between -1 and 1, where -1 means perfect anti-correlation, 0 means no correlation, and 1 means perfect correlation. The loss is calculated as 1 -CCC.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ccc(X, Y)",
      "text": "We also compute the mean square error (MSE), which is defined as Eq. (  4 ). The reasoning behind adding an additional regression loss is that CCC loss alone proved to be less stable during training in our experiments.\n\nIn addition to regressing the scores, we also add a classification head that predicts the category the scores belong to. Jointly estimating continuous and categorical emotions from faces has been shown to be effective for facial affect analysis in the wild  [40] . While the Affwild2 dataset is annotated in terms of both continuous and categorical emotions, the rules of the ABAW challenge do not allow using multiple annotations for the valence-arousal track. Therefore, we discretise the labels, by dividing the twodimensional affect space into 24 sections. These are derived by plotting valence and arousal in polar coordinates, with 3 equidistant radial subdivisions and 8 angular subdivisions.\n\nCrossentropy loss is used as loss function for the classification head. Since the Affwild2 dataset is imbalanced towards positive arousal and valence, we weigh the logits to emphasise minority classes.\n\nOur total loss is thus composed of three terms. We add weights to the MSE and crossentropy losses to adjust their contribution, leading to our loss function Eq. (  5 )",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "We describe or experimental settings and the obtained results on the validation set of the challenge.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "We We use the cropped and aligned faces from the videos provided by the challenge organisers. Some frames are annotated as invalid, after discarding them, we create sequences from the remaining frames. We use a fixed sequence length of 16 frames for our experiments. Audio clips are extracted at a fixed window length of 0.5s, centered at the frame timestamps. We convert the audio of the entire dataset to 16 kHz mono, 16 bit PCM.\n\nThe frame rate of the Affwild2 dataset is 30fps for the majority of videos. Thus, consecutive frames are very similar. In order to provide our model with more temporal information, one option would be to increase sequence length, at the cost of additional computational resources. We choose instead an approach similar to  [26]  and use dilated sampling, i.e. we select only 1 in N frames. With sequence length T , this gives a temporal context t of:\n\nIn order to not reduce the size of the training set, we also apply an interleaved sampling method to select the remaining frames.\n\nWe do not apply this dilated sampling method for the validation set. While this introduces some discrepancy with the training, it maintains equal conditions to the test set.\n\nThe images are resized to the the shape required by the CNN feature extractor. We use randomly affine transformations and changes in saturation, brightness and contrast as data augmentation on the images. We also apply gaussian noise to the audio frames.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training",
      "text": "We implement out models in the PyTorch framework and train them on servers with Nvidia RTX3090 and A40 GPUs.\n\nPer model training, we allocate 40 CPUs and 40GB RAM in order to accelerate the loading of batches. The batch size is 64.\n\nModels are trained using the AdamW optimiser  [28] . We apply cosine annealing with warm restarts as learning rate scheduling, setting it to restart after 200 steps.\n\nIn order to find the best configurations for our models, we perform extensive hyperparameter optimisation. We train our models in groups, choosing first the feature extractors and the general architecture (recurrent or transformer), then varying the architecture's parameters as well as the learning rate for our optimiser and the contributions of our losses. A listing of the hyperparameters used is given in. Since the potential number of hyperparameter combinations is very large, a simple grid search would be inefficient. Instead, we make use of a tuning algorithm to cover a larger number of choices efficiently. For this, we choose Ray Tune 3 , a flexible tuning toolkit supports parallel training on multiple GPUs. We use the ASHA scheduling algorithm to quickly discover suitable configurations and stop trials early if they are not performing well.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hyper",
      "text": "In a first round of experiments, we freeze the layer weights of the feature extraction networks to limit the num- ber of trainable parameters. Then, we test end-to-end learning with the full set of parameters. For these experiments, we restrict the choice of the visual encoder network to Mo-bileFaceNet to avoid overfitting.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Validation Results",
      "text": "The validation results for preliminary experiments on models with frozen feature extraction networks are reported in Tab. 1. We denote the three types of architectures employed as Audiovisual-[RNN, SA, CMA] for recurrent, self-attention, and cross-modal attention respectively. The second column specifies the feature extraction network used for visual information, as Inception or Mobile for Inception-Resnetv1 and MobileFaceNet, respectively. For comparison, we also state the results of unimodal models trained with self-attention and RNN. It can be seen from Sec. 4.3 that our audiovisual models outperform the challenge baseline by a wide margin.\n\nWe report the validation results of the best models per architecture, trained end-to-end, in Tab.  3  urations of the best models are given in Tab.  4 .\n\nIn addition, we report the number of parameters for the best performing audiovisual models to allow for comparison of computational costs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "When judging performance, we analyse the mean value of CCC for valence and arousal, which is the metric used in the VA Track of the ABAW 2022 challenge. We first discuss how the choice of the visual CNN impacts the RNN models, and the impact of end-to-end learning. We then compare the performance of self-attention and cross-attention, before contrasting RNNs and attention models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Method",
      "text": "Visual   5 . Size of our models. Shown are the total number of parameters for the audiovisual models, grouped by architecture. For clarity, we report the number of parameters in the sequence models and the full number of parameters separately.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Rnn Performance",
      "text": "When using RNN as the sequence model, performance decreases significantly when replacing the FaceNet feature encoder with the less complex MobileFaceNet (0.413 to 0.378). At the same time, the number of parameters in the trainable part of the model increases sharply from 109 k to 4.4 M. We interpret this as the model having difficulty to learn valence and arousal effectively from the features returned by the smaller CNN. However, using the lightweight architecture together with end-to-end-learning presents a very different picture. When the feature extractors are fully trainable, the performance of the recurrent model was greatly increased, yielding an average CCC of 0.456. At the same time, the number of model parameters decreased, to merely 76 k in the sequence part. Examining the hyperparameter configuration of this winning model showed that it has a single, unidirectional LSTM layer, with a hidden dimension d = 64. We conclude from this that the RNN architecture is very efficient in learning representations of the emotional state if it is trained end-to-end in combination with shallow CNN encoders.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparing Self-Attention And Cross-Attention",
      "text": "When comparing the self-attention (SA) models with different visual CNNs, it can be seen that the model with Mobile-FaceNet performs better than the one with FaceNet, with average CCC scores of 0.389 and 0.374, respectively. At the same time, the size of the attention model is smaller for the architecture with MobileFaceNet (482 k parameters compared to 765 k parameters).\n\nFor the cross-modal attention (CMA) models, replacing FaceNet with MobileFaceNet also increases performance, from 0.378 to 0.392. However, the transformer network becomes much larger, going from 134 k parameters to 2.1 M parameters. As can be seen from these scores, the performance of self-attention and cross-attention appears to be similar if the feature extractors are frozen, with CMA performing slightly better.\n\nWhen using end-to-end learning, performance increases significantly for the self-attention model, with an average validation CCC of 0.450. At the same time, the number of parameters in the sequence part of the network shrinks to 193 k. The cross-attention model also benefits greatly from end-to-end learning, achieving a score of 0.44. The number of parameters in the sequence part of the model is 2.4 M.\n\nComparing the two best models trained end-to-end shows that the self-attention model outperforms the crossattention, while being significantly smaller. We hypothesise that the lower complexity of the self-attention model helped discover a more efficient architecture during end-toend training.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison Between Rnn And Attention For Sequence Modeling",
      "text": "We now compare the performances of our RNN models and attention models directly based on the results discussed in the two previous sections. In the case the feature extractors are frozen, for the larger FaceNet, the RNN outperforms the attention models. If frozen MobileFaceNet is used, the attention models outperform RNN. With end-to-end learning, RNN beats both self-attention and cross-attention, while also having fewer parameters. We conclude from this that our initial hypothesis that attention-based models consistently outperform RNNs for emotion recognition in the wild has not been confirmed. When end-to-end learning is used in combination with shallow CNNs for feature encoding, RNNs perform superior to the attention-based models investigated in this paper.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Outlook",
      "text": "We compared fusion performance using two CNNs of different sizes as visual feature extractors, while using a small 1D-CNN for extracting audio features. Another study could focus on choosing different audio networks, e.g. a larger model like VGGish  [13] , and comparing the effects.\n\nThe models used in this work had limited temporal context due to computational constraints. Future studies could extend towards greater sequence lengths to investigate how well the models capture long-term dependencies.\n\nOur analysis has focused on the average of valence and arousal as a metric, in order to judge the overall perfor-mance of the models. We leave the analysis of trade-offs between valence and arousal for future work.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "On a wide range of sequence modelling tasks, attention models demonstrated superior generalisation performance than recurrent models in recent years. However, it is worth noting that the recurrent models have the natural ability to cope with the challenges in learning from time-continuous sequence data, by inferring the latent states with unbounded context, at least in principle. Therefore, in the case of continuous-time multimodal affect recognition, a recurrent neural network architecture may still be a natural choice to model the latent states of face and voice data and their interactions in a time-continuous manner. Extensive evaluation of LSTM-RNNs, self-attention and cross-modal attention on in-the-wild audiovisual affect recognition, suggests that attention models may not necessarily be the optimal choice to perform continuous-time multimodal information fusion.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 3.1. Visual Features",
      "page": 3
    },
    {
      "caption": "Figure 1: Our proposed approach. We use pre-trained CNNs as feature extractors from sequences faces and raw audio clips (left). Then,",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Augsburg, Germany\nNottingham, UK": "Abstract",
          "Augsburg, Germany": "home, or even inside our vehicles.\nIf\ntechnology is going"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "to accompany us in all aspect of our\nlives, powering ma-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "In this paper, we present our submission to 3rd Affective",
          "Augsburg, Germany": "chines with affective capabilities is a requirement to human-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "Behavior Analysis in-the-wild (ABAW) challenge. Learning",
          "Augsburg, Germany": "ise technology towards a more natural Human-Computer"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "complex interactions among multimodal sequences is criti-",
          "Augsburg, Germany": "Interaction (HCI). Creating more human-like technology is"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "cal to recognise dimensional affect from in-the-wild audio-",
          "Augsburg, Germany": "one of the objectives of Affective Computing [31]."
        },
        {
          "Augsburg, Germany\nNottingham, UK": "visual data.\nRecurrence and attention are the two widely",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "This paper\nfocuses on the automatic recognition of va-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "used sequence modelling mechanisms in the literature. To",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "lence and arousal with the aim to develop Emotional Ar-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "clearly understand the performance differences between re-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "tiﬁcial\nIntelligence solutions that could allow machines to"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "current and attention models in audiovisual affect recogni-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "adapt\nto the users’ affective states.\nFor\ninstance,\nin the"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "tion, we present a comprehensive evaluation of fusion mod-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "vehicle environment,\nif\nthe car detects that\nthe driver has"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "els based on LSTM-RNNs,\nself-attention and cross-modal",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "been showing high levels of arousal and negative levels of"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "attention,\ntrained for valence and arousal estimation. Par-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "valence,\nthe system could interpret\nthat\nthe driver is expe-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "ticularly, we study the impact of some key design choices:",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "riencing some sort of anger.\nAlternatively,\nlow levels of"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "the modelling complexity of CNN backbones that provide",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "arousal and negative valence may indicate sadness or\nfa-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "features to the the temporal models, with and without end-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "tigue [10].\nIn this case,\nthe car could suggest playing calm"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "to-end learning. We trained the audiovisual affect recog-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "music or even pulling over to take some rest and relax for"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "nition models on in-the-wild ABAW corpus by systemati-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "the safety of the own driver and those in the surroundings."
        },
        {
          "Augsburg, Germany\nNottingham, UK": "cally tuning the hyper-parameters involved in the network",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "Mood improvement and relaxation systems already exist on"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "architecture design and training optimisation. Our exten-",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "the market\nfor some premium brands, but knowing when"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "sive evaluation of the audiovisual fusion models shows that",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "to suggest\nthem and adapting them based on the detected"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "LSTM-RNNs\ncan outperform the attention models when",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "emotions could greatly enhance the user experience [2]."
        },
        {
          "Augsburg, Germany\nNottingham, UK": "coupled with low-complex CNN backbones and trained in",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "an end-to-end fashion,\nimplying that attention models may",
          "Augsburg, Germany": "However, automatically detecting emotions and moods"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "not necessarily be the optimal choice for continuous-time",
          "Augsburg, Germany": "in a setting as described above, or any scenario in an un-"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "multimodal emotion recognition.",
          "Augsburg, Germany": "controlled environment,\nremains an open problem.\nIt\nis"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "commonly referred to as emotion recognition in the wild,"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "and presents several challenges: Data is often noisy, e.g."
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "for\nthe visual modality,\na person’s\nface may be partially"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "1. Introduction",
          "Augsburg, Germany": ""
        },
        {
          "Augsburg, Germany\nNottingham, UK": "",
          "Augsburg, Germany": "occluded, or\nthere may be rapid changes\nin illumination."
        },
        {
          "Augsburg, Germany\nNottingham, UK": "The growing market penetration of smart devices is radi-",
          "Augsburg, Germany": "Audio from the voice may be indistinct due to background"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "cally increasing the number of scenarios where we interact",
          "Augsburg, Germany": "noise, or missing if the person is silent. Another issue lies"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "with machines. Nowadays, such interactions take place in",
          "Augsburg, Germany": "in cross-cultural emotion recognition [34],\ni.e. automatic"
        },
        {
          "Augsburg, Germany\nNottingham, UK": "a wide range of environments,\nincluding the workplace, at",
          "Augsburg, Germany": "affect\nrecognition systems needing to perform reliably for"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "people of very diverse backgrounds, who may express their": "feelings differently.",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "different complexities on the performance of LSTM-"
        },
        {
          "people of very diverse backgrounds, who may express their": "In order to tackle this difﬁcult problem, various methods",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "RNNs for audiovisual affect\nrecognition in the wild,"
        },
        {
          "people of very diverse backgrounds, who may express their": "have been proposed. These frequently involve fusing mul-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "and show the effectiveness of end-to-end-learning."
        },
        {
          "people of very diverse backgrounds, who may express their": "tiple modalities in order to better judge the emotional state",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "2. We\ncontrast\nthe performance of LSTM-RNNs with"
        },
        {
          "people of very diverse backgrounds, who may express their": "from complementary information [32]. Another common",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "self-attention and cross-modal attention, and show that"
        },
        {
          "people of very diverse backgrounds, who may express their": "strategy is to make use of temporal\ninformation, since the",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "recurrent models can outperform attention models in"
        },
        {
          "people of very diverse backgrounds, who may express their": "emotional state ﬂuctuates over time.",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "combination with low-complexity CNN backbones."
        },
        {
          "people of very diverse backgrounds, who may express their": "For the purpose of processing time series, recurrent neu-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "ral networks (RNNs) continue to be popular. RNNs look at",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "The rest of the paper is structured as follows: We present"
        },
        {
          "people of very diverse backgrounds, who may express their": "each element of the input sequentially and update their hid-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "our methodology in Sec. 3, and describe our experimental"
        },
        {
          "people of very diverse backgrounds, who may express their": "den state. Recent works in emotion recognition have also",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "settings and results in Sec. 4. A discussion of\nthe results"
        },
        {
          "people of very diverse backgrounds, who may express their": "made use of networks based on self-attention and cross-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "follows in Sec. 5, and make suggestions for future work in"
        },
        {
          "people of very diverse backgrounds, who may express their": "modal attention [5, 15]. While self-attention ﬁnds relations",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "Sec. 6. Finally, Sec. 7 concludes this paper."
        },
        {
          "people of very diverse backgrounds, who may express their": "between the elements of one sequence, cross-modal atten-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "2. Related Work"
        },
        {
          "people of very diverse backgrounds, who may express their": "tion relates two sequences from different modalities to each",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "other [41]. Compared to RNNs, attention-based network ar-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "Recurrence vs. Attention for Sequence Modelling.\nTo"
        },
        {
          "people of very diverse backgrounds, who may express their": "chitectures have the advantage of allowing for parallel com-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "model\nthe underlying temporal dynamics embedded in the"
        },
        {
          "people of very diverse backgrounds, who may express their": "putation. However, adding recurrence may still improve an",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "continuous-time\ndata,\nrecurrent\n[14]\nand\nattention\n[43]"
        },
        {
          "people of very diverse backgrounds, who may express their": "attention-based network [15].",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "mechanisms have been widely used. While the recurrence"
        },
        {
          "people of very diverse backgrounds, who may express their": "Although the recurrence and attention models widely ap-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "models rely on gated sequential propagation of temporal dy-"
        },
        {
          "people of very diverse backgrounds, who may express their": "plied to the multimodal\nfusion for affect\nrecognition and",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "namics encoded into a latent state, the attention models by-"
        },
        {
          "people of very diverse backgrounds, who may express their": "sentiment analysis [3, 29, 38, 39],\nit\nis not very clear how",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "pass the sequential propagation of information and directly"
        },
        {
          "people of very diverse backgrounds, who may express their": "their performance vary depending on the quality of\ninput",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "attend to the past\ninputs.\nThus,\nthe attention models can"
        },
        {
          "people of very diverse backgrounds, who may express their": "feature embeddings when modelling complex interactions",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "easily capture\nlong-range\ntemporal\ncontingencies by cir-"
        },
        {
          "people of very diverse backgrounds, who may express their": "among the modalities, particularly in end-to-end learning",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "cumventing the problem of vanishing gradients. Although,"
        },
        {
          "people of very diverse backgrounds, who may express their": "approaches. Speciﬁcally,\nto the best of our knowledge, not",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "LSTM-RNNs [14] are designed to capture the long-range"
        },
        {
          "people of very diverse backgrounds, who may express their": "much attention is paid to comprehensively analysing the",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "dependencies by controlling the information ﬂow, they still"
        },
        {
          "people of very diverse backgrounds, who may express their": "performance of RNNs and attention models based on the",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "fall short\nin practice due to their ﬁxed dimensional\nlatent"
        },
        {
          "people of very diverse backgrounds, who may express their": "underlying CNN backbones’ characteristics.\nTo this end,",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "state to hold the past\ninformation, unlike in the attention"
        },
        {
          "people of very diverse backgrounds, who may express their": "we consider\ntwo commonly used CNN backbone models",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "models.\nHowever,\nthis advantage with attention models"
        },
        {
          "people of very diverse backgrounds, who may express their": "of\ntwo different complexity levels for extracting face im-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "comes at\nthe cost of poor\n(quadratic) scalability with the"
        },
        {
          "people of very diverse backgrounds, who may express their": "age features: FaceNet based on InceptionResNetV1 archi-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "sequence length, which is not the case with RNNs. Further-"
        },
        {
          "people of very diverse backgrounds, who may express their": "tecture and MobileFaceNet based on MobileNetV2 archi-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "more, attention models can operate only within a ﬁxed tem-"
        },
        {
          "people of very diverse backgrounds, who may express their": "tecture.\nUsing the visual\nfeatures\nextracted using these",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "poral context window whereas the RNNs can easily handle"
        },
        {
          "people of very diverse backgrounds, who may express their": "two CNN backbones and systematically tuning the hyper-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "unbounded context [12]."
        },
        {
          "people of very diverse backgrounds, who may express their": "parameters of network design and optimisation, we compre-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "Some recent works [16, 30] made systematic efforts to"
        },
        {
          "people of very diverse backgrounds, who may express their": "hensively evaluate the performance of LSTM-RNNs, self-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "understand the trade-offs between the recurrence and atten-"
        },
        {
          "people of very diverse backgrounds, who may express their": "attention and cross-modal attention models on the task of",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "tion mechanisms. However,\nin the case of continuous-time"
        },
        {
          "people of very diverse backgrounds, who may express their": "audiovisual affect recognition.",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": ""
        },
        {
          "people of very diverse backgrounds, who may express their": "",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "multimodal\nfusion which requires modelling complex in-"
        },
        {
          "people of very diverse backgrounds, who may express their": "Herein, we present\nthis comparative analysis of RNNs,",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "teractions among different modalities, not much is known"
        },
        {
          "people of very diverse backgrounds, who may express their": "self-attention and cross-modal attention as part of our entry",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "about how their performance is inﬂuenced by some key de-"
        },
        {
          "people of very diverse backgrounds, who may express their": "to the third Affective Behavior in the Wild (ABAW) chal-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "sign choices,\nfor\ninstance,\nthe CNN backbone modelling"
        },
        {
          "people of very diverse backgrounds, who may express their": "lenge. While similar comparisons have been performed,",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "complexity and the resultant input features quality. This ob-"
        },
        {
          "people of very diverse backgrounds, who may express their": "we focus our analysis speciﬁcally on the task of continu-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "servation motivates our attempt to study the impact of CNN"
        },
        {
          "people of very diverse backgrounds, who may express their": "ous emotion recognition in the wild. The Affwild2 dataset",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "backbones on the performance of LSTMs, self-attention and"
        },
        {
          "people of very diverse backgrounds, who may express their": "used in the challenge is the largest in the wild corpus anno-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "cross-modal attention models, by systematically tuning the"
        },
        {
          "people of very diverse backgrounds, who may express their": "tated in terms of valence and arousal [22].\nIts data presents",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "hyper-parameters involved in the network architecture de-"
        },
        {
          "people of very diverse backgrounds, who may express their": "many of the difﬁculties listed above, including a high diver-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "sign and training optimisation."
        },
        {
          "people of very diverse backgrounds, who may express their": "sity of subjects, varying illumination and occlusions, and",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "In-The-Wild Audiovisual Affect Recognition.\nThe ﬁrst"
        },
        {
          "people of very diverse backgrounds, who may express their": "frequently noisy audio or silence. We believe that it is ben-",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "affect\nin the wild challenge based on the Aff-wild dataset"
        },
        {
          "people of very diverse backgrounds, who may express their": "eﬁcial to benchmark the algorithms on such a dataset.",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "was\nintroduced at CVPR 2017 [44].\nIn [21],\nthe dataset"
        },
        {
          "people of very diverse backgrounds, who may express their": "Our main contributions are:",
          "1. We\ninvestigate\nthe\nimpact of CNN backbones with": "and the challenge are described. Aff-wild has 298 videos"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "to a variety of stimuli, e.g. ﬁlm trailers. Subsequently,\nthe",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "lustrated in Fig. 1."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "corpus was extended with additional videos, and renamed",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "3.1. Visual Features"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "to Aff-wild2 dataset [22]. Aff-wild2 has 548 videos, with a",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "total of about 2, 78 M frames. The total number of subjects",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "Visual features are extracted with the help of 2D-CNNs. We"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "is 455, 277 of\nthem male.\nThe dataset\nis annotated with",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "use pre-trained networks trained on facial recognition tasks."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "three sets of labels: continuous affect (valence and arousal),",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "Speciﬁcally, we use FaceNet\n[37] based on the Inception-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "basic expressions (six emotions and neutral), and facial ac-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "Resnetv1 architecture, and trained on VGGFace2 [4]. Al-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "tion units (FAUs). 545 videos have annotations for valence",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "ternatively, we employ MobileFaceNet\n[6],\na lightweight"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "and arousal.",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "architecture designed for facial recognition in embedded de-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "The ﬁrst ABAW challenge was held as a workshop at",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "vices. MobileFaceNet is built upon residual blocks used in"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "FG2020 [18].\nIt consisted of\nthree sub-challenges for es-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "the MobileNetv2 network [36]\nIts usefulness as a feature"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "timating valence-arousal\n(VA track), classifying facial ex-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "extractor for emotion recognition was demonstrated in [8]."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "pressions (EXPR track), and detecting 8 facial action units",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "Both CNNs\nreturn 512-dimensional\nfeature\nembeddings."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "(AU track).\nThe winning team of\nthe VA track [7]\nrelied",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "The FaceNet has approximately 27M parameters, while the"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "on a multi-task learning approach. To deal with the prob-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "MobileFaceNet has 0.99M parameters."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "lem of incomplete labels in Aff-wild2 data used for the ﬁrst",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "3.2. Audio Features"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "ABAW competition, i.e. not all samples being annotated for",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "each task, Deng et al. [7] proposed a teacher-student frame-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "For audio feature extraction, we choose a 1D CNN network"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "work. An ensemble of deep models was trained with semi-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "based on the architecture proposed by Zhao et al. [47]. The"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "supervised learning, where the teacher predicted missing la-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "CNN encoder has 4 local\nfeature learning blocks consist-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "bels to guide the student.",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "ing of 1D convolutions and maxpooling layers. The kernel"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "In 2021, the second ABAW challenge took place in con-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "sizes and output channels are [3,3,3,3] and [64, 64, 128,"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "junction with ICCV 2021 [24]. Compared to the previous",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "128]. The choice of this architecture is motivated by its low"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "year,\nthe database had been extended with more annota-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "parameter count\n(about 88k) and proven effectiveness for"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "tions. The challenge tracks were identical, but the AU track",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "speech emotion recognition on a number of corpora."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "now included 12 AUs.\nThe winner of\nthe VA track [8]",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "We use the RECOLA dataset\n[35],\na corpus of\nspon-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "was the same team as in the previous year, again utilising",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "taneous affective interactions between French speakers,\nto"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "a multi-task teacher-student framework. The approach also",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "pre-train the audio network. For this purpose, we combine"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "included the prediction uncertainty of an ensemble of stu-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "the 1D-CNN with a 2-layer LSTM and a fully connected"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "dent models to further improve performance.",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "output\nlayer and train the model end-to-end using the the"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "Several multi-task learning models [7, 9, 26] effectively",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "End2You toolkit 1 [42]. Then, the LSTM and output layers"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "leveraged the availability of Aff-wild2 data jointly anno-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "were removed to obtain the convolutional feature extractor."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "tated with the labels of dimensional affect, categorical ex-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "We then added a global average pooling layer at the end so"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "pressions, and AUs.\nA holistic multi-task, multi-domain",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "the network returns 128-dimensional embeddings."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "network for\nfacial emotion analysis named FaceBehavior-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "3.3. Sequence Modelling"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "Net was developed on Aff-wild2 and validated in a cross-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "corpus setting in [19, 20, 23].",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "Recurrence Models are widely used for\nsequential data"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "Building on the success of attention mechanism [43] in",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "modelling, whose fundamental\nstrength lies\nin their abil-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "sequence data modelling in recent years, cross-modal atten-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "ity to learn the underlying temporal context in the form of a"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "tion based audiovisual\nfusion has been widely applied to",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "hidden state i.e. ht = f (ht−1, ..). This approach based on"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "the emotion recognition tasks [11,25,33,45,46]. Unlike the",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "maintaining the hidden states is a natural solution to model"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "aforementioned works that solely rely on the fusion of facial",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "the sequential data that\nis irregularly sampled from an un-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "and vocal expressions for affect recognition, Antoniadis et",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "derlying continuous-time series phenomenon [12] such as"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "al. [1] proposed to use the features of body and background",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "dimensional affect recognition. However, the limitations of"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "visual context additionally.",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "recurrence models in terms of capturing cross-modal\ninter-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "actions in multimodal\ntemporal data, which is critical\nfor"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "3. Methodology",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "audiovisual emotion recognition,\nis not very clear.\nIn this"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "work, we consider the canonical Long-Short Term Memory"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "Since we want\nto\ncompare\nfusion methods\nfor\ntime-",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "(LSTM) RNNs [14], using both unidirectional and bidirec-"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "continuous emotion recognition, our method is based on",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "tional models,\nfor a comprehensive evaluation on valence"
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "deep neural networks operating on sequences of\nfeatures",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "and arousal estimation from face and speech data."
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "extracted from the visual and audio modalities. We use the",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": ""
        },
        {
          "sourced from YouTube.\nShown in it are subjects reacting": "cropped and aligned faces from the videos as visual\ninputs",
          "and ﬁxed-length clips as audio inputs. Our approach is il-": "1https://github.com/end2you/end2you"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Q K V\nMHA": "Figure 1. Our proposed approach. We use pre-trained CNNs as feature extractors from sequences faces and raw audio clips (left). Then,",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "we process them with three different architectures: Recurrent Neural Networks with LSTM cells (RNN, top), Self-Attention (SA, middle)",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "and Cross-Modal Attention (CMA, bottom). Each model predicts a sequence of scores for valence and arousal for each timestep of the",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "input (right).",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "Self-Attention (SA). Second, we use networks based on the",
          "FFN": "superior generalisation performance compared to LSTM-"
        },
        {
          "Q K V\nMHA": "Transformer architecture [43].\nSpeciﬁcally, we use multi-",
          "FFN": "RNNs\n[41].\nHowever, when\nit\ncomes\nto\nthe\ncontinu-"
        },
        {
          "Q K V\nMHA": "headed scaled dot-product attention blocks with feedfor-",
          "FFN": "ous\nemotion recognition from multimodal data,\nthe per-"
        },
        {
          "Q K V\nMHA": "ward networks as employed in the transformer encoder. The",
          "FFN": "formance gains that CMA can achieve over\nthe canonical"
        },
        {
          "Q K V\nMHA": "scaled dot-product attention is deﬁned as:",
          "FFN": "RNNs is unclear.\nTo delineate the trade-offs between the"
        },
        {
          "Q K V\nMHA": "",
          "FFN": "CMA and the other aforementioned sequence models,\nin"
        },
        {
          "Q K V\nMHA": "",
          "FFN": "this work we evaluate different CMA-based audiovisual fu-"
        },
        {
          "Q K V\nMHA": "(cid:18) QK T",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "√\nV\nAttention(Q, K, V ) = softmax\n(1)",
          "FFN": "sion models. We implemented audiovisual CMA models"
        },
        {
          "Q K V\nMHA": "dk",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "by tailoring the multimodal transformer architecture2 which"
        },
        {
          "Q K V\nMHA": "",
          "FFN": "was originally designed for text, audio and visual modali-"
        },
        {
          "Q K V\nMHA": "Multi-head attention linearly projects the query, key and",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "ties."
        },
        {
          "Q K V\nMHA": "value pairs into different sub-spaces and performs attention",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "Our\ncross-modal\narchitecture\nis\nbased\non\nthe\ncross-"
        },
        {
          "Q K V\nMHA": "on them in parallel, before recombining and projecting into",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "modal attention blocks introduced by [41]. In self-attention"
        },
        {
          "Q K V\nMHA": "the output dimension. It is deﬁned as:",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "used in the transformer encoder, Q, K and V are identical."
        },
        {
          "Q K V\nMHA": "",
          "FFN": "In the cross-modal attention however,\nthe queries and the"
        },
        {
          "Q K V\nMHA": "MHA(Q, K, V ) = Concat (head1, ..., headn) W O",
          "FFN": "key, value pairs come from two different modalities, where"
        },
        {
          "Q K V\nMHA": "(2)\n(cid:16)\n(cid:17)",
          "FFN": "Q is denoted as the target and K,V as the source respec-"
        },
        {
          "Q K V\nMHA": "QW Q\n, KW K\n, V W V\nwhere headi = Attention\ni\ni",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "i",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "tively.\nIt is similar to the transformer decoder, but does not"
        },
        {
          "Q K V\nMHA": "",
          "FFN": "involve self-attention.\nAt each layer,\nthe target modality"
        },
        {
          "Q K V\nMHA": "In order to fuse modalities within our models, we either",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "is reinforced with the low-level\ninformation of\nthe source"
        },
        {
          "Q K V\nMHA": "use a simple concatenation of our feature embeddings, or a",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "modality [41]."
        },
        {
          "Q K V\nMHA": "cross-modal fusion architecture.",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "When employing concatenation of\nfeature vectors, we"
        },
        {
          "Q K V\nMHA": "Cross-Modal Attention (CMA) Fusion is proposed in Tsai",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "pass the result\nthrough either a stack of recurrent\nlayers or"
        },
        {
          "Q K V\nMHA": "et al.\n[41]\nto implement\nthe Multimodal Transformer net-",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "a self-attention stack. When using cross-modal fusion, we"
        },
        {
          "Q K V\nMHA": "work in which pair-wise attention modelling across differ-",
          "FFN": ""
        },
        {
          "Q K V\nMHA": "",
          "FFN": "pass the features through two cross-modal blocks in paral-"
        },
        {
          "Q K V\nMHA": "ent modalities is performed. On the task of discrete emotion",
          "FFN": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "lel, one of\nthem using the audio features to attend to the": "visual features and the other vice versa. We then concate-",
          "Our total\nloss is thus composed of three terms. We add": "weights to the MSE and crossentropy losses to adjust\ntheir"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "nate the outputs of\nthe cross-modal blocks before passing",
          "Our total\nloss is thus composed of three terms. We add": "contribution, leading to our loss function Eq. (5)"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "them to a self-attention stack.",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "We use fully connected and 1D convolutional\nlayers to",
          "Our total\nloss is thus composed of three terms. We add": "(5)\nL = Lccc + λmse ∗ Lmse + λce ∗ Lce"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "reduce the dimension features returned by our extractor net-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "works before passing them to our sequence models. When",
          "Our total\nloss is thus composed of three terms. We add": "4. Experiments and Results"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "using 1D-CNNs with kernel\nsize larger\nthan 1,\nthis also",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "We describe or experimental settings and the obtained re-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "serves to encode the local\ntemporal context. For the trans-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "sults on the validation set of the challenge."
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "former networks, we\nadd additional position embedding",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "layers with ﬁxed sinusoidal patterns, since they would oth-",
          "Our total\nloss is thus composed of three terms. We add": "4.1. Dataset"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "erwise not be able to distinguish the order of the sequence",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "We\nuse\nsubset\nof Affwild2\nannotated\nfor\nthe Valence-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "passed to them [43].",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "Arousal (VA) Estimation task. The training set consists of"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "3.4. Loss Functions",
          "Our total\nloss is thus composed of three terms. We add": "341 videos,\nthe validation set consists of 71 videos and the"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "test set consists of 152 videos.\nSeveral videos have more"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "We use fully connected layers to return the outputs of our.",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "than one person in them,\nthose videos are annotated sep-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "Each model has two output heads. The ﬁrst head has size 2",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "arately for each person and are considered like multiple"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "and is used for prediction of valence and arousal scores.",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "videos.\nFrames are annotated with valence and arousal\nin"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "We use two losses\nfor\nthe regression head.\nThe ﬁrst",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "the range [-1, 1]."
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "is based on the concordance correlation coefﬁcient (CCC)",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "We use the cropped and aligned faces from the videos"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "[27], which is deﬁned as in Eq. (3). It measures the correla-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "provided by the\nchallenge organisers.\nSome\nframes\nare"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "tion between two sequences, and ranges between -1 and 1,",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "annotated as invalid, after discarding them, we create se-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "where -1 means perfect anti-correlation, 0 means no corre-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "quences\nfrom the remaining frames. We use a ﬁxed se-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "lation, and 1 means perfect correlation. The loss is calcu-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "quence length of 16 frames for our experiments. Audio clips"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "lated as 1 − CCC.",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "are extracted at a ﬁxed window length of 0.5s, centered at"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "the frame timestamps. We convert\nthe audio of\nthe entire"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "2 ∗ cov(x, y)",
          "Our total\nloss is thus composed of three terms. We add": "dataset to 16 kHz mono, 16 bit PCM."
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "CCC(x, y) =",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "σ2\nx + σ2\ny + (µx − µy)2\n(3)",
          "Our total\nloss is thus composed of three terms. We add": "The frame rate of\nthe Affwild2 dataset\nis 30fps for\nthe"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "majority of videos. Thus, consecutive frames are very simi-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "(cid:88)",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "where cov(x, y) =\n(x − µx) ∗ (y − µy)",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "lar. In order to provide our model with more temporal infor-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "mation, one option would be to increase sequence length, at"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "We also compute the mean square error\n(MSE), which",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "the cost of additional computational resources. We choose"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "is deﬁned as Eq. (4). The reasoning behind adding an ad-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "instead an approach similar\nto [26] and use dilated sam-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "ditional regression loss is that CCC loss alone proved to be",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "pling,\ni.e. we select only 1 in N frames. With sequence"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "less stable during training in our experiments.",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "length T , this gives a temporal context t of:"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "(cid:88)",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "M SE(x, y) =\n(x − y)2\n(4)",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "N 3\n∗ T\nt =\n(6)"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "0"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "In addition to regressing the scores, we also add a clas-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "siﬁcation head that predicts the category the scores belong",
          "Our total\nloss is thus composed of three terms. We add": "In order to not reduce the size of the training set, we also"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "to.\nJointly estimating continuous and categorical emotions",
          "Our total\nloss is thus composed of three terms. We add": "apply an interleaved sampling method to select the remain-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "from faces has been shown to be effective for\nfacial af-",
          "Our total\nloss is thus composed of three terms. We add": "ing frames."
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "fect analysis in the wild [40]. While the Affwild2 dataset",
          "Our total\nloss is thus composed of three terms. We add": "We do not apply this dilated sampling method for\nthe"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "is annotated in terms of both continuous and categorical",
          "Our total\nloss is thus composed of three terms. We add": "validation set. While this introduces some discrepancy with"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "emotions,\nthe rules of\nthe ABAW challenge do not allow",
          "Our total\nloss is thus composed of three terms. We add": "the training, it maintains equal conditions to the test set."
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "using multiple\nannotations\nfor\nthe valence-arousal\ntrack.",
          "Our total\nloss is thus composed of three terms. We add": "The images are resized to the the shape required by the"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "Therefore, we discretise the labels, by dividing the two-",
          "Our total\nloss is thus composed of three terms. We add": "CNN feature extractor. We use randomly afﬁne transforma-"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "dimensional affect space into 24 sections. These are derived",
          "Our total\nloss is thus composed of three terms. We add": "tions and changes in saturation, brightness and contrast as"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "by plotting valence and arousal in polar coordinates, with 3",
          "Our total\nloss is thus composed of three terms. We add": "data augmentation on the images. We also apply gaussian"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "equidistant radial subdivisions and 8 angular subdivisions.",
          "Our total\nloss is thus composed of three terms. We add": "noise to the audio frames."
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "Crossentropy loss is used as loss function for\nthe clas-",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "",
          "Our total\nloss is thus composed of three terms. We add": "4.2. Training"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "siﬁcation head.\nSince the Affwild2 dataset\nis imbalanced",
          "Our total\nloss is thus composed of three terms. We add": ""
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "towards positive arousal and valence, we weigh the logits to",
          "Our total\nloss is thus composed of three terms. We add": "We implement out models in the PyTorch framework and"
        },
        {
          "lel, one of\nthem using the audio features to attend to the": "emphasise minority classes.",
          "Our total\nloss is thus composed of three terms. We add": "train them on servers with Nvidia RTX3090 and A40 GPUs."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "in order to accelerate the loading of batches. The batch size",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "is 64.",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": "Models are trained using the AdamW optimiser [28]. We"
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "apply cosine annealing with warm restarts as learning rate",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "scheduling, setting it to restart after 200 steps.",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "In order",
          "training, we allocate 40 CPUs and 40GB RAM": "to ﬁnd the best"
        },
        {
          "Per model": "els, we perform extensive hyperparameter optimisation. We",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "train our models in groups, choosing ﬁrst the feature extrac-",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "tors and the general architecture (recurrent or transformer),",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "then varying the architecture’s parameters as well as",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "learning rate for our optimiser and the contributions of our",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "losses. A listing of the hyperparameters used is given in.",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "Hyper-parameter",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "nlayers",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "dmodel",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "activation",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "dropout",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        },
        {
          "Per model": "learning rate",
          "training, we allocate 40 CPUs and 40GB RAM": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "0.393\nAV-CMA\nInception\n0.363\n0.378"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "AV-CMA\nMobile\n0.324\n0.460\n0.392"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "Table 2. The validation results in CCC ↑, evaluated on the vali-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "dation set of Affwild2 as partitioned in ABAW 2022 for unimodal"
        },
        {
          "Cross-Modal Attention (CMA) Models": "and multimodal models with frozen feature extractors. Valence,"
        },
        {
          "Cross-Modal Attention (CMA) Models": "arousal and their average are stated for comparison. Results are"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "given for each type of architecture investigated -\nrecurrent net-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "work, self-attention and cross-modal attention."
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "ber of trainable parameters. Then, we test end-to-end learn-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "ing with the full set of parameters. For these experiments,"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "we restrict the choice of the visual encoder network to Mo-"
        },
        {
          "Cross-Modal Attention (CMA) Models": "bileFaceNet to avoid overﬁtting."
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "4.3. Validation Results"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "The validation results for preliminary experiments on mod-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "els with frozen feature extraction networks are reported in"
        },
        {
          "Cross-Modal Attention (CMA) Models": "Tab. 1. We denote\nthe\nthree\ntypes of\narchitectures\nem-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "ployed\nas Audiovisual-[RNN, SA, CMA]\nfor\nrecurrent,"
        },
        {
          "Cross-Modal Attention (CMA) Models": "self-attention, and cross-modal attention respectively. The"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "second column speciﬁes the feature extraction network used"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "for visual information, as Inception or Mobile for Inception-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "Resnetv1 and MobileFaceNet,\nrespectively.\nFor compari-"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "son, we also state the results of unimodal models trained"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "with self-attention and RNN."
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "It can be seen from Sec. 4.3 that our audiovisual models"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "outperform the challenge baseline by a wide margin."
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "We report\nthe validation results of\nthe best models per"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "architecture, trained end-to-end, in Tab. 3. All models share"
        },
        {
          "Cross-Modal Attention (CMA) Models": ""
        },
        {
          "Cross-Modal Attention (CMA) Models": "the same feature encoders,\ni.e., MobileFaceNet and the 1D"
        },
        {
          "Cross-Modal Attention (CMA) Models": "CNN pre-trained on RECOLA. The hyperparameter conﬁg-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: Validation results in CCC ↑, evaluated on the valida- Self-Attention(SA)Models",
      "data": [
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "Recurrent Models (RNNs)",
          "Ptotal": ""
        },
        {
          "Method": "E2E-AV-RNN",
          "Valence\nArousal\nAvg.": "0.551\n0.456\n0.361",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "AV-RNN\nInception\n109 K",
          "Ptotal": "28.8 M"
        },
        {
          "Method": "E2E-AV-SA",
          "Valence\nArousal\nAvg.": "0.380\n0.520\n0.450",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "AV-RNN\nMobile\n4.4 M",
          "Ptotal": "5.4 M"
        },
        {
          "Method": "E2E-AV-CMA",
          "Valence\nArousal\nAvg.": "0.388\n0.492\n0.440",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "E2E-AV-RNN\nMobile\n76 K",
          "Ptotal": "1.1 M"
        },
        {
          "Method": "Table 3.\nValidation results",
          "Valence\nArousal\nAvg.": "in CCC ↑, evaluated on the valida-",
          "Method\nVisual Encoder\nPsequence": "Self-Attention (SA) Models",
          "Ptotal": ""
        },
        {
          "Method": "tion set of Affwild2 in ABAW 2022. Reported results are for the",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "AV-SA\nInception\n765 K",
          "Ptotal": "28.1 M"
        },
        {
          "Method": "best multimodal models trained end-to-end with MobileFaceNet",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "AV-SA\nMobile\n482 K",
          "Ptotal": "1.51 M"
        },
        {
          "Method": "as visual encoder and 1D CNN pretrained on RECOLA as audio",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "encoder, and using RNN, SA and CMA for sequence modelling.",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "E2E-AV-SA\nMobile\n193 K",
          "Ptotal": "1.2 M"
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "AV-CMA\nInception\n134 K",
          "Ptotal": "28.1 M"
        },
        {
          "Method": "Hyper-Parameter",
          "Valence\nArousal\nAvg.": "E2E Models",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "AV-CMA\nMobile\n2.1 M",
          "Ptotal": "3.1 M"
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "AV-RNN\nAV-SA\nAV-CMA",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "E2E-AV-CMA\nMobile\n2.4 M",
          "Ptotal": "3.4 M"
        },
        {
          "Method": "nlayers",
          "Valence\nArousal\nAvg.": "1\n3\n4",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "dmodel",
          "Valence\nArousal\nAvg.": "64\n64\n256",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "activation",
          "Valence\nArousal\nAvg.": "SELU\nGELU\nGELU",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "dropout",
          "Valence\nArousal\nAvg.": "0.5\n0.5\n0.6",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "and the full number of parameters separately.",
          "Ptotal": ""
        },
        {
          "Method": "learning rate",
          "Valence\nArousal\nAvg.": "0.0002\n0.002\n0.0001",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "weight decay",
          "Valence\nArousal\nAvg.": "0.023\n0.008\n0.06",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "5.1. RNN performance",
          "Ptotal": ""
        },
        {
          "Method": "λmse",
          "Valence\nArousal\nAvg.": "0.84\n0.78\n0.18",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "λce",
          "Valence\nArousal\nAvg.": "0.88\n0.27\n0.76",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "creases",
          "Ptotal": "feature"
        },
        {
          "Method": "df eedf orward",
          "Valence\nArousal\nAvg.": "-\n256\n256",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "encoder with the less complex MobileFaceNet",
          "Ptotal": "(0.413 to"
        },
        {
          "Method": "nheads",
          "Valence\nArousal\nAvg.": "-\n8\n4",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "0.378). At\nthe same time,",
          "Ptotal": ""
        },
        {
          "Method": "nV −→A",
          "Valence\nArousal\nAvg.": "-\n-\n3",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "layers",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "trainable part of the model",
          "Ptotal": ""
        },
        {
          "Method": "nA−→V",
          "Valence\nArousal\nAvg.": "-\n-\n1",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "layers",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "4.4 M. We interpret",
          "Ptotal": ""
        },
        {
          "Method": "Context aggregation",
          "Valence\nArousal\nAvg.": "uni\n-\n-",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "turned by the smaller CNN.",
          "Ptotal": ""
        },
        {
          "Method": "dhidden",
          "Valence\nArousal\nAvg.": "64\n-\n-",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "However,\nusing\nthe\nlightweight",
          "Ptotal": "together"
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "Table 4. Hyperparameter conﬁgurations for",
          "Valence\nArousal\nAvg.": "the best performing",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "models. Models are trained end-to-end with recurrent neural net-",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "When the feature extractors are fully trainable,",
          "Ptotal": "the perfor-"
        },
        {
          "Method": "work, self-attention, and cross-modal attention networks, respec-",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "tively.",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "of model parameters decreased,",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "urations of the best models are given in Tab. 4.",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "In addition, we report",
          "Valence\nArousal\nAvg.": "the number of parameters for the",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "conclude from this that",
          "Ptotal": ""
        },
        {
          "Method": "best performing audiovisual models to allow for compari-",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "cient",
          "Ptotal": ""
        },
        {
          "Method": "son of computational costs.",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "coders.",
          "Ptotal": ""
        },
        {
          "Method": "5. Discussion",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "When judging performance, we analyse the mean value of",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "CCC for valence and arousal, which is the metric used in",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "the VA Track of the ABAW 2022 challenge. We ﬁrst discuss",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        },
        {
          "Method": "how the choice of the visual CNN impacts the RNN models,",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "FaceNet performs better",
          "Ptotal": ""
        },
        {
          "Method": "and the impact of end-to-end learning. We then compare",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "average CCC scores of 0.389 and 0.374,",
          "Ptotal": ""
        },
        {
          "Method": "the performance of self-attention and cross-attention, before",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "the same time,\nthe size of\nthe attention model",
          "Ptotal": "smaller"
        },
        {
          "Method": "contrasting RNNs and attention models.",
          "Valence\nArousal\nAvg.": "",
          "Method\nVisual Encoder\nPsequence": "",
          "Ptotal": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "Panagiotis Antoniadis, Ioannis Pikoulis, Panagiotis P Filnti-"
        },
        {
          "choice to perform continuous-time multimodal information": "sis, and Petros Maragos. An audiovisual and contextual ap-"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "proach for categorical and continuous emotion recognition"
        },
        {
          "choice to perform continuous-time multimodal information": "the IEEE/CVF International\nin-the-wild.\nIn Proceedings of"
        },
        {
          "choice to perform continuous-time multimodal information": "Conference on Computer Vision, pages 3645–3651, 2021. 3"
        },
        {
          "choice to perform continuous-time multimodal information": "[2] Michael Braun, Jonas Schubert, Bastian Pﬂeging, and Flo-"
        },
        {
          "choice to perform continuous-time multimodal information": "rian Alt. Improving driver emotions with affective strategies."
        },
        {
          "choice to perform continuous-time multimodal information": "Multimodal Technologies and Interaction, 3(1), 2019. 1"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "[3] Cong Cai, Yu He, Licai Sun, Zheng Lian, Bin Liu, Jianhua"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "Tao, Mingyu Xu, and Kexin Wang. Multimodal sentiment"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "analysis based on recurrent neural network and multimodal"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "the 2nd on Multimodal Senti-\nattention.\nIn Proceedings of"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "ment Analysis Challenge, pages 61–67. 2021. 2"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "[4] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An-"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "drew Zisserman. Vggface2: A dataset for recognising faces"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "across pose and age.\nIn 2018 13th IEEE International Con-"
        },
        {
          "choice to perform continuous-time multimodal information": "ference on Automatic Face Gesture Recognition (FG 2018),"
        },
        {
          "choice to perform continuous-time multimodal information": "pages 67–74, 2018. 3"
        },
        {
          "choice to perform continuous-time multimodal information": "Jiang,\nand H. Sahli.\nTransformer\nencoder"
        },
        {
          "choice to perform continuous-time multimodal information": "with multi-modal multi-head attention for continuous affect"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "recognition.\nIEEE Transactions on Multimedia, pages 1–1,"
        },
        {
          "choice to perform continuous-time multimodal information": "2020. 2"
        },
        {
          "choice to perform continuous-time multimodal information": "Sheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mo-"
        },
        {
          "choice to perform continuous-time multimodal information": "bilefacenets: Efﬁcient cnns for accurate real-time face ver-"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "iﬁcation on mobile devices.\nIn Jie Zhou, Yunhong Wang,"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "Zhenan Sun, Zhenhong Jia, Jianjiang Feng, Shiguang Shan,"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "Kurban Ubul, and Zhenhua Guo, editors, Biometric Recog-"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "nition, pages 428–438, Cham, 2018. Springer\nInternational"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "Publishing. 3"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "[7] D. Deng, Z. Chen, and B. E. Shi. Multitask Emotion Recog-"
        },
        {
          "choice to perform continuous-time multimodal information": ""
        },
        {
          "choice to perform continuous-time multimodal information": "nition with Incomplete Labels.\nIn 2020 15th IEEE Interna-"
        },
        {
          "choice to perform continuous-time multimodal information": "tional Conference on Automatic Face and Gesture Recogni-"
        },
        {
          "choice to perform continuous-time multimodal information": "tion (FG 2020), pages 592–599, 2020. 3"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "compared to 765 k parameters).": "For the cross-modal attention (CMA) models, replacing",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "between valence and arousal for future work."
        },
        {
          "compared to 765 k parameters).": "FaceNet with MobileFaceNet also increases performance,",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "from 0.378 to 0.392. However, the transformer network be-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "7. Conclusion"
        },
        {
          "compared to 765 k parameters).": "comes much larger, going from 134 k parameters to 2.1 M",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "On a wide range of\nsequence modelling tasks,\nattention"
        },
        {
          "compared to 765 k parameters).": "parameters. As can be seen from these scores,\nthe perfor-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "models demonstrated superior generalisation performance"
        },
        {
          "compared to 765 k parameters).": "mance of\nself-attention and cross-attention appears\nto be",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "than recurrent models in recent years. However,\nit is worth"
        },
        {
          "compared to 765 k parameters).": "similar if the feature extractors are frozen, with CMA per-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "noting that\nthe recurrent models have the natural ability to"
        },
        {
          "compared to 765 k parameters).": "forming slightly better.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "cope with the challenges in learning from time-continuous"
        },
        {
          "compared to 765 k parameters).": "When using end-to-end learning, performance increases",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "sequence data, by inferring the latent states with unbounded"
        },
        {
          "compared to 765 k parameters).": "signiﬁcantly for\nthe self-attention model, with an average",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "context,\nat\nleast\nin principle.\nTherefore,\nin the\ncase of"
        },
        {
          "compared to 765 k parameters).": "validation CCC of 0.450. At\nthe same time,\nthe number of",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "continuous-time multimodal affect recognition, a recurrent"
        },
        {
          "compared to 765 k parameters).": "parameters in the sequence part of\nthe network shrinks to",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "neural network architecture may still be a natural choice to"
        },
        {
          "compared to 765 k parameters).": "193 k. The cross-attention model also beneﬁts greatly from",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "model\nthe latent states of face and voice data and their in-"
        },
        {
          "compared to 765 k parameters).": "end-to-end learning, achieving a score of 0.44. The number",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "teractions in a time-continuous manner. Extensive evalua-"
        },
        {
          "compared to 765 k parameters).": "of parameters in the sequence part of the model is 2.4 M.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "tion of LSTM-RNNs, self-attention and cross-modal atten-"
        },
        {
          "compared to 765 k parameters).": "Comparing\nthe\ntwo\nbest models\ntrained\nend-to-end",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "tion on in-the-wild audiovisual affect recognition, suggests"
        },
        {
          "compared to 765 k parameters).": "shows that\nthe self-attention model outperforms the cross-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "that attention models may not necessarily be the optimal"
        },
        {
          "compared to 765 k parameters).": "attention, while being signiﬁcantly smaller. We hypothe-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "choice to perform continuous-time multimodal information"
        },
        {
          "compared to 765 k parameters).": "sise that\nthe lower complexity of\nthe self-attention model",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "fusion."
        },
        {
          "compared to 765 k parameters).": "helped discover a more efﬁcient architecture during end-to-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "end training.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "References"
        },
        {
          "compared to 765 k parameters).": "5.3. Comparison between RNN and attention for",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[1]\nPanagiotis Antoniadis, Ioannis Pikoulis, Panagiotis P Filnti-"
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "sis, and Petros Maragos. An audiovisual and contextual ap-"
        },
        {
          "compared to 765 k parameters).": "sequence modeling",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "proach for categorical and continuous emotion recognition"
        },
        {
          "compared to 765 k parameters).": "We now compare the performances of our RNN models and",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "the IEEE/CVF International\nin-the-wild.\nIn Proceedings of"
        },
        {
          "compared to 765 k parameters).": "attention models directly based on the results discussed in",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "Conference on Computer Vision, pages 3645–3651, 2021. 3"
        },
        {
          "compared to 765 k parameters).": "the two previous sections.\nIn the case the feature extractors",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[2] Michael Braun, Jonas Schubert, Bastian Pﬂeging, and Flo-"
        },
        {
          "compared to 765 k parameters).": "are frozen, for the larger FaceNet, the RNN outperforms the",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "rian Alt. Improving driver emotions with affective strategies."
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "Multimodal Technologies and Interaction, 3(1), 2019. 1"
        },
        {
          "compared to 765 k parameters).": "attention models.\nIf frozen MobileFaceNet\nis used,\nthe at-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[3] Cong Cai, Yu He, Licai Sun, Zheng Lian, Bin Liu, Jianhua"
        },
        {
          "compared to 765 k parameters).": "tention models outperform RNN. With end-to-end learning,",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "Tao, Mingyu Xu, and Kexin Wang. Multimodal sentiment"
        },
        {
          "compared to 765 k parameters).": "RNN beats both self-attention and cross-attention, while",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "analysis based on recurrent neural network and multimodal"
        },
        {
          "compared to 765 k parameters).": "also having fewer parameters.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "the 2nd on Multimodal Senti-\nattention.\nIn Proceedings of"
        },
        {
          "compared to 765 k parameters).": "We conclude from this\nthat our\ninitial hypothesis\nthat",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "ment Analysis Challenge, pages 61–67. 2021. 2"
        },
        {
          "compared to 765 k parameters).": "attention-based models consistently outperform RNNs for",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[4] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An-"
        },
        {
          "compared to 765 k parameters).": "emotion recognition in the wild has not been conﬁrmed.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "drew Zisserman. Vggface2: A dataset for recognising faces"
        },
        {
          "compared to 765 k parameters).": "When end-to-end learning is used in combination with shal-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "across pose and age.\nIn 2018 13th IEEE International Con-"
        },
        {
          "compared to 765 k parameters).": "low CNNs for feature encoding, RNNs perform superior to",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "ference on Automatic Face Gesture Recognition (FG 2018),"
        },
        {
          "compared to 765 k parameters).": "the attention-based models investigated in this paper.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "pages 67–74, 2018. 3"
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[5] H. Chen, D.\nJiang,\nand H. Sahli.\nTransformer\nencoder"
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "with multi-modal multi-head attention for continuous affect"
        },
        {
          "compared to 765 k parameters).": "6. Outlook",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "recognition.\nIEEE Transactions on Multimedia, pages 1–1,"
        },
        {
          "compared to 765 k parameters).": "We compared fusion performance using two CNNs of dif-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "2020. 2"
        },
        {
          "compared to 765 k parameters).": "ferent sizes as visual feature extractors, while using a small",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[6]\nSheng Chen, Yang Liu, Xiang Gao, and Zhen Han. Mo-"
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "bilefacenets: Efﬁcient cnns for accurate real-time face ver-"
        },
        {
          "compared to 765 k parameters).": "1D-CNN for extracting audio features. Another study could",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "iﬁcation on mobile devices.\nIn Jie Zhou, Yunhong Wang,"
        },
        {
          "compared to 765 k parameters).": "focus on choosing different audio networks, e.g. a larger",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "Zhenan Sun, Zhenhong Jia, Jianjiang Feng, Shiguang Shan,"
        },
        {
          "compared to 765 k parameters).": "model like VGGish [13], and comparing the effects.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "Kurban Ubul, and Zhenhua Guo, editors, Biometric Recog-"
        },
        {
          "compared to 765 k parameters).": "The models used in this work had limited temporal con-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "nition, pages 428–438, Cham, 2018. Springer\nInternational"
        },
        {
          "compared to 765 k parameters).": "text due to computational constraints. Future studies could",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "Publishing. 3"
        },
        {
          "compared to 765 k parameters).": "extend towards greater sequence lengths to investigate how",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "[7] D. Deng, Z. Chen, and B. E. Shi. Multitask Emotion Recog-"
        },
        {
          "compared to 765 k parameters).": "well the models capture long-term dependencies.",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": ""
        },
        {
          "compared to 765 k parameters).": "",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "nition with Incomplete Labels.\nIn 2020 15th IEEE Interna-"
        },
        {
          "compared to 765 k parameters).": "Our analysis has focused on the average of valence and",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "tional Conference on Automatic Face and Gesture Recogni-"
        },
        {
          "compared to 765 k parameters).": "arousal as a metric,\nin order\nto judge the overall perfor-",
          "mance of\nthe models. We leave the analysis of\ntrade-offs": "tion (FG 2020), pages 592–599, 2020. 3"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "tillation for better uncertainty estimates in multitask emotion",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "the IEEE/CVF International\nrecognition.\nIn Proceedings of",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Conference on Computer Vision (ICCV) Workshops, pages",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "3557–3566, October 2021. 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[9] Nhu-Tai Do, Tram-Tran Nguyen-Quynh,\nand Soo-Hyung",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "pages 1–23, 2019. 2"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Kim. Affective expression analysis in-the-wild using multi-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[22] Dimitrios Kollias and Stefanos Zafeiriou. Expression, affect,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "task temporal statistical deep learning model.\nIn 2020 15th",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "action unit\nrecognition: Aff-wild2, multi-task learning and"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "IEEE International Conference on Automatic Face and Ges-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "arcface. arXiv preprint arXiv:1910.04855, 2019. 2, 3"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "ture Recognition (FG 2020), pages 624–628. IEEE, 2020. 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[23] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[10]\nFlorian Eyben, Martin W¨ollmer,\nTony Poitschke, Bj¨orn",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Schuller, Christoph Blaschke, Berthold F¨arber,\nand Nhu",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Nguyen-Thien. Emotion on the road—necessity, acceptance,",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "3"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "and feasibility of affective computing in the car. Advances in",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[24] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "human-computer interaction, 2010, 2010. 1",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[11] Ziwang Fu, Feng Liu, Hanyang Wang,\nJiayin Qi, Xian-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "puter Vision, pages 3652–3660, 2021. 3"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "gling Fu, Aimin Zhou,\nand Zhibin Li.\nA cross-modal",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[25] DN Krishna and Ankita Patil. Multimodal emotion recogni-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "fusion network based on self-attention and residual\nstruc-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "tion using cross-modal attention and 1d convolutional neural"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "arXiv preprint\nture\nfor multimodal\nemotion recognition.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "networks.\nIn Interspeech, pages 4243–4247, 2020. 3"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "arXiv:2111.02172, 2021. 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[26]\nF. Kuhnke, L. Rumberg,\nand J. Ostermann.\nTwo-stream"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[12] Albert Gu, Karan Goel,\nand Christopher R´e.\nEfﬁciently",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "aural-visual affect analysis in the wild.\nIn 2020 15th IEEE"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "modeling long sequences with structured state spaces. arXiv",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "International Conference on Automatic Face and Gesture"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "preprint arXiv:2111.00396, 2021. 2, 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Recognition (FG 2020), pages 600–605, 2020. 3, 5"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[13]\nS. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[27] Lawrence I.-Kuei Lin. A Concordance Correlation Coefﬁ-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "cient\nto Evaluate Reproducibility.\nBiometrics, 45(1):255–"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Seybold, M. Slaney, R. J. Weiss, and K. Wilson. Cnn archi-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "268, 1989. 5"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "tectures for\nlarge-scale audio classiﬁcation.\nIn 2017 IEEE",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[28]\nIlya Loshchilov and Frank Hutter. Decoupled weight decay"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "International Conference on Acoustics, Speech and Signal",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "regularization. arXiv preprint arXiv:1711.05101, 2017. 6"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Processing (ICASSP), pages 131–135, March 2017. 8",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[29] Ziyu Ma, Fuyan Ma, Bin Sun, and Shutao Li. Hybrid muti-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[14]\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "modal fusion for dimensional emotion recognition.\nIn Pro-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "memory. Neural computation, 9(8):1735–1780, 1997. 2, 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "ceedings of the 2nd on Multimodal Sentiment Analysis Chal-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[15]\nJ. Huang,\nJ. Tao, B. Liu, Z. Lian,\nand M. Niu.\nMulti-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "lenge, pages 29–36. 2021. 2"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "modal\ntransformer\nfusion for continuous emotion recogni-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[30] Danny Merkx\nand\nStefan\nL\nFrank.\nHuman\nsentence"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "tion. In ICASSP 2020 - 2020 IEEE International Conference",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "arXiv\npreprint\nprocessing:\nRecurrence\nor\nattention?"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "on Acoustics, Speech and Signal Processing (ICASSP), pages",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "arXiv:2005.09471, 2020. 2"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "3507–3511, May 2020. 2",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[31] Rosalind W Picard. Affective computing:\nfrom laughter to"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[16] Giancarlo Kerg, Bhargav Kanuparthi, Anirudh Goyal, Kyle",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "ieee.\nIEEE Transactions on Affective Computing, 1(1):11–"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Goyette, Yoshua Bengio, and Guillaume Lajoie. Untangling",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "17, 2010. 1"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "tradeoffs between recurrence and self-attention in neural net-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[32]\nSoujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hus-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "works. arXiv preprint arXiv:2006.09471, 2020. 2",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "sain. A review of affective computing: From unimodal anal-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[17] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "ysis to multimodal fusion.\nInformation Fusion, 37:98–125,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "2017. 2"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "learning challenges. arXiv preprint arXiv:2202.10659, 2022.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[33] Vandana Rajan, Alessio Brutti, and Andrea Cavallaro.\nIs"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "6",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "cross-attention preferable to self-attention for multi-modal"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[18] D. Kollias,\nA.\nSchulc,\nE. Hajiyev,\nand\nS.\nZafeiriou.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "emotion recognition?\narXiv preprint arXiv:2202.09263,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Analysing affective behavior\nin the ﬁrst abaw 2020 com-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "2022. 3"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "petition.\nIn 2020 15th IEEE International Conference on",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "[34]\nFabien Ringeval, Bj¨orn Schuller, Michel Valstar, Nicholas"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Automatic Face and Gesture Recognition (FG 2020), pages",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Cummins, Roddy Cowie, Leili Tavabi, Maximilian Schmitt,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "637–643, 2020. 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": ""
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Sina Alisamir,\nShahin Amiriparian,\nEva-Maria Messner,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[19] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Siyang Song, Shuo Liu, Ziping Zhao, Adria Mallol-Ragolta,"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Zhao Ren, Mohammad Soleymani, and Maja Pantic. Avec"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "arXiv preprint\nfect and action units\nin a single network.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "2019 workshop and challenge: State-of-mind, detecting de-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "arXiv:1910.11111, 2019. 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "pression with ai, and cross-cultural affect\nrecognition.\nIn"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "[20] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "Proceedings of\nthe 9th International on Audio/Visual Emo-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "Zafeiriou.\nDistribution matching for heterogeneous multi-",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "tion Challenge and Workshop, AVEC ’19, page 3–12, New"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy.",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "York, NY, USA, 2019. Association for Computing Machin-"
        },
        {
          "[8] Didan Deng, Liang Wu, and Bertram E. Shi.\nIterative dis-": "arXiv:2105.03790, 2021. 3",
          "[21] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,": "ery. 1"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "ducing the recola multimodal corpus of remote collaborative",
          "mation in the wild. In 2020 15th IEEE International Confer-": "ence on Automatic Face and Gesture Recognition (FG 2020),"
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "and affective interactions.\nIn 2013 10th IEEE International",
          "mation in the wild. In 2020 15th IEEE International Confer-": "pages 632–636. IEEE, 2020. 3"
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Conference and Workshops on Automatic Face and Gesture",
          "mation in the wild. In 2020 15th IEEE International Confer-": "Jianfeng Zhao, Xia Mao, and Lijiang Chen. Speech emotion"
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Recognition (FG), pages 1–8, 2013. 3",
          "mation in the wild. In 2020 15th IEEE International Confer-": "recognition using deep 1d & 2d cnn lstm networks. Biomed-"
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[36] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-",
          "mation in the wild. In 2020 15th IEEE International Confer-": "ical signal processing and control, 47:312–323, 2019. 3"
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "moginov,\nand Liang-Chieh Chen.\nMobilenetv2:\nInverted",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "residuals and linear bottlenecks.\nIn Proceedings of the IEEE",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Conference on Computer Vision and Pattern Recognition,",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "pages 4510–4520, 2018. 3",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[37]\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Facenet: A uniﬁed embedding for face recognition and clus-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "tering.\nIn 2015 IEEE Conference on Computer Vision and",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Pattern Recognition (CVPR), pages 815–823, June 2015. 3",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[38] Licai Sun, Zheng Lian, Jianhua Tao, Bin Liu, and Mingyue",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Niu. Multi-modal continuous dimensional emotion recogni-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "tion using recurrent neural network and self-attention mecha-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "nism.\nIn Proceedings of the 1st International on Multimodal",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Sentiment Analysis in Real-life Media Challenge and Work-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "shop, pages 27–34, 2020. 2",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[39] Licai Sun, Mingyu Xu, Zheng Lian, Bin Liu, Jianhua Tao,",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Meng Wang, and Yuan Cheng. Multimodal emotion recogni-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "tion and sentiment analysis via attention enhanced recurrent",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "the 2nd on Multimodal Sentiment\nmodel.\nIn Proceedings of",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Analysis Challenge, pages 15–20. 2021. 2",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[40] Antoine Toisoul, Jean Kossaiﬁ, Adrian Bulat, Georgios Tz-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "imiropoulos, and Maja Pantic. Estimation of continuous va-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "lence and arousal levels from faces in naturalistic conditions.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Nature Machine Intelligence, 3(1):42–50, 2021. 5",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[41] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Multimodal\ntransformer for unaligned multimodal\nlanguage",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "the 57th Annual Meeting of\nsequences.\nIn Proceedings of",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "the Association for Computational Linguistics, pages 6558–",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "6569, Florence,\nItaly, July 2019. Association for Computa-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "tional Linguistics. 2, 4",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[42]\nPanagiotis\nTzirakis,\nStefanos\nZafeiriou,\nand\nBjorn W",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Schuller.\nEnd2you–the\nimperial\ntoolkit\nfor\nmulti-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "arXiv\npreprint\nmodal\nproﬁling\nby\nend-to-end\nlearning.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "arXiv:1802.01115, 2018. 3",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "reit, Llion Jones, Aidan N. Gomez, Ł. ukasz Kaiser, and Illia",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Polosukhin.\nAttention is all you need.\npages 5998–6008.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Curran Associates, Inc, 2017. 2, 3, 4, 5",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[44]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "and Pattern Recognition Workshops\nIn Computer Vision",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987.",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "IEEE, 2017. 2",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[45]\nSu Zhang, Yi Ding, Ziquan Wei, and Cuntai Guan. Contin-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "uous emotion recognition with audio-visual\nleader-follower",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "the IEEE/CVF Interna-\nattentive fusion.\nIn Proceedings of",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "tional Conference on Computer Vision, pages 3567–3574,",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "2021. 3",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "[46] Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, and Shiguang",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        },
        {
          "[35]\nF. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne. Intro-": "Shan. M 3 f: Multi-modal continuous valence-arousal esti-",
          "mation in the wild. In 2020 15th IEEE International Confer-": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Panagiotis P Filntisis, and Petros Maragos. An audiovisual and contextual approach for categorical and continuous emotion recognition in-the-wild",
      "authors": [
        "Panagiotis Antoniadis",
        "Ioannis Pikoulis"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "Improving driver emotions with affective strategies",
      "authors": [
        "Michael Braun",
        "Jonas Schubert",
        "Bastian Pfleging",
        "Florian Alt"
      ],
      "year": "2019",
      "venue": "Multimodal Technologies and Interaction"
    },
    {
      "citation_id": "3",
      "title": "Multimodal sentiment analysis based on recurrent neural network and multimodal attention",
      "authors": [
        "Cong Cai",
        "Yu He",
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao",
        "Mingyu Xu",
        "Kexin Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "4",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Qiong Cao",
        "Li Shen",
        "Weidi Xie",
        "M Omkar",
        "Andrew Parkhi",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "5",
      "title": "Transformer encoder with multi-modal multi-head attention for continuous affect recognition",
      "authors": [
        "H Chen",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "Sheng Chen",
        "Yang Liu",
        "Xiang Gao",
        "Zhen Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition"
    },
    {
      "citation_id": "7",
      "title": "Multitask Emotion Recognition with Incomplete Labels",
      "authors": [
        "D Deng",
        "Z Chen",
        "B Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "8",
      "title": "Iterative distillation for better uncertainty estimates in multitask emotion recognition",
      "authors": [
        "Didan Deng",
        "Liang Wu",
        "Bertram Shi"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "9",
      "title": "Affective expression analysis in-the-wild using multitask temporal statistical deep learning model",
      "authors": [
        "Nhu-Tai Do",
        "Soo-Hyung Tram-Tran Nguyen-Quynh",
        "Kim"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "10",
      "title": "Emotion on the road-necessity, acceptance, and feasibility of affective computing in the car",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Tony Poitschke",
        "Björn Schuller",
        "Christoph Blaschke",
        "Berthold Färber",
        "Nhu Nguyen-Thien"
      ],
      "year": "2010",
      "venue": "Advances in human-computer interaction"
    },
    {
      "citation_id": "11",
      "title": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "authors": [
        "Ziwang Fu",
        "Feng Liu",
        "Hanyang Wang",
        "Jiayin Qi",
        "Xiangling Fu",
        "Aimin Zhou",
        "Zhibin Li"
      ],
      "year": "2021",
      "venue": "A cross-modal fusion network based on self-attention and residual structure for multimodal emotion recognition",
      "arxiv": "arXiv:2111.02172"
    },
    {
      "citation_id": "12",
      "title": "Efficiently modeling long sequences with structured state spaces",
      "authors": [
        "Albert Gu",
        "Karan Goel",
        "Christopher Ré"
      ],
      "year": "2021",
      "venue": "Efficiently modeling long sequences with structured state spaces",
      "arxiv": "arXiv:2111.00396"
    },
    {
      "citation_id": "13",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold",
        "M Slaney",
        "R Weiss",
        "K Wilson"
      ],
      "year": "2008",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "15",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2002",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "16",
      "title": "Untangling tradeoffs between recurrence and self-attention in neural networks",
      "authors": [
        "Giancarlo Kerg",
        "Bhargav Kanuparthi",
        "Anirudh Goyal",
        "Kyle Goyette",
        "Yoshua Bengio",
        "Guillaume Lajoie"
      ],
      "year": "2020",
      "venue": "Untangling tradeoffs between recurrence and self-attention in neural networks",
      "arxiv": "arXiv:2006.09471"
    },
    {
      "citation_id": "17",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "19",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "20",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "21",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "23",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "24",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "25",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "Krishna Dn",
        "Ankita Patil"
      ],
      "year": "2020",
      "venue": "In Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Two-stream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "27",
      "title": "A Concordance Correlation Coefficient to Evaluate Reproducibility",
      "authors": [
        "I.-Kuei Lawrence",
        "Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "28",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "29",
      "title": "Hybrid mutimodal fusion for dimensional emotion recognition",
      "authors": [
        "Ziyu Ma",
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "30",
      "title": "Human sentence processing: Recurrence or attention?",
      "authors": [
        "Danny Merkx",
        "Stefan Frank"
      ],
      "year": "2020",
      "venue": "Human sentence processing: Recurrence or attention?",
      "arxiv": "arXiv:2005.09471"
    },
    {
      "citation_id": "31",
      "title": "Affective computing: from laughter to ieee",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Rajiv Bajpai, and Amir Hussain. A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "33",
      "title": "Is cross-attention preferable to self-attention for multi-modal emotion recognition",
      "authors": [
        "Alessio Vandana Rajan",
        "Andrea Brutti",
        "Cavallaro"
      ],
      "year": "2022",
      "venue": "Is cross-attention preferable to self-attention for multi-modal emotion recognition",
      "arxiv": "arXiv:2202.09263"
    },
    {
      "citation_id": "34",
      "title": "Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Leili Tavabi",
        "Maximilian Schmitt",
        "Sina Alisamir",
        "Shahin Amiriparian",
        "Eva-Maria Messner",
        "Siyang Song",
        "Shuo Liu",
        "Ziping Zhao",
        "Adria Mallol-Ragolta",
        "Zhao Ren",
        "Mohammad Soleymani",
        "Maja Pantic"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop, AVEC '19"
    },
    {
      "citation_id": "35",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "36",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "Mark Sandler",
        "Andrew Howard",
        "Menglong Zhu",
        "Andrey Zhmoginov",
        "Liang-Chieh Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2003",
      "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Multi-modal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Jianhua Tao",
        "Bin Liu",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "39",
      "title": "Multimodal emotion recognition and sentiment analysis via attention enhanced recurrent model",
      "authors": [
        "Licai Sun",
        "Mingyu Xu",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao",
        "Meng Wang",
        "Yuan Cheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "40",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "41",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "End2you-the imperial toolkit for multimodal profiling by end-to-end learning",
      "authors": [
        "Panagiotis Tzirakis",
        "Stefanos Zafeiriou",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "End2you-the imperial toolkit for multimodal profiling by end-to-end learning",
      "arxiv": "arXiv:1802.01115"
    },
    {
      "citation_id": "43",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Kaiser",
        "Illia Polosukhin"
      ],
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "44",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "45",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "Su Zhang",
        "Yi Ding",
        "Ziquan Wei",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "M 3 f: Multi-modal continuous valence-arousal esti-mation in the wild",
      "authors": [
        "Yuan-Hang Zhang",
        "Rulin Huang",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "47",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical signal processing and control"
    }
  ]
}