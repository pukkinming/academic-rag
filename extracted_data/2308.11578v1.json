{
  "paper_id": "2308.11578v1",
  "title": "Refashioning Emotion Recognition Modelling: The Advent Of Generalised Large Models",
  "published": "2023-08-21T13:14:32Z",
  "authors": [
    "Zixing Zhang",
    "Liyizhe Peng",
    "Tao Pang",
    "Jing Han",
    "Huan Zhao",
    "Bjorn W. Schuller"
  ],
  "keywords": [
    "Emotion Recognition",
    "Large Language Model",
    "In-Context Learning",
    "Few-Shot Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, accuracy, generalisation, and explanation. Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion recognition in the new era of advanced and generalised large models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affect modelling is key for human-centred machine intelligence systems, allowing for more empathetic, adaptive, and engaging interactions, by considering users' emotions. Developing valid and reliable affective models that accurately capture and understand human emotions can pave the way for various applications, including affective dialogue systems, affect-aware recommender systems, and emotion-based adaptive interfaces. Over the past decades, the availability of large-scale affective datasets and rich computational power, as well as the advancements in deep learning algorithms, have revolutionised the field of affect modelling research.\n\nIn particular, deep learning techniques, such as neural networks, have proven to be highly effective in capturing complex patterns and relationships within affective data  [1] ,  [2] ,  [3] . For instance, Convolutional Neural Networks (CNNs) are efficient to learn and extract relevant features, and to capture local patterns and dependencies, which have demonstrated remarkable success in emotion recognition tasks across various domains and datasets  [4] ,  [5] . Likewise, Recurrent Neural Networks (RNNs) are also widely used in affective modelling when processing sequential data like speech and text, by capturing temporal dependencies and contextual information  [6] ,  [7] . Moreover, researchers have also made massive efforts to develop and implement various algorithms and approaches, aiming at more accurate and robust emotion recognition systems to understand human emotions. Such efforts include but are not limited to integrating information from different modalities  [8] ,  [9] , developing large-scale emotion datasets for model training and validation  [10] ,  [11] , transferring knowledge of other relevant tasks by leveraging pre-trained models  [12] ,  [13] , and increasing the interpretability and providing insights into the decisions  [14] ,  [15] .\n\nHowever, all aforementioned works are built typically for emotion recognition tasks, and normally the employed models focus on limited and specific domains. In other words, though prevalent in various emotion-aware systems and applications today, these specialised models lack broad generalisation and adaptability. For instance, a system specifically designed to predict a set of six emotions may not be applicable or effective in predicting an unseen or different emotion. Similarly, an emotion recognition model designed specifically for the Chinese language cannot be directly applied to the English language. These models and systems designed for specific tasks are often referred to as Artificial Narrow Intelligence (ANI).\n\nIn contrast to ANI, a more advanced and comprehensive form of artificial intelligence is Artificial General Intelligence (AGI), being able to learn, adapt, and apply knowledge across a wide range of tasks. While AGI systems may not be specifically tailored for emotion recognition tasks, they have the potential to achieve comparable levels of accuracy and effectiveness as ANI ones that are specifically designed for the task. In particular, recent breakthroughs in large language models (LLMs), such as GPT-4, are demonstrating significant advancements in natural language processing. These advancements may also open a new frontier for arXiv:2308.11578v1 [cs.CL] 21 Aug 2023 emotion recognition. These models are normally pre-trained over large-scale corpora, and have showcased their strong capabilities in various domains and tasks, such as text generation and natural language understanding.\n\nWhen utilising LLMs for emotion recognition, there are several potential benefits that may enhance the performance and capabilities of the models: 1) Large Training Data: being trained on vast amounts of data, LLMs can capture more diverse patterns, linguistic cues, and contextual information related to emotions, and thus might lead to better recognition performance; 2) Explanations: LLMs can potentially explain the reasoning behind their decisions, and therefore can increase the interpretability and transparency of the emotion recognition process; 3) Generalisation: LLMs are designed to learn general language patterns and can generalise well to unseen data. This allows them to recognise emotions that may not have been explicitly encountered during training; 4) Cross-Domain Application: LLMs have the potential to be applied across various domains, since they are trained on a wide range of data sources, and thus hold the potential to understand emotions expressed in various domains ranging from customer reviews to conversational data, which allows for wider applicability.\n\nTherefore, the focus of this work is on the analysis of the emotion recognition capabilities of LLMs and their implications. Also, we share our views of a future in LLMs-based affective modelling. For this aim, we choose three LLMs and conduct a comparative analysis of their emotion recognition performance against other state-of-theart (SOTA) non-LLM-based works using seven emotional datasets. This comparison allows us to assess the advancements and effectiveness of LLM-based approaches over specific deep-learning models in the context of emotion recognition. More specifically, we investigate three different scenarios for LLM-based emotion recognition: 1) emotion prediction of a given sample in the absence of any contextual information; 2) emotion prediction of a sample with context information of that sample; 3) emotion prediction of a sample while an appropriate number of examples are provided as demonstrations. By exploring these three scenarios, we aim to assess LLM-based emotion recognition models' effectiveness in different contextual settings. Besides, this work further delves into examining the generalisation, and interpretability of LLM-based affective modelling. Last, we engage in comprehensive discussions on the associated challenges and opportunities, hoping to pave the way for advancements and improvements in the field.\n\nThe main contributions of this paper are as follows:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "We for the first time carry out a deep detailed investigation of the emotion recognition capability of LLMs. In specific, we systematically evaluated three selected LLMs for emotion recognition tasks on seven datasets from two different languages.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "We extensively compare the performance with other state-of-the-art models, showing that LLMs can achieve comparable or superior performance on emotion recognition tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "•",
      "text": "We comprehensively discuss the obtained results and point out challenges and opportunities that are still faced in the research and development of LLM-based affective modelling\n\nThe structure of this work is organised as follows: Section 2 presents related works; Section 3 introduces the selected LLMs, datasets for evaluation, and implementation details. Experimental results, along with their analysis and comparison, are provided in Section 4. Challenges and opportunities of LLMs in the context of affective modelling are discussed in Section 5. The work concludes in Section 6.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "This section provides a brief overview of the relevant background and concepts.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Recent Advances Of Emotion Recognition",
      "text": "Over the past decades, particularly with the emergence of deep learning, significant endeavours have been dedicated to developing effective and robust emotion recognition models. In the following, we briefly introduce related studies primarily focusing on three key aspects/challenges: enhancing accuracy, facilitating generalisation, and enabling explainability of emotion recognition models.\n\nOne main research direction is to improve the correctness of the model, namely increasing the accuracy and precision of emotion recognition models. For this aim, several research studies have investigated various advanced feature extraction techniques, aiming to capture salient emotional representations. The features could be extracted from different modalities, such as speech, video, text, and physiological signals. For instance, three novel domain-specific audiovisual tasks were designed in  [16]  to learn better representations, by exploring large volumes of unlabelled data via self-supervised learning. There are also a huge number of studies that leverage different deep learning architectures to refine emotion recognition performance. For instance, a Transformer-based model was proposed in  [17]  to process ECG signals for emotion recognition, and the obtained contextualised representations achieved state-ofthe-art performance on the AMIGOS dataset. In addition, some other works focus on combining information from multiple modalities, so that a more comprehensive and holistic understanding of users' emotional states can be obtained  [9] .\n\nAiming at improving the generalisation of emotion recognition models, several advanced deep learning techniques have been explored, such as knowledge transfer  [18] , multitask learning  [19] , self-supervised learning  [20] , and domain adaptation  [21] . Applying these techniques appropriately could make the learnt emotion recognition model more effective across different domains, different contexts, and diverse scenarios. For example, a pre-trained model's knowledge gained from a large amount of data can be transferred to downstream tasks. Specifically, pre-trained language models (PLMs), such as bidirectional encoder representations from transformers (BERT) and their variants have been utilised in text-based emotion recognition and achieved promising performance. In  [22] , the efficacy of BERT and its variants has been comparatively examined on a cross-culture emotional database and Robustly optimised BERT pre-training Approach (RoBERTa) achieved the best performance among others.\n\nEnhancing interpretability is also critical for a reliable affective modelling system, where explanations should be provided along with its predictions. For this reason, advanced techniques have been applied in emotion recognition to improve the model interpretability. Such techniques include model-agnostic interpretation methods  [23] , attention mechanisms  [24] , and Bayesian neural networks  [25] .\n\nThese aforementioned efforts facilitate the development of correct, generalisible, and explainable affective computing, and tremendous progress in this research field has been made thanks to these efforts. However, these efforts deal with the three above-mentioned challenges separately, and it is not straightforward to integrate all as a whole.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The New Era Of Large Models",
      "text": "While transfer learning and PLMs have already shown promising advances on various downstream tasks, a new era of large models has started when researchers observed that scaling PLMs can largely lead to further improvement of the model capacity  [26] . These large-sized PLMs are termed Large Language Models (LLMs). Compared with smaller PLMs, LLMs significantly extend the model size and data size, even through the model architectures and the pretraining tasks are kept the same  [27] .\n\nVarious tech companies and research labs are striving to develop their own LLMs, and among these, there are some popular ones, including ChatGPT and the latest GPT-4 released by OpenAI, and LLaMA models introduced by Meta AI. Particularly, the launch of ChatGPT represents a significant step forward in the evolution of LLMs. Most LLMs are trained in two stages; LLMs normally are first pre-trained on a vast amount of text data, and then further adapted via a reinforcement learning from human feedback (RLHF) algorithm  [27] . While still based on standard deep learning and transfer learning, LLMs display a large performance improvement in terms of accuracy and robustness, achieving state-of-the-art results across multiple domains.\n\nFurther, LLMs exhibit surprising generalisation capability on unseen tasks, and are capable of solving a diverse range of complex tasks, even without further fine-tuning on each specialised task. This is mainly because LLMs have exhibited new emergent capabilities which were not present in smaller-scale PLMs  [28] . One such new emergent ability is in-context learning (ICL), which was first introduced by GPT-3 in  [29] . Being fed a prompt composed of task instructions and demonstrations (input-output examples of that task), the model learns to solve this new task and generate the output of an unseen inference-time example, without any further model training or gradient updates. Other emergent abilities include step-by-step reasoning, instruction following, program execution, and model calibration  [28] . With these advanced functionalities, LLMs have attracted increasing and widespread attention, leading to a paradigm shift in AI paradigm, shifting from task-specific models to more general-purpose models. In other words, the rapid technical progress of LLMs provides a fertile ground for AGI.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "At The Crossroad",
      "text": "Emerging as powerful tools to understand and generate human-readable text, LLMs have demonstrated exceptional capabilities in a wide range of NLP tasks. Their impact on many domains is significant, such as question-answering, code generation, and creative writing. Standing at the crossroads of traditional affective modelling and large models, in this paper, we aim to explore LLMs' impact on affective computing, as the effectiveness of LLMs on this task is not fully investigated, yet.\n\nHaving trained on massive amounts of textual data, LLMs offer exciting possibilities for understanding the subtle variations in emotional content. Besides, though not trained explicitly for emotion recognition like previous taskspecific models, the general-purpose LLMs have strong adaptation capabilities. Especially, prompting-based adaptation methods enable human-like interactions with an LLM via very simple prompts, entailing temporary learning during inference without specialised fine-tuning.\n\nWhile LLMs hold promising opportunities in this domain, there still lacks a thorough investigation of the effectiveness of LLMs in affective computing. Specifically, there are still several key questions that need to be answered:\n\n• How do LLMs perform in emotion recognition tasks, compared with state-of-the-art task-specific models?\n\n• Can one LLM be utilised across different affective computing-related tasks without further training or updating?",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "•",
      "text": "To what level can LLMs explain the reasons behind the detected emotion category from a given emotional input? Different from  [30]  which focused on evaluating the effectiveness of ChatGPT on three datasets, this paper explores the efficiency and effectiveness of three LLMs on seven emotional datasets from two languages. More importantly, we explore the emergent capability of in-context learning, showing that with few demonstration examples, LLMs perform better. The generalisation and explainability of LLMs for affective computing are also analysed, which is missing in previous works.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition With Large Lan-Guage Models",
      "text": "In this section, we outline our evaluation strategy for LLMbased emotion recognition. We begin by introducing the three LLMs selected for this evaluation. Then, we give an overview of the seven emotional datasets used in the evaluation. Lastly, we detail the implementation process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Large Language Models",
      "text": "In this work, we investigate the capabilities of LLMs to perform emotion recognition. For this aim, three LLMs are selected, including ChatGPT, Claude, and Bing Chat.\n\nChatGPT: Released by OpenAI in November 2022, Chat-GPT represents a significant advancement in the NLP field. When given a prompt, ChatGPT is designed to provide detailed responses. It builds upon the prior GPT-3.5 model, which has 175 billion parameters, but is specifically optimised for dialogues. Notably, ChatGPT underwent finetuning via the RLHF method to produce more human-like responses. The RLHF procedure comprises three stages: supervised fine-tuning, reward model training, and reinforcement learning fine-tuning. Through RLHF, ChatGPT's behaviour is better aligned with human values and preferences. Consequently, ChatGPT excels at human-like communication, generating coherent responses and engaging in meaningful dialogues. This innovation paves the way for diverse applications. In our study, we delved into the emotion recognition capabilities of ChatGPT by interacting with the OpenAI API using their official Python bindings. For our API interactions, we specified the model as gpt-3.5-turbo, which underpins ChatGPT. We also set the temperature to 0.01 to ensure more focused and deterministic outputs.\n\nClaude: Claude, launched in March 2023 by Anthropic, is an LLM-based AI Chatbot. As a strong competitor to ChatGPT, Claude is also capable of a wide variety of conversational and text processing tasks. These include creative writing, Q&A, project planning, and coding. With the aim of creating AI systems that are helpful, honest, and harmless, the RLHF technique was also employed in Claude  [31] . Further, Claude was trained using a \"Constitutional AI\" technique, where a model rather than a human determines the rankings/preferences based on a set of underlying principles  [32] . In this manner, the output generated by Claude can align with human values and goals from inception. Currently, access to the Claude API is limited, requiring an application to request access. However, the Claude app can be integrated into the Slack workspace free of charge without any application. Users can engage with Claude by mentioning @Claude in a channel or by sending Claude direct messages. For our purposes, we leveraged the Python Slack SDK to mimic functions akin to an API. By obtaining the User Token and Claude Bot ID from Slack and embedding them in our Python code, we can interact with Slack via a Python program. This automation not only streamlines our testing process but also significantly reduces our workload.\n\nBing Chat: Microsoft announced the next version of its web search engine -the new Bing in February 2023, augmented with a variety of cutting-edge technologies from both Microsoft and its partner OpenAI. With this, Microsoft aims at bringing together search, browsing, and chat into one unified experience. Particularly, the new Bing introduces a chat feature, Bing Chat, designed to deliver answers that feel more conversational than typical search engine responses. Bing Chat is powered by OpenAI's GPT-4 but customised specifically for search. Also, Bing Chat has access to the internet and thus can provide more up-todate information. This is different from ChatGPT, which was trained on data only up until September 2021. Additionally, in Bing Chat, the response to a user's query is grounded with search results, incorporating footnotes that trace back to original sources for verification. Although Bing Chat is exclusive to web access and lacks an API service, we leveraged ChatALL to interface with it. ChatALL serves as a client, granting users the ability to converse with several AI chatbots, Bing Chat included. A notable advantage of using ChatALL is the absence of token limitations when engaging with Bing Chat. It is worth mentioning that Bing Chat offers three distinct response formats: more creative, more balanced, and more precise. For the purposes of our study, we opted for the \"more precise\" format, aiming to obtain succinct and direct answers to our emotion-predictionrelated queries.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Datasets",
      "text": "In this part, we introduce the seven datasets employed in our study. A summary of their statistics can be found in Table  1 . As these datasets are publicly accessible, they facilitate easy verification of our results.\n\n• SST The Stanford Sentiment Treebank is an English corpus with fine-grained sentiment labels of 11,855 single sentences extracted from movie review data  [33] . These sentences were parsed using the Stanford parser, resulting in a total of 215,154 unique phrases from those parsed trees. Those phrases were then annotated by the classic Mechanical Turk for sentiment into 25 different levels. The obtained annotations for each phrase were then used to define fine-grained and binary versions of the task. For the fine-grained task, each sentence is labelled with one of five sentiment classes: negative, somewhat negative, neutral, somewhat positive, and positive; for the binary task, each sentence is labelled as either positive or negative (the neutral class is ignored). In this study, we do not consider phrase-level sentiment analysis. Sentences were split into training (8544), development (1101), and test splits (2210) for the fine-grained classification task. For the binary task, the three sets have 6920, 872, and 1821 sentences, respectively.\n\n• Friends Friends is a corpus derived from the Friends TV shows, comprising transcriptions of 1,000 dialogue spoken by the characters in the show across seasons 1 to 9  [34] . Each dialogue was regarded as an annotation task on Amazon Mechanical Turk. The goal was to label each of 14,503 utterances within the 1,000 dialogues into one of seven classes: anger, disgust, fear, joy, sadness, surprise, and neutral. The annotators were instructed to consider the context of the entire dialogue while assigning the sentiment labels. The dataset was further divided into three splits, resulting in 10,561/720, 1,178/80, and 2,764/200 utterances/dialogues in the training, development, and test sets, separately.\n\n• Mastodon The Mastodon dataset was built by crawling social media posts from Mastodon, and non-English posts were filtered out automatically  [35] . The corpus was completely manually anonymised and annotated by two students. While the dataset was originally designed for sentiment recognition and dialogue act recognition, here, we consider the sentiment analysis task only. Each post was labelled as positive, negative, or neutral. All posts were partitioned into two splits: 1,075 posts from 239 dialogues in the training set, and another 1,142 posts from 266 dialogues in the test set.\n\n• MOSI The Multimodal Opinion-level Sentiment Intensity (MOSI) dataset  [36]  is a multimodal dataset for studying sentiment and subjectivity in opinion videos. The dataset consists of 2199 opinion video segments, segmented from 93 videos generated by 89 distinct English-speaking YouTube users. The sentiment intensity annotation was carried out by master workers via Amazon Mechanical Turk. Each opinion segment was annotated as a sentiment spectrum between highly negative and highly positive in the range [-3,3]: strongly positive (labelled as +3), positive (+2), weakly positive (+1), neutral (0), weakly negative (-1), negative (-2), and strongly negative (-3). Besides, transcriptions are provided for each segment.\n\n• MOSEI CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) is the largest dataset of sentence-level sentiment analysis and emotion recognition in online videos  [11] . It contains 23,453 sentence utterance videos from 1000 Englishspeaking YouTube speakers. These sentence utterances were randomly chosen from various topics and monologue videos. Similar to MOSI, the dataset was annotated by 3 master judges via the Amazon Mechanical Turk platform. Each sentence in the MOSEI dataset is annotated for sentiment on a Likert scale from -3 to +3. It is also annotated for six emotions: happiness, sadness, anger, fear, disgust, and surprise.\n\n• CH-SIMS A Chinese single-and multi-modal sentiment analysis dataset, CH-SIMS, was introduced in  [37] . It contains 2281 refined video segments in the wild, collected from different movies, TV serials, and variety shows. The dataset includes both multimodal and independent unimodal annotations, allowing both unimodal and multimodal sentiment analysis. Each clip has a manual transcription. The dataset was annotated by five students to label each clip into one of three classes: -1 (negative), 0 (neutral), or 1 (positive). Then, the final sentiment annotation falls into one of five categories: negative, weakly negative, neutral, weakly positive, and positive. For experiments, the data was further split into training (1368), validation (456), and test (457) sets.\n\n• M 3 ED A Multi-modal Multi-scene Multi-label Emotional Dialogue (M 3 ED) dataset was recently proposed in  [38] . The dataset contains 990 dyadic emotional dialogues, 9082 turns, and 24,449 utterances, derived from 56 different TV series. M 3 ED is the first multimodal emotional dialogue dataset in Chinese. Similar to Friends, the dataset is annotated on utterance level with seven emotion categories, including Ekman's six basic emotions (happy, surprise, sad, disgust, anger, and fear) and neutral.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "Tasks: We performed sentiment analysis and emotion recognition tasks on the selected seven datasets, as the testbed to evaluate the affective modelling ability of three LLMs. Particularly, we adhered to others' existing works to ensure a fair comparison. For the SST dataset, we undertook both binary and five-class classification tasks. Binary sentiment classification tasks, distinguishing between positive and negative sentiments, were applied to the MOSI, MOSEI, and CH-SIMS datasets. The Mastodon dataset and the MOSI underwent a three-class classification: positive, neutral, and negative. Lastly, for the Friends and M 3 ED datasets, we executed a seven-class emotion classification.\n\nUsage of LLMs: When working with LLMs such as ChatGPT, we use a \"prompt\" -a specific input text -to elicit a response from the model. In our study on emotion recognition, we thoroughly investigated various prompting strategies. For context-free datasets (SST, MOSI, MO-SEI, and CH-SIMS), we assessed two strategies: contextfree zero-shot and context-free few-shot prompting. Meanwhile, for context-dependent datasets (Friends, Mastodon, and M 3 ED), we explored three strategies: context-free zeroshot prompting, context-aware zero-shot prompting, and context-aware few-shot prompting. Essentially, 'contextfree' refers to predicting the emotion-based solely on the content of the sentence itself. In contrast, 'context-aware' entails the prediction process that takes into account the surrounding context within the same conversation. Furthermore, 'zero-shot' indicates that no prior knowledge or examples are provided for the specific task, whereas 'fewshot' implies that a limited number of demonstration examples are given to facilitate inference-time learning. Detailed prompts for each strategy are provided as follows:  gories, determined by each specific task.\n\nFor example, the prompt is \"Classify the sentiment of sentence to Positive or Negative\" for MOSEI as k = 2. The instruction \"No need for you to explain. Don't repeat my sentence. Give me the simplest answer with a list and corresponding number.\" is aimed at a more concise and clearer response, making it easier to retrieve the desired sentiment/emotion prediction. Furthermore, for those binary datasets (positive and negative), we add \"Don't answer neutral.\" into the prompt, which can significantly reduce the number of neutral responses. However, there are still unexpected neutral responses in the binary classification (positive vs negative) in such a manner. In addition, for any sentence, whether from the training set (as task demonstrations) or from the test set, we add two asterisks (**) as separators before and after the sentence. These separators can make it easier for the model to distinguish the beginning and end of sentences, avoiding the model considering multiple sentences as one paragraph and only providing one label instead. Also, when performing zero/few-shot prompting on context-free datasets, incorporating multiple sentences into a single query notably decreased the evaluation's time and cost requirements. Specifically, 100 test sentences were fed in one prompt for zero-shot prompting, while 50 sentences in one prompt for few-shot prompting. In contrast, when performing zero-shot prompting on contextdependent datasets, we reduced the number of sentences to one. This was to prevent the model from gaining any contextual information. Last, the number of task demonstrations added to the prompt under few-shot prompting settings cannot exceed 100 due to the limited number of tokens (4096) set in ChatGPT. We selected an equal number of sentences per sentiment/emotion category: 50 sentences per category for binary classification, 30 per category for three-class classification, 20 per category for five-class classification, and 10 per category for seven-class classification. To ensure fairness in comparison between the three LLMs, for ICL strategies, we applied the same training data points per dataset as task demonstrations when testing each LLM.\n\nAfter collecting the LLM responses, we post-processed them to address any inconsistencies. Notably, due to occasional model instability, we sometimes received results outside our predefined classification range. For instance, the model might return \"Embarrassed\" or \"Grateful\", which are not within our specified emotion categories. To counteract this, we adjusted the model's temperature setting it to its maximum value of 2 and re-prompted the model until we obtained a result within the expected range. If, after five attempts, we still did not get a response within our classification range, we deemed the sentiment classification for that sentence as incorrect and randomly assigned it to an incorrect classification category.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Explainability Of Llms:",
      "text": "Providing clear and understandable explanations for the decisions made by complex models is an active area of research. As LLMs generate natural language responses, one potential approach could be to design prompts that request the models to elucidate the reasoning behind their decisions in human-readable format. To showcase the potential of this approach, we opted to conduct an exploratory experiment, using ChatGPT and selecting a few conversations from the Friends dataset. For each conversation, one of the sentences was chosen to be asked for the explanation behind the model's prediction. To solicit this additional information, we appended the following request to the original prompt this further request \"In addition, please explain how you judge the emotion of the <order of the selected sentence> sentence in the conversation based on the context\". The primary aim of this experiment was to showcase whether ChatGPT is capable of providing sound explanations for its emotion recognition decisions. By doing so, we hoped to gain a deeper insight into the model's decision-making process and to further explore the possibilities of integrating XAI principles with LLM capabilities.\n\nSOTA specialised models: For performance comparison with specialised affective computing models, we selected recently published SOTA works with competitive performance on each selected dataset respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics:",
      "text": "The primary metrics we employed for performance evaluation were accuracy and the macro F1 score. For the M 3 ED dataset, we used the weighted average F1 score to ensure a fair comparison with other studies utilising the same dataset. Similarly, for the Mastodon dataset, we calculated both macro precision and macro recall.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation And Discussions",
      "text": "We conducted experiments using seven datasets, with the comparative results presented in Tables  2 3 4 5 . Compared with SOTAs benchmarks, the LLM-based model demonstrates comparable, if not superior, performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "In-Context Learning",
      "text": "Equipping with a few demonstration examples, few-shot prompting can significantly improve the LLM's performance in affective modelling tasks.\n\nAs shown in Table  5  and Table  4 , on these four contextfree sentiment analysis datasets, few-shot prompting surpasses zero-shot prompting in most cases. For example, when testing with ChatGPT, the few-shot prompting strategy obtained increased accuracy (95.31% over 92.32%) on the SST-2 task. For the same task, Claude and Bing Chat also gain benefit from the in-context learning strategy, achieving improved accuracy when compared with zero-shot prompting where demonstration examples are not available. Similar observations can be found on SST-5, CH-SIMS, MOSI-2, MOSI-3, and MOSEI as well.\n\nTable  2  presents the result performance on the Mastondon dataset. As this dataset is composed of dialogues, i. e., posts/sentences with context information, we consider the performance comparison between zero-shot and fewshot promptings when the whole test dialogue is fed into the model as the input query. Table  2  shows that fewshot prompting outperforms zero-shot prompting in all measures on this dataset.\n\nFor multi-class emotion recognition tasks on two context-dependent datasets (Friends and M 3 ED), results in terms of WA, UA, F1, and accuracy per emotion category are given in Table  3 . When the performance metrics of WA, UA, and F1 is concerned, performance enhancements are obtained by the in-context learning strategy in 11 out of the total 18 cases (derived from 3 measures * 3 LLMs * 2 datasets = 18 cases).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Accuracy",
      "text": "For each of the selected datasets, SOTA performance from specialised models in previous works is provided for comparison. It is shown that, in general, LLM-based models yield competent or even better performance, both in binary classification and multi-class classification tasks. For instance, on the SST-2 and CH-SIMS datasets (cf. Table  5 ), the best performance in accuracy is obtained by Claude, reaching 95.86% and 88.70%, respectively, outperforming SOTA specialised models.\n\nWhen comparing the three selected LLMs, it is found that in general, ChatGPT and Claude are better than Bing Chat. For instance, as shown in Table  4 , on the MOSI-3 and MOSEI datasets, Bing Chat's performance is lower than ChatGPT and Claude. One of the potential reasons for the inferiority of Bing Chat on affective modelling is that the model is tailored for search. However, there is no consistent observation suggesting a particular LLM or prompting strategy as superior over the rest, as the optimal model and strategy setting vary across different datasets.\n\nFurther, we observe that incorporating context information (both context within the dialogue and context knowledge from the demonstrations) could enhance the recognition performance for emotional sentences within dialogues. Table  2  and Table  3  display results on context-available datasets. In all 10 global metrics (4 for Mastodon, 3 for Friends, and 3 for M 3 ED), 7 of them are improved when ChatGPT is deployed, 8 metrics are increased by Claude, and all 10 metrics are increased when leveraging Bing Chat.\n\nNot only do LLMs achieve comparable overall performance, but they also display better recognition performance in minority emotion classes. The ratio of each emotion class within two emotion datasets is listed in Table 3. Conventional models tend to struggle with emotions constituting less than 10% of the dataset samples compared to more prevalent emotion classes. For example, the 'fear' emotion class (which makes up 1.44% of the Friends dataset) achieved a peak accuracy of 31.03% in state-of-theart (SOTA) methodologies. In contrast, LLMs across various prompting strategies outperformed this, with Claude achieving the best result at 81.25%. Similarly, Claude's accuracy of recognising the 'angry' emotion in Friends, at 80.75%, was markedly superior to SOTA results. Comparable trends are observed with other minority emotions on the M 3 ED dataset. This can be attributed to the vast and diverse text LLMs have been trained on, which encompasses a broader emotional range than a specific dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Generalisation",
      "text": "Our experimental results show that, without explicitly training on a specific dataset, LLMs exhibit an impressive generalisation ability in multiple affective modelling tasks. Instead of learning specialised models for each of the seven distinct emotional datasets separately, all three selected models have shown their capabilities in managing varied emotion recognition tasks, underlining their versatility and broad applicability.\n\nIn particular, as discussed earlier, the LLMs could achieve comparable performance in zero-shot prompting scenarios. And this performance is further enhanced by the implementation of few-shot prompting, indicating that LLMs can adapt and learn from limited contextual information during the inference.\n\nMost importantly, the LLMs show a strong generalisation ability in their emotion analysis across different corpora and domains. The datasets utilised in this study encompass a wide array of text types, including TV show scripts, movie reviews, social media posts, and YouTube video transcriptions. Despite the variances in style, tone, and context inherent to these data sources, LLMs consistently produce reliable emotion recognition results.\n\nAdditionally, the results attest to the LLMs' crosslanguage generalisation capability. The corpora used in this study spanned both English and Chinese languages, further emphasising the models' adaptability. Such cross-language generalisation ability holds considerable potential for universal emotion recognition models that overcome language barriers.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Explanation",
      "text": "To further explore the explainability of LLMs in their emotion recognition decisions, we delve into ChatGPT's explanations for two sample conversations, as showcased in Table  6 . This provides an initial insight into how LLMs rationalise their decisions in the context of emotion recognition.\n\nAs presented in Table  6 , ChatGPT provides reasonable explanations for the sentences under analysis, effectively utilising the context information within the conversation. For example, taken out of context, the sentence \"Ohh, Well, this is just perfect!\" might seem positive. However, when considering the preceding conversation, the model correctly identifies it as conveying a negative emotion. It is important to note that while these intelligible explanations offer some insight into the model's decision-making process, the correlation between these human-readable explanations and the actual decision-making mechanism of the model is still ambiguous. This relationship warrants further investigation, which is crucial for establishing more reliable and transparent emotion recognition systems.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Challenges And Opportunities",
      "text": "In this section, we delve into several challenges and opportunities associated with the utilisation of LLMs for affective computing tasks.\n\nRobustness The robustness of LLMs in affective computing tasks is also a crucial topic. When users interact with LLMs like ChatGPT, they may introduce a range of  TABLE 4  Performance comparison between LLMs and SOTA works on the MOSI and MOSEI datasets measured by accuracy (Acc) and macro-F1 (F1).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Mosi-2 Mosi-3 Mosei Model",
      "text": "TFR-Net (2021)  [43]  83.49 -----CHFN (2022)  [44]  85. natural errors, encompassing typographical mistakes and grammatical inaccuracies. These errors mirror the imperfect, real-world language usage seen in everyday communications. Hence, it becomes pivotal to investigate the resilience of LLMs to such inconsistencies. It's equally important to explore strategies to enhance model robustness against this \"noise\", especially when discerning emotions from textual content. How well these models handle and respond to these errors not only tests their robustness but also their capability to simulate human-like comprehension and interaction. An LLM's capacity to correctly interpret sentiment or emotional intent in the face of such errors may serve as a strong indicator of its utility and effectiveness in real-world scenarios.\n\nAdaptation/fine-tuning In this present study, we have explored the capabilities of off-the-shelf LLMs without any additional adaptation/fine-tuning. Although LLMs are trained on vast amounts of text data across various domains, their abilities can be further tailored to specific goals through continual pre-training or fine-tuning by leveraging domain-specific data.\n\nIn the context of affective computing, to further enhance the affective modelling ability of LLMs, it would be of interest to deploy further pre-training approaches such as domain-adaptive pretraining (DAPT)  [55]  and task-adaptive pretraining (TAPT)  [56] . In DAPT, an LLM undergoes addi-tional unsupervised pre-training on a large corpus of unlabelled domain-specific data, with the aim of adapting the LLM to a particular domain. Conversely, in TAPT, an LLM is further pre-trained on the unlabelled training set for a specific task. Compared to DAPT, TAPT uses a significantly smaller, but far more task-relevant pretraining corpus. Research in  [55]  suggests that tailoring a pre-trained model to the domain of a target task via multiphase adaptive pretraining can significantly enhance task performance. It may be valuable to investigate whether further pretraining an LLM towards the domain of the emotion recognition corpus of interest can yield benefits. However, it is worth paying attention to issues such as the time and cost of pretraining, as well as the potential for incurring inductive biases.\n\nIn addition, fine-tuning an LLM in a supervised learning way can provide benefits for a specific affective modelling task. For instance, the original model may excel in tasks such as binary sentiment classification, but underperform in more nuanced sentiment analysis tasks such as sevenclass sentiment classification. Likewise, the original model may be good at analysing emotions from structured movie reviews, but struggle with less formal and more diverse forms of expression in social media posts. In these conditions, it would be helpful to conduct further fine-tuning for domain-specific adaptation for optimal performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Table 6",
      "text": "Two examples illustrating ChatGPT's explanations for conversational emotion recognition. The sentences selected for explanation, along with their corresponding decisions and associated explanations, are highlighted for clarity. One primary approach for fine-tuning LLMs is instruction tuning. This approach involves formatting the specific task training data with natural language task descriptions to guide the LLM towards the task objective. Along with the paired input-output training data, these specially designed task descriptions in natural language play a pivotal role in helping the LLM to understand the desired goal. In the realm of affective computing, we may fine-tune an LLM using customised emotion recognition dataset(s) to enhance the model's adaptability. Interestingly, even when using the same labelled instances on the same dataset, different task instructions can lead to varying performance outcomes. This indicates the significant impact that the phrasing and clarity of task descriptions can have on the effectiveness of instruction tuning  [57] . It could be highly beneficial to investigate directions such as identifying the most effective task descriptions for emotional datasets, exploring strategies to optimise fine-tuning using a minimal amount of target domain emotional data, or how to efficiently fine-tune an LLM for affective computing. Finetuning LLMs enables models to learn domainspecific knowledge, using a smaller dataset within the target domain. However, given the massive number of parameters that LLMs possess, full-parameter tuning can be highly resource-intensive, potentially leading to increased computational costs and time. To address this, several parameterefficient fine-tuning (PEFT) techniques have been devised, which only fine-tune a small number of (extra) parameters and meanwhile maintain the strong performance  [58] . These include strategies such as prefix tuning which optimises trainable vectors added to each Transformer block as a prefix, prompt tuning which learns task-specific prompt embeddings to complement the input text embeddings, adapter tuning which incorporates small neural network modules without changing existing parameters  [59] , and low-rank adaptation (LoRA) which decomposes the weight update matrix into lower-dimensional matrices without losing too much important information  [60] . It would be interesting to examine the effectiveness of these various PEFT techniques to tailor existing open-source LLMs for emotion recognition tasks. This will expand our understanding of how to most effectively enhance LLMs to meet specific aims in the field of affective computing.\n\nPrivacy and security Ensuring user privacy and data security are crucial considerations when deploying LLMs for affective computing in real-world applications. The processed text is inherently sensitive, including users' emotional states, personal experiences, and various forms of identifiable information. Thus, it is essential to implement robust safeguards to protect this information and address privacy concerns when leveraging the advantages of LLMs. As highlighted in  [61] , an empirical investigation indicates that larger and more complex models are more prone to leaking private information. During their experiments, four evaluated LLMs inadvertently disclosed demographic information (location, age, and gender) to varying degrees during the sentiment analysis process  [61] . Moreover, they also conducted a critical evaluation of various privacypreserving technologies, such as differential privacy, to quantify the impact of several privatising methods on alleviating information leakage. An interesting research direction could be the exploration of solutions that effectively balance performance and privacy, with the goal of developing privacy-preserving LLM-based affective computing models.\n\nModel compression On one hand, LLMs are powerful and show appealing performance across varied tasks. On the other hand, they also present challenges due to their substantial memory requirements during the inference stage, making deployment costly. In the context of affective computing, computation often takes place locally rather than on cloud-based systems. This approach is primarily adopted to address privacy and security concerns, as it prevents sensitive data from being transmitted elsewhere. However, these local processors are frequently deployed on resource-constrained devices such as mobile phones, where the substantial size and computational requirements of LLMs pose a significant challenge. Therefore, it becomes essential to explore model compression techniques, such as quantisation methods  [62] , to adapt these powerful models for local deployment. These approaches aim to reduce the memory footprint and latency of LLMs and meanwhile maintaining high accuracy, making them more suitable for implementation on local, resource-limited devices. By pur-suing this research direction, we can potentially bring the power of LLMs to real-world applications within the area of affective computing, while respecting user privacy and security.\n\nLarge Multimodal Models: In March 2023, openAI released GPT-4  [63] , extending the text input to multimodal signals. To be more specific, this latest GPT-4 model accepts text and images inputs and produces text outputs, demonstrating its capability in conducting multimodal dialogues with humans. Within the realm of emotion recognition, the strategy of integrating information from multiple modalities is typically superior to relying solely on one single modality. Emotion, as a complex human phenomenon, is expressed through multiple channels, including text, speech, facial expressions, body language, and more. Instead of considering text only, jointly analysing multiple modalities can achieve a more holistic and accurate understanding of the emotional state being expressed. While LLMs provide a straightforward approach for estimating emotions based on text inputs, integrating these models with other modalities holds substantial promise. Future research directions may focus on investigating optimal approaches to fuse the multimodality information effectively without losing crucial emotional context.\n\nFurthermore, other research efforts are being directed towards the development of large vision models  [64] ,  [65] , large speech models  [66] ,  [67] , and multimodal large language models  [68] . Instead of utilising traditional models and gaining knowledge from small-scale emotional datasets, these large models hold immense potential to bring large gains in emotion recognition performance, in both single modality and multimodality. Incorporating these advanced models could potentially transform the future landscape of affective computing.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we provided a deep insight into the capability of Large language models (LLMs) in the domain of emotion recognition. We explored this by evaluating the performance of three leading LLMs across seven datasets, and compared the results against state-of-the-art works. The experimental outcomes have shown that the LLMs have shown superior performance in sentiment analysis and emotion recognition tasks, especially when identifying minority emotion categories. Unlike traditional specialised models, the LLMs benefit from being trained on expansive data volumes. This vast exposure equips them with impressive generalisation capabilities and paves the way for enhanced explainability in their predictions. Importantly, our findings also indicate that LLMs harness context to enhance emotion estimation. Furthermore, we have mapped out potential trajectories for the future of LLM-based emotion recognition systems. Our vision for these systems combines adaptability, multimodality, robustness, privacy preservation, and compatibility with resource-constrained devices.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "Huan Zhao, Bj ¨orn W. Schuller, Fellow,\nIEEE"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "it has increasingly become an active research topic due to\nAbstract—After the inception of emotion recognition or affective computing,"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "models to neural network-based deep models, which can significantly boost\nthe performance of emotion recognition models and"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "consistently achieve the best results on different benchmarks. Therefore,\nin recent years, deep models have always been considered"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "the first option for emotion recognition. However,\nthe debut of\nlarge language models (LLMs), such as ChatGPT, has remarkably"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "astonished the world due to their emerged capabilities of zero/few-shot\nlearning,\nin-context\nlearning, chain-of-thought, and others that"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "are never shown in previous deep models.\nIn the present paper, we comprehensively investigate how the LLMs perform in emotion"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "recognition in terms of diverse aspects,\nincluding in-context\nlearning,\nfew-short\nlearning, accuracy, generalisation, and explanation."
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "Moreover, we offer some insights and pose other potential challenges, hoping to ignite broader discussions about enhancing emotion"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "recognition in the new era of advanced and generalised large models."
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "In-Context Learning, Few-Shot Learning.\nIndex Terms—Emotion Recognition, Large Language Model,"
        },
        {
          "Zixing Zhang, Senior Member,\nIEEE, Liyizhe Peng, Tao Pang, Jing Han, Senior Member,\nIEEE,": "✦"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        },
        {
          "✦": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "affective modelling"
        },
        {
          "2": ""
        },
        {
          "2": "The\nstructure\nof\nthis work\nis\norganised\nas\nfollows:"
        },
        {
          "2": ""
        },
        {
          "2": "Section 2 presents related works; Section 3 introduces the"
        },
        {
          "2": ""
        },
        {
          "2": "selected LLMs, datasets for evaluation, and implementation"
        },
        {
          "2": ""
        },
        {
          "2": "details. Experimental results, along with their analysis and"
        },
        {
          "2": ""
        },
        {
          "2": "comparison, are provided in Section 4. Challenges and op-"
        },
        {
          "2": ""
        },
        {
          "2": "portunities of LLMs in the context of affective modelling are"
        },
        {
          "2": ""
        },
        {
          "2": "discussed in Section 5. The work concludes in Section 6."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "2\nBACKGROUND"
        },
        {
          "2": ""
        },
        {
          "2": "This section provides a brief overview of the relevant back-"
        },
        {
          "2": "ground and concepts."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "2.1\nRecent Advances of Emotion Recognition"
        },
        {
          "2": ""
        },
        {
          "2": "Over\nthe past decades, particularly with the\nemergence"
        },
        {
          "2": "of deep learning,\nsignificant endeavours have been dedi-"
        },
        {
          "2": "cated to developing effective and robust emotion recogni-"
        },
        {
          "2": "tion models.\nIn the following, we briefly introduce related"
        },
        {
          "2": "studies primarily focusing on three key aspects/challenges:"
        },
        {
          "2": "enhancing accuracy, facilitating generalisation, and enabling"
        },
        {
          "2": "explainability of emotion recognition models."
        },
        {
          "2": ""
        },
        {
          "2": "One main research direction is to improve the correctness"
        },
        {
          "2": "of the model, namely increasing the accuracy and precision"
        },
        {
          "2": "of\nemotion recognition models. For\nthis\naim,\nseveral\nre-"
        },
        {
          "2": "search studies have investigated various advanced feature"
        },
        {
          "2": "extraction techniques, aiming to capture salient emotional"
        },
        {
          "2": "representations. The features could be extracted from dif-"
        },
        {
          "2": "ferent modalities,\nsuch as\nspeech, video,\ntext, and physi-"
        },
        {
          "2": "ological\nsignals. For\ninstance,\nthree novel domain-specific"
        },
        {
          "2": "audiovisual\ntasks were designed in [16]\nto\nlearn better"
        },
        {
          "2": "representations, by exploring large volumes of unlabelled"
        },
        {
          "2": "data via\nself-supervised learning. There\nare\nalso a huge"
        },
        {
          "2": "number\nof\nstudies\nthat\nleverage different deep learning"
        },
        {
          "2": "architectures to refine emotion recognition performance. For"
        },
        {
          "2": "instance, a Transformer-based model was proposed in [17]"
        },
        {
          "2": "to process ECG signals\nfor\nemotion recognition, and the"
        },
        {
          "2": "obtained contextualised representations achieved state-of-"
        },
        {
          "2": "the-art performance on the AMIGOS dataset.\nIn addition,"
        },
        {
          "2": "some other works\nfocus on combining information from"
        },
        {
          "2": "multiple modalities,\nso\nthat\na more\ncomprehensive\nand"
        },
        {
          "2": "holistic understanding of users’\nemotional\nstates\ncan be"
        },
        {
          "2": "obtained [9]."
        },
        {
          "2": "Aiming at improving the generalisation of emotion recog-"
        },
        {
          "2": "nition models, several advanced deep learning techniques"
        },
        {
          "2": "have been explored, such as knowledge transfer [18], mul-"
        },
        {
          "2": ""
        },
        {
          "2": "titask learning [19],\nself-supervised learning [20], and do-"
        },
        {
          "2": "main adaptation [21]. Applying these\ntechniques\nappro-"
        },
        {
          "2": "priately could make the learnt emotion recognition model"
        },
        {
          "2": "more effective across different domains, different contexts,"
        },
        {
          "2": "and diverse scenarios. For example, a pre-trained model’s"
        },
        {
          "2": "knowledge gained from a\nlarge\namount\nof data\ncan be"
        },
        {
          "2": "transferred to downstream tasks. Specifically, pre-trained"
        },
        {
          "2": "language models\n(PLMs),\nsuch\nas\nbidirectional\nencoder"
        },
        {
          "2": "representations from transformers (BERT) and their variants"
        },
        {
          "2": "have been utilised in text-based emotion recognition and"
        },
        {
          "2": "achieved promising performance.\nIn [22],\nthe\nefficacy of"
        },
        {
          "2": "BERT and its variants has been comparatively examined on"
        },
        {
          "2": "a cross-culture emotional database and Robustly optimised"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "main, there still\nlacks a thorough investigation of the effec-"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "tiveness of LLMs in affective computing. Specifically,\nthere"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "are still several key questions that need to be answered:"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "•\nHow do LLMs perform in emotion recognition tasks,"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "compared with state-of-the-art task-specific models?"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "•\nCan one LLM be utilised across different affective"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "computing-related tasks without\nfurther training or"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "updating?"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "•\nTo what\nlevel can LLMs explain the reasons behind"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "the detected emotion category from a given emo-"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": "tional input?"
        },
        {
          "While LLMs hold promising opportunities\nin this do-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "2.3\nAt the Crossroad"
        },
        {
          "3": ""
        },
        {
          "3": "Emerging as powerful\ntools\nto understand and generate"
        },
        {
          "3": ""
        },
        {
          "3": "human-readable text, LLMs have demonstrated exceptional"
        },
        {
          "3": ""
        },
        {
          "3": "capabilities in a wide range of NLP tasks. Their impact on"
        },
        {
          "3": ""
        },
        {
          "3": "many domains\nis\nsignificant,\nsuch as question-answering,"
        },
        {
          "3": ""
        },
        {
          "3": "code generation, and creative writing. Standing at the cross-"
        },
        {
          "3": ""
        },
        {
          "3": "roads of\ntraditional affective modelling and large models,"
        },
        {
          "3": ""
        },
        {
          "3": "in this paper, we aim to explore LLMs’\nimpact on affective"
        },
        {
          "3": ""
        },
        {
          "3": "computing, as the effectiveness of LLMs on this task is not"
        },
        {
          "3": ""
        },
        {
          "3": "fully investigated, yet."
        },
        {
          "3": ""
        },
        {
          "3": "Having trained on massive\namounts\nof\ntextual data,"
        },
        {
          "3": ""
        },
        {
          "3": "LLMs offer exciting possibilities for understanding the sub-"
        },
        {
          "3": ""
        },
        {
          "3": "tle variations\nin emotional\ncontent. Besides,\nthough not"
        },
        {
          "3": ""
        },
        {
          "3": "trained explicitly for emotion recognition like previous task-"
        },
        {
          "3": ""
        },
        {
          "3": "specific models,\nthe\ngeneral-purpose LLMs\nhave\nstrong"
        },
        {
          "3": "adaptation capabilities. Especially, prompting-based adap-"
        },
        {
          "3": "tation methods enable human-like interactions with an LLM"
        },
        {
          "3": ""
        },
        {
          "3": "via very simple prompts, entailing temporary learning dur-"
        },
        {
          "3": "ing inference without specialised fine-tuning."
        },
        {
          "3": ""
        },
        {
          "3": "While LLMs hold promising opportunities\nin this do-"
        },
        {
          "3": ""
        },
        {
          "3": "main, there still\nlacks a thorough investigation of the effec-"
        },
        {
          "3": ""
        },
        {
          "3": "tiveness of LLMs in affective computing. Specifically,\nthere"
        },
        {
          "3": ""
        },
        {
          "3": "are still several key questions that need to be answered:"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "•\nHow do LLMs perform in emotion recognition tasks,"
        },
        {
          "3": ""
        },
        {
          "3": "compared with state-of-the-art task-specific models?"
        },
        {
          "3": ""
        },
        {
          "3": "•\nCan one LLM be utilised across different affective"
        },
        {
          "3": ""
        },
        {
          "3": "computing-related tasks without\nfurther training or"
        },
        {
          "3": "updating?"
        },
        {
          "3": "•\nTo what\nlevel can LLMs explain the reasons behind"
        },
        {
          "3": "the detected emotion category from a given emo-"
        },
        {
          "3": "tional input?"
        },
        {
          "3": ""
        },
        {
          "3": "Different\nfrom [30] which focused on evaluating the effec-"
        },
        {
          "3": ""
        },
        {
          "3": "tiveness of ChatGPT on three datasets,\nthis paper explores"
        },
        {
          "3": ""
        },
        {
          "3": "the\nefficiency and effectiveness of\nthree LLMs on seven"
        },
        {
          "3": ""
        },
        {
          "3": "emotional datasets from two languages. More importantly,"
        },
        {
          "3": ""
        },
        {
          "3": "we explore the emergent capability of\nin-context\nlearning,"
        },
        {
          "3": ""
        },
        {
          "3": "showing that with few demonstration examples, LLMs per-"
        },
        {
          "3": ""
        },
        {
          "3": "form better. The generalisation and explainability of LLMs"
        },
        {
          "3": ""
        },
        {
          "3": "for affective computing are also analysed, which is missing"
        },
        {
          "3": ""
        },
        {
          "3": "in previous works."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "WITH\n3\nEMOTION\nRECOGNITION\nLARGE\nLAN-"
        },
        {
          "3": ""
        },
        {
          "3": "GUAGE MODELS"
        },
        {
          "3": ""
        },
        {
          "3": "In this section, we outline our evaluation strategy for LLM-"
        },
        {
          "3": "based emotion recognition. We begin by introducing the"
        },
        {
          "3": "three LLMs\nselected for\nthis\nevaluation. Then, we\ngive"
        },
        {
          "3": "an overview of\nthe seven emotional datasets used in the"
        },
        {
          "3": "evaluation. Lastly, we detail the implementation process."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "3.1\nLarge Language Models"
        },
        {
          "3": ""
        },
        {
          "3": "In this work, we\ninvestigate\nthe\ncapabilities of LLMs\nto"
        },
        {
          "3": "perform emotion recognition. For this aim,\nthree LLMs are"
        },
        {
          "3": "selected, including ChatGPT, Claude, and Bing Chat."
        },
        {
          "3": "ChatGPT: Released by OpenAI in November 2022, Chat-"
        },
        {
          "3": "GPT represents a significant advancement in the NLP field."
        },
        {
          "3": "When given a prompt, ChatGPT is designed to provide"
        },
        {
          "3": "detailed responses. It builds upon the prior GPT-3.5 model,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: As these datasets are publicly accessible, they",
      "data": [
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": ""
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "in our\nstudy. A summary of\ntheir\nstatistics\ncan be found"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": ""
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "in Table 1. As\nthese datasets are publicly accessible,\nthey"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": ""
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "facilitate easy verification of our results."
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": ""
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "•\nSST The\nStanford Sentiment Treebank\nis\nan En-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "glish corpus with fine-grained sentiment\nlabels of"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "11,855 single sentences extracted from movie review"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "data\n[33]. These\nsentences were parsed using the"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "Stanford parser, resulting in a total of 215,154 unique"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "phrases from those parsed trees. Those phrases were"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "then annotated by the classic Mechanical Turk for"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "sentiment\ninto 25 different\nlevels. The obtained an-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "notations for each phrase were then used to define"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "fine-grained and binary versions\nof\nthe\ntask.\nFor"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "the fine-grained task, each sentence is labelled with"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "one of five\nsentiment\nclasses: negative,\nsomewhat"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "negative, neutral,\nsomewhat positive, and positive;"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "for the binary task, each sentence is labelled as either"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "positive or negative (the neutral class is ignored). In"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "this study, we do not consider phrase-level sentiment"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "analysis. Sentences were\nsplit\ninto training (8544),"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "development\n(1101),\nand test\nsplits\n(2210)\nfor\nthe"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "fine-grained classification task. For\nthe binary task,"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "the three sets have 6920, 872, and 1821 sentences,"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "respectively."
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "•\nFriends Friends is a corpus derived from the Friends"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "TV shows,\ncomprising transcriptions of\n1,000 dia-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "logue spoken by the characters in the show across"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "seasons\n1\nto\n9\n[34]. Each dialogue was\nregarded"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "as an annotation task on Amazon Mechanical Turk."
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "The\ngoal was\nto\nlabel\neach\nof\n14,503\nutterances"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "within the 1,000 dialogues into one of seven classes:"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "anger, disgust,\nfear,\njoy, sadness, surprise, and neu-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "tral. The annotators were instructed to consider the"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "context of\nthe\nentire dialogue while assigning the"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "sentiment\nlabels. The dataset was\nfurther divided"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "into three splits,\nresulting in 10,561/720, 1,178/80,"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "and 2,764/200 utterances/dialogues in the training,"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "development, and test sets, separately."
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "•\nMastodon The Mastodon dataset was built by crawl-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "ing social media posts\nfrom Mastodon,\nand non-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "English posts were filtered out automatically [35]."
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "The\ncorpus was\ncompletely manually anonymised"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "and annotated by two students. While the dataset"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "was originally designed for\nsentiment\nrecognition"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "and dialogue act recognition, here, we consider the"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "sentiment analysis task only. Each post was labelled"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "as positive, negative, or neutral. All posts were parti-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "tioned into two splits: 1,075 posts from 239 dialogues"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "in the training set, and another 1,142 posts from 266"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "dialogues in the test set."
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "•\nMOSI The Multimodal Opinion-level Sentiment\nIn-"
        },
        {
          "In this part, we\nintroduce\nthe\nseven datasets\nemployed": "tensity (MOSI) dataset\n[36]\nis a multimodal dataset"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: As these datasets are publicly accessible, they",
      "data": [
        {
          "4": "Chat offers three distinct\nresponse formats: more creative,"
        },
        {
          "4": "more balanced, and more precise. For the purposes of our"
        },
        {
          "4": "study, we opted for the “more precise” format, aiming to ob-"
        },
        {
          "4": "tain succinct and direct answers to our emotion-prediction-"
        },
        {
          "4": "related queries."
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "3.2\nEmotion Datasets"
        },
        {
          "4": ""
        },
        {
          "4": "In this part, we\nintroduce\nthe\nseven datasets\nemployed"
        },
        {
          "4": ""
        },
        {
          "4": "in our\nstudy. A summary of\ntheir\nstatistics\ncan be found"
        },
        {
          "4": ""
        },
        {
          "4": "in Table 1. As\nthese datasets are publicly accessible,\nthey"
        },
        {
          "4": ""
        },
        {
          "4": "facilitate easy verification of our results."
        },
        {
          "4": ""
        },
        {
          "4": "•\nSST The\nStanford Sentiment Treebank\nis\nan En-"
        },
        {
          "4": "glish corpus with fine-grained sentiment\nlabels of"
        },
        {
          "4": "11,855 single sentences extracted from movie review"
        },
        {
          "4": "data\n[33]. These\nsentences were parsed using the"
        },
        {
          "4": "Stanford parser, resulting in a total of 215,154 unique"
        },
        {
          "4": "phrases from those parsed trees. Those phrases were"
        },
        {
          "4": "then annotated by the classic Mechanical Turk for"
        },
        {
          "4": "sentiment\ninto 25 different\nlevels. The obtained an-"
        },
        {
          "4": "notations for each phrase were then used to define"
        },
        {
          "4": "fine-grained and binary versions\nof\nthe\ntask.\nFor"
        },
        {
          "4": "the fine-grained task, each sentence is labelled with"
        },
        {
          "4": "one of five\nsentiment\nclasses: negative,\nsomewhat"
        },
        {
          "4": "negative, neutral,\nsomewhat positive, and positive;"
        },
        {
          "4": "for the binary task, each sentence is labelled as either"
        },
        {
          "4": "positive or negative (the neutral class is ignored). In"
        },
        {
          "4": "this study, we do not consider phrase-level sentiment"
        },
        {
          "4": "analysis. Sentences were\nsplit\ninto training (8544),"
        },
        {
          "4": "development\n(1101),\nand test\nsplits\n(2210)\nfor\nthe"
        },
        {
          "4": "fine-grained classification task. For\nthe binary task,"
        },
        {
          "4": "the three sets have 6920, 872, and 1821 sentences,"
        },
        {
          "4": "respectively."
        },
        {
          "4": "•\nFriends Friends is a corpus derived from the Friends"
        },
        {
          "4": "TV shows,\ncomprising transcriptions of\n1,000 dia-"
        },
        {
          "4": "logue spoken by the characters in the show across"
        },
        {
          "4": "seasons\n1\nto\n9\n[34]. Each dialogue was\nregarded"
        },
        {
          "4": "as an annotation task on Amazon Mechanical Turk."
        },
        {
          "4": "The\ngoal was\nto\nlabel\neach\nof\n14,503\nutterances"
        },
        {
          "4": "within the 1,000 dialogues into one of seven classes:"
        },
        {
          "4": "anger, disgust,\nfear,\njoy, sadness, surprise, and neu-"
        },
        {
          "4": "tral. The annotators were instructed to consider the"
        },
        {
          "4": "context of\nthe\nentire dialogue while assigning the"
        },
        {
          "4": "sentiment\nlabels. The dataset was\nfurther divided"
        },
        {
          "4": "into three splits,\nresulting in 10,561/720, 1,178/80,"
        },
        {
          "4": "and 2,764/200 utterances/dialogues in the training,"
        },
        {
          "4": "development, and test sets, separately."
        },
        {
          "4": "•\nMastodon The Mastodon dataset was built by crawl-"
        },
        {
          "4": "ing social media posts\nfrom Mastodon,\nand non-"
        },
        {
          "4": "English posts were filtered out automatically [35]."
        },
        {
          "4": "The\ncorpus was\ncompletely manually anonymised"
        },
        {
          "4": "and annotated by two students. While the dataset"
        },
        {
          "4": "was originally designed for\nsentiment\nrecognition"
        },
        {
          "4": "and dialogue act recognition, here, we consider the"
        },
        {
          "4": "sentiment analysis task only. Each post was labelled"
        },
        {
          "4": "as positive, negative, or neutral. All posts were parti-"
        },
        {
          "4": "tioned into two splits: 1,075 posts from 239 dialogues"
        },
        {
          "4": "in the training set, and another 1,142 posts from 266"
        },
        {
          "4": "dialogues in the test set."
        },
        {
          "4": "•\nMOSI The Multimodal Opinion-level Sentiment\nIn-"
        },
        {
          "4": "tensity (MOSI) dataset\n[36]\nis a multimodal dataset"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "negative\nsentiments, were\napplied to the MOSI, MOSEI,"
        },
        {
          "5": "and CH-SIMS datasets. The Mastodon dataset and the MOSI"
        },
        {
          "5": "underwent a three-class classification: positive, neutral, and"
        },
        {
          "5": "negative. Lastly,\nfor\nthe Friends\nand M3ED datasets, we"
        },
        {
          "5": "executed a seven-class emotion classification."
        },
        {
          "5": "Usage\nof LLMs: When working with LLMs\nsuch as"
        },
        {
          "5": "ChatGPT, we use a “prompt” – a specific\ninput\ntext – to"
        },
        {
          "5": "elicit a response from the model.\nIn our study on emotion"
        },
        {
          "5": "recognition, we\nthoroughly investigated various prompt-"
        },
        {
          "5": "ing strategies. For\ncontext-free datasets\n(SST, MOSI, MO-"
        },
        {
          "5": "SEI,\nand CH-SIMS), we\nassessed two strategies:\ncontext-"
        },
        {
          "5": "free zero-shot and context-free few-shot prompting. Mean-"
        },
        {
          "5": "while,\nfor context-dependent datasets (Friends, Mastodon,"
        },
        {
          "5": "and M3ED), we explored three strategies: context-free zero-"
        },
        {
          "5": "shot prompting,\ncontext-aware\nzero-shot prompting,\nand"
        },
        {
          "5": "context-aware\nfew-shot\nprompting.\nEssentially,\n‘context-"
        },
        {
          "5": "free’\nrefers\nto predicting the emotion-based solely on the"
        },
        {
          "5": "content of\nthe sentence itself.\nIn contrast,\n‘context-aware’"
        },
        {
          "5": "entails\nthe prediction process\nthat\ntakes\ninto account\nthe"
        },
        {
          "5": "surrounding\ncontext within the\nsame\nconversation.\nFur-"
        },
        {
          "5": "thermore,\n‘zero-shot’\nindicates that no prior knowledge or"
        },
        {
          "5": "examples are provided for the specific task, whereas ‘few-"
        },
        {
          "5": "shot’ implies that a limited number of demonstration exam-"
        },
        {
          "5": "ples are given to facilitate inference-time learning. Detailed"
        },
        {
          "5": "prompts for each strategy are provided as follows:"
        },
        {
          "5": ""
        },
        {
          "5": "•\ncontext-free zero-shot prompting (i. e., w/o context,"
        },
        {
          "5": ""
        },
        {
          "5": "w/o ICL). Classify the sentiment of\nthe sentence to"
        },
        {
          "5": ""
        },
        {
          "5": "Emotion 1, Emotion 2,\n... or Emotion k. No need for"
        },
        {
          "5": ""
        },
        {
          "5": "you to explain. Don’t repeat my sentence. Give me"
        },
        {
          "5": ""
        },
        {
          "5": "the simplest answer with a list and corresponding"
        },
        {
          "5": ""
        },
        {
          "5": "number:\n(provide multiple sentences from a test set"
        },
        {
          "5": ""
        },
        {
          "5": "with serial number)"
        },
        {
          "5": ""
        },
        {
          "5": "•\ncontext-free few-shot prompting (i. e., w/o context,"
        },
        {
          "5": ""
        },
        {
          "5": "w/\nICL). Examples of sentiment classification:\n(pro-"
        },
        {
          "5": ""
        },
        {
          "5": "vide few training data and corresponding labels from"
        },
        {
          "5": ""
        },
        {
          "5": "a train set). According to the above examples, classify"
        },
        {
          "5": ""
        },
        {
          "5": "the sentiment of the sentence as Emotion 1, Emotion"
        },
        {
          "5": ""
        },
        {
          "5": "2,\n... or Emotion k. No need for you to explain. Don’t"
        },
        {
          "5": ""
        },
        {
          "5": "repeat my sentence. Give me\nthe\nsimplest answer"
        },
        {
          "5": ""
        },
        {
          "5": "with a list and corresponding number: (provide mul-"
        },
        {
          "5": ""
        },
        {
          "5": "tiple sentences from a test set with serial number)"
        },
        {
          "5": ""
        },
        {
          "5": "•\ncontext-aware zero-shot prompting (i. e., w/ context,"
        },
        {
          "5": ""
        },
        {
          "5": "w/o ICL). According to the context of a conversation,"
        },
        {
          "5": ""
        },
        {
          "5": "classify the sentiment of every sentence to Emotion"
        },
        {
          "5": ""
        },
        {
          "5": "1, Emotion 2,\n... or Emotion k. No need for you to"
        },
        {
          "5": ""
        },
        {
          "5": "explain. Don’t repeat my sentence. Give me the sim-"
        },
        {
          "5": ""
        },
        {
          "5": "plest answer with a list and corresponding number:"
        },
        {
          "5": ""
        },
        {
          "5": "(provide\nall\nsentences of one dialogue with serial"
        },
        {
          "5": ""
        },
        {
          "5": "number)"
        },
        {
          "5": ""
        },
        {
          "5": "•\ncontext-aware few-shot prompting (i. e., w/ context,"
        },
        {
          "5": ""
        },
        {
          "5": "w/\nICL). Examples of sentiment classification:\n(pro-"
        },
        {
          "5": "vide few training data and corresponding labels from"
        },
        {
          "5": "a train set). According to the above examples, based"
        },
        {
          "5": ""
        },
        {
          "5": "on the context of conversation, classify the sentiment"
        },
        {
          "5": ""
        },
        {
          "5": "of\nevery sentence\nto Emotion 1, Emotion 2,\n... or"
        },
        {
          "5": ""
        },
        {
          "5": "Emotion k. No need for you to explain. Don’t repeat"
        },
        {
          "5": ""
        },
        {
          "5": "my sentence. Give me the simplest answer with a list"
        },
        {
          "5": ""
        },
        {
          "5": "and corresponding number: (provide all sentences of"
        },
        {
          "5": ""
        },
        {
          "5": "one dialogue with serial number)"
        },
        {
          "5": ""
        },
        {
          "5": "Note\nthat, K is\nthe number of\nsentiment/emotion cate-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "#sp."
        },
        {
          "TABLE 1": "-"
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "-"
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "-"
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "89"
        },
        {
          "TABLE 1": "1 000"
        },
        {
          "TABLE 1": "474"
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": ""
        },
        {
          "TABLE 1": "626"
        },
        {
          "TABLE 1": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "disgust, anger, fear, neutral)"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "gories, determined by each specific task.",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "model might return “Embarrassed” or “Grateful”, which are"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "not within our specified emotion categories. To counteract"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "For example,\nthe prompt\nis “Classify the sentiment of",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "this, we adjusted the model’s temperature setting it\nto its"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "sentence\nto Positive or Negative”\nfor MOSEI\nas k = 2.",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "maximum value\nof\n2\nand re-prompted the model until"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "The instruction “No need for you to explain. Don’t repeat",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "we obtained a result within the\nexpected range.\nIf, after"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "my sentence. Give me the simplest answer with a list and",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "five attempts, we still did not get a response within our"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "corresponding number.” is aimed at a more\nconcise and",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "classification range, we deemed the sentiment classification"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "clearer\nresponse, making it\neasier\nto retrieve\nthe desired",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "for that sentence as incorrect and randomly assigned it to an"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "sentiment/emotion prediction. Furthermore,\nfor\nthose bi-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "incorrect classification category."
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "nary datasets (positive and negative), we add “Don’t answer",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "neutral.” into the prompt, which can significantly reduce",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "Model Explainability of LLMs: Providing clear and un-"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "the number of neutral\nresponses. However,\nthere are still",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "derstandable explanations for the decisions made by com-"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "unexpected neutral\nresponses\nin the binary classification",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "plex models is an active area of research. As LLMs generate"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "(positive vs negative)\nin such a manner.\nIn addition,\nfor",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "natural\nlanguage responses, one potential approach could"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "any sentence, whether from the training set (as task demon-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "be to design prompts that request\nthe models to elucidate"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "strations) or from the test set, we add two asterisks (**) as",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "the reasoning behind their decisions in human-readable for-"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "separators before and after\nthe sentence. These separators",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "mat. To showcase the potential of\nthis approach, we opted"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "can make it easier for the model\nto distinguish the begin-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "to conduct an exploratory experiment, using ChatGPT and"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "ning and end of sentences, avoiding the model considering",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "selecting a few conversations from the Friends dataset. For"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "multiple sentences as one paragraph and only providing",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "each conversation, one of\nthe sentences was chosen to be"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "one\nlabel\ninstead. Also, when performing zero/few-shot",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "asked for\nthe explanation behind the model’s prediction."
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "prompting on context-free datasets,\nincorporating multiple",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "To\nsolicit\nthis\nadditional\ninformation, we\nappended the"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "sentences\ninto a single query notably decreased the eval-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "following request to the original prompt this further request"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "uation’s\ntime and cost\nrequirements. Specifically, 100 test",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "“In addition, please\nexplain how you judge\nthe\nemotion"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "sentences were fed in one prompt for zero-shot prompting,",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "of\nthe <order of\nthe\nselected sentence> sentence\nin the"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "while 50 sentences in one prompt for few-shot prompting. In",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "conversation based on the context”. The primary aim of this"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "contrast, when performing zero-shot prompting on context-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "experiment was to showcase whether ChatGPT is capable"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "dependent datasets, we reduced the number of\nsentences",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "of providing sound explanations for its emotion recognition"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "to one. This was\nto prevent\nthe model\nfrom gaining any",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "decisions. By doing so, we hoped to gain a deeper insight"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "contextual\ninformation. Last,\nthe number of\ntask demon-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "into the model’s decision-making process\nand to further"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "strations added to the prompt under\nfew-shot prompting",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "explore the possibilities of\nintegrating XAI principles with"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "settings\ncannot exceed 100 due to the limited number of",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "LLM capabilities."
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "tokens (4096) set in ChatGPT. We selected an equal number",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "of sentences per sentiment/emotion category: 50 sentences",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "SOTA specialised models: For performance comparison"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "per\ncategory for binary classification, 30 per\ncategory for",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "with specialised affective computing models, we selected"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "three-class classification, 20 per category for five-class clas-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "recently published SOTA works with competitive perfor-"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "sification, and 10 per category for seven-class classification.",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "mance on each selected dataset respectively."
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "To ensure fairness in comparison between the three LLMs,",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "for ICL strategies, we applied the same training data points",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "Evaluation Metrics: The primary metrics we employed"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "per dataset as task demonstrations when testing each LLM.",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": ""
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "for performance evaluation were accuracy and the macro F1"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "After collecting the LLM responses, we post-processed",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "score. For the M3ED dataset, we used the weighted average"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "them to address any inconsistencies. Notably, due to oc-",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "F1 score to ensure a fair comparison with other studies util-"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "casional model\ninstability, we sometimes\nreceived results",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "ising the same dataset. Similarly, for the Mastodon dataset,"
        },
        {
          "M3ED\nMandarin\na, v, t\nyes\nTV series\n626": "outside our predefined classification range. For instance, the",
          "990\n24 449 (4 201)\n7.4\n7 (happy, surprise, sad,": "we calculated both macro precision and macro recall."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: and Table 3 display results on context-available",
      "data": [
        {
          "7": "Further, we observe that incorporating context informa-"
        },
        {
          "7": "tion (both context within the dialogue and context knowl-"
        },
        {
          "7": ""
        },
        {
          "7": "edge from the demonstrations) could enhance the recogni-"
        },
        {
          "7": ""
        },
        {
          "7": "tion performance for emotional sentences within dialogues."
        },
        {
          "7": ""
        },
        {
          "7": "Table\n2\nand Table\n3 display results on context-available"
        },
        {
          "7": ""
        },
        {
          "7": "datasets.\nIn all\n10 global metrics\n(4\nfor Mastodon,\n3\nfor"
        },
        {
          "7": "Friends, and 3 for M3ED), 7 of\nthem are improved when"
        },
        {
          "7": "ChatGPT is deployed, 8 metrics are increased by Claude,"
        },
        {
          "7": ""
        },
        {
          "7": "and all 10 metrics are increased when leveraging Bing Chat."
        },
        {
          "7": ""
        },
        {
          "7": "Not\nonly do LLMs\nachieve\ncomparable\noverall per-"
        },
        {
          "7": ""
        },
        {
          "7": "formance,\nbut\nthey\nalso\ndisplay\nbetter\nrecognition\nper-"
        },
        {
          "7": ""
        },
        {
          "7": "formance\nin minority emotion classes. The\nratio of\neach"
        },
        {
          "7": ""
        },
        {
          "7": "emotion class within two emotion datasets is listed in Ta-"
        },
        {
          "7": ""
        },
        {
          "7": "ble 3. Conventional models tend to struggle with emotions"
        },
        {
          "7": ""
        },
        {
          "7": "constituting\nless\nthan 10% of\nthe dataset\nsamples\ncom-"
        },
        {
          "7": ""
        },
        {
          "7": "pared to more prevalent emotion classes. For example,\nthe"
        },
        {
          "7": ""
        },
        {
          "7": "‘fear’ emotion class (which makes up 1.44% of\nthe Friends"
        },
        {
          "7": ""
        },
        {
          "7": "dataset) achieved a peak accuracy of 31.03% in state-of-the-"
        },
        {
          "7": ""
        },
        {
          "7": "art\n(SOTA) methodologies.\nIn contrast, LLMs across vari-"
        },
        {
          "7": ""
        },
        {
          "7": "ous prompting strategies outperformed this, with Claude"
        },
        {
          "7": ""
        },
        {
          "7": "achieving the best result at 81.25%. Similarly, Claude’s ac-"
        },
        {
          "7": ""
        },
        {
          "7": "curacy of\nrecognising the\n‘angry’\nemotion in Friends,\nat"
        },
        {
          "7": ""
        },
        {
          "7": "80.75%, was markedly superior\nto SOTA results. Compa-"
        },
        {
          "7": ""
        },
        {
          "7": "rable trends are observed with other minority emotions on"
        },
        {
          "7": ""
        },
        {
          "7": "the M3ED dataset. This can be attributed to the vast and"
        },
        {
          "7": ""
        },
        {
          "7": "diverse text LLMs have been trained on, which encompasses"
        },
        {
          "7": ""
        },
        {
          "7": "a broader emotional range than a specific dataset."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "4.3\nGeneralisation"
        },
        {
          "7": ""
        },
        {
          "7": "Our experimental results show that, without explicitly train-"
        },
        {
          "7": ""
        },
        {
          "7": "ing on a specific dataset, LLMs exhibit an impressive gen-"
        },
        {
          "7": ""
        },
        {
          "7": "eralisation ability in multiple affective modelling tasks.\nIn-"
        },
        {
          "7": ""
        },
        {
          "7": "stead of\nlearning specialised models for each of\nthe seven"
        },
        {
          "7": ""
        },
        {
          "7": "distinct\nemotional datasets\nseparately,\nall\nthree\nselected"
        },
        {
          "7": ""
        },
        {
          "7": "models have shown their capabilities\nin managing varied"
        },
        {
          "7": ""
        },
        {
          "7": "emotion recognition tasks, underlining their versatility and"
        },
        {
          "7": ""
        },
        {
          "7": "broad applicability."
        },
        {
          "7": "In\nparticular,\nas\ndiscussed\nearlier,\nthe\nLLMs\ncould"
        },
        {
          "7": ""
        },
        {
          "7": "achieve\ncomparable performance\nin zero-shot prompting"
        },
        {
          "7": "scenarios. And this performance\nis\nfurther\nenhanced by"
        },
        {
          "7": ""
        },
        {
          "7": "the implementation of\nfew-shot prompting,\nindicating that"
        },
        {
          "7": ""
        },
        {
          "7": "LLMs can adapt and learn from limited contextual informa-"
        },
        {
          "7": ""
        },
        {
          "7": "tion during the inference."
        },
        {
          "7": ""
        },
        {
          "7": "Most\nimportantly,\nthe LLMs show a strong generalisa-"
        },
        {
          "7": "tion ability in their emotion analysis across different corpora"
        },
        {
          "7": "and domains. The datasets utilised in this study encompass"
        },
        {
          "7": "a wide\narray\nof\ntext\ntypes,\nincluding TV show scripts,"
        },
        {
          "7": "movie reviews, social media posts, and YouTube video tran-"
        },
        {
          "7": "scriptions. Despite the variances in style,\ntone, and context"
        },
        {
          "7": "inherent\nto these data sources, LLMs consistently produce"
        },
        {
          "7": "reliable emotion recognition results."
        },
        {
          "7": "Additionally,\nthe\nresults\nattest\nto\nthe\nLLMs’\ncross-"
        },
        {
          "7": "language generalisation capability. The corpora used in this"
        },
        {
          "7": "study spanned both English and Chinese languages, further"
        },
        {
          "7": "emphasising the models’ adaptability. Such cross-language"
        },
        {
          "7": "generalisation ability holds considerable potential\nfor uni-"
        },
        {
          "7": "versal emotion recognition models that overcome language"
        },
        {
          "7": "barriers."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 6: , ChatGPT provides reasonable",
      "data": [
        {
          "TABLE 2": ""
        },
        {
          "TABLE 2": "macro-precision (Precision), and macro-recall (Recall)."
        },
        {
          "TABLE 2": "Acc"
        },
        {
          "TABLE 2": "-"
        },
        {
          "TABLE 2": "65.50"
        },
        {
          "TABLE 2": "65.41"
        },
        {
          "TABLE 2": "65.94"
        },
        {
          "TABLE 2": "63.05"
        },
        {
          "TABLE 2": "64.02"
        },
        {
          "TABLE 2": "64.27"
        },
        {
          "TABLE 2": "60.51"
        },
        {
          "TABLE 2": "60.95"
        },
        {
          "TABLE 2": "63.66"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 6: , ChatGPT provides reasonable",
      "data": [
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": ""
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "in the test set"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "Acc"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": ""
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "77.40"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "72.10"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "72.82"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "81.30"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "72.29"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "63.65"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "63.38"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "56.63"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "51.51"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "58.43"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "40.31"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "55.62"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "56.91"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "Acc"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": ""
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "-"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "-"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "-"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "44.47"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "45.39"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "46.32"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "34.90"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "53.73"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "53.80"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "36.44"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "43.42"
        },
        {
          "Performance comparison on Friends (first half) and M3ED (second half) in terms of accuracy (Acc), F1, and unweighted accuracy (UA). The best": "47.73"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MOSI-2": "F1",
          "MOSI-3": "F1",
          "MOSEI": "F1"
        },
        {
          "MOSI-2": "-",
          "MOSI-3": "-",
          "MOSEI": "-"
        },
        {
          "MOSI-2": "-",
          "MOSI-3": "-",
          "MOSEI": "-"
        },
        {
          "MOSI-2": "-",
          "MOSI-3": "-",
          "MOSEI": "-"
        },
        {
          "MOSI-2": "-",
          "MOSI-3": "-",
          "MOSEI": "-"
        },
        {
          "MOSI-2": "-",
          "MOSI-3": "-",
          "MOSEI": "-"
        },
        {
          "MOSI-2": "-",
          "MOSI-3": "-",
          "MOSEI": "-"
        },
        {
          "MOSI-2": "85.92",
          "MOSI-3": "62.21",
          "MOSEI": "84.43"
        },
        {
          "MOSI-2": "88.93",
          "MOSI-3": "63.32",
          "MOSEI": "79.80"
        },
        {
          "MOSI-2": "86.55",
          "MOSI-3": "63.67",
          "MOSEI": "84.81"
        },
        {
          "MOSI-2": "88.37",
          "MOSI-3": "63.92",
          "MOSEI": "81.33"
        },
        {
          "MOSI-2": "70.72",
          "MOSI-3": "55.76",
          "MOSEI": "68.36"
        },
        {
          "MOSI-2": "88.12",
          "MOSI-3": "55.97",
          "MOSEI": "70.28"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model [%]": "GNN (2020) [49]",
          "Acc": "-",
          "F1": "-"
        },
        {
          "Model [%]": "BT-TAPT (2021) [50]",
          "Acc": "-",
          "F1": "-"
        },
        {
          "Model [%]": "SEMGraph-P (2022) [51]",
          "Acc": "-",
          "F1": "-"
        },
        {
          "Model [%]": "ELECTRA (2020) [52]",
          "Acc": "-",
          "F1": "-"
        },
        {
          "Model [%]": "SentiLARE (2020) [53]",
          "Acc": "-",
          "F1": "-"
        },
        {
          "Model [%]": "SentiWSP (2022) [54]",
          "Acc": "-",
          "F1": "-"
        },
        {
          "Model [%]": "MLF-DNN (2020) [37]",
          "Acc": "80.26",
          "F1": "-"
        },
        {
          "Model [%]": "ChatGPT (w/o ICL)",
          "Acc": "79.66",
          "F1": "78.78"
        },
        {
          "Model [%]": "ChatGPT (w/ ICL)",
          "Acc": "87.28",
          "F1": "86.43"
        },
        {
          "Model [%]": "Claude (w/o ICL)",
          "Acc": "88.70",
          "F1": "87.44"
        },
        {
          "Model [%]": "Claude (w/ ICL)",
          "Acc": "77.40",
          "F1": "76.86"
        },
        {
          "Model [%]": "Bing Chat (w/o ICL)",
          "Acc": "72.03",
          "F1": "70.99"
        },
        {
          "Model [%]": "Bing Chat (w/ ICL)",
          "Acc": "82.77",
          "F1": "82.09"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bing Chat (w/o ICL)\n81.56": "Bing Chat (w/ ICL)\n85.64",
          "81.53\n48.64\n48.63\n72.03\n70.99": "85.64\n50.32\n50.25\n82.77\n82.09"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "natural errors, encompassing typographical mistakes and",
          "81.53\n48.64\n48.63\n72.03\n70.99": "tional unsupervised pre-training on a large corpus of unla-"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "grammatical inaccuracies. These errors mirror the imperfect,",
          "81.53\n48.64\n48.63\n72.03\n70.99": "belled domain-specific data, with the aim of adapting the"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "real-world language usage seen in everyday communica-",
          "81.53\n48.64\n48.63\n72.03\n70.99": "LLM to a particular domain. Conversely,\nin TAPT, an LLM"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "tions. Hence, it becomes pivotal to investigate the resilience",
          "81.53\n48.64\n48.63\n72.03\n70.99": "is\nfurther pre-trained on the unlabelled training set\nfor a"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "of LLMs to such inconsistencies.\nIt’s equally important\nto",
          "81.53\n48.64\n48.63\n72.03\n70.99": "specific task. Compared to DAPT, TAPT uses a significantly"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "explore strategies to enhance model robustness against this",
          "81.53\n48.64\n48.63\n72.03\n70.99": "smaller, but\nfar more task-relevant pretraining corpus. Re-"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "“noise”, especially when discerning emotions from textual",
          "81.53\n48.64\n48.63\n72.03\n70.99": "search in [55]\nsuggests\nthat\ntailoring a pre-trained model"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "content. How well\nthese models handle\nand respond to",
          "81.53\n48.64\n48.63\n72.03\n70.99": "to the domain of\na\ntarget\ntask via multiphase\nadaptive"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "these errors not only tests\ntheir\nrobustness but also their",
          "81.53\n48.64\n48.63\n72.03\n70.99": "pretraining can significantly enhance task performance.\nIt"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "capability to simulate human-like comprehension and inter-",
          "81.53\n48.64\n48.63\n72.03\n70.99": "may be valuable to investigate whether further pretraining"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "action. An LLM’s capacity to correctly interpret sentiment",
          "81.53\n48.64\n48.63\n72.03\n70.99": "an LLM towards\nthe domain of\nthe\nemotion recognition"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "or emotional intent in the face of such errors may serve as a",
          "81.53\n48.64\n48.63\n72.03\n70.99": "corpus of\ninterest can yield benefits. However,\nit\nis worth"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "strong indicator of its utility and effectiveness in real-world",
          "81.53\n48.64\n48.63\n72.03\n70.99": "paying attention to issues such as the time and cost of pre-"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "scenarios.",
          "81.53\n48.64\n48.63\n72.03\n70.99": "training,\nas well\nas\nthe potential\nfor\nincurring inductive"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "Adaptation/fine-tuning In this present\nstudy, we have",
          "81.53\n48.64\n48.63\n72.03\n70.99": "biases."
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "explored\nthe\ncapabilities\nof\noff-the-shelf\nLLMs without",
          "81.53\n48.64\n48.63\n72.03\n70.99": "In addition, fine-tuning an LLM in a supervised learning"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "any additional adaptation/fine-tuning. Although LLMs are",
          "81.53\n48.64\n48.63\n72.03\n70.99": "way can provide benefits for a specific affective modelling"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "trained on vast\namounts of\ntext data\nacross various do-",
          "81.53\n48.64\n48.63\n72.03\n70.99": "task. For\ninstance,\nthe original model may excel\nin tasks"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "mains, their abilities can be further tailored to specific goals",
          "81.53\n48.64\n48.63\n72.03\n70.99": "such as binary sentiment classification, but underperform"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "through continual pre-training or fine-tuning by leveraging",
          "81.53\n48.64\n48.63\n72.03\n70.99": "in more nuanced sentiment analysis\ntasks\nsuch as\nseven-"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "domain-specific data.",
          "81.53\n48.64\n48.63\n72.03\n70.99": "class sentiment classification. Likewise,\nthe original model"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "In the context of affective computing, to further enhance",
          "81.53\n48.64\n48.63\n72.03\n70.99": "may be good at analysing emotions from structured movie"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "the\naffective modelling ability of LLMs,\nit would be\nof",
          "81.53\n48.64\n48.63\n72.03\n70.99": "reviews, but\nstruggle with less\nformal and more diverse"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "interest\nto deploy further pre-training approaches such as",
          "81.53\n48.64\n48.63\n72.03\n70.99": "forms of expression in social media posts.\nIn these condi-"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "domain-adaptive pretraining (DAPT) [55] and task-adaptive",
          "81.53\n48.64\n48.63\n72.03\n70.99": "tions,\nit would be helpful\nto conduct\nfurther fine-tuning"
        },
        {
          "Bing Chat (w/o ICL)\n81.56": "pretraining (TAPT) [56]. In DAPT, an LLM undergoes addi-",
          "81.53\n48.64\n48.63\n72.03\n70.99": "for domain-specific\nadaptation for\noptimal performance."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": "Example 1"
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": "Prompt:"
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": "ChatGPT response:"
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": "Example 2"
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": "Prompt:"
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": "ChatGPT response:"
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        },
        {
          "Two examples illustrating ChatGPT’s explanations for conversational emotion recognition. The sentences selected for explanation, along with their": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "and negatively, indicating Rachel’s annoyance and aligning with the emotion of ”Anger.”"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "task instructions can lead to varying performance outcomes."
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "This indicates the significant\nimpact\nthat\nthe phrasing and"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "clarity of\ntask descriptions\ncan have on the\neffectiveness"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "of\ninstruction tuning [57].\nIt could be highly beneficial\nto"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "investigate directions such as identifying the most effective"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "task descriptions for emotional datasets, exploring strategies"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "to optimise fine-tuning using a minimal amount of\ntarget"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "domain emotional data, or how to efficiently fine-tune an"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "LLM for affective computing."
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "Finetuning\nLLMs\nenables models\nto\nlearn\ndomain-"
        },
        {
          "has hair, Rachel’s reaction suggests a negative response. The phrase ”this is just perfect” is used sarcastically": "specific knowledge, using a smaller dataset within the target"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "11": "suing this research direction, we can potentially bring the"
        },
        {
          "11": "power of LLMs to real-world applications within the area"
        },
        {
          "11": "of affective computing, while respecting user privacy and"
        },
        {
          "11": "security."
        },
        {
          "11": "Large Multimodal Models:\nIn March 2023, openAI re-"
        },
        {
          "11": "leased GPT-4 [63], extending the text\ninput\nto multimodal"
        },
        {
          "11": "signals. To be more specific, this latest GPT-4 model accepts"
        },
        {
          "11": "text and images inputs and produces text outputs, demon-"
        },
        {
          "11": "strating its capability in conducting multimodal dialogues"
        },
        {
          "11": "with humans. Within the realm of emotion recognition, the"
        },
        {
          "11": "strategy of integrating information from multiple modalities"
        },
        {
          "11": "is typically superior to relying solely on one single modality."
        },
        {
          "11": "Emotion, as a complex human phenomenon,\nis expressed"
        },
        {
          "11": "through multiple\nchannels,\nincluding text,\nspeech,\nfacial"
        },
        {
          "11": "expressions, body language, and more.\nInstead of consid-"
        },
        {
          "11": "ering text only,\njointly analysing multiple modalities\ncan"
        },
        {
          "11": "achieve a more holistic and accurate understanding of\nthe"
        },
        {
          "11": "emotional\nstate being expressed. While LLMs provide\na"
        },
        {
          "11": "straightforward approach for estimating emotions based on"
        },
        {
          "11": "text\ninputs,\nintegrating these models with other modali-"
        },
        {
          "11": "ties holds\nsubstantial promise. Future\nresearch directions"
        },
        {
          "11": "may focus on investigating optimal approaches to fuse the"
        },
        {
          "11": "multimodality information effectively without losing crucial"
        },
        {
          "11": "emotional context."
        },
        {
          "11": "Furthermore, other\nresearch efforts are being directed"
        },
        {
          "11": "towards the development of\nlarge vision models [64],\n[65],"
        },
        {
          "11": "large speech models\n[66],\n[67], and multimodal\nlarge lan-"
        },
        {
          "11": "guage models\n[68].\nInstead of utilising traditional mod-"
        },
        {
          "11": "els\nand\ngaining\nknowledge\nfrom small-scale\nemotional"
        },
        {
          "11": "datasets,\nthese\nlarge models\nhold immense potential\nto"
        },
        {
          "11": "bring large gains\nin emotion recognition performance,\nin"
        },
        {
          "11": "both single modality and multimodality. Incorporating these"
        },
        {
          "11": "advanced models\ncould potentially transform the\nfuture"
        },
        {
          "11": "landscape of affective computing."
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": "6\nCONCLUSION"
        },
        {
          "11": ""
        },
        {
          "11": "In this study, we provided a deep insight into the capability"
        },
        {
          "11": ""
        },
        {
          "11": "of Large language models (LLMs) in the domain of emotion"
        },
        {
          "11": ""
        },
        {
          "11": "recognition. We explored this by evaluating the performance"
        },
        {
          "11": ""
        },
        {
          "11": "of three leading LLMs across seven datasets, and compared"
        },
        {
          "11": ""
        },
        {
          "11": "the results against state-of-the-art works. The experimental"
        },
        {
          "11": ""
        },
        {
          "11": "outcomes have shown that\nthe LLMs have shown superior"
        },
        {
          "11": ""
        },
        {
          "11": "performance\nin sentiment\nanalysis\nand emotion recogni-"
        },
        {
          "11": ""
        },
        {
          "11": "tion tasks,\nespecially when identifying minority emotion"
        },
        {
          "11": ""
        },
        {
          "11": "categories. Unlike traditional specialised models, the LLMs"
        },
        {
          "11": ""
        },
        {
          "11": "benefit from being trained on expansive data volumes. This"
        },
        {
          "11": ""
        },
        {
          "11": "vast exposure equips them with impressive generalisation"
        },
        {
          "11": ""
        },
        {
          "11": "capabilities and paves the way for enhanced explainability"
        },
        {
          "11": ""
        },
        {
          "11": "in their predictions.\nImportantly, our findings also indicate"
        },
        {
          "11": ""
        },
        {
          "11": "that LLMs harness context\nto enhance emotion estimation."
        },
        {
          "11": ""
        },
        {
          "11": "Furthermore, we have mapped out potential\ntrajectories"
        },
        {
          "11": ""
        },
        {
          "11": "for the future of LLM-based emotion recognition systems."
        },
        {
          "11": ""
        },
        {
          "11": "Our vision for these systems combines adaptability, multi-"
        },
        {
          "11": ""
        },
        {
          "11": "modality, robustness, privacy preservation, and compatibil-"
        },
        {
          "11": ""
        },
        {
          "11": "ity with resource-constrained devices."
        },
        {
          "11": ""
        },
        {
          "11": ""
        },
        {
          "11": "ACKNOWLEDGMENT"
        },
        {
          "11": ""
        },
        {
          "11": "The work leading to this research was partially supported"
        },
        {
          "11": "by the National Science Foundation of China under Grant"
        },
        {
          "11": "Number 62076092."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": "[19]\nS. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. W. Schuller, “Multi-"
        },
        {
          "12": "task learning from augmented auxiliary data for improving speech"
        },
        {
          "12": ""
        },
        {
          "12": "IEEE Transactions\nemotion recognition,”\non Affective Computing,"
        },
        {
          "12": ""
        },
        {
          "12": "Nov. 2022, 13 pages, in press."
        },
        {
          "12": ""
        },
        {
          "12": "[20] P. Sarkar and A. Etemad, “Self-supervised ECG representation"
        },
        {
          "12": ""
        },
        {
          "12": "learning for emotion recognition,” IEEE Transactions on Affective"
        },
        {
          "12": ""
        },
        {
          "12": "Computing, vol. 13, no. 3, pp. 1541–1554, July 2022."
        },
        {
          "12": ""
        },
        {
          "12": "[21]\nJ. Deng, Z. Zhang, F. Eyben, and B. Schuller, “Autoencoder-based"
        },
        {
          "12": ""
        },
        {
          "12": "unsupervised domain adaptation for speech emotion recognition,”"
        },
        {
          "12": ""
        },
        {
          "12": "IEEE Signal Processing Letters, vol. 21, no. 9, pp. 1068–1072, Sep."
        },
        {
          "12": ""
        },
        {
          "12": "2014."
        },
        {
          "12": ""
        },
        {
          "12": "[22] A. F. Adoma, N.-M. Henry, and W. Chen, “Comparative analyses"
        },
        {
          "12": ""
        },
        {
          "12": "of BERT, RoBERTa, DistilBERT, and XLNet for text-based emotion"
        },
        {
          "12": ""
        },
        {
          "12": "17th\nInternational Computer Conference\non\nrecognition,”\nin Proc."
        },
        {
          "12": ""
        },
        {
          "12": "Wavelet Active Media Technology\nand\nInformation Processing\n(IC-"
        },
        {
          "12": ""
        },
        {
          "12": "CWAMTIP), Chengdu, China, 2020, pp. 117–121."
        },
        {
          "12": ""
        },
        {
          "12": "[23] M. Deramgozin,\nS.\nJovanovic, H. Rabah,\nand N. Ramzan,\n“A"
        },
        {
          "12": ""
        },
        {
          "12": "hybrid explainable AI framework applied to global and local facial"
        },
        {
          "12": ""
        },
        {
          "12": "IEEE International Conference on\nexpression recognition,” in Proc."
        },
        {
          "12": ""
        },
        {
          "12": "Imaging Systems and Techniques (IST), Kaohsiung, Taiwan, 2021, pp."
        },
        {
          "12": ""
        },
        {
          "12": "1–5."
        },
        {
          "12": ""
        },
        {
          "12": "[24] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “AU-assisted"
        },
        {
          "12": ""
        },
        {
          "12": "graph\nattention\nconvolutional\nnetwork\nfor micro-expression"
        },
        {
          "12": ""
        },
        {
          "12": "the\n28th ACM International Conference\non\nrecognition,”\nin Proc."
        },
        {
          "12": ""
        },
        {
          "12": "Multimedia, Seattle, WA, 2020, pp. 2871–2880."
        },
        {
          "12": ""
        },
        {
          "12": "[25] X. Wang, M. Li, Y. Chang, X. Luo, Y. Yao, and Z. Li, “Multimodal"
        },
        {
          "12": ""
        },
        {
          "12": "cross-attention bayesian network for social news emotion recog-"
        },
        {
          "12": ""
        },
        {
          "12": "Joint Conference on Neural Networks\nnition,” in 2023 International"
        },
        {
          "12": ""
        },
        {
          "12": "(IJCNN).\nIEEE, 2023, pp. 1–9."
        },
        {
          "12": ""
        },
        {
          "12": "[26] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von"
        },
        {
          "12": ""
        },
        {
          "12": "Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill et al., “On"
        },
        {
          "12": ""
        },
        {
          "12": "the opportunities and risks of foundation models,” arXiv preprint"
        },
        {
          "12": ""
        },
        {
          "12": "arXiv:2108.07258, 2021."
        },
        {
          "12": ""
        },
        {
          "12": "[27] W. X. Zhao, K. Zhou,\nJ. Li, T. Tang, X. Wang, Y. Hou, Y. Min,"
        },
        {
          "12": ""
        },
        {
          "12": "B. Zhang,\nJ. Zhang, Z. Dong et al., “A survey of\nlarge language"
        },
        {
          "12": ""
        },
        {
          "12": "models,” arXiv preprint arXiv:2303.18223, 2023."
        },
        {
          "12": ""
        },
        {
          "12": "[28]\nJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yo-"
        },
        {
          "12": ""
        },
        {
          "12": "gatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto,"
        },
        {
          "12": ""
        },
        {
          "12": "O. Vinyals, P. Liang, J. Dean, and W. Fedus, “Emergent abilities of"
        },
        {
          "12": ""
        },
        {
          "12": "large language models,” Transactions on Machine Learning Research,"
        },
        {
          "12": ""
        },
        {
          "12": "pp. 1–30, Mar. 2022."
        },
        {
          "12": ""
        },
        {
          "12": "[29] T. B. Brown, B. Mann, N. Ryder, M. Subbiah,\nJ. Kaplan, P. Dhari-"
        },
        {
          "12": ""
        },
        {
          "12": "wal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Lan-"
        },
        {
          "12": ""
        },
        {
          "12": "guage models are few-shot learners,” in Proc. the 34th International"
        },
        {
          "12": ""
        },
        {
          "12": "Conference on Neural Information Processing Systems (NeurIPS), Van-"
        },
        {
          "12": ""
        },
        {
          "12": "couver, Canada, 2020, pp. 1877–1901."
        },
        {
          "12": ""
        },
        {
          "12": "[30] M. M. Amin, E. Cambria,\nand B. W.\nSchuller,\n“Will\naffective"
        },
        {
          "12": ""
        },
        {
          "12": "computing emerge from foundation models and general artificial"
        },
        {
          "12": ""
        },
        {
          "12": "intelligence? A first evaluation of ChatGPT,” IEEE Intelligent Sys-"
        },
        {
          "12": ""
        },
        {
          "12": "tems, vol. 38, no. 2, pp. 15–23, Mar. 2023."
        },
        {
          "12": ""
        },
        {
          "12": "[31] Y. Bai, A.\nJones, K. Ndousse, A. Askell, A. Chen, N. DasSarma,"
        },
        {
          "12": ""
        },
        {
          "12": "D. Drain, S. Fort, D. Ganguli, T. Henighan et al., “Training a helpful"
        },
        {
          "12": ""
        },
        {
          "12": "and harmless assistant with reinforcement\nlearning from human"
        },
        {
          "12": ""
        },
        {
          "12": "feedback,” arXiv preprint arXiv:2204.05862, 2022."
        },
        {
          "12": ""
        },
        {
          "12": "[32] Y. Bai, S. Kadavath, S. Kundu, A. Askell,\nJ. Kernion, A.\nJones,"
        },
        {
          "12": "A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon et\nal., “Con-"
        },
        {
          "12": "arXiv\npreprint\nstitutional AI: Harmlessness\nfrom AI\nfeedback,”"
        },
        {
          "12": ""
        },
        {
          "12": "arXiv:2212.08073, 2022."
        },
        {
          "12": "[33] R. Socher, A. Perelygin,\nJ. Wu,\nJ. Chuang, C. D. Manning, A. Y."
        },
        {
          "12": "Ng,\nand C. Potts,\n“Recursive deep models\nfor\nsemantic\ncom-"
        },
        {
          "12": "on\npositionality over a sentiment\ntreebank,” in Proc. Conference"
        },
        {
          "12": "Empirical Methods in Natural Language Processing (EMNLP), Seattle,"
        },
        {
          "12": "WA, 2013, pp. 1631–1642."
        },
        {
          "12": "[34] C.-C. Hsu, S.-Y. Chen, C.-C. Kuo, T.-H. Huang, and L.-W. Ku,"
        },
        {
          "12": "“EmotionLines: An emotion corpus of multi-party conversations,”"
        },
        {
          "12": "the 11th International Conference on Language Resources and\nin Proc."
        },
        {
          "12": "Evaluation (LREC), Miyazaki, Japan, 2018, pp. 1597–1601."
        },
        {
          "12": "[35] C. Cerisara, S.\nJafaritazehjani, A. Oluokun, and H. T. Le, “Multi-"
        },
        {
          "12": "task dialog act and sentiment recognition on mastodon,” in Proc."
        },
        {
          "12": "the 27th International Conference on Computational Linguistics (COL-"
        },
        {
          "12": "ING), Santa Fe, NM, 2018, pp. 745–754."
        },
        {
          "12": "[36] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Multimodal"
        },
        {
          "12": "sentiment\nintensity analysis in videos: Facial gestures and verbal"
        },
        {
          "12": "messages,” IEEE Intelligent Systems, vol. 31, no. 6, pp. 82–88, Nov."
        },
        {
          "12": "2016."
        },
        {
          "12": "[37] W. Yu, H. Xu, F. Meng, Y. Zhu, Y. Ma, J. Wu, J. Zou, and K. Yang,"
        },
        {
          "12": "“CH-SIMS: A Chinese multimodal sentiment analysis dataset with"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": "for\nsentiment analysis,” in Proc. Conference on Empirical Methods"
        },
        {
          "13": "in Natural Language Processing (EMNLP), Abu Dhabi, United Arab"
        },
        {
          "13": "Emirates, 2022, pp. 4984–4994."
        },
        {
          "13": "[55]\nS. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo,\nI. Beltagy,"
        },
        {
          "13": "D. Downey,\nand N. A. Smith,\n“Don’t\nstop pretraining: Adapt"
        },
        {
          "13": "the 58th Annual\nlanguage models to domains and tasks,” in Proc."
        },
        {
          "13": "Meeting of\nthe Association for Computational Linguistics (ACL), Vir-"
        },
        {
          "13": "tual, 2020, pp. 8342–8360."
        },
        {
          "13": "[56]\nJ. Howard and S. Ruder, “Universal\nlanguage model fine-tuning"
        },
        {
          "13": "the 56th Annual Meeting of\nthe As-\nfor text classification,” in Proc."
        },
        {
          "13": "sociation for Computational Linguistics (ACL), Melbourne, Australia,"
        },
        {
          "13": "2018, pp. 328–339."
        },
        {
          "13": "[57]\nJ. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,"
        },
        {
          "13": "A. M. Dai, and Q. V. Le, “Finetuned language models are zero-"
        },
        {
          "13": "International Conference on Learning Repre-\nshot\nlearners,” in Proc."
        },
        {
          "13": "sentations (ICLR), Virtual, 2022, pp. 1–46."
        },
        {
          "13": "[58] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-"
        },
        {
          "13": "M. Chan, W. Chen et al., “Parameter-efficient fine-tuning of large-"
        },
        {
          "13": "scale pre-trained language models,” Nature Machine\nIntelligence,"
        },
        {
          "13": "vol. 5, no. 3, pp. 220–235, Mar. 2023."
        },
        {
          "13": "[59] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Larous-"
        },
        {
          "13": "silhe, A. Gesmundo, M. Attariyan,\nand S. Gelly,\n“Parameter-"
        },
        {
          "13": "efficient transfer learning for NLP,” in Proc. International Conference"
        },
        {
          "13": ""
        },
        {
          "13": "on Machine Learning (ICML), Long Beach, CA, 2019, pp. 2790–2799."
        },
        {
          "13": ""
        },
        {
          "13": "[60] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,"
        },
        {
          "13": ""
        },
        {
          "13": "and W. Chen, “LORA: Low-rank adaptation of\nlarge\nlanguage"
        },
        {
          "13": ""
        },
        {
          "13": "International Conference on Learning Representa-\nmodels,” in Proc."
        },
        {
          "13": ""
        },
        {
          "13": "tions (ICLR), Virtual, 2022, pp. 1–26."
        },
        {
          "13": ""
        },
        {
          "13": "[61] R. Plant, V. Giuffrida, and D. Gkatzia, “You are what you write:"
        },
        {
          "13": ""
        },
        {
          "13": "Preserving privacy in the era of\nlarge language models,” arXiv"
        },
        {
          "13": ""
        },
        {
          "13": "preprint arXiv:2204.09391, 2022."
        },
        {
          "13": ""
        },
        {
          "13": "[62] A. Gholami,\nS. Kim, Z. Dong, Z. Yao, M. W. Mahoney,\nand"
        },
        {
          "13": ""
        },
        {
          "13": "K. Keutzer, “A survey of quantization methods for efficient neural"
        },
        {
          "13": ""
        },
        {
          "13": "network inference,” in Low-Power Computer Vision.\nChapman and"
        },
        {
          "13": ""
        },
        {
          "13": "Hall/CRC, 2022, pp. 291–326."
        },
        {
          "13": ""
        },
        {
          "13": "arXiv\npreprint\n[63] OpenAI,\n“GPT-4\ntechnical\nreport,”"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2303.08774v3, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[64] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision"
        },
        {
          "13": ""
        },
        {
          "13": "transformers,” in Proc. the IEEE/CVF Conference on Computer Vision"
        },
        {
          "13": ""
        },
        {
          "13": "and Pattern Recognition (CVPR), New Orleans, LA, 2022, pp. 12 104–"
        },
        {
          "13": ""
        },
        {
          "13": "12 113."
        },
        {
          "13": ""
        },
        {
          "13": "[65] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei,\nJ. Ning, Y. Cao,"
        },
        {
          "13": ""
        },
        {
          "13": "Z. Zhang, L. Dong et al., “Swin transformer v2: Scaling up capacity"
        },
        {
          "13": ""
        },
        {
          "13": "and resolution,” in Proc. IEEE/CVF conference on computer vision and"
        },
        {
          "13": ""
        },
        {
          "13": "pattern recognition, New Orleans, LA, 2022, pp. 12 009–12 019."
        },
        {
          "13": ""
        },
        {
          "13": "[66] R. Huang, M.\nLi, D. Yang,\nJ.\nShi, X. Chang, Z. Ye, Y. Wu,"
        },
        {
          "13": ""
        },
        {
          "13": "Z. Hong,\nJ. Huang,\nJ. Liu et al., “AudioGPT: Understanding and"
        },
        {
          "13": ""
        },
        {
          "13": "generating speech, music, sound, and talking head,” arXiv preprint"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2304.12995, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[67] Y. Zhang, W. Han,\nJ. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen,"
        },
        {
          "13": ""
        },
        {
          "13": "B. Li, V. Axelrod, G. Wang et\nal., “Google USM: Scaling auto-"
        },
        {
          "13": ""
        },
        {
          "13": "matic speech recognition beyond 100 languages,” arXiv preprint"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2303.01037, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[68]\nS. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, “A"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv\npreprint\nsurvey\non multimodal\nlarge\nlanguage models,”"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2306.13549, 2023."
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "received his mas-\nZixing Zhang (M’15-SM’22)"
        },
        {
          "13": "ter degree in physical electronics from the Bei-"
        },
        {
          "13": "jing University of Posts and Telecommunications"
        },
        {
          "13": ""
        },
        {
          "13": "(BUPT), China,\nin 2010, and his PhD degree"
        },
        {
          "13": ""
        },
        {
          "13": "in computer engineering from Technical Univer-"
        },
        {
          "13": ""
        },
        {
          "13": "sity of Munich (TUM), Germany,\nin 2015. He"
        },
        {
          "13": ""
        },
        {
          "13": "is now a full professor at\nthe College of Com-"
        },
        {
          "13": ""
        },
        {
          "13": "puter Science and Electronic Engineering, Hu-"
        },
        {
          "13": ""
        },
        {
          "13": "nan University, China. From 2017 to 2019, he"
        },
        {
          "13": ""
        },
        {
          "13": "was a research associate with the Department of"
        },
        {
          "13": ""
        },
        {
          "13": "Computing at the Imperial College London (ICL),"
        },
        {
          "13": ""
        },
        {
          "13": "UK. Before that, he was a postdoctoral\nresearcher at\nthe University of"
        },
        {
          "13": ""
        },
        {
          "13": "Passau, Germany. His research focuses on human-centred emotion and"
        },
        {
          "13": ""
        },
        {
          "13": "health computation. To date, he has authored more than 110 publica-"
        },
        {
          "13": ""
        },
        {
          "13": "tions in peer-reviewed books,\njournals, and conference proceedings,"
        },
        {
          "13": ""
        },
        {
          "13": "leading to more than 5 000 citations (h-index 40). He serves as an"
        },
        {
          "13": ""
        },
        {
          "13": "associate editor of\nthe IEEE Transactions on Affective Computing and"
        },
        {
          "13": ""
        },
        {
          "13": "the Frontiers in Signal Processing, an editorial board member of\nthe"
        },
        {
          "13": ""
        },
        {
          "13": "Nature Scientific Reports, and a guest editor of\nthe IEEE Transactions"
        },
        {
          "13": ""
        },
        {
          "13": "on Emerging Topics in Computational\nIntelligence."
        },
        {
          "13": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": "for\nsentiment analysis,” in Proc. Conference on Empirical Methods"
        },
        {
          "13": "in Natural Language Processing (EMNLP), Abu Dhabi, United Arab"
        },
        {
          "13": "Emirates, 2022, pp. 4984–4994."
        },
        {
          "13": "[55]\nS. Gururangan, A. Marasovi´c, S. Swayamdipta, K. Lo,\nI. Beltagy,"
        },
        {
          "13": "D. Downey,\nand N. A. Smith,\n“Don’t\nstop pretraining: Adapt"
        },
        {
          "13": "the 58th Annual\nlanguage models to domains and tasks,” in Proc."
        },
        {
          "13": "Meeting of\nthe Association for Computational Linguistics (ACL), Vir-"
        },
        {
          "13": "tual, 2020, pp. 8342–8360."
        },
        {
          "13": "[56]\nJ. Howard and S. Ruder, “Universal\nlanguage model fine-tuning"
        },
        {
          "13": "the 56th Annual Meeting of\nthe As-\nfor text classification,” in Proc."
        },
        {
          "13": "sociation for Computational Linguistics (ACL), Melbourne, Australia,"
        },
        {
          "13": "2018, pp. 328–339."
        },
        {
          "13": "[57]\nJ. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,"
        },
        {
          "13": "A. M. Dai, and Q. V. Le, “Finetuned language models are zero-"
        },
        {
          "13": "International Conference on Learning Repre-\nshot\nlearners,” in Proc."
        },
        {
          "13": "sentations (ICLR), Virtual, 2022, pp. 1–46."
        },
        {
          "13": "[58] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-"
        },
        {
          "13": "M. Chan, W. Chen et al., “Parameter-efficient fine-tuning of large-"
        },
        {
          "13": "scale pre-trained language models,” Nature Machine\nIntelligence,"
        },
        {
          "13": "vol. 5, no. 3, pp. 220–235, Mar. 2023."
        },
        {
          "13": "[59] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Larous-"
        },
        {
          "13": "silhe, A. Gesmundo, M. Attariyan,\nand S. Gelly,\n“Parameter-"
        },
        {
          "13": "efficient transfer learning for NLP,” in Proc. International Conference"
        },
        {
          "13": ""
        },
        {
          "13": "on Machine Learning (ICML), Long Beach, CA, 2019, pp. 2790–2799."
        },
        {
          "13": ""
        },
        {
          "13": "[60] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,"
        },
        {
          "13": ""
        },
        {
          "13": "and W. Chen, “LORA: Low-rank adaptation of\nlarge\nlanguage"
        },
        {
          "13": ""
        },
        {
          "13": "International Conference on Learning Representa-\nmodels,” in Proc."
        },
        {
          "13": ""
        },
        {
          "13": "tions (ICLR), Virtual, 2022, pp. 1–26."
        },
        {
          "13": ""
        },
        {
          "13": "[61] R. Plant, V. Giuffrida, and D. Gkatzia, “You are what you write:"
        },
        {
          "13": ""
        },
        {
          "13": "Preserving privacy in the era of\nlarge language models,” arXiv"
        },
        {
          "13": ""
        },
        {
          "13": "preprint arXiv:2204.09391, 2022."
        },
        {
          "13": ""
        },
        {
          "13": "[62] A. Gholami,\nS. Kim, Z. Dong, Z. Yao, M. W. Mahoney,\nand"
        },
        {
          "13": ""
        },
        {
          "13": "K. Keutzer, “A survey of quantization methods for efficient neural"
        },
        {
          "13": ""
        },
        {
          "13": "network inference,” in Low-Power Computer Vision.\nChapman and"
        },
        {
          "13": ""
        },
        {
          "13": "Hall/CRC, 2022, pp. 291–326."
        },
        {
          "13": ""
        },
        {
          "13": "arXiv\npreprint\n[63] OpenAI,\n“GPT-4\ntechnical\nreport,”"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2303.08774v3, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[64] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision"
        },
        {
          "13": ""
        },
        {
          "13": "transformers,” in Proc. the IEEE/CVF Conference on Computer Vision"
        },
        {
          "13": ""
        },
        {
          "13": "and Pattern Recognition (CVPR), New Orleans, LA, 2022, pp. 12 104–"
        },
        {
          "13": ""
        },
        {
          "13": "12 113."
        },
        {
          "13": ""
        },
        {
          "13": "[65] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei,\nJ. Ning, Y. Cao,"
        },
        {
          "13": ""
        },
        {
          "13": "Z. Zhang, L. Dong et al., “Swin transformer v2: Scaling up capacity"
        },
        {
          "13": ""
        },
        {
          "13": "and resolution,” in Proc. IEEE/CVF conference on computer vision and"
        },
        {
          "13": ""
        },
        {
          "13": "pattern recognition, New Orleans, LA, 2022, pp. 12 009–12 019."
        },
        {
          "13": ""
        },
        {
          "13": "[66] R. Huang, M.\nLi, D. Yang,\nJ.\nShi, X. Chang, Z. Ye, Y. Wu,"
        },
        {
          "13": ""
        },
        {
          "13": "Z. Hong,\nJ. Huang,\nJ. Liu et al., “AudioGPT: Understanding and"
        },
        {
          "13": ""
        },
        {
          "13": "generating speech, music, sound, and talking head,” arXiv preprint"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2304.12995, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[67] Y. Zhang, W. Han,\nJ. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen,"
        },
        {
          "13": ""
        },
        {
          "13": "B. Li, V. Axelrod, G. Wang et\nal., “Google USM: Scaling auto-"
        },
        {
          "13": ""
        },
        {
          "13": "matic speech recognition beyond 100 languages,” arXiv preprint"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2303.01037, 2023."
        },
        {
          "13": ""
        },
        {
          "13": "[68]\nS. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, “A"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv\npreprint\nsurvey\non multimodal\nlarge\nlanguage models,”"
        },
        {
          "13": ""
        },
        {
          "13": "arXiv:2306.13549, 2023."
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "received his mas-\nZixing Zhang (M’15-SM’22)"
        },
        {
          "13": "ter degree in physical electronics from the Bei-"
        },
        {
          "13": "jing University of Posts and Telecommunications"
        },
        {
          "13": ""
        },
        {
          "13": "(BUPT), China,\nin 2010, and his PhD degree"
        },
        {
          "13": ""
        },
        {
          "13": "in computer engineering from Technical Univer-"
        },
        {
          "13": ""
        },
        {
          "13": "sity of Munich (TUM), Germany,\nin 2015. He"
        },
        {
          "13": ""
        },
        {
          "13": "is now a full professor at\nthe College of Com-"
        },
        {
          "13": ""
        },
        {
          "13": "puter Science and Electronic Engineering, Hu-"
        },
        {
          "13": ""
        },
        {
          "13": "nan University, China. From 2017 to 2019, he"
        },
        {
          "13": ""
        },
        {
          "13": "was a research associate with the Department of"
        },
        {
          "13": ""
        },
        {
          "13": "Computing at the Imperial College London (ICL),"
        },
        {
          "13": ""
        },
        {
          "13": "UK. Before that, he was a postdoctoral\nresearcher at\nthe University of"
        },
        {
          "13": ""
        },
        {
          "13": "Passau, Germany. His research focuses on human-centred emotion and"
        },
        {
          "13": ""
        },
        {
          "13": "health computation. To date, he has authored more than 110 publica-"
        },
        {
          "13": ""
        },
        {
          "13": "tions in peer-reviewed books,\njournals, and conference proceedings,"
        },
        {
          "13": ""
        },
        {
          "13": "leading to more than 5 000 citations (h-index 40). He serves as an"
        },
        {
          "13": ""
        },
        {
          "13": "associate editor of\nthe IEEE Transactions on Affective Computing and"
        },
        {
          "13": ""
        },
        {
          "13": "the Frontiers in Signal Processing, an editorial board member of\nthe"
        },
        {
          "13": ""
        },
        {
          "13": "Nature Scientific Reports, and a guest editor of\nthe IEEE Transactions"
        },
        {
          "13": ""
        },
        {
          "13": "on Emerging Topics in Computational\nIntelligence."
        },
        {
          "13": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "(M’05-SM’15-F’18)\nreceived his\nBj ¨orn Schuller"
        },
        {
          "14": "diploma in 1999, his doctoral degree in 2006,"
        },
        {
          "14": "and his habilitation and Adjunct Teaching Profes-"
        },
        {
          "14": "sorship in the subject area of signal processing"
        },
        {
          "14": "and machine intelligence in 2012, all\nin electri-"
        },
        {
          "14": "cal engineering and information technology from"
        },
        {
          "14": "Technische Universit ¨at Munchen\n(TUM), Ger-"
        },
        {
          "14": "many. He is a tenured Full Professor heading"
        },
        {
          "14": "the Chair of Embedded Intelligence for Health"
        },
        {
          "14": "Care\nand Wellbeing, University\nof Augsburg,"
        },
        {
          "14": "Germany,\nand\na Professor\nof Artificial\nIntelli-"
        },
        {
          "14": "gence heading GLAM, Department of Computing, at\nthe Imperial Col-"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "many. He is a tenured Full Professor heading"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "the Chair of Embedded Intelligence for Health"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "Care\nand Wellbeing, University\nof Augsburg,"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "Germany,\nand\na Professor\nof Artificial\nIntelli-"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "gence heading GLAM, Department of Computing, at\nthe Imperial Col-"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "lege London in London, UK. Dr. Schuller\nis the Field Chief Editor of"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "Frontiers in Digital Health,\nformer Editor\nin Chief of\nthe IEEE Trans-"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "actions on Affective Computing, President-emeritus and Fellow of\nthe"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "AAAC, Fellow of the IEEE, Golden Core Awardee of the IEEE Computer"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "Society, Fellow of\nthe ISCA, Fellow of\nthe BCS, Fellow of\nthe ELLIS,"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "and Senior Member of\nthe ACM. He (co-)authored 5 books and more"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "than 1200 publications in peer-reviewed books, journals, and conference"
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": "proceedings leading to more than 56 k citations (h-index 106)."
        },
        {
          "Technische Universit ¨at Munchen\n(TUM), Ger-": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Science from University of Augsburg, Germany.": "Since 2019,\nshe has been a postdoctoral\nre-"
        },
        {
          "Science from University of Augsburg, Germany.": "searcher\nin the Department of Computer Sci-"
        },
        {
          "Science from University of Augsburg, Germany.": "ence and Technology, University of Cambridge,"
        },
        {
          "Science from University of Augsburg, Germany.": "UK. Her research interests are in affective com-"
        },
        {
          "Science from University of Augsburg, Germany.": "puting and digital health. She (co)authored more than 60 publications"
        },
        {
          "Science from University of Augsburg, Germany.": "in peer-reviewed journals and conference proceedings. She has served"
        },
        {
          "Science from University of Augsburg, Germany.": "as a program committee member of the Audio/Visual Emotion Challenge"
        },
        {
          "Science from University of Augsburg, Germany.": "and Workshop in 2018 and a technical program committee member of"
        },
        {
          "Science from University of Augsburg, Germany.": "the Association for Computing Machinery (ACM) Multimedia since 2019,"
        },
        {
          "Science from University of Augsburg, Germany.": "and is a leading guest editor of\nthe IEEE Transactions on Emerging"
        },
        {
          "Science from University of Augsburg, Germany.": "Topics in Computational Intelligence, and an associate editor of the IEEE"
        },
        {
          "Science from University of Augsburg, Germany.": "Transactions on Affective Computing. She is a senior member of\nIEEE."
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Y Wang",
        "W Song",
        "W Tao",
        "A Liotta",
        "D Yang",
        "X Li",
        "S Gao",
        "Y Sun",
        "W Ge",
        "W Zhang",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Siddharth",
        "T Jung",
        "Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya"
      ],
      "year": "2019",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "5",
      "title": "Exploiting multi-CNN features in CNN-RNN based dimensional emotion recognition on the OMG in-the-wild dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with LSTM neural networks",
      "authors": [
        "Z Zhang",
        "F Ringeval",
        "J Han",
        "J Deng",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proc. 17th Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "7",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proc. AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Emobed: Strengthening monomodal emotion recognition via training with crossmodal emotion embeddings",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "10",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller",
        "K Star",
        "E Hajiyev",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. the 56th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "12",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "Bi-branch vision transformer network for EEG emotion recognition",
      "authors": [
        "W Lu",
        "T Tan",
        "H Ma"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "14",
      "title": "Sentiment analysis using novel and interpretable architectures of hidden markov models",
      "authors": [
        "I Perikos",
        "S Kardakis",
        "I Hatzilygeroudis"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "15",
      "title": "Towards the explainability of multimodal speech emotion recognition",
      "authors": [
        "P Kumar",
        "V Kaushik",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Proc. 22nd Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "16",
      "title": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Transformer-based self-supervised learning for emotion recognition",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "Proc. the 26th IEEE International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "18",
      "title": "Knowledge transfer for on-device speech emotion recognition with neural structured learning",
      "authors": [
        "Y Chang",
        "Z Ren",
        "T Nguyen",
        "K Qian",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Self-supervised ECG representation learning for emotion recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Autoencoder-based unsupervised domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "22",
      "title": "Comparative analyses of BERT, RoBERTa, DistilBERT, and XLNet for text-based emotion recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "Proc. 17th International Computer Conference on Wavelet Active Media Technology and Information Processing (IC-CWAMTIP)"
    },
    {
      "citation_id": "23",
      "title": "A hybrid explainable AI framework applied to global and local facial expression recognition",
      "authors": [
        "M Deramgozin",
        "S Jovanovic",
        "H Rabah",
        "N Ramzan"
      ],
      "year": "2021",
      "venue": "Proc. IEEE International Conference on Imaging Systems and Techniques (IST)"
    },
    {
      "citation_id": "24",
      "title": "AU-assisted graph attention convolutional network for micro-expression recognition",
      "authors": [
        "H.-X Xie",
        "L Lo",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Proc. the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Multimodal cross-attention bayesian network for social news emotion recognition",
      "authors": [
        "X Wang",
        "M Li",
        "Y Chang",
        "X Luo",
        "Y Yao",
        "Z Li"
      ],
      "year": "2023",
      "venue": "2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "26",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "27",
      "title": "A survey of large language models",
      "authors": [
        "W Zhao",
        "K Zhou",
        "J Li",
        "T Tang",
        "X Wang",
        "Y Hou",
        "Y Min",
        "B Zhang",
        "J Zhang",
        "Z Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "28",
      "title": "Emergent abilities of large language models",
      "authors": [
        "J Wei",
        "Y Tay",
        "R Bommasani",
        "C Raffel",
        "B Zoph",
        "S Borgeaud",
        "D Yogatama",
        "M Bosma",
        "D Zhou",
        "D Metzler",
        "E Chi",
        "T Hashimoto",
        "O Vinyals",
        "P Liang",
        "J Dean",
        "W Fedus"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "29",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell"
      ],
      "year": "2020",
      "venue": "Proc. the 34th International Conference on Neural Information Processing Systems (NeurIPS), Vancouver"
    },
    {
      "citation_id": "30",
      "title": "Will affective computing emerge from foundation models and general artificial intelligence? A first evaluation of ChatGPT",
      "authors": [
        "M Amin",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "31",
      "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "authors": [
        "Y Bai",
        "A Jones",
        "K Ndousse",
        "A Askell",
        "A Chen",
        "N Dassarma",
        "D Drain",
        "S Fort",
        "D Ganguli",
        "T Henighan"
      ],
      "year": "2022",
      "venue": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "arxiv": "arXiv:2204.05862"
    },
    {
      "citation_id": "32",
      "title": "Constitutional AI: Harmlessness from AI feedback",
      "authors": [
        "Y Bai",
        "S Kadavath",
        "S Kundu",
        "A Askell",
        "J Kernion",
        "A Jones",
        "A Chen",
        "A Goldie",
        "A Mirhoseini",
        "C Mckinnon"
      ],
      "year": "2022",
      "venue": "Constitutional AI: Harmlessness from AI feedback",
      "arxiv": "arXiv:2212.08073"
    },
    {
      "citation_id": "33",
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "authors": [
        "R Socher",
        "A Perelygin",
        "J Wu",
        "J Chuang",
        "C Manning",
        "A Ng",
        "C Potts"
      ],
      "year": "2013",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "34",
      "title": "EmotionLines: An emotion corpus of multi-party conversations",
      "authors": [
        "C.-C Hsu",
        "S.-Y Chen",
        "C.-C Kuo",
        "T.-H Huang",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Proc. the 11th International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "35",
      "title": "Multitask dialog act and sentiment recognition on mastodon",
      "authors": [
        "C Cerisara",
        "S Jafaritazehjani",
        "A Oluokun",
        "H Le"
      ],
      "year": "2018",
      "venue": "Proc. the 27th International Conference on Computational Linguistics (COL-ING)"
    },
    {
      "citation_id": "36",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "37",
      "title": "CH-SIMS: A Chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proc. the 58th Annual Meeting of the Association for Computational Linguistics (ACL), Virtual"
    },
    {
      "citation_id": "38",
      "title": "M3ED: multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proc. the 60th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "39",
      "title": "DARER: dual-task temporal relational recurrent reasoning network for joint dialog sentiment classification and act recognition",
      "authors": [
        "B Xing",
        "I Tsang"
      ],
      "year": "2022",
      "venue": "Proc. the 60th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "40",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proc. the 55th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "41",
      "title": "Language models as emotional classifiers for textual conversation",
      "authors": [
        "C Heaton",
        "D Schwartz"
      ],
      "year": "2020",
      "venue": "Proc. the 28th ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "42",
      "title": "Exploiting unsupervised data for emotion recognition in conversations",
      "authors": [
        "W Jiao",
        "M Lyu",
        "I King"
      ],
      "year": "2020",
      "venue": "Proc. the 58th Annual Meeting of the Association for Computational Linguistics (ACL), Virtual"
    },
    {
      "citation_id": "43",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proc. the 29th ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "44",
      "title": "Dynamically adjust word representations using unaligned multimodal information",
      "authors": [
        "J Guo",
        "J Tang",
        "W Dai",
        "Y Ding",
        "W Kong"
      ],
      "year": "2022",
      "venue": "Proc. the 30th ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "45",
      "title": "Seq2Seq2Sentiment: Multimodal sequence to sequence models for sentiment analysis",
      "authors": [
        "H Pham",
        "T Manzini",
        "P Liang",
        "B Pocz"
      ],
      "year": "2018",
      "venue": "Proc. Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "46",
      "title": "CTFN: hierarchical learning for multimodal sentiment analysis using coupled-translation fusion network",
      "authors": [
        "J Tang",
        "K Li",
        "X Jin",
        "A Cichocki",
        "Q Zhao",
        "W Kong"
      ],
      "venue": "Proc. the 59th Annual Meeting of the Association for Computational Linguistics (ACL), Virtual, 2021"
    },
    {
      "citation_id": "47",
      "title": "A Transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "J.-B Delbrouck",
        "N Tits",
        "M Brousmiche",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Proc. Second Grand-Challenge and Workshop on Multimodal Language"
    },
    {
      "citation_id": "48",
      "title": "COGMEN: contextualized GNN based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Proc. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)"
    },
    {
      "citation_id": "49",
      "title": "Every document owns its structure: Inductive text classification via graph neural networks",
      "authors": [
        "Y Zhang",
        "X Yu",
        "Z Cui",
        "S Wu",
        "Z Wen",
        "L Wang"
      ],
      "year": "2020",
      "venue": "Proc. the 58th Annual Meeting of the Association for Computational Linguistics (ACL), Virtual"
    },
    {
      "citation_id": "50",
      "title": "Back-translated task adaptive pretraining: Improving accuracy and robustness on text classification",
      "authors": [
        "J Lee",
        "J Kim",
        "P Kang"
      ],
      "year": "2021",
      "venue": "Back-translated task adaptive pretraining: Improving accuracy and robustness on text classification",
      "arxiv": "arXiv:2107.10474"
    },
    {
      "citation_id": "51",
      "title": "SEMGraph: Incorporating sentiment knowledge and eye movement into graph model for sentiment analysis",
      "authors": [
        "B Wang",
        "B Liang",
        "J Du",
        "M Yang",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "52",
      "title": "Pre-training transformers as energy-based cloze models",
      "authors": [
        "K Clark",
        "M Luong",
        "Q Le",
        "C Manning"
      ],
      "year": "2020",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP), Virtual"
    },
    {
      "citation_id": "53",
      "title": "SentiLARE: Sentimentaware language representation learning with linguistic knowledge",
      "authors": [
        "P Ke",
        "H Ji",
        "S Liu",
        "X Zhu",
        "M Huang"
      ],
      "year": "2020",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP), Virtual"
    },
    {
      "citation_id": "54",
      "title": "Sentiment-aware word and sentence level pre-training for sentiment analysis",
      "authors": [
        "S Fan",
        "C Lin",
        "H Li",
        "Z Lin",
        "J Su",
        "H Zhang",
        "Y Gong",
        "J Guo",
        "N Duan"
      ],
      "year": "2022",
      "venue": "Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "55",
      "title": "Don't stop pretraining: Adapt language models to domains and tasks",
      "authors": [
        "S Gururangan",
        "A Marasović",
        "S Swayamdipta",
        "K Lo",
        "I Beltagy",
        "D Downey",
        "N Smith"
      ],
      "year": "2020",
      "venue": "Proc. the 58th Annual Meeting of the Association for Computational Linguistics (ACL), Virtual"
    },
    {
      "citation_id": "56",
      "title": "Universal language model fine-tuning for text classification",
      "authors": [
        "J Howard",
        "S Ruder"
      ],
      "year": "2018",
      "venue": "Proc. the 56th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "57",
      "title": "Finetuned language models are zeroshot learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Zhao",
        "K Guu",
        "A Yu",
        "B Lester",
        "N Du",
        "A Dai",
        "Q Le"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Learning Representations (ICLR), Virtual"
    },
    {
      "citation_id": "58",
      "title": "Parameter-efficient fine-tuning of largescale pre-trained language models",
      "authors": [
        "N Ding",
        "Y Qin",
        "G Yang",
        "F Wei",
        "Z Yang",
        "Y Su",
        "S Hu",
        "Y Chen",
        "C.-M Chan",
        "W Chen"
      ],
      "year": "2023",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "59",
      "title": "Parameterefficient transfer learning for NLP",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "60",
      "title": "LORA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Learning Representations (ICLR), Virtual"
    },
    {
      "citation_id": "61",
      "title": "You are what you write: Preserving privacy in the era of large language models",
      "authors": [
        "R Plant",
        "V Giuffrida",
        "D Gkatzia"
      ],
      "year": "2022",
      "venue": "You are what you write: Preserving privacy in the era of large language models",
      "arxiv": "arXiv:2204.09391"
    },
    {
      "citation_id": "62",
      "title": "A survey of quantization methods for efficient neural network inference",
      "authors": [
        "A Gholami",
        "S Kim",
        "Z Dong",
        "Z Yao",
        "M Mahoney",
        "K Keutzer"
      ],
      "year": "2022",
      "venue": "Low-Power Computer Vision"
    },
    {
      "citation_id": "63",
      "title": "GPT-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report",
      "arxiv": "arXiv:2303.08774v3"
    },
    {
      "citation_id": "64",
      "title": "Scaling vision transformers",
      "authors": [
        "X Zhai",
        "A Kolesnikov",
        "N Houlsby",
        "L Beyer"
      ],
      "year": "2022",
      "venue": "Proc. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "65",
      "title": "Swin transformer v2: Scaling up capacity and resolution",
      "authors": [
        "Z Liu",
        "H Hu",
        "Y Lin",
        "Z Yao",
        "Z Xie",
        "Y Wei",
        "J Ning",
        "Y Cao",
        "Z Zhang",
        "L Dong"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "66",
      "title": "AudioGPT: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "R Huang",
        "M Li",
        "D Yang",
        "J Shi",
        "X Chang",
        "Z Ye",
        "Y Wu",
        "Z Hong",
        "J Huang",
        "J Liu"
      ],
      "year": "2023",
      "venue": "AudioGPT: Understanding and generating speech, music, sound, and talking head",
      "arxiv": "arXiv:2304.12995"
    },
    {
      "citation_id": "67",
      "title": "Google USM: Scaling automatic speech recognition beyond 100 languages",
      "authors": [
        "Y Zhang",
        "W Han",
        "J Qin",
        "Y Wang",
        "A Bapna",
        "Z Chen",
        "N Chen",
        "B Li",
        "V Axelrod",
        "G Wang"
      ],
      "year": "2023",
      "venue": "Google USM: Scaling automatic speech recognition beyond 100 languages",
      "arxiv": "arXiv:2303.01037"
    },
    {
      "citation_id": "68",
      "title": "UK. Before that, he was a postdoctoral researcher at the University of Passau, Germany. His research focuses on human-centred emotion and health computation. To date, he has authored more than 110 publications in peer-reviewed books, journals, and conference proceedings, leading to more than 5 000 citations (h-index 40)",
      "authors": [
        "S Yin",
        "C Fu",
        "S Zhao",
        "K Li",
        "X Sun",
        "T Xu",
        "E Chen"
      ],
      "year": "2015",
      "venue": "He serves as an associate editor of the IEEE Transactions on Affective Computing and the Frontiers in Signal Processing",
      "arxiv": "arXiv:2306.13549"
    }
  ]
}