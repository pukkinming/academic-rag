{
  "paper_id": "2206.02119v1",
  "title": "A Multimodal Corpus For Emotion Recognition In Sarcasm",
  "published": "2022-06-05T08:01:09Z",
  "authors": [
    "Anupama Ray",
    "Shubham Mishra",
    "Apoorva Nunna",
    "Pushpak Bhattacharyya"
  ],
  "keywords": [
    "Emotion understanding",
    "sarcasm",
    "multimodal",
    "valence-arousal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While sentiment and emotion analysis have been studied extensively, the relationship between sarcasm and emotion has largely remained unexplored. A sarcastic expression may have a variety of underlying emotions. For example, \"I love being ignored\" belies sadness, while \"my mobile is fabulous with a battery backup of only 15 minutes!\" expresses frustration. Detecting the emotion behind a sarcastic expression is non-trivial yet an important task. We undertake the task of detecting the emotion in a sarcastic statement, which to the best of our knowledge, is hitherto unexplored. We start with the recently released multimodal sarcasm detection dataset (MUStARD) pre-annotated with 9 emotions. We identify and correct 343 incorrect emotion labels (out of 690). We double the size of the dataset, label it with emotions along with valence and arousal which are important indicators of emotional intensity. Finally, we label each sarcastic utterance with one of the four sarcasm types-Propositional, Embedded, Likeprefixed and Illocutionary, with the goal of advancing sarcasm detection research. Exhaustive experimentation with multimodal (text, audio, and video) fusion models establishes a benchmark for exact emotion recognition in sarcasm and outperforms the state-of-art sarcasm detection. We release the dataset enriched with various annotations and the code for research purposes: https://github.com/apoorva-nunna/MUStARD_Plus_Plus",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion understanding leads to a deeper insight into the intent of the speaker and is key to generating the right response in conversational systems. Detecting emotions and sarcasm is crucial for all services involving human interactions, such as chatbots, e-commerce, e-tourism, and several other businesses. To be able to understand the user's intent, we started with the research problem on understanding the emotions that lead to the usage of sarcasm in a conversation. Sarcasm is a very sophisticated linguistic articulation where the surface meaning often stands in contrast to the underlying deeper meaning. While this incongruity is the key element of sarcasm, the intent could be to appear humorous, ridicule someone, or to express contempt. Thus sarcasm is considered a very nuanced or intelligent language construct that poses several challenges to emotion recognition; for example, perceived emotion could be completely flipped due to the presence of sarcasm. Sarcasm often relies on verbal and non-verbal cues (pitch, tone, emphasis in speech, and body language in video). Even for humans, understanding the underlying emotion is challenging without the audio/video or the context of the conversation. However, researchers have worked on sarcasm detection on text modality with textual datasets (such as tweets  (Oprea and Magdy, 2020) , Reddit short texts  (Khodak et al., 2017) , dialogue  (Oraby et al., 2016)  etc) for a decade. Recently we have seen multimodal datasets in the space of sarcasm detection, for example, image data from Twitter  (Cai et al., 2019) , code-mixed sarcasm and humor detection dataset  (Bedi et al., 2021) .  (Castro et al., 2019)  released a video dataset for sarcasm detection called MUStARD with 345 sarcastic videos and 345 non-sarcastic videos.  (Chauhan et al., 2020)  annotated MUStARD data with 9 emotion labels and sentiment (all sarcastic utterances having negative sentiment) and used emotion and sentiment to improve sarcasm detection. We started with this emotion-labeled variant of MUS-tARD provided by  (Chauhan et al., 2020)  to build a multiclass emotion recognizer on sarcastic utterances and observed several labeling errors while performing error analysis. During the annotation effort, we doubled the dataset by adding new utterances from similar sitcom genre series as in MUStARD while maintaining 50% sarcastic and 50% non-sarcastic videos. The affective dimensions of valence and arousal are commonly studied in the psychological and cognitive exploration of emotion  (Mohammad, 2021)  and help in better understanding of emotion category and intensity. Thus the entire dataset is annotated with arousal and valence along with the perceived emotion of the speaker. While valence indicates the extent to which the emotion is positive or negative, arousal measures the intensity of the emotion associated  (Cowie and Cornelius, 2003) . Finally, we also add sarcasm type as metadata which would help advance sarcasm detection research as well as give an understanding of what kind of information/modality is required to improve sarcasm detection. The four types of sarcasm are: Propositional, Embedded, Like-Prefixed and Illocutionary  (Camp, 2012) . Propositional sarcasm needs context information to be able to detect whether it's sarcasm or not. For example: \"your plan sounds fantastic!\" may seem non-sarcastic if the context information is not present  (Zvolenszky, 2012) . Embedded sarcasm has an embedded incongruity within the utterance; thus, the text itself is sufficient to detect sarcasm. For example: \"It's so much fun working at 2 am at night\". Like-prefixed sarcasm as the name suggests uses a like-phrase to show the incongruity of the argu-ment being said, for example, \"Like you care\"  (Joshi et al., 2017) . Illocutionary sarcasm is a type of sarcasm that bears the sarcasm in the non-textual cues, and the text is often the opposite of the attitude captured in the audio or video modality.  (Zvolenszky, 2012)  give an example of rolling eyes while saying \"Yeah right\" being a sarcastic sentence; although the text is sincere prosodic features in audio and eye movement in the video clearly show the sarcasm. The main contributions of this paper are:\n\n• An extended data resource which we call MUS-tARD++ where we have doubled the existing MUStARD dataset and added labels for emotion, valence, arousal, and sarcasm-type information.\n\n• Identify and correct labeling issues in emotion labels on MUStARD data presented in  (Chauhan et al., 2020) .\n\n• Exhaustive experimentation to benchmark multimodal fusion models for emotion detection in sarcasm.\n\nFigure  1  is a sample in MUStARD++ with the labels and metadata information added to each video utterance.\n\nThe text in the red bubble is the transcription of the sarcastic utterance, and the text in the yellow bubbles is the contextual sentences transcribed from the contextual video frames. The sarcasm is clearly evident just from the text modality (Embedded sarcasm). This utterance is also an illustration of cases where the explicit emotion and implicit emotion of the speaker are different. This is common in sarcastic utterances where the speaker makes a sarcastic comment with either no expression or vocal changes but means quite the opposite.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "While there exist several studies on sentiment and emotion analysis, the relationship between emotion and sar-casm has been largely unaddressed. Most of the existing research has focused on the detection of sarcasm  (Joshi et al., 2016) ,  (Joshi et al., 2018) . Research studying the impact of sarcasm on sentiment analysis  (Maynard and Greenwood, 2014)  showed that sarcasm often has a negative sentiment, but the associated emotion(s) is important to frame the response and follow-up communication.  (Schifanella et al., 2016)  extended sarcasm detection to multimodal data (images and text) from social media and observed that visual features did boost the performance over the textual models. Along similar lines,  (Sangwan et al., 2020)  reported the improvement of the sarcasm detection task by using image data in addition to text. The dataset is curated from Instagram and the authors consider the image, text caption, and the transcript embedded within the image as multiple modalities.  (Cai et al., 2019 ) used text features, image features and image attributes as three modalities and proposed a multimodal hierarchical fusion model for sarcasm detection on tweets.\n\nMUStARD  (Castro et al., 2019)  is a subset of Multimodal Emotion Lines Dataset (MELD)  (Poria et al., 2018)  and MELD is a multimodal extension of textual dataset EmotionLines  (Chen et al., 2018) . MELD contains about 13,000 utterances from the TV series Friends, labeled with one of the seven emotions (anger, disgust, sadness, joy, neutral, surprise, and fear) and sentiment. EmotionLines  (Chen et al., 2018)  and EmoryNLP  (Zahiri and Choi, 2017)  are textual datasets with conversational data, the former containing data from the TV show Friends and private Facebook messenger dialogues, while the latter was also curated from the series Friends. Iemocap  (Busso et al., 2008 ) is a wellknown multimodal, dyadic dataset with 151 recorded videos annotated with categorical emotion labels, as well as dimensional labels such as valence, activation, and dominance. However, none of them have sarcastic utterances.  (Bedi et al., 2021)  released a Hindi-English code-mixed dataset for the problem of Multimodal Sarcasm Detection and Humour Classification in a conversational dialog. They also propose an attention-based architecture named MSH-COMICS for enhanced utterance classification. Along with categorical classification of basic emotions, seminal works  (Russell, 1980; Plutchick, 1980)  also propose dimensional models of emotion (Ex: Valence, Arousal, Dominance), which could help in capturing the complicated nature of human emotions better.  (Zafeiriou et al., 2017 ) created a database of 298 videos (non-enacted, in-the-wild) and captured facial affect in their subjects in terms of valence arousal annotations ranging between -1 to +1. Similar work was undertaken in  (Preot ¸iuc-Pietro et al., 2016)  where valence and arousal were annotated on a nine-point scale on Facebook data. They also release bag-of-words regression models trained on their data which outperform popular sentiment analysis lexicons in valence-arousal prediction tasks.\n\n3. Label Changes in MUStARD  (Chauhan et al., 2020)  annotated the MUStARD dataset with emotions and sentiment and showed that emotion and sentiment labels could help sarcasm detection.\n\nSince our study mainly focuses on understanding the speaker's emotion leading to the use of sarcasm, we used their basic emotion annotation. After performing extensive experiments with all combinations of Video, Text and Audio and several state-of-art models, we observed that most of the errors arise from the model predicting negative emotions when the true label is either Neutral or Happy. On detailed qualitative analysis, we observed that the labels for those sarcastic datapoints seemed intuitively incorrect. We built several models using each modality separately and also in combinations using different types of feature extractors and classifiers on the dataset. We flagged cases where majority of the models agreed with each other but disagreed with ground truth labels to obtain instances that needed re-annotation. We also grouped the error categories and the flagged cases to few distinct categories and observed that most of the errors are in sarcastic utterances.\n\nOur analysis flagged 399 cases of disagreement with  (Chauhan et al., 2020)  out of the 690 video utterances.\n\nWe initiated an unbiased manual labeling effort by a team of annotators on the entire dataset without giving them the labels from  (Chauhan et al., 2020) . The reannotation effort led to identifying 88 labeling issues in non-sarcastic and 255 labeling issues in sarcastic sentences.\n\nA major chunk of errors (90 out of 345 sarcastic sentences) is in utterances previously labeled as Neutral.\n\nLiterature shows that people resort to sarcasm in their utterances when they have negative intent or negative sentiment  (Joshi et al., 2017) . In sarcasm, the explicit emotion can be positive, but the implied emotion/sentiment must have opposite polarity; hence it seemed unlikely for neutral or happy to appear in implicit emotion. Table  1  shows an example of a label error wherein the utterance is marked as neutral for both explicit and implicit emotion. The sarcastic utterance (in gray) is expressed out of Disgust and cannot be Neutral. Also, the audio and video clearly indicate that the speaker was overexcited to place the order before anyone else could speak. This particular utterance is an example of Propositional sarcasm since we need the prior conversations to understand the sarcasm but the textual sentences are enough and doesn't need additional modalities for sarcasm detection. Additional modalities are however crucial for understanding the emotions for such cases. Our annotators felt that the cases labeled as neutral originally might have been difficult to annotate and thus were marked as neutral under majority voting.\n\nTable  2  shows the number of label changes that were done on original MUStARD dataset. As seen in Table  2 , most labeling errors are in sarcastic utterances due to the challenges sarcasm adds. There were 62 sarcastic utterances which were labeled with happy as implicit The backwash into this glass is every pathogen that calls your mouth home, sweet home. Not to mention the visitors who arrive on the dancing tongue of your subtropical girlfriend.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Neu Neu Exi Dis",
      "text": "Hey! That's my sister and my country you're talking about. Leonard may have defiled one, but I won't have you talking smack about the other.\n\nYou guys ready to order?\n\nYes, I'd like a seven-day course of penicillin, some, uh, syrup of ipecac-to induce vomiting-and a mint. emotion and 114 sarcastic sentences had happy as explicit emotion. While happy can be an explicit emotion, our annotators suggested that the correct intent for such sarcastic utterances should be ridicule or mockery as these shows belong to the genre of situational comedy (sit-com), wherein characters use sarcasm to ridicule their friends while demonstrating happiness explicitly. Thus we introduced a new label Ridicule and allowed annotators to label as per these labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "Towards understanding emotions in sarcasm, we had two main challenges: difficulty in getting multimodal sarcastic data, and challenges of annotating the perceived emotion of a speaker for every sarcastic utterance. While 10,000 non-sarcastic videos could be gathered in MELD  (Poria et al., 2018) , only 345 out of them were sarcastic  (Castro et al., 2019)  which stands proof to the difficulty in finding multimodal sarcasm data. In this work, we doubled the size of this dataset, by carefully adding sarcastic videos from similar genre, annotate it with sarcasm presence or absence, as well as emotions, arousal and valence. We also point out that to improve sarcasm detection, it is important to understand the type of sarcasm present and thus annotate each video with the sarcasm types -Propositional, Illocutionary, Like-Prefixed and Embedded.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "In  (Castro et al., 2019) , authors collected videos from sit-com TV shows: Friends, Big Bang Theory (seasons 1-8), The Golden Girls, and Burnistoun (also referred to as Sarcasmaholics Anonymous). We collected all videos from The Big Bang Theory season 9-12, out of which we could only get 216 sarcastic video utterances. We considered another series of similar genre called The Silicon Valley 1 which has 6 seasons of 53 episodes.\n\nOut of the 53 episodes, we could only find 41 video utterances that are sarcastic. Although all such videos have humor, not all are sarcastic, thus needing careful observation while selecting and manual annotation. We added equal number of non-sarcastic videos with context to create balanced sarcasm detection dataset. While in non-sarcastic sentences a speaker might have only one emotion, sarcasm due to its incongruous nature, exhibits an extrinsic and an intrinsic emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Annotation Protocol And Observations",
      "text": "We employ seven annotators proficient in English.\n\nWhile one annotator has a Ph.D. in Humanities, two of them were linguists; others were engineering graduates. They were selected from a pool of annotators due to their prior working experience in the field of Sentiment and Emotion Analysis and understanding of emotion and sarcasm. We have four male and three female annotators and all annotators were in the age group of 18-30. They were provided the detailed instructions on the annotation protocol before beginning the annotation process with examples of each type of sarcasm. In the first round of manual annotation, we gave our annotators the original MUStARD dataset for emotion annotation and asked them to put their labels for extrinsic and intrinsic emotion of the speaker without access to the emotion labels provided by  (Chauhan et al., 2020) . Instead of annotating only those videos where we observed the incorrect labels, we decided to annotate all videos of existing dataset as well as our newly collected video utterances. Annotators had access to full videos for annotation but were instructed to start with only text transcription of the utterance, then proceed with text transcriptions of contextual frames and finally watch the utterance and context video. Following this annotation protocol is especially important to be able to correctly classify the sarcasm type. For example, text to be observed in isolation for Embedded and Like-Prefixed sarcasm, transcript of utterance and context should be observed in isolation for Propositional sarcasm, and the whole video to be considered for Illocutionary sarcasm. Out of the 601 sarcastic videos, we have 333 Propositional ( 55.4%), 178 Illocutionary 29.6%, 87 Embedded 14.4% and 3 Like-prefixed 0.4%  Initially we kept the emotion labels the same as  (Chauhan et al., 2020)  (i.e. Anger, Excitement, Fear, Sad, Surprise, Frustrated, Happy, Neutral, Disgust), but after annotating 25 to 30% videos, annotators suggested for a label which is in between frustration and disgust, and close to mockery. We looked at some examples (one of them is mentioned in Figure  -1 ) and named this category Ridicule. According to Plutchik's wheel of emotion  (Plutchick, 1980)  contempt is a higher-order emotion composed of the two basic emotions -anger and disgust. Since in this genre of situational comedy, the most likely emotion is frustration and disgust, and not anger, we cannot directly call all such instances contempt. Also, the intent of the speaker of the sarcastic utterance is predominantly to mock or ridicule; thus, we called this category Ridicule. Also, Ridicule is not a basic emotion present in the gold standard emotion scales  (Plutchick, 1980; Ekman, 1999) , and we call it just a category in our labeling scheme and not a basic emotion.  During re-annotation, 345 of the sarcastic videos from MUStARD are annotated as sarcastic, implying that the annotators have good understanding of sarcasm. Also, out of 264 new videos, 8 videos are moved to nonsarcastic because at least one annotator annotated it as non-sarcastic, making the total number of sarcastic videos in MUStARD++ 601. We increased the number of non-sarcastic videos to 601 in the extended dataset.\n\nThe final label was chosen via majority voting. The overall inter-annotator agreement was calculated with a Kappa score of 0.595, which is comparable with the Kappa score of 0.5877 of original MUStARD annotations. Table  5  shows the explicit and implicit emotion distribution of the extended dataset. Additionally, annotators were instructed to rate each utterance with a valence and an arousal rating ranging from 1 to 9, with 1 being extremely low, 9 being extremely high and 5 being neutral. Pair-wise Quadratic Cohen Kappa was used to evaluate the inter-annotator agreement, and the average agreement was found to be 0.638 for valence and 0.689 for arousal.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "Although our primary goal is to detect emotions in sarcasm, we also benchmark the proposed extended dataset We analyze the impact of context and speaker information in each of the models. In the speaker dependent setup we passed the speaker information as a one-hot vector along with the utterance. When no such speaker information is passed, that method is being referred to as speaker-independent method. This was done as we observed that there are specific characters in each series who pass most of the sarcastic comments. Thus we wanted to see if the models benefit from the speaker information or not.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preprocessing And Feature Extraction",
      "text": "Owing to the presence of multiple modalities, the features from text, audio and video were separately extracted and fused appropriately to act as input into our model. We discuss our feature extraction methods in detail below.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Textual Modality",
      "text": "In order to extract features from transcript (context and utterance), we tried using different transformer models such as BERT  (Devlin et al., 2018) ,BART  (Lewis et al., 2019) , RoBERTa  (Liu et al., 2019)  and T5  (Raffel et al., 2020) . BART performed slightly better in all experiments (only text, as well as in combination with audio and video) over BERT, RoBERTa and T5 models, thus we continued with BART-large representations for text. BART provides a feature vector representation x t ∈ R dt for every instance x. We encode the text using BART Large model with d t = 1024 and use the mean of the last four transformer layer representations to get a unique embedding representation for both utterance and context.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Audio Modality",
      "text": "We extract low-level features from the audio data stream for each utterance in the dataset to take advantage of information from the audio modality. We sampled the audio signal at 22.5KHz. Since the audio has background noise and canned laughter, we used vocal-separation method to process it 2  . We extracted three low-level features: Mel Frequency Cepstral Coefficients (MFCC), Mel spectrogram (using Librosa library(McFee et al., 2022)), and prosodic features using OpenSMILE 3  .\n\nWe split the audio signal into equal length segments to maintain consistent feature representation in all instances. Since the audio signal length varies for different utterances, this segmentation helps in keeping vector size constant across dataset. For each segment we extract MFCC, Mel spectrogram and prosodic features of size d m , d s , d p respectively. Then we take the average across segments to get the final feature vector. Here d m = 128 , d s = 128 , d p = 35 , so our audio feature vector is of size d a = 291 . We had also experimented with self-supervised speech encoders  (Pascual et al., 2019) . However due to the very small number of sarcastic utterances, such models are unable to learn and thus we decided to stick with our low-level audio features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video Modality",
      "text": "In order to extract visual features from the videos, we have used a pool5 layer of pre-trained ResNet-152  (He et al., 2016)  image classification model. To improve the video representation and reduce noise, we extracted the key frames to be passed to ResNet-152, instead of feeding in information from all of the frames. Key frame extraction is widely used in the vision community and is defined as the frames that form the most appropriate summary of a given video  (Jadon and Jasim, 2019) . We used an open source tool called Katna 4  , to perform keyframe extraction. For final feature vectors we average the vectors of each key frame of an instance (context and utterance) extracted from ResNet-152. The size of final video feature representation in d v = 2048 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "We perform multi-class emotion classification experiments using the features extracted. Since we have three modalities, context and speaker information, we perform several ablation studies to understand the impact of presence or absence of each of these aspects.\n\nFor the multi-modal fusion, we use collaborative gating architecture introduced in  (Liu et al., 2020) , with the only difference that not all the input embeddings need to be pretrained. First we calculate projection Ψ (i) (V ) where i ∈ {t, tc, a, ac, v , vc} and t, a, v , c are text, audio, video and corresponding context. The collaborative gating module implements two tasks: first, we find the attention vector prediction for three main input projections referred to as projection embeddings (i.e for our utterances in three modalities)\n\nwhere functions h φ and g θ are used to model the pairwise relationship between projection Ψ (i) and Ψ (j ) . Also i ∈ {t, a, v } and j ∈ {t, tc, a, ac, v , vc}.\n\nThen we perform expert response modulation using the attention vector prediction calculated. For response modulation of each modality projection we perform-\n\nwhere σ is an element-wise sigmoid activation and • is the element-wise multiplication (Hadamard product).\n\nAll the modulated projections are concatenated and passed to fully connected linear layers (ReLU) followed by a softmax layer to predict target class probability distribution. Cross entropy loss is used for all classification experiments.\n\nFor completeness in bench-marking our data, we also perform Majority sampling (assigns the emotion class with majority examples as all samples), Random Sampling (predictions are sampled equally throughout the test set using this baseline). We also perform one-vs-rest experiments for each emotion which contain a sigmoid layer instead of a softmax layer as classification head.\n\nThe results of these baselines are reported in the supplementary material section 8.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Analysis",
      "text": "This section discusses the benchmarking experiments done with the proposed dataset for sarcasm detection, emotion recognition and arousal-valence prediction.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results Of Sarcasm Detection",
      "text": "Table  7  shows results of our best model for sarcasm detection on both MUStARD and MUStARD++. Our results outperform state-of-art models significantly on MUStARD which demonstrates the superiority of the collaborative gating-based multimodal fusion, and the best modality features selected were BART for text, low-level audio descriptors, and ResNET video features). We performed attention over modalities and intra-modality attention which helped us understand the importance of features. Also in  (Chauhan et al., 2020)  emotion and sentiment labels are used in the sarcasm detection task, but we are able to outperform them without using emotion or sentiment label. Table  8 : Sarcasm detection results for MUStARD++, Weighted Average might bias the system towards sarcasm. We intend to use this sarcasm detection module as the first module of our system followed by emotion recognition on the sarcastic sentences and valence and arousal prediction to understand the degree of emotions identified, Thus did not use emotion, sentiment, or valence-arousal for sarcasm detection task. However, we plan to use sarcasm detection output and valence-arousal predictions to see if we can improve emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emotion Recognition Results",
      "text": "Of the various models and features that we used, BART for text, MFCC, spectrogram and prosodic for audio and features learnt from keyframes using ResNET for video worked best for this dataset. Due to the small size of this dataset, we pretrained models on IEMOCAP  (Busso et al., 2008)  and MELD  (Poria et al., 2018)  and then tried zero-shot experiments on MUStARD and MUS-tARD++. Since IEMOCAP and MELD do not have any sarcastic utterances, the models saw a significant drop in F-score when tested on sarcastic data. We also extracted learnt audio features using state-of-art self-supervised PASE network  (Pascual et al., 2019)  but models built on PASE features require significantly large sarcastic data although pretrained on different utterances. Table  9  and Table  10  show the detailed emotion classification results on sarcastic utterances for both implicit and explicit emotion in speaker-dependent and speakerindependent setting. Ablation studies across all modalities show that audio and video when used with text perform better. This is intuitive because we consider the variation in speech signals in audio and visual features from video while ignoring the actual spoken content. We observe that for emotion classification, contextual information plays a key role. Although we observe similar numbers in speaker-dependent and speaker-independent setting, it is better to have a speaker-independent setting than limit the overall method by passing speaker's information.\n\nPost hyper-parameter search, best parameters in a 5-fold cross validation is selected across 28 different experimental configurations: 7 modality combinations (rows of Table  9 ) each run with 4 settings (columns of Table  9 ). Table  11  shows results of valence and arousal predictions across different modalities Here also we observed that in a speaker-independent setting the model performs better, but as the length of the speaker vector increases with more people, the effect of speaker information confuses the models. We also observe that contextual information doesn't affect valence arousal prediction as that of the actual utterance, which is intuitive.\n\nIn order to prove the performance improvement due to correcting labels, we ran a Wilcoxon signed rank test  (Wilcoxon, 1945)  on old versus new labeled data. We made 10 runs with the best trained multimodal model on old as well as new labels on the sarcastic MUStARD data for proper comparison. The mean F1 score of emotion recognition on the sarcastic set with old labels is 20.64±1.15 while with new labels on same set the mean is 39.5 ± 1.00 which is statistically significant with a p-value of 0.002. We observed reduction in label confusion in the confusion matrix. Thus these re-labeling",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This paper presents a multimodal sarcasm dataset that can be used by researchers in the area of sarcasm detection and emotion recognition. We start with the version of MUStARD data provided by  (Chauhan et al., 2020)  and correct several emotion labels, while appending the sarcasm type and arousal-valence labels as additional metadata that is useful for both the research avenues. We doubled the number of sarcastic videos, finding which is very challenging, thus making it a beneficial contribution to the research community. We also added equal number of non-sarcastic videos with their context along with similar metadata annotations. To the best of our knowledge this is the first work on emotion recognition in sarcasm and towards that we present a curated dataset which is benchmarked using several pretrained feature extractors and multimodal fusion techniques in different setups. Sarcasm type information enables a multimodal system to choose the right modality combination for a given utterance, thereby optimizing performance of the sarcasm detector and the emotion recognizer for different utterances. In this work, we have used arousal only to understand the degree/intensity of an emotion, for example, annoyance to anger to rage. In future we want to use arousal and valence to investigate its effect on emotion classification. While we currently explore the use of sarcasm label in emotion recognition, an interesting research direction would be using emotion labels to improve sarcasm detection.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ethical Considerations In Data Curation",
      "text": "This paper does not claim to find the exact intended emotion of the speaker that led to a sarcastic sentence. Rather we try to predict the perceived emotion. Our annotators annotate on the recorded video, thus observing the perceived emotion, arousal and valence. This is very important for conversational systems where the bot needs to understand the emotion, valence and intensity to be able to respond better. Also, this is in accordance to the suggestions in the ethics sheet for automated emotion recognition  (Mohammad, 2021)  where authors explain that given the complexity of human emotion, it is very difficult to predict the exact emotional state of the speaker. The authors hereby acknowledge that there could be a possibility of bias in the final emotion label assigned since the label is chosen based on majority voting. In order to minimize the effect of bias, we collect videos from a diverse set of sources and ask seven annotators of different age, gender, and educational background to label their perceived emotions. We have considered the guidelines  (Mohammad, 2021)  for responsible development and use of Automated Emo-tion Recognition systems (AER) and adhered to them in our research statement, data collection, annotation protocol, and during the benchmarking experiments. From the valence-arousal ratings of the dataset, it was observed that all sarcastic utterances received lower valence values, inclined towards the unpleasant end of the spectrum. Non-sarcastic utterances, however, have a more diverse set of valence values with ratings from both pleasant and unpleasant halves of the spectrum.\n\nWhile arousal values in sarcastic utterances have majority ratings as 7-8, in non-sarcastic utterances the distribution is more diverse in comparison with sarcastic utterances and they have majority ratings fall into the range of 5-7. This shows the implied negativity and intensity that sarcasm usually tends to portray.\n\nIn each of the 5 TV shows which form the source of our dataset, different characters contribute to the sarcastic instances with different emotions. To understand the number of speakers in each show and their contribution to various implicit emotions in the sarcastic subset of our data, Figure  3  provides the distribution of implicit emotions in the utterances of each character from the shows.\n\nFor sarcastic utterances, the dataset contains sarcasm type metadata which can help provide deeper insights into the task of sarcasm detection. To also understand how these sarcasm types co-occur with different implicit emotions, Table  13",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Additional Results",
      "text": "As part of the bench-marking exercise, as mentioned in Section 6, we perform Majority sampling (assigns the emotion class with majority examples as all samples), Random Sampling (predictions are sampled equally throughout the test set using this baseline). We also perform one-vs-rest experiments for each emotion which contain a sigmoid layer instead of a softmax layer as classification head. The results of these baselines are reported in the table 14. In order to study the advantage of our model, we presented a comparison of sarcasm detection results on original MUStARD dataset using our proposed model against the numbers reported by the authors of  (Castro et al., 2019; Chauhan et al., 2020)",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: is a sample in MUStARD++ with the labels",
      "page": 2
    },
    {
      "caption": "Figure 1: Example to show that different explicit and",
      "page": 2
    },
    {
      "caption": "Figure 2: Distribution of Emotion over Sarcasm Type",
      "page": 5
    },
    {
      "caption": "Figure 3: provides the distribution of implicit",
      "page": 10
    },
    {
      "caption": "Figure 3: Character-label ratio per source over Implicit Emotion in sarcastic utterances on MUStARD++ data",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker": "",
          "Utterance": "",
          "(Chauhan et al., 2020)": "Explicit",
          "New": "Explicit"
        },
        {
          "Speaker": "",
          "Utterance": "The\nbackwash\ninto\nthis\nglass\nis\nevery\npathogen that calls your mouth home, sweet\nhome. Not to mention the visitors who ar-\nrive on the dancing tongue of your subtrop-\nical girlfriend.",
          "(Chauhan et al., 2020)": "Neu",
          "New": "Exi"
        },
        {
          "Speaker": "",
          "Utterance": "Hey!\nThat’s my sister and my country\nyou’re talking about.\nLeonard may have\ndeﬁled one, but I won’t have you talking\nsmack about the other.",
          "(Chauhan et al., 2020)": "",
          "New": ""
        },
        {
          "Speaker": "",
          "Utterance": "You guys ready to order?",
          "(Chauhan et al., 2020)": "",
          "New": ""
        },
        {
          "Speaker": "",
          "Utterance": "Yes,\nI’d like a seven-day course of peni-\ncillin, some, uh, syrup of ipecac– to induce\nvomiting– and a mint.",
          "(Chauhan et al., 2020)": "",
          "New": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Non-Sarcastic\nSarcastic\nExplicit\nImplicit\nExplicit\nImplicit": "Anger\n28\nExcitement\n15\nFear\n6\nSad\n62\nSurprise\n20\nFrustrated\n13\nHappy\n92\nNeutral\n115\nDisgust\n6\nRidicule*\n-",
          "OLD OUR": "68\n0\n0\n18\n0\n88\n0\n0\n81\n89"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sr. No": "1",
          "Speaker": "",
          "Utterance": "I think Howard hurting my feelings has in some\nways made me a better person.",
          "E": "Sur",
          "I": "Rid",
          "TYPE": "EMB"
        },
        {
          "Sr. No": "",
          "Speaker": "",
          "Utterance": "Hmm. Look at you, improving on perfection. How\nso?",
          "E": "",
          "I": "",
          "TYPE": ""
        },
        {
          "Sr. No": "2",
          "Speaker": "",
          "Utterance": "Wow. That’s a lot of luggage for a weekend.",
          "E": "Neu",
          "I": "Rid",
          "TYPE": "PRO"
        },
        {
          "Sr. No": "",
          "Speaker": "",
          "Utterance": "(groans) I know. I didn’t know what to wear, so I\nbrought a few options.",
          "E": "",
          "I": "",
          "TYPE": ""
        },
        {
          "Sr. No": "",
          "Speaker": "",
          "Utterance": "Was one of the options the option to never come\nback?",
          "E": "",
          "I": "",
          "TYPE": ""
        },
        {
          "Sr. No": "3",
          "Speaker": "",
          "Utterance": "So,\nare you gonna give us a clue where we’re\nheaded?",
          "E": "Neu",
          "I": "Rid",
          "TYPE": "PRO"
        },
        {
          "Sr. No": "",
          "Speaker": "",
          "Utterance": "Uh, okay,\nlet’s see... They’ve got spicy food and\nthere’s a chance you’ll get diarrhea.",
          "E": "",
          "I": "",
          "TYPE": ""
        },
        {
          "Sr. No": "",
          "Speaker": "",
          "Utterance": "India",
          "E": "",
          "I": "",
          "TYPE": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker": "",
          "Utterance": "",
          "(Chauhan et al., 2020)": "Explicit",
          "New": "Explicit"
        },
        {
          "Speaker": "",
          "Utterance": "My name’s Scott and I am a sarcasmaholic.",
          "(Chauhan et al., 2020)": "Sur",
          "New": "Sur"
        },
        {
          "Speaker": "",
          "Utterance": "Nooo.\nWe thought you were just here for the com-\npany. He is a sarcasmoholic Stewart.",
          "(Chauhan et al., 2020)": "",
          "New": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 9: ) each run with 4 settings (columns of Ta-",
      "data": [
        {
          "Methods": "",
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Methods": "(Castro et al., 2019)\n(Chauhan et al., 2020)\nProposed MUStARD*",
          "Speaker Independent": "64.7\n62.9\n63.1\n69.53\n66.0\n65.9\n72.1\n72\n72",
          "Speaker Dependent": "72.1\n71.7\n71.8\n73.40\n72.75\n72.57\n74.2\n74.2\n74.2"
        },
        {
          "Methods": "%∆MUStARD",
          "Speaker Independent": "↑ 3.69%\n↑ 9.09%\n↑ 9.25%",
          "Speaker Dependent": "↑ 1.08%\n↑ 1.99%\n↑ 2.24%"
        },
        {
          "Methods": "Proposed MUStARD++",
          "Speaker Independent": "70.2\n70.2\n70.2",
          "Speaker Dependent": "70.3\n70.3\n70.3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 9: ) each run with 4 settings (columns of Ta-",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "67.9\n67.7\n67.7\n63.9\n63.5\n63.6\n59.5\n59.4\n59.4\n68.8\n68.6\n68.7\n65.7\n65.4\n65.5\n68.2\n68.1\n68.1\n69.5\n69.4\n69.4",
          "Speaker Dependent": "69.4\n69.3\n69.3\n65.3\n65.2\n65.2\n61.8\n61.7\n61.7\n69.8\n69.5\n69.5\n64.9\n64.5\n64.5\n69.1\n69.0\n69.0\n69.6\n69.3\n69.3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "33±0.9\n33.6±1\n33.3±0.9\n26.7±1.1\n27.1±1.4\n26.8±1.1\n28.8±0.9\n29.4±1.3\n29±1.1\n31.5±1.7\n31.6±1.8\n31.6±1.7\n25.9±1.9\n26.3±2\n26.1±1.9\n31.9±0.8\n32.5±0.7\n32.2±0.7\n31.2±1\n31.6±0.1\n31.4±1",
          "Speaker Dependent": "29.9±0.9\n30.3±0.8\n30.1±0.8\n24.3±0.8\n24.7±0.6\n24.5±0.7\n30.3±1.4\n31.4±1.2\n30.6±1.4\n29.1±1.6\n29.2±1.5\n29.1±1.5\n29.7±0.6\n30.6±0.9\n30.1±0.7\n31.1±0.8\n31.2±0.7\n31.1±0.7\n30.9±0.3\n30.5±0.4\n30.7±0.3"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "38.5±0.8\n39.2±0.9\n38.8±0.8\n26.4±0.9\n28±1.4\n27.1±1\n25.1±0.6\n25.9±0.6\n25.5±0.6\n38.9±1.1\n39.5±1.3\n39.2±1.2\n26.4±1.4\n26.5±1.5\n26.4±1.4\n38.6±0.7\n39.2±0.8\n38.8±0.8\n37.8±0.1\n38.3±0.8\n38.0±0.9",
          "Speaker Dependent": "38.7±0.5\n39.3±0.5\n39±0.5\n28.1±1.1\n29.3±1.0\n28.6±1.1\n25.7±1.4\n36.1±0.8\n27.7±0.8\n39.1±0.8\n39.7±0.7\n39.4±0.7\n27.6±1\n28.2±1.2\n27.9±1.1\n39.8±0.1\n40±0.2\n39.8±0.2\n40±0.6\n39.8±0.5\n39.9±0.9"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence Prediction": "Speaker Independent",
          "Arousal Prediction": "Speaker Independent"
        },
        {
          "Valence Prediction": "w/o Context",
          "Arousal Prediction": "w/o Context"
        },
        {
          "Valence Prediction": "MAE\nRMSE",
          "Arousal Prediction": "MAE\nRMSE"
        },
        {
          "Valence Prediction": "0.91\n0.74\n0.82\n0.65\n0.85\n0.68\n0.83\n0.67\n0.83\n0.66\n0.82\n0.66\n0.78\n0.62",
          "Arousal Prediction": "1.42\n1.15\n1.24\n1.00\n1.22\n0.96\n1.18\n0.96\n1.19\n0.96\n1.22\n0.97\n1.20\n0.95"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 13: Implicit Emotion Distribution per Sarcasm",
      "data": [
        {
          "An": "47\n26\n13\n1",
          "Sa": "15\n4\n5\n0",
          "Fr": "61\n48\n20\n1",
          "Ri": "129\n70\n27\n0",
          "Di": "81\n30\n22\n1",
          "Total\n333\n178\n87\n3": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 13: Implicit Emotion Distribution per Sarcasm",
      "data": [
        {
          "Implicit": "P\nR\nF1",
          "Explicit": "P\nR\nF1"
        },
        {
          "Implicit": "14.1\n37.6\n20.6\n25.3\n20.3\n21.4\n25.9\n25.9\n25.9\n35.6\n37.6\n36.5\n34.4\n36.1\n34.9",
          "Explicit": "17.0\n41.3\n24.1\n27.9\n11.8\n15.5\n26.1\n26.1\n26.1\n41.2\n43.6\n42.0\n41.7\n44.0\n42.6"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "68.8\n68.8\n68.8\n65.3\n65.2\n65.3\n68.7\n68.4\n68.4\n70.2\n70.2\n70.2\n71.1\n71.1\n71.1\n69.9\n69.8\n69.8\n72.1\n72.0\n72.0",
          "Speaker Dependent": "74.3\n74.2\n74.2\n72.4\n72.3\n72.3\n70.5\n70.5\n70.5\n72.5\n72.5\n72.5\n71.8\n71.6\n71.5\n73.2\n73.1\n73.1\n73.2\n73.1\n73.1"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "34.1\n35.4\n34.6\n29.4\n30.3\n29.7\n31.7\n34.6\n32.9\n34.7\n37.9\n35.8\n29.8\n31.6\n30.5\n34.0\n36.1\n34.9\n32.5\n37.3\n32.9",
          "Speaker Dependent": "33.6\n34.4\n33.8\n30.7\n31.1\n30.6\n32.2\n33.6\n32.5\n33.9\n34.4\n34.0\n31.8\n33.4\n32.5\n34.5\n35.4\n34.8\n32.5\n32.6\n32.4"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "39.8\n42.8\n41.0\n28.3\n31.8\n29.3\n26.6\n30.6\n28.2\n41.1\n43.9\n42.2\n29.6\n31.9\n30.4\n41.2\n43.6\n42.1\n38.6\n41.6\n39.7",
          "Speaker Dependent": "37.6\n41.6\n38.5\n29.8\n33.4\n31.3\n29.3\n33.6\n30.9\n38.1\n40.6\n39.1\n30.1\n34.3\n31.2\n38.8\n40.4\n39.5\n36.3\n38.8\n37.3"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "22.6\n23.6\n23.0\n21.1\n22.3\n21.5\n18.5\n19.4\n18.9\n25.5\n26.9\n26.0\n19.6\n20.9\n20.1\n24.1\n24.9\n24.4\n23.4\n24.7\n23.8",
          "Speaker Dependent": "25.7\n25.9\n25.1\n20.7\n22.0\n21.2\n19.9\n21.4\n20.5\n25.6\n27.1\n26.1\n20.6\n21.9\n21.2\n23.0\n24.4\n23.5\n25.6\n26.9\n25.9"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Speaker Independent": "w/o Context",
          "Speaker Dependent": "w/o Context"
        },
        {
          "Speaker Independent": "P\nR\nF1",
          "Speaker Dependent": "P\nR\nF1"
        },
        {
          "Speaker Independent": "34.8\n37.5\n35.8\n26.2\n30.0\n27.6\n23.9\n27.0\n25.3\n34.2\n37.9\n35.5\n25.8\n29.2\n27.0\n34.2\n37.2\n35.4\n34.7\n36.4\n35.4",
          "Speaker Dependent": "35.5\n36.7\n35.7\n24.4\n28.6\n26.1\n24.3\n28.4\n26.1\n34.2\n37.8\n35.7\n25.6\n29.7\n27.2\n33.3\n36.8\n34.7\n34.7\n37.1\n35.7"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-modal sarcasm detection and humor classification in code-mixed conversations",
      "authors": [
        "M Bedi",
        "S Kumar",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "Multi-modal sarcasm detection and humor classification in code-mixed conversations"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Multi-modal sarcasm detection in Twitter with hierarchical fusion model",
      "authors": [
        "Y Cai",
        "H Cai",
        "X Wan"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "Sarcasm, pretense, and the semantics/pragmatics distinction",
      "authors": [
        "E Camp"
      ],
      "year": "2012",
      "venue": "Noûs"
    },
    {
      "citation_id": "5",
      "title": "Emotionlines: An emotion corpus of multiparty conversations",
      "authors": [
        "S Chen",
        "C Hsu",
        "C Kuo",
        "T Huang",
        "L Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multiparty conversations"
    },
    {
      "citation_id": "6",
      "title": "Describing the emotional states that are expressed in speech",
      "authors": [
        "R Cowie",
        "R Cornelius"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "7",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "8",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "10",
      "title": "Video summarization using keyframe extraction and video skimming",
      "authors": [
        "S Jadon",
        "M Jasim"
      ],
      "year": "2019",
      "venue": "Video summarization using keyframe extraction and video skimming",
      "arxiv": "arXiv:1910.04792"
    },
    {
      "citation_id": "11",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2016",
      "venue": "Automatic sarcasm detection: A survey"
    },
    {
      "citation_id": "12",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2017",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "13",
      "title": "Investigations in Computational Sarcasm",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2018",
      "venue": "Investigations in Computational Sarcasm"
    },
    {
      "citation_id": "14",
      "title": "A large self-annotated corpus for sarcasm",
      "authors": [
        "M Khodak",
        "N Saunshi",
        "K Vodrahalli"
      ],
      "year": "2017",
      "venue": "A large self-annotated corpus for sarcasm"
    },
    {
      "citation_id": "15",
      "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2019",
      "venue": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension"
    },
    {
      "citation_id": "16",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "17",
      "title": "Use what you have: Video retrieval using representations from collaborative experts",
      "authors": [
        "Y Liu",
        "S Albanie",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "Use what you have: Video retrieval using representations from collaborative experts"
    },
    {
      "citation_id": "18",
      "title": "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis",
      "authors": [
        "D Maynard",
        "M Greenwood"
      ],
      "year": "2014",
      "venue": "LREC 2014 Proceedings"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "B Mcfee",
        "A Metsai",
        "M Mcvicar",
        "S Balke",
        "C Thomé",
        "C Raffel",
        "F Zalkow",
        "A Malek",
        "Dana",
        "K Lee",
        "O Nieto",
        "D Ellis",
        "J Mason",
        "E Battenberg",
        "S Seyfarth",
        "R Yamamoto",
        "K Choi",
        "J Moore",
        "R Bittner",
        "S Hidaka",
        "Z Wei",
        "A Weiss",
        "D Hereñú",
        "F.-R Stöter",
        "P Friesch",
        "M Vollrath",
        "T Kim"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "20",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "S Mohammad"
      ],
      "year": "2021",
      "venue": "Ethics sheet for automatic emotion recognition and sentiment analysis"
    },
    {
      "citation_id": "21",
      "title": "iSarcasm: A dataset of intended sarcasm",
      "authors": [
        "S Oprea",
        "W Magdy"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Creating and characterizing a diverse corpus of sarcasm in dialogue",
      "authors": [
        "S Oraby",
        "V Harrison",
        "L Reed",
        "E Hernandez",
        "E Riloff",
        "M Walker"
      ],
      "year": "2016",
      "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "23",
      "title": "Learning Problem-Agnostic Speech Representations from Multiple Self-Supervised Tasks",
      "authors": [
        "S Pascual",
        "M Ravanelli",
        "J Serrà",
        "A Bonafonte",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "24",
      "title": "Emotion: a psychoevolutionary synthesis",
      "authors": [
        "R Plutchick"
      ],
      "year": "1980",
      "venue": "Emotion: a psychoevolutionary synthesis"
    },
    {
      "citation_id": "25",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea",
        "D Schwartz",
        "H Park",
        "G Eichstaedt",
        "J Kern",
        "M Ungar",
        "L Shulman"
      ],
      "year": "2016",
      "venue": "Proceedings of the 7th workshop on computational approaches to subjectivity, sentiment and social media analysis"
    },
    {
      "citation_id": "26",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Exploring the limits of transfer learning with a unified text-to-text transformer"
    },
    {
      "citation_id": "27",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "28",
      "title": "I didn't mean what i wrote! exploring multimodality for sarcasm detection",
      "authors": [
        "S Sangwan",
        "M Akhtar",
        "P Behera",
        "A Ekbal"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "29",
      "title": "Detecting sarcasm in multimodal social platforms",
      "authors": [
        "R Schifanella",
        "P De Juan",
        "J Tetreault",
        "L Cao"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM international conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Individual comparisons by ranking methods",
      "authors": [
        "F Wilcoxon"
      ],
      "year": "1945",
      "venue": "Biometrics Bulletin"
    },
    {
      "citation_id": "31",
      "title": "Aff-wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "1980",
      "venue": "Aff-wild: Valence and arousal 'in-the-wild' challenge"
    },
    {
      "citation_id": "32",
      "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2017",
      "venue": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks"
    },
    {
      "citation_id": "33",
      "title": "A gricean rearrangement of epithets",
      "authors": [
        "Z Zvolenszky"
      ],
      "year": "2012",
      "venue": "A gricean rearrangement of epithets"
    },
    {
      "citation_id": "34",
      "title": "Towards Multimodal Sarcasm Detection",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "Pérez-Rosas",
        "Verónica",
        "Roger Zimmermann",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Towards Multimodal Sarcasm Detection"
    },
    {
      "citation_id": "35",
      "title": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis",
      "authors": [
        "Dushyant Chauhan",
        "Singh",
        "Dhanush",
        "Asif Ekbal",
        "Pushpak Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis"
    }
  ]
}