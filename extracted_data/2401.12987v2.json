{
  "paper_id": "2401.12987v2",
  "title": "Telme: Teacher-Leading Multimodal Fusion Network For Emotion Recognition In Conversation",
  "published": "2024-01-16T07:18:41Z",
  "authors": [
    "Taeyang Yun",
    "Hyunkuk Lim",
    "Jeonghwan Lee",
    "Min Song"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) plays a crucial role in enabling dialogue systems to effectively respond to user requests. The emotions in a conversation can be identified by the representations from various modalities, such as audio, visual, and text. However, due to the weak contribution of non-verbal modalities to recognize emotions, multimodal ERC has always been considered a challenging task. In this paper, we propose Teacher-leading Multimodal fusion network for ERC (TelME). TelME 1 incorporates cross-modal knowledge distillation to transfer information from a language model acting as the teacher to the nonverbal students, thereby optimizing the efficacy of the weak modalities. We then combine multimodal features using a shifting fusion approach in which student networks support the teacher. TelME achieves state-of-the-art performance in MELD, a multi-speaker conversation dataset for ERC. Finally, we demonstrate the effectiveness of our components through additional experiments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition holds paramount importance, enhancing the engagement of conversations by providing appropriate responses to the emotions of users in dialogue systems  (Ma et al., 2020) . The application of emotion recognition spans various domains, including chatbots, healthcare systems, and recommendation systems, demonstrating its versatility and potential to enhance a wide range of applications  (Poria et al., 2019) . Emotion Recognition in Conversation (ERC) aims to identify emotions expressed by participants at each turn within a conversation. The dynamic emotions in a conversation can be detected through multiple modalities such as textual utterances, facial expressions, and acoustic signals  (Baltrušaitis et al., 2018; Liang et al. , 2022;  Majumder et al., 2019; Hu et al., 2022b; Chudasama et al., 2022) . Figure  1  illustrates an example of a multimodal ERC.\n\nMuch research on ERC has mainly focused on context modeling from text modality, disregarding the rich representations that can be obtained from audio and visual modalities. Text-based ERC methods have demonstrated that contextual information derived from text data is a powerful resource for emotion recognition  (Kim and Vossen, 2021; Lee and Lee, 2021; Song et al., 2022a,b) . However, non-verbal cues such as facial expressions and tone of voice, which are not covered by text-based methods, provide important information that needs to be explored in the field of ERC. Multimodal approaches demonstrate the possibility of integrating features from three modalities to improve the robustness of ERC systems  (Mao et al., 2021; Chudasama et al., 2022; Hu et al., 2022b) . Nevertheless, these frameworks frequently ignore the varying degrees of impact the individual modalities have on emotion recognition and instead treat them as homogeneous components. This implies a promising opportunity to improve the ERC system by differentiating the level of contribution made by each modality.\n\nIn this paper, we propose Teacher-leading Multimodal fusion network for the ERC task (TelME) that strengthens and fuses multimodal information by accentuating the powerful modality while bolstering the weak modalities. Knowledge Distillation (KD) can be extended to transfer knowledge across modalities, where a powerful modality can play the role of a teacher to share knowledge with a weak modality  (Hinton et al., 2015; Xue et al., 2022) . While Figure  2  shows the robustness of text in ERC tasks compared to the other two modalities, the other modalities present valuable information nonetheless. Thus, TelME enhances the representations of the two weak modalities through KD utilizing the text encoder as the teacher. Our approach aims to mitigate heterogeneity between modalities while allowing students to learn the preferences of the teacher. TelME then incorporates Attentionbased modality Shifting Fusion, where the student networks strengthened by the teacher at the distillation stage assist the robust teacher encoder in reverse, providing details that may not be present in the text. Specifically, our fusion method creates displacement vectors from non-verbal modalities, which are used to shift the emotion embeddings of the teacher.\n\nWe conduct experiments on two widely used benchmark datasets and compare our proposed method with existing ERC methods. Our results show that TelME performs well on both datasets and particularly excels in multi-party conversations, achieving state-of-the-art performance. The ablation study also demonstrates the effectiveness of our knowledge distillation strategy and its interaction with our fusion method.\n\nOur contributions can be summarized as follows:\n\n• We propose Teacher-leading Multimodal fusion network for Emotion Recognition in  Conversation (TelME) . The proposed method considers different contributions of text and nonverbal modalities to emotion recognition for better prediction.\n\n• To the best of our knowledge, we are the first to enhance the effectiveness of weak nonverbal modalities for the ERC task through cross-modal distillation.\n\n• TelME shows comparable performance in two widely used benchmark datasets and especially achieves state-of-the-art in multi-party conversational scenarios. Recently, ERC has gained considerable attention in the field of emotion analysis. ERC can be categorized into text-based and multimodal methods, depending on the input format. Text-based methods primarily focus on context modeling and speaker relationships  (Jiao et al., 2019; Li et al., 2020; Hu et al., 2021a) . In recent studies  (Lee and Lee, 2021; Song et al., 2022a) , context modeling has been carried out to enhance the understanding of contextual information by pre-trained language models using dialogue-level input compositions. Additionally, there are graph-based approaches  (Zhang et al., 2019; Ishiwatari et al., 2020; Shen et al., 2021; Ghosal et al., 2019)  and approaches that utilize external knowledge  (Zhong et al., 2019; Ghosal et al., 2020; Zhu et al., 2021) . On the contrary, multimodal methods  (Poria et al., 2017; Hazarika et al., 2018a,b; Majumder et al., 2019 ) reflect dialogue-level multimodal features through recurrent neural network-based models. Other multimodal approaches  (Mao et al., 2021; Chudasama et al., 2022)  integrate and manipulate utterance-level features through hierarchical structures to extract dialogue-level features from each modality. EmoCaps  (Li et al., 2022)  considers both multimodal information and contextual emotional tendencies to predict emotions. UniMSE  (Hu et al., 2022b)  proposes a framework that leverages complementary information between Multimodal Sentiment Analysis and ERC. Unlike these methods, our proposed TelME is one in which the strong teacher leads emotion recognition while simultaneously bolstering attributes from weaker modalities",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "The initial proposition of KD  (Hinton et al., 2015)  involves transferring knowledge by reducing the KL divergence between the prediction logits of teachers and students, demonstrating its effectiveness through improved performance of the student models. Subsequently, KD has been extended to distillation between intermediate features  (Heo et al., 2019) . Furthermore, KD approaches  (Gupta et al., 2016; Jin et al., 2021; Tran et al., 2022)  have also been shown to transfer knowledge between modalities effectively in multimodal studies.  Li et al. (2023b)  mitigate multimodal heterogeneity by constructing dynamic graphs in which each vertex exhibits modality and each edge exhibits dynamic KD. However, since this work is not a study of ERC and is based on graph distillation, there is an intrinsic difference from our KD strategy.  Ma et al. (2023)  proposes a transformer-based model utilizing self-distillation for ERC. Our proposed method, in contrast, uses response and feature-based distillation simultaneously to maximize the effectiveness of two other modalities by the teacher network based on text modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Statement",
      "text": "Given a set of conversation participants S, utterances U , and emotion labels Y , a conversation consisting of k utterances is represented as [(s i , u 1 , y 1 ), (s j , u 2 , y 2 , ..., (s i , u k , y k )], where s i , s j ∈ S are the conversation participants. If i = j, then s i and s j refer to the same speaker. y k ∈ Y is the emotion of the k-th utterance in a conversation, which belongs to one of the predefined emotion categories. Additionally, u k ∈ U is the k-th utterance. u k is provided in the format of a video clip, speech segment, and text transcript. i.e., u k = {t k , a k , v k }, where {t, a, v} denotes a text transcript, speech segment, and a video clip. The objective of ERC is to predict y k , the emotion corresponding to the k-th utterance in a conversation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Telme",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Overview",
      "text": "We propose Teacher-leading Multimodal fusion network for ERC (TelME), as illustrated in Figure 3. This framework is devised based on the hypothesis that by exploiting the varying levels of modality-specific contributions to emotion recognition, there is a potential to enhance the overall performance of an ERC system. Therefore, we introduce a strategic approach that focuses on accentuating the powerful modality while bolstering the weak modalities. We first extract powerful emotional representations through context modeling from text modality while capturing auditory and visual features of the current speaker from non-verbal modalities. However, due to the limited emotional recognition capability of audio and visual features as well as the heterogeneity between the modalities, effective multimodal interactions cannot be guaranteed  (Zheng et al., 2022) . We thus mitigate the heterogeneity between modalities while maxi-mizing the effectiveness of non-verbal modalities by distilling emotion-relevant knowledge of the teacher model into non-verbal students. We also use a fusion method in which strong emotional features from the teacher encoder are shifted by referring to representations of students strengthened in reverse. In the subsequent sections, we discuss the three components of TelME: Feature Extraction, Knowledge Distillation, and Attention-based modality Shifting Fusion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extraction",
      "text": "Figure  3  visually illustrates how each modality encoder receives its corresponding input to extract emotional features. In this section, we explain the methodologies employed to generate emotional features corresponding to each modality's input signals.\n\nText: Following previous research  (Lee and Lee, 2021; Song et al., 2022a) , we conduct context modeling, considering all utterances from the inception of the conversation up to the k-th turn as the context. To handle speaker dependencies and differentiate between speakers, we represent speakers using the special token, < s i >. Additionally, we construct the prompt, \"Now < s i > feels < mask >\" to emphasize the emotion of the most recent speaker. We report the effect of the prompt in Appendix A.1. The emotional features are derived from the embedding of the special token, < mask >. For our text encoder, we employ the modified Roberta  (Liu et al., 2019) , which has exhibited its efficacy across various natural language processing tasks. We can extract emotional features from the text encoder as follows.\n\nwhere < s i > is the special token indicating the speaker and < /s > is the separation token of Roberta. F T k ∈ R 1×d is the embeddings of the mask token, < mask > and d is the dimension of the encoder.\n\nAudio: Self-supervised learning using Transformer has witnessed remarkable achievement, not only within the field of natural language processing but also in the realms of audio and video  (Bertasius et al., 2021; Baevski et al., 2022) . In line with this trend, we set the initial state of our audio encoder with data2vec  (Baevski et al., 2022) . To focus solely on the voice of the current speaker, we only utilize a speech segment of the k-th utterance, denoted as a k . This speech segment is processed according to the pre-trained processor. The audio encoder then extracts emotional features from the processed input as follows.\n\nwhere F a k ∈ R 1×d is the embeddings of a k and d is the dimension of the encoder.\n\nVisual: Following the same reasoning as the audio modality, we configure the initial state of our visual encoder using Timesformer  (Bertasius et al., 2021) . In order to concentrate exclusively on the facial expressions of the current speaker, we solely utilize a video clip of the k-th utterance, denoted as v k . We extract the frames corresponding to the k-th utterance from the video and construct v k through image processing. The visual encoder then extracts emotional features from the processed input as follows.\n\nwhere\n\nis the embedding of v k and d is the dimension of the encoder.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "Addressing the challenge of heterogeneity between modalities and low emotional recognition contributions of non-verbal modalities holds great potential in facilitating satisfactory multimodal interactions  (Zheng et al., 2022) . Thus, we distill strong emotion-related knowledge of a language model that understands linguistic contexts, thereby augmenting the emotional features extracted from the other two modalities with comparatively lower contributions. We employ two distinct types of knowledge distillation concurrently: response and feature-based distillation. The overall loss for the student can be composed of the classification loss, response-based distillation loss, and feature-based distillation loss, i.e.,\n\nwhere α and β are the factors for balancing the losses.\n\nL response utilizes DIST  (Huang et al., 2022) , a technique originally used in image networks, as a cross-modal distillation for ERC. As shown in Figure  2 , effective knowledge distillation can be challenging due to the significant gap between the text modality and the other two modalities. Therefore, unlike conventional KD methods, we use a KD approach(L response ) that utilizes Pearson correlation coefficients instead of KL divergence as follows.\n\nwhere ρ(µ, υ) is the Pearson correlation coefficient between two probability vectors µ and υ. Specifically, L response aims to distill preferences (relative rankings of predictions) by teachers through the correlations between teacher and student predictions, which can usefully perform knowledge distillation even in the extreme differences between teacher and student. We gather the predicted probability distributions for all instances within a batch and calculate the Pearson correlation coefficient between the teacher and student for inter-class and intra-class relations (Figure  3 ). Subsequently, we transfer the inter-class and intra-class relation to the student. The specific formulation of the response-based distillation can be described as follows.\n\nY s i,: = sof tmax(Z s i,: /τ ) (9)\n\nGiven a training batch B and the emotion categories C, Z s ∈ R B×C is the prediction matrix of the student and Z t ∈ R B×C is the prediction matrix of the teacher. τ > 0 is a temperature parameter to control the softness of logits. However, rather than relying solely on L response , we introduce L f eature as an additional distillation loss to better leverage the embedded information in the teacher network. L f eature aims to mitigate the heterogeneity between the representations of the teacher and student models, allowing us to distill richer knowledge from the teacher compared to using only L response . Through this, the features of the   3 ). We construct the target similarity matrix by performing a dot product between the representation matrix of the teacher and its transposition matrix. By applying the softmax function to this matrix, we derive the target probability distribution as follows.\n\nwhere B is a training batch and M ∈ R B×B is the target similarity matrix. τ > 0 is a temperature parameter controlling the smoothness of the distribution. P i is the target probability distribution.\n\nSimilarly, we can compute the similarity matrix between the teacher and the student by taking the dot product of their representations. Subsequently, we can calculate the similarity probability distribution as follows.\n\nwhere M ′ ∈ R B×B is the similarity matrix of student and teacher. Q i is the similarity probability distribution of teacher and student. With these two probability distributions, we compute the KL divergence as the loss for the featurebased distillation.\n\nwhere KL is the Kullback-Leibler divergence.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Attention-Based Modality Shifting Fusion",
      "text": "The emotional features from the enhanced student networks can impact the teacher model's emotionrelevant representations, providing information that may not be captured from the text. To fully utilize these features, we adopt a multimodal fusion approach where feature vectors from the student models manipulate the representation vectors from the teacher, effectively incorporating non-verbal information into the representation vector. To highlight non-verbal characteristics, we concatenate the vectors of the student models and perform multi-head self-attention. The vectors of non-verbal information generated through the multi-head self-attention process and emotional features of the teacher encoder enter the input of the shifting step (Figure  4 ). We are inspired by  Rahman et al. (2020)  to construct the shifting step. In the shifting step, a gating vector is generated by concatenating and transforming the vector of the teacher model and the vector of the non-verbal information.\n\nwhere <,> is the operation of vector concatenation, R(x) is a non-linear activation function, W 1 is the weight matrix for linear transform, and b 1 is scalar bias. F attention is the emotional representation vectors of non-verbal information. g AV is the gating vector. The gating vector highlights the relevant information in the non-verbal vector according to the representations of the teacher model. We define the displacement vector by applying the gating vector as follows.\n\nwhere W 2 is the weight matrix for linear transform and b 2 is scalar bias. H is the non-verbal information-based displacement vector. We subsequently utilize the weighted sum between the representation vector of the teacher and the displacement vector to generate a multimodal vector. Finally, we predict emotions using the multimodal vector.\n\nwhere Z is the multimodal vector. We apply the scaling factor λ to control the magnitude of the displacement vector and θ as a threshold hyperparameter. ∥F k ∥ 2 , ∥H k ∥ 2 denote the L2 norm of the F k and H k vectors respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate our proposed network on MELD  (Poria et al., 2018)  and IEMOCAP  (Busso et al., 2008)  following other works on ERC listed in Appendix A.3. The statistics are shown in Table  1 .\n\nMELD is a multi-party dataset comprising over 1400 dialogues and over 13,000 utterances extracted from the TV series Friends. This dataset contains seven emotion categories for each utterance: neutral, surprise, fear, sadness, joy, disgust, and anger.\n\nIEMOCAP consists of 7433 utterances and 151 dialogues in 5 sessions, each involving two speakers per session. Each utterance is labeled as one of six emotional categories: happy, sad, angry, excited, frustrated and neutral. The train and development datasets consist of the first four sessions randomly divided at a 9:1 ratio. The test dataset consists of the last session.\n\nWe purposely exclude CMU-MOSEI (Zadeh et al., 2018), a well-known multimodal sentiment analysis dataset, as it comprises single-speaker videos and is not suitable for ERC, where emotions dynamically change within each conversation turn.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiment Settings",
      "text": "We evaluate all experiments using the weighted average F1 score on two class-imbalanced datasets. We use the initial weight of the pre-trained models from Huggingface's Transformers  (Wolf et al., 2019) . The output dimension of all encoders is unified to 768. The optimizer is AdamW and the initial learning rate is 1e-5. We use a linear schedule with As shown in Table  2 , we report the performance of various methods for emotion labels in MELD. TelME outperforms other models in all emotions except Surprise and Anger. However, assuming that Surprise and Fear, as well as Disgust and Anger, are similar emotions, Emocaps shows a bias towards Surprise and Anger during inference, only achieving 3.03% and 7.69% in F1 score for Fear and Disgust, respectively. On the other hand, TelME distinguishes these similar emotions better, bringing the scores for Fear and Disgust up to 26.97% and 26.42%. We speculate that our framework predicts minority emotions more accurately as the non-verbal modality information (e.g., intensity and pitch of an utterance) enhanced through our KD strategy better assists the teacher in judging the confusing emotions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Methods",
      "text": "Remarks",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "The Impact Of Each Modality",
      "text": "Table  3  presents the results for single-modality and multimodal combinations. The single-modality performances for audio and visual are the results after applying our knowledge distillation method, and the same fusion approach as TelME is used for dual-modality results. The text modality performs the best among the single-modality, which supports our decision to use the text encoder as the teacher model. Additionally, the combination of non-verbal modalities and text modality achieves superior performance compared to using only text.\n\nOur findings indicate that the audio modality significantly contributes more to emotion recognition and holds greater importance compared to the visual modality. We speculate this can be attributed to its ability to capture the intensity of emotion through variations in the tone and pitch of the speaker. Overall, our method achieves 3.52% improvement in IEMOCAP and 0.8% in MELD over using only text.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct an ablation study to validate our knowledge distillation and fusion strategies in come of training each modality encoder using crossentropy loss and concatenating the embeddings without incorporating distillation loss and our fusion method.\n\nUsing our fusion method alone, IEMOCAP showed performance improvement, but MELD showed poor performance. The effectiveness of our fusion method in achieving optimal modality interaction cannot be guaranteed without knowledge distillation. Because each encoder is trained independently, focusing solely on improving its performance without considering the multimodal interaction. On the other hand, as our knowledge distillation components are added, these bring about consistent improvements for both datasets.\n\nWhen we examine the specific effects of the KD strategy, we observe performance improvements for both datasets, even when using only L response , presenting its efficacy in closing the gap between the teacher and the students. Furthermore, adding L f eature aimed to leverage the richer knowledge of the teacher is more effective in IEMOCAP and shows marginal performance enhancements in MELD. However, we speculate that the slight improvement in MELD may be attributed to class imbalance. While TelME significantly outperforms existing approaches in minority classes of MELD, the weighted F1 score is only slightly improved due to the low number of samples. We show an analysis of this class imbalance problem in Section 4.7 as well as an error analysis of the emotion classes in Appendix A.4.\n\nFigure  5  shows the individual performance of the audio and visual modalities based on the distillation loss. We observe that applying both types of distillation loss is more effective compared to not applying them. The performance of visual modality on the IEMOCAP dataset has declined, possi- bly because facial expressions are not effectively captured in the limited image frames of a short utterance. However, even with lower individual performance, all modalities have been shown to contribute to the improvement of emotion recognition performance through our approach (Table  3 ).  To assess the optimality of employing text modality as the teacher, we conduct comparative experiments by setting each modality as the teacher modality, which is shown in Table  5 . Our study shows that the TelME framework performs best with the text encoder as the teacher, while treating the other modalities as the teacher significantly hinders model performance.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Study On Teacher",
      "text": "Additionally, Knowledge Distillation (KD). Our comparative experiment results show that a robust text encoder can most effectively serve as the teacher. Specifically, designating the text encoder as the teacher enhances the performance of all student models except for the visual student in IEMOCAP. On the other hand, it is evident that treating a weak nonverbal model as the teacher impairs student performance, thereby performing suboptimally compared to having a text-based teacher. Figure  6  illustrates the label distribution within the MELD and IEMOCAP datasets. Notably, the MELD dataset exhibits a pronounced imbalance, with the \"neutral\" class comprising the majority at 47% of the data, followed by \"joy\" with 17% and \"surprise\" with 12%. This substantial class imbalance presents a challenge in the context of distillation, specifically for the teacher encoder to initially identify the minority classes and subsequently transfer this information to the non-verbal student encoders. We believe that this class im-balance is a contributing factor to the limited observed improvements associated with L f eature in the MELD dataset compared to the IEMOCAP dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Class Imbalance",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes Teacher-leading Multimodal fusion network for ERC (TelME), a novel multimodal ERC framework. TelME incorporates a cross-modal distillation that transfers the knowledge of text encoders trained in linguistic contexts to enhance the effectiveness of non-verbal modalities. Moreover, we employ the fusion method that shifts the features of the teacher model by referring to non-verbal information. We show through experiments on two benchmarks that our approach is practical in ERC. TelME delivers robust performance in both datasets and especially achieves state-of-the-art results in the MELD dataset, which consists of multi-party conversational scenarios. We believe that this research presents a new direction that can incorporate multimodal information for ERC.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "This study has a limitation wherein the visual modality shows a lower capability to recognize emotions compared to the audio modality. To address this limitation, future research should focus on developing techniques to accurately capture and interpret the facial expressions of the speaker during brief utterances. By improving the extraction of visual features, the effectiveness of knowledge distillation can be significantly enhanced, thus showcasing its potential to make a more substantial contribution to emotion recognition. set to 1 regardless of the dataset. We also use a fusion method that shifts vectors in the teacher model, where the threshold parameter is set to 0.01 for IEMOCAP and 0.1 for MELD. Furthermore, Dropout is adjusted to 0.2 for MELD and 0.1 for IEMOCAP. The number of heads used in the multihead attention process is 4 for IEMOCAP and 3 for MELD.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.3 Compared Models",
      "text": "We compare TelME against the following models: DialogueRNN  (Majumder et al., 2019)  employs Recurrent Neural Networks (RNNs) to capture the speaker identity as well as the historical context and the emotions of past utterances to capture the nuances of conversation dynamics. ConGCN  (Zhang et al., 2019)  utilizes a Graph Convolutional Network (GCN) to represent relationships within a graph that incorporates both context and speaker information of multiple conversations. MMGCN  (Hu et al., 2021b ) also proposes a GCN-based approach, but captures representations of a conversation through a graph that contains long-distance flow of information as well as speaker information. DialogueTRM  (Mao et al., 2021)  focuses on modeling both local and global context of conversations to capture the temporal and spatial dependencies. DAG-ERC  (Shen et al., 2021)  studies how conversation background affects information of the surrounding context of a conversation. MMDFN  (Hu et al., 2022a)  proposes a framework that aims to enhance integration of multimodal features through dynamic fusion. EmoCaps  (Li et al., 2022)  introduces an emotion capsule that fuses information from multiple modalities with emotional tendencies to provide a more nuanced understanding of emotions within a conversation. UniMSE  (Hu et al., 2022b)  seeks to unify ERC with multimodal sentiment analysis through a T5-based framework. GA2MIF  (Li et al., 2023a ) introduces a two-stage multimodal fusion of information from a graph and an attention network. FacialMMT  (Zheng et al., 2023)  focuses on extracting the real speaker's face sequence from multi-party conversation videos and then leverages auxiliary frame-level facial expression recognition tasks to generate emotional visual representations. Figure  7  shows the normalized confusion matrices of the TelME and the understated model for two datasets. We can evaluate the quality of the emotion prediction through the confusion matrix. TelME shows better True Positive results in almost all emotion classes. This suggests that TelME is extracting and fusing finer-grained features to infer emotions without bias. TelME better classifies similar emotions compared to the understated model(e.g., excited and happy, angry and frustrated). However, the result of misclassifying happy as exciting is a little high. This result is due to the lowest percentage of happy in IEMOCAP with unbalanced classes. Even in the case of MELD, the emotion in which most emotion classes are misclassified is neutral, with the highest count. We can observe a similar misclassification tendency in other research  (Chudasama et al., 2022; Hu et al., 2023)  as well. Hence, we suspect that the cause of misclassification is not a problem with the method we proposed but rather stems from a class imbalance issue.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.4 Error Analysis",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.5 Results Of Random Seed Numbers",
      "text": "We report all outcomes based on the seed number 42 following previous studies  (Lee and Lee, 2021; Song et al., 2022a; Hu et al., 2022b)    10  presents a segment of the ground truth label from the MELD dataset, along with inference outcomes of each unimodal model (Text teacher, non-verbal students) and TelME. The results indicate that student models can make different judgments than the text teacher even after knowledge distillation. Moreover, the final decision of TelME, supported by complementary information from non-verbal modalities, might diverge from the prediction of the text teacher, rectifying any inaccuracies. This implies that TelME utilizes multimodal information instead of heavily depending on any of the three modalities.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Examples of multimodal ERC. Even the same",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates an",
      "page": 1
    },
    {
      "caption": "Figure 2: shows the robustness of text",
      "page": 2
    },
    {
      "caption": "Figure 2: Unimodal Performance on MELD dataset",
      "page": 2
    },
    {
      "caption": "Figure 3: The overview of TelME",
      "page": 3
    },
    {
      "caption": "Figure 3: visually illustrates how each modality en-",
      "page": 4
    },
    {
      "caption": "Figure 2: , effective knowledge distillation can be",
      "page": 5
    },
    {
      "caption": "Figure 4: Attention-based modality Shifting Fusion",
      "page": 5
    },
    {
      "caption": "Figure 4: ). We are inspired by Rahman et al. (2020) to",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the individual performance of",
      "page": 8
    },
    {
      "caption": "Figure 5: Individual performance of audio and visual",
      "page": 8
    },
    {
      "caption": "Figure 6: Count distribution of emotion classes for both",
      "page": 9
    },
    {
      "caption": "Figure 6: illustrates the label distribution within",
      "page": 9
    },
    {
      "caption": "Figure 7: Confusion Matrices on IEMOCAP and MELD",
      "page": 13
    },
    {
      "caption": "Figure 7: shows the normalized confusion matri-",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Statistics of the two benchmark datasets.",
      "page": 6
    },
    {
      "caption": "Table 1: MELD is a multi-party dataset comprising over",
      "page": 6
    },
    {
      "caption": "Table 2: Performance comparisons on MELD (7-way) and IEMOCAP",
      "page": 7
    },
    {
      "caption": "Table 2: TelME demonstrates robust",
      "page": 7
    },
    {
      "caption": "Table 2: , we report the performance",
      "page": 7
    },
    {
      "caption": "Table 3: Performance comparison for single modality",
      "page": 7
    },
    {
      "caption": "Table 3: presents the results for single-modality and",
      "page": 7
    },
    {
      "caption": "Table 4: The initial row for each dataset represents the out-",
      "page": 7
    },
    {
      "caption": "Table 4: Results of ablation study. Here, Lresponse is",
      "page": 8
    },
    {
      "caption": "Table 5: TelME Performance by Teacher Modality",
      "page": 8
    },
    {
      "caption": "Table 5: Our study",
      "page": 8
    },
    {
      "caption": "Table 6: reports the individual per-",
      "page": 8
    },
    {
      "caption": "Table 6: represent results without performing",
      "page": 8
    },
    {
      "caption": "Table 6: Teacher Modality Study on MELD and IEMO-",
      "page": 9
    },
    {
      "caption": "Table 7: Comparison of the teacher performance based",
      "page": 12
    },
    {
      "caption": "Table 7: shows an ablation experiment on the",
      "page": 12
    },
    {
      "caption": "Table 8: hyperparameter settings of TelME on two",
      "page": 12
    },
    {
      "caption": "Table 9: Performance of the full framework for five",
      "page": 13
    },
    {
      "caption": "Table 9: The results in Table 10 demonstrate",
      "page": 13
    },
    {
      "caption": "Table 10: inference results of each unimodal model and",
      "page": 13
    },
    {
      "caption": "Table 10: presents a segment of the ground truth",
      "page": 13
    },
    {
      "caption": "Table 11: Performance comparisons on IEMOCAP (6-way)",
      "page": 14
    },
    {
      "caption": "Table 11: shows a performance comparison of our",
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Is space-time attention all you need for video understanding",
      "authors": [
        "Gedas Bertasius",
        "Heng Wang",
        "Lorenzo Torresani"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2fnet: multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Commonsense knowledge for emotion identification in conversations",
      "arxiv": "arXiv:2010.02795"
    },
    {
      "citation_id": "7",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "8",
      "title": "Cross modal distillation for supervision transfer",
      "authors": [
        "Saurabh Gupta",
        "Judy Hoffman",
        "Jitendra Malik"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "2018a. Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "10",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "11",
      "title": "A comprehensive overhaul of feature distillation",
      "authors": [
        "Byeongho Heo",
        "Jeesoo Kim",
        "Sangdoo Yun",
        "Hyojin Park",
        "Nojun Kwak",
        "Jin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "13",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "year": "2023",
      "venue": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "arxiv": "arXiv:2306.01505"
    },
    {
      "citation_id": "14",
      "title": "2022a. Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "2021a. Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "venue": "2021a. Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "16",
      "title": "2022b. Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "venue": "2022b. Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "arxiv": "arXiv:2211.11256"
    },
    {
      "citation_id": "17",
      "title": "2021b. Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "venue": "2021b. Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "18",
      "title": "Knowledge distillation from a stronger teacher",
      "authors": [
        "Tao Huang",
        "Shan You",
        "Fei Wang",
        "Chen Qian",
        "Chang Xu"
      ],
      "year": "2022",
      "venue": "Knowledge distillation from a stronger teacher",
      "arxiv": "arXiv:2205.10536"
    },
    {
      "citation_id": "19",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael R Lyu"
      ],
      "year": "2019",
      "venue": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "arxiv": "arXiv:1904.04446"
    },
    {
      "citation_id": "21",
      "title": "Xiang Ren, and Hamed Firooz",
      "authors": [
        "Woojeong Jin",
        "Maziar Sanjabi",
        "Shaoliang Nie",
        "Liang Tan"
      ],
      "year": "2021",
      "venue": "Msd: Saliency-aware knowledge distillation for multimodal understanding",
      "arxiv": "arXiv:2101.01881"
    },
    {
      "citation_id": "22",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "23",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2021",
      "venue": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "arxiv": "arXiv:2108.11626"
    },
    {
      "citation_id": "24",
      "title": "2023a. Ga2mif: Graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "2023b. Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Yong Li",
        "Yuanzhi Wang",
        "Zhen Cui"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "28",
      "title": "Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2022",
      "venue": "Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions",
      "arxiv": "arXiv:2209.03430"
    },
    {
      "citation_id": "29",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "30",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Yukun Ma",
        "Linh Khanh",
        "Frank Nguyen",
        "Erik Xing",
        "Cambria"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "32",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "33",
      "title": "Dialoguetrm: Exploring multimodal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "34",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "35",
      "title": "Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "37",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amir Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "38",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "39",
      "title": "2022a. Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "venue": "2022a. Supervised prototypical contrastive learning for emotion recognition in conversation",
      "arxiv": "arXiv:2210.08713"
    },
    {
      "citation_id": "40",
      "title": "2022b. Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "41",
      "title": "From within to between: Knowledge distillation for cross modality retrieval",
      "authors": [
        "Niranjan Vinh Tran",
        "Minh Balasubramanian",
        "Hoai"
      ],
      "year": "2022",
      "venue": "Proceedings of the Asian Conference on Computer Vision"
    },
    {
      "citation_id": "42",
      "title": "Huggingface's transformers: State-ofthe-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Rémi Louf",
        "Morgan Funtowicz"
      ],
      "year": "2019",
      "venue": "Huggingface's transformers: State-ofthe-art natural language processing",
      "arxiv": "arXiv:1910.03771"
    },
    {
      "citation_id": "43",
      "title": "The modality focusing hypothesis: Towards understanding crossmodal knowledge distillation",
      "authors": [
        "Zihui Xue",
        "Zhengqi Gao",
        "Sucheng Ren",
        "Hang Zhao"
      ],
      "year": "2022",
      "venue": "The modality focusing hypothesis: Towards understanding crossmodal knowledge distillation",
      "arxiv": "arXiv:2206.06487"
    },
    {
      "citation_id": "44",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "46",
      "title": "Multi-channel weightsharing autoencoder based on cascade multi-head attention for multimodal emotion recognition",
      "authors": [
        "Jiahao Zheng",
        "Sen Zhang",
        "Zilu Wang",
        "Xiaoping Wang",
        "Zhigang Zeng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "47",
      "title": "A facial expression-aware multimodal multitask learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "Wenjie Zheng",
        "Jianfei Yu",
        "Rui Xia",
        "Shijin Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "48",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "arxiv": "arXiv:1909.10681"
    },
    {
      "citation_id": "49",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "arxiv": "arXiv:2106.01071"
    }
  ]
}