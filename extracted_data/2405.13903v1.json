{
  "paper_id": "2405.13903v1",
  "title": "St-Gait++: Leveraging Spatio-Temporal Convolutions For Gait-Based Emotion Recognition On Videos",
  "published": "2024-05-22T18:24:21Z",
  "authors": [
    "Maria Luísa Lima",
    "Willams de Lima Costa",
    "Estefania Talavera Martinez",
    "Veronica Teichrieb"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is relevant for human behaviour understanding, where facial expression and speech recognition have been widely explored by the computer vision community. Literature in the field of behavioural psychology indicates that gait, described as the way a person walks, is an additional indicator of emotions. In this work, we propose a deep framework for emotion recognition through the analysis of gait. More specifically, our model is composed of a sequence of spatial-temporal Graph Convolutional Networks that produce a robust skeleton-based representation for the task of emotion classification. We evaluate our proposed framework on the E-Gait dataset, composed of a total of 2177 samples. The results obtained represent an improvement of ≈ 5% in accuracy compared to the state of the art. In addition, during training we observed a faster convergence of our model compared to the state-of-the-art methodologies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Humans have a natural perception capability that allows us to capture, process, and understand behavioral cues from other people naturally  [15] . There are several biological triggers inside our brains that plan our interactions with other people based on this perception  [20] . However, human behavior is a very broad cognitive spectrum with multiple different nuances that can affect this planning procedure. When looking at social interactions, however, emotion is a specific part of behavior that plays a significant role. The ability to perceive and respond to emotional aspects is essential to develop and maintain links with peers in society.\n\nIn multiple contexts of applications, we could argue that understanding the emotions of users is also a significant aspect of the development of systems that are more inclusive and fair. These systems could adapt the way they perform according to how they perceive their users. However, to allow these systems to understand the emotions of their user, first, they need to be able to extract and process emotional information from them. Researchers have been studying how humans perceive and process these affective cues for a while, and evidence from the behavioral psychology literature suggests that a significant part of affective information is communicated naturally and intuitively through a medium known as nonverbal communication  [2, 13, 16] .\n\nAmong the vast list of nonverbal communication cues, some studies highlight the importance of bodily expression for emotion recognition. Early studies such as those by Wallbott and Scherer  [31]  suggested that there are specific body movements that lead to an accurate perception of emotions. Therefore, this strong evidence from the literature on behavioral psychology validates that the body acts as an outlet for the person's emotional state, as well as signals that, by extracting and processing these cues, someone can perceive emotional aspects through observation of certain characteristics. Recent studies have shown significant success in encoding body language to recognize affect in humans through deep learning and computer vision, indicating that body expressions are a significant cue when building affect-aware technology  [4, 14, 27, 33] .\n\nHowever, body language, as well as other affective fea-tures such as facial expressions, gaze, gestures, or physiological indicators such as cardiac frequency or respiratory rate, share a common limitation related to applications: they require the user to be facing toward a camera so that the affective sources (e.g., face, eyes, arms...) are visible at all times. Although this constraint is not harmful in some scenarios, such as when people are facing a computer or other affective agents (such as social robots), it is also a limiting aspect in ubiquitous applications since the user would not be able to freely interact with the environment. We could, however, communicate to the user this constraint and ask the user to face the system; however, this would alter the user's behavior, changing how they would communicate their emotions and removing all naturality from this interaction, which would now be an artificial interaction.\n\nA possible way to encode body language and still not limit the user is to look at the person's gait to extract affective information. Gait is the description of the way that someone walks, and previous research in behavioral psychology has found that humans are able to identify multiple social aspects through gait-related parameters  [18] . Roether et al.  [25]  applied machine learning to a set of recorded gaits and found that some features, such as movement speed and limb flexion, were essential in correlating emotions and gaits. These pieces of evidence suggest that evaluating gait in a spatiotemporal manner can lead to strong classifications of emotion. With this, new applications in multiple scenarios are enabled, for example: we could leverage gait information to monitor freely moving citizens for public safety, where collective negative emotions could indicate a dangerous situation taking place  [10] ; wellbeing applications such as urbanism, where collective spaces can be changed according to what people experience in them  [30] ; or even healthcare where the psychological monitoring of a patient in internment can prove to be useful not only as an overall better treatment, especially if non-invasive, but also to enable mental health professionals to better identify mental health issues in their patients by having another information source to look into (Dhuheir et al.  [8] ).\n\nBased on this inspiration, we propose an approach for emotion recognition using gait. Through the use of spatiotemporal processing blocks, we overcome limitations present in the current state-of-the-art  [1] , which possess a limited processing capability in this aspect. We also show that by overcoming this limitation, our results in the proposed quantitative metric are also better than the state-ofthe-art. This work's contributions are as follows:\n\n• An emotion recognition method through gait and body language, compatible with behavioral psychology studies, with a ≈ 5% increase in accuracy relative to the stateof-the-art • A model that converges 3.63 times faster in training, saving time and computational resources, allowing for faster testing and experimentation. The rest of the paper is organized as follows: Next, in Section 2, we visit the state of the art. In Section 3, we present the methods utilized and in Section 4, we expose our experimental setup. Later, in Section 5, we present the results and discuss them. Finally, Section 6 shows our conclusions and present ideas for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Early works for emotion recognition through gaits were based on extracting features from motion capture data, which is acquired by using special clothing with landmark points and a motion capture camera, and using algorithms to calculate similarity indexes with databases. Venture et al.  [29]  captured walks from four different performers and applied PCA to verify that these emotions could be distinguished through some affective features.\n\nA significant improvement was proposed by Daoudi et al.  [7] , which evaluated this task from a geometric approach. They have represented skeleton joints over time using covariance matrices, which were mapped to the Riemannian manifold of symmetric positive definite matrices. This allowed the authors to exploit multiple geometric properties for emotion classification. However, this approach is limited by how much temporal information can be encoded, imposing a limited sequence modeling.\n\nThe natural next step here was, then, related to improving the temporality aspect of these previous approaches. Randhavane et al.  [23]  presented a new approach, now focused on RNNs, thus allowing improved spatiotemporal relationship. They combined affective features, such as the angles between joints and stride length, with deep features that were learned using a Long Short-Term Memory architecture.\n\nHowever, the advances on Graph Convolutional Networks at that time, especially the proposal of the ST-GCNs by Yan et al.  [32]  allowed for an even more robust way of learning these relationships. Bhattacharya et al.  [1]  proposed using such architecture to extract features from videos and classify the emotions in an implicit manner.\n\nStill, while ST-GCNs provide an effective approach, there are some limitations that this work aims to address. The representational capacity of the base ST-GCN is predefined rather than learned, which was sufficient for its originally intended application of activity recognition. However, for perceiving emotional cues through nonverbal behaviors like gait, these cues are often more subtle than the movements used for activity recognition. Therefore, not learning the topology may limit the ability to capture these subtle movement patterns that are indicative of different emotions. This is solved by using the ST-GCN++ architecture, which brings the novelty of learning the topology during training. Another limitation is in the temporal aspect, with ST-GCNs having a fixed size temporal kernel, which was rendered adjustable in the ST-GCN++ architecture. Therefore, switching from ST-GCN to ST-GCN++, is very advantageous for the emotion recognition using gaits problem.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Given a video V ∈ R n×h×w×3 with n frames, height h width w and a set of emotions K, our task is to classify the perceived emotion of a person present in such video by extracting features related to body language and gait. We first extract a set of 3D body keypoints K ∈ R 16×3 , in which k 1 , k 2 , . . . , k 16 , each k i represents the location of a body joint in space related to the person in the video.\n\n(a) Skeletal trajectory extraction. One of the possible ways to represent a skeleton is through a graph, since these representations can be considered analog. Each body joint, such as the right shoulder or right elbow can be considered a vertex, and the bones that connects these two joints can be considered edges. This is a clear indicative on why GCNs can be used to process these types of data. Therefore, at a given timestamp t, we extract the skeleton of the person visible on the scene and represent it as a graph: G = (V, E), where V is the vertices (or joints) set and E is the edge (or bones) set and N = |V| the number of vertices.\n\n(b) Skeletal trajectory classification. We use this graph G as input for our gait processing model. We propose using ST-GCN++ blocks  [9]  to learn the joint representations and discover movement patterns related to perceived emotions. This way, nonverbal cues such as step size, arm swinging, head angle relative to the shoulders, among others, can be extracted automatically and without user intervention.\n\nThe extracted gait features are propagated from the body joints in the shape of X ∈ R n×f with x i ∈ R f representing a feature of the i th vertex. The propagation rule is done in the following manner: Z (l+1) = σ(AZ (l) W (l) ), where Z l and Z l+1 are the inputs to the network layers l and l + 1, with Z 0 = X. W l and W l+1 are the weight matrices between layers l and l + 1, and A is the adjacency matrix of G and σ(.) is a nonlinear activation function. Each weight matrix W represents a convolutional kernel which can be used to obtain features. For example, the application of k kernels of dimension f × d in an input X, the output corresponds to a feature vector of dimension n × d × k. There's also the set of adjacent vertices A t i ⊆ V to v t i at time t  [1] . Our proposed model ST-Gait++ is composed of 3 ST-GCN++ blocks with 32, 64, and 64 kernels each, followed by an average pooling, a 2D 1x1 convolution layer, and a softmax layer for the 4 emotion categories. This design was chosen empirically to overcome the limitations presented previously in Section 2, but also because other works ex-periment with similar architectures, such as STEP  [1] . Also, according to the methodology described by the author  [1] , affective features can extracted from the data, so those are extracted and added to the used data as well. We overview our model in Figure  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4. Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "4.1. Dataset",
      "text": "We used the Emotion-Gait (E-Gait) dataset in our experiments. The dataset that is currently available is a modified version of the original dataset made available by Bhattacharya et al.  [1] . The authors  [1]  do not share specifics of what has changed or when it did, so we try our best to keep a clear comparison with other techniques from the state of the art. We only used the real data from the E-Gait, because of some quality issues perceived during early experimentation with the synthetic data, alongside the issue of the changes on the dataset.\n\nThe data consists on 342 samples collected by the authors and 1,835 samples collected by the Edinburgh Locomotion Mocap Database (ELMD)  [11]  and its distribution per train/validation/test split and categories can be found in Table  1 . The data is composed of already extracted and normalized skeleton sequences forming different labeled gaits in the 4 emotions targeted in this paper: Happy, Neutral, Sad and Angry. Each sample is shaped T × V with T being the number of time steps and V the number of coordinates which is equal to 48 here since there are 16 joins with 3 dimensions each. We show samples of the skeletons present in the dataset in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Validation Metric",
      "text": "We have used the accuracy metric, which is a common metric used in multiple emotion recognition baselines  [5] . This metric is also employed in other works that use E-Gait as the evaluation benchmark.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "We have used PyTorch 1.7 to build and train our model. We have used the publicly available implementation for the   Each item is a sample from one of the categories of the E-Gait dataset, with each frame of the six-frame sequence taken from the whole sample gait sequence. This was done to provide a sense of movement to the reader, so they can better understand E-Gait's characteristics.\n\nST-GCN++ block published by Duan et al.  [9]  1 . From this point, we have applied a Bayesian Search algorithm for hyperparameter tuning. Bayesian Search is a powerful yet simple technique for optimizing hyperparameters by modelling objective functions and updating its belief based 1 Available at https : / / github . com / kennymckormick / pyskl on observed results. Therefore, instead of using a random set of hyperparameters, which would be computationally costly, this method will choose the set of hyperparameters that has the highest chance of leading to better results and will discard those with low chance. The search space we navigated using this algorithm is shown in Table  2 , and the set of chosen hyperparameters in comparison to those of STEP  [1]  are shown in Table  3 . We have trained ST-Gait++ for 200 epochs. Table  2 . Search space defined for the Bayesian Search. For each parameter, a set of values are chosen as a search space. As the Bayesian Search finds an optimal value within the search space for each parameter, this value is represented on the third column of the table. These optimal values were used to train ST-Gait++.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Parameter",
      "text": "Search Space Value Basic Learning Rate 0.01, 0.  The loss used during training for both STEP and ST-Gait++ was Cross Entropy Loss. We used the Pytorch implementation  2  . Since the predictions for each sample were in the form of probabilities for each class, the Cross Entropy Loss can be defined as:\n\nWhere l is the loss, x is the input, y is the target, w is the weight C is the number of classes and N is the minibatch size.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "Quantitative analysis. We compare our results with other different approaches in Table  5 . Our proposed method outperforms other graph-related methods, such as STEP  [1] . It is interesting to notice that our implementation of this work has led to an increased accuracy than their reported results. In this case, our model has had an increase of 5.4% regarding their result, and 4.2% regarding our implementation. Besides the accuracy increase, our model was also able to converge faster, highlighting several improvements, such as fewer requirements for computational resources or training time, increased possibilities for scaling, and better generalization of data. ST-Gait++ converged on epoch #127, while STEP converged on epoch #462, a reduction of 72% in training time. A runtime analysis was also performed, over the test set, and can be found on table  Table    4 . As observed, STEP has a better time for inference, which can be explained by the simpler architecture.\n\nOur model also highly outperforms other approaches with a temporal focus, such as Randhavane et al.  [22] . In this case, we report an accuracy increase of 6.8%. Overall, this quantitative evaluation highlights that our model not only has increased accuracy in relation to the current state-of-the-art, but is also more optimized towards training requirements.\n\nWe have also generated confusion matrices for ST-Gait++ and STEP in order to compare how the accuracy of these two models can be represented in how they perceive the emotional classes differently. We show these results in Figure  3 . As expected, this result reaffirm the results we have discussed before; STEP has a higher confusion between Neutral and Happy than our model. In the case of facial expressions, the literature on behavioral psychology has shown that there is a structural resemblance between neutral and happy faces that could lead to confusion, and it is common to have this type of ambiguity in these tasks  [4, 26] ; however, there is still no evidence that the same could happen to gait perception. Therefore, it is difficult to judge if STEP's ambiguity between these two classes could be justified. In our model, we also notice an ambiguity between Sad and Happy. We do not have any insights regarding this curious behavior at this point, but we also highlight that our accuracy for the Sad class is still higher than the reported for STEP.\n\nFinally, we have also applied a t-SNE representation of the features extracted from the last layer of the models to visualize and analyze the learned representations of the data. The last layer usually contains higher-level, abstract features of the input data, which could usually be represented by activation maps or feature maps. In this case, since we are not directly working with images at that point, applying t-SNE could reveal clusters or groups that could highlight the ability of the model to distinguish classes or categories effectively, as well as to identify outliers or anomalies in the data. We show in Figure  4  that the Angry class has a very distinct and separated clustering, which explains the better performance in this class as we have shown before in Figure  3 . This better performance can also be attributed to the far larger amount of samples for this category, making the model better able to separate it from the rest. Neutral and Happy categories are distinguishable enough to show why they also show a good performance, while Sad, the least numerous category, is very mingled with the others.\n\nQualitative analysis. In Figure  5  we show some qualitative visualizations with examples of correct and incorrect predictions. By looking just at the skeletons it's impossible to distinguish the emotions. For some, we may agree  As can be seen, there is a more pronounced diagonal on (a), emphasizing the better accuracy of ST-Gait++. Also, there is less confusion between the emotions Neutral and Happy on ST-Gait++ than on STEP. However, there is some increase in confusion between Happy and Sad on ST-Gait++.\n\nwith the annotation provided and understand why the model made a correct prediction. For others, it's dubious to infer the perceived emotion with just the skeleton, and we can understand why the model made a mistake. On (a) we can see the wide stride, arm swinging and attribute that to happiness, but we can also see the somewhat lowered head, which could indicate sadness. On (b) we can se a fast stride and arm swinging that could indicate some more energetic emotion, but the model still correctly classifies it as sadness. (c) gives us a lowered head, which could have lead the model to infer on sadness, even though the swinging arms and big stride indicate anger. Lastly, (d) shows a fast stride with slightly swinging arms and upperbody which could have misled the model into inferring happiness and not some neutral emotion. Overall, we can understand that there is some dubiousness in the data that leads to explainable mistakes. However, given the temporality aspect of this evaluation, we are limited to what we can show in this research paper. We have prepared a video with a more in-depth overview available at to be made available upon publication.\n\nLimitations of the E-Gait dataset. To the best of our knowledge, E-Gait is one of the only public datasets of emotion recognition through gait, alongside Emotion-Walk (E-Walk)  [22] . However, since they have data overlap, we decided to test on E-Gait, given that it is the one used by the state-of-the-art approaches of emotion recognition from gait.\n\nIn addition, the E-Gait dataset  [1]  is very obscure, with only the skeletons being available to researchers. Because of this, the annotations cannot be confirmed and neither can we know the transformations taken to process the videos and the skeletons which are already normalized. This can be observed in Figure  5 . Furthermore, the dataset does not disclose the demographics of the subjects and a great part of the data comes from the capture with a single individual and adds a heavy bias. For an application in other contexts, such as Latin American contexts, for example, it would be very interesting to have a dataset that could show the local cultural emotion expression. Also, publishing an open dataset also containing the original videos, not only the skeletons, would give researchers a higher freedom for experimenting, validating ideas and checking for biases to correct.\n\nThe first step to making such a dataset would be to gather other available datasets for emotion recognition through gait, or curate new ones, focusing on bringing high quality data that is representative, diverse and as unbiased as possible. Testing ST-Gait++ on more data will certainly point to new paths of improvement for this research.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Diversity And Bias In Emotion Recognition Related Datasets.",
      "text": "There are many studies focused on trying to find biases in intelligent systems. One such study is Rhue  [24] , which found racial disparities in facial emotion recognition and raised the question of whether artificial inteligence could in fact determine emotion better than people. To answer this question, we need to understand that demographics such as gender, race and ethnicity heavily influence the perception of human characteristics, such as in facial emotion recognition algorithms  [3] . But this is also due to the fact that we as humans perceive the world based on our own biases. To annotate the data that will be fed into algorithms is to accept that the data will have biases that the model will propagate. Studies such as Pahl et al.  [21]  bring to attention the age bias, besides the ethnicity bias, in prominent Action Unit Datasets. The algorithms studied had problems with glasses but not with beards. This shows data collection problem: Are there no diverse people available or are these people not even considered as a possible variation in the target user public? These limitations can be extended to gaits, as it is a characteristic and emotion expression outlet that can vary across different demographics.\n\nFurthermore, although some emotion expression may have universal features  [28] , it is noted that cultural particularities are very influential in affective cues encoding Kleinsmith and Bianchi-Berthouze  [14] . For a general application, having an analysis on the demographic characteristics can help researchers gain more insight on the limi-  tations of the data and, as a consequence, of the real world application of their research. As such, having such a skewed dataset, such as ELMD (Habibie et al.  [11] ), being a considerable part of the total data available, brings to question the applicability of the entire dataset in in-the-wild scenarios.\n\nGiven the ELMD dataset, of the 342 samples recorded, 90 participants were involved, with no demographic of this public being disclosed. It is disclosed that the data, all 1, 835 samples, was collected using a sole male actor. With this in mind, it's important to point out the importance of diversity and fairness in this data collection.\n\nAlso, it's important to emphasize that what is being dealt with here are the perceived emotions, since whatever actual emotion was being felt by the person at the time of data capture is only available if the person is questioned (in real scenarios) or if we know what they are trying to convey (actors portraying emotions). Even in real scenarios, the reported emotion after questioning could be biased due to the type of question asked or recall biases  [12] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we proposed ST-Gait++, a novel framework for emotion recognition from gait. Its skeleton-based spatio-temporal representation approach results in state-ofthe-art classification performance on the E-Gait dataset. We also discussed some of the limitations of the field with the objective of presenting research opportunities.\n\nIn addition, given the faster training convergence on a consumer grade laptop, we expect ST-Gait++ to provide new research opportunities in the field of human behaviour analysis for researchers with a lower budget or limited resources.\n\nFuture work will explore the relevance of the different body parts for recognising emotion and the inclusion of additional gait descriptors.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 4. Experimental setup",
      "page": 3
    },
    {
      "caption": "Figure 2: 4.2. Validation metric",
      "page": 3
    },
    {
      "caption": "Figure 1: The proposed architecture for ST-Gait++, composed of 3 ST-GCN++ blocks with outputs sized 32, 64 and 64, followed by a",
      "page": 4
    },
    {
      "caption": "Figure 2: Examples of the four categories of the E-Gait dataset.",
      "page": 4
    },
    {
      "caption": "Figure 3: As expected, this result reaffirm the results we",
      "page": 5
    },
    {
      "caption": "Figure 4: that the Angry class has a very",
      "page": 5
    },
    {
      "caption": "Figure 5: we show some qualita-",
      "page": 5
    },
    {
      "caption": "Figure 3: Confusion matrices generated from evaluating the models on the test set for (a) ST-Gait++ and (b) STEP. As can be seen, there",
      "page": 6
    },
    {
      "caption": "Figure 5: Furthermore, the dataset does not",
      "page": 6
    },
    {
      "caption": "Figure 4: T-SNE representation of the features extracted from the",
      "page": 7
    },
    {
      "caption": "Figure 5: Examples of correct and incorrect inferences by ST-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Runtime analysis on the test set, which has a total of Figure 3. As expected, this result reaffirm the results we",
      "data": [
        {
          "Methods": "Ventureetal.(2014)",
          "Acc(%)": "30.8"
        },
        {
          "Methods": "Daoudietal.(2017)",
          "Acc(%)": "42.5"
        },
        {
          "Methods": "Lietal.(2016)",
          "Acc(%)": "53.7"
        },
        {
          "Methods": "Crennetal.(2016)",
          "Acc(%)": "66.2"
        },
        {
          "Methods": "Randhavaneetal.(2019)",
          "Acc(%)": "80.7"
        },
        {
          "Methods": "Narayananetal.(2020)",
          "Acc(%)": "82.4"
        },
        {
          "Methods": "Bhattacharyaetal.(2020)(STEP)",
          "Acc(%)": "82.1"
        },
        {
          "Methods": "Bhattacharyaetal.(2020)(STEP)(Ourimplementation)",
          "Acc(%)": "83.3"
        },
        {
          "Methods": "YuMengetal.(2024)",
          "Acc(%)": "85.2"
        },
        {
          "Methods": "ST-Gait++(Ours)",
          "Acc(%)": "87.5"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "Uttaran Bhattacharya",
        "Trisha Mittal",
        "Rohan Chandra"
      ],
      "year": "2006",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Motivation, emotion and cognition: A developmental-interactionist view",
      "authors": [
        "Buck"
      ],
      "year": "1991",
      "venue": "International review of studies on emotion"
    },
    {
      "citation_id": "3",
      "title": "Gender shades: intersectional phenotypic and demographic evaluation of face datasets and gender classifiers",
      "authors": [
        "Joy Adowaa"
      ],
      "year": "2017",
      "venue": "Gender shades: intersectional phenotypic and demographic evaluation of face datasets and gender classifiers"
    },
    {
      "citation_id": "4",
      "title": "A fast multiple cue fusing approach for human emotion recognition. Available at SSRN 4255748",
      "authors": [
        "Willams Costa",
        "David Macêdo",
        "Cleber Zanchettin",
        "Estefanía Talavera",
        "Lucas Silva Figueiredo",
        "Veronica Teichrieb"
      ],
      "year": "2022",
      "venue": "A fast multiple cue fusing approach for human emotion recognition. Available at SSRN 4255748"
    },
    {
      "citation_id": "5",
      "title": "A survey on datasets for emotion recognition from vision: Limitations and in-the-wild applicability",
      "authors": [
        "Willams Costa",
        "Estefanía Talavera",
        "Renato Oliveira",
        "Lucas Figueiredo",
        "Marcelo João",
        "João Teixeira",
        "Paulo Lima",
        "Veronica Teichrieb"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "6",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "Arthur Crenn",
        "Rizwan Ahmed Khan",
        "Alexandre Meyer",
        "Saida Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "Mohamed Daoudi",
        "Stefano Berretti",
        "Pietro Pala",
        "Yvonne Delevoye",
        "Alberto Bimbo"
      ],
      "year": "2017",
      "venue": "Image Analysis and Processing-ICIAP 2017: 19th International Conference"
    },
    {
      "citation_id": "8",
      "title": "Emotion recognition for healthcare surveillance systems using neural networks: A survey",
      "authors": [
        "Marwan Dhuheir",
        "Abdullatif Albaseer",
        "Emna Baccour",
        "Aiman Erbad",
        "Mohamed Abdallah",
        "Mounir Hamdi"
      ],
      "year": "2021",
      "venue": "2021 International Wireless Communications and Mobile Computing (IWCMC)"
    },
    {
      "citation_id": "9",
      "title": "Pyskl: Towards good practices for skeleton action recognition",
      "authors": [
        "Jiaqi Haodong Duan",
        "Kai Wang",
        "Dahua Chen",
        "Lin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "The importance of perceiving social contexts when predicting crime and antisocial behaviour in cctv images",
      "authors": [
        "Dawn Grant",
        "David Williams"
      ],
      "year": "2011",
      "venue": "Legal and Criminological Psychology"
    },
    {
      "citation_id": "11",
      "title": "A recurrent variational autoencoder for human motion synthesis",
      "authors": [
        "Ikhsanul Habibie",
        "Daniel Holden",
        "Jonathan Schwarz",
        "Joe Yearsley",
        "Taku Komura"
      ],
      "year": "2007",
      "venue": "Proceedings of the British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "12",
      "title": "Guidelines for assessing and minimizing risks of emotion recognition applications",
      "authors": [
        "Javier Hernandez",
        "Josh Lovejoy",
        "Daniel Mcduff",
        "Jina Suh",
        "Tim O' Brien",
        "Arathi Sethumadhavan",
        "Gretchen Greene",
        "Rosalind Picard",
        "Mary Czerwinski"
      ],
      "year": "2021",
      "venue": "2021 9th International conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "13",
      "title": "Effects of emotional intelligence on the impression of irony created by the mismatch between verbal and nonverbal cues",
      "authors": [
        "Heike Jacob",
        "Benjamin Kreifelts",
        "Sophia Nizielski",
        "Astrid Schütz",
        "Dirk Wildgruber"
      ],
      "year": "2016",
      "venue": "PloS one"
    },
    {
      "citation_id": "14",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "Andrea Kleinsmith",
        "Nadia Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "19 expressing emotion through posture and gesture. The Oxford handbook of affective computing",
      "authors": [
        "Margaux Lhommet",
        "Stacy Marsella"
      ],
      "year": "2014",
      "venue": "19 expressing emotion through posture and gesture. The Oxford handbook of affective computing"
    },
    {
      "citation_id": "16",
      "title": "Expressing emotion through posture. The Oxford handbook of affective computing",
      "authors": [
        "Margaux Lhommet",
        "Stacy Marsella"
      ],
      "year": "2014",
      "venue": "Expressing emotion through posture. The Oxford handbook of affective computing"
    },
    {
      "citation_id": "17",
      "title": "Identifying emotions from non-contact gaits information based on microsoft kinects",
      "authors": [
        "Baobin Li",
        "Changye Zhu",
        "Shun Li",
        "Tingshao Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "The identification of emotions from gait information",
      "authors": [
        "Joann Montepare",
        "Sabra Goldstein",
        "Annmarie Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "19",
      "title": "Proxemo: Gait-based emotion learning and multiview proxemic fusion for socially-aware robot navigation",
      "authors": [
        "Venkatraman Narayanan",
        "Bala Murali Manoghar",
        "Vishnu Sashank Dorbala",
        "Dinesh Manocha",
        "Aniket Bera"
      ],
      "year": "2020",
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "20",
      "title": "The perception/cognition distinction",
      "authors": [
        "Anders Nes",
        "Kristoffer Sundberg",
        "Sebastian Watzl"
      ],
      "year": "2023",
      "venue": "Inquiry"
    },
    {
      "citation_id": "21",
      "title": "Female, white, 27? bias evaluation on data and algorithms for affect recognition in faces",
      "authors": [
        "Jaspar Pahl",
        "Ines Rieger",
        "Anna Möller",
        "Thomas Wittenberg",
        "Ute Schmid"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "22",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "Tanmay Randhavane",
        "Uttaran Bhattacharya",
        "Kyra Kapsaskis",
        "Kurt Gray",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "23",
      "title": "Learning perceived emotion using affective and deep features for mental health applications",
      "authors": [
        "Tanmay Randhavane",
        "Uttaran Bhattacharya",
        "Kyra Kapsaskis",
        "Kurt Gray",
        "Aniket Bera",
        "Dinesh Manocha"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)"
    },
    {
      "citation_id": "24",
      "title": "Racial influence on automated perceptions of emotions. Available at SSRN 3281765",
      "authors": [
        "Lauren Rhue"
      ],
      "year": "2018",
      "venue": "Racial influence on automated perceptions of emotions. Available at SSRN 3281765"
    },
    {
      "citation_id": "25",
      "title": "Critical features for the perception of emotion from gait",
      "authors": [
        "Claire Roether",
        "Lars Omlor",
        "Andrea Christensen",
        "Martin Giese"
      ],
      "year": "2009",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "26",
      "title": "Structural resemblance to emotional expressions predicts evaluation of emotionally neutral faces",
      "authors": [
        "Nicu Christopher P Said",
        "Alexander Sebe",
        "Todorov"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "27",
      "title": "Emosec: Emotion recognition from scene context",
      "authors": [
        "Selvarajah Thuseethan",
        "Sutharshan Rajasegarar",
        "John Yearwood"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "The spontaneous expression of pride and shame: Evidence for biologically innate nonverbal displays",
      "authors": [
        "L Jessica",
        "David Tracy",
        "Matsumoto"
      ],
      "year": "2008",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "29",
      "title": "Recognizing emotions conveyed by human gait",
      "authors": [
        "Gentiane Venture",
        "Hideki Kadone",
        "Tianxiang Zhang",
        "Julie Grèzes",
        "Alain Berthoz",
        "Halim Hicheur"
      ],
      "year": "2014",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "30",
      "title": "Correction to: affective urbanism: towards inclusive design praxis",
      "authors": [
        "Tihomir Viderman",
        "Sabine Knierbein"
      ],
      "year": "2020",
      "venue": "Urban Design International"
    },
    {
      "citation_id": "31",
      "title": "Cues and channels in emotion recognition",
      "authors": [
        "G Harald",
        "Klaus Wallbott",
        "Scherer"
      ],
      "year": "1986",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "32",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "Sijie Yan",
        "Yuanjun Xiong",
        "Dahua Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition for multiple context awareness",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Shunli Wang",
        "Yang Liu",
        "Peng Zhai",
        "Liuzhen Su",
        "Mingcheng Li",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "Affective-pose gait: perceiving emotions from gaits with body pose and human affective prior knowledge. Multimedia Tools and Applications",
      "authors": [
        "Zhao Yumeng",
        "Liu Liu Zhen",
        "Wang Tingting",
        "Chai Yuanyi",
        "Yanjie"
      ],
      "year": "2024",
      "venue": "Affective-pose gait: perceiving emotions from gaits with body pose and human affective prior knowledge. Multimedia Tools and Applications"
    }
  ]
}