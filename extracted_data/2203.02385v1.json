{
  "paper_id": "2203.02385v1",
  "title": "Mm-Dfn: Multimodal Dynamic Fusion Network For Emotion Recognition In Conversations",
  "published": "2022-03-04T15:42:53Z",
  "authors": [
    "Dou Hu",
    "Xiaolong Hou",
    "Lingwei Wei",
    "Lianxin Jiang",
    "Yang Mo"
  ],
  "keywords": [
    "emotion recognition",
    "emotion recognition in conversations",
    "multimodal fusion",
    "dialogue systems"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal context features in a conversation. The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversations (ERC) aims to detect emotions in each utterance of the conversation. It has considerable prospects for developing empathetic machines  [1] . This paper studies ERC under a multimodal setting, i.e., acoustic, visual, and textual modalities.\n\nA conversation often contains rich contextual clues  [2, 3] , which are essential for identifying emotions. The key success factors of multimodal ERC are accurate context understanding and multimodal fusion. Previous context-dependent works  [3] [4] [5]  model conversations as sequence or graph structures to explore contextual clues within a single modality. Although these methods can be naturally extended multimodal paradigms by performing early/late fusion such as  [6] [7] [8] , it is difficult to capture contextual interactions between modalities, which limits the utilization of multiple modalities. Besides, some carefully-designed hybrid fusion methods  [9] [10] [11] [12]  Corresponding author. Email: HUDOU470@pingan.com.cn focus on the alignment and interaction between modalities in isolated or sequential utterances. These methods ignore complex interactions between utterances, resulting in leveraging context information in conversations insufficiently.\n\nRecent remarkable works  [13, 14]  model unimodal and cross-modal interactions in a graph structure, which provides complementarity between modalities for tracking emotions. However, these graph-based fusion methods aggregate contextual information in a specific semantic space at each layer, gradually accumulating redundant information. It limits context understanding between modalities. The contextual information continuously aggregated can be regarded as specific views where each view can have its individual representation space and dynamics. We believe that modeling these dynamics of contextual information in different semantic spaces can reduce redundancy and enhance complementarity, accordingly boosting context understanding between modalities.\n\nIn this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize utterance-level emotion by sufficiently understanding multimodal conversational context. Firstly, we utilize a modality encoder to track speaker states and context in each modality. Secondly, inspired by  [15, 16] , we improve the graph convolutional layer  [17]  with gating mechanisms and design a new Graphbased Dynamic Fusion (GDF) module to fuse multimodal context information. The module utilizes graph convolution operation to aggregation context information of both interand intra-modality in a specific semantic space at each layer. Meanwhile, the gating mechanism is used to learn the intrinsic sequential patterns of contextual information in adjacent semantic space. The GDF module can control information flow between layers, reducing redundancy and promoting the complementarity between modalities. The stack of GDFs can naturally fuse multimodal context features by embedding them into a dynamic semantic space. Finally, an emotion classifier is used to predict the emotion label of the utterance.\n\nWe conduct a series of experiments on two public benchmark datasets, i.  graph-based dynamic fusion module to fuse multimodal conversational context. This module can reduce redundancy and enhance complementarity between modalities. 3) Extensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of the proposed model 1 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Formally, Given A Conversation",
      "text": "where N is the number of utterances. u a i , u v i , u t i denote the raw feature representation of u i from the acoustic, visual, and textual modality, respectively. There are M speakers P = {p 1 , ..., p M }(M ≥ 2). Each utterance u i is spoken by the speaker p φ(ui) , where φ maps the index of the utterance into the corresponding speaker. Moreover, we define U λ to represent the set of utterances spoken by the party p λ .\n\nThe goal of multimodal ERC is to predict the emotion label y i for each utterance u i from pre-defined emotions Y.\n\nIn this section, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to fully understand the multimodal conversational context for ERC, as shown in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Encoder",
      "text": "To capture context features for the textual modality, we apply a bi-directional gated recurrent unit (BiGRU); for the acoustic and visual modalities, we apply a fully connected network. The context embedding can be computed as:\n\nwhere ← --→ GRU c is a BiGRU to obtain context embeddings and\n\nConsidering the impact of speakers in a conversation, 1 The code is available at https://github.com/zerohd4869/MM-DFN we also employ a shared-parameter BiGRU to encode different contextual information from multiple speakers:\n\nwhere ← --→ GRU s indicates a BiGRU to obtain speaker embeddings. h s λ,j is the j-th hidden state of the party p λ . λ = φ(u i ). U λ refers to all utterances of p λ in a conversation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph-Based Dynamic Fusion Modules",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph Construction",
      "text": "Following  [13] , we build an undirected graph to represent a conversation, denoted as G = (V, E). V refers to a set of nodes. Each utterance can be represented by three nodes for differentiating acoustic, visual, and textual modalities. Given N utterances, there are 3N nodes in the graph. We add both context embedding and speaker embedding to initialize the embedding of nodes in the graph:\n\nwhere γ a , γ v , γ t are trade-off hyper-parameters. E refers to a set of edges, which are built based on two rules. The first rule is that any two nodes of the same modality in the same conversation are connected. The second rule is that each node is connected with nodes corresponding to the same utterance but from different modalities. Following  [18] , edge weights are computed as:\n\n, where sim(•) is cosine similarity function.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Fusion Module",
      "text": "Based on the graph, we improve  [17]  with gating mechanisms to fuse multimodal context features in the conversation. We utilize graph convolution operation to aggregate context information of both inter-and intra-modality in a specific semantic space at each layer. Meanwhile, inspired by  [15] , we leverage gating mechanisms to learn intrinsic sequential patterns of contextual information in different semantic spaces. The updating process using gating mechanisms is defined as:\n\nwhere\n\no refer to the update gate, the forget gate, and the output gate in the k-th layer, respectively. g (0) is initialized with zero. reads selectively for passing into a graph convolution operation. Following  [16] , the modified convolution operation can be defined as:\n\nwhere P = D-1/2 Ã D-1/2 is the graph convolution matrix with the renormalization trick. α, β k are two hyperparameters. β k = log( ρ k + 1). ρ is also a hyperparameter. W (k) is the weight matrix. H (0) is initialized with X a , X v , X t . I n is an identity mapping matrix. Then, the output of k-th layer can be computed as,",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Classifier",
      "text": "After the stack of K layers, representations of three modalities for each utterance i can be refined as o a i , o v i , o t i . Finally, a classifier is used to predict the emotion of each utterance:\n\nwhere W z and b z are trainable parameters. We apply crossentropy loss along with L2-regularization to train the model:\n\nwhere L is the total number of samples in the training set. τ (i) is the number of utterances in sample i. y l i,j and ŷl i,j denote the one-hot vector and probability vector for emotion class j of utterance i of sample l, respectively. Θ refers to all trainable parameters. η is the L2-regularization weight.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "IEMOCAP  [19]  contains dyadic conversation videos between pairs of ten unique speakers. It includes 7,433 utterances and 151 dialogues. Each utterance is annotated with one of six emotion labels. We follow the previous studies  [5, 13]  that use the first four sessions for training, use the last session for testing, and randomly extract 10% of the training dialogues as validation split. MELD  [2]  contains multi-party conversation videos collected from Friends TV series, where two or more speakers are involved in a conversation. It contains 1,433 conversations, 13,708 utterances and 304 different speakers. Each utterance is annotated with one of seven emotion labels. For a fair comparison, we conduct experiments using the predefined train/validation/test splits in MELD.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison Methods",
      "text": "TFN  [9]  and LMF  [10]  make non-temporal multimodal fusion by tensor product. MFN  [11]  synchronizes multimodal sequences using a multi-view gated memory. bc-LSTM  [6]  leverages an utterance-level LSTM to capture multimodal features. ICON  [7] , an extension of CMN  [20] , provides conversational features from modalities by multi-hop memories. DialogueRNN  [4]  introduces a recurrent network to track speaker states and context during the conversation.\n\nDialogueCRN  [3]  designs multi-turn reasoning modules to understand conversational context. DialogueGCN  [5]  utilizes graph structures to combine contextual dependencies. MMGCN  [13]  uses a graph-based fusion module to capture intra-and inter-modality contextual features. All baselines are reproduced under the same environment, except  [7] , which is only applicable for dyadic conversation and the results are from the original paper. Because  [3] [4] [5]  are designed for unimodal ERC, a early concatenation fusion is introduced to capture multimodal features in their implementations. Implementation Details. Following  [13] , raw utterancelevel features of acoustic, visual, and textual modality are extracted by TextCNN  [21] , OpenSmile  [22] , and DenseNet  [23] , respectively. We use focal loss  [24]  for training due to the class imbalance. The number of layers K are 16 and 32 for IEMOCAP and MELD. α is set to 0.2 and ρ is set to 0.5.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "Overall Results and Ablation Study. The overall results are reported in Table  1 . MM-DFN consistently obtains the best performance over the comparison methods on both datasets, which shows the superiority of our model. Table  2  shows ablation studies by removing key components of the proposed model. When removing either the graph-based dynamic fusion (GDF) module or speaker embedding (Speaker), the results decline significantly on both datasets. When further removing the context embedding (Context), the results decrease further. It shows the effectiveness of the three components.\n\nComparison with Different Fusion Modules. After the modality encoder, we replace GDF with the following six fusion modules: Concat/Gate Fusion, Tensor/Memory Fusion  [10, 11] , Early/Late Fusion + GCN  [13] , and Graphbased Fusion (GF)  [13] . From Table  3 , GF and GDF outperform all fusion modules in the first block since the two  els on both datasets. Under unimodal types, textual modality performs better than acoustic and visual. Under bimodal types, GDF outperforms GF consistently. It again confirms the superiority of GDF. Meanwhile, under acoustic and textual modalities (A+T), both GF and GDF achieve the best performance over other bimodal types, which indicates a stronger complementarity between rich textual semantics and affective audio features. GDF can reduce redundancy as well as enhance complementarity between modalities and thus obtain better results. Moreover, under acoustic and visual modalities (A+V), GDF outperforms GF by a large margin. This phenomenon reflects that the acoustic and visual features have high entanglement and redundancy, limiting the performance of GF. Our GDF encourages disentangling and reduces redundancy by controlling information flow between modalities, accordingly obtaining better fusion representations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes a Multimodal Dynamic Fusion Network (MM-DFN) to fully understand conversational context for multimodal ERC task. A graph-based dynamic fusion (GDF) module is designed to fuse multimodal features in a conversation. The stack of GDFs learns dynamics of contextual information in different semantic spaces, successfully reducing redundancy and enhancing complementarity between modalities. Extensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of MM-DFN.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of the proposed MM-DFN. Given input multimodal features, modality encoder ﬁrst captures features",
      "page": 2
    },
    {
      "caption": "Figure 1: 2.1. Modality Encoder",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Results of graph-based fusion methods under dif-",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated",
          "MELD": "Neutral\nSurprise\nSadness\nHappy\nAnger"
        },
        {
          "Methods": "TFN [9]\nLMF [10]\nMFN [11]\nbc-LSTM [6]\nICON [7]\nDialogueRNN [4]\nDialogueCRN [3]\nDialogueGCN [5]\nMMGCN [13]",
          "IEMOCAP": "37.26\n65.21\n51.03\n54.64\n58.75\n56.98\n37.76\n66.53\n52.39\n57.53\n58.41\n59.27\n48.19\n73.41\n56.28\n63.04\n64.11\n61.82\n33.82\n78.76\n56.75\n64.35\n60.25\n60.75\n32.80\n74.40\n60.60\n68.20\n68.40\n66.20\n32.20\n80.26\n57.89\n62.82\n73.87\n59.76\n53.23\n83.37\n62.96\n66.09\n75.40\n66.07\n51.57\n80.48\n57.69\n53.95\n72.81\n57.33\n45.14\n77.16\n64.36\n68.82\n74.71\n61.40",
          "MELD": "77.43\n47.89\n18.06\n51.28\n44.15\n76.97\n47.06\n21.15\n54.20\n46.64\n77.27\n48.29\n23.24\n52.63\n41.32\n75.66\n48.57\n22.06\n52.10\n44.39\n-\n-\n-\n-\n-\n76.97\n47.69\n20.41\n50.92\n45.52\n77.01\n50.10\n26.63\n52.77\n45.15\n75.97\n46.05\n19.60\n51.20\n40.83\n26.74\n76.33\n48.15\n53.02\n46.09"
        },
        {
          "Methods": "MM-DFN",
          "IEMOCAP": "66.42∗\n69.77∗\n75.56∗\n66.33∗\n42.22\n78.98",
          "MELD": "77.76∗\n50.69∗\n54.78∗\n47.82∗\n22.93"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Results of graph-based fusion methods under dif-",
      "data": [
        {
          "Methods": "MM-DFN\n- w/o GDF - w Speaker\n- w Context\n- w GDF\n- w/o Speaker\n- w Context\n- w/o GDF - w/o Speaker\n- w Context\n- w/o GDF - w/o Speaker\n- w/o Context",
          "IEMOCAP": "68.18\n63.80\n66.89\n62.90\n54.81",
          "MELD": "59.46\n58.50\n58.45\n58.50\n58.08"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Results of graph-based fusion methods under dif-",
      "data": [
        {
          "Modality": "",
          "IEMOCAP": "GDF\nGF",
          "MELD": "GDF\nGF"
        },
        {
          "Modality": "A / V / T",
          "IEMOCAP": "-\n47.79 / 27.46 / 61.07",
          "MELD": "-\n42.72 / 32.34 / 56.95"
        },
        {
          "Modality": "A + V\nA + T\nV + T\nA + V + T",
          "IEMOCAP": "54.73\n56.35\n65.03\n65.41\n62.07\n62.63\n67.02\n68.18",
          "MELD": "42.74\n44.67\n57.85\n58.34\n57.78\n58.49\n58.54\n59.46"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Results of graph-based fusion methods under dif-",
      "data": [
        {
          "Fusion Modules": "Concat / Gate Fusion\nTensor / Memory Fusion\nEarly / Late Fusion + GCN",
          "IEMOCAP": "63.80 / 64.30\n61.05 / 65.51\n64.19 / 65.34",
          "MELD": "58.50 / 57.87\n58.54 / 58.48\n58.69 / 58.43"
        },
        {
          "Fusion Modules": "Graph-based Fusion (GF)\n- w/o Inter-Modal\n- w Intra-Modal\n- w Inter-Modal\n- w/o Intra-Modal",
          "IEMOCAP": "67.02\n66.91\n66.11",
          "MELD": "58.54\n58.53\n58.29"
        },
        {
          "Fusion Modules": "Graph-based Dynamic Fusion (GDF)\n- w/o Inter-Modal\n- w Intra-Modal\n- w Inter-Modal\n- w/o Intra-Modal",
          "IEMOCAP": "68.18\n67.82\n66.22",
          "MELD": "59.46\n59.15\n58.31"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Yukun Ma",
        "Linh Khanh",
        "Frank Nguyen",
        "Xing"
      ],
      "year": "2020",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "3",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "4",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "5",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "6",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "EMNLP/IJCNLP"
    },
    {
      "citation_id": "7",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "8",
      "title": "ICON: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "9",
      "title": "CONSK-GCN: conversational semantic-and knowledge-oriented graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Yahui Fu",
        "Shogo Okada",
        "Longbiao Wang"
      ],
      "year": "2021",
      "venue": "ICME"
    },
    {
      "citation_id": "10",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "11",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan"
      ],
      "year": "2018",
      "venue": "ACL"
    },
    {
      "citation_id": "12",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Learning what and when to drop: Adaptive multimodal and contextual dynamics for emotion recognition in conversation",
      "authors": [
        "Feiyu Chen",
        "Zhengxiao Sun",
        "Deqiang Ouyang"
      ],
      "venue": "ACM Multimedia, 2021"
    },
    {
      "citation_id": "14",
      "title": "MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao"
      ],
      "year": "2021",
      "venue": "ACL/IJCNLP"
    },
    {
      "citation_id": "15",
      "title": "Multimodal emotion recognition with capsule graph convolutional based representation fusion",
      "authors": [
        "Jiaxing Liu",
        "Sen Chen",
        "Longbiao Wang"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Long shortterm memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "17",
      "title": "Simple and deep graph convolutional networks",
      "authors": [
        "Ming Chen",
        "Zhewei Wei",
        "Zengfeng Huang"
      ],
      "year": "2020",
      "venue": "ICML"
    },
    {
      "citation_id": "18",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "Thomas Kipf",
        "Max Welling"
      ],
      "year": "2017",
      "venue": "ICLR (Poster)"
    },
    {
      "citation_id": "19",
      "title": "Fusing document, collection and label graph-based representations with word embeddings for text classification",
      "authors": [
        "Konstantinos Skianis",
        "Fragkiskos Malliaros",
        "Michalis Vazirgiannis"
      ],
      "year": "2018",
      "venue": "TextGraphs@NAACL-HLT"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "21",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh"
      ],
      "year": "2018",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "22",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "23",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "W Björn",
        "Anton Schuller",
        "Stefan Batliner",
        "Steidl"
      ],
      "year": "2011",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "24",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "25",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick"
      ],
      "year": "2017",
      "venue": "ICCV"
    }
  ]
}