{
  "paper_id": "2508.18092v1",
  "title": "Speech-Based Depressive Mood Detection In The Presence Of Multiple Sclerosis: A Cross-Corpus And Cross-Lingual Study",
  "published": "2025-08-25T14:54:13Z",
  "authors": [
    "Monica Gonzalez-Machorro",
    "Uwe Reichel",
    "Pascal Hecker",
    "Helly Hammer",
    "Hesam Sagha",
    "Florian Eyben",
    "Robert Hoepner",
    "Björn W. Schuller"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Depression commonly co-occurs with neurodegenerative disorders like Multiple Sclerosis (MS), yet the potential of speech-based Artificial Intelligence for detecting depression in such contexts remains unexplored. This study examines the transferability of speechbased depression detection methods to people with MS (pwMS) through cross-corpus and cross-lingual analysis using English data from the general population and German data from pwMS. Our approach implements supervised machine learning models using: 1) conventional speech and language features commonly used in the field, 2) emotional dimensions derived from a Speech Emotion Recognition (SER) model, and 3) exploratory speech feature analysis. Despite limited data, our models detect depressive mood in pwMS with moderate generalisability, achieving a 66% Unweighted Average Recall (UAR) on a binary task. Feature selection further improved performance, boosting UAR to 74%. Our findings also highlight the relevant role emotional changes have as an indicator of depressive mood in both the general population and within PwMS. This study provides an initial exploration into generalising speech-based depression detection, even in the presence of co-occurring conditions, such as neurodegenerative diseases.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Depression is the most common psychiatric mood disorder  (World Health Organization, 2023) . Its prevalence is around 5% worldwide  (World Health Organization, 2023) . Despite its prevalence, depression often goes untreated  (Johnson et al., 2022)  due to factors such as socioeconomic barriers and a shortage of healthcare professionals  (Evans-Lacko et al., 2018) .\n\nSpeech-based Artificial Intelligence (AI) methods offer a promising approach for fast and noninvasive screening of neurological and mental health during routine examinations  (Milling et al., 2022; Hecker et al., 2022) , leveraging speech changes like reduced pitch, slower speaking rate, and articulation errors, which are common in individuals with depression  (Cummins et al., 2015) . These methods are accessible, scalable, and could enhance help-seeking behaviour and on-going monitoring  (Johnson et al., 2022) .\n\nPrior work has utilised Machine Learning (ML) methods to detect depression using acoustic and linguistic features  (Kappen et al., 2023) .  Mallol-Ragolta et al. (2019)  trained a Recurrent Neural Network (RNN) on linguistic features for binary classification on the Distress Analysis Interview Corpus from the Wizard-of-Oz interviews (DAIC-WoZ) dataset, achieving an F1 score of 63%.  Zhang et al. (2024)  used wav2vec 2.0 for feature extraction and a Long Short-Term Memory (LSTM) network for binary classification using the DAIC-WoZ dataset, which yielded a 79% F1 score.\n\nSimilar work has also been conducted in other languages, such as for the German language,  Menne et al. (2024)  reported a balanced accuracy 88% for predicting depressive disorder against healthy controls using acoustic information, and for Italian language, in which  Tao et al. (2023)  reported an F1 score of 85% on the binary task of identifying depression using speech information from a reading task.\n\nAutomatic Speech Emotion Recognition (SER) research has also been effective in depression detection  (Wang et al., 2020) , for instance,  Wang et al. (2021)  developed a SER model on the DAIC-WoZ dataset for binary classification, reporting a 60% F1 score.\n\nDepression is a common co-morbidity among people with neurodegenerative diseases, such as Multiple Sclerosis (MS), Parkinson's Disease (PD), and Alzheimer's Disease (AD), among others  (Brenes, 2007) , worsening both the Quality of Life (QoL) and disease prognosis  (Hussain et al., 2020) . In MS, for example, the lifetime risk of depression is estimated around 50%  (Arnett et al., 2008) . The overlapping symptomatology of the two conditions can lead to misdiagnosis, with either one of them frequently overlooked  (Hussain et al., 2020) . While prior research highlights the potential of speech-based AI methods for depression detection  (Cummins et al., 2015) , further work is needed to assess their transferability in patients with neurodegenerative diseases like MS.\n\nHowever, MS, due to its impact on the central nervous system, frequently leads to speech impairment, primarily dysarthria  (Noffs et al., 2018) . As a result, MS speech typically presents irregular articulatory breakdowns, distorted vowels, pitch breaks, harsh voice quality, and slow speaking rate  (Noffs et al., 2018) . This raises the question of whether speech-based depression detection can distinguish depressive symptoms in people with a coexisting speech impairment, such as dysarthria, due to a neurodegenerative disease, such as MS. We hypothesise that these methods would struggle to generalise and distinguish depressive symptoms in people with MS (pwMS), since some of the MS speech characteristics are similar to those found in people with depression.\n\nThis contribution aims to address this challenge by assessing the performance of common speechbased methods for depressive mood detection in pwMS. To do so, we conduct a cross-corpus and cross-lingual analysis using a well-known Englishlanguage corpus with depressive mood assessments, along with a German-language dataset of people with low MS disability, who also underwent depressive mood assessments. Our research questions are:\n\n1. Do ML methods for depressive mood detection generalise to depressive mood detection in pwMS?\n\n2. Given that SER models have shown promise in detecting emotional changes  (Wang et al., 2021) , which output from a fine-tuned SER model is more effective for depression detection: the model's final results (the classification or regression head output from a SER model) corresponding to the emotional dimensions -arousal, valence, and dominance-or the model's contextualised representations?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Can Exploratory Feature Selection Analysis Improve Generalisability Of Depression Detection In Pwms?",
      "text": "This contribution is structured as follows. Section 2 introduces the datasets, features, and methods employed. Sections 3, 4, 5 present the results, limitations, and discussions. Finally, section 6 draws conclusions from the analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Materials And Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "We employ two datasets: 1) The DAIC-WoZ depression dataset in English presented in  (Gratch et al., 2014) , and 2) a Swiss German dataset for pwMS collected under the scope of the COMMIT-MENT trial  (Gonzalez-Machorro et al., 2023) . The trial protocol was approved by national regulatory authorities and local ethic committee (BASEC-ID number 2021-02423) and registered on clinicaltrials.gov (NCT05561621). The DAIC-WoZ is a collection of semi-structured interviews containing speech samples of 189 participants  (Gratch et al., 2014) . It provides predefined speaker-independent training, development, and testing sets, and is segmented at the turn level  (Valstar et al., 2016) . The dataset includes scores from the Patient Health Questionnaire-8 (PHQ-8) self-assessed depression questionnaire.\n\nThe COMMITMENT (Prediction of Non-motor Symptoms in Fully Ambulatory MS Patients Using Vocal Biomarkers) dataset consists of 50 fully ambulatory pwMS and 20 control participants. Participants with MS have low levels of disability, with a median Expanded Disability Status Scale (EDSS) score of 1.0-indicating minimal impairment-and a min/max EDSS score of 0.0/3.0, which indicates no disability to moderate disability but still walking unaided. For this paper, we only use the MS cohort. Details on the speech recordings are described in  (Gonzalez-Machorro et al., 2023) . Depressive mood scores for each participant are available using the Beck Depression Inventory-II (BDI-II) questionnaire. The dataset contains multiple speech tasks. However, in this paper, we utilise two spontaneous speech tasks from each patient: (1) describing the weather on the day of recording and (2) recalling a neutral memory prompted by the word \"grass\". These tasks are chosen because they elicit spontaneous speech and resemble the interview style of the DAIC-WoZ dataset. Data was collected using the AISoundLab web platform, which is a web app, in which each patient could navigate through a voice recording session under the supervision of a study nurse  (Gonzalez-Machorro et al., 2023) . All participants provided informed consent prior to participation, and all data was pseudoanonymised to protect patient privacy. The ethics consent unfortunately does not permit the publication of the recorded data.\n\nIn this paper, participants from the two datasets are categorised as having depression or no depression based on clinically validated threshold scores from two depression questionaries (BDI-II and PHQ-8). For the PHQ-8, participants with a score of 10 or higher are classified as having depression  (Kroenke et al., 2001; Dhingra et al., 2011) ; and for the BDI-II participants with a score higher than 19 were defined as having depression  (Beck et al., 1961) . It is important to keep in mind that these scores serve as indicators of depressive symptoms rather than definitive clinical diagnoses of depression.\n\nAudio files are downsampled to 16 kHz. Diarisation for the DAIC-WoZ data is performed using the turn-level segments provided for each speaker. A Voice Activity Recognition (VAD) algorithm 1 is applied to segment audio files from both datasets, which due to license restrictions, is not open-source. For consistency with previous work, we employ the same VAD parameter values as in  (Gonzalez-Machorro et al., 2023) . Transcripts are automatically obtained for each VAD segment using Whisper version 2  (Radford et al., 2023)  with the base model for English and German language. For the DAIC-WoZ dataset, we merge the original training and development sets while the original testing set is left intact. The motivation is that due to the small dataset, we opt to use a Cross-Validation (CV) strategy for a more robust evaluation. The COMMIT-MENT dataset, as its purpose is purely for evaluating cross-corpus and cross-lingual generalisation, is not partitioned and it is used as an additional testing set.\n\nTable  1  describes the metadata for both datasets across the different dataset partitions. Missing values for the questionnaires are dropped before processing. Models trained solely on the COMMIT-MENT dataset would likely over-fit due to insufficient participants with depressive symptoms to learn acoustic and linguistic markers of depression. Given the imbalance of the two classes, random oversampling with replacement for the two classes and a random seed of 42 is applied. To do so, we employ the package imbalanced-learn (Lemaître 1 provided by audEERING GmbH",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction",
      "text": "We extract six commonly used acoustic and linguistic feature sets, and normalise them per dataset using the Robust Scaler, which is robust against outliers. All features are extracted at a VAD segmentlevel.\n\n1. The Wav2Vec2 contextualised representations of length 1024 correspond to the mean pooling of the encoder output. These representations are extracted using a publicly available finetuned Wav2Vec2 model for 3-dimensional SER task  (Wagner et al., 2023) .\n\n2. SER-dimensions -arousal, valence, and dominance-are obtained using the same Wav2Vec2 SER model  (Wagner et al., 2023) . These features represent the final outputs of the model returned by the 2-layer multitask regression head  (Wagner et al., 2023) . By extracting both types of information -the contextualised representations and the emotion dimensions-from the Wav2Vec2 SER model, we aim to investigate which one is more effective for depression detection.\n\n3. Praat features (Feinberg, 2022) are extracted using Nkululeko  (Burkhardt et al., 2022)  and correspond to 39 features, such as voice quality, shimmer, jitter, and duration. This type of features has shown significance for depression detection  (Cummins et al., 2015) .\n\n4. extended Geneva minimalistic acoustic parameter set (eGeMAPS)  (Eyben et al., 2016)  is extracted using the Speech & Music Interpretation by Large-space Extraction (openSMILE) feature extraction tool  (Eyben et al., 2010) . It contains 22 acoustic features related to prosody, voice quality, and articulation. Previous work has reported promising results in depression detection  (Cummins et al., 2015) . We employ the 88 functionals and summary statistics from these features.\n\n5. The psycholinguistic feature set consists of 51 linguistic features that represent the syntactic complexity, the proportion of sentiment tokens, and the proportion of nouns, verbs, negations, adjectives, among others.\n\n6. RoBERTa embeddings are extracted using a multilingual model -XLM Large RoBERTa  (Conneau et al., 2020)  -. These embeddings correspond to the [CLS] pooling output applied to the last hidden states of the model. Each segment is defined with a maximum length of 512 tokens and represented by a size of 768.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "We define the following three modelling scenarios to investigate whether ML methods for depressive mood detection generalise in the presence of MS: C_A) Models are trained and evaluated on the DAIC-WoZ training and testing sets using selected features. In other words, it is Task A with selected features. This task assesses whether feature selection improves performance within the general population.\n\nC_B) Models are trained on the DAIC-WoZ training set using selected features and evaluated on both the DAIC-WoZ testing set and the COMMITMENT dataset. This scenario, equivalent to Task B with selected features, explores whether feature selection improves generalisability to pwMS data.\n\nExploratory feature analysis. To investigate which features are significant to distinguish between speakers with and without depression in the training set, we use the Mann-Whitney U test (p < 0.05) because it is non-parametric and does not require the assumption of a normal distribution. This makes it suitable for our data, where not all features follow a normal distribution. Additionally, it is more conservative than other statistical tests, reducing the risk of Type I errors. To quantify the effect size, we use Cohen-R  (Cohen, 1988) . Relevant features are found by selecting among the significant ones those with an r ≥ .30. Corrections for Type 1 errors are not performed due to the large size of the feature sets, so that the aim of this analysis is restricted to explore acoustic and linguistic feature trends.\n\nModelling. We implement supervised ML classification for implementing the three modelling tasks. For reproducibility, we seed the pseudorandom number generation. The models used are Support Vector Machine (SVM), Random Forest (RF), and eXtreme Gradient Boosting (XGB). These supervised learning algorithms were selected due to their consistently strong performance across a wide range of classification tasks  (Fernández-Delgado et al., 2014) . Each model is trained using Grid search 5-fold speaker-independent CV on the training set.\n\nThe hyper-parameter values optimised for the Grid Search for each model are as follows: for SVM, C ∈ [10 -4 , 10 -3 , 10 -2 , 10 -1 , 1, 10], the kernel options include linear and rbf, and the gamma parameter is chosen from scale and auto. For XGB, the number of estimators ∈  [200, 300, 450, 500] , the learning rate ∈ [0.001, 0.01, 0.1, 0.2], the maximum tree depth ∈ [4, 5, 6], the column subsample ratio ∈ [1, 0.3, 0.5], and the subsample ratio ∈ [0.8, 1]. Lastly, for the RF model, the number of estimators ∈ [50, 100, 300, 500, 800, 1000], the criterion is either gini or entropy, the minimum number of samples required to split an internal node is ∈ [2, 3], and bootstrap sampling is either True or False.\n\nThe optimal hyper-parameters identified through this process are then used to train the model on the entire training set. Class weights are calculated from the training set and are incorporated to address the class imbalance in the data.\n\nEvaluation. We calculate speaker-level Unweighted Average Recall (UAR), F1-score, precision, and recall. Receiver Operating Characteristic (ROC) curves and the Area Under the Curve (AUC) scores were also calculated at a speaker-level. Due to space limitations, only the ROC curves for the best-performing tasks are presented. We also compute the 95% Confidence Interval (CI) for the UAR. The CIs were calculated using 1000 bootstrapping iterations 2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Exploratory Feature Analysis",
      "text": "The Mann-Whitney U test is applied to each feature in the training set of the DAIC-WoZ dataset. Due to interpretability limitations, the Wav2Vec2 and the RoBERTa representations are excluded from the analysis. The number of significant (p < 0.05) features with a sufficiently high effect size (r ≥ 0.30) identified per feature set are: 1) SER-dimensions: 1 feature-valence-; 2) Praat features: 33 out of 39 features; 3) eGeMAPS: 64 out of 88 functionals; 4) Psycholinguistic feature set: 18 out of 51 features. These selected features are used in the modelling task C_A and C_B to assess whether feature selection improves modelling performance. Figure  1  shows the valence distributions for the binary depression class (\"no_depression\" and \"depression\"), which is the only significant features found for the SERdimensions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modelling Results",
      "text": "Table  2  shows UAR and its CIs, F1-score, precision, and recall for depression (Dep.) and no depression (No Dep.) classes, across the best-performing models and all feature sets. As we are tackling a binary classification problem, the chance-level UAR is 50%. The best result for Task A (Baseline Performance) with acoustic features is achieved using SVM and SER-dimensions (UAR: 73%), while the best result with linguistic features is achieved using SVM and RoBERTa embeddings (UAR: 56%). For Task B (Generalisability Evaluation), Wav2Vec2 embeddings and Psycholinguistic features achieved the best performances (UAR: 66% and 62%, respectively). SER-dimensions in Task B show a performance drop. For Tasks C_A and C_B (Feature Selection Modelling on Tasks A and B), XGB with SER-dimensions obtained the highest UARs of 79% and 74%, respectively. Since SERdimensions shows consistently good performance in all tasks, Figure  2  shows the ROC curves and AUC values for all tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Discussion",
      "text": "In this paper, we explore three research questions: 1) Do ML methods for depressive mood detection generalise to depressive mood detection in pwMS? Results in Table  2  indicate that for Task B (Generalisability Evaluation), acoustic-based features show reasonable generalisability to distinguish depression in pwMS, with only a modest performance decline compared to results from Task A (Baseline Performance).\n\nIn the case of the Wav2Vec2 features, a drop in   performance for the two tasks is not found, which suggests that these features are transferable to other languages and groups with other co-morbidities such as MS. Interestingly, in the case of the eGeMAPS features, a minimal increase in performance is observed in Task B, which also suggests a generalisability capacity.\n\nFor Tasks C_A and C_B (Feature Selection Modelling), similar patterns are observed as in Tasks A and B, with SER-dimensions consistently outperforming other feature sets and demonstrating strong transferability in detecting depression among pwMS. This is further illustrated in Figure  2 , which highlights the effectiveness of SERdimensions in the context of MS.\n\nThe top-performing results for Tasks A (using SER-dimensions) and B (using Wav2Vec2 features) demonstrate greater precision in predicting the absence of depression (90% for \"No Dep.\" in Task A; 88% for \"No Dep.\" in Task B) compared to predicting depression. This finding indicates that identifying depression using speech presents similar challenges in both same-language and crosslingual contexts, as well as in the general population and among groups with co-morbidities, such as MS.\n\nInterestingly, RF models did not outperform XGB or SVM in any task or feature set; consequently, they are excluded from Table  2 . This was already reported by  (Fernández-Delgado et al., 2014) , where XGB has been shown to outperform RF in many cases.\n\n2) Given that SER models have shown promise in detecting emotional changes, which output from a fine-tuned SER model is more effective for depression detection: the model's final predictions corresponding to the emotional dimensions or the model's contextualised representations? As shown in Table  2 , the SER-dimensions and Wav2Vec2 representations achieve the highest UAR for Task A and Task B, respectively. SER-dimensions also outperform all other feature sets in Task C reaching the highest performance. Likely due to the high dimensionality of the Wav2Vec2 embeddings, SER-dimensions show overall better results by a small margin. However, the performance of SERdimensions and Wav2Vec2 features heavily relies on the performance of the underlying SER model  (Wagner et al., 2023) , which was finetuned using the MSP-Podcast dataset (English language) (Lotfian and Busso, 2019). It is, therefore, unclear the cross-lingual generalisability of these features when training data would include languages other than English.\n\n3) Can feature selection improve generalisability of depression detection in pwMS? Results for acoustic-feature-based models, with the exception of the Praat features, suggest that indeed, feature selection can improve the performance of depression detection. The feature analysis for SERdimensions reveals that only valence among the three dimensions is significantly predictive, highlighting its important role as an indicator of depression in both the general population and pwMS. This finding is illustrated in Figure  2 , which shows that individuals without depressive symptoms tend to use higher positive valence in spontaneous interviews compared to those with depressive symptoms. This aligns with prior research, such as  (Trifu et al., 2024) , which found that individuals with de-pression display lower positive valence than those without. This pattern may be attributed to a core symptom of depression: emotional dysfunction characterised by a predominant negative emotional state  (Yang et al., 2023) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Limitations",
      "text": "In the case of text-based models, RoBERTa embeddings achieve above-chance performance in both Task A and Task B while psycholinguistic-featurebased models exhibit an unexpected trend: their performance on Task B surpassed that of Task A, C_A, and C_B. The suboptimal performances of text-based models may be due to the use of VAD segments for feature extraction, which ensured a consistent preprocessing pipeline across acoustic and text features, enabling direct comparisons between model types in detecting depression. While VAD segments effectively captured acoustic cues, contributing to strong performances, their short duration may have been less optimal for text-based features, such as word class proportions, which benefit from longer discourse contexts. The languagespecific nature of these features also might have contributed to their struggle to generalise to the German-speaking MS population. Future work should explore longer segments to optimise textbased models, building on this study's foundation.\n\nA limitation of this contribution arises from the use of different languages, recording conditions, and depression assessments. Although we try to tackle this by feature normalisation and the restriction to spontaneous speech, further research should explore the impact of language, depression assessments, and recording variations on the generalisability of speech-based depression detection. In this paper, we cannot definitively differentiate the extent to which the drop in model performance when evaluated on the MS population is influenced by language differences, recording conditions or the presence of MS itself.\n\nMoreover, since both MS and depression are heterogenous conditions (Gaitán and Correale, 2019), implementing personalised approaches when screening for depression in pwMS is a crucial next step. Future work should also explore different stages of MS -this study focused on low-disability patients-and account for other co-morbidities in MS, like fatigue and cognitive decline, which may also influence speech. Also, the MS cohort was receiving pharmacological treatment, including com-mon antidepressants for those MS patients diagnosed with depression, that could influence mood and, consequently, speech patterns. Although the general population diagnosed with depression from the DAIC-WoZ dataset may also have been undergoing pharmacological treatments, this information is not available in the dataset, preventing analysis of this potential confounding factor.\n\nTo further evaluate the transferability of speechbased depression detection, it is important to examine other common diseases where depression is a common co-morbidity and speech is impacted, such as PD or AD. A lack of depression scores in speech datasets for these disorders is a major limitation in this regard. Finally, acoustic and linguistic features alone cannot fully capture the multifaceted nature of depression. These ML methods are intended to augment established screening approaches. Incorporating other bio-signals, such as physiological data, could not only enhance performance but also provide a more comprehensive understanding of the disorder.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this cross-corpus and cross-lingual study, we explore the efficacy of speech-based depressive mood detection in the presence of MS and across English and German languages. Our findings highlight the significance of emotional dimensions -arousal, valence, and dominance-in identifying depressive symptoms, not only in the general population but also within pwMS. Additionally, acoustic feature sets like eGeMAPS also demonstrate potential for generalisability in this context. However, further research is needed to establish robust conclusions. This study, despite its limitations, represents a step forward towards the integration and generalisability of speech-based depression detection methods. Non-invasive speech-based AI systems for depression detection hold the potential to improve the QoL for individuals with this disorder, even in the presence of other illnesses.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Feature distributions for the binary depression",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the ROC curves and",
      "page": 5
    },
    {
      "caption": "Figure 2: ROC curve and AUC value at a speaker-",
      "page": 6
    },
    {
      "caption": "Figure 2: , which shows",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "AUC Tas",
          "Column_13": "k A = 0.831",
          "Column_14": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "AUC Tas\nAUC Tas\nAUC Tas",
          "Column_13": "k B = 0.669\nk C_A = 0.89\nk C_B = 0.77",
          "Column_14": "3\n1"
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "Random",
          "Column_13": "",
          "Column_14": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Depression in multiple sclerosis: Review and theoretical proposal",
      "authors": [
        "Peter Arnett",
        "Fiona Barwich",
        "Joe Beeney"
      ],
      "year": "2008",
      "venue": "Journal of the International Neuropsychological Society",
      "doi": "10.1017/S1355617708081174"
    },
    {
      "citation_id": "2",
      "title": "An inventory for measuring depression",
      "authors": [
        "Aaron Beck",
        "Clyde Ward",
        "John Myer Mendelson",
        "John Mock",
        "Erbaugh"
      ],
      "year": "1961",
      "venue": "Archives of General Psychiatry",
      "doi": "10.1001/archpsyc.1961.01710120031004"
    },
    {
      "citation_id": "3",
      "title": "Anxiety, depression, and quality of life in primary care patients",
      "authors": [
        "Gretchen Brenes"
      ],
      "year": "2007",
      "venue": "Primary care companion to the Journal of clinical psychiatry",
      "doi": "10.4088/pcc.v09n0606"
    },
    {
      "citation_id": "4",
      "title": "Nkululeko: A tool for rapid speaker characteristics detection",
      "authors": [
        "Felix Burkhardt",
        "Johannes Wagner",
        "Hagen Wierstorf",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "2022 Language Resources and Evaluation Conference, LREC 2022"
    },
    {
      "citation_id": "5",
      "title": "Statistical Power Analysis for the Behavioral Sciences, 2nd edition",
      "authors": [
        "Jacob Cohen"
      ],
      "year": "1988",
      "venue": "Statistical Power Analysis for the Behavioral Sciences, 2nd edition",
      "doi": "10.4324/9780203771587"
    },
    {
      "citation_id": "6",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.747"
    },
    {
      "citation_id": "7",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "Nicholas Cummins",
        "Stefan Scherer",
        "Jarek Krajewski",
        "Sebastian Schnieder",
        "Julien Epps",
        "Thomas Quatieri"
      ],
      "year": "2015",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2015.03.004"
    },
    {
      "citation_id": "8",
      "title": "PHQ-8 Days: A Measurement Option for DSM-5 Major Depressive Disorder (MDD) Severity",
      "authors": [
        "S Subash",
        "Kurt Dhingra",
        "Matthew Kroenke",
        "Tara Zack",
        "Lina Strine",
        "Balluz"
      ],
      "year": "2011",
      "venue": "Population Health Metrics",
      "doi": "10.1186/1478-7954-9-11"
    },
    {
      "citation_id": "9",
      "title": "Socio-economic variations in the mental health treatment gap for people with anxiety, mood, and substance use disorders: results from the who world mental health (wmh) surveys",
      "authors": [
        "Sara Evans-Lacko",
        "Sergio Aguilar-Gaxiola",
        "Ahmad Al-Hamzawi",
        "Jordi Alonso",
        "Corina Benjet",
        "Ronny Bruffaerts",
        "Wai Chiu",
        "Silvia Florescu",
        "Giovanni De Girolamo",
        "Oye Gureje",
        "Maria Josep",
        "Yanling Haro",
        "Chiyi He",
        "Elie Hu",
        "Norito Karam",
        "Sing Kawakami",
        "Crick Lee",
        "Viviane Lund",
        "Daphna Kovess-Masfety",
        "Fernando Levinson",
        "Graham Navarro-Mateu",
        "Thornicroft"
      ],
      "year": "2018",
      "venue": "Psychological Medicine",
      "doi": "10.1017/S0033291717003336"
    },
    {
      "citation_id": "10",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "S Shrikanth",
        "Khiet Narayanan",
        "Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2457417"
    },
    {
      "citation_id": "11",
      "title": "opensmile -the munich versatile and fast opensource audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia (ACM MM)",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "12",
      "title": "Parselmouth praat scripts in python",
      "authors": [
        "David R Feinberg"
      ],
      "year": "2022",
      "venue": "Parselmouth praat scripts in python"
    },
    {
      "citation_id": "13",
      "title": "Do we need hundreds of classifiers to solve real world classification problems?",
      "authors": [
        "Manuel Fernández-Delgado",
        "Eva Cernadas",
        "Senén Barro",
        "Dinani Amorim"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "14",
      "title": "Multiple sclerosis misdiagnosis: A persistent problem to solve",
      "authors": [
        "I María",
        "Jorge Gaitán",
        "Correale"
      ],
      "year": "2019",
      "venue": "Frontiers in Neurology",
      "doi": "10.3389/fneur.2019.00466"
    },
    {
      "citation_id": "15",
      "title": "Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features",
      "authors": [
        "Monica Gonzalez-Machorro",
        "Pascal Hecker",
        "Uwe Reichel",
        "N Helly",
        "Robert Hammer",
        "Lisa Hoepner",
        "Alisha Pedrotti",
        "Hesam Zmutt",
        "Johan Sagha",
        "Florian Van Beek",
        "Dagmar Eyben",
        "Björn Schuller",
        "Bert Schuller",
        "Arnrich"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023",
      "doi": "10.21437/Interspeech.2023-1759"
    },
    {
      "citation_id": "16",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "Jonathan Gratch",
        "Ron Artstein",
        "Gale Lucas",
        "Giota Stratou",
        "Stefan Scherer",
        "Angela Nazarian",
        "Rachel Wood",
        "Jill Boberg",
        "David Devault",
        "Stacy Marsella",
        "David Traum",
        "Skip Rizzo",
        "Louis-Philippe Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the Language Resources and Evaluation Conference (LREC)"
    },
    {
      "citation_id": "17",
      "title": "Voice analysis for neurological disorder recognition-a systematic review and perspective on emerging trends",
      "authors": [
        "Pascal Hecker",
        "Nico Steckhan",
        "Florian Eyben",
        "Björn Schuller",
        "Bert Arnrich"
      ],
      "year": "2022",
      "venue": "Front. Digit. Health",
      "doi": "10.3389/fdgth.2022.842301"
    },
    {
      "citation_id": "18",
      "title": "Similarities between depression and neurodegenerative diseases: Pathophysiology, challenges in diagnosis and treatment options",
      "authors": [
        "Madiha Hussain",
        "Prabhat Kumar",
        "Sara Khan",
        "Safeera Domonick K Gordon",
        "Khan"
      ],
      "year": "2020",
      "venue": "Cureus",
      "doi": "10.7759/cureus.11613"
    },
    {
      "citation_id": "19",
      "title": "Technology-based interventions to improve help-seeking for mental health concerns: A systematic review",
      "authors": [
        "Jemimah Johnson",
        "Prachi Sanghvi",
        "Seema Mehrotra"
      ],
      "year": "2022",
      "venue": "Indian J. Psychol. Med",
      "doi": "10.1177/02537176211034578"
    },
    {
      "citation_id": "20",
      "title": "Speech as a promising biosignal in precision psychiatry",
      "authors": [
        "Marie-Anne Mitchel Kappen",
        "George Vanderhasselt",
        "Slavich"
      ],
      "year": "2023",
      "venue": "Neuroscience & Biobehavioral Reviews",
      "doi": "10.1016/j.neubiorev.2023.105121"
    },
    {
      "citation_id": "21",
      "title": "The phq-9: Validity of a brief depression severity measure",
      "authors": [
        "Kurt Kroenke",
        "Robert Spitzer",
        "Janet Williams"
      ],
      "year": "2001",
      "venue": "Journal of General Internal Medicine",
      "doi": "10.1046/j.1525-1497.2001.016009606.x"
    },
    {
      "citation_id": "22",
      "title": "Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning",
      "authors": [
        "Guillaume Lemaître",
        "Fernando Nogueira",
        "Christos Aridas"
      ],
      "year": "2017",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "23",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2736999"
    },
    {
      "citation_id": "24",
      "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
      "authors": [
        "Adria Mallol-Ragolta",
        "Ziping Zhao",
        "Lukas Stappen",
        "Nicholas Cummins",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech",
      "doi": "10.21437/Interspeech.2019-2036"
    },
    {
      "citation_id": "25",
      "title": "The voice of depression: speech features as biomarkers for major depressive disorder",
      "authors": [
        "Felix Menne",
        "Felix Dörr",
        "Julia Schräder",
        "Johannes Tröger",
        "Alexandra König",
        "Lisa Wagel"
      ],
      "year": "2024",
      "venue": "BMC Psychiatry",
      "doi": "10.1186/s12888-024-06253-6"
    },
    {
      "citation_id": "26",
      "title": "Is speech the new blood? recent progress in ai-based disease detection from audio in a nutshell",
      "authors": [
        "Manuel Milling",
        "Florian Pokorny",
        "D Katrin",
        "Björn Bartl-Pokorny",
        "Schuller"
      ],
      "year": "2022",
      "venue": "Frontiers in Digital Health",
      "doi": "10.3389/fdgth.2022.886615"
    },
    {
      "citation_id": "27",
      "title": "What speech can tell us: A systematic review of dysarthria characteristics in multiple sclerosis",
      "authors": [
        "Gustavo Noffs",
        "Thushara Perera",
        "Scott Kolbe",
        "Camille Shanahan",
        "M Frederique",
        "Andrew Boonstra",
        "Helmut Evans",
        "Anneke Butzkueven",
        "Walt Van Der",
        "Adam Vogel"
      ],
      "year": "2018",
      "venue": "Autoimmunity Reviews",
      "doi": "10.1016/j.autrev.2018.06.010"
    },
    {
      "citation_id": "28",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning",
      "doi": "10.5555/3618408.3619590"
    },
    {
      "citation_id": "29",
      "title": "The androids corpus: A new publicly available benchmark for speech based depression detection",
      "authors": [
        "Fuxiang Tao",
        "Anna Esposito",
        "Alessandro Vinciarelli"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2023-894"
    },
    {
      "citation_id": "30",
      "title": "Linguistic markers for major depressive disorder: a cross-sectional study using an automated procedure",
      "authors": [
        "Raluca Nicoleta Trifu",
        "Bogdan Nemes",
        "Cristina Herta",
        "Carolina Bodea-Hategan",
        "Dorina Anca Talas",
        "Horia Coman"
      ],
      "year": "2024",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2024.1355734"
    },
    {
      "citation_id": "31",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Maja Pantic",
        "Jonathan Gratch",
        "Björn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge",
      "doi": "10.1145/2988257.2988258"
    },
    {
      "citation_id": "32",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3263585"
    },
    {
      "citation_id": "33",
      "title": "Depression speech recognition with a threedimensional convolutional network",
      "authors": [
        "Hongbo Wang",
        "Yu Liu",
        "Xiaoxiao Zhen",
        "Xuyan Tu"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience",
      "doi": "10.3389/fnhum.2021.713823"
    },
    {
      "citation_id": "34",
      "title": "Human emotion recognition by optimally fusing facial expression and speech feature. Signal Processing: Image Communication",
      "authors": [
        "Xusheng Wang",
        "Xing Chen",
        "Congjun Cao"
      ],
      "year": "2020",
      "venue": "Human emotion recognition by optimally fusing facial expression and speech feature. Signal Processing: Image Communication"
    },
    {
      "citation_id": "35",
      "title": "Depression",
      "year": "2023",
      "venue": "Depression"
    },
    {
      "citation_id": "36",
      "title": "Emotion-dependent language featuring depression",
      "authors": [
        "Chaoqing Yang",
        "Xinying Zhang",
        "Yuxuan Chen",
        "Yunge Li",
        "Shu Yu",
        "Bingmei Zhao",
        "Tao Wang",
        "Lizhu Luo",
        "Shan Gao"
      ],
      "year": "2023",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry",
      "doi": "10.1016/j.jbtep.2023.101883"
    },
    {
      "citation_id": "37",
      "title": "Improving speech depression detection using transfer learning with wav2vec 2.0 in low-resource environments",
      "authors": [
        "Xu Zhang",
        "Xiangcheng Zhang",
        "Weisi Chen",
        "Chenlong Li",
        "Chengyuan Yu"
      ],
      "year": "2024",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-024-60278-1"
    }
  ]
}