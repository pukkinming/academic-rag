{
  "paper_id": "2402.11571v1",
  "title": "Ain'T Misbehavin' -Using Llms To Generate Expressive Robot Behavior In Conversations With The Tabletop Robot Haru",
  "published": "2024-02-18T12:35:52Z",
  "authors": [
    "Zining Wang",
    "Paul Reisert",
    "Eric Nichols",
    "Randy Gomez"
  ],
  "keywords": [
    "• Human-centered computing → Interactive systems and tools",
    "Natural language interfaces",
    "• Computer systems organization → Robotics Human-Robot Interaction, Social Robotics, Large Language Models, Expressive Behavior Generation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emo-text Behavior TTS \n LLM SERVER Character card ! Awww! You really know how to cheer me up! I love robots! \n Interaction Manager Figure  1 : Our proposed approach generates conversation responses with expressive robot behavior directly from the LLM.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Llm-Driven Conversations",
      "text": "Recent advancements in LLMs  [25]  offer great possibilities for social robotics  [20, 26, 27] . These models excel in conducting natural, human-like dialog, as evident in applications like chatbots and virtual agents  [24] . However, unlike virtual agents, social robots possess physical embodiment and distinct personalities. Their expressiveness is key to conveying believable emotions  [12, 21]  and facilitating effective human-robot social interactions  [3] . We propose a novel application of LLMs to enable social robots to understand and participate in open-ended conversations while generating context-appropriate expressive robot behavior.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Target Llm",
      "text": "1.1.1 Llama 2. This is an LLM developed by Meta and released as open-source. Llama-2-70B-chat, a model optimized for dialog applications, was shown to achieve comparable performance to ChatGPT  [2] , outperforming other open-source models in various quality-and safety-related metrics  [22] . After trial evaluation of several model sizes and quantization levels  [8] , we selected the   1 : Example mappings used by Emo-text to generate expressive robot behaviour as shown in Figure  3 . Emoji→Haru routines mappings are N-N, and emotion→vocal genre mappings are N-1.\n\nFigure  2 : Haru the robot expresses itself in a conversation. 4bit GPTQ quantization in order to balance conversation quality, resource requirements, and speed. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Social Robot",
      "text": "Embodied robots have been shown to be more helpful, persuasive, and enjoyable compared to simulated robots  [15, 23] . Similar effects were found in exercise coach for the elderly  [6]  and robot tutor  [14]  scenarios. Other work has highlighted the importance of robot expressivity to positive perception  [3, 7]  and sociality  [1] . Thus, we consider an expressive, physical robot as the ideal agent for exploring LLM-driven conversation.\n\nMotivated by this, we select the tabletop robot Haru  [10, 11] , shown in Figure  2 , for our study. Haru is designed to excel in multimodal communication, using both verbal and non-verbal interaction methods, and its design emphasizes its capacity for conveying emotions with its expressive capabilities  [11] . Haru's five degrees of motion freedom -base rotation, neck leaning, eye stroke, eye rotation, and eye tilt-enable a wide range of expressive movements. Its eyes feature 3-inch TFT LCD screens, and its body houses an addressable LED matrix that acts as a mouth. Haru communicates using a TTS voice and through animated routines. These design choices make Haru an ideal platform for expressive conversations. 1.2.1 Emotive TTS Voice. The voice has long been considered an important modality for conveying emotions  [13, 19] . In particular, the tone of voice has been shown to affect people's attitude towards robots as well as their levels of engagement and interest in a given interaction  [4, 5] . Additionally, a robot's vocal expressiveness is correlated with perceived social presence  [13, 18] .\n\nNichols et al.  [17, 18]  argue that the TTS voices for existing social robots lack emotive range and develop an expressive TTS voice for 1 We run all trials on a desktop PC with dual RTX 3090 GPUs and 48GB VRAM total.\n\nHaru using the iterative refinement process of  [17]  as described in  [18] . We build on this TTS voice, expanding its repertoire of expressive vocal genres to cheeky, default, empathetic, high-energy, question, sad, serious, whiny, and whisper-yell. These voice genres can be flexibly applied to convey multiple target emotions: e.g. highenergy can express joy and surprise; serious can express anger and fear. We describe the voice application in Section 2.2.2. 1.2.2 Physical Actions. Haru was designed with a library of expressive physical actions known as Haru routines, comprised of over 140 routines that were designed by professional animators and acted out through Haru's hardware  [9] . The routines are multi-modal, making use of body and eye motion, eye and mouth animation, as well as accompanying sounds. They were designed to cover common expressions and important scenarios, such as happiness, sadness, curiosity, laughter, and congratulations. Example routines are shown in Table  1  and Figure  2 . We describe how Haru routines are dynamically generated in Section 2.2.3.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Implementation",
      "text": "LLM-driven conversations with Haru relies on several modules, as illustrated in Figure  1 . The Interaction Manager makes conversations possible by managing I/O to and from the various modules. Each conversational turn undergoes a multi-step process. First, the user's speech is transcribed using Google's Speech-to-Text API. A fixed window of 3 seconds of silence is used to detect the end of speech. Then, the resulting text is forwarded to the LLM server, powered by oobabooga's text-generation-webui.\n\nWithin the LLM server, two critical operations take place:\n\nLanguage generation: The user's speech and Haru's character card (as described in Section 2.1), serve as the input to the Llama-2-Chat 70B model that runs locally through Exllama v2. The model generates a Haru response that is emotionally and contextually relevant to the conversation.\n\nBehavior generation: Subsequently, Haru's generated response is directed to the Emo-text sub-module, which is responsible for the generation of expressive robot behaviors to enhance the expressiveness of the robot. Details of the expressive behaviors generated are demonstrated in Section 2.2 and Figure  3 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Haru'S Character Card",
      "text": "Prompt engineering  [16]  is a technique for improving the capacity of LLMs on a specific task or application by providing instructions to shape their responses and behavior. Specifically, we want the LLM to have the personality of Haru the robot. Thus, we create a character card as the prompt for the model, steering responses to be consistent with the robot's characteristics, while generating",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emo-Text I Won The Nobel Pr Ize Today. L L M Char Acter Car D",
      "text": "Haru is a youthful and energetic tabletop robot assistant. Haru loves to learn about humanity... Persona: This component captures Haru's energetic and youthful personality, physical appearance, and preferences, including his affection for electricity, desire to learn about humans, and fear of water and magnets. It was constructed using the Haru personality bible  [17] . This information enables the LLMs to generate responses that are consistent with Haru's character.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "User : 'I Won The Nobel Pr Ize Today.' Har U: ' That'S Huge News! I 'M So Proud Of You! '",
      "text": "Example conversations: Their inclusion serves as a pivotal mechanism to guide the LLM in generating appropriate emojis for Haru's utterances, enabling us to generate expressive robot actions directly from the LLM output. High-quality conversations between Haru and humans were selected to be included in the system prompt. Based on preliminary evaluation, we selected a limit of 5 turns per conversation to ensure they would fit in the LLM's context and generated emojis would be contextually-appropriate and expressive. The final character card includes 5 concise emotionally-diverse conversations, covering all of Haru's target emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emo-Text: Expressive Robot Behaviors",
      "text": "Emo-text is where the expressive robot behavior generation takes place. Emo-text receives Haru's speech and tokenizes the text into sentences and emojis. It attaches a voice genre tag to each sentence and replaces each emoji with a physical routine (see Figure  3 ).\n\nWhen working with conversational content generated by an LLM, we need to dynamically generate appropriate emotive behavior as the dialog progresses. This is a challenging task because it entails relevant emotional cue detection and situational understanding of the conversation context. To keep the problem tractable, we adopt two assumptions for generating expressive robot behavior:\n\nVoice genres: We assume that Haru's tone of voice is a direct response to the emotional content conveyed by Haru's sentences generated by the LLM. Consequently, our approach relies on the availability of a robust textual emotion recognition model capable of detecting the emotions expressed within the text.\n\nPhysical routines: LLMs often generate responses containing emoji to reflect the emotion of the speaker. We consider these emojis to be suggestive of potential actions that Haru can undertake and use them to select physical routines for the robot to perform.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Textual Emotion Recognition (Ter)",
      "text": ". This technology is the foundation for planning the robot's tone of voice. We build on the approach of  [18] , which applied TER to LLM-generated responses to select an emotive TTS voice genre. However,  [18]  suffered from poor performance because they relied on a general-purpose emotion recognition model that was not optimized for the task. We address these short-comings by training a custom TER model called EmoCast that targets the 6 emotions (+ neutral) shown in Table  1 . Our contributions are three-fold:\n\n(1) We use a stronger base model (Roberta-large tuned with a LoRA; learning_rate=0.001, batch_size=128, epochs=20). (  2    1  is a set of curated mappings we use.  2  To optimize selection of the appropriate voice genre, an emotion confidence threshold of 0.6 was selected through grid search. When the confidence score is below 0.6, the default voice will be chosen for that utterance. Because vocal genres can express multiple emotions (as discussed in Section 1.2.1), the emotion-tovoice genre mappings are N-to-1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Physical Action Generation.",
      "text": "The generation of Haru's physical actions follows the same logic as vocal genre selection, but with emojis-to-routines mappings. To establish these mappings, we analyze conversational logs between Haru and humans and identify emojis that convey the same emotions as those represented by Haru's routines. Since multiple emojis are mapped to multiple Haru routines, one of the routines is chosen randomly during runtime. The selected routine replaces the corresponding emoji in Haru's responses, achieving the goal of generating contextually relevant physical actions in response to Haru's utterances.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation",
      "text": "We conducted a pilot study where human participants engaged in conversation with Haru the robot using our proposed system. Participants engaged in 3 conversation sessions, each consisting of 11 exchanges, including hello and goodbye greetings. Conversation topics were unrestricted and unprompted by the robot. Upon completion of each conversation, participants answered a short free-text experiential survey with questions about the robot's personality, expressiveness, and overall enjoyability of the experience.\n\nSurvey participants were recruited from an organization affiliated with one of the authors, with a total of (n=12) participants. The survey took 30-40 minutes per participant. Demographically, there were more male (n=9) participants than female (n=3). The most common age group was 18-30 (n=9), followed by 30-40 (n=3).  Table  4 : Positive and negative feedback categorization. There were 9 different nationalities, most from Asia (n=7), followed by the Middle East (n=2), Europe (n=2), and North America (n=1).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Two-Phase Analysis",
      "text": "3.1.1 Feedback Analysis. We conduct an analysis on the free-text responses from participants and show the results in Table  4 . Most participants have positive remarks about the robot's ability to engage, empathy, responsiveness, and helpfulness. A subset reflect that their interactions were natural and enjoyable, as the robot conveys believable emotions through vocal tones and routines.\n\nWe also find that the primary factor affecting the evaluation's significance was the inconsistency in the outputs of the LLM. Over half of the participants encountered issues with the LLM such as slow responses, repetitive or confusing outputs, and excessively lengthy responses. Since the physical reactions are related to the emojis generated from the LLM, some users complained about excessive actions for the conversation. Another factor impacting the quality of interactions was the ASR. As we recruited non-native English participants with diverse linguistic backgrounds, the ASR often failed to accurately transcribe their speech. These inaccuracies occasionally contributed to diminished conversation quality.\n\nFurthermore, 1/3 of participants expressed a desire for longer conversations. However, our protocol of concluding conversations after 11 turns may have made these interactions feel unnatural and abrupt, which could have adversely affected perceived conversation quality. Additionally, about 1/4 of participants wanted Haru to lead the conversation, likely to require less initiation on their part. Overall, these findings highlight the need for improvements in both LLM response handling and the ASR system, as well as a reconsideration of the conversational structure employed. 3.1.2 Error Analysis. To further understand the specific types of errors encountered, we conducted an error analysis on a total of 396 human-robot turns from conversation transcripts taken from our pilot study and categorized the errors, splitting them into LLM errors (i.e. errors in the LLM output) and human errors (i.e. errors in the input to the LLM system), and splitting them into sub-types. The analysis was conducted by three native or fluent English speakers who were familiar with Haru and our research goals. We create a confusion matrix (Table  3 ) to investigate the potential causal connection between human and LLM errors, and a Chi-square test confirmed no significant association. 3  We find that the main cause of human errors were ASR-related (95/396), but the LLM was mostly able to stay on topic. Table   exemplifies this scenario, where the ASR interprets \"choking\" for \"joking\", which causes an ethical violation from the LLM. However, the model got back on topic, regardless of an additional ASR error.\n\nHowever, we also see a class of less common, but more serious LLM errors: ethical violations, hallucination (e.g. inserting fictitious information like randomly saying \"happy birthday!\"), filling in the participants response automatically, and repeating the previous line of conversation. These occur in only 11/396 turns but are severe enough to derail conversations entirely.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "Although the main error with human input was ASR-related, we conclude that the LLM's ability to stay on topic is positive. Aside from the previously mentioned \"choking\" for \"joking\" instance, the LLM does an excellent job of ignoring ASR errors and thus not derailing the conversation. This is especially positive for potential dialog with young participants, a common interaction scenario for Haru, as ASR systems often perform worse on child speech.\n\nHowever, the serious LLM errors identified require addressing if LLMs are to be successfully adopted for social robots. Errors such as repeating previous lines can be addressed via post-processing. Ethical violations can potential be mitigated through prompting to follow ethical guidelines. Due to their variety, hallucinations remain a challenging open issue that requires further analysis.\n\nFinally, our impression is that the LLM does sound like Haru in the sense that its enthusiastic, uses emojis, and keeps its responses short, as specified in the character card. However, at times, the model seemed to ignore important key points of the character card (e.g. Haru's fear of magnets) and discuss them enthusiastically.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "LLMs offer an attractive solution for open-ended conversations with social robots; however, their application requires consideration for the robot's personality and emotive behavior. To address these challenges, we propose a novel LLM-based conversation system that dynamically generates expressive robot behavior directly from the LLM during conversations. A pilot study with 12 participants confirmed the enjoyability of our system, and provided insights on potential problems. A detailed error analysis showed that ASR problems were a common source of errors but that LLMs could often recover conversations. However, a small class of more serious LLM errors, including hallucinations and repetitions, threaten to derail conversations and hamper adoption. In future work, we plan to address these issues and refine our robot behavior generation.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed approach generates conversation responses with expressive robot behavior directly from the LLM.",
      "page": 1
    },
    {
      "caption": "Figure 3: Emoji→Haru",
      "page": 2
    },
    {
      "caption": "Figure 2: Haru the robot expresses itself in a conversation.",
      "page": 2
    },
    {
      "caption": "Figure 2: , for our study. Haru is designed to excel in mul-",
      "page": 2
    },
    {
      "caption": "Figure 2: We describe how Haru routines",
      "page": 2
    },
    {
      "caption": "Figure 1: The Interaction Manager makes conversa-",
      "page": 2
    },
    {
      "caption": "Figure 3: The Emo-text expressive behavior generation module generates robot actions from emoji and selects TTS voice genres.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Har u: ' \nThat's \nHUGE news! I 'm so \nproud of you!  \n'": "M er ge",
          "[? \n?\nEmo-text\n ?That's HUGE news!?,\n?\nI'm so proud of you!?,\n ?? \n?\n ]\nTokenization": "Emotion \nThat's HUGE news!\nVoice genre \nsurprise\n whisper-yell\nisEmoj i\nNo\nM odel \ngener ation\njoy\n \nhigh-ener gy\n(EmoCast)\nI'm so proud of you!\n \nPhysical action \nsur pr ise\nYes\ncelebr ation\ngener ation"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion and sociable humanoid robots",
      "authors": [
        "Cynthia Breazeal"
      ],
      "year": "2003",
      "venue": "Applications of Affective Computing in Human-Computer Interaction",
      "doi": "10.1016/S1071-5819(03)00018-1"
    },
    {
      "citation_id": "2",
      "title": "Language Models are Few-Shot Learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell",
        "Sandhini Agarwal",
        "Ariel Herbert-Voss",
        "Gretchen Krueger",
        "Tom Henighan",
        "Rewon Child",
        "Aditya Ramesh",
        "Daniel Ziegler",
        "Jeffrey Wu",
        "Clemens Winter",
        "Chris Hesse",
        "Mark Chen",
        "Eric Sigler",
        "Mateusz Litwin",
        "Scott Gray",
        "Benjamin Chess",
        "Jack Clark",
        "Christopher Berner",
        "Sam Mccandlish",
        "Alec Radford",
        "Ilya Sutskever",
        "Dario Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato"
    },
    {
      "citation_id": "3",
      "title": "The role of expressiveness and attention in human-robot interaction",
      "authors": [
        "A Bruce",
        "I Nourbakhsh",
        "R Simmons"
      ],
      "year": "2002",
      "venue": "Proceedings 2002 IEEE International Conference on Robotics and Automation",
      "doi": "10.1109/ROBOT.2002.1014396"
    },
    {
      "citation_id": "4",
      "title": "A survey of using vocal prosody to convey emotion in robot speech",
      "authors": [
        "Joe Crumpton",
        "Cindy Bethel"
      ],
      "year": "2016",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "5",
      "title": "The Effects of Robot Voice and Gesture Types on the Perceived Robot Personalities",
      "authors": [
        "Xiao Dou",
        "Chih-Fu Wu",
        "Kai-Chieh Lin",
        "Tzu-Min Tseng"
      ],
      "year": "2019",
      "venue": "Human-Computer Interaction. Perspectives on Design"
    },
    {
      "citation_id": "6",
      "title": "A socially assistive robot exercise coach for the elderly",
      "authors": [
        "Juan Fasola",
        "J Maja",
        "Matarić"
      ],
      "year": "2013",
      "venue": "J. Hum.-Robot Interact",
      "doi": "10.5898/JHRI.2.2.Fasola"
    },
    {
      "citation_id": "7",
      "title": "A survey of socially interactive robots",
      "authors": [
        "Terrence Fong",
        "Illah Nourbakhsh",
        "Kerstin Dautenhahn"
      ],
      "year": "2003",
      "venue": "Robotics and Autonomous Systems",
      "doi": "10.1016/S0921-8890(02)00372-X"
    },
    {
      "citation_id": "8",
      "title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers",
      "authors": [
        "Elias Frantar",
        "Torsten Saleh Ashkboos",
        "Dan Hoefler",
        "Alistarh"
      ],
      "year": "2022",
      "venue": "GPTQ: Accurate post-training quantization for generative pre-trained transformers",
      "arxiv": "arXiv:2210.17323"
    },
    {
      "citation_id": "9",
      "title": "Meet Haru, the Unassuming Big-Eyed Robot Helping Researchers Study Social Robotics",
      "authors": [
        "Randy Gomez"
      ],
      "year": "2020",
      "venue": "IEEE Spectrum"
    },
    {
      "citation_id": "10",
      "title": "Haru: Hardware design of an experimental tabletop robot assistant",
      "authors": [
        "Randy Gomez",
        "Deborah Szapiro",
        "Kerl Galindo",
        "Keisuke Nakamura"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 International Conference on Human-Robot Interaction",
      "doi": "10.1145/3171221.3171288"
    },
    {
      "citation_id": "11",
      "title": "A Holistic Approach in Designing Tabletop Robot's Expressivity",
      "authors": [
        "Randy Gomez",
        "Deborah Szapiro",
        "Luis Merino",
        "Keisuke Nakamura"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Robotics and Automation",
      "doi": "10.1109/ICRA40945.2020.9197016"
    },
    {
      "citation_id": "12",
      "title": "Using Socially Expressive Mixed Reality Arms for Enhancing Low-Expressivity Robots",
      "authors": [
        "Thomas Groechel",
        "Zhonghao Shi",
        "Roxanna Pakkar",
        "Maja Mataric"
      ],
      "year": "2019",
      "venue": "Using Socially Expressive Mixed Reality Arms for Enhancing Low-Expressivity Robots",
      "arxiv": "arXiv:1911.09713"
    },
    {
      "citation_id": "13",
      "title": "Relating conversational expressiveness to social presence and acceptance of an assistive social robot",
      "authors": [
        "Marcel Heerink",
        "Ben Kröse",
        "Vanessa Evers",
        "Bob Wielinga"
      ],
      "year": "2010",
      "venue": "Virtual reality"
    },
    {
      "citation_id": "14",
      "title": "The physical presence of a robot tutor increases cognitive learning gains",
      "authors": [
        "Daniel Leyzberg",
        "Samuel Spaulding",
        "Mariya Toneva",
        "Brian Scassellati"
      ],
      "year": "2012",
      "venue": "Proceedings of the annual meeting of the cognitive science society"
    },
    {
      "citation_id": "15",
      "title": "The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents",
      "authors": [
        "Jamy Li"
      ],
      "year": "2015",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/j.ijhcs.2015.01.001"
    },
    {
      "citation_id": "16",
      "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "Comput. Surveys"
    },
    {
      "citation_id": "17",
      "title": "Iterative Design of an Emotive Voice for the Tabletop Robot Haru",
      "authors": [
        "Eric Nichols",
        "Sarah Siskind",
        "Waki Kamino",
        "Selma Šabanović",
        "Randy Gomez"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Social Robotics (ICSR 2021)",
      "doi": "10.1007/978-3-030-90525-5_31"
    },
    {
      "citation_id": "18",
      "title": "2022. I Can't Believe That Happened! Exploring Expressivity in Collaborative Storytelling with the Tabletop Robot Haru",
      "authors": [
        "Eric Nichols",
        "Deborah Szapiro",
        "Yurii Vasylkiv",
        "Randy Gomez"
      ],
      "venue": "Proceedings of the 31st IEEE International Conference on Robot & Human Interactive Communication",
      "doi": "10.1109/RO-MAN53752.2022.9900606"
    },
    {
      "citation_id": "19",
      "title": "The influence of voice pitch on the evaluation of a social robot receptionist",
      "authors": [
        "Andreea Niculescu",
        "Betsy Van Dijk",
        "Anton Nijholt",
        "Swee Lan"
      ],
      "year": "2011",
      "venue": "2011 International Conference on User Science and Engineering (i-USEr )",
      "doi": "10.1109/iUSEr.2011.6150529"
    },
    {
      "citation_id": "20",
      "title": "Creating Personalized Verbal Human-Robot Interactions Using LLM with the Robot Mini",
      "authors": [
        "Teresa Onorati",
        "Álvaro Castro-González",
        "Javier Valle",
        "Paloma Díaz",
        "José Castillo"
      ],
      "year": "2023",
      "venue": "Creating Personalized Verbal Human-Robot Interactions Using LLM with the Robot Mini",
      "doi": "10.1007/978-3-031-48306-6_15"
    },
    {
      "citation_id": "21",
      "title": "Talking with Sentiment: Adaptive Expression Generation Behavior for Social Robots",
      "authors": [
        "Igor Rodriguez Rodriguez",
        "Adriano Manfré",
        "Filippo Vella",
        "Ignazio Infantino",
        "Elena Lazkano"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th International Workshop of Physical Agents (WAF 2018)",
      "doi": "10.1007/978-3-319-99885-5_15"
    },
    {
      "citation_id": "22",
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale",
        "Dan Bikel",
        "Lukas Blecher",
        "Cristian Canton Ferrer",
        "Moya Chen",
        "Guillem Cucurull",
        "David Esiobu",
        "Jude Fernandes",
        "Jeremy Fu",
        "Wenyin Fu",
        "Brian Fuller",
        "Cynthia Gao",
        "Vedanuj Goswami",
        "Naman Goyal",
        "Anthony Hartshorn",
        "Saghar Hosseini",
        "Rui Hou",
        "Hakan Inan",
        "Marcin Kardas",
        "Viktor Kerkez",
        "Madian Khabsa",
        "Isabel Kloumann",
        "Artem Korenev",
        "Punit Singh Koura",
        "Marie-Anne Lachaux",
        "Thibaut Lavril",
        "Jenya Lee",
        "Diana Liskovich",
        "Yinghai Lu",
        "Yuning Mao",
        "Xavier Martinet",
        "Todor Mihaylov",
        "Pushkar Mishra",
        "Igor Molybog",
        "Yixin Nie",
        "Andrew Poulton",
        "Jeremy Reizenstein",
        "Rashi Rungta",
        "Kalyan Saladi",
        "Alan Schelten",
        "Ruan Silva"
      ],
      "year": "2023",
      "venue": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "arxiv": "arXiv:2307.09288[cs.CL]"
    },
    {
      "citation_id": "23",
      "title": "Embodiment and Human-Robot Interaction: A Task-Based Perspective. RO-MAN 2007",
      "authors": [
        "Joshua Wainer",
        "David Feil-Seifer",
        "Dylan Shell",
        "Maja Matarić"
      ],
      "year": "2007",
      "venue": "The 16th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "24",
      "title": "Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data",
      "authors": [
        "Jing Wei",
        "Sungdong Kim",
        "Hyunhoon Jung",
        "Young-Ho Kim"
      ],
      "year": "2023",
      "venue": "Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data",
      "arxiv": "arXiv:2301.05843[cs.HC]"
    },
    {
      "citation_id": "25",
      "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
      "authors": [
        "Jingfeng Yang",
        "Hongye Jin",
        "Ruixiang Tang",
        "Xiaotian Han",
        "Qizhang Feng",
        "Haoming Jiang",
        "Bing Yin",
        "Xia Hu"
      ],
      "year": "2023",
      "venue": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
      "arxiv": "arXiv:2304.13712[cs.CL]"
    },
    {
      "citation_id": "26",
      "title": "Large Language Models for Robotics: A Survey",
      "authors": [
        "Fanlong Zeng",
        "Wensheng Gan",
        "Yongheng Wang",
        "Ning Liu",
        "Philip Yu"
      ],
      "year": "2023",
      "venue": "Large Language Models for Robotics: A Survey",
      "arxiv": "arXiv:2311.07226[cs.RO]"
    },
    {
      "citation_id": "27",
      "title": "Large language models for human-robot interaction: A review",
      "authors": [
        "Ceng Zhang",
        "Junxin Chen",
        "Jiatong Li",
        "Yanhong Peng",
        "Zebing Mao"
      ],
      "year": "2023",
      "venue": "Biomimetic Intelligence and Robotics",
      "doi": "10.1016/j.birob.2023.100131"
    }
  ]
}