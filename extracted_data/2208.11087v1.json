{
  "paper_id": "2208.11087v1",
  "title": "Locally Temporal-Spatial Pattern Learning With Graph Attention Mechanism For Eeg-Based Emotion Recognition",
  "published": "2022-08-19T12:15:10Z",
  "authors": [
    "Yiwen Zhu",
    "Kaiyu Gan",
    "Zhong Yin"
  ],
  "keywords": [
    "Emotion recognition",
    "electroencephalogram",
    "graph attention network",
    "domain adaptation",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Technique of emotion recognition enables computers to classify human affective states into discrete categories. However, the emotion may fluctuate instead of maintaining a stable state even within a short time interval. There is also a difficulty to take the full use of the EEG spatial distribution due to its 3-D topology structure. To tackle the above issues, we proposed a locally temporal-spatial pattern learning graph attention network (LTS-GAT) in the present study. In the LTS-GAT, a divide-and-conquer scheme was used to examine local information on temporal and spatial dimensions of EEG patterns based on the graph attention mechanism. A dynamical domain discriminator was added to improve the robustness against interindividual variations of the EEG statistics to learn robust EEG feature representations across different participants. We evaluated the LTS-GAT on two public datasets for affective computing studies under individual-dependent and independent paradigms. The effectiveness of LTS-GAT model was demonstrated when compared to other existing mainstream methods. Moreover, visualization methods were used to illustrate the relations of different brain regions and emotion recognition. Meanwhile, the weights of different time segments were also visualized to investigate emotion sparsity problems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are closely related to human cognitive functionality and play a fundamental role in regulating human behavior of perception and/or decision-making. In the field of human-computer interaction, technique of emotion recognition enables computers to automatically classify human affective states into discrete categories. Such an emotion recognizer facilitates building an intelligent module that makes machine agents adapt to human emotional fluctuations. When human users are collaborating with medical care devices  [1] ,\n\ntransportation systems  [2] , and/or industry robots  [3] , understanding accurate emotional states of human participants could improve overall efficiency of human-computer collaborations.\n\nEmotion categories can be defined by discrete or dimensional models. The discrete model categorizes the affective state into six basic emotions, i.e., joy, sadness, surprise, fear, anger, and disgust. The dimensional model defines the continuous emotion based on core affective dimensions, e.g., valence and arousal scales.\n\nEmotions can be continuously labeled from positive to negative on the valence scale and passive to active on the arousal scale. As shown in Fig.  1 , combinations of specific coordinates on these dimensions can elicit basic emotions defined by the discrete model. To measure emotional states, a variety of data modalities from human users have been employed, such as videos of body movements  [4] , images of facial expressions  [5] , records of speeches  [6] , and segments of psychophysiological signals  [7] . Among these modalities, electroencephalography (EEG), a type of neurophysiological signal recorded from central nervous system, is particularly sensitive to identifying human inner cognitive states. The ongoing EEG is the sum of the voltage variations from pyramidal cells in of studies  [8, 9] . The goal of this study is to build an EEG-based computational framework, i.e., an emotion recognizer, with the EEG as its input and emotion categories as its output.\n\nRecent studies introduced advanced signal filtering and machine learning approaches to build EEG-based emotion recognizers  [10, 11] . The procedure mainly consists of four steps, i.e., EEG signal preprocessing, feature extraction, feature selection and/or feature learning, and training pattern classifiers. However, there are still three challenges required to be properly investigated. The first issue is that emotions may fluctuate in a wide range thus show a great sparsity and may lead to incorrect predictions. Second, the optimal spatial feature representation of the EEG is still unknown. It leads to difficulty in taking full use of the topological structure of the EEG energy shown by connections between different cortical regions and channels. The last issue is that the statistics and/or distributions of EEG features may drift greatly across different individuals.\n\nIt always leads to a significant impairment in accuracy of individual-independent emotion recognizers.\n\nAttempting to mitigate the above obstacles, we proposed a locally temporal-spatial pattern learning graph attention network, (termed LTS-GAT). Inspired by  [12] , we applied graph analysis to human cortical topology by considering each EEG channel as a node in the graph. Different from previous works  [13, 14] , a graph attention network was employed instead of applying traditional spectral graph convolution. In particular, a divide-and-conquer scheme was used to examine local information on temporal and spatial dimensions of EEG patterns. It is aimed to quantitatively identify fluctuations of emotions over time instants and to discover spatial hidden variables according to the EEG channel paradigm. To improve the robustness against the inter-individual variations of the EEG statistics, a dynamical domain discriminator was added to the LTS-GAT to learn robust EEG feature representations across different participants.\n\nIn summary, the main contributions of our study are as follows:\n\n1) We propose a novel LTS-GAT network to predict emotions based on the EEG. This model can capture local temporal information between different time instants and spatial information within and across brain regions.\n\n2) A novel domain discriminator is proposed to alleviate distributional differences in the EEG data that are completely recorded from different individuals.\n\n3) Importance of each brain region can be properly identified. The weights of specific attention modules can be used to interpret which cortical region plays the most significant role in measuring emotion variations.\n\n4) The proposed emotion recognizer is validated via both individual-dependent and individualindependent classification tasks. Two publicly available EEG databases, HCI and DEAP, have been employed to evaluate the performance of the proposed algorithms.\n\nThe remainder of the paper is organized as follows. In Section 2, we review related works on EEG-based emotion recognition. Section 3 briefly summarizes preliminaries of graph theory and introduces the methods of applying graph analysis to the EEG. In Section 4, we elaborate on the feedforward structure of each computational module and the training algorithm of the LTS-GAT. Section 5 shows detailed experiments to validate generalization capability of the proposed method. Sections 6 and 7 provide some useful discussions and conclude the main findings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Selecting more representative and discriminative EEG features is an effective way to improve the accuracy of emotion recognition. Atkinson et al.  [15]  combined statistical-based feature selection methods to improve the performance of the individual-dependent emotion classification performance. A locally robust EEG feature selection method was proposed in  [16]  that could find invariant EEG indicators within a subset of individuals. A series of shallow learning models were applied to decode the EEG into emotion categories. In these works, support vector machine (SVM)  [17]  and k-nearest neighbors (kNN)  [18]  were widely used to identify emotions according to high-dimensional EEG features.\n\nRecently, emotion recognition frameworks based on deep neural networks have attracted a lot of attention.\n\nTzirakis et al. applied a deep convolutional neural network (ConvNet) as an emotion classifier. Cui et al.  [19]  proposed an end-to-end regional-asymmetric ConvNet that consists of temporal, regional, and asymmetric feature extractors to capture discriminative information. Although many previous studies have proved that the ConvNet shows great discriminant capability between different emotions, its feature learning pipeline is restricted to the 2-D Euclidean space. Considering that the EEG data are recorded via discrete 3-D locations in the spatial domain, only adopting the ConvNet structure may not be the optimal choice to take full use of the topological structure of cortices.\n\nGraph neural network (GNN) was first proposed to deal with the data with a graph structure  [20] . In early works, spectral convolution in the graph domain was extensively analyzed. Kipf et al.  [21]  proposed a standard graph convolutional neural network (GCN), which is a mature model combining GNN with spectral theory. Li et al.  [22]  proposed a curvature graph neural network, which exploits the structural properties of graph curvature effectively to make GNNs more adaptive to locating.\n\nConsidering the potential limitations of the classical GCN, non-spectral approaches have been investigated. It intuitively defines convolution operations on a spatial domain. Hechtlinger et al.  [23]  generalized the ConvNet to graph-structured data by utilizing a random walk procedure to unfold salient feature representations. Hamilton et al.  [24]  proposed GraphSAGE which is an inductive framework for generating node information on large graphs. This framework used a function that generated embedding by sampling and aggregating features from a local neighborhood around a node.\n\nTo improve the interpretability and capability of EEG feature learning with classical GCNs, we developed our model based on graph attention networks (GAT)  [12] . On one hand, the EEG channels can be considered as nodes in the graph according to cortical topology which is more adaptive than using a 2-D matrix with ConvNets. Moreover, the GAT adopts the self-attention mechanism  [25]  to compute attention coefficients between each node and its neighbors. It thus can discover inner connections between different EEG channels and cortical regions related to emotion variation among multiple individuals.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Preliminary",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Organizing The Eeg Data With A Graph",
      "text": "Formally, a graph is defined as ( , ) =\n\n, where represents a set of the nodes and denotes an edge set between nodes in . Each edge has a corresponding weight, which can be represented by a symmetric adjacency matrix\n\n, where n represents the node number and ij a denotes the edge weight between nodes i and j . Then, a degree matrix 12 ( , ,..., )\n\nis defined where each value on the diagonal is the sum of the corresponding row of the adjacency matrix i ij j da =  . When representing EEG data, each node is corresponding to an EEG channel. Thus, EEG features on the node set can be represented by a\n\n, where d denotes the dimension of feature on each node. After series of graph convolutional operations, the input X turns into output\n\n, where d denotes the dimension of the learned EEG feature representation. features of all channels were extracted for each segment. Specifically, DE values within theta  (4-7Hz), alpha  (8-12 Hz), beta  (13-30 Hz), and gamma  (>30 Hz) bands were computed. To integrate graph analysis theory for representing the EEG data, the node number n was equal to the number of the EEG channels. As a result, the feature tensor on the node set in one graph can be defined as\n\n, where d denotes the number of frequency bands. The whole procedure for organizing the EEG data with a graph structure is shown in Fig.  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Graph Neural Network",
      "text": "There are two important operations in graph neural networks, i.e., aggregation and combination. For node i , an aggregating function AG f is defined to obtain information from its neighbor node j , i.e., ( ) ( )\n\n({ : }).\n\nIn the equation, i represents the neighborhood of the node i . The terms i a and j h denote aggregation information and an implicit vector, respectively. The term l denotes the index of the hidden layer.\n\nClassically, function AG f can be computed via spectral graph convolution  [21]  based on the symmetric adjacency matrix A and a degree matrix D with fixed weights as presented in Section 3.1 In this study, the AG f is defined via the graph attention mechanism to improve the capability for feature representation.\n\nThen, a combining function CO f is introduced to fuse the information from the l -th hidden layer and implicit vectors of the  ( 1)  l --th hidden layer,  ) .\n\nFunction CO f can be implemented with a concatenation operation combined with a fully connected layer. Moreover, a readout function",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Locally Temporal-Spatial Pattern Learning Graph Attention Network",
      "text": "The proposed LTS-GAT framework can be summarized into four modules as shown in Fig.  3 . First, a temporal pattern learning module is introduced to discover local information in the time domain across node features. Then, the derived feature matrix is fed into a bidirectional long short term memory (Bi-LSTM) network to learn high-level spatial feature abstractions. Specifically, local spatial structures of EEG feature distribution are represented by dividing electrodes into several subsets based on the local regions of the scalp.\n\nThe learned feature matrix serves as the input of a GAT to predict emotion categories corresponding to the given EEG sample. For individual-independent tasks, a dynamical domain discriminator is implemented as an additional module to improve the capability of domain adaptation for the EEG data distribution between different individuals. The codes of the LTS-GAT model built in the study are available at https://github.com/CFSRgroup/LTS-GAT-model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Locally Temporal Pattern Learning",
      "text": "As described in Section 3, we could define  To alleviate the emotion sparsity existing in different segments, we applied an attention module to process feature time courses and compute segments' weights. For a DE feature matrix X , three affine transformations are applied to obtain query, key and value matrices in an attention module  [25] . The query matrix of temporal pattern learning module ( )\n\nthe EEG feature vector of a segment after the affine transformation.\n\nTo explore weights between different time segments, every key vector of () t K is supposed to compute attention coefficient with () t Q . The process of attention mechanism is shown in Fig.  5  and the corresponding attention coefficient is defined as, (\n\nexp( )\n\nIn the equation, i and j represent different time segments, () t i k is the i -th column vector of () t K that denotes the key vector of segment i . Here, the inner product operator is used as the scoring function for\n\nThen, the weight matrix ( )\n\nw w w is defined to reveal importance of time segments quantitatively. Thus, the transformed feature matrix\n\nThus, the learnable parameter set of the locally temporal pattern learning module is",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Locally Spatial Pattern Learning",
      "text": "The spatial patterns of the EEG are learned with two stages. First, the feature matrix  X obtained by locally temporal pattern learning module is fed to a Bi-LSTM to discover the relationship between different EEG channels. Then, we divide EEG channels into several subsets according to their adjacent spatial locations and perform a region-attention operation.\n\ndenotes the input of the Bi-LSTM and is used to learn spatial feature abstractions across EEG channels. We compare n EEG channels with a continuous n -order sequence and each channel corresponds to one cell in the Bi-LSTM. In this case, for the i -th EEG cell, the input, forget, and output gates can be expressed as,\n\n).\n\nIn above equations, () ).\n\nIn the equation, c W denotes the weight matrix of the cell, c b is the bias and 1 i-c represents the state of i-th cell. Update the cell state i c based on previous cell state and i c with\n\nThe learned locally temporal patterns EEG samples\n\nrepresents the Hadamard product. The output of a cell i h is computed by tanh( )\n\nRecent studies found that neuron responses to different emotion stimuli would be varied across different cortical areas  [27] . Therefore, we divided feature map S into several local subsets. The region division paradigm based on 32 channels is illustrated in Fig.  6 . Due to each cortical area containing different number of EEG channels, the dimension of local subsets varies. Thus, we employed affine transformations via { , } gg WB to derive the identical dimension. where N represents the number of total regions and m denotes the dimension of feature subset after the linear transformation. Next, we applied an attention layer to discover interactions of different regions. The attention coefficient of i -th region is calculated as,",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Exp( )",
      "text": "In the equation, ( ) W is used to illustrate the importance of each cortical area. The learnable parameters of the regional attention layer are defined as",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Graph Based Temporal-Spatial Feature Learning",
      "text": "To take full use of the channel topology and the local temporal information from the learned with F  representing the output feature dimension of each node. Next, a shared attentional mapping is employed  [12] . The derived attention coefficient measures the importance of the EEG feature from the node j to that from the node i .To embed into graph structure, assuming that i j  with i denoting the set of neighbor nodes of node i in the graph (including i ), j represents the first-order neighbor nodes of i  [12] . Then, compute attention coefficients for node j with a softmax function,\n\nwhere || denotes the concatenation operation, T v a is learnable weights, and () Leaky f  denotes the LeakyReLU activation function. Then, the output of node i can be represented with multi-head attention  [25] ,\n\nMulti-head attention is employed to make the model obtain information jointly from different representation subspaces and reduce computational cost. We applied K independent attention operators compute the coefficients in a parallel manner. Finally, the learned graph-based geometrical information is presented as 12 { , ,..., },\n\n. The learnable parameters in Eqns. (  10 )-(  11 ) are defined as",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotion Classifier With Domain Adaptation",
      "text": "Predicting emotion categories corresponding to an EEG sample can be derived by the output of the graph attention network  Z . To define the posterior probability of a specific emotion category, a softmax function () When the LTS-GAT is implemented with individual-independent paradigm, domain adaptation is adopted aiming to extract useful information from source domain (EEG data from a set of known individuals) and apply the transferable knowledge to the target domain (EEG data from an unknown individual). We assume that the source domain so consists of a set of labeled EEG samples () so X with the corresponding set of emotion categories () so Y . For the target domain ta , we have a set of unlabeled EEG samples () ta X . A dynamical domain discriminator is introduced along with the emotion classifier to estimate the domain labels  , where 0 and 1 denote an EEG sample from the source and target domains, respectively.\n\nTherefore, it is expected that the domain discriminator gradually unable to distinguish data from two domains along with the training process, which indicates the EEG distribution between s and t is similar to each other. Thus, the loss of domain discriminator should be maximized with a gradient reverse layer (GRL). The posterior probability of two domains can be calculated by,\n\nwhere the feedforward operation of () R   is defined as () R x x  = and the feedback operation is defined as\n\nto reverse the gradient I . The term  denotes the estimated domain. It is noted that  is a parameter changing dynamically as iterations go on, 2 1. 1 exp  ( 10 )  p\n\nIn the equation, p represents the ratio of current iterations to total iterations. ,,\n\nCompute the locally spatial pattern S with Eqns. (  6 )-(  7 ) based on  X 6\n\nMap → SG to match the region dimension 7\n\nCompute regional attention mappings   is set as the default value. Estimations of the first and second order gradients { ', '} mv are computed with the ADAM optimizer. The  and  are the target and estimated domain labels, respectively. The y and ŷ are the target and estimated emotional labels, respectively.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Database Descriptions",
      "text": "We validated the performance of the LTS-GAT on two public databases, HCI and DEAP. The HCI database recorded EEG data of 30 participants at a sampling frequency of 256 Hz with 32 active AgCl electrodes as shown in Fig. 6  [29] . Each participant watched 20 video clips (i.e., 20 trials) and each clip lasted between 34.9 and 117-s long. Each clip was selected to stimulate specific emotions such as happiness, sadness, disgust, and amusement. Similarly, subjective ratings with an interval from 1 to 9 on valence and arousal dimensions for each trial from 1 to 9 were collected. The data recorded from six participants were not analyzed due to the unfinished data collection.\n\nThe DEAP database is a multimodal dataset for emotion analysis which recorded EEG data of 32 participants  [30] . Each participant was required to watch 40 one-minute-long clips of music videos (i.e., 40 trials). After participants watched each clip, they accomplished a self-assessment questionnaire by rating on valence, arousal, and dominance scales from a range of 1 to 9. All the EEG data are recorded at a sampling rate of 512 Hz using 32 active AgCl electrodes. The locations of these electrodes are identical to those employed in the HCI database.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Eeg Data And Target Emotion Categories",
      "text": "For the two databases, all the EEG signals are downsampled to 128 Hz. Due to the motion and respiratory artifacts, a high pass filter with the cutoff frequency of 3 Hz and a bandpass filter with the cutoff frequencies of 4 and 45 Hz were applied to the HCI and DEAP databases, respectively. For the HCI database, we removed the 15-s baseline EEG and the 46-s EEG when the participants were performing subjective ratings in each trial. We then chose the last 30-s EEG of each trial for further analysis to avoid the label noise of emotion categories  [31] . For the DEAP database, we removed the 3-second baseline signals and the length of filtered EEG of each trial is 60 s for each participant. For both databases, EEG signals from each trial were evenly divided into three samples. Then, DE features of all channels and four frequency bands were extracted from each sample. All the EEG features were standardized to zero mean and unit variance for each participant. We assigned two target emotion categories to each EEG sample based on the subjective ratings on arousal and valence dimensions for each trial. For arousal and valence ratings, the rating value higher than 5 indicates the high class and the value lower than 5 indicates the low class.\n\nHyper-parameter settings under two paradigms of the LTS-GAT are listed in Table  1 .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Individual-Dependent Emotion Classification",
      "text": "For two databases, we both applied 10-fold cross validation on the EEG samples of each participant. That is, the training and testing EEG data were defined within a participant. In particular, the cross validation was performed at the video level. The training and testing EEG data were drawn from non-overlapped video clips.\n\nThat is, to compare the performance, seven emotion recognizers were employed, i.e., support vector machine (SVM)  [32] , extreme gradient boosting (XGBoost)  [33] , ConvNet (LeNet-5)  [34] , Bi-LSTM  [35] , GCN  [21] ,\n\nsimple graph convolution (SGC)  [36] , and GAT  [12] . Average classification performances over all participants on the HCI and DEAP databases are presented in Table  2 . Inter-emotion discriminant capability of the arousal dimension is higher than that of the valence for the HCI database while the opposite observation is found for the DEAP. In general, the proposed model achieves higher F1-score and accuracy than other approaches on both databases. It is shown that the LTS-GAT has improved the average accuracy (or F1-score) by 3~11% (or 3~13%) compared to classical GAT model.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Individual-Independent Emotion Classification",
      "text": "The generalization capability of all emotion recognizers was also validated under the individualindependent paradigm for each database. We adopted leave-one-participant-out cross validation to calculate classifiers' performances. That is, the EEG data of one participant was used for testing and that of the other participants were taken as the training data to build the emotion recognizer. In every round of the cross validation, we defined training and testing data as the source and target domains to build the domain discriminator, respectively. To validate the performance, we adopted several transfer learning methods and state-of-the-art deep learning approaches for comparison purposes, which are listed as follows: (1) Two baseline methods: SVM  [32]  and random forest (RF)  [37] . (2) Two transfer learning approaches:\n\nkernel principal component analysis (KPCA)  [38]  and transfer component analysis (TCA)  [39] . (3) Four deep neural networks: ConvNet (with a LeNet-5 structure)  [34] , t-LSTM  [35] , DANN  [40] , and BiDANN-R2  [27] . To achieve a fair comparison, we used the same settings of the domain discriminator for defining the source and target domains for these approaches except for the SVM and RF models.\n\nTable  3  shows the participant-average classification results of the HCI and DEAP databases. The LTS-GAT has better performance in the arousal dimension for the HCI database while better performance in the valence dimension for the DEAP. Overall, the proposed model achieved higher performance than other approaches on both databases except for the F1-score of the DANN of the arousal dimension for the DEAP.\n\nIt is shown that the LTS-GAT improves the average accuracy (or F1-score) by 2~10% (or 2%~5%) compared to other advanced models with domain adaption.\n\nComparing with the results shown in Table  2 , it can be concluded that the accuracy obtained in the participant-independent paradigm is worse than that obtained in the participant-dependent paradigm by around 0.6% to 9% for the LTS-GAT model due to discrepancies between EEG signals from different individuals.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Individual-Dependent Performance On Different Frequency Bands",
      "text": "We further want to investigate the impacts of different frequency bands of EEG signals on individualdependent emotion classification. Experiments were conducted on two databases using the DE features from four frequency bands, as reported in Tables  4  and 5 . From the tables, the LTS-GAT achieved the highest accuracy from the theta and gamma band compared with all other methods on the HCI. It performs marginally worse than the Bi-LSTM and GCN in alpha and gamma bands. It also performs best for all four frequency bands on the DEAP database.\n\nFor both databases and emotional dimensions, performances of all emotion recognizers derived by the alpha, theta, and beta bands were insignificantly varied within 1%. One observation is found that the features from the gamma band impair the classification accuracy and F1-score in the valence scale of the HCI.\n\nCompared with Table  2 , the classification performance of the LTS-GAT using all bands of DE features increased by 1~4% than using the single-band features.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Individual-Independent Performance On Different Frequency Bands",
      "text": "Classification performance from four frequency bands on two datasets under the individual-independent paradigm is reported in Tables  6  and 7 . It can be seen that the LTS-GAT achieves the highest accuracy in all frequency bands compared to most of the methods on two databases. The exception is F1-score of the DANN in the HCI database.\n\nFor both databases and emotional dimensions, performances of all emotion recognizers derived by four frequency bands are varied within 1~5%. Compared with Table  3 , the classification performance of the LTS-GAT using all bands of DE features increased by 1~3% than adopting the single-band features.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Discussion",
      "text": "In this section, we first performed an ablation study to investigate contributions of each module for the LTS-GAT and then analyzed the importance of each time segment and cortical region for discriminating emotional states. The performance of the LTS-GAT is compared with the reported works in the literature.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Ablation Study",
      "text": "An ablation study was conducted to investigate the contribution of each module in the LTS-GAT. We first created several reduced versions of the network as follows. The LS-GAT denotes the locally temporal pattern learning module was removed and the LT-GAT represents the locally spatial pattern learning module was removed. We removed both the two modules to obtain a classical GAT model. When it comes to participantindependent classification tasks, we also tested a case where the domain discriminator was removed (denoted as \"-DA\"). Performances of reduced models are shown in Fig.  7 . It is observed that the order of accuracies of four models is, GAT < LT-GAT < LS-GAT < LTS-GAT, in both arousal and valence dimensions in individual-dependent paradigm. While in individual-independent paradigm, there is an exceptional case where the GAT outperforms the LT-GAT.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Feature Importance Visualization",
      "text": "We computed the average weights of each time instant defined in (1) from eight participants and drew heatmaps to investigate whether emotion sparsity exists in ten different time instants. As shown in Fig.  8 , the range of the weight derived from the HCI participants is from 0.36 to 2.60, which is much larger than that of the DEAP (from 0.86 to 1.20). This fact can be the potential reason that the performance of the HCI of multiple classifiers is lower than that of the DEAP under the participant-dependent paradigm.\n\nThe importance of different cortical regions contributing to variations of affective states is shown in Fig.  9 . We computed average attention weights defined in  (11)  of all participants in each database. Then, the weights are accurately mapped to cortical locations. We can observe that in the arousal dimension, the left pre-frontal and right-temporal regions are activated with higher weights for both databases and different bands. In the valence dimensions, frontal and parietal regions are highly activated for both databases and all frequency bands. These findings are consistent with the reported studies in  [41, 42] .",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Performance Comparison With Reported Studies",
      "text": "In Section 6.3, we enumerated recently reported results on two databases to evaluate our model furtherly.\n\nAs shown in Table  8 , on the DEAP database, the LTS-GAT obtains more discriminative patterns in the valence scale to improve the performance. The accuracy is improved by 8% using the LTS-GAT compared with  [30, 43]  under the participant-dependent paradigm. In particular, our model improves the average accuracy by 6% for valence compared with  [8] , which is another advanced GNN-based method. On the HCI, it can be concluded that the LTS-GAT increases the accuracy by 0.5~3% and 3~7% for the arousal and valence scale under individual-dependent paradigms, respectively. However, it should be noted that the number of training samples, data processing methods, and definitions of the emotional classes could be different among these reported studies. These factors may induce lurking variables to compare the accuracy. The term \"D\" denotes the individual-dependent paradigm and \"ID\" denotes the individual-independent paradigm.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a locally temporal-spatial pattern learning graph attention network to facilitate EEG-based emotion classification. Our model was inspired by neuroscience findings that human emotion may be sparse in continuous time and affective responses can be closely related to the EEG entropy distribution between different channels and cortical sub-regions. We also introduced a dynamical domain discriminator to mitigate the distributional difference between EEG features across different participants.\n\nOur experimental results on two public datasets illustrate that the proposed LTS-GAT has better performance than several competitive methods under both individual-dependent and independent paradigms. Through analysis of cortical regions, we can infer that left pre-frontal, right-temporal, frontal, and parietal lobes are more important in EEG emotion classification. In terms of emotion sparsity, we can conclude that the HCI database possesses a potential higher sparsity than the DEAP database.\n\nThe main limitation of the study lies in two folds:\n\n1) The classification performance under the individual-independent paradigm is far from perfect.\n\nBalancing the losses of the classifier and the discriminator may improve the generalization capability.",
      "page_start": 22,
      "page_end": 22
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , combinations of specific coordinates on these dimensions can elicit",
      "page": 2
    },
    {
      "caption": "Figure 1: Illustration of discrete and dimensional models for labeling emotions.",
      "page": 2
    },
    {
      "caption": "Figure 2: Procedures for organizing the EEG data with a graph.",
      "page": 6
    },
    {
      "caption": "Figure 2: 3.2. Graph Neural Network",
      "page": 6
    },
    {
      "caption": "Figure 3: Framework of the proposed LTS-GAT model for binary emotion recognition, where GRL denotes the gradient",
      "page": 7
    },
    {
      "caption": "Figure 3: First, a",
      "page": 7
    },
    {
      "caption": "Figure 4: shows a pattern of variations of the emotional intensity across consecutive EEG",
      "page": 7
    },
    {
      "caption": "Figure 4: Fluctuations of emotion intensity across different time segments in an EEG sample.",
      "page": 8
    },
    {
      "caption": "Figure 5: and the corresponding",
      "page": 8
    },
    {
      "caption": "Figure 5: Layout of the locally temporal pattern learning module.",
      "page": 9
    },
    {
      "caption": "Figure 6: Due to each cortical area containing different number",
      "page": 10
    },
    {
      "caption": "Figure 6: Scheme of defining the EEG electrode subsets via different cortical regions.",
      "page": 10
    },
    {
      "caption": "Figure 6: [29]. Each participant watched 20 video clips (i.e., 20 trials) and each clip lasted",
      "page": 14
    },
    {
      "caption": "Figure 7: It is observed that the order of accuracies",
      "page": 19
    },
    {
      "caption": "Figure 7: Accuracies of the reduced models and the LTS-GAT: (a) Individual-dependent paradigm. (b) Individual-",
      "page": 20
    },
    {
      "caption": "Figure 8: Heatmaps based on the average weights of each time instant in an EEG sample derived from four different",
      "page": 20
    },
    {
      "caption": "Figure 9: We computed average attention weights defined in (11) of all participants in each database. Then, the",
      "page": 21
    },
    {
      "caption": "Figure 9: Topographical maps based on the participant-average weights of cortical regions of different frequency bands.",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "= LTS-GAT_FF (\n,\n) \nX\n'Z\npθ": "Input: \nAn EEG sample \n  \nX\n \nParameters \n of the LTS-GAT \npθ\nOutput: \nGraph-based geometrical information \n  \n'Z"
        },
        {
          "= LTS-GAT_FF (\n,\n) \nX\n'Z\npθ": "Compute locally temporal attention mappings \n1 \n(\nt\n)\n(\nt\n)\n(\nt\n)\nQ\n,\nK\n,\nV\nbased on \n  \nX\n(\n)t\n2 \nCompute \n according to Eqn. (4) \nAW\n(\nt\n)\n(\nt\n)\n X\nV W\n3 \n  \nA\nS\nCompute the locally spatial pattern \n  \n5 \nX\nwith Eqns. (6)-(7) based on \n  \n6 \nMap \n to match the region dimension \n→S\nG\nCompute regional attention mappings \n7 \n(\nr\n)\n(\nr\n)\n(\nr\n)\nQ\n,\nK\n,\nV\nbased on \n  \nG\n8 \nCompute \n according to Eqn. (8) \n)rW\n(\nr\n)\n(\nr\n)\n9 \n  \nH\nW V\n→H\nZ\n10 \nMap \n to match the channel dimension \nZ\nCompute  graph-based  geometrical  mapping \n using \n11 \nZ\nEqns. (10)-(11) with"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(\n(\nta\nso\n)\n)\nθ\n'θ\n \n = LTS-GAT_Train(\n,\n,\n,\n) \ntrS\ntrS\ntrS": "= =\n= =\nb\nInput: \nNumbers of batches \n, epochs \n, learning rate \n  \n\nn\n \nTraining set \n  \nS\n= X\n}b\ntr\nl\nl\n=\n1\n(\nso\n)\n(\nso\n)\n(\nso\n)\n(\nso\n)\nl\nb\nS\n= X\n,\ny\n,\n\n}\n \nSource domain set \n  \ntr\nl\nl\nl\nl\n1\n(\nta\n)\n(\nta\n)\n(\nta\n)\n(\nta\n)\nl\nb\nS\n= X\n,\ny\n,\n\n}\n \nTarget domain set \n  \ntr\nl\nl\nl\nl\n1\n \nInitialized parameters \n  \n=θ\n{\nθ\n,\nθ\n,\nθ\n}\np\nc\nd\n'θ\nOutput: \nLearned parameters"
        },
        {
          "(\n(\nta\nso\n)\n)\nθ\n'θ\n \n = LTS-GAT_Train(\n,\n,\n,\n) \ntrS\ntrS\ntrS": "i\n=\n1 :\nn\nl\n=\n1 :\nb\n1 \nfor \n, \n \np\n=\n(\nl\n+\ni\n\nb\n) / (\nn\n\nb\n)\n2 \n \n\n3 \nCompute \n with Eqn. (15) \n4 \nLTS-GAT_FF(\n,\n) \nZ\npθ\niX\n'i\n,\nl \n(\nso\n)\n(\nso\n)\np\nˆ(\nX\n)\n5 \nCompute \n with Eqn. (12) \ni\n,\nl\ni\n,\nl\n(\nso\n)\n(\nso\n)\n(\nta\n)\n(\nta\n)\nˆ\nˆ\np\n(\n\nX\n),\np\n(\n\nX\n)\n6 \nCompute \n with Eqn. (14) \ni\n,\nl\ni\n,\nl\ni\n,\nl\ni\n,\nl\n(\nso\n)\nL\n(\nθ\n,\nθ\nX\n)\n7 \nCompute \n with Eqn. (13) \nc\np\nc\ni\n,\nl\n(\nso\n)\n(\nta\n)\nL\n(\nθ\n,\nθ\n|\nX\n,\nX\n)\n8 \nCompute \n with Eqn. (13) \nd\np\nd\ni\n,\nl\ni\n,\nl\n(\nso\n)\n(\nta\n)\nL θ\n,\nθ\n,\nθ\n|\nX\n,\nX\n)\n9 \nCompute \n with Eqn. (16) \np\nc\nd\ni\n,\nl\ni\n,\nl\n10 \n  \n\nL\n\n(\n\nL\n−\n\n\nL\n),\n\nL\n,\n\nL\nθ\nθ\nc\nθ\nd\nθ\nc\nθ\nd\n(\n)\np\np\nc\nd\nˆ 'v\n'm\n11 \nCompute \n and \n based on \n  \nLθ\n1/ 2\nθ\n +\n\n\nm\n'/ (\n+\n\n)\n12 \n  \nˆ '\n' θ\nθ\n13"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sariç iç ek, L. Mete, Facial emotion recognition deficits in abstinent cannabis dependent patients",
      "authors": [
        "A Bayrakç",
        "E Sert",
        "N Zorlu",
        "A Erol"
      ],
      "year": "2015",
      "venue": "Compr. Psychiatry",
      "doi": "10.1016/j.comppsych.2014.11.008"
    },
    {
      "citation_id": "2",
      "title": "A driver fatigue recognition model based on information fusion and dynamic Bayesian network",
      "authors": [
        "G Yang",
        "Y Lin",
        "P Bhattacharya"
      ],
      "year": "2010",
      "venue": "Inf. Sci. (Ny)",
      "doi": "10.1016/j.ins.2010.01.011"
    },
    {
      "citation_id": "3",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Inf. Sci. (Ny)",
      "doi": "10.1016/j.ins.2019.09.005"
    },
    {
      "citation_id": "4",
      "title": "A Sociable Human-robot Interaction Scheme Based on Body Emotion Analysis",
      "authors": [
        "T Zhu",
        "Z Xia",
        "J Dong",
        "Q Zhao"
      ],
      "year": "2019",
      "venue": "Int. J. Control. Autom. Syst",
      "doi": "10.1007/s12555-017-0423-5"
    },
    {
      "citation_id": "5",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Inf. Sci. (Ny)",
      "doi": "10.1016/j.ins.2021.10.005"
    },
    {
      "citation_id": "6",
      "title": "Exploiting the potentialities of features for speech emotion recognition",
      "authors": [
        "D Li",
        "Y Zhou",
        "Z Wang",
        "D Gao"
      ],
      "year": "2021",
      "venue": "Inf. Sci. (Ny)",
      "doi": "10.1016/j.ins.2020.09.047"
    },
    {
      "citation_id": "7",
      "title": "Separation and recovery Markov boundary discovery and its application in EEG-based emotion recognition",
      "authors": [
        "X Wu",
        "B Jiang",
        "K Yu",
        "H Chen"
      ],
      "year": "2021",
      "venue": "Inf. Sci. (Ny)",
      "doi": "10.1016/j.ins.2021.04.071"
    },
    {
      "citation_id": "8",
      "title": "Recognizing Emotion from Multichannel EEG Signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng",
        "Sparsedgcnn"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2021.3051332"
    },
    {
      "citation_id": "9",
      "title": "Domain Adaptation Techniques for EEG-Based Emotion Recognition: A Comparative Study on Two Public Datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Muller-Putz"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Cogn. Dev. Syst",
      "doi": "10.1109/TCDS.2018.2826840"
    },
    {
      "citation_id": "10",
      "title": "Human emotion recognition using non linear and non stationary EEG signal",
      "authors": [
        "P Ghare",
        "A Paithane"
      ],
      "year": "2016",
      "venue": "Control Dyn. Optim. Tech. ICACDOT",
      "doi": "10.1109/ICACDOT.2016.7877739"
    },
    {
      "citation_id": "11",
      "title": "Stable Feature Selection for EEG-based Emotion Recognition",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "Y Liu",
        "G Muller-Putz"
      ],
      "year": "2018",
      "venue": "Int. Conf. Cyberworlds, 2018: pp",
      "doi": "10.1109/CW.2018.00042"
    },
    {
      "citation_id": "12",
      "title": "Graph attention networks, 6th Int",
      "authors": [
        "P Veličković",
        "A Casanova",
        "P Liò",
        "G Cucurull",
        "A Romero",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Conf. Learn. Represent. ICLR 2018 -Conf. Track Proc"
    },
    {
      "citation_id": "13",
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/taffc.2020.2994159"
    },
    {
      "citation_id": "14",
      "title": "EEG Emotion Recognition Using Dynamical Graph Convolutional Neural Networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2817622"
    },
    {
      "citation_id": "15",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Ca_Mpos"
      ],
      "year": "2016",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2015.10.049"
    },
    {
      "citation_id": "16",
      "title": "Locally Robust Feature Selection of EEG Signals for the Inter-subject Emotion Recognition",
      "authors": [
        "Z Yin",
        "W Zhang",
        "Z Zheng"
      ],
      "year": "2020",
      "venue": "2020 39th Chinese Control Conf",
      "doi": "10.23919/CCC50068.2020.9189239"
    },
    {
      "citation_id": "17",
      "title": "EEG-based emotion recognition in music listening",
      "authors": [
        "Y Lin",
        "C Wang",
        "T Jung",
        "T Wu",
        "S Jeng",
        "J Duann",
        "J Chen"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Biomed. Eng",
      "doi": "10.1109/TBME.2010.2048568"
    },
    {
      "citation_id": "18",
      "title": "EEG-based Emotion Recognition Using Self-Organizing Map for Boundary Detection",
      "authors": [
        "R Khosrowabadi",
        "H Quek",
        "A Wahab",
        "K Ang"
      ],
      "year": "2010",
      "venue": "th Int. Conf. Pattern Recognit",
      "doi": "10.1109/ICPR.2010.1031"
    },
    {
      "citation_id": "19",
      "title": "EEG-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Syst",
      "doi": "10.1016/j.knosys.2020.106243"
    },
    {
      "citation_id": "20",
      "title": "The graph neural network model",
      "authors": [
        "F Scarselli",
        "M Gori",
        "A Tsoi",
        "M Hagenbuchner",
        "G Monfardini"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Neural Networks",
      "doi": "10.1109/TNN.2008.2005605"
    },
    {
      "citation_id": "21",
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2016",
      "venue": "Semi-Supervised Classification with Graph Convolutional Networks"
    },
    {
      "citation_id": "22",
      "title": "Curvature graph neural network",
      "authors": [
        "H Li",
        "J Cao",
        "J Zhu",
        "Y Liu",
        "Q Zhu",
        "G Wu"
      ],
      "year": "2022",
      "venue": "Inf. Sci. (Ny)",
      "doi": "10.1016/j.ins.2021.12.077"
    },
    {
      "citation_id": "23",
      "title": "A Generalization of Convolutional Neural Networks to Graph-Structured Data",
      "authors": [
        "Y Hechtlinger",
        "P Chakravarti",
        "J Qin"
      ],
      "year": "2017",
      "venue": "A Generalization of Convolutional Neural Networks to Graph-Structured Data"
    },
    {
      "citation_id": "24",
      "title": "Inductive Representation Learning on Large Graphs",
      "authors": [
        "W Hamilton",
        "R Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Proc. 31st Int. Conf. Neural Inf. Process. Syst",
      "doi": "10.48550/arXiv.1706.02216"
    },
    {
      "citation_id": "25",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention Is All You Need"
    },
    {
      "citation_id": "26",
      "title": "EmotioNet: A 3-D Convolutional Neural Network for EEG-based Emotion Recognition",
      "authors": [
        "Y Wang",
        "Z Huang",
        "B Mccane",
        "P Neo"
      ],
      "year": "2018",
      "venue": "Jt. Conf. Neural Networks",
      "doi": "10.1109/IJCNN.2018.8489715"
    },
    {
      "citation_id": "27",
      "title": "Hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou",
        "Bi"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2885474"
    },
    {
      "citation_id": "28",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A Method for Stochastic Optimization"
    },
    {
      "citation_id": "29",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.25"
    },
    {
      "citation_id": "30",
      "title": "DEAP: A database for emotion analysis; Using physiological signals",
      "authors": [
        "S Koelstra",
        "C Mühl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "31",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals from Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE J. Biomed. Heal. Informatics",
      "doi": "10.1109/JBHI.2017.2688239"
    },
    {
      "citation_id": "32",
      "title": "Least squares support vector machine classifiers: a large scale algorithm",
      "authors": [
        "J Suykens",
        "L Lukas",
        "P Van",
        "D De",
        "M Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Process. Lett",
      "doi": "10.1023/A:1018628609742"
    },
    {
      "citation_id": "33",
      "title": "XGBoost: A Scalable Tree Boosting System, in: 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "XGBoost: A Scalable Tree Boosting System, in: 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min",
      "doi": "10.1145/2939672.2939785"
    },
    {
      "citation_id": "34",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proc. IEEE",
      "doi": "10.1109/5.726791"
    },
    {
      "citation_id": "35",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jurgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "36",
      "title": "Simplifying Graph Convolutional Networks",
      "authors": [
        "F Wu",
        "T Zhang",
        "A De Souza",
        "C Fifty",
        "T Yu",
        "K Weinberger"
      ],
      "year": "2019",
      "venue": "Simplifying Graph Convolutional Networks"
    },
    {
      "citation_id": "37",
      "title": "RANDOM FORESTS--RANDOM FEATURES",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Mach. Learn",
      "doi": "10.1023/A:1010933404324"
    },
    {
      "citation_id": "38",
      "title": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem",
      "authors": [
        "B Schölkopf",
        "A Smola",
        "K Müller"
      ],
      "year": "1998",
      "venue": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem",
      "doi": "10.1162/089976698300017467"
    },
    {
      "citation_id": "39",
      "title": "Domain Adaptation via Transfer Component Analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Y Qiang"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Neural Networks",
      "doi": "10.1109/TNN.2010.2091281"
    },
    {
      "citation_id": "40",
      "title": "Domain-Adversarial Training of Neural Networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "J. Mach. Learn. Res",
      "doi": "10.1007/978-3-319-58347-1_10"
    },
    {
      "citation_id": "41",
      "title": "Frontal EEG asymmetry as a moderator and mediator of emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2004",
      "venue": "Biol. Psychol",
      "doi": "10.1016/j.biopsycho.2004.03.002"
    },
    {
      "citation_id": "42",
      "title": "Effects of prefrontal cortex damage on emotion understanding: EEG and behavioural evidence",
      "authors": [
        "A Perry",
        "S Saunders",
        "J Stiso",
        "C Dewar",
        "J Lubell",
        "T Meling",
        "A Solbakk",
        "T Endestad",
        "R Knight"
      ],
      "year": "2017",
      "venue": "Brain",
      "doi": "10.1093/brain/awx031"
    },
    {
      "citation_id": "43",
      "title": "EEG Based Emotion Identification Using Unsupervised Deep Feature Learning",
      "authors": [
        "B Li",
        "Xiang Zhang",
        "; Peng",
        "Song",
        "; Dawei",
        "Guangliang Yu",
        "Yuexian Hou",
        "Hu"
      ],
      "year": "2015",
      "venue": "SIGIR2015 Work"
    },
    {
      "citation_id": "44",
      "title": "Emotion Recognition from EEG Signals Using Multidimensional Information in EMD Domain",
      "authors": [
        "Z Ning",
        "Z Ying",
        "T Li",
        "Z Chi",
        "Z Hanming",
        "Y Bin"
      ],
      "year": "2017",
      "venue": "Biomed Res. Int",
      "doi": "10.1155/2017/8317357"
    },
    {
      "citation_id": "45",
      "title": "EEGFuseNet: Hybrid Unsupervised Deep Feature Characterization and Fusion for High-Dimensional EEG with an Application to Emotion Recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng",
      "doi": "10.1109/TNSRE.2021.3111689"
    },
    {
      "citation_id": "46",
      "title": "An Efficient LSTM Network for Emotion Recognition from Multichannel EEG Signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y Lai",
        "G Zhao",
        "X Deng",
        "Y Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.3013711"
    },
    {
      "citation_id": "47",
      "title": "Emotion Recognition from EEG Signals using Hierarchical Bayesian Network with Privileged Information",
      "authors": [
        "Z Gao",
        "S Wang"
      ],
      "year": "2015",
      "venue": "th ACM Int. Conf. Multimed. Retr",
      "doi": "10.1145/2671188.2749364"
    },
    {
      "citation_id": "48",
      "title": "EEG-based emotion recognition utilizing wavelet coefficients",
      "authors": [
        "Ali Momennezhad"
      ],
      "year": "2018",
      "venue": "Multimed. Tools Appl",
      "doi": "10.1007/s11042-018-5906-8"
    },
    {
      "citation_id": "49",
      "title": "CROSS-CORPUS EEG-BASED EMOTION RECOGNITION",
      "authors": [
        "S Rayatdoost",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "IEEE 28th Int. Work. Mach. Learn. Signal Process",
      "doi": "10.1109/MLSP.2018.8517037"
    },
    {
      "citation_id": "50",
      "title": "Multi-modal emotion analysis from facial expressions and electroencephalogram",
      "authors": [
        "X Huang",
        "J Kortelainen",
        "G Zhao",
        "X Li",
        "A Moilanen",
        "T Seppnen",
        "M Pietikinen"
      ],
      "year": "2016",
      "venue": "Comput. Vis. Image Underst",
      "doi": "10.1016/j.cviu.2015.09.015"
    },
    {
      "citation_id": "51",
      "title": "Selecting transferrable neurophysiological features for interindividual emotion recognition via a shared-subspace feature elimination approach",
      "authors": [
        "W Zhang",
        "Z Yin",
        "Z Sun",
        "Y Tian",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Comput. Biol. Med",
      "doi": "10.1016/j.compbiomed.2020.103875"
    }
  ]
}