{
  "paper_id": "2110.03007v2",
  "title": "Unsupervised Multimodal Language Representations Using Convolutional Autoencoders",
  "published": "2021-10-06T18:28:07Z",
  "authors": [
    "Panagiotis Koromilas",
    "Theodoros Giannakopoulos"
  ],
  "keywords": [
    "multimodal temporal representations",
    "unsupervised multimodal language",
    "mutlimodal sentiment analysis",
    "multimodal emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Language Analysis is a demanding area of research, since it is associated with two requirements: combining different modalities and capturing temporal information. During the last years, several works have been proposed in the area, mostly centered around supervised learning in downstream tasks. In this paper we propose extracting unsupervised Multimodal Language representations that are universal and can be applied to different tasks. Towards this end, we map the word-level aligned multimodal sequences to 2-D matrices and then use Convolutional Autoencoders to learn embeddings by combining multiple datasets. Extensive experimentation on Sentiment Analysis (MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned representations can achieve near-state-of-the-art performance with just the use of a Logistic Regression algorithm for downstream classification. It is also shown that our method is extremely lightweight and can be easily generalized to other tasks and unseen data with small performance drop and almost the same number of parameters. The proposed multimodal representation models are open-sourced and will help grow the applicability of Multimodal Language.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Language Analysis focuses on extracting information from temporal language sequences using acoustic, visual and textual information. The two core application-specific problems of this area are Multimodal Sentiment Analysis, where the goal is to determine whether a utterance contains positive or negative sentiment, and Multimodal Emotion Recognition in which the objective is to predict the underlying emotion of the utterance. In the past years, research has been conducted in these problems, resulting in powerful models that achieve great performance on the particular tasks. Different types of fusion approaches, such as early fusion  [1] , memory fusion  [2, 3] , multistage fusion  [4]  and tensor fusion  [5]  have been examined. Modifications on LSTM architectures for multiview learning  [6]  or context-dependent analysis  [7]  have also been proposed, as well as the concept of attention on recurrent networks  [8] , context-aware attention  [9]  and some transformer architectures  [10]  have all been researched in depth.\n\nHowever, most of the works treat the learning process in a supervised fashion. The individual subdomains (i.e. the different applications and datasets) of Multimodal Language have the particularity of a high level of variability in the recording conditions and the experimental setup in general. For example the most widely used dataset for Emotion Recognition, IEMOCAP  [11] , is recorded in a laboratory with organized camera networks, while MOSEI  [3] , the largest dataset for Multimodal Sentiment Analysis, is collected from Youtube videos. Thus, strongly supervised methods are of limited use and cannot generalize to unseen recording set-ups and different tasks.\n\nOne of the few works that includes an unsupervised factor in their hybrid (including supervised and unsupervised factors) loss function is presented in  [12]  were the trained representations are sequential which limits the usage of such embeddings to sequential architectures. Another method that learns unsupervised representations is introduced in  [13] , where the proposed architecture extracts text-based embeddings and thus the information of other modalities is only used to enrich the textual information. However, the resulted representations of  [12]  and  [13]  cannot properly model tri-modal interactions, since there is always a modality that interacts with the bimodal representation of the remaining modalities rather than the actual input sequences. Adding that such approaches make the overall architecture task or data (eg.  [14] ) specific and that they do not test the generalization ability of the produced representations to other tasks and data set-ups, we have no proof for their performance on unseen data and tasks.\n\nIn this work, we propose a simple, yet powerful (in terms of both performance and computational efficiency), unsupervised Multimodal Language representation scheme that adopts a convolutional autoencoder architecture to discover multimodal relationships between aligned representations of audio, text and visual aligned feature sequences. The core contributions of the proposed method are the following:\n\n1. To our knowledge, this is the first method in the field of general Multimodal Language Analysis that is both multimodal in all three modalities and unsupervised 2. The proposed architecture is extremely transferable to other domains without negative impact neither on the performance or in the number of model parameters used. External experimentation proves that the performance is just slightly reduced when transferring knowledge from one dataset to another. And this happens without retraining the representation method itself, just using its embeddings in the target domain and classified by a simple logistic regression classifier. In this paper we follow a widely used approach for basic unimodal feature extraction and multimodal alignment, similar to the one in various proposed methods, such as  [8, 2, 5, 15, 3, 16, 10, 17, 9, 13] . More specifically, after extracting the features on each modality(visual, textual and aural), the procedure of word-level alignment that was firstly used for this task in  [17] , is performed. That is, the aligned video and audio features are obtained by computing the expectation of their modality feature values over each word utterance time interval. The feature extraction procedure results in a time sequence of feature vectors for each modality that can be formalized by a N x M matrix, where N is the number of timestamps and M is the number of features. Due to the performed alignment, sequences of all modalities include the same number of timestamps N and thus they can be combined in an N x (M audio +Mvision+Mtext) multimodal matrix X. This merged matrix is the selected initial multimodal representation in our method. The resulted matrix contains sequential information from different modalities that are represented by low level features with distinct arithmetic properties. For that reason, it is difficult for a learning algorithm to learn multimodal relationships and thus a proper normalization is needed in order to map the different vectors of each modality in an universal arithmetic range. In this work, we have selected to apply a sequence of standard and min-max scaling as a normalization procedure of the multimodal matrix X in order to produce the Xn (ie. normalized X) matrix.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Representation Learning",
      "text": "Xn is a two-dimensional representation of a multimodal sequence. By applying Convolutional Networks with 2-D kernels on Xn, we extract local information that is able to capture both uni-modal and cross-modal dynamics across time. More specifically, the kernels of the first layers mostly model unimodal dynamics that range in neighboring timestamps, while kernels of deeper layers are expected to be able to model cross-modal interactions across a wider range of time. Of course, for each task, there is an optimal balance between short-term unimodal and long-term multimodal modeling that can be found during the training procedure. Based on this formulation of the initial multimodal language sequences, we then select to train unsupervised Convolutional architectures, in order to extract universal representations for this problem. A common choice in the literature is that of Convolutional Autoencoders which have been proven to be effective in image-associated representation learning tasks. More details on the specifics of the Convolutonal AE we trained can be found in the experiments section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Downstream Classification",
      "text": "The Encoder part of the trained Convolutional AE can serve as a feature extractor that maps the N x (M audio + Mvision + Mtext) multimodal matrix to a K sized feature vector of the AE code. If the learned embeddings have high representational power, the application of a basic Machine Learning algorithm would be effective on downstream tasks of Multimodal Language Analysis. In this work, we focus on testing this hypothesis so our proposed methodology is illustrated in figure  1 , where the Convolutional AE is firstly fitted (in an unsupervised manner) on Multimodal Language datasets, while Logistic Regression is then used to train a model that maps the resulted embeddings to the class labels of a particular downstream task. Obviously, our test and validation sets are not be part of the AE's training procedure. The MOSEI  [3]  and IEMOCAP  [11]  datasets have been used for representation learning, as well as Sentiment Analysis and Emotion Recognition respectively. The processed version of IEMOCAP consists of 7318 segments of recorded dyadic dialogues annotated for the presence of the human emotions happiness, sadness, anger and neutral, while MOSEI is a large scale sentiment analysis dataset made up of 22,777 movie review video clips from more than 1000 online Youtube speakers. The data and feature extraction, as well as the train, validation and test splits were obtained from the widely used in the literature CMU-MultimodalSDK repository (https://github.com/A2Zadeh/CMU-MultimodalSDK).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "After feature extraction was performed (COVAREP  [18]  for audio, GloVe  [19]  for text and Facet [20] for visual) each segment was represented by a 74-d acoustic, a 35-d visual and 300-d textual feature vector. With the use of the aforementioned word-level alignment and concatenation, a matrix, X, of 20 x 409 dimensions is obtained for each utterance. After performing a sequence of standard and min-max scaling for each of the 409 features across all 20 timestamps and all dataset instances, the 2-dimensional inputs are properly prepossessed.\n\nIn order to train the multimodal representations in an unsupervised manner, we initialized a Convolutional AE with a 4-layer Encoder and its corresponding 4-layer Decoder. The kernel size of the first 2 layers of the Encoder was 3x3, while for the last two layers two 5x5 kernels were used. 2x2 padding along with 1x1 stride and 2x2 max-pooling were used in all layers. The channels of the layers were 32, 64, 128 and 10 respectively. This yields in a (flattened) representation of 250 dimensions in the code part of the AE. For better representation ability we used the Gelu activation function, while batch normalization was included across all layers. We trained the Convolutional AE using the mean-squared error as loss function, while Adam was chosen to be the optimizer with initial learning rate of 0.002 and a reduce-on-plateau learning rate scheduler. We also performed normal weight initialization and early stopping based on the mean-squared error of the validation set. Fig.  1 . Word-level alignment is performed among the feature sequence of each modality resulting in a 2-D tensor (\"image\") for each utterance. After proper normalization, a Convolutional Autoencoder is fitted in order to learn unsupervised representations. A Logistic Regression algorithm then trains a model on the downstream task using the produced embeddings as features. For the unsupervised experimental results, we gathered the train sets of MOSEI and IEMOCAP to form the train set and the corresponding validation sets in order to create a multi-dataset validation set. For different kinds of pre-training as well as the performance of the Convolutional AE in terms of the mean-squared error function, we refer the reader to section 3.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results On Downstream Classification",
      "text": "Using the Encoder part of the Convolutional AE as a feature extractor, we get a 250-d feature vector for each utterance. This means that the initial 20 x 409 (=8180 feature values) matrix is reduced to a 250-d vector (32 times less elements). These embeddings have been used to train a Logistic Regression model for the two downstream tasks of Sentiment Analysis and Emotion Recognition. Following the existing literature  [8, 2, 5, 15, 3, 16, 10, 17, 9, 13] , we report the binary accuracy and weighted averaged f1 metrics on sentiment for MOSEI, and on each of the 4 emotions of IEMOCAP in an one-vsall manner.\n\nIn tables 1 and 2 we compare our results with state-of-the-art and well established architectures in the tasks of Sentiment Analysis and Emotion Recognition. More specifically, we compare to MV-LSTM  [6] , MFN  [2] , Graph-MFN  [3] , RAVEN  [15] , MCTN  [12] , CIA  [9] , MulT  [10] , DF  [1] , BC-LSTM  [7] , MARN  [8] , TFN  [5] , RMFN  [4]  and ICCN  [13]  which, with the use of the CMU-MultimodalSDK repository, are trained and evaluated on the exact same sets. For each model we reference both the original work and the one that evaluated the method on the MOSEI or IEMOCAP datasets. For detailed explanation and comparison of the aforementioned architectures, we refer the reader to detailed reviews on Multimodal Sentiment Analysis  [21]  and Multimodal Emotion Recognition  [22] .\n\nAs seen in 1 and 2 the proposed generic mutlimodal representations achieve competitive performance with the use of a basic Machine Learning algorithm (Logistic Regression). More specifically, our lightweight method achieves an average (calculated on state-of-the art competitive models) performance on sentiment analysis with 2 absolute points above minimum and 4.5 below maximum binary accuracy. It also performs close to average and always above minimum for the task of Emotion Recognition. Therefore, a very simple simple classification algorithm can score the average performance of the SotA models, which clearly indicates that the learned multimodal language embeddings have strong representation power and can be used across different multimodal language tasks, despite the different recording set-up across datasets. The code for all experiments can be found in the mlr repository (https://github.com/lobracost/mlr).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we examine the role of the modalities combinations, as well as different datasets used for the unsupervised training of the embeddings. To this end, we trained the Convolutional AE on a range of different modality and training data combinations. For each combination, we report the mean-squared error in table  3 . In that way we can gain an insight on the quality of the learnt information compression for each modality combination. It is easily derived that the learnt embeddings of our method are more informative with respect to the original visual modality and less collective for the textual one. Using the produced embeddings of each modality combination in order to perform downstream classification, we end up with the results of table  4  for the MOSEI dataset. As it can be clearly seen (i) using all three modalities enriches the representation power, and (ii) the textual modality is the most informative for the task of Multimodal Language Analysis, a fact that is also known in the literature  [13] . Thus our method achieves to effectively express unimodal information and learn multimodal interactions of temporal language sequences.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Domain Generalization Ability",
      "text": "In order to examine whether information of one dataset can be used to other tasks, we performed different pretraining to the Convolutional Autoencoder and record the performance on downstream classification for the MOSEI dataset. The two crucial remarks from table 5 that make our work widely useful is that (i) our methodology leads to embeddings that can easily generalize to new data that have been recorded in a different way and for different tasks, since the performance for Sentiment Analysis when using embeddings pretrained on IEMOCAP is 0.5 absolute points (in terms of binary accuracy) below the ones trained in the MOSEI train data and (ii) our embeddings can be enriched from information of different datasets, since the classification using the embeddings trained on MOSEI & IEMO-CAP performs better than the ones trained on just the MOSEI training set. That clearly indicates that the proposed Encoder can serve as feature extractor in a range of Multimodal Language tasks and used for generalization in unseen data formats.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Complexity",
      "text": "In  [21]  the authors retrained 11 of the most powerful and widely used models for Multimodal Language Analysis and list the number of parameters for some of them. However, this study, due to different pretraining, reports smaller amount of parameters for some models (eg.  [23]  reports 1,549,321 parameters for MulT on the MO-SEI dataset with a different pretraining procedure). However, for the sake of a complete comparison, we choose to report the results of  [21] , though this may underestimate the number of parameters of the reported models. In table  6  we report the number of parameters of the proposed methods against the models reported in  [21] . It can be noticed that the Encoder used for feature extraction uses the same number of parameters across both datasets, which is not the case for other architectures where the parameter difference across tasks ranges from 123% to 340% (probably due to the number of classes). That is a direct outcome from the fact that our method does not require extensive fine-tuning techniques in order to be generalized to other tasks. Combining the feature extraction (256,202 parameters for both tasks) with the inference part (251 parameters for MOSEI and 1004 for IEMOCAP) we end up with an end-toend method that includes parameters in the range 256,202-257,206 while the rest architectures are highly greedy in terms of parameter amounts since they range from 415,521 to  23, 198, 398 . Thus the CAE-LR is a lightweight method that can be easily generalized to other tasks, without impact in the number of parameters (shown in Table  6 ), nor significant performance loss (shown in section 3.3.2).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we have presented a method for extracting unsupervised Multimodal Language representations using 2-D aligned multimodal sequences and Convolutional Autoencoders, in a totally unsupervised learning process. Extensive experimentation in Sentiment Analysis and Emotion Recognition prove that the performance of the proposed method is competitive related to the SotA, though the respective representation model is much smaller (around 200K parameters). Apart from being extremely lightweight, the proposed architecture is transferable to other domains without negative impact on the performance: experiments prove that transferring knowledge from one dataset to another, without retraining -not even tuning-the representation model itself (just training a logistic regression classifier on the extracted embeddings), does not significantly affect the classification performance in the target domain. Finally, the proposed representation method is openly available through an easy-touse model (https://github.com/lobracost/mlr) which can be directly applied to any similar Multimodal Language Modeling downstream task. The proposed approach could be further enriched in a future work, by training multimodal embeddings on a greater amount of datasets and reporting performance on more Multimodal Language tasks, such as Persuasiveness Prediction.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Word-level alignment is performed among the feature sequence of each modality resulting in a 2-D tensor (“image”) for each",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "ture sequences. The core contributions of the proposed method are"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "the following:"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "1. To our knowledge, this is the ﬁrst method in the ﬁeld of gen-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "eral Multimodal Language Analysis that\nis both multimodal"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "in all three modalities and unsupervised"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "2. The proposed architecture is extremely transferable to other"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "domains without negative impact neither on the performance"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "or\nin the number of model parameters used.\nExternal ex-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "perimentation proves that\nthe performance is just slightly re-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "duced when transferring knowledge from one dataset\nto an-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "other. And this happens without retraining the representation"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "method itself,\njust using its embeddings in the target domain"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "and classiﬁed by a simple logistic regression classiﬁer."
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "3. The performance\nis\ncompetitive\nrelated to the SotA, with"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "a major\nadvantage with regards\nto the\ncomplexity of\nthe"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "proposed method:\nthe proposed multimodal\nrepresentation"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "model has around 200K parameters."
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "4. The proposed method can even be used, with similar advan-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "tages (ultralight and accurate),\nto extract compact and low-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "dimensional unimodal or bimodal representations from a tem-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "poral sequence."
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": ""
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "5. This representation is openly available through an easy-to-use"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "model (https://github.com/lobracost/mlr). The ultra-light pre-"
        },
        {
          "tween aligned representations of audio,\ntext and visual aligned fea-": "trained Multimodal Language Analysis model (1MB of size)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "largest dataset for Multimodal Sentiment Analysis, is collected from"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "Youtube videos. Thus, strongly supervised methods are of\nlimited"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "use and cannot generalize to unseen recording set-ups and different"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "tasks."
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "One of\nthe few works that\nincludes an unsupervised factor\nin"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "their hybrid (including supervised and unsupervised factors)\nloss"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "function is presented in [12] were the trained representations are se-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "quential which limits the usage of such embeddings to sequential"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "architectures. Another method that\nlearns unsupervised representa-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "tions is introduced in [13], where the proposed architecture extracts"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "text-based embeddings and thus the information of other modalities"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "is only used to enrich the textual information. However, the resulted"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "representations of [12] and [13] cannot properly model tri-modal in-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "teractions, since there is always a modality that interacts with the bi-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "modal representation of the remaining modalities rather than the ac-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "tual input sequences. Adding that such approaches make the overall"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "architecture task or data (eg.\n[14]) speciﬁc and that\nthey do not\ntest"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "the generalization ability of\nthe produced representations to other"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "tasks and data set-ups, we have no proof\nfor\ntheir performance on"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "unseen data and tasks."
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "In this work, we propose a simple, yet powerful\n(in terms of"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "both performance and computational efﬁciency), unsupervised Mul-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "timodal Language representation scheme that adopts a convolutional"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "autoencoder architecture to discover multimodal\nrelationships be-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "tween aligned representations of audio,\ntext and visual aligned fea-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "ture sequences. The core contributions of the proposed method are"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "the following:"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "1. To our knowledge, this is the ﬁrst method in the ﬁeld of gen-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "eral Multimodal Language Analysis that\nis both multimodal"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "in all three modalities and unsupervised"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "2. The proposed architecture is extremely transferable to other"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "domains without negative impact neither on the performance"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "or\nin the number of model parameters used.\nExternal ex-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "perimentation proves that\nthe performance is just slightly re-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "duced when transferring knowledge from one dataset\nto an-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "other. And this happens without retraining the representation"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "method itself,\njust using its embeddings in the target domain"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "and classiﬁed by a simple logistic regression classiﬁer."
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "3. The performance\nis\ncompetitive\nrelated to the SotA, with"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "a major\nadvantage with regards\nto the\ncomplexity of\nthe"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "proposed method:\nthe proposed multimodal\nrepresentation"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "model has around 200K parameters."
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "4. The proposed method can even be used, with similar advan-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "tages (ultralight and accurate),\nto extract compact and low-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "dimensional unimodal or bimodal representations from a tem-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "poral sequence."
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": ""
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "5. This representation is openly available through an easy-to-use"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "model (https://github.com/lobracost/mlr). The ultra-light pre-"
        },
        {
          "Institute of Informatics & Telecommunications, NCSR - Demokritos, Greece": "trained Multimodal Language Analysis model (1MB of size)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: Comparison of binary accuracy and f1 weighted average",
      "data": [
        {
          "aligned video and audio features are obtained by computing the ex-": "pectation of their modality feature values over each word utterance",
          "metrics against literature models for sentiment classiﬁcation on MO-": "SEI. With ↑ and ↓ we denote the distance from the minimum and the"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "time interval.",
          "metrics against literature models for sentiment classiﬁcation on MO-": ""
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "The feature extraction procedure results in a time sequence of",
          "metrics against literature models for sentiment classiﬁcation on MO-": "Acc2"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "feature vectors for each modality that can be formalized by a N x",
          "metrics against literature models for sentiment classiﬁcation on MO-": "76.4"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "",
          "metrics against literature models for sentiment classiﬁcation on MO-": "76"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "M matrix, where N is the number of timestamps and M is the num-",
          "metrics against literature models for sentiment classiﬁcation on MO-": ""
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "",
          "metrics against literature models for sentiment classiﬁcation on MO-": "76.9"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "ber of\nfeatures. Due to the performed alignment, sequences of all",
          "metrics against literature models for sentiment classiﬁcation on MO-": ""
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "",
          "metrics against literature models for sentiment classiﬁcation on MO-": "79.1"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "modalities include the same number of timestamps N and thus they",
          "metrics against literature models for sentiment classiﬁcation on MO-": ""
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "",
          "metrics against literature models for sentiment classiﬁcation on MO-": "79.8"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "can be combined in an N x (Maudio+Mvision+Mtext) multimodal",
          "metrics against literature models for sentiment classiﬁcation on MO-": "80.4"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "matrix X. This merged matrix is the selected initial multimodal rep-",
          "metrics against literature models for sentiment classiﬁcation on MO-": "82.5"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "resentation in our method. The resulted matrix contains sequential",
          "metrics against literature models for sentiment classiﬁcation on MO-": "77.6"
        },
        {
          "aligned video and audio features are obtained by computing the ex-": "information from different modalities\nthat are represented by low",
          "metrics against literature models for sentiment classiﬁcation on MO-": "78 (2↑ 4.5↓)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Comparison of binary accuracy and f1 weighted average",
      "data": [
        {
          "can be directly applied without retraining on similar aligned": "datasets.",
          "we focus on testing this hypothesis so our proposed methodology": "is illustrated in ﬁgure 1, where the Convolutional AE is ﬁrstly ﬁt-"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "ted (in an unsupervised manner) on Multimodal Language datasets,"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "while Logistic Regression is then used to train a model that maps the"
        },
        {
          "can be directly applied without retraining on similar aligned": "2. CONVOLUTIONAL AUTOENCODER FOR",
          "we focus on testing this hypothesis so our proposed methodology": "resulted embeddings to the class labels of a particular downstream"
        },
        {
          "can be directly applied without retraining on similar aligned": "MULTIMODAL SEQUENCES",
          "we focus on testing this hypothesis so our proposed methodology": "task. Obviously, our\ntest and validation sets are not be part of\nthe"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "AE’s training procedure."
        },
        {
          "can be directly applied without retraining on similar aligned": "2.1.\n2-D multimodal sequence representation",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "In this paper we follow a widely used approach for basic unimodal",
          "we focus on testing this hypothesis so our proposed methodology": "3. EXPERIMENTS AND RESULTS"
        },
        {
          "can be directly applied without retraining on similar aligned": "feature extraction and multimodal alignment, similar\nto the one in",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "3.1. Experimental Setup"
        },
        {
          "can be directly applied without retraining on similar aligned": "various proposed methods,\nsuch as\n[8, 2, 5, 15, 3, 16, 10, 17, 9,",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "13]. More speciﬁcally, after extracting the features on each modal-",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "ity(visual, textual and aural), the procedure of word-level alignment",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "that was ﬁrstly used for this task in [17],\nis performed. That\nis,\nthe",
          "we focus on testing this hypothesis so our proposed methodology": "Table 1. Comparison of binary accuracy and f1 weighted average"
        },
        {
          "can be directly applied without retraining on similar aligned": "aligned video and audio features are obtained by computing the ex-",
          "we focus on testing this hypothesis so our proposed methodology": "metrics against literature models for sentiment classiﬁcation on MO-"
        },
        {
          "can be directly applied without retraining on similar aligned": "pectation of their modality feature values over each word utterance",
          "we focus on testing this hypothesis so our proposed methodology": "SEI. With ↑ and ↓ we denote the distance from the minimum and the"
        },
        {
          "can be directly applied without retraining on similar aligned": "time interval.",
          "we focus on testing this hypothesis so our proposed methodology": "maximum performance respectively"
        },
        {
          "can be directly applied without retraining on similar aligned": "The feature extraction procedure results in a time sequence of",
          "we focus on testing this hypothesis so our proposed methodology": "Model (Reference, Evaluation)\nAcc2\nF1-weighted"
        },
        {
          "can be directly applied without retraining on similar aligned": "feature vectors for each modality that can be formalized by a N x",
          "we focus on testing this hypothesis so our proposed methodology": "MV-LSTM ([6], [3])\n76.4\n76.4"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "MFN ([2], [3])\n76\n76"
        },
        {
          "can be directly applied without retraining on similar aligned": "M matrix, where N is the number of timestamps and M is the num-",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "Graph-MFN ([3], [10])\n76.9\n77.0"
        },
        {
          "can be directly applied without retraining on similar aligned": "ber of\nfeatures. Due to the performed alignment, sequences of all",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "RAVEN ([15], [10])\n79.1\n79.5"
        },
        {
          "can be directly applied without retraining on similar aligned": "modalities include the same number of timestamps N and thus they",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "MCTN ([12], [10])\n79.8\n80.6"
        },
        {
          "can be directly applied without retraining on similar aligned": "can be combined in an N x (Maudio+Mvision+Mtext) multimodal",
          "we focus on testing this hypothesis so our proposed methodology": "CIA ([9], [9])\n80.4\n78.2"
        },
        {
          "can be directly applied without retraining on similar aligned": "matrix X. This merged matrix is the selected initial multimodal rep-",
          "we focus on testing this hypothesis so our proposed methodology": "82.5\n82.3\nMulT ([10], [10])"
        },
        {
          "can be directly applied without retraining on similar aligned": "resentation in our method. The resulted matrix contains sequential",
          "we focus on testing this hypothesis so our proposed methodology": "Average\n77.6\n76.9"
        },
        {
          "can be directly applied without retraining on similar aligned": "information from different modalities\nthat are represented by low",
          "we focus on testing this hypothesis so our proposed methodology": "CAE-LR (Ours)\n78 (2↑ 4.5↓)\n76.3 (0.3↑ 4.3↓)"
        },
        {
          "can be directly applied without retraining on similar aligned": "level features with distinct arithmetic properties. For that reason,\nit",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "is difﬁcult for a learning algorithm to learn multimodal relationships",
          "we focus on testing this hypothesis so our proposed methodology": "The MOSEI\n[3] and IEMOCAP [11] datasets have been used"
        },
        {
          "can be directly applied without retraining on similar aligned": "and thus a proper normalization is needed in order to map the differ-",
          "we focus on testing this hypothesis so our proposed methodology": "for representation learning, as well as Sentiment Analysis and Emo-"
        },
        {
          "can be directly applied without retraining on similar aligned": "ent vectors of each modality in an universal arithmetic range. In this",
          "we focus on testing this hypothesis so our proposed methodology": "tion Recognition respectively. The processed version of IEMOCAP"
        },
        {
          "can be directly applied without retraining on similar aligned": "work, we have selected to apply a sequence of standard and min-max",
          "we focus on testing this hypothesis so our proposed methodology": "consists\nof\n7318\nsegments\nof\nrecorded\ndyadic\ndialogues\nanno-"
        },
        {
          "can be directly applied without retraining on similar aligned": "scaling as a normalization procedure of the multimodal matrix X in",
          "we focus on testing this hypothesis so our proposed methodology": "tated for\nthe presence of\nthe human emotions happiness,\nsadness,"
        },
        {
          "can be directly applied without retraining on similar aligned": "order to produce the Xn (ie. normalized X) matrix.",
          "we focus on testing this hypothesis so our proposed methodology": "anger and neutral, while MOSEI is a large scale sentiment analysis"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "dataset made up of 22,777 movie review video clips\nfrom more"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "than 1000 online Youtube speakers. The data and feature extraction,"
        },
        {
          "can be directly applied without retraining on similar aligned": "2.2. Representation Learning",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "as well as the train, validation and test splits were obtained from"
        },
        {
          "can be directly applied without retraining on similar aligned": "Xn is a two-dimensional representation of a multimodal sequence.",
          "we focus on testing this hypothesis so our proposed methodology": "the widely used in the literature CMU-MultimodalSDK repository"
        },
        {
          "can be directly applied without retraining on similar aligned": "By applying Convolutional Networks with 2-D kernels on Xn, we",
          "we focus on testing this hypothesis so our proposed methodology": "(https://github.com/A2Zadeh/CMU-MultimodalSDK)."
        },
        {
          "can be directly applied without retraining on similar aligned": "extract\nlocal\ninformation that\nis able to capture both uni-modal and",
          "we focus on testing this hypothesis so our proposed methodology": "After feature extraction was performed (COVAREP [18] for au-"
        },
        {
          "can be directly applied without retraining on similar aligned": "cross-modal dynamics across time. More speciﬁcally,\nthe kernels",
          "we focus on testing this hypothesis so our proposed methodology": "dio, GloVe [19] for text and Facet [20] for visual) each segment was"
        },
        {
          "can be directly applied without retraining on similar aligned": "of\nthe ﬁrst\nlayers mostly model unimodal dynamics that\nrange in",
          "we focus on testing this hypothesis so our proposed methodology": "represented by a 74-d acoustic, a 35-d visual and 300-d textual fea-"
        },
        {
          "can be directly applied without retraining on similar aligned": "neighboring timestamps, while kernels of deeper layers are expected",
          "we focus on testing this hypothesis so our proposed methodology": "ture vector. With the use of\nthe aforementioned word-level align-"
        },
        {
          "can be directly applied without retraining on similar aligned": "to be able to model cross-modal interactions across a wider range of",
          "we focus on testing this hypothesis so our proposed methodology": "ment and concatenation, a matrix, X, of 20 x 409 dimensions\nis"
        },
        {
          "can be directly applied without retraining on similar aligned": "time. Of course, for each task,\nthere is an optimal balance between",
          "we focus on testing this hypothesis so our proposed methodology": "obtained for each utterance. After performing a sequence of stan-"
        },
        {
          "can be directly applied without retraining on similar aligned": "short-term unimodal and long-term multimodal modeling that can",
          "we focus on testing this hypothesis so our proposed methodology": "dard and min-max scaling for each of the 409 features across all 20"
        },
        {
          "can be directly applied without retraining on similar aligned": "be found during the training procedure. Based on this formulation of",
          "we focus on testing this hypothesis so our proposed methodology": "timestamps and all dataset\ninstances,\nthe 2-dimensional\ninputs are"
        },
        {
          "can be directly applied without retraining on similar aligned": "the initial multimodal language sequences, we then select to train un-",
          "we focus on testing this hypothesis so our proposed methodology": "properly prepossessed."
        },
        {
          "can be directly applied without retraining on similar aligned": "supervised Convolutional architectures, in order to extract universal",
          "we focus on testing this hypothesis so our proposed methodology": "In order to train the multimodal representations in an unsuper-"
        },
        {
          "can be directly applied without retraining on similar aligned": "representations for this problem. A common choice in the literature",
          "we focus on testing this hypothesis so our proposed methodology": "vised manner, we initialized a Convolutional AE with a 4-layer En-"
        },
        {
          "can be directly applied without retraining on similar aligned": "is that of Convolutional Autoencoders which have been proven to",
          "we focus on testing this hypothesis so our proposed methodology": "coder and its corresponding 4-layer Decoder. The kernel size of the"
        },
        {
          "can be directly applied without retraining on similar aligned": "be effective in image-associated representation learning tasks. More",
          "we focus on testing this hypothesis so our proposed methodology": "ﬁrst 2 layers of the Encoder was 3x3, while for the last\ntwo layers"
        },
        {
          "can be directly applied without retraining on similar aligned": "details on the speciﬁcs of\nthe Convolutonal AE we trained can be",
          "we focus on testing this hypothesis so our proposed methodology": "two 5x5 kernels were used. 2x2 padding along with 1x1 stride and"
        },
        {
          "can be directly applied without retraining on similar aligned": "found in the experiments section.",
          "we focus on testing this hypothesis so our proposed methodology": "2x2 max-pooling were used in all layers. The channels of the layers"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "were 32, 64, 128 and 10 respectively.\nThis yields in a (ﬂattened)"
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "representation of 250 dimensions in the code part of\nthe AE. For"
        },
        {
          "can be directly applied without retraining on similar aligned": "2.3. Downstream Classiﬁcation",
          "we focus on testing this hypothesis so our proposed methodology": ""
        },
        {
          "can be directly applied without retraining on similar aligned": "",
          "we focus on testing this hypothesis so our proposed methodology": "better\nrepresentation ability we used the Gelu activation function,"
        },
        {
          "can be directly applied without retraining on similar aligned": "The Encoder part of\nthe trained Convolutional AE can serve as a",
          "we focus on testing this hypothesis so our proposed methodology": "while batch normalization was included across all layers. We trained"
        },
        {
          "can be directly applied without retraining on similar aligned": "feature extractor that maps the N x (Maudio + Mvision + Mtext)",
          "we focus on testing this hypothesis so our proposed methodology": "the Convolutional AE using the mean-squared error as loss function,"
        },
        {
          "can be directly applied without retraining on similar aligned": "multimodal matrix to a K sized feature vector of\nthe AE code.\nIf",
          "we focus on testing this hypothesis so our proposed methodology": "while Adam was chosen to be the optimizer with initial learning rate"
        },
        {
          "can be directly applied without retraining on similar aligned": "the learned embeddings have high representational power, the appli-",
          "we focus on testing this hypothesis so our proposed methodology": "of 0.002 and a reduce-on-plateau learning rate scheduler. We also"
        },
        {
          "can be directly applied without retraining on similar aligned": "cation of a basic Machine Learning algorithm would be effective on",
          "we focus on testing this hypothesis so our proposed methodology": "performed normal weight\ninitialization and early stopping based on"
        },
        {
          "can be directly applied without retraining on similar aligned": "downstream tasks of Multimodal Language Analysis.\nIn this work,",
          "we focus on testing this hypothesis so our proposed methodology": "the mean-squared error of the validation set."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "on IEMOCAP. With ↑ and ↓ we denote the distance from the minimum and the maximum performance respectively"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": ""
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "Model (Reference, Evaluation)"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "DF ([1], [15])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "MV-LSTM ([6], [15])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "BC-LSTM ([7], [15])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "MARN ([8], [15])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "TFN ([5], [5])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "RMFN ([4], [15])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "MFN ([2], [15])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "MCTN ([12], [10])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "MulT ([10], [10])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "ICCN ([13], [13])"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "Average"
        },
        {
          "Table 2. Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition": "CAE-LR (Ours)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 6: we report the number of parame-",
      "data": [
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "dataset combinations",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "Modalities",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "",
          "Table 6. Comparison of model parameters": "MOSEI"
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "Audio",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "",
          "Table 6. Comparison of model parameters": "6,804,859"
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "Vision",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "",
          "Table 6. Comparison of model parameters": "-"
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "Text",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "",
          "Table 6. Comparison of model parameters": "415,521"
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "[Audio, Vision]",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "[Vision, Text]",
          "Table 6. Comparison of model parameters": "874,651"
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "[Audio, Text]",
          "Table 6. Comparison of model parameters": "256,453"
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "[Audio, Vision, Text]",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "[Audio, Vision, Text]",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "[Audio, Vision, Text]",
          "Table 6. Comparison of model parameters": ""
        },
        {
          "Table 3. Convolutional AE pretraining for different modalities and": "",
          "Table 6. Comparison of model parameters": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 6: we report the number of parame-",
      "data": [
        {
          "[Audio, Vision, Text]\nMOSEI": "[Audio, Vision, Text]\nIEMOCAP",
          "23.12": "20.58"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "Table 4. Performance on MOSEI sentiment classiﬁcation using em-",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "beddings trained on different modality combinations",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "Modalities\nAcc2",
          "23.12": "F1-weighted"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "Audio\n71.06",
          "23.12": "59.1"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "Visual\n71.06",
          "23.12": "59.1"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "Text\n75.4",
          "23.12": "72.63"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "[Audio, Visual]\n71.06",
          "23.12": "59.87"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "[Visual, Text]\n71.04",
          "23.12": "59.01"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "[Audio, Text]\n77.86",
          "23.12": "76.09"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "78\n[Audio, Visual, Text]",
          "23.12": "76.3"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": "Table 5. Performance on MOSEI sentiment classiﬁcation using em-"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "beddings trained on different dataset combinations",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "Pretrained representations\nAcc2",
          "23.12": "F1-weighted"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "MOSEI\n77.2",
          "23.12": "75.4"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "IEMOCAP\n76.7",
          "23.12": "74.5"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "78\nMOSEI & IEMOCAP",
          "23.12": "76.3"
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        },
        {
          "[Audio, Vision, Text]\nMOSEI": "",
          "23.12": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Lan-\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "5. REFERENCES": "[1] Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "guage resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        },
        {
          "5. REFERENCES": "Tadas Baltruˇsaitis, and Louis-Philippe Morency, “Deep multi-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "2008."
        },
        {
          "5. REFERENCES": "modal fusion for persuasiveness prediction,” in Proceedings of",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "[12] Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe"
        },
        {
          "5. REFERENCES": "the 18th ACM International Conference on Multimodal Inter-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Morency, and Barnab´as P´oczos, “Found in translation: Learn-"
        },
        {
          "5. REFERENCES": "action, 2016, pp. 284–288.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "ing robust\njoint representations by cyclic translations between"
        },
        {
          "5. REFERENCES": "[2] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Po-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "modalities,” in Proceedings of the AAAI Conference on Artiﬁ-"
        },
        {
          "5. REFERENCES": "ria, Erik Cambria, and Louis-Philippe Morency, “Memory fu-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "cial Intelligence, 2019, vol. 33, pp. 6892–6899."
        },
        {
          "5. REFERENCES": "sion network for multi-view sequential\nlearning,”\nin Proceed-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Sun,\nPrathusha\nSarma, William Sethares,\nand"
        },
        {
          "5. REFERENCES": "ings of\nthe AAAI Conference on Artiﬁcial\nIntelligence, 2018,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Yingyu Liang,\n“Learning relationships between text, audio,"
        },
        {
          "5. REFERENCES": "vol. 32.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "and video via deep canonical correlation for multimodal\nlan-"
        },
        {
          "5. REFERENCES": "[3] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "the AAAI Conference on\nguage analysis,”\nin Proceedings of"
        },
        {
          "5. REFERENCES": "Cambria, and Louis-Philippe Morency, “Multimodal language",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Artiﬁcial Intelligence, 2020, vol. 34, pp. 8992–8999."
        },
        {
          "5. REFERENCES": "analysis in the wild: Cmu-mosei dataset and interpretable dy-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Jean-Benoit Delbrouck, No´e Tits, Mathilde Brousmiche, and"
        },
        {
          "5. REFERENCES": "namic fusion graph,” in Proceedings of the 56th Annual Meet-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "St´ephane Dupont,\n“A transformer-based joint-encoding for"
        },
        {
          "5. REFERENCES": "ing of\nthe Association for Computational Linguistics (Volume",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "arXiv preprint\nemotion recognition and sentiment analysis,”"
        },
        {
          "5. REFERENCES": "1: Long Papers), 2018, pp. 2236–2246.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "arXiv:2006.15955, 2020."
        },
        {
          "5. REFERENCES": "[4]\nPaul Pu Liang, Ziyin Liu, AmirAli Bagher Zadeh, and Louis-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "[15] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir"
        },
        {
          "5. REFERENCES": "Philippe Morency, “Multimodal language analysis with recur-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Zadeh, and Louis-Philippe Morency,\n“Words can shift: Dy-"
        },
        {
          "5. REFERENCES": "rent multistage fusion,” in Proceedings of the 2018 Conference",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "namically adjusting word representations using nonverbal be-"
        },
        {
          "5. REFERENCES": "on Empirical Methods in Natural Language Processing, 2018,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "haviors,”\nin Proceedings of the AAAI Conference on Artiﬁcial"
        },
        {
          "5. REFERENCES": "pp. 150–161.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Intelligence, 2019, vol. 33, pp. 7216–7223."
        },
        {
          "5. REFERENCES": "[5] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "[16] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan,"
        },
        {
          "5. REFERENCES": "and Louis-Philippe Morency, “Tensor fusion network for mul-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Paul Pu Liang, AmirAli Bagher Zadeh,\nand Louis-Philippe"
        },
        {
          "5. REFERENCES": "the 2017 Con-\ntimodal sentiment analysis,”\nin Proceedings of",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Morency,\n“Efﬁcient\nlow-rank multimodal\nfusion with"
        },
        {
          "5. REFERENCES": "ference on Empirical Methods in Natural Language Process-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "the 56th Annual\nmodality-speciﬁc factors,”\nin Proceedings of"
        },
        {
          "5. REFERENCES": "ing, 2017, pp. 1103–1114.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Meeting of the Association for Computational Linguistics (Vol-"
        },
        {
          "5. REFERENCES": "[6]\nShyam Sundar Rajagopalan, Louis-Philippe Morency, Tadas",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "ume 1: Long Papers), 2018, pp. 2247–2256."
        },
        {
          "5. REFERENCES": "Baltrusaitis, and Roland Goecke,\n“Extending long short-term",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "[17] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-"
        },
        {
          "5. REFERENCES": "memory for multi-view structured learning,” in European Con-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Philippe Morency, and Ruslan Salakhutdinov,\n“Learning fac-"
        },
        {
          "5. REFERENCES": "ference on Computer Vision. Springer, 2016, pp. 338–353.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "torized multimodal representations,”\nin International Confer-"
        },
        {
          "5. REFERENCES": "[7]\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "ence on Representation Learning, 2019."
        },
        {
          "5. REFERENCES": "Majumder,\nAmir\nZadeh,\nand\nLouis-Philippe Morency,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "[18] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio,"
        },
        {
          "5. REFERENCES": "“Context-dependent\nsentiment\nanalysis\nin\nuser-generated",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "and Stefan Scherer,\n“Covarep—a collaborative voice analy-"
        },
        {
          "5. REFERENCES": "videos,” in Proceedings of the 55th annual meeting of the asso-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "sis repository for speech technologies,”\nin 2014 ieee interna-"
        },
        {
          "5. REFERENCES": "ciation for computational linguistics (volume 1: Long papers),",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "tional conference on acoustics, speech and signal processing"
        },
        {
          "5. REFERENCES": "2017, pp. 873–883.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "(icassp). IEEE, 2014, pp. 960–964."
        },
        {
          "5. REFERENCES": "[8] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Jeffrey Pennington, Richard Socher, and Christopher D Man-"
        },
        {
          "5. REFERENCES": "Cambria, and Louis-Philippe Morency, “Multi-attention recur-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "ning,\n“Glove: Global vectors\nfor word representation,”\nin"
        },
        {
          "5. REFERENCES": "rent network for human communication comprehension,”\nin",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Proceedings of\nthe 2014 conference on empirical methods in"
        },
        {
          "5. REFERENCES": "Proceedings of the AAAI Conference on Artiﬁcial Intelligence,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "natural language processing (EMNLP), 2014, pp. 1532–1543."
        },
        {
          "5. REFERENCES": "2018, vol. 32.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "“imotions,” Oct 2021."
        },
        {
          "5. REFERENCES": "[9] Dushyant Singh Chauhan, Md Shad Akhtar, Asif Ekbal, and",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "Pushpak Bhattacharyya,\n“Context-aware interactive attention",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "[21] Dimitris Gkoumas, Qiuchi Li, Christina Lioma, Yijun Yu, and"
        },
        {
          "5. REFERENCES": "for multi-modal sentiment and emotion analysis,” in Proceed-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Dawei Song, “What makes the difference? an empirical com-"
        },
        {
          "5. REFERENCES": "ings of\nthe 2019 Conference on Empirical Methods in Natu-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "parison of fusion strategies for multimodal language analysis,”"
        },
        {
          "5. REFERENCES": "ral Language Processing and the 9th International Joint Con-",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Information Fusion, vol. 66, pp. 184–197, 2021."
        },
        {
          "5. REFERENCES": "ference on Natural Language Processing (EMNLP-IJCNLP),",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Panagiotis Koromilas and Theodoros Giannakopoulos,\n“Deep"
        },
        {
          "5. REFERENCES": "Hong Kong, China, Nov. 2019, pp. 5647–5657, Association",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "multimodal emotion recognition on human speech: A review,”"
        },
        {
          "5. REFERENCES": "for Computational Linguistics.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Applied Sciences, vol. 11, no. 17, 2021."
        },
        {
          "5. REFERENCES": "[10] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "Sijie Mai, Songlong Xing, and Haifeng Hu,\n“Analyzing mul-"
        },
        {
          "5. REFERENCES": "Kolter, Louis-Philippe Morency,\nand Ruslan Salakhutdinov,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "timodal sentiment via acoustic-and visual-lstm with channel-"
        },
        {
          "5. REFERENCES": "“Multimodal\ntransformer\nfor unaligned multimodal\nlanguage",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "IEEE/ACM Transac-\naware temporal convolution network,”"
        },
        {
          "5. REFERENCES": "the\nconference. Association\nsequences,”\nin Proceedings of",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "tions on Audio, Speech, and Language Processing, vol. 29, pp."
        },
        {
          "5. REFERENCES": "for Computational Linguistics. Meeting. NIH Public Access,",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": "1424–1437, 2021."
        },
        {
          "5. REFERENCES": "2019, vol. 2019, p. 6558.",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "[11] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        },
        {
          "5. REFERENCES": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N",
          "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "Behnaz Nojavanasghari",
        "Deepak Gopinath",
        "Jayanth Koushik",
        "Tadas Baltrušaitis",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "3",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Extending long short-term memory for multi-view structured learning",
      "authors": [
        "Shyam Sundar Rajagopalan",
        "Louis-Philippe Morency",
        "Tadas Baltrusaitis",
        "Roland Goecke"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "9",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Context-aware interactive attention for multi-modal sentiment and emotion analysis",
      "authors": [
        "Dushyant Singh Chauhan",
        "Shad Akhtar",
        "Asif Ekbal",
        "Pushpak Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "11",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "12",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "13",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Zhongkai Sun",
        "Prathusha Sarma",
        "William Sethares",
        "Yingyu Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "arxiv": "arXiv:2006.15955"
    },
    {
      "citation_id": "16",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "International Conference on Representation Learning"
    },
    {
      "citation_id": "19",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "20",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "21",
      "title": "What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis",
      "authors": [
        "Dimitris Gkoumas",
        "Qiuchi Li",
        "Christina Lioma",
        "Yijun Yu",
        "Dawei Song"
      ],
      "year": "2021",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "Deep multimodal emotion recognition on human speech: A review",
      "authors": [
        "Panagiotis Koromilas",
        "Theodoros Giannakopoulos"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "23",
      "title": "Analyzing multimodal sentiment via acoustic-and visual-lstm with channelaware temporal convolution network",
      "authors": [
        "Sijie Mai",
        "Songlong Xing",
        "Haifeng Hu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    }
  ]
}