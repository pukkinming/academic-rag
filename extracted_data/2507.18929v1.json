{
  "paper_id": "2507.18929v1",
  "title": "Mghft: Multi-Granularity Hierarchical Fusion Transformer For Cross-Modal Sticker Emotion Recognition",
  "published": "2025-07-25T03:42:26Z",
  "authors": [
    "Jian Chen",
    "Yuxuan Hu",
    "Haifeng Lu",
    "Wei Wang",
    "Min Yang",
    "Chengming Li",
    "Xiping Hu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although pre-trained visual models with text have demonstrated strong capabilities in visual feature extraction, sticker emotion understanding remains challenging due to its reliance on multi-view information, such as background knowledge and stylistic cues. To address this, we propose a novel multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view sticker interpreter based on Multimodal Large Language Models. Specifically, inspired by the human ability to interpret sticker emotions from multiple views, we first use Multimodal Large Language Models to interpret stickers by providing rich textual context via multiview descriptions. Then, we design a hierarchical fusion strategy to fuse the textual context into visual understanding, which builds upon a pyramid visual transformer to extract both global and local sticker features at multiple stages. Through contrastive learning and attention mechanisms, textual features are injected at different stages of the visual backbone, enhancing the fusion of global-and local-granularity visual semantics with textual guidance. Finally, we introduce a text-guided fusion attention mechanism to effectively integrate the overall multimodal features, enhancing semantic understanding. Extensive experiments on 2 public sticker emotion datasets demonstrate that MGHFT significantly outperforms existing sticker emotion recognition approaches, achieving higher",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Stickers, as a popular and expressive form of online communication, serve as an important medium for users to convey emotions in online chatting  [51] . Compared to plain text, stickers encapsulate a rich combination of visual elements and accompanying textual cues, enabling more vivid and nuanced emotional expression  [12, 35] . With the increasing integration of stickers into social media and instant messaging platforms, Sticker Emotion Recognition (SER) has emerged as a promising research direction, drawing growing interest from the academic community. SER aims to automatically identify and classify the emotional content conveyed through stickers by leveraging both visual and textual modalities  [26] . This research enhances natural and empathetic human-computer interaction while advancing affective computing with novel insights and practical implications for emotion-aware AI systems.\n\nCompared to conventional image-based emotion recognition tasks  [43, 46] , SER introduces a set of unique and more complex Figure  1 : Comparison with pre-trained models on SER30K dataset. Multimodal Large Language Models such as LLaVA and GPT-4o can understand the content of stickers but cannot effectively recognize the emotion in them. Pre-trained visual models such as CLIP and BLIP can extract visual features for classification, but the performance is not good enough. Compared to these methods, our proposed MGHFT demonstrates an obvious improvement in both accuracy and F1 score.\n\nchallenges. One of the most prominent difficulties lies in the implicit nature of emotional cues commonly found on stickers, such as culture background and style, which are often context-dependent and subtle  [37, 52] . Additionally, the vast diversity in sticker styles, themes, and visual representations further complicates the task of accurately identifying emotional content  [5] . Unlike standard facial expression recognition or sentiment analysis from images, interpreting emotions in stickers often requires a holistic understanding that combines multiple views, including the intent behind the sticker  [17, 18] , its stylistic tone, character actions, and even cultural or conversational context. Even for humans, accurate emotion understanding of stickers frequently relies on background knowledge and contextual inference, underscoring the complexity of SER and the need for advanced multimodal modeling approaches.\n\nRecent studies show that vision-language pre-trained models can effectively extract meaningful visual representations and have been widely applied to image understanding tasks  [10, 22, 30, 53] . On the one hand, multimodal foundation models such as LLaVA  [25]  and GPT-4o  [15]  excel in image-based generation tasks, leveraging their powerful visual-textual reasoning abilities and extensive background knowledge. On the other hand, vision-language models (VLMs) like CLIP  [30]  and BLIP  [22] , which are pre-trained through image-text alignment, have shown competitive performance in image classification tasks. Robust visual understanding provides a crucial foundation for the SER task, but it alone is not sufficient. More rich information about stickers from multi-views like style and details is also important. The results of the experiment presented in Figure  1  also prove this point. As illustrated in Figure  1 , experimental results on the SER30K dataset reveal a significant performance gap between these state-of-the-art VLMs and our proposed method. This discrepancy highlights an important insight. Accurately perceiving and interpreting subtle, implicit emotional cues like intention remains a significant challenge. Therefore, achieving effective emotion recognition in stickers requires a more comprehensive integration of contextual knowledge, emotional reasoning, and multi-view understanding.\n\nIn this context, inspired by the way humans understand the emotion of stickers from multiple views, we propose a Multi-Granularity Hierarchical Fusion Transformer (MGHFT) to effectively integrate contextual text information into visual feature extraction. First, we introduce a multi-view sticker interpreter that leverages the powerful visual understanding capabilities of Multimodal Large Language Models (MLLMs) to generate rich textual descriptions from four views: intent, overall style, main character, and character details. These descriptions serve as rich semantic cues, aligning the model's emotional reasoning with human perception. Second, we develop a hierarchical fusion strategy that incorporates multi-view textual features into different stages of visual representation learning. Specifically, we adopt a Pyramid Vision Transformer (PVT) as the visual backbone and inject view-specific textual features at each layer. In every stage, a ğ¶ğ¿ğ‘† token captures global semantics, while local features are selectively attended based on attention weights. Through attention-based fusion and contrastive learning, the model performs fine-grained visual-textual alignment at both the global and local granularity. Finally, we introduce a Text-Guided Fusion Attention (TGFA) mechanism that aggregates and aligns multiview textual semantics with visual representations across all stages, yielding robust emotion-aware features for SER. Extensive experiments on two large-scale sticker emotion datasets, SER30K and MET-MEME, show that our proposed MGHFT model consistently performs best, validating the effectiveness of MGHFT.\n\nOur main contributions can be summarized as follows:\n\nâ€¢ We design a multi-view sticker interpreter that utilizes MLLMs to decompose stickers into multiple views, providing multiview descriptions, thereby aligning human perceptual modalities to enrich the understanding of stickers. By introducing the knowledge of MLLMs, our model further improves the performance of emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Sticker Emotion Recognition",
      "text": "Accurate sticker emotion recognition remains a highly challenging task  [6] . As one of the most popular forms of imagery in online conversations, stickers, particularly those with cartoon-style illustrations, enable users to convey emotions more effectively than plain text alone  [13, 21, 38] . However, due to the nature of stickers being sent as standalone messages, they are also more prone to emotional misinterpretation  [3] .\n\nExisting work has focused on annotated datasets and evaluation protocols for training sticker emotion recognition models. For instance, Liu et al.  [26]",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-View Description",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Soft-Fusion",
      "text": "Multi-Granularity Fusion specifically for this task, while Xu et al.  [41]  focuses on collecting and organizing sticker data rich in metaphorical content. Other studies prioritize modeling relationships between multimodal features. For example, Luo et al.  [27]  introduces overlapping patch embeddings to preserve local relationships within sticker images, enhancing the stability of feature representations. On the other hand, Xu et al.  [42]  extracts visual features from linguistic attributes and incorporates a text-conditioned generative adversarial network to better convey underlying language concepts. Furthermore, recognizing that stickers with similar themes often exhibit similar emotional characteristics, recent research begins to explore the role of thematic information in sticker emotion recognition. Liu et al.  [26]  extracts global and local thematic information and applies attention mechanisms for end-to-end sticker emotion recognition. Chen et al.  [5]  further introduces a theme ID for each sticker and designs a theme-guided attention mechanism to enhance emotional recognition performance. Nonetheless, while these approaches achieve success in recognizing explicit emotional content in stickers, they still fall short in capturing deeper, implicit emotional cues such as metaphors and sarcasm, and that's what we consider in our work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Modal Sticker Understanding",
      "text": "In recent years, multimodal sticker understanding has attracted widespread attention  [2, 16, 20, 24, 47] . Unlike general multimodal understanding tasks, sticker comprehension relies more heavily on contextual and external information  [52] .\n\nMany multimodal fusion methods rely heavily on linguistic features extracted from OCR-recognized text. For example, references  [9, 54]  employ LSTM networks to encode textual information obtained from OCR. Wang et al.  [37] introduced both cross-modal and intra-modal attention mechanisms, along with a multimodal matching loss, to better capture the interactions between text and image for enhanced multimodal sticker understanding. Zheng et al.  [52]  focused on finer-grained features and proposed an object-level multimodal interaction framework. Considering that stickers frequently appear in dialogue scenarios, some works have integrated sticker understanding into conversational contexts. For instance, certain studies incorporate causal knowledge from text to help models recognize the emotional states expressed in stickers, thereby improving understanding performance  [4] . Other approaches leverage large-scale knowledge learning and distillation to enrich the model's feature representations to further enhance sticker comprehension  [40] . Additionally, some research emphasizes dialogue intent recognition, using intent-driven alignment mechanisms to achieve deeper understanding of stickers within conversations  [23] . Distinct from these methods, this study focuses on leveraging multiview emotional cues to guide multimodal fusion, aiming to enhance the model's ability to comprehend stickers more effectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "Problem Definition. SER is formulated as an image classification task. The input is a sticker image of size ğ» Ã— ğ‘Š Ã— 3, where ğ» and ğ‘Š denote the height and width, respectively, and 3 represents the RGB channels. Our objective is to feed a sticker into MGHFT, which leverages the capabilities of MLLMs to interpret the sticker from multiple views and accurately predict its emotional category. Overview. Our proposed MGHFT method can be divided into three main parts. First, we adopt MLLM to obtain multi-view description information of stickers to align with how humans understand stickers. Then, we use PVT to extract the multi-granularity features of the stickers in stages, and we design a novel multi-granularity crossmodal fusion mechanism to integrate the descriptive information from different views into different stages for better representation. Finally, a topic-guided fusion attention is proposed to fuse the visual features with textual features. The whole framework of our proposed MGHFT can be seen in Figure  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vision Backbone",
      "text": "PVT is a widely used vision backbone for extracting multi-scale image features, and we adopt it to support our hierarchical crossmodal fusion approach. PVT contains four stages, which generate feature maps at various scales through an asymptotic reduction strategy applied at each stage's patch-embedding layer. It also leverages a self-attention mechanism to preserve a global receptive field. Consequently, at each stage ğ‘–, PVT produces global features ğ‘‰ ğ‘– ğ‘” , which are CLS tokens that capture aggregated global information, and local features ğ‘‰ ğ‘– ğ‘™ , which are key tokens selected based on attention weights  [26] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mllm-Based Multi-View Sticker Interpreter",
      "text": "When encountering a new sticker, humans typically interpret it from multiple views. On the one hand, humans will consider the intended usage scenario  [29, 32]  and the overall visual style  [1, 32, 49]  to form an initial understanding of its meaning. On the other hand, humans pay attention to the specific characters and their detailed features to assess whether the sticker suits their communicative needs  [19, 23, 28, 31] . Inspired by this human sticker comprehension process, we divide sticker understanding into four views. To help the model better understand the contextual information of stickers, we designed an MLLM-based Multi-View Sticker Interpreter. This module extracts multi-perspective information from stickers and incrementally injects it into the model through a multi-stage fusion process. Specifically, we define four essential views to comprehensively capture sticker semantics: intention ğ· ğ¼ , overall style ğ· ğ‘† , main roles ğ· ğ‘€ğ‘… , and fine-grained character details ğ· ğ‘ƒğ¸ . These views enable a more holistic and detailed interpretation of sticker content. In recent years, MLLMs have demonstrated remarkable performance in the field of image understanding. These models can effectively activate their deep feature extraction capabilities through carefully designed prompts, enabling precise semantic comprehension of visual content. To enhance the semantic representation of sticker images, we employ the MLLM LLaVA-NeXT 1  [25] , leveraging its powerful cross-modal understanding ability to generate multi-view attribute descriptions of stickers.\n\nAs illustrated in Figure  3 , we adopt a multi-round interaction strategy with an MLLM, leveraging carefully crafted prompts to direct its attention to specific aspects of each sticker view. This process facilitates the generation of fine-grained and informative descriptions across multiple views. Subsequently, we adopt a BERTbased text encoder as the textual backbone to convert these attribute descriptions into dense textual representations for downstream processing as Eq. 2.\n\n1 https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Granularity Cross-Modal Fusion",
      "text": "After obtaining multi-view textual representations, our goal is to integrate this knowledge into the visual feature extraction process, guiding the model to focus on relevant visual cues. Specifically, we design a novel hierarchical fusion mechanism that injects multiview knowledge into different stages of visual representation learning. Considering that textual descriptions may capture both global and local aspects of a sticker, we introduce a multi-granularity fusion strategy to ensure effective alignment and integration between textual features and visual representations. On the one hand, local features ğ‘‰ ğ‘™ correspond to key visual tokens, and directly aligning them with textual features may lead to the loss of important visual semantics. To address this, we adopt a fusion strategy at the local granularity level, integrating textual and local visual features to obtain richer representations. On the other hand, as global features encapsulate the overall semantics of a sticker, aligning and integrating them with multi-view textual features helps the model better understand emotional information and learn more expressive global representations. To bridge the semantic gap between global visual features ğ‘‰ ğ‘” and textual features ğ‘‡ , we introduce an alignment loss. Furthermore, after completing all stages of feature extraction, we perform a final fusion between the overall global features and textual features to mitigate the potential performance impact caused by the sequential order of multi-view integration.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Local-Granularity. For Local Features ğ‘‰ ğ‘–",
      "text": "ğ‘™ , we adopt a Soft-Fusion attention mechanism to fuse it with textual features. Specifically, we first use linear layers to project the textual feature ğ‘‡ ğ‘– into ğ‘‡ â€² ğ‘– , which has the same dimension of local-granularity vision features ğ‘‰ ğ‘– ğ‘™ . Then, we compute the attention scores between ğ‘‰ ğ‘– ğ‘™ and ğ‘‰ â€²ğ‘– ğ‘¡ and apply softmax normalization to the scores. Then, it performs a weighted summation to enhance the representation of local features. The whole process can be illustrated in Eq. 3.\n\nIn this way, the proposed Soft-Fusion mechanism effectively leverages additional information from ğ‘‡ ğ‘– and utilizes the attention mechanism to enable ğ‘‰ ğ‘– ğ‘™ to focus on the most relevant parts of ğ‘‡ ğ‘– , significantly enriching the feature representation of ğ‘‰ ğ‘– ğ‘™ . Moreover, we introduce a residual connection to prevent gradient vanishing, ensuring that the original feature information is preserved and facilitating more robust learning. We adopt this mechanism in each stage to hierarchically fuse the different views of textual features into the local-granularity visual features. Then, we use MLPs to project the ğ‘‰ â€²ğ‘– ğ‘™ and ğ‘‡ â€² ğ‘– into the output dimension for further fusion. 3.3.2 Global-Granularity. For global features ğ‘‰ ğ‘– ğ‘” , we employ an alignment loss to leverage textual representations as guidance for global visual feature extraction, encouraging the model to learn cross-modal features effectively. This loss consists of two components: Contrastive Loss L ğ‘ğ‘™ and Multi-Level Cross-Entropy Loss (MLCE Loss) L ğ‘šğ‘™ğ‘ğ‘’  [45] . Together, they facilitate the alignment of visual and textual features, thereby enhancing the cross-modal representations effectively. We normalize the global visual and textual features to obtain the contrastive loss as Eq. 4.\n\nwhere ğ‘† ğ‘£ğ‘¡ is the similarity between global visual and textual features, ğœ is a temperature parameter, L ğ‘ğ‘’ denotes the Cross-Entropy Loss, and y represents the matching indices. Furthermore, we introduce the MLCE Loss to improve the visualtext alignment robustness, which utilizes cosine similarity and Kullback-Leibler Divergence ğ· KL to distill information between features as Eq. 5.\n\nwhere ğ¶ ğ‘™ and ğ¶ â„ are the normalized self-similarity matrices of The final cross-modal alignment loss is given by Eq. 6.\n\nwhere ğœ† is set as 30 to control the contribution of MLCE loss to the total alignment loss  [45] .\n\nWe then separately stack the global features â„ ğ‘” and textual features â„ ğ‘¡ and apply the Soft-Fusion Mechanism to enhance the globalgranularity features, resulting in â„ â€² ğ‘” , as shown in Eq. 7.\n\nUltimately, for both textual and visual features, we obtain corresponding local-granularity and global-granularity representations, which are then separately stacked for the final cross-modal fusion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Text-Guided Fusion Attention",
      "text": "To better guide the integration of visual representations with textual features, we propose a Text-Guided Fusion Attention (TGFA) mechanism, as shown in Figure  4 .\n\nSpecifically, given visual and textual features, we first apply cross-attention, where â„ ğ‘£ is used to generate the query ğ‘„ ğ‘£ while â„ ğ‘¡ produces the key ğ¾ ğ‘¡ and value ğ‘‰ ğ‘¡ . The output, denoted as ğ‘„ â€² ğ‘“ , integrates cross-modal features. Next, we use ğ‘„ â€² ğ‘“ as the query for a second cross-attention step, with â„ ğ‘£ providing the key ğ¾ ğ‘£ and value ğ‘‰ ğ‘£ . To further enhance cross-modal feature fusion, we incorporate an MLP and a residual connection. The complete process of TGFA is illustrated in Eq. 8.\n\nwhere ğ‘„ ğ‘— = ğ‘Š ğ‘ â„ ğ‘— + ğ‘,ğ¾ ğ‘— = ğ‘Š ğ‘˜ â„ ğ‘— + ğ‘,ğ‘‰ ğ‘— = ğ‘Š ğ‘£ â„ ğ‘— + ğ‘ are linear transformations. The final output, ğ¹ â€² ğ‘“ , is then fed into an FC layer for emotion recognition. It is also worth noting that we employ multi-head attention, and Eq. 8 illustrates the process for each individual attention head. We then use Cross Entropy Loss L ğ¶ğ¸ to optimize the model. The whole loss function of model training is shown as Eq. 9.\n\nwhere the weight of alignment loss is set as 0.5 to enhance the classification task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setting",
      "text": "Implementation Details. We adopt the Pytorch framework to conduct all experiments on 2 NVIDIA A800 80GB GPUs. We adopt the pre-trained PVT-small model  [39]  as the visual backbone. The max length of the sequence features obtained by the pre-trained Bert model  [7]  is set as 512. We adopt AdamW to optimize the model with a learning rate of 1e-3. The epoch is set as 50, while the batch size is set as 16. It should be noted that, adopting the same setting of LORA  [26] , the parameters of the Bert model are frozen, while the parameters of the PVT model are trainable. Evaluation metrics. Since SER is a multi-class classification task, we adopt precision, recall, accuracy, and F1 score as evaluation metrics. We also provide the precision of each category on SER30K for better comparison. Considering that SER30K is a large-scale dataset, we conduct most experiments on this dataset and use MET-MEME as a complement to robustness validation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison Results",
      "text": "On the SER30K dataset, we conduct two sets of comparative experiments: (1) compared with SER models or image classification models and (2) compared with pre-trained visual models. For (1), we adopt baselines include: VGG  [53]  and ResNet  [10] , ViT  [8] , WSC-NET  [44] , PDANet  [50] , LORA  [26] , MAM  [48] , and TGCA-PVT  [5] . We followed the baseline result of Ref.  [26] . For (2), we adopt CLIP-Base  [30] , BLIP2-2.7B  [22] , LLaVA-NeXT-Mistral-7B  [25]  and GPT-4o  [15] . For CLIP-Base and BLIP2-2.7B, only a classifier head is retrained for classification. Table  1  shows the emotion recognition results of (1) on the SER30K dataset with SER models. According to the comparison results shown in the table, our proposed MGHFT demonstrates a significant performance advantage on the SER30k dataset, achieving the highest accuracy of 73.31% and an F1 score of 72.52%. Compared to the best-performing baseline method, TGCA-PVT, which achieves 71.63% accuracy and 70.93% F1 score, MGHFT improves the accuracy by 2.3% and the F1 score by 2.2%. Compared to the image emotion recognition method MAM, our proposed MGHFT shows a much more significant improvement. These consistent improvements across both metrics highlight the effectiveness of our method in more comprehensively capturing the emotion-related features of stickers.\n\nIn particular, the results of (2) are shown in Figure  1 . The much lower performance results than other methods suggest that LLaVA and GPT-4o are still struggling with the task of directly understanding sticker emotions. In addition, CLIP and BLIP2, with their strong image feature extraction capability, achieved a relatively good performance after training on the classification head, but there is still a gap in comparison to the models specifically used for sticker emotion understanding. Compared with these pre-trained visual models, our proposed MGHFT model demonstrates significant advantages in emotion recognition results, achieving an improvement of 5.4% in F1 score and 4.0% in accuracy over the BLIP2, which is the best-performing pre-trained model. To further evaluate the robustness of our proposed MGHFT, we conduct experiments on the MET-MEME dataset, as shown in Table  2 . We adopt the baselines and experimental results from Ref.  [52] . Compared to the SER30k dataset, emotion recognition on MET-MEME is more challenging due to the significantly smaller number of training samples and greater visual-textual variation. Despite the increased difficulty, MGHFT still achieves the best performance across all evaluation metrics, with an accuracy of 35.13%, a precision of 34.75%, and a recall of 35.12%. Compared with the strongest baseline method, MGMCF, which achieves a precision of 34.36%, a precision of 37. 77% and a recall of 34.88%, our method achieves a 0.77 percentage point gain in accuracy while maintaining a comparable performance in recall and more balanced precision. These results demonstrate that MGHFT not only achieves state-ofthe-art performance on large-scale datasets like SER30K but also maintains strong generalization and robustness in low-resource, more challenging scenarios such as MET-MEME.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To verify the effectiveness of each individual component in our proposed MGHFT model, we conduct comprehensive ablation studies. Specifically, we perform two types of experiments: retaining each module individually to assess its standalone contribution and removing each module separately to evaluate its impact on overall performance. CL means the contrastive learning between ğ‘‰ ğ‘” and ğ‘‡ ğ‘– used in our method, TGFA denotes our proposed Text-Guided Fusion Attention module, while GF and LF represent the soft fusion mechanisms for global-granularity and local-granularity feature fusion, respectively. Results are shown in Table  3 . As shown in Table  3 , none of the ablated variants outperform the full model, which underscores the complementary roles and necessity of each component in achieving optimal performance. For the model with a single module, we can observe that the model with only the TGFA module exhibited the most significant performance drop compared to the full model. Compared to the backbone without any module, the model with CL has the highest F1 score (from 69.89% to 70.98%). This result is also consistent with the results of removing each module from the full model: the model without CL has the most significant performance degradation compared to the full model than with the other modules removed.\n\nFurthermore, we evaluate combinations of two components to explore their joint effect. We also adopt the commonly used crossattention module to replace our proposed Soft-Fusion mechanism (CA). CG refers to using CL and GF, GL refers to using GF and LF, and CT refers to using CL and TGFA. The performance of each variant is shown in Figure  5 .\n\nThe results indicate that combining any two of the proposed modules consistently enhances sticker emotion classification performance. Notably, the combinations of CL with GF and CL with TGFA yielded the most significant improvements. Specifically, CL helps align multi-view textual features with global visual representations, effectively narrowing the semantic gap between modalities and enhancing the performance of both GF and TGFA modules. Interestingly, the combination of GF and LF, without contrastive learning, resulted in a more substantial improvement in the F1 score than accuracy. This suggests that multi-granularity feature fusion improves the model's ability to recognize diverse emotions, leading to more balanced classification outcomes. The results of CA also show that our proposed Soft-Fusion not only achieves better fusion performance but also avoids introducing extra parameters, thereby improving computational efficiency.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Analysis Of Text Description",
      "text": "Moreover, we conduct experiments to further evaluate the effectiveness of the proposed multi-view textual descriptions in sticker emotion understanding. Given that our method leverages contextual descriptions encompassing multiple views, it is essential to investigate how the integration of various views at different stages influences performance. Additionally, to assess whether the diverse views contribute richer contextual information, we replace all views with only the character details (ğ‘‡ 4 ) and evaluate the impact. Furthermore, we concatenate the descriptions from all views to form a unified multi-view representation ğ‘‡ , integrating it at each stage of the process. The experimental results, as presented in Table  4 , demonstrate the effectiveness and importance of incorporating multi-view textual information. According to the experimental results, we observe that incorporating contextual knowledge in the order of [ğ‘‡ 1 ,ğ‘‡ 2 ,ğ‘‡ 3 ,ğ‘‡ 4 ] yields the best performance for sticker emotion understanding. Interestingly, this order also aligns with the way humans typically interpret emotional content in stickers-first considering whether the intention  is sarcastic, then perceiving the overall style, followed by attention to characters and posture details. Moreover, we find that injecting only the character details (ğ‘‡ 4 ) at each stage results in the lowest classification performance. This highlights the importance of our proposed multi-view description, which provides richer and more diverse contextual information, thereby enabling the model to better comprehend the emotional content embedded in the stickers. Additionally, incorporating all views at every stage also leads to performance degradation, which strongly supports the effectiveness of our proposed hierarchical fusion strategy. By guiding the model to focus on different information at different stages, this approach mitigates conflicts among multiple views and facilitates more effective cross-modal understanding.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "T-Sne Visualization Analysis",
      "text": "We hypothesize that the performance improvement of the MGHFT model primarily stems from its more efficient sticker representation method during the learning process. To validate this assumption, we conducted a visual analysis of the sticker representations using t-distributed Stochastic Neighbor Embedding (t-SNE)  [36] , as illustrated in Figure  6 . The visualization results reveal that the sticker features extracted by the CLIP, BLIP and LLaVA models exhibit relatively scattered distributions with limited inter-class separability. In contrast, although the sticker features learned by the MGHFT model also maintain a degree of dispersion in the semantic space, images belonging to the same emotional category form more compact clusters. This observation suggests that, compared to conventional pre-trained vision models, our approach achieves a more distinct separation of sticker features across different emotional categories in the feature space, thereby demonstrating its superior capability in learning effective sticker representations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Attention Visualization Analysis",
      "text": "To better illustrate the effectiveness of our proposed MGHFT model, we visualize the model's regions of interest using attention heatmaps and compare them with those generated by models without multiview descriptions (W/O MVD). Specifically, we extract attention weights from each token in the final layer of the visual backbone and overlay them onto the original stickers, as shown in Figure  7 . The visualizations clearly show that MGHFT successfully highlights the most informative regions of the stickers, such as facial expressions, eye direction, and other crucial emotional cues. In contrast, models without multi-view descriptions tend to focus on irrelevant regions. Notably, MGHFT assigns lower attention weights to semantically unimportant background areas, reflecting its ability to filter out non-informative content and concentrate on emotionally salient features. These visual comparisons provide strong evidence for the effectiveness of our multi-view textual guidance in directing the visual model's focus toward more critical emotional features.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "Inspired by the way humans understand the emotion of stickers, this paper proposes a novel Multi-granularity Hierarchical Fusion Transformer to overcome the limitation of existing pre-trained visual models for sticker emotion recognition. Our approach first leverages a MLLM to generate multi-view textual descriptions of stickers, providing rich semantic background knowledge to improve the model's understanding of stickers' emotions. Built upon the PVT architecture, we innovatively integrate contrastive learning with attention mechanisms to achieve multi-granularity fusion of textual features. Furthermore, we design a text-guided multimodal attention fusion mechanism that effectively integrates visual and textual features, significantly enhancing the representational power and emotion classification accuracy of sticker data. Extensive experimental results demonstrate that each component of the proposed framework contributes meaningfully to the overall performance. We hope this work can serve as a valuable reference and inspiration for future research in sticker emotion analysis.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison with pre-trained models on SER30K",
      "page": 2
    },
    {
      "caption": "Figure 1: also prove this point. As illustrated in",
      "page": 2
    },
    {
      "caption": "Figure 1: , experimental results on the SER30K dataset reveal a sig-",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework of our proposed MGHFT. MGHFT adopts PVT as the backbone to extract the multi-granularity features of",
      "page": 3
    },
    {
      "caption": "Figure 3: , we adopt a multi-round interaction",
      "page": 4
    },
    {
      "caption": "Figure 3: Multi-view sticker description generation.",
      "page": 4
    },
    {
      "caption": "Figure 4: Framework of TGFA for cross-modal fusion.",
      "page": 5
    },
    {
      "caption": "Figure 4: Specifically, given visual and textual features, we first apply",
      "page": 5
    },
    {
      "caption": "Figure 1: . The much",
      "page": 6
    },
    {
      "caption": "Figure 5: The results indicate that combining any two of the proposed",
      "page": 7
    },
    {
      "caption": "Figure 5: Performance of each model variants.",
      "page": 7
    },
    {
      "caption": "Figure 6: t-SNE visualization of 642 sticker features extracted",
      "page": 8
    },
    {
      "caption": "Figure 6: The visualization results reveal that the",
      "page": 8
    },
    {
      "caption": "Figure 7: Visualization of the region of interest of the model",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the emotion recognition results of (1) on the To further evaluate the robustness of our proposed MGHFT,",
      "data": [
        {
          "Precisiononeachemotioncategory": "Anger Disgust Fear Happiness Neutral Sadness Surprise"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0018\u0000\u0015",
          "Column_13": "",
          "Column_14": "",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0015\u0000\u0013",
          "Column_2": "",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0014\u0000\u0019",
          "Column_7": "",
          "Column_8": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0016\u0000\u0016",
          "Column_13": "",
          "Column_14": "",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u001b\u0000\u0019",
          "Column_7": "",
          "Column_8": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "",
          "Column_10": "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0019\u0000\u0017",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "",
          "Column_4": "\u0000\u001a\u0000\u0014\u0000\u0011\u0000\u0014\u0000\u0018",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "\u0000&",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "\u0000*",
          "Column_4": "",
          "Column_5": "\u0000*",
          "Column_6": "\u0000/",
          "Column_7": "",
          "Column_8": "\u0000&",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "\u00007",
          "Column_10": "",
          "Column_11": "\u0000&",
          "Column_12": "\u0000$",
          "Column_13": "",
          "Column_14": "\u00000\u0000*",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "\u0000+\u0000)\u00007",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "re5\nult\ntive\nng\n,th\nulte\ny.T\nmo\nnce\nrpr\nbu\nomp\nlys\nec\nep\ners\nion\now\nrfo\nute\nec\neco\nlti-\nss.\nth\next\ne4",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": ":P\ni-vi\nly\nthe\nec\ndin\nhis\ndel\ndc\nop\ntal\nut\nis\nond\nrop\ntan\nse\nthe\nrma\nric\nhar\nnc\nvie\nThe\ne e\nual\n:E",
          "Column_4": "erf\new\nnar\npe\nom\na\nsu\nâ€™sa\nlas\nose\nsoa\natio\nof\nuc\nos\ndin\nnco\nin\nnc\nher\nact\nate\nwr\nex\nffec\ninf\nffec",
          "Column_5": "or\ntex\nrow\nrfo\nbin\nmor\ngge\nbili\nsifi\ndS\nvoi\nnal\nTe\ntex\ned\ng.\nmp\nteg\ne.A\nco\nerd\nnat\nepr\nper\ntiv\norm\nto",
          "Column_6": "ma\ntua\ning\nrm\nati\nesu\nsts\ntyt\ncati\noft-\nds\neffi\nxt\nper\nmul\nGiv\nass\nrati\nddi\nnte\neta\neth\nese\nim\nene\nati\nfm",
          "Column_7": "nce\nlfe\nth\nanc\non\nbs\nth\nor\non\nFus\nint\ncie\nD\nim\nti-\nen\nin\non\ntio\nxtu\nils\ned\nnta\nent\nss\non.\nul",
          "Column_8": "of\natu\nese\neo\nof\ntan\natm\neco\nou\nion\nrod\nnc\nes\nent\nview\ntha\ngm\nofv\nnall\nali\n(ğ‘‡\n4\nesc\ntio\nalr\nand\nti-v",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "ea\nres\nma\nfb\nGF\ntial\nult\ngni\ntco\nno\nucin\ny.\ncri\nst\nte\ntou\nulti\nari\ny,t\nnfor\n)an\nript\nnğ‘‡,\nesu\nim\nie",
          "Column_10": "ch\nwi\nnti\noth\nand\nim\ni-g\nze\nme\nto\ng\npt\nofu\nxtu\nr\npl\nous\noa\nm\nd\nio\nin\nlts\npo\nwt",
          "Column_11": "mo\nth\ncg\nG\nLF\npro\nran\ndiv\ns.T\nnly\nextr\nio\nrth\nal\nmet\nevi\nvi\nsse\natio\neva\nnsf\nteg\n,as\nrta\next",
          "Column_12": "de\nglob\napb\nFan\n,w\nvem\nula\nerse\nhe\nach\nap\nn\ner\ndes\nhod\new\news\nssw\nn,w\nlua\nrom\nrati\npre\nnce\nde",
          "Column_13": "lv\nal\net\nd\nith\nen\nrit\ne\nres\niev\nara\neva\ncrip\nle\ns,i\nat\nhe\ner\ntet\nal\nng\nse\no\nscr",
          "Column_14": "ari\nvis\nwee\nTG\nout\ntin\nyfe\nmoti\nult\nes\nme\nlua\ntio\nver\ntis\ndiff\nthe\nepl\nhe\nlvi\nita\nnte\nf in\nipt",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "ant\nual\nnm\nFA\nco\nthe\natu\nons\nsof\nbett\nters\ntet\nnsi\nage\ness\nere\nrth\nace\nimp\new\ntea\ndin\ncor\nion",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "1",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "Sta",
          "Column_4": "ge",
          "Column_5": "2",
          "Column_6": "Stag",
          "Column_7": "e3",
          "Column_8": "St",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "age",
          "Column_10": "4",
          "Column_11": "Ac",
          "Column_12": "cura",
          "Column_13": "cy",
          "Column_14": "",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "F1",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "",
          "Column_4": "ğ‘‡\n2\nğ‘‡\n2\nğ‘‡\n1\nğ‘‡\n1\nğ‘‡\n4\nğ‘‡\n3\nğ‘‡\n4\nğ‘‡",
          "Column_5": "",
          "Column_6": "ğ‘‡\n3\nğ‘‡\n4\nğ‘‡\n3\nğ‘‡\n4\nğ‘‡\n1\nğ‘‡\n2\nğ‘‡\n4\nğ‘‡",
          "Column_7": "",
          "Column_8": "",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "ğ‘‡\n4\nğ‘‡\n3\nğ‘‡\n4\nğ‘‡\n3\nğ‘‡\n2\nğ‘‡\n1\nğ‘‡\n4\nğ‘‡",
          "Column_10": "",
          "Column_11": "7\n7\n7\n7\n7\n7\n7\n7",
          "Column_12": "3.31\n3.02\n3.08\n3.10\n2.97\n3.13\n2.92\n3.02",
          "Column_13": "",
          "Column_14": "7\n7\n7\n7\n7\n7\n7\n7",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "2.52\n2.39\n2.46\n2.49\n2.39\n2.49\n2.31\n2.28",
          "Column_16": ""
        },
        {
          "\u0000)\u0000\u0014\n\u0000$\u0000F\u0000F": "",
          "Column_2": "to\ntua\nanc\noal\ntin",
          "\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0015": "the\nlkn\nef\nign\nst",
          "Column_4": "",
          "Column_5": "per\nled\ntick\nith\ners",
          "Column_6": "ime\nge\ner\nthe\nâ€”fir",
          "Column_7": "",
          "Column_8": "alr\nhe\noti\nay\ncon",
          "\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u001c\u0000\u0018": "esu\nord\nonu\nhum\nsid",
          "Column_10": "",
          "Column_11": "we\nf[ğ‘‡\ners\nsty\nng",
          "Column_12": "ob\n,ğ‘‡\n1\ntan\npic\nwhe",
          "Column_13": "",
          "Column_14": "vet\n,ğ‘‡\n3\ng.I\nyin\nert",
          "\u0000\u001a\u0000\u0016\u0000\u0011\u0000\u0016\u0000\u0014": "hat\n]y\n4\nnte\nter\nhei",
          "Column_16": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happiness\nInput MGHFT W/O MVD": "Surprise\nInput MGHFT W/O MVD"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Recognition-by-components: a theory of human image understanding",
      "authors": [
        "Irving Biederman"
      ],
      "year": "1987",
      "venue": "Psychological review"
    },
    {
      "citation_id": "2",
      "title": "Modularized networks for fewshot hateful meme detection",
      "authors": [
        "Rui Cao",
        "Ka-Wei Lee",
        "Jing Jiang"
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM Web Conference 2024"
    },
    {
      "citation_id": "3",
      "title": "Complex and ambiguous: Understanding sticker misinterpretations in instant messaging",
      "authors": [
        "Yoonjeong Cha",
        "Jongwon Kim",
        "Sangkeun Park",
        "Yong Yi",
        "Uichin Lee"
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "4",
      "title": "Deconfounded Emotion Guidance Sticker Selection with Causal Inference",
      "authors": [
        "Jiali Chen",
        "Yi Cai",
        "Ruohang Xu",
        "Jiexin Wang",
        "Jiayuan Xie",
        "Qing Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "TGCA-PVT: Topic-Guided Context-Aware Pyramid Vision Transformer for Sticker Emotion Recognition",
      "authors": [
        "Jian Chen",
        "Wei Wang",
        "Yuzhu Hu",
        "Junxin Chen",
        "Han Liu",
        "Xiping Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "The role of emotion in computer-mediated communication: A review",
      "authors": [
        "Daantje Derks",
        "Agneta Fischer",
        "E Arjan",
        "Bos"
      ],
      "year": "2008",
      "venue": "Computers in human behavior"
    },
    {
      "citation_id": "7",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "8",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "9",
      "title": "BROWALLIA at Memotion 2.0 2022: Multimodal memotion analysis with modified ogb strategies",
      "authors": [
        "Baishan Duan",
        "Yuesheng Zhu"
      ],
      "year": "2022",
      "venue": "Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection"
    },
    {
      "citation_id": "10",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Rethinking spatial dimensions of vision transformers",
      "authors": [
        "Byeongho Heo",
        "Sangdoo Yun",
        "Dongyoon Han",
        "Sanghyuk Chun",
        "Junsuk Choe",
        "Seong Joon Oh"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "12",
      "title": "Nice picture comment!\" Graphicons in Facebook comment threads",
      "authors": [
        "Susan Herring",
        "Ashley Dainas"
      ],
      "year": "2017",
      "venue": "Nice picture comment!\" Graphicons in Facebook comment threads"
    },
    {
      "citation_id": "13",
      "title": "Aptness: Incorporating appraisal theory and emotion support strategies for empathetic response generation",
      "authors": [
        "Yuxuan Hu",
        "Minghuan Tan",
        "Chenwei Zhang",
        "Zixuan Li",
        "Xiaodan Liang",
        "Min Yang",
        "Chengming Li",
        "Xiping Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 33rd ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "14",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Gpt-4o system card",
      "authors": [
        "Aaron Hurst",
        "Adam Lerer",
        "Adam Goucher",
        "Adam Perelman",
        "Aditya Ramesh",
        "Aidan Clark",
        "Akila Ostrow",
        "Alan Welihinda",
        "Alec Hayes",
        "Radford"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "arxiv": "arXiv:2410.21276"
    },
    {
      "citation_id": "16",
      "title": "Memeguard: An llm and vlm-based framework for advancing content moderation via meme intervention",
      "authors": [
        "Prince Jha",
        "Raghav Jain",
        "Konika Mandal",
        "Aman Chadha",
        "Sriparna Saha",
        "Pushpak Bhattacharyya"
      ],
      "year": "2024",
      "venue": "Memeguard: An llm and vlm-based framework for advancing content moderation via meme intervention",
      "arxiv": "arXiv:2406.05344"
    },
    {
      "citation_id": "17",
      "title": "An efficient deep learning technique for facial emotion recognition",
      "authors": [
        "Asad Khattak",
        "Muhammad Asghar",
        "Mushtaq Ali",
        "Ulfat Batool"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "18",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "Chul Byoung",
        "Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "19",
      "title": "Reading images: The grammar of visual design",
      "authors": [
        "Gunther Kress",
        "Theo Van Leeuwen"
      ],
      "year": "2020",
      "venue": "Reading images: The grammar of visual design"
    },
    {
      "citation_id": "20",
      "title": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
      "authors": [
        "Gitanjali Kumari",
        "Kirtan Jain",
        "Asif Ekbal"
      ],
      "year": "2024",
      "venue": "M3Hop-CoT: Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought",
      "arxiv": "arXiv:2410.09220"
    },
    {
      "citation_id": "21",
      "title": "Smiley face: why we use emoticon stickers in mobile messaging",
      "authors": [
        "Joon Young",
        "Nahi Hong",
        "Soomin Kim",
        "Jonghwan Oh",
        "Joonhwan Lee"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct",
      "doi": "10.1145/2957265.2961858"
    },
    {
      "citation_id": "22",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "23",
      "title": "Reply with Sticker: New Dataset and Model for Sticker Retrieval",
      "authors": [
        "Bin Liang",
        "Bingbing Wang",
        "Zhixin Bai",
        "Qiwei Lang",
        "Mingwei Sun",
        "Kaiheng Hou",
        "Lanjun Zhou",
        "Ruifeng Xu",
        "Kam-Fai Wong"
      ],
      "year": "2024",
      "venue": "Reply with Sticker: New Dataset and Model for Sticker Retrieval",
      "arxiv": "arXiv:2403.05427"
    },
    {
      "citation_id": "24",
      "title": "Towards explainable harmful meme detection through multimodal debate between large language models",
      "authors": [
        "Hongzhan Lin",
        "Ziyang Luo",
        "Wei Gao",
        "Jing Ma",
        "Bo Wang",
        "Ruichao Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM Web Conference 2024"
    },
    {
      "citation_id": "25",
      "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Bo Li",
        "Yuanhan Zhang",
        "Sheng Shen",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
    },
    {
      "citation_id": "26",
      "title": "SER30K: A large-scale dataset for sticker emotion recognition",
      "authors": [
        "Shengzhe Liu",
        "Xin Zhang",
        "Jufeng Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "ELEMO: Elements Focused Emotion Recognition for Sticker Images",
      "authors": [
        "Min Luo",
        "Boda Lin",
        "Binghao Tang",
        "Haolong Yan",
        "Si Li"
      ],
      "year": "2024",
      "venue": "Chinese Conference on Pattern Recognition and Computer Vision (PRCV)"
    },
    {
      "citation_id": "28",
      "title": "Computational Meme Understanding: A Survey",
      "authors": [
        "Khoi Nguyen",
        "Vincent Ng"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "MemeIntent: Benchmarking Intent Description Generation for Memes",
      "authors": [
        "Jeongsik Park",
        "P Khoi",
        "Terrence Nguyen",
        "Suyesh Li",
        "Megan Shrestha",
        "Jerry Vu",
        "Vincent Wang",
        "Ng"
      ],
      "year": "2024",
      "venue": "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "30",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "31",
      "title": "What do you meme? generating explanations for visual semantic role labelling in memes",
      "authors": [
        "Shivam Sharma",
        "Siddhant Agarwal",
        "Tharun Suresh",
        "Preslav Nakov",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Memes in digital culture",
      "authors": [
        "Limor Shifman"
      ],
      "year": "2013",
      "venue": "Memes in digital culture"
    },
    {
      "citation_id": "33",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "34",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "Emoticon, emoji, and sticker use in computermediated communication: A review of theories and research findings",
      "authors": [
        "Ying Tang",
        "Khe Foon"
      ],
      "year": "2019",
      "venue": "International journal of communication"
    },
    {
      "citation_id": "36",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "37",
      "title": "What do they \"meme\"? A metaphor-aware multi-modal multi-task framework for fine-grained meme understanding",
      "authors": [
        "Bingbing Wang",
        "Shijue Huang",
        "Bin Liang",
        "Geng Tu",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "38",
      "title": "More than words? The effect of line character sticker use on intimacy in the mobile communication environment",
      "authors": [
        "Sharon Shaojung",
        "Wang"
      ],
      "year": "2016",
      "venue": "Social Science Computer Review"
    },
    {
      "citation_id": "39",
      "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "authors": [
        "Wenhai Wang",
        "Enze Xie",
        "Xiang Li",
        "Deng-Ping Fan",
        "Kaitao Song",
        "Ding Liang",
        "Tong Lu",
        "Ping Luo",
        "Ling Shao"
      ],
      "year": "2021",
      "venue": "Proceedings"
    },
    {
      "citation_id": "40",
      "title": "Perceive before Respond: Improving Sticker Response Selection by Emotion Distillation and Hard Mining",
      "authors": [
        "Wuyou Xia",
        "Shengzhe Liu",
        "Qin Rong",
        "Guoli Jia",
        "Eunil Park",
        "Jufeng Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Met-meme: A multimodal meme dataset rich in metaphors",
      "authors": [
        "Bo Xu",
        "Tingting Li",
        "Junzhe Zheng",
        "Mehdi Naseriparsa",
        "Zhehuan Zhao",
        "Hongfei Lin",
        "Feng Xia"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval"
    },
    {
      "citation_id": "42",
      "title": "Generating Multimodal Metaphorical Features for Meme Understanding",
      "authors": [
        "Bo Xu",
        "Junzhe Zheng",
        "Jiayuan He",
        "Yuxuan Sun",
        "Hongfei Lin",
        "Liang Zhao",
        "Feng Xia"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Emoset: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "Jingyuan Yang",
        "Qirui Huang",
        "Tingting Ding",
        "Dani Lischinski",
        "Danny Cohen-Or",
        "Hui Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Weakly supervised coupled networks for visual sentiment analysis",
      "authors": [
        "Jufeng Yang",
        "Dongyu She",
        "Yu-Kun Lai",
        "Paul Rosin",
        "Ming-Hsuan Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "45",
      "title": "Accurate and Lightweight Learning for Specific Domain Image-Text Retrieval",
      "authors": [
        "Rui Yang",
        "Shuang Wang",
        "Jianwei Tao",
        "Yingping Han",
        "Qiaoling Lin",
        "Yanhe Guo",
        "Biao Hou",
        "Licheng Jiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "47",
      "title": "Skeletal Spatial-Temporal Semantics Guided Homogeneous-Heterogeneous Multimodal Network for Action Recognition",
      "authors": [
        "Chenwei Zhang",
        "Yuxuan Hu",
        "Min Yang",
        "Chengming Li",
        "Xiping Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Affective image recognition with multi-attribute knowledge in deep neural networks",
      "authors": [
        "Hao Zhang",
        "Gaifang Luo",
        "Yingying Yue",
        "Kangjian He",
        "Dan Xu"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "49",
      "title": "Stickerconv: generating multimodal empathetic responses from scratch",
      "authors": [
        "Yiqun Zhang",
        "Fanheng Kong",
        "Peidong Wang",
        "Shuang Sun",
        "Lingshuai Wang",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Kaisong Song"
      ],
      "year": "2024",
      "venue": "Stickerconv: generating multimodal empathetic responses from scratch",
      "arxiv": "arXiv:2402.01679"
    },
    {
      "citation_id": "50",
      "title": "PDANet: Polarity-consistent deep attention network for fine-grained visual emotion regression",
      "authors": [
        "Sicheng Zhao",
        "Zizhou Jia",
        "Hui Chen",
        "Leida Li",
        "Guiguang Ding",
        "Kurt Keutzer"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "51",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "Sicheng Zhao",
        "Xingxu Yao",
        "Jufeng Yang",
        "Guoli Jia",
        "Guiguang Ding",
        "Tat-Seng Chua",
        "Bjoern Schuller",
        "Kurt Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "52",
      "title": "Multi-Granular Multimodal Clue Fusion for Meme Understanding",
      "authors": [
        "Li Zheng",
        "Hao Fei",
        "Ting Dai",
        "Zuquan Peng",
        "Fei Li",
        "Huisheng Ma",
        "Chong Teng",
        "Donghong Ji"
      ],
      "year": "2025",
      "venue": "Multi-Granular Multimodal Clue Fusion for Meme Understanding",
      "arxiv": "arXiv:2503.12560"
    },
    {
      "citation_id": "53",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "Bolei Zhou",
        "Agata Lapedriza",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "Yet at Memotion 2.0 2022: Hate speech detection combining bilstm and fully connected layers",
      "authors": [
        "Yan Zhuang",
        "Yanru Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection"
    }
  ]
}