{
  "paper_id": "2410.02804v1",
  "title": "Leveraging Retrieval Augment Approach For Multimodal Emotion Recognition Under Missing Modalities",
  "published": "2024-09-19T02:31:12Z",
  "authors": [
    "Qi Fan",
    "Hongyu Yuan",
    "Haolin Zuo",
    "Rui Liu",
    "Guanglai Gao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition utilizes complete multimodal information and robust multimodal joint representation to gain high performance. However, the ideal condition of full modality integrity is often not applicable in reality and there always appears the situation that some modalities are missing. For example, video, audio, or text data is missing due to sensor failure or network bandwidth problems, which presents a great challenge to MER research. Traditional methods extract useful information from the complete modalities and reconstruct the missing modalities to learn robust multimodal joint representation. These methods have laid a solid foundation for research in this field, and to a certain extent, alleviated the difficulty of multimodal emotion recognition under missing modalities. However, relying solely on internal reconstruction and multimodal joint learning has its limitations, especially when the missing information is critical for emotion recognition. To address this challenge, we propose a novel framework of Retrieval Augment for Missing Modality Multimodal Emotion Recognition (RAMER), which introduces similar multimodal emotion data to enhance the performance of emotion recognition under missing modalities. By leveraging databases, that contain related multimodal emotion data, we can retrieve similar multimodal emotion information to fill in the gaps left by missing modalities. Various experimental results demonstrate that our framework is superior to existing state-of-the-art approaches in missing modality MER tasks. Our whole project is publicly available on GitHub.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition (MER) aims to recognize and understand human emotions by integrating multiple modalities such as text, audio, and * Equal contributions. † Equal contributions. ‡ Corrsponding author. video. By fusing information from different modalities, MER provides a more comprehensive and accurate sentiment analysis, which plays a crucial role in applications like video conferencing, virtual assistants, and social media analysis, thereby enhancing the emotional understanding and humancomputer interaction experience of systems  (Moin et al., 2023; Chandrasekaran et al., 2021) .\n\nHowever, in real-world scenarios, MER tasks frequently encounter situations where certain modalities are missing due to factors such as sensor failures or network bandwidth issues causing data corruption or loss. This significantly increases the difficulty of accurately understanding emotions  (Hazarika et al., 2020; Zhao et al., 2021) . Existing approaches to address this problem primarily focus on two strategies: (1) reconstructing the missing modality representations, and (2) learning robust multimodal joint representations using the available modalities. For instance,  Zuo et al. (2023)  employed autoencoders to reconstruct missing modality representations and learn robust joint representations based on modality-specific and modalityinvariant features;  Liu et al. (2024)  utilized contrastive methods to extract modality-invariant features and reconstruct missing modalities under different missing conditions. Despite these efforts, the negative effects of missing modalities can still adversely affect the performance of multimodal learning, leading to incorrect predictions. This is because most existing methods either assume that the missing modalities can be accurately reconstructed from the available data or that the available modalities contain sufficient complementary information to compensate for the missing ones, effectively eliminating the noise introduced by missing data. However, these assumptions may not always hold, especially when the missing information is substantial or when the available modalities lack sufficient correlation with the missing ones. Consequently, the information loss from missing modalities continues to pose a significant challenge.\n\nTo address these limitations, we propose a novel framework that introduces retrieval augmentation into MER under missing modality conditions. By constructing a multimodal emotion feature database and employing a retrieval approach, our method acquires and supplements the lost information by retrieving relevant emotional features from similar instances in the database. This retrieval augmentation does not rely on the possibly flawed assumptions. Instead, it introduces external information that can more effectively compensate for the missing modalities, thereby enhancing the robustness and accuracy of emotion recognition even in the presence of missing or noisy data.\n\nThe main contribution of this paper can be summarized as follows:\n\n• We explore a new application area of retrieval augment approach: construct multimodal emotion feature databases, and apply them to enhance multimodal emotion recognition.\n\n• We propose a novel retrieval augment framework for multimodal emotion recognition under missing modalities called \"RAMER\", to introduce the emotion feature retrieval approach to bring more emotion information and reduce the information loss by the missing modalities.\n\n• Experimental results under various missing conditions show that our framework outperforms all the baselines and demonstrates robustness in the face of missing data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mer Under Missing Modalities",
      "text": "Existing methods for multimodal emotion recognition under missing modalities can mainly summarized into two kinds: generation or reconstruction of missing modalities; and learning robust multimodal joint representations. Notably, these approaches have been used together in recent studies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Generation And Reconstruction Methods",
      "text": "This method concentrates on obtaining the feature of missing modalities through available modalities.  Tran et al. (2017)  proposed the Cascaded Residual Autoencoder (CRA) that leverages a residual mechanism within an autoencoder framework to effectively restore incomplete data from corrupted inputs.  Cai et al. (2018)  utilized an encoder-decoder structure, to generate missing modalities (positron emission tomography, PET) from existing modalities (magnetic resonance imaging, MRI).  Zhao et al. (2021)  combined the CRA with cycle consistency loss for cross-modal imputation and imagined the features of missing modalities.  Fan et al. (2023)  used a variational autoencoder (VAE) to reduce the effect of noise and generate multimodal joint representations.  Liu et al. (2024)  proposed a contrastive learning method to acquire modality-invariant features and used for missing modality imagination.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Joint Representation Learning",
      "text": "This approach leverages robust multimodal joint representation to mitigate the negative effects of missing modalities  Pham et al. (2019)  took incomplete utterances as input and learned utterance-level representations through cyclic translation to ensure robustness to the missing modalities.  Zuo et al. (2023)  learned robust multimodal joint representation with modality-specific and modality-invariant features. Graph network was used to capture the speaker and time dependence, which improves the accuracy of conversation emotion recognition under the condition of missing modalities situations  (Lian et al., 2023a) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Retrieval Augment Methods",
      "text": "Retrieval-Augmented (RA) methods have received increasing attention in recent years, effectively addressing some of the limitations of generative models, such as knowledge updating and long-tailed data coverage, by introducing a retrieval mechanism and integrating external knowledge or context into the generation process  (Chen et al., 2024; Lewis et al., 2020) . Initially, RA methods were mainly applied to text generation tasks like question answering, dialogue systems, and summarization  (Zhao et al., 2024) . By incorporating retrieved relevant documents during the generation phase, these methods produce more accurate and informative content, especially in scenarios requiring large amounts of external knowledge. Beyond the textual domain, RA methods have also made significant progress in multimodal tasks involving audio and video. In audio tasks, the quality of audio generation is enhanced by retrieving relevant acoustic features or textual descriptions  (Yuan et al., 2024) .\n\nIn video tasks, the quality of subtitle generation for first-person videos can be improved by retrieving relevant third-person videos  (Xu et al., 2024) . Previous research has shown that the retrieval augment approach can be effectively applied to multimodal inputs by identifying and leveraging relevant content for each modality. In this paper, we introduce an innovative approach that involves constructing an emotional feature database and applying retrieval-augmented methods to query emotional features. This approach can address information loss due to missing modalities, providing a novel solution for enhancing multimodal tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "3.1 Data Preparation",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "We utilize the MER2024 multimodal emotion dataset  (Lian et al., 2024)  as the data source for our research, which is an extension of the MER2023 dataset  (Lian et al., 2023b) . MER2024 is an extensive Chinese multimodal emotion dataset comprising 6 emotion categories, 5,030 labeled samples, and 115,595 unlabeled data samples. Table  1  shows the specific emotion distribution of the dataset. MER2024 dataset includes a wide range of film and TV show clips, which have been carefully selected to enhance the diversity and representativeness of the data. Additionally, filtering steps are employed to reduce noise and eliminate irrelevant content, thereby improving data quality and the accuracy of emotional labeling. The emotion labels are assigned by multiple emotion recognition experts through a rigorous labeling and cross-verification process, ensuring the high quality and reliability of the dataset.\n\nWe employ pretrained language/vision/audio models to obtain text/video/audio embeddings. The preprocessing of the dataset and feature extracting process are placed in the Appendix A. Finally, we obtain the embeddings with audio, visual, and text  modality, noted as F a , F v , and F t .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset Division",
      "text": "We divide the MER2024 dataset into four scales, which are listed in Table  2 . The inclusion relationships between different parts are exhibited in Fig.  2 . \"MER_small\" contains all the labeled data (5,030 samples) in the MER2024 dataset. This part of the data can identify distinct emotional categories and is reliable  (Lian et al., 2023b) . \"MER_medium\" consists of about half of all the unlabeled samples (57,780/115,595), which are randomly selected. This scale balanced the size of the retrieval database and the retrieval time. \"MER_large\" includes all the unlabeled samples (115,595) in the MER2024 dataset. \"MER_turbo\" involves all the labeled and unlabeled data (120,625 samples).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Retrieval Augment Mer Under Missing Modalities",
      "text": "Our approach is based on the assumption that, in most cases, different modalities in multimodal emotion data express correlated and similar emotions rather than irrelevant ones. Under this circumstance, we employ the retrieval approach to introduce more emotional information to help emotion recognition and reduce the negative influence of information loss. The whole structure of our RAMER framework is illustrated in Fig.  1 , which includes three stages: Full-modality pretraining, retrieval vector store construction, and missing modality training.\n\nIn the full-modality pretraining stage, we employ the complete labeled data to train a base MER model, ensuring that the model captures comprehensive unimodal emotion features. In the second stage, we use the pretrained model saved in the first stage to reason about the entire dataset (including labeled and unlabeled data) and save the unimodal emotion hidden features before each unimodal classifier. Finally, we proceed to the missing-modalities training stage, where we train the model to handle scenarios with missing modal- ities by leveraging the retrieval vector store. This allows the model to effectively predict emotions even when some modalities are unavailable.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Full-Modality Pretraining",
      "text": "We first employ three Modality Encoder Networks to capture unimodal emotion features for each modality under complete modalities. They share the same structure of a one-layer Transformer Encoder to encode the embedding, a linear layer to map features to certain dimensions, and a classifier. This stage is trained on the labeled data. After training, we save the best model weights of the epoch that performed best on the test set.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Retrieval Database Construction",
      "text": "We utilize the pretrained model saved in the first stage to infer the whole dataset and respectively save tri-modal emotion hidden features h s to three hidden feature databases D a , D v , and D t . h s comes from the output of the last layer before the classifier. Note that the hidden features are aligned according to the sample names, which means that we can search the tri-modal most similar emotion hidden features with one feature at any modality. Then we select Facebook AI Similarity Search (FAISS) to construct the feature index for h s through inner product similarity, which has demonstrated superior performance in various retrieval tasks, particularly when dealing with highdimensional data  (Johnson et al., 2019) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Missing Modality Training",
      "text": "In the final stage, we train the model under various missing modalities. Previous research has proved that the addition of full-modality knowledge helps to improve performance in the absence of modalities. Therefore, we first load the pretrained model saved in stage one to leverage its knowledge and send the missing modality data F miss s into the model and obtain the multimodal hidden feature under missing modalities h miss s . We take text and video modality as missing modalities and the audio modality as an available modality for an example. As Fig.  1 (c  Table  3 : Main results of baselines and our model on six modality missing conditions. We report WA(%) and UA(%) results and choose WA as the main metric. Both metrics are positively correlated with the model performance. \"a\" means that only modality \"Audio\" is available. \"Avg\" indicates the average result of six conditions. ∆ sota shows the improvement of RAMER compared to state-of-the-art systems.\n\nThen we acquire the top-K similar audio emotion hidden features with feature h a from D a , where\n\nAccording to the corresponding sample names, we can subsequently acquire the corresponding top-K substitute missing modality multimodal emotional hidden features h retris t and h retris v , where h retris t = {h retri t1 , h retri t2 , . . . , h retri tK }, and h retris v = {h retri v1 , h retri v2 , . . . , h retri vK }, K ∈ Z + . Then we fuse the top-K features since the nearest one feature may not share the same emotion with the h a feature. We take text modality as an example. First, we sum the retrieved top-K feature h retris t :\n\nThen, L2 normalization is performed on the sum vector h sum t to obtain the fused features:\n\n, where the L2 norm ∥h sum t ∥is defined as:\n\nThrough the fusion method, we can further obtain missing modality data that on the whole is closer to the available modality emotion. Finally, the completed multimodal emotion hidden features h comp s will be concatenated and sent to the classifier.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments 4.1 Baselines",
      "text": "To evaluate the performance of our proposed approach, we select the following state-of-the-art missing modality multimodal emotion recognition methods as the baselines. The implementation details are listed in Appendix B.\n\nAutoEncoder (AE)  (Bengio et al., 2006)  has been used widely in solving missing modality problems  (Wong et al., 2014) . It used a self-supervised approach to impute missing data from input incomplete data. Following previous work  (Lian et al., 2023a) , we optimize the reconstruction loss of the autoencoder and the classification loss jointly in our implementation.\n\nCascaded Residual Autoencoder (CRA)  (Tran et al., 2017)  is a strong baseline that combines a series of residual autoencoders to restore missing data from corrupted inputs.\n\nMMIN  (Zhao et al., 2021)  performs well in missing modality problems, which combines CRA with cycle consistency loss to learn latent representations of missing modalities.  (Zuo et al., 2023)  is an enhancement of MMIN. It utilizes modality-specific and modality-invariant features to extend the performance of the MMIN.  (Liu et al., 2024)  learns modalityinvariant features through the contrastive learning method and imagines the missing modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "If-Mmin",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cif-Mmin",
      "text": "Table  4 : The ablation study results of our framework. Data scales have been listed in Table  2 . \"top5\" means we fuse top-5 most similar emotion hidden features in the retrieval results. \"top1\" means we only use the most similar feature of the retrieval results.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We design several ablation studies to prove the effectiveness of our framework in Table  4 . For the experiment \"w/o retrieval\", we remove the retrieval process in Fig. ??c and concatenate the h a , h miss t , h miss v directly. For the experiment \"Unimodal F s \", we select raw embedding F a , F v , F t instead of the emotion hidden feature h a , h v , h t as the source to construct the vector database. The experiment \"Multimodal h s \" takes the multimodal fusion strategy in the pretrain stage instead of the unimodal pretraining for comparison and utilizes emotional hidden features for the database construction. For experiment \"Euclidean\", we employ Euclidean distance to retrieve features instead of cosine similarity. Experiment \"MER_turbo_top1\" only takes the top1 similar feature while not employing the top-K fusion method in Sec. 3.2.3.\n\nFor the rest of the experiments in Table  4 , we validate the generalization performance of our method through using various scales and scopes of vector databases, also various top-K value selections.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "For both datasets, we follow  Liu et al. (2024)  and other previous works to employ weighted accuracy (WA)  (Baidari and Honnikoll, 2020)  and unweighted accuracy (UA)  (Gupta et al., 2020)  to evaluate performance. Due to class imbalance in the dataset, we choose WA as the preferred metric.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Main Results",
      "text": "Table  3  shows that our model achieves significant performance improvements under all missing modality conditions. The performance gains are particularly notable when the text modality is involved  (l, al, vl) , indicating that the text modality provides rich emotional information and enables the retrieval of more reliable emotional features. The audio modality follows in importance. For example, in the last two rows of Table  3 , the model achieves the largest improvement, reaching 26.55%, when only the text modality is available.\n\nEven when only the audio or video modality is available, the model still shows over a 7% improvement compared to the baselines, demonstrating the robustness of our method. In terms of absolute performance, the WA exceeds 80% in all cases except when only the video modality is available and even surpasses 90% in the al and vl conditions. This demonstrates that our method exhibits strong performance on most missing modality conditions.\n\nBased on the results from retrieval databases of different scales, the \"small\" database, despite its limited size, offers higher data quality, leading to the greatest performance (85.13%). In contrast, the performance of the \"medium\" database declines compared to the \"large\", indicating that the size of the retrieval database has a certain impact on performance. Both the \"large\" and \"turbo\" databases contain substantial amounts of unlabeled data, which may include instances that negatively affect performance, resulting in lower performance levels. Overall, data quality remains a significant factor influencing retrieval performance, and this quality is largely derived from high-quality manual selections and annotations, making large-scale applications challenging. In practical scenarios, it may be advisable to use \"medium\" or \"large\" databases to balance performance and cost.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Results",
      "text": "From Table  4 , experiment \"w/o retrieval\" in row 3 exhibits the outstanding performance of our approach. On row 4 and 5, the utilizing of unimodal F s and multimodal h s shows the significant influence of source feature when constructing the retrieval database. The results indicate that directly using raw embeddings to create a retrieval database performs poorly. This may be due to the fact that raw embeddings tend to capture shallow features such as semantics, rather than deeper features like emotions. To effectively capture emotional tendencies, it is necessary to train deep models and apply emotion-related constraints to the learned features. The experiments also reveal that multi-modal emotional hidden features do not perform well as unimodal ones in retrieval tasks. This could be attributed to two main reasons: (1) Emotional features are relatively high-level and complex, and the multimodal hidden features learned by current pre-trained models are insufficient for retrieving similar emotions. (2) The modality gap between different types of data makes cross-modal alignment challenging. As a result, the incorporation of multimodal interactions during pre-training may cause emotional features to become confused in unimodal retrieving, leading to a decrease in retrieval accuracy. Row 6 on Table  4  highlights the effect of similarity algorithm selection. Similarity algorithms that are not suitable for data distribution and type may result in a significant decrease in retrieval accuracy. The experiment in row 7 indicates that the top1 similar feature sometimes may also not share similar emotions with the query feature and lead to misjudgment. However, the fusion strategy in Sec. 3.2.3 mitigates this situation: one similar feature may deviate from the original emotion, but more similar features may bring the retrieval results back to the correct distribution.\n\nFrom row 8 to the end, our method demonstrates a fluctuation range of only 2.28% in WA across different sizes of retrieval databases, varying data scopes, and different top-k values (best performance: 85.13%, worst performance: 82.85%). This indicates that our method maintains a considerable level of generalization performance across various retrieval conditions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visualization",
      "text": "We randomly select 1,000 samples for each modality in the vector database and visualize their distribution utilizing the T-SNE algorithm. From Fig.  X  we observe that audio and text modality features are approximately clustered into six groups based on emotion categories, with samples of the same cate-gory positioned closer together. This demonstrates that our method effectively learns and represents the emotional features of the data. The clustering effect of the video modality features is less distinct compared to the first two modalities. This could be because the visual differences between certain emotion categories are not pronounced, for example, surprise and happy, making it more difficult for the model to distinguish between them.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces a novel method for solving multimodal emotion recognition under missing modalities. We construct multimodal emotional hidden feature databases on the basis of the fullmodality pretraining and utilize the retrieval augment approach to fill up the missing modalities and alleviate the information loss. Various experimental results exhibit the advantages of our approach over previous works. We intend to consider learning more robust emotional features and achieve more reliable multimodal emotion recognition under missing modalities.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitation",
      "text": "Despite the promising results of our proposed method, there are two key limitations that need to be addressed in future work. First, the current approach does not take retrieval time into consideration. As the size of the database increases, the retrieval time also grows correspondingly, which may present challenges in practical applications. Second, the retrieved content still requires filtering. Current feature fusion strategy may inadvertently incorporate irrelevant emotional features, which can lead to a decline in overall performance. Developing a more accurate feature selection algorithm will be crucial for improving the performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Feature Extraction",
      "text": "We follow the procedure outlined in the MER2024 Baseline  (Lian et al., 2024)  and extract utterancelevel features. Initially, we preprocess the data by cropping and aligning the facial regions of each frame using the OpenFace toolkit  (Baltrusaitis et al., 2018) . Next, we employ the CLIP model  (Radford et al., 2021)  to extract frame-level features for each face image. These visual features are aggregated using average pooling to generate video-level embeddings. For the audio processing, we use the FFmpeg toolkit to separate the audio from the video at a sampling rate of 16 kHz. We then utilize the Chinese-HuBERT-Large model  (Hsu et al., 2021; Guo and Liu, 2022)  to extract acoustic features, leveraging its superior performance on Chinese sentiment corpora. The final acoustic features are obtained by averaging the hidden representations from the last four layers of the model. Additionally, we transcribe the audio files into text using WeNet  (Yao et al., 2021) , an opensource automatic speech recognition toolkit. For textual feature extraction, we use the Baichuan2-13B-Base model  (Baichuan, 2023) , which has been pretrained on a large-scale corpus.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Implement Details",
      "text": "We split labeled data into train/validation/test sets, and the ratio of data set segmentation is 8:1:1 and they are independent of each other. We select the best model on the validation set and report the performance on the test set. For all the experiments, we run five-fold cross-validation three times and take the average result to reduce the impact of random parameter initialization, where each fold contains 40 epochs. For retrieval, we exclude the query sample from the retrieval results. Additionally, if one retrieval result comes from the validation or test set, it is also excluded to prevent any potential data leakage.\n\nThe embedding dimensions for the audio, video, and text features are 1024, 768, and 5120. The hidden size and the saving dimension of the vector store for each modality is 256. The batch size is set to 128 and the dropout rate is 0.5.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: “MER_small” contains all the labeled data (5,030",
      "page": 3
    },
    {
      "caption": "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,",
      "page": 4
    },
    {
      "caption": "Figure 2: Inclusion relationships between data.",
      "page": 4
    },
    {
      "caption": "Figure 1: (c) shows, Fa, Fmiss",
      "page": 4
    },
    {
      "caption": "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Correspondence: liurui_imu@163.com": "video. By fusing information from different modal-"
        },
        {
          "Correspondence: liurui_imu@163.com": "ities, MER provides a more comprehensive and"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "accurate sentiment analysis, which plays a crucial"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "role in applications like video conferencing, vir-"
        },
        {
          "Correspondence: liurui_imu@163.com": "tual assistants, and social media analysis, thereby"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "enhancing the emotional understanding and human-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "computer interaction experience of systems (Moin"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "et al., 2023; Chandrasekaran et al., 2021)."
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "However, in real-world scenarios, MER tasks fre-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "quently encounter situations where certain modali-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "ties are missing due to factors such as sensor fail-"
        },
        {
          "Correspondence: liurui_imu@163.com": "ures or network bandwidth issues causing data cor-"
        },
        {
          "Correspondence: liurui_imu@163.com": "ruption or loss. This significantly increases the dif-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "ficulty of accurately understanding emotions (Haz-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "arika et al., 2020; Zhao et al., 2021).\nExisting"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "approaches to address this problem primarily focus"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "on two strategies:\n(1) reconstructing the missing"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "modality representations, and (2) learning robust"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "multimodal\njoint representations using the avail-"
        },
        {
          "Correspondence: liurui_imu@163.com": "able modalities. For instance, Zuo et al. (2023) em-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "ployed autoencoders to reconstruct missing modal-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "ity representations and learn robust joint represen-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "tations based on modality-specific and modality-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "invariant features; Liu et al. (2024) utilized con-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "trastive methods to extract modality-invariant fea-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "tures and reconstruct missing modalities under dif-"
        },
        {
          "Correspondence: liurui_imu@163.com": "ferent missing conditions."
        },
        {
          "Correspondence: liurui_imu@163.com": "Despite these efforts, the negative effects of miss-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "ing modalities can still adversely affect\nthe per-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "formance of multimodal\nlearning,\nleading to in-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "correct predictions. This is because most existing"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "methods either assume that the missing modalities"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "can be accurately reconstructed from the available"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "data or that the available modalities contain suffi-"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "cient complementary information to compensate"
        },
        {
          "Correspondence: liurui_imu@163.com": "for\nthe missing ones, effectively eliminating the"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "noise introduced by missing data. However, these"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "assumptions may not always hold, especially when"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "the missing information is substantial or when the"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "available modalities lack sufficient correlation with"
        },
        {
          "Correspondence: liurui_imu@163.com": ""
        },
        {
          "Correspondence: liurui_imu@163.com": "the missing ones. Consequently,\nthe information"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "loss from missing modalities continues to pose a": "significant challenge.",
          "emission tomography, PET) from existing modali-": "ties (magnetic resonance imaging, MRI). Zhao et al."
        },
        {
          "loss from missing modalities continues to pose a": "To\naddress\nthese\nlimitations, we\npropose\na",
          "emission tomography, PET) from existing modali-": "(2021) combined the CRA with cycle consistency"
        },
        {
          "loss from missing modalities continues to pose a": "novel framework that introduces retrieval augmen-",
          "emission tomography, PET) from existing modali-": "loss for cross-modal imputation and imagined the"
        },
        {
          "loss from missing modalities continues to pose a": "tation into MER under missing modality condi-",
          "emission tomography, PET) from existing modali-": "features of missing modalities. Fan et al. (2023)"
        },
        {
          "loss from missing modalities continues to pose a": "tions. By constructing a multimodal emotion fea-",
          "emission tomography, PET) from existing modali-": "used a variational autoencoder (VAE) to reduce the"
        },
        {
          "loss from missing modalities continues to pose a": "ture database and employing a retrieval approach,",
          "emission tomography, PET) from existing modali-": "effect of noise and generate multimodal joint repre-"
        },
        {
          "loss from missing modalities continues to pose a": "our method acquires and supplements the lost in-",
          "emission tomography, PET) from existing modali-": "sentations. Liu et al. (2024) proposed a contrastive"
        },
        {
          "loss from missing modalities continues to pose a": "formation by retrieving relevant emotional features",
          "emission tomography, PET) from existing modali-": "learning method to acquire modality-invariant fea-"
        },
        {
          "loss from missing modalities continues to pose a": "from similar\ninstances in the database.\nThis re-",
          "emission tomography, PET) from existing modali-": "tures and used for missing modality imagination."
        },
        {
          "loss from missing modalities continues to pose a": "trieval augmentation does not rely on the possibly",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "2.1.2\nMultimodal joint representation"
        },
        {
          "loss from missing modalities continues to pose a": "flawed assumptions. Instead, it introduces external",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "learning"
        },
        {
          "loss from missing modalities continues to pose a": "information that can more effectively compensate",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "This approach leverages robust multimodal\njoint"
        },
        {
          "loss from missing modalities continues to pose a": "for the missing modalities, thereby enhancing the",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "representation to mitigate the negative effects of"
        },
        {
          "loss from missing modalities continues to pose a": "robustness and accuracy of emotion recognition",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "missing modalities Pham et al. (2019) took incom-"
        },
        {
          "loss from missing modalities continues to pose a": "even in the presence of missing or noisy data.",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "plete utterances as input and learned utterance-level"
        },
        {
          "loss from missing modalities continues to pose a": "The main contribution of this paper can be sum-",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "representations through cyclic translation to ensure"
        },
        {
          "loss from missing modalities continues to pose a": "marized as follows:",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "robustness to the missing modalities.\nZuo et al."
        },
        {
          "loss from missing modalities continues to pose a": "• We explore a new application area of retrieval",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "(2023) learned robust multimodal joint representa-"
        },
        {
          "loss from missing modalities continues to pose a": "augment approach: construct multimodal emotion",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "tion with modality-specific and modality-invariant"
        },
        {
          "loss from missing modalities continues to pose a": "feature databases, and apply them to enhance mul-",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "features. Graph network was used to capture the"
        },
        {
          "loss from missing modalities continues to pose a": "timodal emotion recognition.",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "speaker and time dependence, which improves the"
        },
        {
          "loss from missing modalities continues to pose a": "• We propose a novel retrieval augment frame-",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "accuracy of conversation emotion recognition un-"
        },
        {
          "loss from missing modalities continues to pose a": "work for multimodal emotion recognition under",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "der the condition of missing modalities situations"
        },
        {
          "loss from missing modalities continues to pose a": "missing modalities called “RAMER”, to introduce",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "(Lian et al., 2023a)."
        },
        {
          "loss from missing modalities continues to pose a": "the emotion feature retrieval approach to bring",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "more emotion information and reduce the infor-",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "2.2\nRetrieval augment methods"
        },
        {
          "loss from missing modalities continues to pose a": "mation loss by the missing modalities.",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "Retrieval-Augmented (RA) methods have received"
        },
        {
          "loss from missing modalities continues to pose a": "• Experimental\nresults under various missing",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "increasing attention in recent years, effectively ad-"
        },
        {
          "loss from missing modalities continues to pose a": "conditions show that our framework outperforms",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "dressing some of the limitations of generative mod-"
        },
        {
          "loss from missing modalities continues to pose a": "all the baselines and demonstrates robustness in the",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "els, such as knowledge updating and long-tailed"
        },
        {
          "loss from missing modalities continues to pose a": "face of missing data.",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "data coverage, by introducing a retrieval mecha-"
        },
        {
          "loss from missing modalities continues to pose a": "2\nRelated Work",
          "emission tomography, PET) from existing modali-": "nism and integrating external knowledge or con-"
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "text into the generation process (Chen et al., 2024;"
        },
        {
          "loss from missing modalities continues to pose a": "2.1\nMER Under Missing Modalities",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "Lewis et al., 2020).\nInitially, RA methods were"
        },
        {
          "loss from missing modalities continues to pose a": "Existing methods for multimodal emotion recogni-",
          "emission tomography, PET) from existing modali-": "mainly applied to text generation tasks like ques-"
        },
        {
          "loss from missing modalities continues to pose a": "tion under missing modalities can mainly summa-",
          "emission tomography, PET) from existing modali-": "tion answering, dialogue systems, and summariza-"
        },
        {
          "loss from missing modalities continues to pose a": "rized into two kinds: generation or reconstruction",
          "emission tomography, PET) from existing modali-": "tion (Zhao et al., 2024). By incorporating retrieved"
        },
        {
          "loss from missing modalities continues to pose a": "of missing modalities; and learning robust mul-",
          "emission tomography, PET) from existing modali-": "relevant documents during the generation phase,"
        },
        {
          "loss from missing modalities continues to pose a": "timodal\njoint representations. Notably,\nthese ap-",
          "emission tomography, PET) from existing modali-": "these methods produce more accurate and informa-"
        },
        {
          "loss from missing modalities continues to pose a": "proaches have been used together in recent studies.",
          "emission tomography, PET) from existing modali-": "tive content, especially in scenarios requiring large"
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "amounts of external knowledge. Beyond the textual"
        },
        {
          "loss from missing modalities continues to pose a": "2.1.1\nGeneration and reconstruction methods",
          "emission tomography, PET) from existing modali-": ""
        },
        {
          "loss from missing modalities continues to pose a": "",
          "emission tomography, PET) from existing modali-": "domain, RA methods have also made significant"
        },
        {
          "loss from missing modalities continues to pose a": "This method concentrates on obtaining the feature",
          "emission tomography, PET) from existing modali-": "progress in multimodal tasks involving audio and"
        },
        {
          "loss from missing modalities continues to pose a": "of missing modalities through available modalities.",
          "emission tomography, PET) from existing modali-": "video.\nIn audio tasks,\nthe quality of audio gener-"
        },
        {
          "loss from missing modalities continues to pose a": "Tran et al. (2017) proposed the Cascaded Resid-",
          "emission tomography, PET) from existing modali-": "ation is enhanced by retrieving relevant acoustic"
        },
        {
          "loss from missing modalities continues to pose a": "ual Autoencoder (CRA) that leverages a residual",
          "emission tomography, PET) from existing modali-": "features or textual descriptions (Yuan et al., 2024)."
        },
        {
          "loss from missing modalities continues to pose a": "mechanism within an autoencoder framework to ef-",
          "emission tomography, PET) from existing modali-": "In video tasks, the quality of subtitle generation for"
        },
        {
          "loss from missing modalities continues to pose a": "fectively restore incomplete data from corrupted in-",
          "emission tomography, PET) from existing modali-": "first-person videos can be improved by retrieving"
        },
        {
          "loss from missing modalities continues to pose a": "puts. Cai et al. (2018) utilized an encoder-decoder",
          "emission tomography, PET) from existing modali-": "relevant third-person videos (Xu et al., 2024)."
        },
        {
          "loss from missing modalities continues to pose a": "structure, to generate missing modalities (positron",
          "emission tomography, PET) from existing modali-": "Previous research has shown that\nthe retrieval"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: The category distribution of the MER2024",
      "data": [
        {
          "Emotion": "Happy",
          "Labeled": "1,038",
          "Unlabeled": "Unknown",
          "Part Name": "MER_small",
          "Data Scale": "5,030",
          "Data Type": "labeled data"
        },
        {
          "Emotion": "Angry",
          "Labeled": "1,208",
          "Unlabeled": "Unknown",
          "Part Name": "MER_medium",
          "Data Scale": "57,780",
          "Data Type": "unlabeled data"
        },
        {
          "Emotion": "Sad",
          "Labeled": "730",
          "Unlabeled": "Unknown",
          "Part Name": "MER_large",
          "Data Scale": "115,595",
          "Data Type": "unlabeled data"
        },
        {
          "Emotion": "Neutral",
          "Labeled": "1,248",
          "Unlabeled": "Unknown",
          "Part Name": "MER_turbo",
          "Data Scale": "120,625",
          "Data Type": "all data in MER2024"
        },
        {
          "Emotion": "Worried",
          "Labeled": "616",
          "Unlabeled": "Unknown",
          "Part Name": "",
          "Data Scale": "",
          "Data Type": ""
        },
        {
          "Emotion": "",
          "Labeled": "",
          "Unlabeled": "",
          "Part Name": "",
          "Data Scale": "Table 2: Various scales of dataset separation.",
          "Data Type": ""
        },
        {
          "Emotion": "Surprise",
          "Labeled": "190",
          "Unlabeled": "Unknown",
          "Part Name": "",
          "Data Scale": "",
          "Data Type": ""
        },
        {
          "Emotion": "Total",
          "Labeled": "5,030",
          "Unlabeled": "115,595",
          "Part Name": "",
          "Data Scale": "",
          "Data Type": ""
        },
        {
          "Emotion": "",
          "Labeled": "",
          "Unlabeled": "",
          "Part Name": "",
          "Data Scale": "modality, noted as Fa, Fv, and Ft.",
          "Data Type": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: The category distribution of the MER2024",
      "data": [
        {
          "Neutral\n1,248\nUnknown": "Worried\n616\nUnknown",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "Table 2: Various scales of dataset separation."
        },
        {
          "Neutral\n1,248\nUnknown": "Surprise\n190\nUnknown",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "Total\n5,030\n115,595",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "modality, noted as Fa, Fv, and Ft."
        },
        {
          "Neutral\n1,248\nUnknown": "Table 1: The category distribution of\nthe MER2024",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "dataset.\n“Unknown” indicates this part of data in the",
          "MER_turbo\n120,625\nall data in MER2024": "3.1.2\nDataset division"
        },
        {
          "Neutral\n1,248\nUnknown": "dataset has not been labeled and counted by the owner.",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "We divide the MER2024 dataset\ninto four scales,"
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "which are listed in Table 2. The inclusion relation-"
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "ships between different parts are exhibited in Fig. 2."
        },
        {
          "Neutral\n1,248\nUnknown": "augment approach can be effectively applied to",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "“MER_small” contains all the labeled data (5,030"
        },
        {
          "Neutral\n1,248\nUnknown": "multimodal\ninputs by identifying and leveraging",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "samples) in the MER2024 dataset. This part of the"
        },
        {
          "Neutral\n1,248\nUnknown": "relevant content for each modality.\nIn this paper,",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "data can identify distinct emotional categories and"
        },
        {
          "Neutral\n1,248\nUnknown": "we introduce an innovative approach that involves",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "is reliable (Lian et al., 2023b).\n“MER_medium”"
        },
        {
          "Neutral\n1,248\nUnknown": "constructing an emotional feature database and ap-",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "consists of about half of all the unlabeled samples"
        },
        {
          "Neutral\n1,248\nUnknown": "plying retrieval-augmented methods to query emo-",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "(57,780/115,595), which are randomly selected."
        },
        {
          "Neutral\n1,248\nUnknown": "tional features. This approach can address infor-",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "This scale balanced the size of the retrieval database"
        },
        {
          "Neutral\n1,248\nUnknown": "mation loss due to missing modalities, providing a",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "and the retrieval time. “MER_large” includes all"
        },
        {
          "Neutral\n1,248\nUnknown": "novel solution for enhancing multimodal tasks.",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "the unlabeled samples (115,595) in the MER2024"
        },
        {
          "Neutral\n1,248\nUnknown": "3\nMethodology",
          "MER_turbo\n120,625\nall data in MER2024": "dataset. “MER_turbo” involves all the labeled and"
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "unlabeled data (120,625 samples)."
        },
        {
          "Neutral\n1,248\nUnknown": "3.1\nData Preparation",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "3.2\nRetrieval Augment MER Under Missing"
        },
        {
          "Neutral\n1,248\nUnknown": "3.1.1\nDataset",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "",
          "MER_turbo\n120,625\nall data in MER2024": "Modalities"
        },
        {
          "Neutral\n1,248\nUnknown": "We\nutilize\nthe MER2024 multimodal\nemotion",
          "MER_turbo\n120,625\nall data in MER2024": ""
        },
        {
          "Neutral\n1,248\nUnknown": "dataset (Lian et al., 2024) as the data source for our",
          "MER_turbo\n120,625\nall data in MER2024": "Our approach is based on the assumption that, in"
        },
        {
          "Neutral\n1,248\nUnknown": "research, which is an extension of the MER2023",
          "MER_turbo\n120,625\nall data in MER2024": "most cases, different modalities in multimodal emo-"
        },
        {
          "Neutral\n1,248\nUnknown": "dataset (Lian et al., 2023b).",
          "MER_turbo\n120,625\nall data in MER2024": "tion data express correlated and similar emotions"
        },
        {
          "Neutral\n1,248\nUnknown": "MER2024 is an extensive Chinese multimodal",
          "MER_turbo\n120,625\nall data in MER2024": "rather\nthan irrelevant ones.\nUnder\nthis circum-"
        },
        {
          "Neutral\n1,248\nUnknown": "emotion dataset comprising 6 emotion categories,",
          "MER_turbo\n120,625\nall data in MER2024": "stance, we employ the retrieval approach to in-"
        },
        {
          "Neutral\n1,248\nUnknown": "5,030 labeled samples, and 115,595 unlabeled data",
          "MER_turbo\n120,625\nall data in MER2024": "troduce more emotional information to help emo-"
        },
        {
          "Neutral\n1,248\nUnknown": "samples. Table 1 shows the specific emotion distri-",
          "MER_turbo\n120,625\nall data in MER2024": "tion recognition and reduce the negative influence"
        },
        {
          "Neutral\n1,248\nUnknown": "bution of the dataset. MER2024 dataset includes",
          "MER_turbo\n120,625\nall data in MER2024": "of\ninformation loss. The whole structure of our"
        },
        {
          "Neutral\n1,248\nUnknown": "a wide range of film and TV show clips, which",
          "MER_turbo\n120,625\nall data in MER2024": "RAMER framework is illustrated in Fig. 1, which"
        },
        {
          "Neutral\n1,248\nUnknown": "have been carefully selected to enhance the diver-",
          "MER_turbo\n120,625\nall data in MER2024": "includes three stages: Full-modality pretraining,"
        },
        {
          "Neutral\n1,248\nUnknown": "sity and representativeness of the data. Addition-",
          "MER_turbo\n120,625\nall data in MER2024": "retrieval vector\nstore construction,\nand missing"
        },
        {
          "Neutral\n1,248\nUnknown": "ally, filtering steps are employed to reduce noise",
          "MER_turbo\n120,625\nall data in MER2024": "modality training."
        },
        {
          "Neutral\n1,248\nUnknown": "and eliminate irrelevant content, thereby improving",
          "MER_turbo\n120,625\nall data in MER2024": "In the full-modality pretraining stage, we em-"
        },
        {
          "Neutral\n1,248\nUnknown": "data quality and the accuracy of emotional labeling.",
          "MER_turbo\n120,625\nall data in MER2024": "ploy the complete labeled data to train a base MER"
        },
        {
          "Neutral\n1,248\nUnknown": "The emotion labels are assigned by multiple emo-",
          "MER_turbo\n120,625\nall data in MER2024": "model, ensuring that\nthe model captures compre-"
        },
        {
          "Neutral\n1,248\nUnknown": "tion recognition experts through a rigorous labeling",
          "MER_turbo\n120,625\nall data in MER2024": "hensive unimodal emotion features.\nIn the sec-"
        },
        {
          "Neutral\n1,248\nUnknown": "and cross-verification process, ensuring the high",
          "MER_turbo\n120,625\nall data in MER2024": "ond stage, we use the pretrained model saved in"
        },
        {
          "Neutral\n1,248\nUnknown": "quality and reliability of the dataset.",
          "MER_turbo\n120,625\nall data in MER2024": "the first stage to reason about\nthe entire dataset"
        },
        {
          "Neutral\n1,248\nUnknown": "We\nemploy pretrained language/vision/audio",
          "MER_turbo\n120,625\nall data in MER2024": "(including labeled and unlabeled data) and save"
        },
        {
          "Neutral\n1,248\nUnknown": "models to obtain text/video/audio embeddings. The",
          "MER_turbo\n120,625\nall data in MER2024": "the unimodal emotion hidden features before each"
        },
        {
          "Neutral\n1,248\nUnknown": "preprocessing of the dataset and feature extracting",
          "MER_turbo\n120,625\nall data in MER2024": "unimodal classifier.\nFinally, we proceed to the"
        },
        {
          "Neutral\n1,248\nUnknown": "process are placed in the Appendix A. Finally, we",
          "MER_turbo\n120,625\nall data in MER2024": "missing-modalities training stage, where we train"
        },
        {
          "Neutral\n1,248\nUnknown": "obtain the embeddings with audio, visual, and text",
          "MER_turbo\n120,625\nall data in MER2024": "the model to handle scenarios with missing modal-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "we acquire Audio top-K similar features and then index the corresponding features with the same sample name"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "from the other two databases."
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "MER_turbo \n120625 Samples"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "MER_large\nMER_small"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "MER_medium"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "57780\n5030\n115595"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "Samples\nSamples\nSamples"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "Figure 2: Inclusion relationships between data."
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "ities by leveraging the retrieval vector store. This"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "allows the model\nto effectively predict emotions"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "even when some modalities are unavailable."
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "3.2.1\nFull-modality pretraining"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "We first employ three Modality Encoder Networks"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "to capture unimodal\nemotion features\nfor\neach"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "modality under complete modalities. They share"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "the same structure of a one-layer Transformer En-"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "coder to encode the embedding, a linear layer to"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "map features to certain dimensions, and a classifier."
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "This stage is trained on the labeled data. After train-"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "ing, we save the best model weights of the epoch"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "that performed best on the test set."
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "3.2.2\nRetrieval database construction"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "We utilize the pretrained model saved in the first"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "stage to infer the whole dataset and respectively"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "save tri-modal emotion hidden features hs to three"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "hidden feature databases Da, Dv, and Dt.\nhs"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "comes\nfrom the output of\nthe last\nlayer before"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": "the classifier. Note that\nthe hidden features are"
        },
        {
          "Figure 1: The main structure of our RAMER framework, which includes three stages. For “Corresponding features”,": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Testing conditions": "av"
        },
        {
          "Testing conditions": "WA"
        },
        {
          "Testing conditions": "75.38"
        },
        {
          "Testing conditions": "76.61"
        },
        {
          "Testing conditions": "77.42"
        },
        {
          "Testing conditions": "72.94"
        },
        {
          "Testing conditions": "79.64"
        },
        {
          "Testing conditions": "82.39"
        },
        {
          "Testing conditions": "2.75"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "Ours\n85.94\n74.88\n71.34\n59.63\n89.84\n88.23",
          "79.64": "82.39",
          "73.93\n76.38\n75.61": "90.96\n88.99\n70.02",
          "76.44": "90.63",
          "74.93": "86.77",
          "71.98": "85.13",
          "68.29": "78.73"
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "11.06\n4.54\n7.22\n7.25\n26.55\n25.78\n∆sota",
          "79.64": "2.75",
          "73.93\n76.38\n75.61": "-3.91\n14.58\n13.22",
          "76.44": "14.19",
          "74.93": "11.84",
          "71.98": "13.15",
          "68.29": "10.44"
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "Table 3: Main results of baselines and our model on six modality missing conditions. We report WA(%) and UA(%)",
          "79.64": "",
          "73.93\n76.38\n75.61": "",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "",
          "79.64": "results and choose WA as the main metric. Both metrics are positively correlated with the model performance. “a”",
          "73.93\n76.38\n75.61": "",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "",
          "79.64": "means that only modality “Audio” is available. “Avg” indicates the average result of six conditions. ∆sota shows",
          "73.93\n76.38\n75.61": "",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "",
          "79.64": "the improvement of RAMER compared to state-of-the-art systems.",
          "73.93\n76.38\n75.61": "",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "Then we acquire the top-K similar audio emotion",
          "79.64": "4",
          "73.93\n76.38\n75.61": "Experiments",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "hidden features with feature ha from Da, where",
          "79.64": "",
          "73.93\n76.38\n75.61": "",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        },
        {
          "CIF-MMIN 72.31\n69.86\n64.12\n52.19\n63.29\n62.45": "{hsim1\n, hsim2\n, . . . , hsimK",
          "79.64": "",
          "73.93\n76.38\n75.61": "",
          "76.44": "",
          "74.93": "",
          "71.98": "",
          "68.29": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Then we acquire the top-K similar audio emotion": "hidden features with feature ha from Da, where",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "{hsim1\n, hsim2\n, . . . , hsimK\n} = arg max\ncos(θi),",
          "4\nExperiments": "4.1\nBaselines"
        },
        {
          "Then we acquire the top-K similar audio emotion": "a\na\na",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "hi\na∈Da",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "i1, i2, . . . , iK ∈ {1, 2, . . . , N }.",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "To evaluate the performance of our proposed ap-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "According to the corresponding sample names,",
          "4\nExperiments": "proach, we select\nthe following state-of-the-art"
        },
        {
          "Then we acquire the top-K similar audio emotion": "we can subsequently acquire the corresponding",
          "4\nExperiments": "missing modality multimodal emotion recognition"
        },
        {
          "Then we acquire the top-K similar audio emotion": "top-K substitute missing modality multimodal",
          "4\nExperiments": "methods as the baselines. The implementation de-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "emotional\nhidden\nfeatures hretris\nand hretris\n,",
          "4\nExperiments": "tails are listed in Appendix B."
        },
        {
          "Then we acquire the top-K similar audio emotion": "t\nv",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": ", hretri\n, . . . , hretri\n= {hretri\nwhere hretris\n}, and",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t1\nt2\ntK",
          "4\nExperiments": "AutoEncoder (AE) (Bengio et al., 2006) has been"
        },
        {
          "Then we acquire the top-K similar audio emotion": "hretris\n= {hretri\n, hretri\n, . . . , hretri\nv\nvK }, K ∈ Z+.",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "v1\nv2",
          "4\nExperiments": "used widely in solving missing modality problems"
        },
        {
          "Then we acquire the top-K similar audio emotion": "Then we fuse the top-K features since the nearest",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "(Wong et al., 2014).\nIt used a self-supervised ap-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "one feature may not share the same emotion with",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "proach to impute missing data from input incom-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "the ha feature. We take text modality as an ex-",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "plete data. Following previous work (Lian et al.,"
        },
        {
          "Then we acquire the top-K similar audio emotion": "ample. First, we sum the retrieved top-K feature",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "2023a), we optimize the reconstruction loss of the"
        },
        {
          "Then we acquire the top-K similar audio emotion": "hretris\n:",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t",
          "4\nExperiments": "autoencoder and the classification loss jointly in"
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "our implementation."
        },
        {
          "Then we acquire the top-K similar audio emotion": "K(cid:88) i\nhsum\n=\nhretri",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t\nti",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "=1",
          "4\nExperiments": "Cascaded Residual Autoencoder (CRA)\n(Tran"
        },
        {
          "Then we acquire the top-K similar audio emotion": "Then, L2 normalization is performed on the sum",
          "4\nExperiments": "et al., 2017) is a strong baseline that combines a"
        },
        {
          "Then we acquire the top-K similar audio emotion": "vector hsum\nto obtain the fused features:\nt",
          "4\nExperiments": "series of residual autoencoders to restore missing"
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "data from corrupted inputs."
        },
        {
          "Then we acquire the top-K similar audio emotion": "hsum",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "hfused\n=",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "∥hsum\n∥",
          "4\nExperiments": "MMIN (Zhao et al., 2021) performs well in miss-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "t",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "ing modality problems, which combines CRA with"
        },
        {
          "Then we acquire the top-K similar audio emotion": ", where the L2 norm ∥hsum\n∥is defined as:\nt",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "cycle consistency loss to learn latent\nrepresenta-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "(cid:118)(cid:117)(cid:117)(cid:116)",
          "4\nExperiments": "tions of missing modalities."
        },
        {
          "Then we acquire the top-K similar audio emotion": "d(cid:88) j\n∥hsum\n∥ =\n(hsum\n[j])2",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "t\nt",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "=1",
          "4\nExperiments": "IF-MMIN (Zuo et\nal., 2023)\nis\nan enhance-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "ment of MMIN. It utilizes modality-specific and"
        },
        {
          "Then we acquire the top-K similar audio emotion": "Through the fusion method, we can further ob-",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "modality-invariant\nfeatures to extend the perfor-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "tain missing modality data that on the whole is",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "",
          "4\nExperiments": "mance of the MMIN."
        },
        {
          "Then we acquire the top-K similar audio emotion": "closer to the available modality emotion. Finally,",
          "4\nExperiments": ""
        },
        {
          "Then we acquire the top-K similar audio emotion": "the completed multimodal emotion hidden features",
          "4\nExperiments": "CIF-MMIN (Liu et al., 2024) learns modality-"
        },
        {
          "Then we acquire the top-K similar audio emotion": "hcomp\nwill be concatenated and sent to the classi-",
          "4\nExperiments": "invariant features through the contrastive learning"
        },
        {
          "Then we acquire the top-K similar audio emotion": "fier.",
          "4\nExperiments": "method and imagines the missing modalities."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: shows that our model achieves signifi-",
      "data": [
        {
          "a": "",
          "v": "",
          "l": "",
          "av": "",
          "al": "",
          "vl": "",
          "Avg": ""
        },
        {
          "a": "",
          "v": "WA",
          "l": "",
          "av": "WA",
          "al": "UA",
          "vl": "WA",
          "Avg": "UA"
        },
        {
          "a": "",
          "v": "56.12",
          "l": "",
          "av": "75.13",
          "al": "70.95",
          "vl": "79.49",
          "Avg": "61.55"
        },
        {
          "a": "",
          "v": "51.03",
          "l": "",
          "av": "69.80",
          "al": "67.12",
          "vl": "73.20",
          "Avg": "55.35"
        },
        {
          "a": "",
          "v": "62.85",
          "l": "",
          "av": "73.25",
          "al": "73.41",
          "vl": "76.12",
          "Avg": "63.67"
        },
        {
          "a": "",
          "v": "58.68",
          "l": "",
          "av": "70.09",
          "al": "75.39",
          "vl": "74.08",
          "Avg": "64.20"
        },
        {
          "a": "",
          "v": "63.28",
          "l": "",
          "av": "83.25",
          "al": "86.24",
          "vl": "87.23",
          "Avg": "75.27"
        },
        {
          "a": "",
          "v": "74.27",
          "l": "",
          "av": "85.52",
          "al": "93.60",
          "vl": "85.93",
          "Avg": "79.93"
        },
        {
          "a": "",
          "v": "71.34",
          "l": "",
          "av": "82.39",
          "al": "88.99",
          "vl": "90.63",
          "Avg": "78.73"
        },
        {
          "a": "",
          "v": "70.94",
          "l": "",
          "av": "85.68",
          "al": "92.12",
          "vl": "89.98",
          "Avg": "80.72"
        },
        {
          "a": "",
          "v": "70.15",
          "l": "",
          "av": "82.21",
          "al": "89.75",
          "vl": "90.30",
          "Avg": "75.50"
        },
        {
          "a": "",
          "v": "70.69",
          "l": "",
          "av": "84.40",
          "al": "90.46",
          "vl": "88.71",
          "Avg": "76.57"
        },
        {
          "a": "",
          "v": "68.48",
          "l": "",
          "av": "82.90",
          "al": "87.27",
          "vl": "88.43",
          "Avg": "76.12"
        },
        {
          "a": "",
          "v": "70.42",
          "l": "",
          "av": "84.72",
          "al": "91.13",
          "vl": "91.42",
          "Avg": "77.67"
        },
        {
          "a": "",
          "v": "70.78",
          "l": "",
          "av": "88.06",
          "al": "92.27",
          "vl": "89.07",
          "Avg": "79.70"
        },
        {
          "a": "",
          "v": "64.83",
          "l": "",
          "av": "85.42",
          "al": "88.55",
          "vl": "89.10",
          "Avg": "78.95"
        },
        {
          "a": "",
          "v": "69.48",
          "l": "",
          "av": "81.35",
          "al": "87.62",
          "vl": "91.98",
          "Avg": "79.29"
        },
        {
          "a": "",
          "v": "72.82",
          "l": "",
          "av": "83.75",
          "al": "90.53",
          "vl": "93.78",
          "Avg": "78.59"
        },
        {
          "a": "",
          "v": "69.48",
          "l": "",
          "av": "78.72",
          "al": "88.26",
          "vl": "91.43",
          "Avg": "77.55"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: shows that our model achieves signifi-",
      "data": [
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "MER_turbo_top10\n79.22\n65.78\n72.82\n57.08\n87.18",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "86.63\n83.75\n71.72\n92.14\n90.53\n93.78\n90.84\n84.77\n78.59"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "MER_turbo_top15\n78.05\n77.45\n69.48\n56.80\n88.98",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "88.65\n78.72\n65.89\n91.38\n88.26\n91.43\n89.45\n82.89\n77.55"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "Table 4: The ablation study results of our framework. Data scales have been listed in Table 2. “top5” means we",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "fuse top-5 most similar emotion hidden features in the retrieval results. “top1” means we only use the most similar",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "feature of the retrieval results.",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "4.2\nAblation study",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "4.3\nEvaluation metrics"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "For both datasets, we follow Liu et al. (2024) and"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "other previous works to employ weighted accu-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "We design several ablation studies to prove the",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "racy (WA) (Baidari and Honnikoll, 2020) and un-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "effectiveness of our\nframework in Table 4.\nFor",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "weighted accuracy (UA)\n(Gupta et al., 2020)\nto"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "the experiment “w/o retrieval”, we remove the re-",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "evaluate performance. Due to class imbalance in"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "trieval process in Fig.\n??c and concatenate the",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "the dataset, we choose WA as the preferred metric."
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": ", hmiss\ndirectly. For the experiment “Uni-\nha, hmiss",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "v",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "modal Fs”, we select raw embedding Fa, Fv, Ft",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "5\nResults"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "instead of the emotion hidden feature ha, hv, ht as",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "5.1\nMain results"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "the source to construct\nthe vector database. The",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "Table 3 shows\nthat our model achieves\nsignifi-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "experiment “Multimodal hs” takes the multimodal",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": ""
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "fusion strategy in the pretrain stage instead of the",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "cant performance improvements under all miss-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "unimodal pretraining for comparison and utilizes",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "ing modality conditions. The performance gains"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "emotional hidden features for\nthe database con-",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "are particularly notable when the text modality is"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "struction. For experiment “Euclidean”, we employ",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "involved (l, al, vl), indicating that the text modal-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "Euclidean distance to retrieve features instead of",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "ity provides rich emotional\ninformation and en-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "cosine similarity. Experiment “MER_turbo_top1”",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "ables the retrieval of more reliable emotional fea-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "only takes the top1 similar feature while not em-",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "tures. The audio modality follows in importance."
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "ploying the top-K fusion method in Sec. 3.2.3.",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "For example,\nin the last\ntwo rows of Table 3,\nthe"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "model achieves the largest improvement, reaching"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "For the rest of the experiments in Table 4, we val-",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "26.55%, when only the text modality is available."
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "idate the generalization performance of our method",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "Even when only the audio or video modality is"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "through using various scales and scopes of vector",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "available, the model still shows over a 7% improve-"
        },
        {
          "MER_turbo_top5\n82.87\n77.80\n69.48\n56.22\n84.66": "databases, also various top-K value selections.",
          "82.68\n81.35\n69.93\n91.90\n87.62\n91.98\n89.72\n84.18\n79.29": "ment compared to the baselines, demonstrating the"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , experiment “w/o retrieval” in row acrossdifferentsizesofretrievaldatabases,varying",
      "data": [
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "robustness of our method.\nIn terms of absolute"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "performance,\nthe WA exceeds 80% in all cases"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "except when only the video modality is available"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "and even surpasses 90% in the al and vl conditions."
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "This demonstrates that our method exhibits strong"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "performance on most missing modality conditions."
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "Based on the results from retrieval databases of"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "different scales,\nthe “small” database, despite its"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "limited size, offers higher data quality, leading to"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "the greatest performance (85.13%). In contrast, the"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "performance of the “medium” database declines"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "compared to the “large”, indicating that the size of"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "the retrieval database has a certain impact on perfor-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "mance. Both the “large” and “turbo” databases con-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "tain substantial amounts of unlabeled data, which"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "may include instances that negatively affect per-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "formance,\nresulting in lower performance levels."
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "Overall, data quality remains a significant factor"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "influencing retrieval performance, and this quality"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "is largely derived from high-quality manual selec-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "tions and annotations, making large-scale applica-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "tions challenging. In practical scenarios, it may be"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "advisable to use “medium” or “large” databases to"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "balance performance and cost."
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": ""
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "5.2\nAblation results"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": ""
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "From Table 4, experiment “w/o retrieval” in row"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "3 exhibits the outstanding performance of our ap-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "proach. On row 4 and 5, the utilizing of unimodal"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "shows the significant\nin-\nFs and multimodal hs"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "fluence of source feature when constructing the"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "retrieval database. The results indicate that directly"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "using raw embeddings to create a retrieval database"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": ""
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "performs poorly. This may be due to the fact that"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "raw embeddings tend to capture shallow features"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "such as semantics, rather than deeper features like"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "emotions.\nTo effectively capture emotional\nten-"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "dencies,\nit\nis necessary to train deep models and"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "apply emotion-related constraints to the learned"
        },
        {
          "Figure 3: The visualization result of 1,000 random samples with each modality in the dataset.": "features. The experiments also reveal\nthat multi-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Hugo Larochelle. 2006. Greedy layer-wise training"
        },
        {
          "gory positioned closer together. This demonstrates": "that our method effectively learns and represents",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "of deep networks.\nIn NIPS, pages 153–160. MIT"
        },
        {
          "gory positioned closer together. This demonstrates": "the emotional features of the data. The clustering",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Press."
        },
        {
          "gory positioned closer together. This demonstrates": "effect of the video modality features is less distinct",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Lei Cai, Zhengyang Wang, Hongyang Gao, Dinggang"
        },
        {
          "gory positioned closer together. This demonstrates": "compared to the first\ntwo modalities. This could",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Shen, and Shuiwang Ji. 2018. Deep adversarial learn-"
        },
        {
          "gory positioned closer together. This demonstrates": "be because the visual differences between certain",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "ing for multi-modality missing data completion.\nIn"
        },
        {
          "gory positioned closer together. This demonstrates": "emotion categories are not pronounced, for exam-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Proceedings of the 24th ACM SIGKDD international"
        },
        {
          "gory positioned closer together. This demonstrates": "ple, surprise and happy, making it more difficult",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "conference on knowledge discovery & data mining,"
        },
        {
          "gory positioned closer together. This demonstrates": "for the model to distinguish between them.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "pages 1158–1166."
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Ganesh Chandrasekaran, Tu N Nguyen, and Jude He-"
        },
        {
          "gory positioned closer together. This demonstrates": "6\nConclusion",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "manth D. 2021. Multimodal sentimental analysis for"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "social media applications: A comprehensive review."
        },
        {
          "gory positioned closer together. This demonstrates": "This paper\nintroduces a novel method for\nsolv-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Wiley Interdisciplinary Reviews: Data Mining and"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Knowledge Discovery, 11(5):e1415."
        },
        {
          "gory positioned closer together. This demonstrates": "ing multimodal emotion recognition under missing",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "modalities. We construct multimodal emotional",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun."
        },
        {
          "gory positioned closer together. This demonstrates": "hidden feature databases on the basis of the full-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "2024.\nBenchmarking large\nlanguage models\nin"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "retrieval-augmented generation.\nIn Proceedings of"
        },
        {
          "gory positioned closer together. This demonstrates": "modality pretraining and utilize the retrieval aug-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "the AAAI Conference on Artificial Intelligence, vol-"
        },
        {
          "gory positioned closer together. This demonstrates": "ment approach to fill up the missing modalities and",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "ume 38, pages 17754–17762."
        },
        {
          "gory positioned closer together. This demonstrates": "alleviate the information loss. Various experimen-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "tal results exhibit the advantages of our approach",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian, and Guanglai"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Gao. 2023. Learning noise-robust\njoint representa-"
        },
        {
          "gory positioned closer together. This demonstrates": "over previous works. We intend to consider learn-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "tion for multimodal emotion recognition under\nre-"
        },
        {
          "gory positioned closer together. This demonstrates": "ing more robust emotional\nfeatures and achieve",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "arXiv preprint\nalistic incomplete data scenarios."
        },
        {
          "gory positioned closer together. This demonstrates": "more reliable multimodal emotion recognition un-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "arXiv:2311.16114."
        },
        {
          "gory positioned closer together. This demonstrates": "der missing modalities.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Pengcheng Guo and Shixing Liu. 2022. Chinese speech"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "pretrain."
        },
        {
          "gory positioned closer together. This demonstrates": "7\nLimitation",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Shruti Gupta, Md. Shah Fahad, and Akshay Deepak."
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "2020. Pitch-synchronous single frequency filtering"
        },
        {
          "gory positioned closer together. This demonstrates": "Despite\nthe promising results of our proposed",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "spectrogram for speech emotion recognition. Multim."
        },
        {
          "gory positioned closer together. This demonstrates": "method,\nthere are two key limitations that need",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Tools Appl., 79(31-32):23347–23365."
        },
        {
          "gory positioned closer together. This demonstrates": "to be addressed in future work. First,\nthe current",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "approach does not take retrieval time into consid-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Devamanyu Hazarika, Roger Zimmermann, and Sou-"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "janya Poria. 2020. Misa: Modality-invariant and-"
        },
        {
          "gory positioned closer together. This demonstrates": "eration. As the size of the database increases, the",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "specific representations\nfor multimodal\nsentiment"
        },
        {
          "gory positioned closer together. This demonstrates": "retrieval time also grows correspondingly, which",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "the 28th ACM interna-\nanalysis.\nIn Proceedings of"
        },
        {
          "gory positioned closer together. This demonstrates": "may present challenges in practical applications.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "tional conference on multimedia, pages 1122–1131."
        },
        {
          "gory positioned closer together. This demonstrates": "Second, the retrieved content still requires filtering.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,"
        },
        {
          "gory positioned closer together. This demonstrates": "Current feature fusion strategy may inadvertently",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-"
        },
        {
          "gory positioned closer together. This demonstrates": "incorporate irrelevant emotional\nfeatures, which",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "rahman Mohamed. 2021. Hubert: Self-supervised"
        },
        {
          "gory positioned closer together. This demonstrates": "can lead to a decline in overall performance. Devel-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "speech representation learning by masked prediction"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "IEEE/ACM Transactions on Audio,\nof hidden units."
        },
        {
          "gory positioned closer together. This demonstrates": "oping a more accurate feature selection algorithm",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Speech, and Language Processing, 29:3451–3460."
        },
        {
          "gory positioned closer together. This demonstrates": "will be crucial for improving the performance.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019."
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "IEEE\nBillion-scale similarity search with GPUs."
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Transactions on Big Data, 7(3):535–547."
        },
        {
          "gory positioned closer together. This demonstrates": "References",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio"
        },
        {
          "gory positioned closer together. This demonstrates": "Baichuan. 2023.\nBaichuan 2: Open large-scale lan-",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Petroni, Vladimir Karpukhin, Naman Goyal, Hein-"
        },
        {
          "gory positioned closer together. This demonstrates": "guage models. arXiv preprint arXiv:2309.10305.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-"
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "täschel, et al. 2020. Retrieval-augmented generation"
        },
        {
          "gory positioned closer together. This demonstrates": "Ishwar Baidari and Nagaraj Honnikoll. 2020. Accuracy",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "for knowledge-intensive nlp tasks. Advances in Neu-"
        },
        {
          "gory positioned closer together. This demonstrates": "Expert\nweighted diversity-based online boosting.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "ral Information Processing Systems, 33:9459–9474."
        },
        {
          "gory positioned closer together. This demonstrates": "Syst. Appl., 160:113723.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": ""
        },
        {
          "gory positioned closer together. This demonstrates": "",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Zheng Lian, Lan Chen, Licai Sun, Bin Liu, and Jian-"
        },
        {
          "gory positioned closer together. This demonstrates": "Tadas Baltrusaitis, Amir Zadeh, Yao Chong Lim, and",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "hua Tao. 2023a. Gcnet: Graph completion network"
        },
        {
          "gory positioned closer together. This demonstrates": "Louis-Philippe Morency. 2018. Openface 2.0: Facial",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "for incomplete multimodal learning in conversation."
        },
        {
          "gory positioned closer together. This demonstrates": "behavior analysis toolkit.\nIn FG, pages 59–66. IEEE",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "IEEE Transactions on Pattern Analysis and Machine"
        },
        {
          "gory positioned closer together. This demonstrates": "Computer Society.",
          "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and": "Intelligence, 45(7):8419–8432."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Mngyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Plumbley,\nand Wenwu Wang.\n2024.\nRetrieval-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Jinming Zhao, et al. 2023b. Mer 2023: Multi-label",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "augmented text-to-audio generation.\nIn IEEE Inter-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "learning, modality robustness, and semi-supervised",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "national Conference on Acoustics, Speech and Signal"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "the 31st ACM Interna-\nlearning.\nIn Proceedings of",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Processing, ICASSP 2024, Seoul, Republic of Korea,"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "tional Conference on Multimedia, pages 9610–9614.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "April 14-19, 2024, pages 581–585. IEEE."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Jinming Zhao, Ruichen Li, and Qin Jin. 2021. Missing"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "modality imagination network for emotion recogni-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Ziyang Ma, Xie Chen, et al. 2024. Mer 2024: Semi-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "tion with uncertain missing modalities.\nIn Proceed-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "supervised learning,\nnoise\nrobustness,\nand open-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "ings of the 59th Annual Meeting of the Association for"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "vocabulary multimodal emotion recognition. arXiv",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Computational Linguistics and the 11th International"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "preprint arXiv:2404.17113.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Joint Conference on Natural Language Processing"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "(Volume 1: Long Papers), pages 2608–2618."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Rui Liu, Haolin Zuo, Zheng Lian, Bjorn W Schuller,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "and Haizhou Li. 2024. Contrastive learning based",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Wang, Yunteng Geng, Fangcheng Fu, Ling Yang,"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "modality-invariant feature acquisition for robust mul-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Wentao Zhang,\nand Bin Cui.\n2024.\nRetrieval-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "timodal emotion recognition with missing modalities.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "augmented generation for ai-generated content: A"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "IEEE Transactions on Affective Computing.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "survey. arXiv preprint arXiv:2402.19473."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Anam Moin, Farhan Aadil, Zeeshan Ali, and Dong-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Haolin Zuo, Rui Liu,\nJinming Zhao, Guanglai Gao,"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "wann Kang. 2023. Emotion recognition framework",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "and Haizhou Li. 2023. Exploiting modality-invariant"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "using multiple modalities for an effective human–",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "feature for robust multimodal emotion recognition"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "computer interaction. The Journal of Supercomput-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "with missing modalities.\nIn ICASSP 2023-2023 IEEE"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "ing, 79(8):9320–9349.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "International Conference on Acoustics, Speech and"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Signal Processing (ICASSP), pages 1–5. IEEE."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Philippe Morency,\nand Barnabás\nPóczos.\n2019.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Found in translation: Learning robust\njoint\nrepre-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "A\nFeature extraction"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "sentations by cyclic translations between modalities.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "We follow the procedure outlined in the MER2024"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "In Proceedings of the AAAI conference on artificial",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "intelligence, volume 33, pages 6892–6899.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "Baseline (Lian et al., 2024) and extract utterance-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "level\nfeatures.\nInitially, we preprocess the data"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "by cropping and aligning the\nfacial\nregions of"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "try, Amanda Askell, Pamela Mishkin, Jack Clark,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "each frame using the OpenFace toolkit\n(Baltru-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "et al. 2021. Learning transferable visual models from",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "saitis et al., 2018). Next, we employ the CLIP"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "natural language supervision.\nIn International confer-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "model (Radford et al., 2021) to extract frame-level"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "ence on machine learning, pages 8748–8763. PMLR.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "features for each face image. These visual features"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "are aggregated using average pooling to generate"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "2017. Missing modalities imputation via cascaded",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "video-level embeddings.\nFor\nthe audio process-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "the IEEE\nresidual autoencoder.\nIn Proceedings of",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "ing, we use the FFmpeg toolkit\nto separate the"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "conference on computer vision and pattern recogni-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "audio from the video at a sampling rate of 16 kHz."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "tion, pages 1405–1414.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "We then utilize the Chinese-HuBERT-Large model"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Liang Ze Wong, Huiling Chen, Shaowei Lin,\nand",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "(Hsu et al., 2021; Guo and Liu, 2022) to extract"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Daniel Chongli Chen. 2014.\nImputing missing val-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "acoustic features,\nleveraging its superior perfor-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "ues in sensor networks using sparse data representa-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "mance on Chinese sentiment corpora. The final"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "tions.\nIn Proceedings of the 17th ACM international",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "conference on Modeling, analysis and simulation of",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "acoustic features are obtained by averaging the hid-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "wireless and mobile systems, pages 227–230.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "den representations from the last four layers of the"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "model. Additionally, we transcribe the audio files"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "into text using WeNet (Yao et al., 2021), an open-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Zhang, Rui Feng, and Weidi Xie. 2024. Retrieval-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "augmented egocentric video captioning.\nIn Proceed-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "source automatic speech recognition toolkit. For"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "ings of the IEEE/CVF Conference on Computer Vi-",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "textual feature extraction, we use the Baichuan2-"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "sion and Pattern Recognition, pages 13525–13536.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "13B-Base model (Baichuan, 2023), which has been"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "pretrained on a large-scale corpus."
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Lei Xie, and Xin Lei. 2021. Wenet:\nProduction",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "B\nImplement Details"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "oriented streaming and non-streaming end-to-end",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "We split labeled data into train/validation/test sets,"
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "speech recognition toolkit.\nIn Proc.\nInterspeech,",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": ""
        },
        {
          "Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen,": "Brno, Czech Republic. IEEE.",
          "Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D.": "and the ratio of data set segmentation is 8:1:1 and"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "they are independent of each other. We select the": "best model on the validation set and report the per-"
        },
        {
          "they are independent of each other. We select the": "formance on the test set. For all the experiments,"
        },
        {
          "they are independent of each other. We select the": ""
        },
        {
          "they are independent of each other. We select the": "take the average result to reduce the impact of ran-"
        },
        {
          "they are independent of each other. We select the": "dom parameter initialization, where each fold con-"
        },
        {
          "they are independent of each other. We select the": "tains 40 epochs. For retrieval, we exclude the query"
        },
        {
          "they are independent of each other. We select the": "sample from the retrieval results. Additionally, if"
        },
        {
          "they are independent of each other. We select the": "one retrieval result comes from the validation or"
        },
        {
          "they are independent of each other. We select the": "test set, it is also excluded to prevent any potential"
        },
        {
          "they are independent of each other. We select the": ""
        },
        {
          "they are independent of each other. We select the": "The embedding dimensions for the audio, video,"
        },
        {
          "they are independent of each other. We select the": "features are 1024, 768, and 5120. The"
        },
        {
          "they are independent of each other. We select the": "hidden size and the saving dimension of the vector"
        },
        {
          "they are independent of each other. We select the": "store for each modality is 256. The batch size is set"
        },
        {
          "they are independent of each other. We select the": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Baichuan 2: Open large-scale language models",
      "authors": [
        "Baichuan"
      ],
      "year": "2023",
      "venue": "Baichuan 2: Open large-scale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "2",
      "title": "Accuracy weighted diversity-based online boosting",
      "authors": [
        "Ishwar Baidari",
        "Nagaraj Honnikoll"
      ],
      "year": "2020",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2020.113723"
    },
    {
      "citation_id": "3",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrusaitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "FG"
    },
    {
      "citation_id": "4",
      "title": "Greedy layer-wise training of deep networks",
      "authors": [
        "Yoshua Bengio",
        "Pascal Lamblin",
        "Dan Popovici",
        "Hugo Larochelle"
      ],
      "year": "2006",
      "venue": "NIPS"
    },
    {
      "citation_id": "5",
      "title": "Deep adversarial learning for multi-modality missing data completion",
      "authors": [
        "Lei Cai",
        "Zhengyang Wang",
        "Hongyang Gao",
        "Dinggang Shen",
        "Shuiwang Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "6",
      "title": "Multimodal sentimental analysis for social media applications: A comprehensive review",
      "authors": [
        "Tu Ganesh Chandrasekaran",
        "Jude Nguyen",
        "D Hemanth"
      ],
      "year": "2021",
      "venue": "Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "7",
      "title": "Benchmarking large language models in retrieval-augmented generation",
      "authors": [
        "Jiawei Chen",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios",
      "authors": [
        "Qi Fan",
        "Haolin Zuo",
        "Rui Liu",
        "Zheng Lian",
        "Guanglai Gao"
      ],
      "year": "2023",
      "venue": "Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios",
      "arxiv": "arXiv:2311.16114"
    },
    {
      "citation_id": "9",
      "title": "Chinese speech pretrain",
      "authors": [
        "Pengcheng Guo",
        "Shixing Liu"
      ],
      "year": "2022",
      "venue": "Chinese speech pretrain"
    },
    {
      "citation_id": "10",
      "title": "Pitch-synchronous single frequency filtering spectrogram for speech emotion recognition",
      "authors": [
        "Md Shruti Gupta",
        "Akshay Shah Fahad",
        "Deepak"
      ],
      "year": "2020",
      "venue": "Multim. Tools Appl",
      "doi": "10.1007/s11042-020-09068-1"
    },
    {
      "citation_id": "11",
      "title": "Misa: Modality-invariant andspecific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "12",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Billion-scale similarity search with GPUs",
      "authors": [
        "Jeff Johnson",
        "Matthijs Douze",
        "Hervé Jégou"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Big Data"
    },
    {
      "citation_id": "14",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "Patrick Lewis",
        "Ethan Perez",
        "Aleksandra Piktus",
        "Fabio Petroni",
        "Vladimir Karpukhin",
        "Naman Goyal",
        "Heinrich Küttler",
        "Mike Lewis",
        "Wen-Tau Yih",
        "Tim Rocktäschel"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2023.3234553"
    },
    {
      "citation_id": "16",
      "title": "2023b. Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Mer 2024: Semisupervised learning, noise robustness, and openvocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semisupervised learning, noise robustness, and openvocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "18",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Bjorn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition framework using multiple modalities for an effective humancomputer interaction",
      "authors": [
        "Anam Moin",
        "Farhan Aadil",
        "Zeeshan Ali",
        "Dongwann Kang"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "20",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "21",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "Luan Tran",
        "Xiaoming Liu",
        "Jiayu Zhou",
        "Rong Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "23",
      "title": "Imputing missing values in sensor networks using sparse data representations",
      "authors": [
        "Huiling Liang Ze Wong",
        "Shaowei Chen",
        "Daniel Lin",
        "Chen Chongli"
      ],
      "year": "2014",
      "venue": "Proceedings of the 17th ACM international conference on Modeling, analysis and simulation of wireless and mobile systems"
    },
    {
      "citation_id": "24",
      "title": "Retrievalaugmented egocentric video captioning",
      "authors": [
        "Jilan Xu",
        "Yifei Huang",
        "Junlin Hou",
        "Guo Chen",
        "Yuejie Zhang",
        "Rui Feng",
        "Weidi Xie"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit",
      "authors": [
        "Zhuoyuan Yao",
        "Di Wu",
        "Xiong Wang",
        "Binbin Zhang",
        "Fan Yu",
        "Chao Yang",
        "Zhendong Peng",
        "Xiaoyu Chen",
        "Lei Xie",
        "Xin Lei"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Retrievalaugmented text-to-audio generation",
      "authors": [
        "Yi Yuan",
        "Haohe Liu",
        "Xubo Liu",
        "Qiushi Huang",
        "Mark Plumbley",
        "Wenwu Wang"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10447898"
    },
    {
      "citation_id": "27",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Retrievalaugmented generation for ai-generated content: A survey",
      "authors": [
        "Penghao Zhao",
        "Hailin Zhang",
        "Qinhan Yu",
        "Zhengren Wang",
        "Yunteng Geng",
        "Fangcheng Fu",
        "Ling Yang",
        "Wentao Zhang",
        "Bin Cui"
      ],
      "year": "2024",
      "venue": "Retrievalaugmented generation for ai-generated content: A survey",
      "arxiv": "arXiv:2402.19473"
    },
    {
      "citation_id": "29",
      "title": "Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "Haolin Zuo",
        "Rui Liu",
        "Jinming Zhao",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}