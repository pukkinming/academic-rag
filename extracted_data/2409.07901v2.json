{
  "paper_id": "2409.07901v2",
  "title": "Bridging Discrete And Continuous: A Multimodal Strategy For Complex Emotion Detection",
  "published": "2024-09-12T10:10:22Z",
  "authors": [
    "Jiehui Jia",
    "Huan Zhang",
    "Jinhua Liang"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "emotional variability",
    "valence-arousal-dominance (VAD) framework",
    "emotion detection",
    "machine learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the domain of human-computer interaction, accurately recognizing and interpreting human emotions is crucial yet challenging due to the complexity and subtlety of emotional expressions. This study explores the potential for detecting a rich and flexible range of emotions through a multimodal approach which integrates facial expressions, voice tones, and transcript from video clips. We propose a novel framework that maps variety of emotions in a three-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect the fluctuations and positivity/negativity of emotions to enable a more variety and comprehensive representation of emotional states. We employed K-means clustering to transit emotions from traditional discrete categorization to a continuous labeling system and built a classifier for emotion recognition upon this system. The effectiveness of the proposed model is evaluated using the MER2024 dataset, which contains culturally consistent video clips from Chinese movies and TV series, annotated with both discrete and open-vocabulary emotion labels. Our experiment successfully achieved the transformation between discrete and continuous models, and the proposed model generated a more diverse and comprehensive set of emotion vocabulary while maintaining strong accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotions are complex and described through diverse vocabularies across languages, reflecting our thoughts, feelings, and reactions via facial expressions, body language, voice tone, and speech.  [1] . Accurate comprehension and response to human emotions by machines can significantly benefit areas such as marketing, mental health monitoring, multimedia generation, and humancomputer interaction  [2, 3, 4, 5, 6, 7] . Thus, developing systems capable of precisely identifying varieties of human emotions is essential.\n\nHowever, the challenge in emotion detection lies in the subjective nature of emotions. It is hard to set a clear boundary to categorize emotions, so as to choose a 'basic' emotion group  [8] . Moreover, emotion datasets vary in their annotation schemes (e.g., differing discrete labels) and domains, hindering direct comparisons across previous works. These variations restrict prior research to specific data sources, limiting their generalizability to real-world applications  [9] .\n\nRecognizing the challenges, this paper proposes a multimodal framework that transforms different discrete emotion labels into one continuous emotion label framework. We propose to use a fixed Anonymous.\n\nFig.  1 . Emotion Vocabularies in 3D VAD Space emotion Valence, Arousal, and Dominance rating scale to standardize the scoring of a variety of emotion labels in a multidimensional space. By using K-means clustering, we build a classifier which could transit discrete emotion with continuous emotions which suits both close-set and open set emotion recognition. In our case, we grouped the emotion labels into six clusters based on the six basic emotion labels which been annotated for the dataset we chosen. We built a multimodal model that integrates facial expressions, voice tones, and transcript from video clips to predict the Valence-Arousal-Dominance (VAD) scores of the emotions. Finally the VAD score was mapped back to the original emotion labels for evaluation. We also generated open-vocabulary emotional responses from discrete emotion inputs to explore the possibility of a more dynamic and adaptable emotion recognition system.\n\nThe experimental results demonstrate that the framework successfully achieved the transformation between discrete and continuous models. By comparing their evaluation metrics, we conclude that the proposed framework performs the transformation with a comparable level of accuracy. We also compared the similarity score between the generated open emotion vocabulary set and the original dataset's open set, achieving a high score that indicates significant semantic overlap. This shows that our model can reliably produce a nuanced and flexible emotion vocabulary, which could be effectively applied to tasks requiring more detailed emotion recognition and analysis. The contribution of this paper is summarized as follows:\n\n• We introduce a multimodal system that aligns with human perception by mapping emotions into a continuous hidden space. The proposed framework outperforms existing classifier in the close-set dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Compared to regular classification tasks  [10, 11, 12] , multimodal emotion detection methods primarily differ in two key areas: the approaches to categorizing and measuring emotions, and the techniques used to extract and integrate features from various modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Measurement Of Emotion",
      "text": "There are two main approaches to measuring emotions: the discrete method and the multidimensional approach  [13] . The discrete method classifies emotions into a few fundamental categories believed to be universal. Proponents argue that combinations of these basic emotions can explain complex emotional states. For instance, fear, surprise, happiness, disgust, anger, and sadness has been identified as the six basic emotions  [14] .\n\nBuilding upon these basic feelings, a thorough emotional model known as Plutchik's wheel of emotions has been developed  [15] , which consists of eight main emotions. It is believed that different intensities of these main emotions might mix to create additional related feelings. Studies have been conducted on dataset with 15 Compound emotion mixed with two basic emotions and 7 Basic facial emotions as a way for compound emotion detection  [16] . In some situations, the discrete method works well enough with a simplified emotional assessment  [17, 18] . For example, when it comes to driving systems in vehicles, determining a driver's stress level can be accomplished by concentrating just on the most fundamentally feelings, like anger and happiness  [18] .\n\nThe multidimensional approach, on the other hand, uses continuous dimensions-valence, arousal, and sometimes dominance-to map emotions. Valence measures positivity or negativity, arousal indicates the level of activity, and dominance reflects control over the emotional state  [19] . This creates a nuanced two-or three-dimensional emotional space, often referred to as the circumplex model of affect  [20] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Detection",
      "text": "Multimodal methods have been widely chosen in research because they can extract features from various modalities  [21, 22, 23, 24] . While basic emotions like happy or angry are readily expressed through body language or facial expressions, complex emotions such as jealousy, pride, or hope require additional context and language for accurate interpretation  [21] . Multimodal methods are thus preferable to unimodal ones for recognizing such emotions. This approach allows researchers to analyze data from different sources-such as visual, audio, and textual inputs-in ways that enhance the accuracy and robustness of emotion detection systems. By utilizing these different streams of information, multimodal methods capture a more comprehensive understanding of emotional states, accommodating variations and subtleties that might be missed by unimodal approaches  [22] .\n\nVisual, audio, and textual features are captured separately using different methods. Visual features primarily focus on facial expressions, which can be seen as facial muscle movements. The Facial Action Coding System (FACS)  [25]  has been developed to manually code facial expressions using action units (AUs) based on specific muscle movements. Inspired by their work, many researchers have utilized image and video processing to analyze and categorize facial expressions by tracking features and measuring movement, applying these methods to the \"basic expressions\" identified in multimodal emotion studies  [26, 27, 28, 29] . Similar to other audio domains  [30, 7] , emotion-related features consist of two main categories: linguistic information, which pertains to the content of speech, and paralinguistic information, which captures emotions through the tone and manner of delivery  [31] . In addition to commonly used models like CNN, RNN, and LSTM  [32, 33] , other approaches also make promising prediction. For instance, the use of features such as mel-frequency cepstral coefficient (MFCC), perceptual linear prediction coefficient (PLPC), and perceptual minimum variance distortionless response (PMVDR) coefficient  [34]  . Textual emotions are expressed through speech and can be abstract by transcripts. Traditional techniques such as the bag-of-words (BoW) model  [35, 36]  have been commonly used. Then word embedding methods like tfidf, word2vec  [37]  and GloVe  [38]  were based mainly on syntactic contexts. For handling larger text, BERT (Bidirectional Encoder Representations from Transformers) uses transformer-based encoders that assess both preceding and subsequent contexts to better predict words in sentences  [39] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "The project aims to bridge discrete and continuous emotion systems, using a multimodal model to detect emotions and produce more nuanced emotion labels beyond the basic six. This approach allows for working with diverse datasets and improves the variety and depth of emotion detection. We first transformed discrete emotion labels into continuous emotion labels, then applied and refined several models for the task. The first model, Multimodal End-to-End Sparse Model (ME2E)  [40] , is a multimodal emotion detection model that uses discrete emotion labels as input and outputs corresponding discrete emotions, serving as our baseline. The second, ME2E Lite model, a refined version of ME2E to better align with our dataset for improved performance. The third, Proposed VAD, builds on the ME2E Lite pipeline but uses 3D VAD score as input, outputting continuous VAD scores that can be mapped to discrete emotions while also generating a broader range of nuanced emotion vocabularies.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The K-Means Clustering Classifier -Transformation Between Discrete Emotion Labels And Continuous Emotion Labels",
      "text": "To transition from discrete to multidimensional emotion methods, we developed a classifier that enables the transformation between discrete and continuous emotion labels using the NRC-VAD lexicon  [41] . We extracted 195 emotion vocabularies with assigned VAD scores from the 20,000-vocabularies NRC-VAD lexicon, using the polar term based on a -1 to 1 scale, as taking the absolute values of these scores is likely to yield better features than those derived from a 0 to 1 scale. The six basic emotions selected-happy, sad, worried, surprised, angry, and neutral-align with the categories used in our dataset. As VAD scores are available for all selected basic emotions except neutral, we assigned a VAD score of (0,0,0) to neutral, reflecting a lack of significant emotional fluctuations typically associated with this state.\n\nAt this stage, each basic emotion can be transformed into a 3D VAD score, but to enable the inversion from the continuous emotions to discrete labels, we employed a K-means clustering classifier. By setting k to 6, the classifier is configured to convert continuous VAD scores back into six basic emotion categories.\n\nTo build the emotion classifier, we first mapped the 195 extracted emotion vocabularies, along with their associated VAD scores, into Fig.  2 . VAD Model Pipline a 3D emotion space. The six basic emotions-happy, sad, worried, surprised, angry, and neutral-were set as the initial cluster centers. We then set the number of clusters to six, corresponding to the number of basic emotions.\n\nThe underlying assumption is that people experiencing similar emotions will exhibit similar VAD scores, meaning their emotional reactions should cluster closely in the VAD space. By applying Kmeans clustering, we grouped these similar emotions into clusters. Each cluster represents a collection of VAD scores that align with one of the basic emotions. After clustering, the centroid of each cluster corresponds to a discrete emotion, allowing us to map continuous VAD scores back to a specific emotion category.\n\nThis approach effectively allows us to bridge continuous and discrete emotional labels, with the results of the clustering visualized in Figure  1  and Table  1 , demonstrating how similar emotions are grouped together based on their VAD scores.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Me2E -Baseline Model",
      "text": "The ME2E model processes data from video, audio, and text modalities. Facial features are extracted from video frames using the MTCNN model from FaceNet  [42] , while both these facial features and audio spectrograms are then processed through convolutional layers. These features are further analyzed using a Transformer  [43]  to capture temporal and contextual nuances. Text data is processed through ALBERT  [44] , which optimized for short sentences and requiring fewer parameters than BERT. Finally, features from each modality are individually processed and merged via a weighted fusion mechanism, producing a final emotion prediction across six possible outcomes: happy, angry, sad, neutral, worried, or surprised.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Me2E Lite -The Refined Model",
      "text": "Building on the work of  [40] , we revised the architecture in our ME2E Lite model by replacing the original 11-layer CNN+VGG video path with AlexNet  [45] , and simplified the audio path's CNN by removing one VGG layer, effectively halving the processed parameters to better suit our smaller dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Vad Model -The Model Bridges The Gap Between Discrete And Continuous Emotions",
      "text": "The proposed VAD model shown in Figure  2  uses VAD scores as input, making it compatible with both continuous labels and discrete emotions which been represented by VAD scores using the NRC-VAD lexicon. We employed the same pipeline as ME2E Lite but replaced the softmax function with mean squared error (MSE) as the loss function. The model features a fully connected layer that directly outputs three continuous values corresponding to the VAD dimensions This model independently processes each of these dimensions by predicting the distribution for valence, arousal, and dominance, then identifying the most likely score for each. These predicted VAD scores are then mapped to the K-means clustering classifier. By determining which emotion cluster each result falls into, we can transform the continuous VAD scores into discrete emotion labels for evaluation. This allows for a comparison of the continuous emotion model's performance against the discrete emotion model. Alternatively, by selecting the emotion close to the predicted VAD score, we can generate a broader, open set of possible emotions, offering more nuanced emotion representations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "We have selected the MER2024 dataset  [46]  for our study. This dataset consists of raw video clips collected from Chinese movies and TV series, ensuring cultural consistency. The raw video samples were split into smaller video clips, each containing a complete segment from the same character. The dataset has been labeled with six discrete emotions: happy, angry, sad, neutral, worried, and surprised. Additionally, there is a subset of open-vocabulary labels and explanations  [47] , which allows us to evaluate a more dynamic emotion output. Each sample contains audio data with a sampling rate of 44.1 kHz, text transcript and video frames. The video operates at 25 frames per second (FPS), and the frames are sampled every 500 milliseconds which result in capturing about two frames each second.We created a dataloader to split the data into training (70%), validation (15%), and test sets (15%).The details of our data split are shown in Table  2 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments Setup And Evaluation",
      "text": "For all three models, the SGD optimizer was employed. To mitigate initial overfitting, batch normalization and dropout layers were added after each convolution layer, coupled with ReLU activation functions to add non-linearity. The learning rate was dynamically adjusted during training using the CosineAnnealingLR scheduler to help the model steer clear of local minima. The model has been trained on a The ME2E and ME2E Lite models We used a batch size of 32, achieving the best performance with a learning rate of 0.0001 and a weight decay of 0.005. As it is a single-label, multi-class task for these two models, we employed the Cross-Entropy loss function, given by L = -N i=1 yi log(pi), where L is the loss for an example, N is the number of classes, yi is the binary indicator for correct class classification, and pi is the predicted probability for class i. We applied Softmax activation which ensures a probability distribution over classes for classification. Evaluation metrics include precision, recall, F1 score, and Accuracy.\n\nThe Proposed VAD model We used a batch size of 8, reaching optimal performance with a learning rate of 0.0005 and a weight decay of 0.0001. Since it is a continuous task, we used Mean Squared Error (MSE) as the loss function and evaluation metric.We also evaluate using L2 distance, Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC) to assess the linear relationship between predictions and actual values, where",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results Analysis",
      "text": "We have trained the baseline model, ME2E Lite model and the proposed VAD model on the MER2024 dataset and evaluated their performance using the metrics described above.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Performance On Continuous Emotion Detection",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Comparison On Discrete Emotion Detection",
      "text": "To assess the performance on discrete emotion detection, we transformed the continuous VAD outputs from the proposed VAD model back into discrete emotion labels. We then evaluated these results alongside the ME2E and ME2E Lite models using F1 score, precision, and recall as performance metrics. Table  4  shows that the the proposed VAD model stands out with higher precision (0.49) and recall (0.45), suggesting it is more effective in accurately identifying and capturing emotions. ME2E Lite model also performs well, with an F1 score of 0.42, and balanced precision and recall at 0.42 and 0.43, respectively. The baseline ME2E model had the lowest performance, with an F1 score of 0.33 and lower precision and recall (both at 0.32).\n\nWhile both the ME2E Lite and proposed VAD models share the same F1 score of 0.42, the VAD model's higher precision indicates it predicts relevant emotions more accurately, and its higher recall means it identifies a greater proportion of true positive emotions. We then applied our proposed VAD model to this subset and mapped the resulting VAD labels into the 3D emotion word space showed in the Introduction. To output the final dataset, we used an L2 distance threshold of 0.25, calculated based on the distance which could output an average of five emotion vocabularies from our 3D Emotion Vocabulary Space.\n\nOur finds indicate that our model effectively predicts nuanced emotional states. For instance, in Sample 00000368, the MER2024 dataset listed emotions like 'Alert,' 'Excited,' 'Confused,' and 'Curious,' and our model predicted 'Shocked.' This aligns well as 'Shocked' can encapsulate alertness, excitement, and confusion. In Sample 0002419, the dataset included 'Calm,' 'Relaxed,' and 'Happy,' while our model suggested 'Caring' and 'Curious.' These predictions are compatible, with 'Caring' reflecting a relaxed and content state. Full output list of To further examine the alignment of our open vocabulary, we assess the semantic similarity across various model's output of emotion labels.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Emotions, inherently complex, can be effectively analyzed through a three-dimensional representation, offering a nuanced approach to categorization adaptable to various needs and contexts. The spatial representation within the VAD framework allows for precise, quantitative analysis and easier conversion between different labeling systems, enhancing our understanding of emotional expressions across multiple modalities.\n\nHowever, our study has limitations, including suboptimal model performance and a small dataset that may hinder generalization. The reliance on the NRC-VAD lexicon, primarily based on English, introduces potential biases in other linguistic or cultural settings.\n\nFuture work could focus on enhancing model performance through larger, more diverse datasets, advanced modeling techniques, and cross-cultural adaptations to better capture the variability in emotional expressions. Further exploration into multimodal integration",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotion Vocabularies in 3D VAD Space",
      "page": 1
    },
    {
      "caption": "Figure 2: VAD Model Pipline",
      "page": 3
    },
    {
      "caption": "Figure 1: and Table 1, demonstrating how similar emotions are",
      "page": 3
    },
    {
      "caption": "Figure 2: uses VAD scores as",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "In the domain of human-computer interaction, accurately recognizing"
        },
        {
          "ABSTRACT": "and interpreting human emotions is crucial yet challenging due to"
        },
        {
          "ABSTRACT": "the complexity and subtlety of emotional expressions. This study ex-"
        },
        {
          "ABSTRACT": "plores the potential for detecting a rich and flexible range of emotions"
        },
        {
          "ABSTRACT": "through a multimodal approach which integrates facial expressions,"
        },
        {
          "ABSTRACT": "voice tones, and transcript from video clips. We propose a novel"
        },
        {
          "ABSTRACT": "framework that maps variety of emotions in a three-dimensional"
        },
        {
          "ABSTRACT": "Valence-Arousal-Dominance (VAD) space, which could reflect the"
        },
        {
          "ABSTRACT": "fluctuations and positivity/negativity of emotions to enable a more"
        },
        {
          "ABSTRACT": "variety and comprehensive representation of emotional states. We"
        },
        {
          "ABSTRACT": "employed K-means clustering to transit emotions from traditional"
        },
        {
          "ABSTRACT": "discrete categorization to a continuous labeling system and built a"
        },
        {
          "ABSTRACT": "classifier for emotion recognition upon this system. The effective-"
        },
        {
          "ABSTRACT": "ness of the proposed model is evaluated using the MER2024 dataset,"
        },
        {
          "ABSTRACT": "which contains culturally consistent video clips from Chinese movies"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "and TV series, annotated with both discrete and open-vocabulary"
        },
        {
          "ABSTRACT": "emotion labels. Our experiment successfully achieved the transfor-"
        },
        {
          "ABSTRACT": "mation between discrete and continuous models, and the proposed"
        },
        {
          "ABSTRACT": "model generated a more diverse and comprehensive set of emotion"
        },
        {
          "ABSTRACT": "vocabulary while maintaining strong accuracy."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Index Terms— Multimodal emotion recognition, emotional vari-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "ability, valence-arousal-dominance (VAD) framework, emotion de-"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "tection, machine learning"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "1.\nINTRODUCTION"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Human emotions are complex and described through diverse vo-"
        },
        {
          "ABSTRACT": "cabularies across languages, reflecting our thoughts, feelings, and"
        },
        {
          "ABSTRACT": "reactions via facial expressions, body language, voice tone, and"
        },
        {
          "ABSTRACT": "speech. [1]. Accurate comprehension and response to human emo-"
        },
        {
          "ABSTRACT": "tions by machines can significantly benefit areas such as market-"
        },
        {
          "ABSTRACT": "ing, mental health monitoring, multimedia generation, and human-"
        },
        {
          "ABSTRACT": "computer\ninteraction [2, 3, 4, 5, 6, 7]. Thus, developing systems"
        },
        {
          "ABSTRACT": "capable of precisely identifying varieties of human emotions is essen-"
        },
        {
          "ABSTRACT": "tial."
        },
        {
          "ABSTRACT": "However, the challenge in emotion detection lies in the subjective"
        },
        {
          "ABSTRACT": "nature of emotions.\nIt is hard to set a clear boundary to categorize"
        },
        {
          "ABSTRACT": "emotions, so as to choose a ‘basic’ emotion group [8]. Moreover, emo-"
        },
        {
          "ABSTRACT": "tion datasets vary in their annotation schemes (e.g., differing discrete"
        },
        {
          "ABSTRACT": "labels) and domains, hindering direct comparisons across previous"
        },
        {
          "ABSTRACT": "works. These variations restrict prior research to specific data sources,"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "limiting their generalizability to real-world applications [9]."
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Recognizing the challenges,\nthis paper proposes a multimodal"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "framework that transforms different discrete emotion labels into one"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "continuous emotion label\nframework. We propose to use a fixed"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "Anonymous."
        },
        {
          "ABSTRACT": "979-8-3503-2411-2/25/$31.00 ©2025 IEEE"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "applying these methods to the “basic expressions” identified in mul-"
        },
        {
          "than learning from discrete categories.": "• We benchmark the open-set emotion classification task by",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "timodal emotion studies [26, 27, 28, 29].\nSimilar\nto other audio"
        },
        {
          "than learning from discrete categories.": "applying the wav2vec model. Experiments demonstrate a high",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "domains [30, 7], emotion-related features consist of two main cate-"
        },
        {
          "than learning from discrete categories.": "correlation between the ground truth and our proposed model.",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "gories:\nlinguistic information, which pertains to the content of speech,"
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "and paralinguistic information, which captures emotions through the"
        },
        {
          "than learning from discrete categories.": "2. RELATED WORK",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "tone and manner of delivery [31].\nIn addition to commonly used"
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "models like CNN, RNN, and LSTM [32, 33], other approaches also"
        },
        {
          "than learning from discrete categories.": "Compared to regular classification tasks [10, 11, 12], multimodal",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "make promising prediction. For instance,\nthe use of features such"
        },
        {
          "than learning from discrete categories.": "emotion detection methods primarily differ in two key areas:\nthe ap-",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "as mel-frequency cepstral coefficient (MFCC), perceptual linear pre-"
        },
        {
          "than learning from discrete categories.": "proaches to categorizing and measuring emotions, and the techniques",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "diction coefficient (PLPC), and perceptual minimum variance distor-"
        },
        {
          "than learning from discrete categories.": "used to extract and integrate features from various modalities.",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "tionless response (PMVDR) coefficient [34] . Textual emotions are"
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "expressed through speech and can be abstract by transcripts. Tradi-"
        },
        {
          "than learning from discrete categories.": "2.1. The measurement of emotion",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "tional\ntechniques such as the bag-of-words (BoW) model [35, 36]"
        },
        {
          "than learning from discrete categories.": "There are two main approaches to measuring emotions:\nthe discrete",
          "facial expressions by tracking features and measuring movement,": "have been commonly used. Then word embedding methods like tf-"
        },
        {
          "than learning from discrete categories.": "method and the multidimensional approach [13]. The discrete method",
          "facial expressions by tracking features and measuring movement,": "idf, word2vec [37] and GloVe [38] were based mainly on syntactic"
        },
        {
          "than learning from discrete categories.": "classifies emotions into a few fundamental categories believed to be",
          "facial expressions by tracking features and measuring movement,": "contexts.\nFor handling larger\ntext, BERT (Bidirectional Encoder"
        },
        {
          "than learning from discrete categories.": "universal. Proponents argue that combinations of these basic emotions",
          "facial expressions by tracking features and measuring movement,": "Representations from Transformers) uses transformer-based encoders"
        },
        {
          "than learning from discrete categories.": "can explain complex emotional states. For instance, fear, surprise,",
          "facial expressions by tracking features and measuring movement,": "that assess both preceding and subsequent contexts to better predict"
        },
        {
          "than learning from discrete categories.": "happiness, disgust, anger, and sadness has been identified as the six",
          "facial expressions by tracking features and measuring movement,": "words in sentences [39]."
        },
        {
          "than learning from discrete categories.": "basic emotions [14].",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "Building upon these basic feelings, a thorough emotional model",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "3. METHODOLOGY"
        },
        {
          "than learning from discrete categories.": "known as Plutchik’s wheel of emotions has been developed [15],",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "which consists of eight main emotions. It is believed that different in-",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "The project aims to bridge discrete and continuous emotion systems,"
        },
        {
          "than learning from discrete categories.": "tensities of these main emotions might mix to create additional related",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "using a multimodal model to detect emotions and produce more nu-"
        },
        {
          "than learning from discrete categories.": "feelings. Studies have been conducted on dataset with 15 Compound",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "anced emotion labels beyond the basic six. This approach allows for"
        },
        {
          "than learning from discrete categories.": "emotion mixed with two basic emotions and 7 Basic facial emotions",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "working with diverse datasets and improves the variety and depth of"
        },
        {
          "than learning from discrete categories.": "as a way for compound emotion detection [16]. In some situations,",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "emotion detection. We first transformed discrete emotion labels into"
        },
        {
          "than learning from discrete categories.": "the discrete method works well enough with a simplified emotional",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "continuous emotion labels, then applied and refined several models"
        },
        {
          "than learning from discrete categories.": "assessment [17, 18]. For example, when it comes to driving systems",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "for the task. The first model, Multimodal End-to-End Sparse Model"
        },
        {
          "than learning from discrete categories.": "in vehicles, determining a driver’s stress level can be accomplished",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "(ME2E)\n[40],\nis a multimodal emotion detection model\nthat uses"
        },
        {
          "than learning from discrete categories.": "by concentrating just on the most fundamentally feelings, like anger",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "discrete emotion labels as input and outputs corresponding discrete"
        },
        {
          "than learning from discrete categories.": "and happiness [18].",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "emotions, serving as our baseline. The second, ME2E Lite model, a"
        },
        {
          "than learning from discrete categories.": "The multidimensional approach, on the other hand, uses contin-",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "refined version of ME2E to better align with our dataset for improved"
        },
        {
          "than learning from discrete categories.": "uous dimensions—valence, arousal, and sometimes dominance—to",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "performance. The third, Proposed VAD, builds on the ME2E Lite"
        },
        {
          "than learning from discrete categories.": "map emotions. Valence measures positivity or negativity, arousal indi-",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "pipeline but uses 3D VAD score as input, outputting continuous VAD"
        },
        {
          "than learning from discrete categories.": "cates the level of activity, and dominance reflects control over the emo-",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "scores that can be mapped to discrete emotions while also generating"
        },
        {
          "than learning from discrete categories.": "tional state [19]. This creates a nuanced two- or three-dimensional",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "a broader range of nuanced emotion vocabularies."
        },
        {
          "than learning from discrete categories.": "emotional space, often referred to as the circumplex model of af-",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "fect [20].",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "3.1. The K-means clustering classifier - Transformation between"
        },
        {
          "than learning from discrete categories.": "",
          "facial expressions by tracking features and measuring movement,": "discrete emotion labels and continuous emotion labels"
        },
        {
          "than learning from discrete categories.": "2.2. Multimodal Emotion Detection",
          "facial expressions by tracking features and measuring movement,": ""
        },
        {
          "than learning from discrete categories.": "Multimodal methods have been widely chosen in research because",
          "facial expressions by tracking features and measuring movement,": "To transition from discrete to multidimensional emotion methods, we"
        },
        {
          "than learning from discrete categories.": "they can extract features from various modalities [21, 22, 23, 24].",
          "facial expressions by tracking features and measuring movement,": "developed a classifier that enables the transformation between discrete"
        },
        {
          "than learning from discrete categories.": "While basic emotions\nlike happy or angry are readily expressed",
          "facial expressions by tracking features and measuring movement,": "and continuous emotion labels using the NRC-VAD lexicon [41]. We"
        },
        {
          "than learning from discrete categories.": "through body language or facial expressions, complex emotions such",
          "facial expressions by tracking features and measuring movement,": "extracted 195 emotion vocabularies with assigned VAD scores from"
        },
        {
          "than learning from discrete categories.": "as jealousy, pride, or hope require additional context and language",
          "facial expressions by tracking features and measuring movement,": "the 20,000-vocabularies NRC-VAD lexicon, using the polar\nterm"
        },
        {
          "than learning from discrete categories.": "for accurate interpretation [21]. Multimodal methods are thus prefer-",
          "facial expressions by tracking features and measuring movement,": "based on a -1 to 1 scale, as taking the absolute values of these scores"
        },
        {
          "than learning from discrete categories.": "able to unimodal ones for recognizing such emotions. This approach",
          "facial expressions by tracking features and measuring movement,": "is likely to yield better features than those derived from a 0 to 1 scale."
        },
        {
          "than learning from discrete categories.": "allows researchers to analyze data from different sources—such as vi-",
          "facial expressions by tracking features and measuring movement,": "The six basic emotions selected—happy,\nsad, worried,\nsurprised,"
        },
        {
          "than learning from discrete categories.": "sual, audio, and textual inputs—in ways that enhance the accuracy and",
          "facial expressions by tracking features and measuring movement,": "angry, and neutral—align with the categories used in our dataset."
        },
        {
          "than learning from discrete categories.": "robustness of emotion detection systems. By utilizing these different",
          "facial expressions by tracking features and measuring movement,": "As VAD scores are available for all selected basic emotions except"
        },
        {
          "than learning from discrete categories.": "streams of information, multimodal methods capture a more compre-",
          "facial expressions by tracking features and measuring movement,": "neutral, we assigned a VAD score of (0,0,0) to neutral, reflecting a"
        },
        {
          "than learning from discrete categories.": "hensive understanding of emotional states, accommodating variations",
          "facial expressions by tracking features and measuring movement,": "lack of significant emotional fluctuations typically associated with"
        },
        {
          "than learning from discrete categories.": "and subtleties that might be missed by unimodal approaches [22].",
          "facial expressions by tracking features and measuring movement,": "this state."
        },
        {
          "than learning from discrete categories.": "Visual, audio, and textual features are captured separately using",
          "facial expressions by tracking features and measuring movement,": "At this stage, each basic emotion can be transformed into a 3D"
        },
        {
          "than learning from discrete categories.": "different methods. Visual features primarily focus on facial expres-",
          "facial expressions by tracking features and measuring movement,": "VAD score, but to enable the inversion from the continuous emotions"
        },
        {
          "than learning from discrete categories.": "sions, which can be seen as facial muscle movements. The Facial",
          "facial expressions by tracking features and measuring movement,": "to discrete labels, we employed a K-means clustering classifier. By"
        },
        {
          "than learning from discrete categories.": "Action Coding System (FACS) [25] has been developed to manu-",
          "facial expressions by tracking features and measuring movement,": "setting k to 6, the classifier is configured to convert continuous VAD"
        },
        {
          "than learning from discrete categories.": "ally code facial expressions using action units (AUs) based on spe-",
          "facial expressions by tracking features and measuring movement,": "scores back into six basic emotion categories."
        },
        {
          "than learning from discrete categories.": "cific muscle movements.\nInspired by their work, many researchers",
          "facial expressions by tracking features and measuring movement,": "To build the emotion classifier, we first mapped the 195 extracted"
        },
        {
          "than learning from discrete categories.": "have utilized image and video processing to analyze and categorize",
          "facial expressions by tracking features and measuring movement,": "emotion vocabularies, along with their associated VAD scores, into"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. VAD Model Pipline": "a 3D emotion space. The six basic emotions—happy, sad, worried,"
        },
        {
          "Fig. 2. VAD Model Pipline": "surprised, angry, and neutral—were set as the initial cluster centers."
        },
        {
          "Fig. 2. VAD Model Pipline": "We then set the number of clusters to six, corresponding to the number"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "of basic emotions."
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "The underlying assumption is that people experiencing similar"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "emotions will exhibit similar VAD scores, meaning their emotional"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "reactions should cluster closely in the VAD space. By applying K-"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "means clustering, we grouped these similar emotions into clusters."
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Each cluster represents a collection of VAD scores that align with one"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "of the basic emotions. After clustering, the centroid of each cluster"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "corresponds to a discrete emotion, allowing us to map continuous"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "VAD scores back to a specific emotion category."
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "This approach effectively allows us to bridge continuous and"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "discrete emotional labels, with the results of the clustering visualized"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "in Figure 1 and Table 1, demonstrating how similar emotions are"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "grouped together based on their VAD scores."
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Table 1. Emotion Clustering"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Cluster\nExamples in Cluster"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Happy\ndelighted, inspired, glad, humorous, cheerful"
        },
        {
          "Fig. 2. VAD Model Pipline": "Sad\nfatigued, mournful, vulnerable, doubtful, regretful"
        },
        {
          "Fig. 2. VAD Model Pipline": "Worried\nguilty, offended, wounded, annoyed, frightened"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Neutral\nkind, warm, thoughtful, sympathetic, humble"
        },
        {
          "Fig. 2. VAD Model Pipline": "Surprised\nemotional, elated, expectant, curious, impressed"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Angry\ngrumpy, vengeful, moody, offended, frantic"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "3.2. ME2E - Baseline Model"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "The ME2E model processes data from video, audio, and text modal-"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "ities.\nFacial\nfeatures are extracted from video frames using the"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "MTCNN model from FaceNet [42], while both these facial features"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "and audio spectrograms are then processed through convolutional"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "layers. These features are further analyzed using a Transformer [43]"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "to capture temporal and contextual nuances. Text data is processed"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "through ALBERT [44], which optimized for short sentences and"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "requiring fewer parameters than BERT. Finally, features from each"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "modality are individually processed and merged via a weighted fusion"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "mechanism, producing a final emotion prediction across six possible"
        },
        {
          "Fig. 2. VAD Model Pipline": "outcomes: happy, angry, sad, neutral, worried, or surprised."
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "3.3. ME2E Lite - The refined Model"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Building on the work of [40], we revised the architecture in our ME2E"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "Lite model by replacing the original 11-layer CNN+VGG video path"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "with AlexNet [45], and simplified the audio path’s CNN by removing"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "one VGG layer, effectively halving the processed parameters to better"
        },
        {
          "Fig. 2. VAD Model Pipline": ""
        },
        {
          "Fig. 2. VAD Model Pipline": "suit our smaller dataset."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Dataset Distribution. C-Avg and W-Avg represent the",
      "data": [
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "and recall as performance metrics."
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "Table 4 shows that the the proposed VAD model stands out with"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "higher precision (0.49) and recall (0.45), suggesting it is more effec-"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "tive in accurately identifying and capturing emotions. ME2E Lite"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "model also performs well, with an F1 score of 0.42, and balanced pre-"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "cision and recall at 0.42 and 0.43, respectively. The baseline ME2E"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "model had the lowest performance, with an F1 score of 0.33 and"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "lower precision and recall (both at 0.32)."
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "While both the ME2E Lite and proposed VAD models share the"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "same F1 score of 0.42, the VAD model’s higher precision indicates"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "it predicts relevant emotions more accurately, and its higher recall"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "means it identifies a greater proportion of true positive emotions."
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "Table 4. Result on Discrete Emotion Detection"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "Model\nF1\nPrecision\nRecall"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "ME2E (Baseline)\n0.33\n0.32\n0.32"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "ME2E Lite\n0.42\n0.42\n0.43"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "Proposed VAD model\n0.42\n0.49\n0.45"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "6.3. Open-vocabulary exploration"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": ""
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "At\nthe final stage of our study, we explored the possibility of gen-"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "erating open-vocabulary emotional responses based on six discrete"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "emotion classes. For this, we utilized a subset of 68 samples from"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "the MER2024 dataset which contains the open-vocabulary outputs."
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "We then applied our proposed VAD model to this subset and mapped"
        },
        {
          "alongside the ME2E and ME2E Lite models using F1 score, precision,": "the resulting VAD labels into the 3D emotion word space showed in"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and more flexible emotion labeling could also provide deeper insights": "and improve model robustness. Lastly, applying time series analysis",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "deep evolutionary roots, a fact that may explain their complex-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "to track emotional shifts over time could offer valuable perspectives",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "ity and provide tools for clinical practice,” American scientist,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "on emotional dynamics.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "vol. 89, no. 4, pp. 344–350, 2001."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[16] P. Heenakausar, N. Sushma, R. Sandeep, K. Lubna,\nand"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "8. REFERENCES",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "V\n. Saurabh, “Compound emotions: A mixed emotion detection"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "(may 26, 2022).” Proceedings of the International Conference"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[1] R. Pally, “Emotional processing: The mind-body connection,”",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "on Innovative Computing and Communication (ICICC), 2022."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "The International journal of psycho-analysis, vol. 79, no. 2, p.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[17] A. Mitchell, E. Brown, R. Deo, Y. Hou, J. Kirton-Wingate,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "349, 1998.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "J. Liang, A. Sheinkman, C. Soelistyo, H. Sood, A. Wongprom-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[2] A. Esposito, A. M. Esposito, and C. Vogel, “Needs and chal-",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "moon, K. Xing, W. Yip, and F. Aletta, “Deep learning tech-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "lenges in human computer\ninteraction for processing social",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "niques for noise annoyance detection: Results from an intensive"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "emotional\ninformation,” Pattern Recognition Letters, vol. 66,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "the\nworkshop at\nthe Alan Turing Institute,” The Journal of"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "pp. 41–51, 2015.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Acoustical Society of America, vol. 153, 03 2023."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[3] H. Zhang, J. Liang, H. Phan, W. Wang, and E. Benetos, “From",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[18] W. Li, B. Zhang, P. Wang, C. Sun, G. Zeng, Q. Tang, G. Guo,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "aesthetics to human preferences: Comparative perspectives of",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "and D. Cao, “Visual-attribute-based emotion regulation of angry"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "evaluating text-to-music systems,” arXiv:2504.21815, 2025.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "driving behaviors,” IEEE Intelligent Transportation Systems"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[4] A. Thieme, D. Belgrave, and G. Doherty, “Machine learning",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Magazine, vol. 14, no. 3, pp. 10–28, 2022."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "in mental health: A systematic review of\nthe HCI\nliterature",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "S. PS and G. Mahalakshmi, “Emotion models: A review,” Inter-\n[19]"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "to support the development of effective and implementable ml",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "national Journal of Control Theory and Applications, vol. 10,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "systems,” ACM Transactions on Computer-Human Interaction",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "no. 8, pp. 651–657, 2017."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "(TOCHI), vol. 27, no. 5, pp. 1–53, 2020.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[20]\nJ. A. Russell, “A circumplex model of affect.” Journal of per-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[5] M. M. Mariani, R. Perez-Vega, and J. Wirtz, “AI in marketing,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "sonality and social psychology, vol. 39, no. 6, p. 1161, 1980."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "consumer\nresearch and psychology: A systematic literature",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "review and research agenda,” Psychology & Marketing, vol. 39,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[21] A. Kazemzadeh, “Natural\nlanguage description of emotion,”"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "no. 4, pp. 755–776, 2022.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Ph.D. dissertation, University of Southern California, 2013."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[6] H. Zhang, S. Chowdhury, C. E. Cancino-Chac´on,\nJ. Liang,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[22] H. Lian, C. Lu, S. Li, Y. Zhao, C. Tang, and Y. Zong, “A"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "S. Dixon, and G. Widmer, “Dexter: Learning and controlling",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "survey of deep learning-based multimodal emotion recognition:"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "performance expression with diffusion models,” Applied Sci-",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Speech, text, and face,” Entropy, vol. 25, no. 10, p. 1440, 2023."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "ences, vol. 14, no. 15, 2024.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[23]\nJ. Liang, X. Liu, H. Liu, H. Phan, E. Benetos, M. D. Plumbley,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[7] H. Zhang, A. Maezawa, and S. Dixon, “Renderbox: Expressive",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "and W. Wang, “Adapting language-audio models as few-shot"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "performance rendering with text control,” arXiv:2502.07711,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "audio learners,” in INTERSPEECH 2023, 2023, pp. 276–280."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "2025.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[24]\nJ. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[8]\nJ. Prinz, “Which emotions are basic,” Emotion, evolution, and",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "E. Benetos, “Acoustic prompt tuning: Empowering large lan-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "rationality, vol. 69, p. 88, 2004.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "guage models with audition capabilities,” IEEE Transactions on"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[9] L. A. M. Oberl¨ander and R. Klinger, “An analysis of annotated",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Audio, Speech and Language Processing, vol. 33, pp. 949–961,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "corpora for emotion classification in text,” in Proceedings of",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "2025."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "the 27th international conference on computational linguistics,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[25] P. Ekman and W. V. Friesen, “Facial action coding system,”"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "2018, pp. 2104–2119.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Environmental Psychology & Nonverbal Behavior, 1978."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[10]\nJ. Liang, H. Phan, and E. Benetos, “Learning from taxonomy:",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[26] Z. Zhang, J. M. Girard, Y. Wu, X. Zhang, P. Liu, U. Ciftci,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "Multi-label few-shot classification for everyday sound recog-",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "S. Canavan, M. Reale, A. Horowitz, H. Yang et al., “Multimodal"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "nition,” in 2024 IEEE International Conference on Acoustics,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "spontaneous emotion corpus for human behavior analysis,” in"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "Speech and Signal Processing (ICASSP), 2024, pp. 771–775.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "Proceedings of\nthe IEEE conference on computer vision and"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[11]\nJ. Liang,\nI. Nolasco, B. Ghani, H. Phan, E. Benetos,\nand",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "pattern recognition, 2016, pp. 3438–3446."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "D. Stowell, “Mind the domain gap: A systematic analysis on",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[27] D. Keltner and D. T. Cordaro, “Understanding multimodal emo-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "bioacoustic sound event detection,” in 2024 32nd European Sig-",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "tional expressions,” The science of facial expression, 1798."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "nal Processing Conference (EUSIPCO), 2024, pp. 1257–1261.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[28]\nP. Metri, J. Ghorpade, and A. Butalia, “Facial emotion recogni-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[12] T. Zhang, J. Liang, and G. Feng, “Adaptive time-frequency",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "tion using context based multimodal approach,” 2011."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "feature resolution network for acoustic scene classification,”",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "Applied Acoustics, vol. 195, p. 108819, 2022.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[29] S. Thushara and S. Veni, “A multimodal emotion recognition"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[13]\nS. K. Khare, V. Blanes-Vidal, E. S. Nadimi, and U. R. Acharya,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "system from video,” in 2016 International Conference on Cir-"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "“Emotion recognition and artificial intelligence: A systematic re-",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "cuit, Power and Computing Technologies (ICCPCT).\nIEEE,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "view (2014–2023) and research recommendations,” Information",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "2016, pp. 1–5."
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "Fusion, vol. 102, p. 102019, 2024.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": ""
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "[30] B. Ding, T. Zhang, C. Wang, G. Liu, J. Liang, R. Hu, Y. Wu,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "[14] P. Ekman and W. V. Friesen, “Constants across cultures in the",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "and D. Guo, “Acoustic scene classification: A comprehensive"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "face and emotion.” Journal of personality and social psychology,",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "survey,” Expert Systems with Applications, vol. 238, p. 121902,"
        },
        {
          "and more flexible emotion labeling could also provide deeper insights": "vol. 17, no. 2, p. 124, 1971.",
          "[15] R. Plutchik, “The nature of emotions: Human emotions have": "2024."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "emotion recognition:\nFeatures,\nclassification schemes,\nand",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "ciates, Inc., 2017."
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "databases,” Pattern recognition, vol. 44, no. 3, pp. 572–587,",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "[44] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "2011.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "R. Soricut, “ALBERT: A lite BERT for self-supervised learning"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[32] W. Lim, D. Jang, and T. Lee, “Speech emotion recognition using",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "of language representations,” in International Conference on"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "convolutional and recurrent neural networks,” in 2016 Asia-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "Learning Representations, 2020."
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Pacific signal and information processing association annual",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "[45] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet clas-"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "summit and conference (APSIPA).\nIEEE, 2016, pp. 1–4.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "sification with deep convolutional neural networks,” Advances"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[33] A. Satt, S. Rozenberg, R. Hoory et al., “Efficient emotion recog-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "in neural information processing systems, vol. 25, 2012."
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "nition from speech using deep learning on spectrograms.” in",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "[46] Z. Lian, H. Sun, L. Sun, Z. Wen, S. Zhang, S. Chen, H. Gu,"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Interspeech, 2017, pp. 1089–1093.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "J. Zhao, Z. Ma, X. Chen et al., “MER 2024: Semi-supervised"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[34] F. Daneshfar, S.\nJ. Kabudian,\nand A. Neekabadi,\n“Speech",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "learning, noise robustness, and open-vocabulary multimodal"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "emotion recognition using hybrid spectral-prosodic features of",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "emotion recognition,” arXiv preprint arXiv:2404.17113, 2024."
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "speech signal/glottal waveform, metaheuristic-based dimension-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "[47] Z. Lian, L. Sun, M. Xu, H. Sun, K. Xu, Z. Wen, S. Chen, B. Liu,"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "ality reduction, and gaussian elliptical basis function network",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "and J. Tao, “Explainable multimodal emotion reasoning,” arXiv"
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "classifier,” Applied Acoustics, vol. 166, p. 107360, 2020.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": "preprint arXiv:2306.15401, 2023."
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[35] M. Schmitt, F. Ringeval, and B. Schuller, “At\nthe border of",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "acoustics and linguistics: Bag-of-audio-words for the recogni-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "tion of emotions in speech,” 2016.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[36] E. Spyrou, T. Giannakopoulos, D. Sgouropoulos, and M. Pa-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "pakostas, “Extracting emotions from speech using a bag-of-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "visual-words approach,” in 2017 12th International Workshop",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "on Semantic and Social Media Adaptation and Personalization",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "(SMAP).\nIEEE, 2017, pp. 80–83.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[37] D. E. Cahyani and I. Patasik, “Performance comparison of",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "TF-IDF and Word2Vec models for emotion text classification,”",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Bulletin of Electrical Engineering and Informatics, vol. 10,",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "no. 5, pp. 2780–2788, 2021.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[38] P. Gupta, I. Roy, G. Batra, and A. K. Dubey, “Decoding emo-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "tions in text using glove embeddings,” in 2021 International",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Conference on Computing, Communication, and Intelligent Sys-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "tems (ICCCIS).\nIEEE, 2021, pp. 36–40.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[39]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Pre-training of deep bidirectional\ntransformers for\nlanguage",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "understanding,” in Proceedings of the 2019 Conference of the",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "North American Chapter of the Association for Computational",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Linguistics: Human Language Technologies, Volume 1 (Long",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "and Short Papers), J. Burstein, C. Doran, and T. Solorio, Eds.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Minneapolis, Minnesota: Association for Computational Lin-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "guistics, Jun. 2019, pp. 4171–4186.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[40] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, “Multimodal",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "end-to-end sparse model for emotion recognition,” in Proceed-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "ings of\nthe 2021 Conference of\nthe North American Chapter",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "of the Association for Computational Linguistics: Human Lan-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "guage Technologies.\nOnline: Association for Computational",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Linguistics, Jun. 2021, pp. 5305–5316.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[41] S. Mohammad, “Obtaining reliable human ratings of valence,",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "arousal, and dominance for 20,000 english words,” in Proceed-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "ings of the 56th annual meeting of the association for computa-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "tional linguistics (volume 1: Long papers), 2018, pp. 174–184.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[42]\nF. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: A unified",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "embedding for face recognition and clustering,” in Proceedings",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "of the IEEE conference on computer vision and pattern recogni-",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "tion, 2015, pp. 815–823.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "need,” in Advances in Neural Information Processing Systems,",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        },
        {
          "[31] M. El Ayadi, M. S. Kamel, and F. Karray, “Survey on speech": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,",
          "S. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Asso-": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotional processing: The mind-body connection",
      "authors": [
        "R Pally"
      ],
      "year": "1998",
      "venue": "The International journal of psycho-analysis"
    },
    {
      "citation_id": "3",
      "title": "Needs and challenges in human computer interaction for processing social emotional information",
      "authors": [
        "A Esposito",
        "A Esposito",
        "C Vogel"
      ],
      "year": "2015",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "4",
      "title": "From aesthetics to human preferences: Comparative perspectives of evaluating text-to-music systems",
      "authors": [
        "H Zhang",
        "J Liang",
        "H Phan",
        "W Wang",
        "E Benetos"
      ],
      "year": "2025",
      "venue": "From aesthetics to human preferences: Comparative perspectives of evaluating text-to-music systems",
      "arxiv": "arXiv:2504.21815"
    },
    {
      "citation_id": "5",
      "title": "Machine learning in mental health: A systematic review of the HCI literature to support the development of effective and implementable ml systems",
      "authors": [
        "A Thieme",
        "D Belgrave",
        "G Doherty"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Computer-Human Interaction (TOCHI)"
    },
    {
      "citation_id": "6",
      "title": "AI in marketing, consumer research and psychology: A systematic literature review and research agenda",
      "authors": [
        "M Mariani",
        "R Perez-Vega",
        "J Wirtz"
      ],
      "year": "2022",
      "venue": "Psychology & Marketing"
    },
    {
      "citation_id": "7",
      "title": "Dexter: Learning and controlling performance expression with diffusion models",
      "authors": [
        "H Zhang",
        "S Chowdhury",
        "C Cancino-Chacón",
        "J Liang",
        "S Dixon",
        "G Widmer"
      ],
      "year": "2024",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "8",
      "title": "Renderbox: Expressive performance rendering with text control",
      "authors": [
        "H Zhang",
        "A Maezawa",
        "S Dixon"
      ],
      "year": "2025",
      "venue": "Renderbox: Expressive performance rendering with text control",
      "arxiv": "arXiv:2502.07711"
    },
    {
      "citation_id": "9",
      "title": "Which emotions are basic",
      "authors": [
        "J Prinz"
      ],
      "year": "2004",
      "venue": "Emotion, evolution, and rationality"
    },
    {
      "citation_id": "10",
      "title": "An analysis of annotated corpora for emotion classification in text",
      "authors": [
        "L Oberländer",
        "R Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th international conference on computational linguistics"
    },
    {
      "citation_id": "11",
      "title": "Learning from taxonomy: Multi-label few-shot classification for everyday sound recognition",
      "authors": [
        "J Liang",
        "H Phan",
        "E Benetos"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Mind the domain gap: A systematic analysis on bioacoustic sound event detection",
      "authors": [
        "J Liang",
        "I Nolasco",
        "B Ghani",
        "H Phan",
        "E Benetos",
        "D Stowell"
      ],
      "year": "2024",
      "venue": "2024 32nd European Signal Processing Conference"
    },
    {
      "citation_id": "13",
      "title": "Adaptive time-frequency feature resolution network for acoustic scene classification",
      "authors": [
        "T Zhang",
        "J Liang",
        "G Feng"
      ],
      "year": "2022",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "15",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "16",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "R Plutchik"
      ],
      "year": "2001",
      "venue": "American scientist"
    },
    {
      "citation_id": "17",
      "title": "Compound emotions: A mixed emotion detection",
      "authors": [
        "P Heenakausar",
        "N Sushma",
        "R Sandeep",
        "K Lubna",
        "V Saurabh"
      ],
      "year": "2022",
      "venue": "Proceedings of the International Conference on Innovative Computing and Communication (ICICC)"
    },
    {
      "citation_id": "18",
      "title": "Deep learning techniques for noise annoyance detection: Results from an intensive workshop at the Alan Turing Institute",
      "authors": [
        "A Mitchell",
        "E Brown",
        "R Deo",
        "Y Hou",
        "J Kirton-Wingate",
        "J Liang",
        "A Sheinkman",
        "C Soelistyo",
        "H Sood",
        "A Wongprommoon",
        "K Xing",
        "W Yip",
        "F Aletta"
      ],
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "19",
      "title": "Visual-attribute-based emotion regulation of angry driving behaviors",
      "authors": [
        "W Li",
        "B Zhang",
        "P Wang",
        "C Sun",
        "G Zeng",
        "Q Tang",
        "G Guo",
        "D Cao"
      ],
      "year": "2022",
      "venue": "IEEE Intelligent Transportation Systems Magazine"
    },
    {
      "citation_id": "20",
      "title": "Emotion models: A review",
      "authors": [
        "S Ps",
        "G Mahalakshmi"
      ],
      "year": "2017",
      "venue": "International Journal of Control Theory and Applications"
    },
    {
      "citation_id": "21",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "22",
      "title": "Natural language description of emotion",
      "authors": [
        "A Kazemzadeh"
      ],
      "year": "2013",
      "venue": "Natural language description of emotion"
    },
    {
      "citation_id": "23",
      "title": "A survey of deep learning-based multimodal emotion recognition: Speech, text, and face",
      "authors": [
        "H Lian",
        "C Lu",
        "S Li",
        "Y Zhao",
        "C Tang",
        "Y Zong"
      ],
      "year": "2023",
      "venue": "Entropy"
    },
    {
      "citation_id": "24",
      "title": "Adapting language-audio models as few-shot audio learners",
      "authors": [
        "J Liang",
        "X Liu",
        "H Liu",
        "H Phan",
        "E Benetos",
        "M Plumbley",
        "W Wang"
      ],
      "venue": "Adapting language-audio models as few-shot audio learners"
    },
    {
      "citation_id": "25",
      "title": "Acoustic prompt tuning: Empowering large language models with audition capabilities",
      "authors": [
        "J Liang",
        "X Liu",
        "W Wang",
        "M Plumbley",
        "H Phan",
        "E Benetos"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "27",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Proceedings"
    },
    {
      "citation_id": "28",
      "title": "Understanding multimodal emotional expressions",
      "authors": [
        "D Keltner",
        "D Cordaro"
      ],
      "year": "1798",
      "venue": "Understanding multimodal emotional expressions"
    },
    {
      "citation_id": "29",
      "title": "Facial emotion recognition using context based multimodal approach",
      "authors": [
        "P Metri",
        "J Ghorpade",
        "A Butalia"
      ],
      "year": "2011",
      "venue": "Facial emotion recognition using context based multimodal approach"
    },
    {
      "citation_id": "30",
      "title": "A multimodal emotion recognition system from video",
      "authors": [
        "S Thushara",
        "S Veni"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)"
    },
    {
      "citation_id": "31",
      "title": "Acoustic scene classification: A comprehensive survey",
      "authors": [
        "B Ding",
        "T Zhang",
        "C Wang",
        "G Liu",
        "J Liang",
        "R Hu",
        "Y Wu",
        "D Guo"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "32",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks,\" in 2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "Speech emotion recognition using convolutional and recurrent neural networks,\" in 2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "34",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition using hybrid spectral-prosodic features of speech signal/glottal waveform, metaheuristic-based dimensionality reduction, and gaussian elliptical basis function network classifier",
      "authors": [
        "F Daneshfar",
        "S Kabudian",
        "A Neekabadi"
      ],
      "year": "2020",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "36",
      "title": "At the border of acoustics and linguistics: Bag-of-audio-words for the recognition of emotions in speech",
      "authors": [
        "M Schmitt",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "At the border of acoustics and linguistics: Bag-of-audio-words for the recognition of emotions in speech"
    },
    {
      "citation_id": "37",
      "title": "Extracting emotions from speech using a bag-ofvisual-words approach",
      "authors": [
        "E Spyrou",
        "T Giannakopoulos",
        "D Sgouropoulos",
        "M Papakostas"
      ],
      "year": "2017",
      "venue": "2017 12th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)"
    },
    {
      "citation_id": "38",
      "title": "Performance comparison of TF-IDF and Word2Vec models for emotion text classification",
      "authors": [
        "D Cahyani",
        "I Patasik"
      ],
      "year": "2021",
      "venue": "Bulletin of Electrical Engineering and Informatics"
    },
    {
      "citation_id": "39",
      "title": "Decoding emotions in text using glove embeddings",
      "authors": [
        "P Gupta",
        "I Roy",
        "G Batra",
        "A Dubey"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)"
    },
    {
      "citation_id": "40",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "41",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "W Dai",
        "S Cahyawijaya",
        "Z Liu",
        "P Fung"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "42",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
      "authors": [
        "S Mohammad"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "43",
      "title": "FaceNet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "44",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg"
    },
    {
      "citation_id": "45",
      "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "46",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "MER 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "Z Wen",
        "S Zhang",
        "S Chen",
        "H Gu",
        "J Zhao",
        "Z Ma",
        "X Chen"
      ],
      "year": "2024",
      "venue": "MER 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "48",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Z Lian",
        "L Sun",
        "M Xu",
        "H Sun",
        "K Xu",
        "Z Wen",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    }
  ]
}