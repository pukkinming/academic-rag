{
  "paper_id": "2111.10803v2",
  "title": "Structure-Preserving Graph Kernel For Brain Network Classification",
  "published": "2021-11-21T12:03:19Z",
  "authors": [
    "Jun Yu",
    "Zhaoming Kong",
    "Aditya Kendre",
    "Hao Peng",
    "Carl Yang",
    "Lichao Sun",
    "Alex Leow",
    "Lifang He"
  ],
  "keywords": [
    "Brain network",
    "graph kernel",
    "tensor product",
    "SVM",
    "EEG",
    "emotion regulation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Brain network analysis is of great importance in clinical diagnosis and treatments. In this paper, we present a novel graphbased kernel learning approach for brain network classification. Specifically, we demonstrate how to exploit the natural graph structure of brain networks to encode prior knowledge in the kernel using the tensor product operator. For each brain network, we first proposed to apply sparse matrix factorization with a symmetric constraint to extract tensor product based approximation. We then used them to derive a structure-persevering symmetric graph kernel to be fed into the support vector machine (SVM). Quantitative evaluations on challenging EEG-based emotion recognition tasks with respect to different frequency bands demonstrate the superior performance of our proposed method, compared with the state-of-the-art traditional and deep learning methods. Together, results show that relevant EEG signals are primarily encoded in the alpha and theta bands during the emotion regulation task, which is consistent with previous findings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Brain network analysis, enriched by the advances of neuroimaging technologies such as electroencephalography (EEG) and diffusion tensor imaging (DTI), has been an appealing research topic in recent years in neuroscience  [1] . The study originates from modeling the human brain connectome as a graph -a mathematical construct mapping the connectivity of anatomically distinct brain regions (i.e., nodes) and inter-regional pathways (i.e., edges). By graph based analysis, the information encoded by the connectome can promote critical understanding on how the brain manages cognition, what signals the connections convey and how these signals affect brain regions  [2] . It has shown great potential in disease diagnosis, clinical outcome prediction, therapeutic adjustment and collection of biological features  [3, 4, 5] . With the development of machine learning algorithms on graph-structured data, it is of great importance to apply such approaches to brain network analysis. In particular, it is desirable to develop more accurate predictive methods as a complement to the effort of pathologists in diagnosis process and treatment decision-making.\n\nIn the past decades, a variety of machine learning methods have been explored for brain network selection and classification. For example, support vector machine (SVM)  [6] , graph kernel  [7] , frequent graph-based pattern mining (gSpan)  [8] , tensor decomposition  [9, 10] . Deep learning methods such as convolution neural network (CNN)  [11]  and graph convolutional network  [12] , which are successful on many tasks, are exploited as well. Although great achievements have been made in various research aspects of these methods, some issues still exists. The human connectome has complex and non-linear characteristics, which may not be well captured by linear models. Meanwhile, deep learning methods suffer from the enormous parameter sizes, which is both difficult for training and vulnerable to overfitting. Besides, many methods do not make good use of graph structure. Thus, it is desirable to develop a concise method for brain network analysis.\n\nIn this paper, we propose a novel graph-based kernel learning approach for brain network predictive analysis, and apply it to the challenging EEG-connectome emotion regulation task. The contributions of this work are threefold:\n\n• We derived a structure-preserving symmetric graph kernel (SSGK) in tensor product space for brain network classification. A new matrix factorization scheme was introduced to incorporate the graph structure as well as the symmetric constraint and sparse layouts.  measuring the similarity of structured data. It has great potentials for a wide range of applications, in conjunction with various kernel-based methods and kernel functions.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Materials And Methods",
      "text": "In this section, we introduce notations and basic concepts, and then describe our method in detail.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Notations And Concepts.",
      "text": "Following  [13] , we denote vectors by lowercase boldface letters, e.g., x; and matrices by uppercase boldface, e.g., X. An index is denoted with a lowercase letter, spanning the range from 1 to the uppercase letter of the index, e.g., i = 1, 2, • • • , I. We denote a matrix as A ∈ R I×J , and their elements by a i,j . We will often use calligraphic letters (A, B, C, • • • ) to denote general space. The inner product of two matrices A, B ∈ R I×J is defined as A, B = I i=1 J j=1 a i,j b i,j . A rank-one matrix A equals to the outer product of two vectors: A = u ⊗ v, where a i,j = u i v j . Note that for rank-one matrices it holds that a ⊗ b, u ⊗ v = a, u b, v .\n\nKernel learning. Support vector machines (SVMs) are one of the most popular kernel-based learning algorithms, which are effective on the data by linear boundaries, and kernel functions are adopted to classify by non-linear boundaries  [6] .\n\nThe kernel function encapsulates the hypothesis language, i.e., how to perform data transformation and knowledge encoding. In general, it maps data from the original input feature space to a higher dimensional feature space (known as Hilbert space), and a kernel function corresponds to the inner product in this higher dimensional feature space. The computational attractiveness of kernel methods comes from the fact that quite often a closed form of 'feature space inner products' exists  [14] . Instead of mapping the data explicitly, the kernel can be calculated directly. According to Mercer's theorem  [6] , we can verify whether a kernel function is valid by the following Theorem  [15] . THEOREM 1 A function κ defined on X ×X is a positive definite kernel of H if and only if there exists a feature mapping function φ(•) : X → H such that κ(x, y) = φ(x), φ(y) , for any (x, y) ∈ X × X .\n\nIn particular, an important property of positive definite kernels is that they are closed under sum, multiplication by a scalar and product  [16] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Structure-Preserving Symmetric Graph Kernel",
      "text": "The brain networks are biologically expected to be both sparse and highly localized in space. Such unique characterizations put specific topological constraints onto machine learning models we can use effectively. We propose a new matrix factorization scheme to incorporate the graph structure as well as the symmetric constraint and sparse layouts, which allows one to interpret brain network as a bilinear tensor product approximation. We then use this approximation to define a structure-preserving symmetric graph kernel function (SSGK) for the SVM classifier. The framework of the proposed method is illustrated in Figure  1 , and we present the key steps of our methods in detail as below.\n\nFeature extraction. The graph provides a natural representation for connectome data, but there is no guarantee that such representation will be good for kernel learning. Since learning will only be successful if the regularities that underlie the data can be discerned by the kernel. From the characteristics of connectome objects, we know that the essential information in the connectome is embedded in the structure of the graph. Thus, one important aspect of kernel learning for such complex objects is to represent them by sets of key structural features which are easier to manipulate. In previous work  [10] , it was found that matrix factorization is particularly effective for extracting this structure. It can take the correlation in the graph matrix into account and represent it directly into a sum of rank one matrices (bilinear bases), yielding a more compact representation of connectome data. Motivated by these observations, we use matrix factorization for feature extraction. In particular, given a graph matrix X ∈ R I×I , we investigate the following optimization problem:\n\nwhere R is the rank of the matrix X defined as the smallest number of rank-one matrices in an exact matrix factorization,\n\n• F is the Frobenius norm of a matrix, and norm, also known as the lasso regularization for a sparse solution. Equation (  2 ) can be solved by the tensorlab toolbox 1 .\n\nGraph structure mapping. Although matrix factorization operation decomposes the graph matrix, the graph structure can still be preserved and retrieved based on the factorized results. We show how the above feature extraction results can be exploited to induce a structure-preserving graph kernel. Suppose we are given the matrix factorization of X, Y ∈ R I×I by X = R r=1 x r ⊗x r and Y = R r=1 y r ⊗y r , respectively. We assume the graph observations are mapped into the Hilbert space H by\n\nImportantly, the mapping result φ(X) is still a symmetric matrix, but its dimension is higher than X, even infinite depending on the feature mapping function φ(•).\n\nBased on the definition of the kernel function, we know that the feature space is a high-dimensional space generated from the original space, equipped with the same operations. Thus, we can factorize graph data directly in the feature space in the same way as in the original space. This is formally equivalent to performing the following mapping:\n\nIn this sense, it corresponds to mapping graphs into highdimensional graphs that retain the original structure. More precisely, it can be regarded as mapping the original graph matrix to matrix feature space and then conducting the matrix factorization in the feature space, as illustrated in Fig.  2 . After mapping the matrix factorization into the outer product feature space, the kernel can be defined directly with the inner product in that feature space. Thus, based on equation (1), we can derive our SSGK model:\n\n(5)\n\n1 https://www.tensorlab.net/ Based on Theorem 1, it is not difficult to see that this kernel is 'valid' as it is described as an inner product of two matrices R r=1 φ(x r ) ⊗ φ(x r ) and R r=1 φ(y r ) ⊗ φ(y r ). From the derivation process, we know that such a kernel can take into account the flexibility of graph structure. In general, SSGK is an extension of the conventional kernels in the vector space to matrix space, and each vector kernel can be used in this framework for EEG-connectome analysis in conjunction with kernel machines. Our positive result can be viewed as saying that designing a good graph kernel function is much like designing a good graph structure in the feature space.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "Participants and data acquisition. The dataset used in this paper were collected from 22 healthy participants at the University of Illinois at Chicago (UIC) and from 11 healthy participants at the University of Michigan (UMich). Each participant underwent an Emotion Regulation Task (ERT). During the ERT session, participants were instructed to look at pictures displayed on the screen. Emotionally neutral pictures (e.g., landscape, everyday objects) and negative pictures (e.g., car crash, nature disasters) would appear on the screen for seven seconds in random orders. One second after the picture on display, a corresponding auditory guide would instruct the participant to neutral: viewing the neutral pictures; to maintain: viewing the negative pictures as they normally would; or to reappraise: viewing the negative pictures while attempting to reduce their emotion response by re-interpreting the meaning of pictures. All subjects were recorded using the Biosemi system equipped with an elastic cap with 34 scalp channels. The acquisition connectivity matrix is 34 × 34 with 130 time points and 50 frequencies ranging from 1Hz to 50Hz in increments of 1Hz. A detailed description about data acquisition and preprocessing is available in  [17] .\n\nTasks. We study multi-class EEG-connectome emotion regulation tasks and analyze the effect of different frequency bands of EEG signals. In emotion regulation, studies have shown that relevant EEG information is primarily encoded in the low frequency bands  [18] . Thus, we analyze the EEGconnectome data in 5 frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-12 Hz), Beta (13-30 Hz), for relative power, as well as the total power of the EEG (1-30 Hz)  [19] . The average EEG-connectome during neutral, maintain and reappraise in the five different frequency bands are shown in Figure  3 , where the xand y-axes represent the vertex id, and the color of the cell represents the strength of the connectivity between vertices x and y. We can see that the connectivity in the alpha band is generally stronger than other frequency bands, and theta band is the second one.\n\nCompared methods. We evaluate eight algorithms in Table  1  on the five tasks above, each of which represents a different strategy: the edge based feature extraction (Edge), where edge values are directly used as features by flatting connectivity matrices of EEG-connectome into vectors; the local clustering coefficients (CC)  [20] , which measures a network's local segregation; the characteristic path length (CPL)  [21]  that quantifies the global information integration; the graph-based substructure pattern mining (gSpan)  [22]  as a discriminative subgraph selection approach; the dual structure-preserving kernel (DuSK)  [10] , which takes multidimensional tensors as input. We use second-(i.e., averaged over time and frequency), third-(i.e., averaged over time), and fourth-order (i.e., all data with dimension 34 × 34 × 130 × x, where x corresponds to a number of the frequency level) version of this scheme, denoted as DuSK-2D, DuSK-3D and DuSK-3D, respectively; the convolutional neural network (CNN) with 2D convolutions for averaged 2D brain network data and 3D convolutions for averaged 3D brain network data  [23] ; the graph convolutional network (GCN) for averaged 2D brain network data  [12] , where the average of all brain networks is used as the graph structure (i.e., adjacency matrix) for information propagation; the proposed method and its variant without sparse-constraint (SSGK and SSGK w/o sparse ). Experimental settings. In our experiments, we use the subjects collected from UIC as the training set (66 samples), and UMich as the testing set (33 samples). Following  [10] , we choose SVM with Gaussian RBF kernel as the classifier for all methods. Classification accuracy is used as the evaluation metric. All compared methods select the optimal trade-off parameter of SVM and kernel width from {2 -8 , 2 -7  are set according to  [22]  and  [10] , respectively. For our SSGK and SSGK w/o sparse methods, the parameter R and λ was selected from the value set of\n\nResults. Detailed results of compared methods are listed in Table  1 . From Table  1 , it can be seen that the proposed SSGK-based methods outperform all compared methods by 10%-20% on almost all five different frequency bands. The superiority of the proposed methods demonstrate the effectiveness of utilizing the structure information within the graph representation during encoding. Specifically, among all five bands, SSGK produces the best performance on Alpha band and second best performance on Theta band, which is consistent with previous findings  [17]  and can also be observed in our visualization in Figure  3 . Furthermore, by comparing SSGK and SSGK w/o sparse , it is noticed that the proposed SSGK approach with sparse regularization consistently outperforms the same approach without sparse regularization, and the advantage of sparsity characterization indicates the importance of modeling the redundant information of observed frequency bands.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes a graph-based kernel learning approach called Structure-preserving Symmetric Graph Kernel (SSGK) for brain network classification task. The proposed method mainly follows two consecutive steps: first, a sparse-inducing symmetric matrix factorization strategy is applied to extract structural features from the natural symmetric graph representations of the brain network data, then the extracted structural features are directly used to define the SSGK function and further fed into the support vector machine for the classification. Experimental results on challenging EEG-based emotion recognition task demonstrates the effectiveness of the proposed method for encoding prior knowledge in the kernel using structural information of brain networks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of graph-based kernel learning.",
      "page": 2
    },
    {
      "caption": "Figure 1: , and we present the",
      "page": 2
    },
    {
      "caption": "Figure 2: Schemic diagram of the feature extraction and graph",
      "page": 3
    },
    {
      "caption": "Figure 2: After mapping the matrix factorization into the outer",
      "page": 3
    },
    {
      "caption": "Figure 3: , where the x– and y–axes represent the vertex id, and",
      "page": 3
    },
    {
      "caption": "Figure 3: Average EEG-connectome during neutral, maintain",
      "page": 4
    },
    {
      "caption": "Figure 3: Furthermore, by compar-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The classification accuracy in percentage (%) by",
      "data": [
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "Delta\nTheta\nAlpha\nBeta\nAll"
        },
        {
          "Category": "Traditional",
          "Method": "Edge\nCC\nCPL\ngSpan\nDuSK–2D\nDuSK–3D\nDuSK–4D",
          "Frequency Band": "42.42\n54.55\n51.52\n51.52\n45.45"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "54.55\n54.55\n42.42\n51.52\n42.42"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "48.48\n42.42\n45.45\n48.48\n39.39"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "39.39\n51.52\n39.39\n54.55\n48.48"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "51.52\n63.64\n51.51\n51.52\n54.55"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "57.58\n57.58\n57.58\n54.55\n48.48"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "54.55\n54.55\n51.52\n54.55\n57.58"
        },
        {
          "Category": "Deep Learning",
          "Method": "CNN–2D\nCNN–3D\nGCN",
          "Frequency Band": "51.11\n43.71\n43.07\n42.54\n41.48"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "46.67\n45.93\n41.48\n57.04\n44.44"
        },
        {
          "Category": "",
          "Method": "",
          "Frequency Band": "41.31\n48.08\n41.01\n40.61\n37.37"
        },
        {
          "Category": "Ours",
          "Method": "SSGKw/o sparse\nSSGK",
          "Frequency Band": "57.58\n66.67\n63.64\n54.55\n57.58\n63.64\n69.70\n72.73\n60.61\n57.58"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Fundamentals of brain network analysis",
      "authors": [
        "A Fornito",
        "A Zalesky",
        "E Bullmore"
      ],
      "year": "2016",
      "venue": "Fundamentals of brain network analysis"
    },
    {
      "citation_id": "3",
      "title": "New graph-blind convolutional network for brain connectome data analysis",
      "authors": [
        "Y Zhang",
        "H Huang"
      ],
      "year": "2019",
      "venue": "New graph-blind convolutional network for brain connectome data analysis"
    },
    {
      "citation_id": "4",
      "title": "Human connectome module pattern detection using a new multi-graph minmax cut model",
      "authors": [
        "D Wang",
        "Y Wang",
        "F Nie",
        "J Yan",
        "W Cai",
        "A Saykin",
        "L Shen",
        "H Huang"
      ],
      "year": "2014",
      "venue": "MICCAI"
    },
    {
      "citation_id": "5",
      "title": "Identifying connectome module patterns via new balanced multigraph normalized cut",
      "authors": [
        "H Gao",
        "C Cai",
        "J Yan",
        "L Yan",
        "J Cortes",
        "Y Wang",
        "F Nie",
        "J West",
        "A Saykin",
        "L Shen"
      ],
      "year": "2015",
      "venue": "Identifying connectome module patterns via new balanced multigraph normalized cut"
    },
    {
      "citation_id": "6",
      "title": "A small number of abnormal brain connections predicts adult autism spectrum disorder",
      "authors": [
        "N Yahata",
        "J Morimoto",
        "R Hashimoto",
        "G Lisi",
        "K Shibata",
        "Y Kawakubo",
        "H Kuwabara",
        "M Kuroda",
        "T Yamada",
        "F Megumi"
      ],
      "year": "2016",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "7",
      "title": "The nature of statistical learning theory",
      "authors": [
        "V Vapnik"
      ],
      "year": "2013",
      "venue": "The nature of statistical learning theory"
    },
    {
      "citation_id": "8",
      "title": "Unified brain network with functional and structural data",
      "authors": [
        "J Yang",
        "Q Zhu",
        "R Zhang",
        "J Huang",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "Unified brain network with functional and structural data"
    },
    {
      "citation_id": "9",
      "title": "Network-based classification of adhd patients using discriminative subnetwork selection and graph kernel pca",
      "authors": [
        "J Du",
        "L Wang",
        "B Jie",
        "D Zhang"
      ],
      "year": "2016",
      "venue": "COMPUT. MED. IMAG. GRAP"
    },
    {
      "citation_id": "10",
      "title": "Identifying network correlates of brain states using tensor decompositions of whole-brain dynamic functional connectivity",
      "authors": [
        "N Leonardi",
        "D Van De",
        "Ville"
      ],
      "year": "2013",
      "venue": "International Workshop on Pattern Recognition in Neuroimaging"
    },
    {
      "citation_id": "11",
      "title": "Dusk: A dual structure-preserving kernel for supervised tensor learning with applications to neuroimages",
      "authors": [
        "L He",
        "X Kong",
        "P Yu",
        "X Yang",
        "A Ragin",
        "Z Hao"
      ],
      "year": "2014",
      "venue": "SDM"
    },
    {
      "citation_id": "12",
      "title": "Structural deep brain network mining",
      "authors": [
        "S Wang",
        "L He",
        "B Cao",
        "C Lu",
        "P Yu",
        "A Ragin"
      ],
      "year": "2017",
      "venue": "ACM SIGKDD"
    },
    {
      "citation_id": "13",
      "title": "Multi-view graph convolutional network and its applications on neuroimage analysis for parkinson's disease",
      "authors": [
        "X Zhang",
        "L He",
        "K Chen",
        "Y Luo",
        "J Zhou",
        "F Wang"
      ],
      "year": "2018",
      "venue": "American Medical Informatics Association"
    },
    {
      "citation_id": "14",
      "title": "Tensor decompositions and applications",
      "authors": [
        "T Kolda",
        "B Bader"
      ],
      "year": "2009",
      "venue": "SIAM review"
    },
    {
      "citation_id": "15",
      "title": "A survey of kernels for structured data",
      "authors": [
        "T Gärtner"
      ],
      "year": "2003",
      "venue": "ACM SIGKDD Explorations Newsletter"
    },
    {
      "citation_id": "16",
      "title": "Reproducing kernel Hilbert spaces in probability and statistics",
      "authors": [
        "A Berlinet",
        "C Thomas-Agnan"
      ],
      "year": "2011",
      "venue": "Reproducing kernel Hilbert spaces in probability and statistics"
    },
    {
      "citation_id": "17",
      "title": "An introduction to support vector machines and other kernel-based learning methods",
      "authors": [
        "B Cristianini",
        "J Shawe-Taylor"
      ],
      "year": "2000",
      "venue": "An introduction to support vector machines and other kernel-based learning methods"
    },
    {
      "citation_id": "18",
      "title": "EEG based functional connectivity reflects cognitive load during emotion regulation",
      "authors": [
        "M Xing",
        "R Tadayonnejad",
        "A Macnamara",
        "O Ajilore",
        "K Luan Phan",
        "H Klumpp",
        "A Leow"
      ],
      "year": "2016",
      "venue": "ISBI"
    },
    {
      "citation_id": "19",
      "title": "What hemodynamic, electrophysiological and autonomic integrated measures can tell us about emotional processing",
      "authors": [
        "M Balconi",
        "E Grippa",
        "M Vanutelli"
      ],
      "year": "2015",
      "venue": "Brain and cognition"
    },
    {
      "citation_id": "20",
      "title": "Mental training enhances attentional stability: neural and behavioral evidence",
      "authors": [
        "A Lutz",
        "H Slagter",
        "N Rawlings",
        "A Francis",
        "L Greischar",
        "R Davidson"
      ],
      "year": "2009",
      "venue": "The Journal of Neuroscience"
    },
    {
      "citation_id": "21",
      "title": "Complex network measures of brain connectivity: uses and interpretations",
      "authors": [
        "M Rubinov",
        "O Sporns"
      ],
      "year": "2010",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "22",
      "title": "Collective dynamics of 'small-world'networks",
      "authors": [
        "D Watts",
        "S Strogatz"
      ],
      "year": "1998",
      "venue": "nature"
    },
    {
      "citation_id": "23",
      "title": "gspan: Graph-based substructure pattern mining",
      "authors": [
        "X Yan",
        "J Han"
      ],
      "year": "2002",
      "venue": "ICDM"
    },
    {
      "citation_id": "24",
      "title": "Natural image bases to represent neuroimaging data",
      "authors": [
        "A Gupta",
        "M Ayhan",
        "A Maida"
      ],
      "year": "2013",
      "venue": "ICML"
    }
  ]
}