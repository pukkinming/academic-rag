{
  "paper_id": "2008.09743v2",
  "title": "A Multimodal Framework For Large-Scale Emotion Recognition By Fusing Music And Electrodermal Activity Signals",
  "published": "2020-08-22T03:13:20Z",
  "authors": [
    "Guanghao Yin",
    "Shouqian Sun",
    "Dian Yu",
    "Dejian Li",
    "Kejun Zhang"
  ],
  "keywords": [
    "multimodal fusion",
    "large-scale emotion recognition",
    "attention mechanism 74.48% A: 77.56% SVM [34] DEAP 32 EEG",
    "EOG",
    "EMG",
    "GSR",
    "TMP",
    "BVP",
    "RSP 10-fold V: 62.70% A: 57.00% -MLP [52] DEAP 32 EDA 10-fold V: 69.80% A: 79.00% -CNN [1] DEAP 32 EDA 10-fold V: 82.00% A: 82.00% V: 83.00% A: 83.00% RTCAG-1D DEAP 32 EDA 10-fold V: 77.15% A: 83.42% V: 79.18% A: 83.77% Alexnet-2D [40] DEAP 32 EEG",
    "EOG",
    "EMG",
    "GSR",
    "TMP",
    "BVP",
    "RSP 10-fold V: 87.30% A: 85.50%"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Considerable attention has been paid to physiological signal-based emotion recognition in the field of affective computing. For reliability and user-friendly acquisition, electrodermal activity (EDA) has a great advantage in practical applications. However, EDA-based emotion recognition with large-scale subjects is still a tough problem. The traditional well-designed classifiers with hand-crafted features produce poorer results because of their limited representation abilities. And the deep learning models with auto feature extraction suffer the overfitting drop-off because of large-scale individual differences. Since music has a strong correlation with human emotion, static music can be involved as the external benchmark to constrain various dynamic EDA signals. In this paper, we make an attempt by fusing the subject's individual EDA features and the external evoked music features. And we propose an end-to-end multimodal framework, the one-dimensional residual temporal and channel attention network (RTCAN-1D). For EDA features, the channel-temporal attention mechanism for EDA-based emotion recognition is first involved in mine the temporal and channel-wise dynamic and steady features. The comparisons with single EDA-based SOTA models on DEAP and AMIGOS datasets prove the effectiveness of RTCAN-1D to mine EDA features. For music features, we simply process the music signal with the open-source toolkit openSMILE to obtain external feature vectors. We conducted systematic and extensive evaluations. The experiments on the current largest music emotion dataset PMEmo validate that the fusion of EDA and music is a reliable and efficient solution for large-scale emotion recognition. CCS Concepts: â€¢ Human-centered computing â†’ HCI theory, concepts and models; â€¢ Computing methodologies â†’ Neural networks; â€¢ Applied computing â†’ Bioinformatics and Media.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "The ability of emotion recognition is a significant hallmark of intelligent human-computer interaction (HCI). Empowering computers to discern human emotions better would make them perform the appropriate actions. In recent two decades, both the research groups and industries have aroused great interest that explores to enhance user experience with algorithms  [45] . With the advancement of sensor-based Internet of Things (IoT) technology, emotion recognition is in unceasing demand by a wide range of applications such as mental healthcare  [21] , driving monitor  [29] , wearable devices  [49] , etc.\n\nHuman emotions are usually defined as a group of affective states that evoke stimuli from external environments or interpersonal events as a response  [46] . According to behavioral researches, emotions are typically described as the set of discrete basic categories  [15] . They can also be represented in the continuous space  [22] , which can quantitatively describe the abstract emotion state percentage, which provides another method for understanding the concrete representations of abstract emotions. The valence-arousal space is one of the common emotion discrete spaces, where the valence reflects the value of the positive to negative state and the arousal measures the active to calm degree  [36] . For the advantage of conducting the Self-Assessment Manikins (SAM), most of the popular open-source emotion datasets applied the valence-arousal annotations, such as DEAP  [34] , AMIGOS  [12] , PMEmo  [63] , or etc.\n\nIn general, the data source of emotion recognition can be categorized into (1) peripheral physical signals such as facial expression  [33] , speech  [16] ; (2) physiological signals including electroencephalography (EEG), electrodermal activity (EDA), electrocardiogram (ECG), or etc. The first method has great advancement in data collection. However, it cannot guarantee reliability in recognition because human can consciously control body language to hide their personal emotionlike speech or facial expression. Using physiological signals can overcome this drawback. The physiological signals are reacted from the autonomic and somatic nervous systems (ANS and SNS). They are largely involuntarily activated and cannot be triggered by any conscious or intentional control  [28] . Thus, the physiological signal-based method provides an avenue to recognize affect changes that are less obvious to perceive visually  [28] . Compared with other physiological signals, the acquisition of EDA can be conveniently accomplished by intelligent wearable devices like smartwatches and wisebraves  [4] . The research of EDA-based emotion recognition can be put faster into practical applications such as emotion monitor  [43] , music liking recommendation  [11] , or etc. Hence, the EDA signal has great significance of scientific value and practicability.\n\nTo mine useful features from physiological signals, two broad methods exist: (1) the traditional well-designed methods with hand-crafted features, and (2) the deep learning models with auto feature extraction. According to previous studies, a wide range of statistical features have been explored to seek predictive power for the classification of emotions, including time domain, frequency domain, and time-frequency domain features  [53] . However, the hand-crafted feature-based methods suffer limitations in the following aspects. First, the traditional methods such as SCL have limited representation ability to extract the complicated EDA features from large-scale subjects. Second, the design of hand-crafted feature strongly depends on prior knowledge of statistics and physiology in the small-scale data. Researchers should take more and more effective hand-crafted features into account for better performance. It is a great challenge for their feature selection method. Although, Shukla  [53]  has provided a systematic exploration of the feature selection method, it cannot guarantee the robustness with large-scale complicated physiological signals.\n\nRecently, the effectiveness of deep learning model has been proven in tackling emotion recognition  [1] . Moreover, the end-to-end DNN model can directly learn discriminative features from data. Therefore, to involve less prior knowledge and explore the general solution for large-scale emotion recognition, we choose the end-to-end deep learning model. Compared with single subject, establishing the relationship between emotional state and the physiological signals with large-scale subjects is a challenging problem, because more subjects involve more complicated individual specificity. For emotion recognition with one subject, previous user-dependent system  [32]  has achieved more than 90% accuracy. However, when extending the subject from one to group, the accuracy of user-independent models decreased greatly  [28]  because of the individual specificity. Moreover, our experimental results in Section 5.2.1 have further supported this. Specifically, we use various hand-crafted classifier and CNN-based models to conduct incremental subject classification. In relatively small-scale datasets with dozens of subjects, such as DEAP (32 subjects), AMIGOS (40 subjects), and 1/10 PMEmo datasets (46 subjects), the models can build accurate relationships between EDA and emotion central states. However, when gradually increasing the subject number of PMEmo dataset to the maximum 457, the representation power of traditional classifiers with hand-crafted features is too weak to produce poor results, and the CNN-based deep learning model suffers overfitting. Since all those two mainstream methods have obvious defections, mining the single EDA single cannot further improve the performance for large-scale emotion recognition.\n\nTowards this, we consider fusing external features with EDA signals. The multimodal fusion has the great advantage to enrich the complementary information from different modalities  [5] . It has a broad range of successes, including multi-sensor management  [61] , and human-computer interaction  [27] . In the scene of psychological rehabilitation  [37] , or emotion tracing  [6] , music is commonly utilized medium because it is effective to modulate subject' s mood  [41] . In PMEmo dataset, music is what the authors use to induce emotions among participants. The tempo, beats, and other acoustic features regarding the music being played will correlate with the participants' observed emotions. Hence, we choose the music signal as the external constraint for supervised learning. As our experiments show, fusing the music features can efficiently improve the performance of RTCAN-1D and also overcome the defect that the EDA signal is relevant for arousal classification  [53, 57] .\n\nIn this paper, we propose RTCAN-1D, a novel end-to-end 1D residual temporal and channel attention network for emotion recognition in the valence-arousal dimension. We directly apply a 1D residual block to build the EDA residual feature extraction (RFE). To provide sufficient useful messages from 1-channel EDA, we apply the convex optimization-based EDA method  [20]  to decompose EDA into phasic, tonic components and feed the 3 channel mixed signals (origin, phasic, and tonic) into RTCAN-1D. Then, we involve the attention mechanism: (1) the signal channel attention module (SCA) to reweight the features of 3-mixed signal channels and (2) the residual nonlocal temporal attention module (RNTA) to explore the temporal correlations of EDA features. To the best of our knowledge, our model is the first attempt to involve the attention mechanism to process physiological signals We conduct extensive evaluations and compare our model with previous researches. Because the music and EDA signals from large-scale subjects are only available from the current largest PMEmo dataset, the multimodal fusion of RTCAN-1D is evaluated on PMEmo. Moreover, we prove the effectiveness of RTCAG with single EDA input from the small-scale DEAP and AMIGOS datasets. For a fair comparison with the SOTA methods, our proposed model was applied to the subject-independent binary classification of valence and arousal.\n\nIn summary, the contributions of our paper are represented as follows:\n\nâ€¢ An end-to-end multimodal framework RTCAN-1D is proposed for user-independent emotion recognition. â€¢ The effectiveness of the attention mechanism is first proved to capture the channel-wise correlations and long-distance temporal information of EDA signals.\n\nâ€¢ The fusion of individual specificity from EDA signals and external static benchmarks from music provides an efficient solution for large-scale emotion recognition.\n\nOur paper is organized as follows. Section 2 introduces literature and reviews related works. In Section 3, our proposed model is explained in detail. Experimental validations and result analysis are described in Section 4, along with the details of implementation. The slights and discussions are expressed in Section 5. Finally, we conclude this paper in Section 6 by summarizing the attributions and future prospects. Our codes have been released at https://github.com/guanghaoyin/RTCAN-1D.",
      "page_start": 2,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Representation",
      "text": "When we focus on emotional recognition, the emotions should be quantitatively represented. Psychologists model the emotion in two types:  (1)  dividing the emotion into discrete classes and (2) using the multidimensional emotion space.\n\nThe discrete emotion representation uses several keywords to describe emotion rates. Plutchik  [47]  proposed a wheel model that includes eight basic emotions. In his model, the stronger emotions are in the center, while the weaker emotions are surrounded. More complex emotions can be formed by a mix of basic ones. Then, Damasio  [13]  categorised the emotions as primary (deriving from innate fast and responsive behaviour) or secondary (deriving from cognitive thoughts). Izard  [26]  extended to 10 basic emotions. However, the discrete description is unsuitable for analyzing complex emotions, because some mixed emotions cannot be precisely expressed in words.\n\nThe second mainstream presented a certain degree of specific emotional level in the multidimensional space of emotion. A first attempt was proposed by Wundt  [60] , where emotions were described in a pleasure-displeasure, excitement-inhibition, tension-relaxation 3D space. In 1979, Russell  [48]  postulated a two-dimensional model, spanned by the axes valence and arousal. The valence reflects the value of the positive or negative state, and the arousal measures the active or calm degree  [14] . Now, the V/A model becomes one of the most frequently employed model because it can be easily assessed using the Self-Assessment Manikins (SAM). The three affective datasets used in this paper are all measured by V/A space.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Fusion For Emotion Recognition",
      "text": "Multimodal information fusion technique is used to improve the performance of the system by integrating different information on the feature level, intermediate level, or decision level. To improve the robustness and performance of emotion recognition, previous researchers fuse the features from different physiological signals  [28, 32] , including ECG, RSP, EMG, EDA, and EEG because the fused signals contain more implicit information with the different sampling rate and more channels. But collecting multi-signals requires many conductive electrodes placed along the subject's scalp. The user-unfriendly acquisition limits practical applications. Therefore, some works added external media messages, which have a great relationship with human emotion. Kim et al.  [31]  systematically present the robust emotion recognition fusing the speech and biosignals. Liu et la.  [42]  combined the evoked video and EEG signals to predict the subject's emotional state. Koelstra et la.  [35]  combined the facial expression with EEG signals. And Thammasan et la.  [56]  fused the EEG and musical features. There is no effective exploration for the fusion of EDA and music signals, especially with large-scale subjects. Hence, our paper focuses on this issue.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eda Processing",
      "text": "The EDA signal is widely used in the psychology or physiology field. The approaches to process EDA signal include Bayesian Statistics  [53] , dictionary learning  [30] , and decomposition  [2] . We focus on the end-to-end general solution. Hence, complex processing is inappropriate. Therefore, we choose to decompose the EDA. The frequently used measurement of EDA is skin conductance (SC), which is composed of tonic, phasic components  [9] . The tonic phenomena (t) reflect slow drifts of the baseline level and spontaneous fluctuations in SC. The phasic phenomena (r) is generally a short-time rapid fluctuation and reflects the response to external stimulation  [9] . For realistic tasks, the measurement noise of equipment is also taken into consideration.\n\nThere have been several mathematical solutions associated with a stimulus for EDA decomposition in recent decades. Referring to Alexander ğ‘’ğ‘¡ ğ‘ğ‘™ .  [2] , the well-known assumption is that the SC equals to the convolution between discrete bursting episodes of sudomotor nerve activity (SMNA) and biexponential impulse response function (IRF):\n\nThere exists some methods for EDA decomposition. Discrete deconvolution analysis (DDA)  [8]  claims the nonnegativity of the driver and maximal compactness of the impulses to decompose EDA through nonnegative deconvolution. Continuous decomposition analysis (CDA)  [7]  improves DDA with the IRF called Bateman function to conduct continuous decomposition. Convex Optimization-Based EDA Method (CvxEDA)  [20]  models the phasic, tonic, noise components and applies convex optimization to solve the maximum a posteriori (MAP) problem. CvxEDA provides a novel method to decompose EDA without preprocessing steps and heuristic solutions  [19] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Attention Mechanism",
      "text": "In human proprioceptive systems, attention generally provides a guide to focus on the most informative components of an input  [24] . Recently, attention models improve the performance of deep neural networks for various tasks, ranging from video classification  [59]  to language translation  [58] . Pei ğ‘’ğ‘¡ ğ‘ğ‘™ .  [44]  proposed a temporal attention module to detect salient parts of the sequence while ignoring irrelevant ones in sequence classification. Wang ğ‘’ğ‘¡ ğ‘ğ‘™ .  [59]  proposed a portable nonlocal module for spatial attention to capture long-range dependencies in video classification. Hu ğ‘’ğ‘¡ ğ‘ğ‘™ .  [24]  proposed SENet to incorporate channel-wise relationships to achieve significant improvement for image classification.\n\nTo the best of the authors' knowledge, the attention mechanism has not been proposed in physiological-based emotion recognition. As the mixed 3-channel input is sequential, each channel may contribute differently to the final prediction in different temporal range. In this work, we use an efficient channel-wise and temporal attention mechanism based on 3-channel mixed EDA characteristics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Network Framework",
      "text": "The great success of CNN frameworks depends on 2D convolution in signal processing and image domain  [23] . Our previous work  [62]  in EDA-based emotion recognition with the PMEmo dataset  [63]  also conducted 2D convolution. However, it is a detour to transform the 1D signal to 2D matrix just for fine-tuning pre-trained 2D CNN backbones. Moreover, if using the 2D convolution, the pre-precessing should be conducted to rearrange the 1D EDA sequences and transform them to 2D matrices as our previous work did  [62] , which is time-consuming and increases the computations. Accordingly, we directly use the 1D residual CNN framework to process EDA signals in this work.\n\nThe overall architecture of the proposed method is illustrated in Fig.  1 . Let the vector ğ‘‹ ğ‘‡ = {ğ‘¥ 1 , ğ‘¥ 2 , ..., ğ‘¥ ğ‘¡ } be the input EDA sequence with ğ‘¡ points. ğ‘Œ ğ‘ğ‘™ğ‘ğ‘ ğ‘  = {ğ‘¦ 1 , ..., ğ‘¦ ğ‘ } denotes the ground truth for ğ‘-classes task. If the emotion state belongs to the ğ‘– ğ‘¡â„ class, ğ‘¦ ğ‘– = 1 and ğ‘¦ ğ‘— = 0 for ğ‘— â‰  ğ‘–. Our goal is to establish the relationships between input EDA and emotion state, specifically decreasing the distance between ground truth label ğ‘Œ ğ‘ğ‘™ğ‘ğ‘ ğ‘  and predicted result Å¶ğ‘ğ‘™ğ‘ğ‘ ğ‘  .\n\nFirst, after z-score normalization, we conduct CvxEDA decomposition. The normalized EDA signal (y) is composed of three N-long column vectors: the phasic (r) and tonic (t) signal plus the noise component (ğœ€):\n\nwhere ğ‘Ÿ = ğ‘€ğ´ -1 ğ‘ and ğ‘¡ = ğµğœ† + ğ¶ğ‘‘. Then, with the transcendental knowledge of physiology and taking logarithmic transtorm, the MAP problem can be rewritten as a standard Quadratic-Programming (QP) convex form and can be solved efficiently using many available solvers (see more details in  [20] ). For denoizing in the process of solving optimization problem, we discarded the prior probability of noise term. After CvxEDA decomposition, the 1-channel EDA signal is expanded to mixed 3-channel signal\n\nAs shown in Fig.  2 , in order to reduce the computational complexity, we uniformly clip shallow feature to 3 parts and the clipped signals are subsequently fed into the residual temporal and channel attention group (RTCAG) with shared attention weights. The EDA feature vector is extracted by three parts: shallow feature extraction, attention module, and residual feature extraction. One convolutional layer extracts shallow feature ğ¹ ğ‘†ğ¹ as\n\nwhere ğ» ğ‘†ğ¹ (â€¢) denotes convolution and batch-normalization operation. Then, the attention module reweights the shallow feature and focuses on more useful parts in temporal and channel-wise dimensions as  where ğ¹ ğ´ is rearranged feature and ğ» ğ´ (â€¢) is the attention module respectively. To mine discriminative representations, we design the residual feature extraction (ğ» ğ‘…ğ¹ (â€¢)) with ResNet-18 backbone  [23]  to output a EDA feature vector (ğ¹ ğ¸ğ¹ ):\n\nIt should be emphasized that there exists other excellent CNN architectures such as VGG  [54] ,\n\nor DenseNet  [25]  to mine deep features, but a comparison of different CNN frameworks is not the focus of our research. To be specific, for dozens of people involved EDA dataset, the subjects' specificity is not so complicated that the RTCAN-1D can handle the single EDA input, which means\n\nFor the large-scale PMEmo dataset with hundreds of subjects, we fuse the EDA feature with an external static media feature (ğ¹ ğ‘€ğ¹ ). Under this circumstances, the full feature vector is the concatenation of EDA feature and media feature:\n\nFinally, 3 fully-connected layers followed with 3 ReLU functions, termed as (ğ» ğ¶ğ¹ (â€¢)), are fed with the full feature vector (ğ¹ ğ¹ ğ¹ ) and output the classifying vector ( Å¶ğ‘ğ‘™ğ‘ğ‘ ğ‘  ) with the Softmax function Å¶ğ‘ğ‘™ğ‘ğ‘ ğ‘  = ğ‘†ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ» ğ¶ğ¹ (ğ¹ ğ¹ ğ¹ )).\n\n(7)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Residual Temporal And Channel Attention Group",
      "text": "Now we show our residual temporal and channel attention group (RTCAG) (seen in Fig.  2 ). After the shallow feature extraction, the feature map is processed with the series structures of signal channel attention module (SCA), residual nonlocal temporal attention module (RNTA), and residual feature extraction (RFE) to output an EDA feature vector. The purpose of the attention mechanism is to adaptively reweight the shallow feature for more useful messages in the channel and temporal dimensions. Hence, the SCA and RNTA are plugged before RFE.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Signal Channel Attention Module.",
      "text": "For multichannel input, different channels make different contributions to a certain target. We involve a channel-wise attention mechanism to exploit the interchannel relationship and adaptively reweights them. It recently has been a hotspot in various researches such as image classification  [24] . By achieving high performance and robust to noisy inputs, the channel-wise attention mechanism is mobile and portable for different tasks. Referring to various channel-wise attention module, our SCA is adapted from the simplest structure of Squeeze-and-Excitation Block in  [24]  and replaces the convolution with 1D operation to fit signal inputs (see in Fig.  2(d) ). The multiplication operations of attention modules (seen in Fig.  2(c)(d) ) cause the unacceptable computational burden, especially with large feature maps. To improve the training efficiency, we consider to uniformly clip the long-temporal shallow feature to smaller groups in sequence dimension. The clipping method can reduce the length of each feature sequence, then reduce the computational complexity of multiplication for each forward propagation. In order not to involve more network parameters, we also didn't add the number of attention modules but use one module with shared parameters to recursively process the clipped parts. With sufficient GPU memory, this recursive process can also execute in parallel. The number of clipping parts is considerable because an inappropriate division leads to the large size for each clip, and the detailed division will cause the gradient and optimal issues for the SCA. Referring to  [14] , the EDA signal evoked from stimulations passes through a procedure of the latency, rise time, half recovery time. Although these intervals are not homogeneous in time, the physiological knowledge guides us to roughly divide the shallow feature to 3 parts in sequence dimension:\n\nwhere ğ¹ ğ‘†ğ¹ âˆˆ ğ‘… ğ¶Ã—ğ¿ and ğ¹ ğ‘†ğ¹ 1 , ğ¹ ğ‘†ğ¹ 2 , ğ¹ ğ‘†ğ¹ 3 âˆˆ ğ‘… ğ¶Ã—ğ¿/3 . We first aggregate the temporal information from the clipped feature with the average-pooling operation to get the temporal average-pooled features ğ¹ ğ¿/3 ğ‘ğ‘£ğ‘” . Then the descriptor is forwarded to a multilayer perceptron (MLP) with only one hidden layer. To reduce parameter overhead, the channel of average-pooled features through the hidden layer ğ‘Š 0 is decreased by the reduction ratio ğ‘Ÿ to ğ‘… ğ¶/ğ‘Ÿ Ã—ğ¿/3 , then recovered by the latter layer ğ‘Š 1 . The sigmoid activation function generates the normalized channel attention weight ğ‘Š ğ‘… âˆˆ ğ‘… ğ¶Ã—ğ¿/3 between 0 to 1. Finally, the original shallow feature is reweighted by the multiplication with the attention weight of the channel. Overall, the reweighted channel-wise feature is computed as\n\nwhere ğ‘– = 1, 2, 3 and\n\nis forwarded to the RNTA to mine temporal-range dependencies.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Residual Nonlocal Temporal Attention Module.",
      "text": "Nonlocal operation is first proposed as a neighborhood filtering algorithm in the image domain  [10] . It can involve the long-range position contribution to the filtered response of a certain position and control the contribution under the guidance of appearance similarity. At the advantage of capturing long-range relationships, the nonlocal neural network has been validated effective in video classification, object detection, instance segmentation, and keypoint detection  [59] . The great generality and portability motivates us to apply nonlocal operation in the recognition of psychological signals based emotions. However, as described in Section 3.2.1, the traditional attention module will multiply the computational complexity for large-size features. Therefore, the RNTA module performs a piecewise nonlocal operation. It takes the three parts of clipping features from the SCA as the inputs. Similar to the SCA, the RNTA is also trained recursively to reduce the model complexity.\n\nReferring to  [10, 59] , the generic nonlocal operation in deep neural networks can be defined as\n\nwhere ğ‘¥ represents the input feature map, ğ‘– is the target index of an output position in time sequence, and ğ‘— is the index of all possible locations which contribute to the filtered response xğ‘– In our module (seen in Fig.  2 (e)), we set 1D convolution operation and an average-pooling operation with kernel of size 1 to conduct linear embedding as\n\nFor pairwise function ğ‘“ (â€¢), there exists various formulations to the relationship between long-range indexes like Embedded Gaussian, Concatenation, or, etc. Wang ğ‘’ğ‘¡ ğ‘ğ‘™ .  [59]  has validated that the performance of nonlocal operation is insensitive to the instantiations of function ğ‘“ (â€¢) and ğ‘”(â€¢).\n\nHence, we choose the commonly used Embedded Gaussian function  [58]  and define ğ‘“ (â€¢) as\n\nwhere ğœƒ (â€¢) and ğœ™ (â€¢) are the convolutional layer. Adapted from  [59] , we set ğ‘ (ğ‘¥) = âˆ€ğ‘— ğ‘“ (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ), then 1 ğ‘ (ğ‘¥) ğ‘“ (ğ‘¥ ğ‘– , ğ‘¥ ğ‘— ) equals to the softmax operation. So, the nonlocal operation is computed as:\n\nFinally, we combine the nonlocal operation and the residual connection to get the output of attention module (ğ¹ ğ´ ğ‘– ):\n\nwhere ğ¹ ğ¶ğ´ ğ‘– is the clipped input from the SCA module, ğ¹ ğ‘ ğ¿ ğ‘– is calculated by Eq.(  12 ), ğ‘Š ğ‘¤ denotes the 1D convolution operation followed with the batch normalization and\n\nis the reweighted output of our attention module. (3) adding a signal channel attention layers at before the 1D convolution to mine the channel relationships between the deep features. Finally, the module extracts the EDA feature vector ğ¹ ğ¸ğ¹ from attention reweighted features ğ¹ ğ´ . Specifically, we found that in small-scale datasets, the SCAs of residual blocks will increase the fitting ability of RTCAG-1D and cause the overfitting drop-off (seen in Section 5.3.2). Therefore, we removed the SCA in RTCAG for DEAP and AMIGOS datasets.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Multimodal Fusion Classification.",
      "text": "Compared with image-or speech-recognition tasks, the complicated shapes of physiological curves show that physiological signal-based emotional recognition is a tough problem. It is not intuitive to perceive the relationship between the emotional state and the input curve. Moreover, personal specificity determines the intricate difference of nonemotional individual contexts between people  [32] , especially in datasets with large-scale subjects. Towards this end, we conduct multimodal fusion by adding some static benchmarks, which are accessible and have a great relationship with an emotional state. For the current multimodal emotion dataset, the media, such as music or video, is widely used to raise emotional states with affection involvement. The current largest PMEmo dataset used music that can be conveniently used as the prior benchmark. We simply applied the open-source toolkit openSMILE to extract coarse music features because the fine EDA features will give us more information about subjective experience. As Fig.  1  illustrates, the EDA feature vector and music feature vector are concatenated and forwarded to the multi-linear classification layers. The classifier consists of three linear layers, followed by a ReLU function. The output channels are 256, 128, and the class number, respectively. Finally, the output vector is normalized by the softmax function, and the maximum one is chosen as the predicted result.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset Analyses",
      "text": "As the performance of the classifier is relevant to the data distribution. In this section, we will briefly introduce and analyze the characteristics of three affective datasets, including DEAP  [34] , AMIGOS  [12] , and PMEmo  [63] , which can explain the experimental results in Section 5. PMEmo. The popular music dataset with emotional annotations (PMEmo) is our previous work  [63] . To the best of our knowledge, it is the current largest emotion recognition dataset with EDA and music signal. The chorus excerpts clipped from 794 pop songs are collected as the emotion elicitation. After the relaxing procedure, the subjects listened to music, and the EDA signals were  collected from subject's finger at a 50-HZ sampling rate. Each chorus was listened by at least 10 subjects. Meanwhile, the subject conducted dynamic V/A discrete annotation and static V/A discrete annotation for each chorus. Discarding some bad cases, 7962 pieces of EDA signals from 457 subjects were finally collected. In this work, we use static annotations to represent the emotional state and fuse the EDA and the music signal for the recognition of large-scale emotions.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Annotation Recreation",
      "text": "The annotations from the above datasets are ranged from 1 to 9. As mentioned before, we attempt to conduct 2-class recognition in DEAP, AMIGOS, and PMEmo for a fair comparison with previous works. Therefore, the annotations should be relabeled to the appropriate classes. Focusing on user-independent emotion recognition with large-scale subjects, we should pay more attention to personal specificity. Different emotion thresholds result in the same V/A score totally mapping to the opposite emotional state between different subjects. It should be noted that we involved the subject threshold just for generating proper subject-specific emotional classes. The user-independent model should not focus on a specific individual user but deal with greatly varying individual differences. But it does not mean that we can ignore the subject specificity when recreating the binary label from the original dispersed annotation. When mapping the 1-9 annotations to the binary classes, previous work  [17]  simply discretized the annotations into low (â‰¤5) and high (â‰¥5) V/A states. The reference  [54]  points out that since the subjective ratings also possess the nonstationarity and subject specificity, the fixed threshold may not be suitable for all individual preferences. If we attempt to validate the generalization of our work in large-scale data, personal threshold generating subject-specific emotional classes is essential. Specifically, inspired by the strategy in  [17]  for binary classification, we calculate the subject's emotion threshold with discrete annotations as the four-steps procedure. The example of one subject (Subject-ID: 10031 from PMEmo) is shown in Fig.  3 . The statistics of each class after label recreation is shown in Table  1 . It validates the relabeled processing will not cause imbalanced label distribution.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Annotation Correlation",
      "text": "Previous works point out the EDA signal is relevant for arousal classification  [53, 57] . However, the results reported by  [12, 18, 53]  show that the classification performance for the valence and arousal dimensions does not diverge considerably. Shukla et al.  [53]  reveals that the subject preferences of V/A annotation scores greatly influence the gap of V/A performance. In the specific dataset, the subject tends to annotate the valance score associated with arousal score, where the correlations of V/A scores are high. To systematically analyze the experiment results in Section 5.2, we should first analyze the annotation correlations in DEAP, AMIGOS, and PMEmo datasets. Following  [53] , we calculate the Spearman correlation coefficient ğ‘Ÿ between the valence and arousal annotations. The significant test of p-Value is also conducted. The statistics are listed in Table  2 . Compared with DEAP dataset (ğ‘Ÿ = 0.15), there are relatively higher correlations (ğ‘Ÿ = 0.56/0.52) between the annotated arousal and valence scores of AMIGOS and PMEmo with the strong p-value evidence (ğ‘ â‰¤ .001). It means the valance annotations in those two datasets have been influenced by arousal annotations.\n\nFor DEAP dataset, the subject has no preference to annotate the valence scores associated with arousal annotations. The characteristics of annotation can explain that our RTCAG with single EDA delivers the accurate valence recognition with AMIGOS and PMEmo datasets, and the performance of valence variable is much lower than arousal one in DEAP dataset. For the multimodel fusion of RTCAN-1D, the involvement of music overcomes the defect of EDA signals to achieve high performance in both the valence and the arousal dimensions.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Experiments",
      "text": "In this section we introduce the training settings briefly. Then, systemic multimodal analyses are conducted. Because the music and EDA signals from large-scale subjects are only available from PMEmo, the analyses with single music and multimodal inputs are conducted on PMEmo.\n\nThe DEAP  [34]  and AMIGOS  [12]  with fewer subjects are used for single EDA-based emotion recognition. Because of the individual specificity of large-scale subjects, we also conduct the incremental subject experiments with single EDA inputs on PMEmo to observe the overfitting of CNN models. Moreover, we carry out the ablation study to present the effects of different components of RTCAN-1D. The comparisons with previous solutions prove that our proposed multimodal framework can efficiently solve those problems. More details are presented as follows.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Training Settings",
      "text": "For fair comparisons with other existing methods, the weights of the whole RTCAN-1D in both DEAP, AMIGOS and PMEmo datasets were initialized from the Gaussian distribution ğ‘ (0, 0.01). We trained our model with cross-entropy loss and empirically selected the size of mini-batches as 256. The Stochastic Gradient Descent (SGD) was applied as the optimizer. Moreover, we set the initial learning rate to 0.001, which was decreased by 0.9 at every 15 epoches. The model suggested was assessed with 10-fold cross-validation. The dataset was divided into 10 disjoint subsets with subject-independent scenarios. At each fold, one subset was set to the validation data for fine-tuning hyperparameters, one subset was set to the test set, and the rest were used for training. Repeated 10 times, all the subsets were tested, and the average metrics of all the 10 folds were calculated as the results. The performance of binary classification is evaluated with average accuracy and F1-score (the harmonic mean of precision and recall).\n\nBefore fed into a deep neural network, the EDA and music signals will be pre-processed. Processed by z-score normalization and CvxEDA, the EDA signal was split into tonic, phasic components. The signal denoizing could be accomplished by convex optimization. Then, the z-score normalization was also conducted for three EDA and music signals. Moreover, we just used EDA signals after 3-second because annotators need some preliminary to evoke their emotion  [3] . For input size alignment, linear interpolation was also conducted to rescale EDA inputs.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Multimodal Analyses",
      "text": "",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Single Eda Recognition.",
      "text": "To prove the efficacy of the proposed multimodal structure, we should first prove that useful EDA characteristics can be extracted by our RTCAG. The single EDA recognition is conducted in three datasets, including DEAP, AMIGOS, and PMEmo. After being pre-processed, the composed 3-channel signals were fed into RTCAG, and the series liner layers classified the EDA feature vectors. The subjects are gradually increased from 10% to 40%, 70%, and 100%. It's clear that there is an significant drop-off from 10%to 40% subject percent and the decrease becomes gentle from 40% to 100%.\n\nThe results of our RTCAG in DEAP, AMIGOS, and PMEmo are listed on the 1st, 2nd, and 14th rows of Table  3 . It is clear that our model has accomplished accurate recognition in DEAP and AMIGOS, which proves the EDA feature extraction of RTCAG is excellent. Moreover, because of the annotation correlations, the results for V/A dimensions in AMIGOS and PMEmo are relatively close, and the performance in DEAP diverges considerably, which is aligned with the dataset characteristics in Section 4.3. However, the performance in the large-scale PMEmo decreases 10% with the increase in the number of participants, which intrigued us. Previous works of neural networks  [38, 39, 55]  reveal the DNN models, especially deeper models with large parameters, may suffer overfitting when the inputs are complicated from various different domains. Considering the various subject specificity, we assume the RTCAG may suffer overfitting.\n\nTo validate our assumption, we conduct an incremental subject experiment. The subjects from PMEmo are gradually increased from 10% to 40%, 70%, and 100%. The benefit of hand-crafted features is that they have been shown to be stable by a variety of studies  [53] . To systematically evaluate our model we add the traditional SVM classifier with time domain, frequency domain, and time-frequency domain features hand-crafted The 128 statistical features are calculated with the methods of  [53] . And we refer to  [12]  to set the regularization parameter ğ¶ of the linear SVM as 0.25. Because the RTCAG is well-designed for 3-channel EDA signals, we wonder if the simple CNN with EDA feature engineering may suffer less over-fitting. Hence, we add another model in which the 128 statistical features are fed into a 3 layers CNN. The basic convolution layer of simple CNN keeps the same with RTCAG. Keeping the same training settings, the results of three models are shown on the 3rd row to 14th row of Table  3  and illustrated in Fig  4 .\n\nAccording to the results with single EDA input, we can make some analyses: (1) for RTCAG, when the number of subjects (PMEmo-10%) is small, the recognition accuracies of RTCAG are competitive to DEAP and AMIGOS. When the involved subjects increase to hundreds, the recognition results decrease drastically. More individual specificity cannot help the model to converge but cause the RTCAG to overfit. (2) For 3 layers CNN with hand-crafted features, the model also suffers overfitting, but the decrease is not as sharp as RTCAG. It reveals that the EDA signals from hundreds of subjects are indeed complicated to cause overfitting even processed with feature engineering. On the other hand, the performance of 3 layers CNN is poorer than RTCAG, which means simpler architecture and fewer parameters limit the performance of CNN model. (3) As shown in Fig.  4 , the advantage of traditional SVM classifier with hand-crafted features is stable. When increasing the subjects, the SVM only shows a slight performance drop. However, it suffers underfitting and produces the worst recognition accuracy limited by the poor representation ability. (4) Although there is a slightly better performance for arousal, the recognition performance for the two variables in four subject scales does not diverge considerably, which is aligned with the high annotation correlation of PMEmo dataset as we have described in Section 4.3. According to the analyses, the defect of the single EDA input motivates us to include additional information through multimodal fusion.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Single Music",
      "text": "Recognition. This situation is common for user-independent emotion recognition: the same song listened by different people will stimulate the different emotional states, then get the opposite labels. The parameter optimization of deep learning model is strongly dependent on the loss between the predicted output and ground-truth label for backpropagation. When the same input data maps the total different ground-truth labels, the network optimization becomes very hard. If there exists many such data, the network training may even get corrupted. Therefore, for single music recognition, we directly used the result (SVM + Music) in our previous work  [62] . The results are presented in the 7th row of Table  3 . We can make some analyses: (1) compared with single EDA input, the performance with single music input get improved for valence recognition because the EDA is less relevant to the valance state  [53, 57]  but the music has a strong relationship with subject's valence/arousal moods  [41] . (2) For single music features, if considering subject specificity, the same music might lead to a completely opposite emotional state, which means the same music input maps to label 0 and 1 with different subject. (3) The single music input can reveal people's general emotion states at a certain extent. But it is not enough for user-independent emotion recognition. Although the SVM classifier with single music feature has achieved 70% accuracies, it has limited ability to recognize the cases which map to different emotion labels because of individual differences. Towards this, we add the music feature as the static emotion benchmark to help the convergence of our RTCAN-1D.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Fusing Music And Eda Recognition.",
      "text": "As shown in the last row of Table  3 , when fusing the music and EDA features, the proposed RTCAN-1D achieves the best performance. For single music recognition, the classifier could not recognize the individual specificity because the music feature had no relationships with a specific human. For single EDA recognition, the traditional classifier with feature engineering suffers underfitting, and our well-designed RTCAG model will get overfitting when large-scale subjects involve. The significant improvement of RTCAN-1D validates that the fusion of external static features from music and individual EDA features is a solution for large-scale emotional recognition.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Ablation Analyses",
      "text": "To avoid the less-effective experiment repetition, we carry out most of the ablation studies in PMEmo. For some experiments of fitting ability, since the network structure has been slightly changed for different datasets, we present the results in DEAP, AMIGOS and PMEmo datasets.   4 .\n\nIn practice, we find that the best performance is given by the simplest structure. By using a deeper convolutional layer and more internal channels, the performance of RTCAN-1D get drop-off, and the network can be easily overfitting, which is more obvious in smaller scale DEAP and AMIGOS datasets. In general, a deeper network leads to better representation performance at the expense of space and time cost. However, when the inner class features are not consistent, and the number of training data scale is not large enough, such as millions of images for image recognition  [24] , the complicated structure is harmful for EDA feature extraction. It is much clear in Table  4 . Therefore, we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block. Experimental results with different attention modules in three datasets are shown in Table  5 . As introduced in Section 5.2.1, the increase of subjects involved may increase the fitting ability of the model. In order to explore the appropriate structure of our network, we should analyze the results in the large-scale PMEmo dataset and two smaller scale DEAP and AMIGOS datasets, respectively.\n\nIn the large-scale PMEmo dataset, ğµğ‘ğ‘ ğ‘’ only contains the ResNet backbone and multi-linear layers to classify the fused EDA+music feature vector, hence it gets the poor accuracy in both three datasets. The higher recognition accuracies from ğ‘… ğ‘ and ğ‘… ğ‘ prove the effectiveness of the individual attention subnet, compared with ğµğ‘ğ‘ ğ‘’ model. When combining all the two modules, we can observe that the joint use of channel and temporal relationships ğ‘… ğ‘ produces further improvement. Reviewing the results in Table  5 , it should be emphasized that both SCA and RTNA are indispensable because all the sequential methods outperform the basic backbone when adding a Basic single attention submodule in the large-scale PMEmo dataset. In smaller scale DEAP and AMIGOS datasets, it can be seen that the simplest ğµğ‘ğ‘ ğ‘’ is not the worst model. When adding the SCA_Res modules, three configurations ğ‘… ğ‘ , ğ‘… ğ‘ , ğ‘… ğ‘“ get performance drop, the accuracies and F1-scores of which are even lower than ğµğ‘ğ‘ ğ‘’. The most complicated structure ğ‘… ğ‘“ produces the worst results. It means when we add SCA modules in each residual block, the fitting ability of RTCAG-1D gets increased and causes the overfitting drop-off. When we only add the SCA and RTNA to process the extracted shallow feature, the ğ‘… ğ‘’ model achieves the best results. Therefore, we remove the SCA modules of residual blocks to avoid over-fitting in small-scale datasets. The experiments in this subsection also reveal that when we design the DNN structure for user-independent emotion recognition, the scale of the subject involvement should be carefully considered.",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Attention Visualization.",
      "text": "Recently, researchers in neural network interpretation have proposed some methods for qualitative analysis. One of the most popular methods is visualizing the weight distribution in the regions of input to show which parts are more important for predictions. Gradientweighted Class Activation Mapping (Grad-CAM)  [51]  is an excellent visualization method that applies back gradient computing to reversely acquire the important weight of the input positions in convolutional layers. In the image domain, the output mask of Grad-CAM is overlaid on the semitransparent input image in the form of a heatmap with a warm-to-cool color spectrum. As warmer color is applied, the highlighted pixels contribute more to the final prediction, and the model will focus more on these messages. For another model, if the Grad-CAM mask covers more target regions in the same input, the network's receptive field of the is larger. And generally, it means the network has superior representation ability.\n\nTo show the ability of our attention submodules intuitively, we compare the visualization attention weights of the 3 trained model in the preceding experiment of Section 5.3.2: ğ‘… ğ‘ (channel attention model), ğ‘… ğ‘ (temporal attention model) and ğ‘… ğ‘ (integrated channel-temporal attention model). In order to fit the proposed framework, we adapt Grad-CAM and start the guided backpropagation from the output layer of each attention submodule in order to invert 1D attention weights in the 3-channel EDA input. Fig.  5  and Fig.  7  show the sequences of pre-processed input signal and output attention weights from Grad-CAM. The input of valence and arousal situation (Subject-ID: 100184, Music-ID: 1) are randomly selected. In order to illustrate the corresponding relationship more clearly, the curves the 3-channel signal curves and the attention weights histogram are shown in one chart. Moreover, attention weights are presented in the form of spectrograms in Fig.  6  and Fig.  8 . We can see that the attention weights of the channel attention module consider the relationships between different channels. The weight of attention of the temporal attention module focuses more on the temporal distribution. The attention weights of the integrated channel-temporal attention model selectively cover the regions better than other models because the corresponding spectrogram contains the most warm color, and black columns in the chart are densest in important regions. It is also clear that the integrated RTCAN-1D pays more attention to the sharp peaks in 3 curves. The quantitative experiment results in Section 5.3.2 and qualitative visualization comparisons in this subsection can show that the channel and temporal attention modules guide the proposed RTCAN-1D to improve the performance of the network.",
      "page_start": 17,
      "page_end": 19
    },
    {
      "section_name": "Comparison With Previous Methods",
      "text": "To validate our proposed network's superiority, we provide extensive comparisons in three emotion datasets. The single EDA signals from the small-scale DEAP and AMIGOS datasets are used to compare our RTCAG with SOTA EDA-based methods. And large-scale PMEmo with music and EDA signals is applied to prove the effectiveness of our multimodal fusion. Those SOTA models include both the hand-crafted feature-based conventional classifiers and deep learning-based frameworks.\n\nFor single EDA, previous works applied various models to classify the hand-craft features such as Support Vector Machine (SVM)  [12, 34] , Multi-Layer Perceptron (MLP)  [52] . The CNN-based methods  [40, 50, 62]  with auto feature extraction were also proposed. We notice that the EDA decomposition is gradually being used for emotion recognition task  [52] . And our previous work  [62]  has also combined CvxEDA  [20]  and ResNet  [23]  for emotion recognition. Recently, Machot et al.\n\n[1] trained a 3-layer SOTA CNN using a grid search technique to get 82% accuracy on the subjectindependent model. It should be noted that there is not much EDA-based emotional recognition in previous works. For example, there is no baseline use of EDA signals for valence and arousal classification in DEAP dataset. Hence, for a comprehensive evaluation, the comparisons are not limited to the EDA-based model. Other multimodal psychological signal-based methods (EEG,ECG) make use of traditional classifiers (ğ‘’.ğ‘”., SVM  [34] ) and deep learning model (ğ‘’.ğ‘”., DCNN  [40] ). Table  6  lists the comparisons between our model and previous studies. Our RTCAG outperforms existing EDA-based SOTA methods. It should be noted that for binary recognition in DEAP, the evaluation results of our model in valence dimension are a little lower than 3 layers CNN model  [1]  because they directly mapped the scales (1 âˆ¼ 9) into 2 levels. For example, the valence scale of 1 âˆ¼ 5 was mapped to \"negative\" and 6 âˆ¼ 9 to \"positive\", respectively. The arousal scale of 6 âˆ¼ 9 was mapped to \"passive\" and 6 âˆ¼ 9 to \"active\", respectively  [1] . The direct value annotation labeling of Machot et al.  [1]  simplified the recognition problem without considering the subject emotion threshold. Moreover, Alexnet-2D  [40]  has fused 7 physiological signals, i.e. EEG, EOG, EMG, GSR, TMP, BVP, RSP, it's natural that our RTCAG-1D only with EDA signals cannot outperform Alexnet-2D  [40]  but the competitive results can also prove the effectiveness of our model.\n\nFor multimodal solution, the better performance of mixed-method multiphysiological signals also indicates the multi-feature fusion as mixed inputs contain more affective messages than a single input if acquisition equipments are sufficient. However, considering the user-friendly applications for reliability and hard acquisition, our RTCAG does not involve more physiological signals but adds the music features. With the effective temporal and channel attention mechanism, the representation ability of our well-designed RTCAG has improved to mine more useful EDA features. Hence, the performance of RTCAN-1D gets a significant improvement than our previous Res-SIN  [62]  without changing music features. All the results in three datasets validate the superiority of our RTCAN-1D.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "The individual differences in the large-scale emotion recognition task is a tough problem, which causes the performance drop of user independent methods. The single input music or EDA signal cannot alleviate this problem. Towards this, we make an attempt with multimodal fusion. In this work we propose an end-to-end framework called the 1-dimensional residual temporal and channel attention network for user-independent emotional recognition. As the first attempt, our proposed RTCAN-1D separately uses the temporal-channel attention modules to mine the longrange temporal correlations and channel-wise relationships to reweight the distribution of the shallow features. Moreover, in large-scale PMEmo dataset, the fusion of emotion benchmarks from external music stimulation and the individual specificity from EDA also significantly improves the performance of our model. Extensive experiments in three popular datasets validate the effectiveness of our model. In future work, the fusion of external static features and traditional physiological signals will be an efficient way for large-scale affective computing. The attention mechanism can also further improve the ability of CNN's end-to-end frameworks. We believe the research of EDA-based emotion recognition will promote a lot of practical applications.",
      "page_start": 21,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of the proposed method. Music signals are only applied for PMEmo dataset,",
      "page": 3
    },
    {
      "caption": "Figure 2: Therefore, to involve less prior knowledge and explore the general solution for large-scale emotion",
      "page": 3
    },
    {
      "caption": "Figure 1: and Fig 2.",
      "page": 4
    },
    {
      "caption": "Figure 1: Let the vector ğ‘‹ğ‘‡=",
      "page": 6
    },
    {
      "caption": "Figure 2: The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow",
      "page": 7
    },
    {
      "caption": "Figure 2: (d)). The multiplication operations of attention modules (seen in Fig. 2(c)(d))",
      "page": 8
    },
    {
      "caption": "Figure 2: (e)), we set 1D convolution operation and an average-pooling",
      "page": 9
    },
    {
      "caption": "Figure 1: illustrates, the EDA feature vector and music feature vector are concatenated",
      "page": 10
    },
    {
      "caption": "Figure 3: Visualization of four-steps binary emotion label generation from one subject (ID:10031). (a) First, we",
      "page": 11
    },
    {
      "caption": "Figure 3: The statistics of each class after label recreation is shown in",
      "page": 12
    },
    {
      "caption": "Figure 4: The recognition results with subject incremental settings for arousal and valence dimensions in PMEmo",
      "page": 14
    },
    {
      "caption": "Figure 4: According to the results with single EDA input, we can make some analyses: (1) for RTCAG, when",
      "page": 14
    },
    {
      "caption": "Figure 1: and Fig. 2 illustrate, the residual feature extraction (RFE) is",
      "page": 15
    },
    {
      "caption": "Figure 2: (b) (noted as SCA)",
      "page": 16
    },
    {
      "caption": "Figure 2: (c) (noted as SCA_Res).",
      "page": 16
    },
    {
      "caption": "Figure 5: The visualization of the curves of three signals and the column of attention weight in valence recognition",
      "page": 18
    },
    {
      "caption": "Figure 6: The valence spectrogram visualization. The spectrograms in (a), (b), (c) separately correspond to the",
      "page": 18
    },
    {
      "caption": "Figure 5: model). In order to fit the proposed framework, we adapt Grad-CAM and start the guided back-",
      "page": 18
    },
    {
      "caption": "Figure 6: and Fig. 8. We can see that",
      "page": 18
    },
    {
      "caption": "Figure 7: The visualization of the curves of three signals and the column of attention weight in arousal recognition",
      "page": 19
    },
    {
      "caption": "Figure 8: The arousal spectrogram visualization. The spectrograms in (a), (b), (c) separately correspond to the",
      "page": 19
    },
    {
      "caption": "Figure 7: The quantitative experiment results in Section 5.3.2 and qualitative visualization comparisons",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion": "Recognition by Fusing Music and Electrodermal Activity"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Signals"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "GUANGHAO YIN, Zhejiang University, China"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "SHOUQIAN SUN, Zhejiang University, China"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "DIAN YU, Zhejiang University, China"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "DEJIAN LI, Zhejiang University, China"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "KEJUN ZHANGâˆ—, Zhejiang University and Alibaba-Zhejiang University Joint Research Institute of Frontier"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Technologies, China"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Considerable attention has been paid to physiological signal-based emotion recognition in the field of affective"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "computing. For reliability and user-friendly acquisition, electrodermal activity (EDA) has a great advantage"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "in practical applications. However, EDA-based emotion recognition with large-scale subjects is still a tough"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "problem. The traditional well-designed classifiers with hand-crafted features produce poorer results because"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "of their limited representation abilities. And the deep learning models with auto feature extraction suffer the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "overfitting drop-off because of large-scale individual differences. Since music has a strong correlation with"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "human emotion, static music can be involved as the external benchmark to constrain various dynamic EDA"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "signals. In this paper, we make an attempt by fusing the subjectâ€™s individual EDA features and the external"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "evoked music features. And we propose an end-to-end multimodal framework, the one-dimensional residual"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "temporal and channel attention network (RTCAN-1D). For EDA features, the channel-temporal attention"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "mechanism for EDA-based emotion recognition is first\ninvolved in mine the temporal and channel-wise"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "dynamic and steady features. The comparisons with single EDA-based SOTA models on DEAP and AMIGOS"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "datasets prove the effectiveness of RTCAN-1D to mine EDA features. For music features, we simply process"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "the music signal with the open-source toolkit openSMILE to obtain external feature vectors. We conducted"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "systematic and extensive evaluations. The experiments on the current largest music emotion dataset PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "validate that the fusion of EDA and music is a reliable and efficient solution for large-scale emotion recognition."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "â€¢ Human-centered computing â†’ HCI theory, concepts and models;\nâ€¢ Computing"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "methodologies â†’ Neural networks; â€¢ Applied computing â†’ Bioinformatics and Media."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Additional Key Words and Phrases: multimodal fusion, large-scale emotion recognition, attention mechanism"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "ACM Reference Format:"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Guanghao Yin, Shouqian Sun, Dian Yu, Dejian Li, and Kejun Zhang. 2018. A Multimodal Framework for"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals. J. ACM 37, 4, Article 111"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "(August 2018), 23 pages. https://doi.org/10.1145/1122445.1122456"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "âˆ—Corresponding author."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Authorsâ€™ addresses: Guanghao Yin, ygh_zju@zju.edu.cn, Zhejiang University, Zheda Road 38, Hangzhou, China, 43017-6221;"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Shouqian Sun, ssq@zju.edu.cn, Zhejiang University, Zheda Road 38, Hangzhou, China; Dian Yu, yudian329@zju.edu.cn,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Zhejiang University, Zheda Road 38, Hangzhou, China; Dejian Li, dejianli@zju.edu.cn, Zhejiang University, Zheda Road"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "38, Hangzhou, China; Kejun Zhang, zhangkejun@zju.edu.cn, Zhejiang University and Alibaba-Zhejiang University Joint"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Research Institute of Frontier Technologies, Zheda Road 38, Hangzhou, China."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "prior specific permission and/or a fee. Request permissions from permissions@acm.org."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "Â© 2018 Association for Computing Machinery."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "0004-5411/2018/8-ART111 $15.00"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion": "https://doi.org/10.1145/1122445.1122456"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:2\nYin and Zhang, et al.": "1\nINTRODUCTION"
        },
        {
          "111:2\nYin and Zhang, et al.": "The ability of emotion recognition is a significant hallmark of intelligent human-computer in-"
        },
        {
          "111:2\nYin and Zhang, et al.": "teraction (HCI). Empowering computers to discern human emotions better would make them"
        },
        {
          "111:2\nYin and Zhang, et al.": "perform the appropriate actions. In recent two decades, both the research groups and industries"
        },
        {
          "111:2\nYin and Zhang, et al.": "have aroused great interest that explores to enhance user experience with algorithms [45]. With"
        },
        {
          "111:2\nYin and Zhang, et al.": "the advancement of sensor-based Internet of Things (IoT) technology, emotion recognition is in"
        },
        {
          "111:2\nYin and Zhang, et al.": "unceasing demand by a wide range of applications such as mental healthcare [21], driving monitor"
        },
        {
          "111:2\nYin and Zhang, et al.": "[29], wearable devices [49], etc."
        },
        {
          "111:2\nYin and Zhang, et al.": "Human emotions are usually defined as a group of affective states that evoke stimuli from external"
        },
        {
          "111:2\nYin and Zhang, et al.": "environments or interpersonal events as a response [46]. According to behavioral researches,"
        },
        {
          "111:2\nYin and Zhang, et al.": "emotions are typically described as the set of discrete basic categories [15]. They can also be"
        },
        {
          "111:2\nYin and Zhang, et al.": "represented in the continuous space [22], which can quantitatively describe the abstract emotion"
        },
        {
          "111:2\nYin and Zhang, et al.": "state percentage, which provides another method for understanding the concrete representations"
        },
        {
          "111:2\nYin and Zhang, et al.": "of abstract emotions. The valence-arousal space is one of the common emotion discrete spaces,"
        },
        {
          "111:2\nYin and Zhang, et al.": "where the valence reflects the value of the positive to negative state and the arousal measures the"
        },
        {
          "111:2\nYin and Zhang, et al.": "active to calm degree [36]. For the advantage of conducting the Self-Assessment Manikins (SAM),"
        },
        {
          "111:2\nYin and Zhang, et al.": "most of the popular open-source emotion datasets applied the valence-arousal annotations, such as"
        },
        {
          "111:2\nYin and Zhang, et al.": "DEAP [34], AMIGOS [12], PMEmo [63], or etc."
        },
        {
          "111:2\nYin and Zhang, et al.": "In general, the data source of emotion recognition can be categorized into (1) peripheral physical"
        },
        {
          "111:2\nYin and Zhang, et al.": "signals such as facial expression [33], speech [16]; (2) physiological signals including electroen-"
        },
        {
          "111:2\nYin and Zhang, et al.": "cephalography (EEG), electrodermal activity (EDA), electrocardiogram (ECG), or etc. The first"
        },
        {
          "111:2\nYin and Zhang, et al.": "method has great advancement\nin data collection. However,\nit cannot guarantee reliability in"
        },
        {
          "111:2\nYin and Zhang, et al.": "recognition because human can consciously control body language to hide their personal emotion-"
        },
        {
          "111:2\nYin and Zhang, et al.": "like speech or facial expression. Using physiological signals can overcome this drawback. The"
        },
        {
          "111:2\nYin and Zhang, et al.": "physiological signals are reacted from the autonomic and somatic nervous systems (ANS and SNS)."
        },
        {
          "111:2\nYin and Zhang, et al.": "They are largely involuntarily activated and cannot be triggered by any conscious or intentional"
        },
        {
          "111:2\nYin and Zhang, et al.": "control [28]. Thus, the physiological signal-based method provides an avenue to recognize affect"
        },
        {
          "111:2\nYin and Zhang, et al.": "changes that are less obvious to perceive visually [28]. Compared with other physiological signals,"
        },
        {
          "111:2\nYin and Zhang, et al.": "the acquisition of EDA can be conveniently accomplished by intelligent wearable devices like"
        },
        {
          "111:2\nYin and Zhang, et al.": "smartwatches and wisebraves [4]. The research of EDA-based emotion recognition can be put faster"
        },
        {
          "111:2\nYin and Zhang, et al.": "into practical applications such as emotion monitor [43], music liking recommendation [11], or etc."
        },
        {
          "111:2\nYin and Zhang, et al.": "Hence, the EDA signal has great significance of scientific value and practicability."
        },
        {
          "111:2\nYin and Zhang, et al.": "To mine useful features from physiological signals, two broad methods exist: (1) the traditional"
        },
        {
          "111:2\nYin and Zhang, et al.": "well-designed methods with hand-crafted features, and (2) the deep learning models with auto"
        },
        {
          "111:2\nYin and Zhang, et al.": "feature extraction. According to previous studies, a wide range of statistical features have been"
        },
        {
          "111:2\nYin and Zhang, et al.": "explored to seek predictive power for the classification of emotions, including time domain, fre-"
        },
        {
          "111:2\nYin and Zhang, et al.": "quency domain, and time-frequency domain features [53]. However, the hand-crafted feature-based"
        },
        {
          "111:2\nYin and Zhang, et al.": "methods suffer limitations in the following aspects. First, the traditional methods such as SCL have"
        },
        {
          "111:2\nYin and Zhang, et al.": "limited representation ability to extract the complicated EDA features from large-scale subjects."
        },
        {
          "111:2\nYin and Zhang, et al.": "Second, the design of hand-crafted feature strongly depends on prior knowledge of statistics and"
        },
        {
          "111:2\nYin and Zhang, et al.": "physiology in the small-scale data. Researchers should take more and more effective hand-crafted"
        },
        {
          "111:2\nYin and Zhang, et al.": "features into account for better performance.\nIt is a great challenge for their feature selection"
        },
        {
          "111:2\nYin and Zhang, et al.": "method. Although, Shukla [53] has provided a systematic exploration of\nthe feature selection"
        },
        {
          "111:2\nYin and Zhang, et al.": "method,\nit cannot guarantee the robustness with large-scale complicated physiological signals."
        },
        {
          "111:2\nYin and Zhang, et al.": "Recently, the effectiveness of deep learning model has been proven in tackling emotion recognition"
        },
        {
          "111:2\nYin and Zhang, et al.": "[1]. Moreover, the end-to-end DNN model can directly learn discriminative features from data."
        },
        {
          "111:2\nYin and Zhang, et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "Origin"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "Input EDA signal"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "CvxEDA \ndecomposation\nRTCAG\nTonic"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "Linear,256,ReLU\nLinear,128,ReLU\nLinear,class_num,\nPhasic"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "Opensmile \nFeature extraction\nInput Music signal"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "Fig. 1. The overall architecture of the proposed method. Music signals are only applied for PMEmo dataset,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "the music-evoked large-scale EDA dataset with hundreds of subjects. For the other three datasets, the model"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "is only fed with single EDA input. For a fair comparison with previous works, the class number is set to 2 in"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "DEAP, AMIGOS, and PMEmo. The structure of residual temporal and channel attention group (RTCAG) is"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "illustrated in Fig. 2."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "Therefore, to involve less prior knowledge and explore the general solution for large-scale emotion"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "recognition, we choose the end-to-end deep learning model."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "physiological signals with large-scale subjects is a challenging problem, because more subjects"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "involve more complicated individual specificity. For emotion recognition with one subject, previous"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "user-dependent system [32] has achieved more than 90% accuracy. However, when extending"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "the subject from one to group, the accuracy of user-independent models decreased greatly [28]"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "because of the individual specificity. Moreover, our experimental results in Section 5.2.1 have further"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "supported this. Specifically, we use various hand-crafted classifier and CNN-based models to conduct"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "incremental subject classification. In relatively small-scale datasets with dozens of subjects, such as"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "DEAP (32 subjects), AMIGOS (40 subjects), and 1/10 PMEmo datasets (46 subjects), the models can"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "build accurate relationships between EDA and emotion central states. However, when gradually"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "increasing the subject number of PMEmo dataset to the maximum 457, the representation power"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "of traditional classifiers with hand-crafted features is too weak to produce poor results, and the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "CNN-based deep learning model suffers overfitting. Since all those two mainstream methods have"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "obvious defections, mining the single EDA single cannot further improve the performance for"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "large-scale emotion recognition."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "has the great advantage to enrich the complementary information from different modalities [5]. It"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "has a broad range of successes, including multi-sensor management [61], and human-computer"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "interaction [27]. In the scene of psychological rehabilitation [37], or emotion tracing [6], music"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "is commonly utilized medium because it is effective to modulate subjectâ€™ s mood [41]. In PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "dataset, music is what the authors use to induce emotions among participants. The tempo, beats,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "and other acoustic features regarding the music being played will correlate with the participantsâ€™"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "observed emotions. Hence, we choose the music signal as the external constraint for supervised"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "mance of RTCAN-1D and also overcome the defect that the EDA signal"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "classification [53, 57]."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "attention network for emotion recognition in the valence-arousal dimension. We directly apply a"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "1D residual block to build the EDA residual feature extraction (RFE). To provide sufficient useful"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": "messages from 1-channel EDA, we apply the convex optimization-based EDA method [20] to"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:4": "",
          "Yin and Zhang, et al.": "decompose EDA into phasic, tonic components and feed the 3 channel mixed signals (origin, phasic,"
        },
        {
          "111:4": "and tonic)",
          "Yin and Zhang, et al.": "into RTCAN-1D. Then, we involve the attention mechanism:\n(1) the signal channel"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "attention module (SCA) to reweight the features of 3-mixed signal channels and (2) the residual"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "nonlocal temporal attention module (RNTA) to explore the temporal correlations of EDA features."
        },
        {
          "111:4": "To the best of our knowledge, our model is the first attempt to involve the attention mechanism to",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "process physiological signals. The EDA decomposition, two attention blocks, and residual feature"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "extraction form the residual temporal and channel attention group (RTCAG) to get the EDA feature"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "vectors. The external music features are simply extracted with the open-source toolkit openSMILE."
        },
        {
          "111:4": "Finally,",
          "Yin and Zhang, et al.": "the classifying layers fuse individual emotion features from EDA signals and external"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "emotion benchmarks from music to predict the emotional state. The overall structure of our model"
        },
        {
          "111:4": "is illustrated in Fig 1 and Fig 2.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "We conduct extensive evaluations and compare our model with previous researches. Because"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "the music and EDA signals from large-scale subjects are only available from the current largest"
        },
        {
          "111:4": "PMEmo dataset,",
          "Yin and Zhang, et al.": "the multimodal\nfusion of RTCAN-1D is evaluated on PMEmo. Moreover, we"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "prove the effectiveness of RTCAG with single EDA input from the small-scale DEAP and AMIGOS"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "datasets. For a fair comparison with the SOTA methods, our proposed model was applied to the"
        },
        {
          "111:4": "subject-independent binary classification of valence and arousal.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "In summary, the contributions of our paper are represented as follows:"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "â€¢ An end-to-end multimodal framework RTCAN-1D is proposed for user-independent emotion"
        },
        {
          "111:4": "recognition.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "â€¢ The effectiveness of the attention mechanism is first proved to capture the channel-wise"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "correlations and long-distance temporal information of EDA signals."
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "â€¢ The fusion of individual specificity from EDA signals and external static benchmarks from"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "music provides an efficient solution for large-scale emotion recognition."
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "Our paper is organized as follows. Section 2 introduces literature and reviews related works. In"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "Section 3, our proposed model is explained in detail. Experimental validations and result analysis"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "are described in Section 4, along with the details of implementation. The slights and discussions are"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "expressed in Section 5. Finally, we conclude this paper in Section 6 by summarizing the attributions"
        },
        {
          "111:4": "",
          "Yin and Zhang, et al.": "and future prospects. Our codes have been released at https://github.com/guanghaoyin/RTCAN-1D."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "or calm degree [14]. Now, the V/A model becomes one of the most frequently employed model"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "because it can be easily assessed using the Self-Assessment Manikins (SAM). The three affective"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "Multimodal Fusion for Emotion Recognition"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "Multimodal information fusion technique is used to improve the performance of the system by"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "information on the feature level,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "improve the robustness and performance of emotion recognition, previous researchers fuse the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "features from different physiological signals [28, 32], including ECG, RSP, EMG, EDA, and EEG"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "because the fused signals contain more implicit information with the different sampling rate and"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "more channels. But collecting multi-signals requires many conductive electrodes placed along"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "the subjectâ€™s scalp. The user-unfriendly acquisition limits practical applications. Therefore, some"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "et al. [31] systematically present the robust emotion recognition fusing the speech and biosignals."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "Liu et la. [42] combined the evoked video and EEG signals to predict the subjectâ€™s emotional state."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "Koelstra et la. [35] combined the facial expression with EEG signals. And Thammasan et la. [56]"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "fused the EEG and musical features. There is no effective exploration for the fusion of EDA and"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "music signals, especially with large-scale subjects. Hence, our paper focuses on this issue."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "EDA Processing"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "The EDA signal is widely used in the psychology or physiology field. The approaches to process"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "EDA signal include Bayesian Statistics [53], dictionary learning [30], and decomposition [2]. We"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "focus on the end-to-end general solution. Hence, complex processing is inappropriate. Therefore,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "(SC), which is composed of tonic, phasic components [9]. The tonic phenomena (t) reflect slow drifts"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "of the baseline level and spontaneous fluctuations in SC. The phasic phenomena (r) is generally a"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "short-time rapid fluctuation and reflects the response to external stimulation [9]. For realistic tasks,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "There have been several mathematical solutions associated with a stimulus for EDA decomposi-"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "tion in recent decades. Referring to Alexander ğ‘’ğ‘¡ ğ‘ğ‘™ . [2], the well-known assumption is that the SC"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "equals to the convolution between discrete bursting episodes of sudomotor nerve activity (SMNA)"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "ğ‘†ğ¶ = ğ‘†ğ‘€ğ‘ ğ´ âˆ— ğ¼ğ‘…ğ¹ ."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "There exists some methods for EDA decomposition. Discrete deconvolution analysis (DDA) [8]"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "claims the nonnegativity of the driver and maximal compactness of the impulses to decompose EDA"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "through nonnegative deconvolution. Continuous decomposition analysis (CDA) [7] improves DDA"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "Based EDA Method (CvxEDA) [20] models the phasic, tonic, noise components and applies convex"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "optimization to solve the maximum a posteriori (MAP) problem. CvxEDA provides a novel method"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "to decompose EDA without preprocessing steps and heuristic solutions [19]."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "Attention Mechanism"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "In human proprioceptive systems, attention generally provides a guide to focus on the most"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "informative components of an input [24]. Recently, attention models improve the performance"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "of deep neural networks for various tasks, ranging from video classification [59] to language"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "translation [58]. Pei ğ‘’ğ‘¡ ğ‘ğ‘™ . [44] proposed a temporal attention module to detect salient parts of"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:5": "the sequence while ignoring irrelevant ones in sequence classification. Wang ğ‘’ğ‘¡ ğ‘ğ‘™ . [59] proposed"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:6\nYin and Zhang, et al.": "a portable nonlocal module for spatial attention to capture long-range dependencies in video"
        },
        {
          "111:6\nYin and Zhang, et al.": "classification. Hu ğ‘’ğ‘¡ ğ‘ğ‘™ . [24] proposed SENet to incorporate channel-wise relationships to achieve"
        },
        {
          "111:6\nYin and Zhang, et al.": "significant improvement for image classification."
        },
        {
          "111:6\nYin and Zhang, et al.": "To the best of\nthe authorsâ€™ knowledge,\nthe attention mechanism has not been proposed in"
        },
        {
          "111:6\nYin and Zhang, et al.": "physiological-based emotion recognition. As the mixed 3-channel input is sequential, each channel"
        },
        {
          "111:6\nYin and Zhang, et al.": "may contribute differently to the final prediction in different temporal range. In this work, we"
        },
        {
          "111:6\nYin and Zhang, et al.": "use an efficient channel-wise and temporal attention mechanism based on 3-channel mixed EDA"
        },
        {
          "111:6\nYin and Zhang, et al.": "characteristics."
        },
        {
          "111:6\nYin and Zhang, et al.": "3\nPROPOSED METHOD"
        },
        {
          "111:6\nYin and Zhang, et al.": "3.1\nNetwork Framework"
        },
        {
          "111:6\nYin and Zhang, et al.": "The great success of CNN frameworks depends on 2D convolution in signal processing and im-"
        },
        {
          "111:6\nYin and Zhang, et al.": "age domain [23]. Our previous work [62] in EDA-based emotion recognition with the PMEmo"
        },
        {
          "111:6\nYin and Zhang, et al.": "dataset [63] also conducted 2D convolution. However, it is a detour to transform the 1D signal to"
        },
        {
          "111:6\nYin and Zhang, et al.": "2D matrix just for fine-tuning pre-trained 2D CNN backbones. Moreover, if using the 2D convolu-"
        },
        {
          "111:6\nYin and Zhang, et al.": "tion, the pre-precessing should be conducted to rearrange the 1D EDA sequences and transform"
        },
        {
          "111:6\nYin and Zhang, et al.": "them to 2D matrices as our previous work did [62], which is time-consuming and increases the"
        },
        {
          "111:6\nYin and Zhang, et al.": "computations. Accordingly, we directly use the 1D residual CNN framework to process EDA signals"
        },
        {
          "111:6\nYin and Zhang, et al.": "in this work."
        },
        {
          "111:6\nYin and Zhang, et al.": "The overall architecture of the proposed method is illustrated in Fig. 1. Let the vector ğ‘‹ğ‘‡ ="
        },
        {
          "111:6\nYin and Zhang, et al.": "{ğ‘¥1, ğ‘¥2, ..., ğ‘¥ğ‘¡ } be the input EDA sequence with ğ‘¡ points. ğ‘Œğ‘ğ‘™ğ‘ğ‘ ğ‘  = {ğ‘¦1, ..., ğ‘¦ğ‘ } denotes the ground"
        },
        {
          "111:6\nYin and Zhang, et al.": "truth for ğ‘-classes task. If the emotion state belongs to the ğ‘–ğ‘¡â„ class, ğ‘¦ğ‘– = 1 and ğ‘¦ ğ‘— = 0 for ğ‘— â‰  ğ‘–. Our"
        },
        {
          "111:6\nYin and Zhang, et al.": "goal is to establish the relationships between input EDA and emotion state, specifically decreasing"
        },
        {
          "111:6\nYin and Zhang, et al.": "the distance between ground truth label ğ‘Œğ‘ğ‘™ğ‘ğ‘ ğ‘  and predicted result\n^ğ‘Œğ‘ğ‘™ğ‘ğ‘ ğ‘  ."
        },
        {
          "111:6\nYin and Zhang, et al.": "First, after z-score normalization, we conduct CvxEDA decomposition. The normalized EDA"
        },
        {
          "111:6\nYin and Zhang, et al.": "signal (y) is composed of three N-long column vectors: the phasic (r) and tonic (t) signal plus the"
        },
        {
          "111:6\nYin and Zhang, et al.": "noise component (ğœ€):"
        },
        {
          "111:6\nYin and Zhang, et al.": "(2)\nğ‘¦ = ğ‘€ğ´âˆ’1ğ‘ + ğµğœ† + ğ¶ğ‘‘ + ğœ€,"
        },
        {
          "111:6\nYin and Zhang, et al.": "where ğ‘Ÿ = ğ‘€ğ´âˆ’1ğ‘ and ğ‘¡ = ğµğœ† + ğ¶ğ‘‘. Then, with the transcendental knowledge of physiology"
        },
        {
          "111:6\nYin and Zhang, et al.": "and taking logarithmic transtorm, the MAP problem can be rewritten as a standard Quadratic-"
        },
        {
          "111:6\nYin and Zhang, et al.": "Programming (QP) convex form and can be solved efficiently using many available solvers (see"
        },
        {
          "111:6\nYin and Zhang, et al.": "more details in [20]). For denoizing in the process of solving optimization problem, we discarded"
        },
        {
          "111:6\nYin and Zhang, et al.": "the prior probability of noise term. After CvxEDA decomposition, the 1-channel EDA signal\nis"
        },
        {
          "111:6\nYin and Zhang, et al.": "expanded to mixed 3-channel signal ğ‘‹ğ‘‡ğ¶ = {(ğ‘¥11, ğ‘¥12, ğ‘¥13), ..., (ğ‘¥ğ‘¡ 1, ğ‘¥ğ‘¡ 2, ğ‘¥ğ‘¡ 3)}. As shown in Fig.2, in"
        },
        {
          "111:6\nYin and Zhang, et al.": "order to reduce the computational complexity, we uniformly clip shallow feature to 3 parts and"
        },
        {
          "111:6\nYin and Zhang, et al.": "the clipped signals are subsequently fed into the residual temporal and channel attention group"
        },
        {
          "111:6\nYin and Zhang, et al.": "(RTCAG) with shared attention weights. The EDA feature vector is extracted by three parts: shallow"
        },
        {
          "111:6\nYin and Zhang, et al.": "feature extraction, attention module, and residual feature extraction."
        },
        {
          "111:6\nYin and Zhang, et al.": "One convolutional layer extracts shallow feature ğ¹ğ‘†ğ¹ as"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Downsample,64\ns = 3,ResCA1D_Unit,64": "",
          "Downsample,64": "Downsample,64"
        },
        {
          "Downsample,64\ns = 3,ResCA1D_Unit,64": "",
          "Downsample,64": "s = 1,conv1d,64"
        },
        {
          "Downsample,64\ns = 3,ResCA1D_Unit,64": "",
          "Downsample,64": ""
        },
        {
          "Downsample,64\ns = 3,ResCA1D_Unit,64": "",
          "Downsample,64": "BatchNorm1d"
        },
        {
          "Downsample,64\ns = 3,ResCA1D_Unit,64": "",
          "Downsample,64": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "feature extraction, (b) attention module, and (c) residual feature extraction (RFE). Specifically, the attention"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "module contains (d) signal channel attention module (SCA) and (e) residual nonlocal temporal attention"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "module (RNTA). It should be emphasized that the three blocks of The SCA and RTNA are the same one,"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "respectively, which is recurrently used to reduce the modulesâ€™ parameters."
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "where ğ¹ğ´ is rearranged feature and ğ»ğ´ (Â·) is the attention module respectively. To mine discrim-"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": ""
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "bone [23] to output a EDA feature vector (ğ¹ğ¸ğ¹ ):"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "ğ¹ğ¸ğ¹ = ğ»ğ‘…ğ¹ (ğ¹ğ´)."
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "It should be emphasized that there exists other excellent CNN architectures such as VGG [54],"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "or DenseNet [25] to mine deep features, but a comparison of different CNN frameworks is not"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "the focus of our research. To be specific, for dozens of people involved EDA dataset, the subjectsâ€™"
        },
        {
          "Fig. 2. The structure of the residual temporal and channel attention group (RTCAG) consists of (a) shallow": "specificity is not so complicated that the RTCAN-1D can handle the single EDA input, which means"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:8\nYin and Zhang, et al.": "is to adaptively reweight the shallow feature for more useful messages in the channel and temporal"
        },
        {
          "111:8\nYin and Zhang, et al.": "dimensions. Hence, the SCA and RNTA are plugged before RFE."
        },
        {
          "111:8\nYin and Zhang, et al.": "3.2.1\nSignal Channel Attention Module. For multichannel input, different channels make different"
        },
        {
          "111:8\nYin and Zhang, et al.": "contributions to a certain target. We involve a channel-wise attention mechanism to exploit the"
        },
        {
          "111:8\nYin and Zhang, et al.": "interchannel relationship and adaptively reweights them. It recently has been a hotspot in various"
        },
        {
          "111:8\nYin and Zhang, et al.": "researches such as image classification [24]. By achieving high performance and robust to noisy"
        },
        {
          "111:8\nYin and Zhang, et al.": "inputs, the channel-wise attention mechanism is mobile and portable for different tasks. Referring"
        },
        {
          "111:8\nYin and Zhang, et al.": "to various channel-wise attention module, our SCA is adapted from the simplest structure of"
        },
        {
          "111:8\nYin and Zhang, et al.": "Squeeze-and-Excitation Block in [24] and replaces the convolution with 1D operation to fit signal"
        },
        {
          "111:8\nYin and Zhang, et al.": "inputs (see in Fig. 2(d)). The multiplication operations of attention modules (seen in Fig. 2(c)(d))"
        },
        {
          "111:8\nYin and Zhang, et al.": "cause the unacceptable computational burden, especially with large feature maps. To improve the"
        },
        {
          "111:8\nYin and Zhang, et al.": "training efficiency, we consider to uniformly clip the long-temporal shallow feature to smaller"
        },
        {
          "111:8\nYin and Zhang, et al.": "groups in sequence dimension. The clipping method can reduce the length of each feature sequence,"
        },
        {
          "111:8\nYin and Zhang, et al.": "then reduce the computational complexity of multiplication for each forward propagation. In order"
        },
        {
          "111:8\nYin and Zhang, et al.": "not to involve more network parameters, we also didnâ€™t add the number of attention modules but"
        },
        {
          "111:8\nYin and Zhang, et al.": "use one module with shared parameters to recursively process the clipped parts. With sufficient"
        },
        {
          "111:8\nYin and Zhang, et al.": "GPU memory, this recursive process can also execute in parallel. The number of clipping parts is"
        },
        {
          "111:8\nYin and Zhang, et al.": "considerable because an inappropriate division leads to the large size for each clip, and the detailed"
        },
        {
          "111:8\nYin and Zhang, et al.": "division will cause the gradient and optimal issues for the SCA. Referring to [14], the EDA signal"
        },
        {
          "111:8\nYin and Zhang, et al.": "evoked from stimulations passes through a procedure of the latency, rise time, half recovery time."
        },
        {
          "111:8\nYin and Zhang, et al.": "Although these intervals are not homogeneous in time, the physiological knowledge guides us"
        },
        {
          "111:8\nYin and Zhang, et al.": "to roughly divide the shallow feature to 3 parts in sequence dimension: ğ¹ğ‘†ğ¹ = [ğ¹ğ‘†ğ¹1\n, ğ¹ğ‘†ğ¹2\n, ğ¹ğ‘†ğ¹3 ],"
        },
        {
          "111:8\nYin and Zhang, et al.": "where ğ¹ğ‘†ğ¹ âˆˆ ğ‘…ğ¶Ã—ğ¿ and ğ¹ğ‘†ğ¹1\n, ğ¹ğ‘†ğ¹2\n, ğ¹ğ‘†ğ¹3 âˆˆ ğ‘…ğ¶Ã—ğ¿/3."
        },
        {
          "111:8\nYin and Zhang, et al.": "We first aggregate the temporal information from the clipped feature with the average-pooling"
        },
        {
          "111:8\nYin and Zhang, et al.": "operation to get the temporal average-pooled features ğ¹ ğ¿/3\nğ‘ğ‘£ğ‘” . Then the descriptor is forwarded to"
        },
        {
          "111:8\nYin and Zhang, et al.": "a multilayer perceptron (MLP) with only one hidden layer. To reduce parameter overhead, the"
        },
        {
          "111:8\nYin and Zhang, et al.": "channel of average-pooled features through the hidden layer ğ‘Š0 is decreased by the reduction ratio"
        },
        {
          "111:8\nYin and Zhang, et al.": "ğ‘Ÿ\nto ğ‘…ğ¶/ğ‘Ÿ Ã—ğ¿/3, then recovered by the latter layer ğ‘Š1. The sigmoid activation function generates the"
        },
        {
          "111:8\nYin and Zhang, et al.": "normalized channel attention weight ğ‘Šğ‘… âˆˆ ğ‘…ğ¶Ã—ğ¿/3 between 0 to 1. Finally, the original shallow"
        },
        {
          "111:8\nYin and Zhang, et al.": "feature is reweighted by the multiplication with the attention weight of the channel. Overall, the"
        },
        {
          "111:8\nYin and Zhang, et al.": "reweighted channel-wise feature is computed as"
        },
        {
          "111:8\nYin and Zhang, et al.": "(8)\nğ‘ğ‘£ğ‘Ÿ ))) âˆ— ğ¹ğ‘†ğ¹ ,\nğ¹ğ¶ğ´ğ‘– = ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ (ğ‘Š1 (ğ‘Š0 (ğ¹ ğ¿/3"
        },
        {
          "111:8\nYin and Zhang, et al.": "where ğ‘– = 1, 2, 3 and ğ¹ğ¶ğ´ = [ğ¹ğ¶ğ´1\n, ğ¹ğ¶ğ´2\n, ğ¹ğ¶ğ´3] is forwarded to the RNTA to mine temporal-range"
        },
        {
          "111:8\nYin and Zhang, et al.": "dependencies."
        },
        {
          "111:8\nYin and Zhang, et al.": "3.2.2\nResidual Nonlocal Temporal Attention Module. Nonlocal operation is first proposed as a"
        },
        {
          "111:8\nYin and Zhang, et al.": "neighborhood filtering algorithm in the image domain [10]. It can involve the long-range position"
        },
        {
          "111:8\nYin and Zhang, et al.": "contribution to the filtered response of a certain position and control\nthe contribution under"
        },
        {
          "111:8\nYin and Zhang, et al.": "the guidance of appearance similarity. At the advantage of capturing long-range relationships,"
        },
        {
          "111:8\nYin and Zhang, et al.": "the nonlocal neural network has been validated effective in video classification, object detection,"
        },
        {
          "111:8\nYin and Zhang, et al.": "instance segmentation, and keypoint detection [59]. The great generality and portability motivates"
        },
        {
          "111:8\nYin and Zhang, et al.": "us to apply nonlocal operation in the recognition of psychological signals based emotions."
        },
        {
          "111:8\nYin and Zhang, et al.": "However, as described in Section 3.2.1, the traditional attention module will multiply the com-"
        },
        {
          "111:8\nYin and Zhang, et al.": "putational complexity for large-size features. Therefore, the RNTA module performs a piecewise"
        },
        {
          "111:8\nYin and Zhang, et al.": "nonlocal operation. It takes the three parts of clipping features from the SCA as the inputs. Similar"
        },
        {
          "111:8\nYin and Zhang, et al.": "to the SCA, the RNTA is also trained recursively to reduce the model complexity."
        },
        {
          "111:8\nYin and Zhang, et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "Referring to [10, 59], the generic nonlocal operation in deep neural networks can be defined as"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "1"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "(9)\nâˆ‘ï¸ ğ‘“ (ğ‘¥ğ‘–, ğ‘¥ ğ‘— )ğ‘”(ğ‘¥ ğ‘— ),\nğ‘¥ğ‘– ="
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "ğ‘ (ğ‘¥)"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "where ğ‘¥ represents the input feature map, ğ‘– is the target index of an output position in time sequence,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "and ğ‘—\nis the index of all possible locations which contribute to the filtered response ^ğ‘¥ğ‘– . The output"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "ğ‘¥ğ‘– has the same size as input ğ‘¥. Specifically, the function ğ‘”(Â·) changes the representation of the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "input feature. The function ğ‘“ (Â·) computes the scale between ğ‘– and all\nğ‘—. The function ğ‘ (Â·) is the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "normalization function before the output. The formulations of functions ğ‘ (Â·), ğ‘”(Â·) and ğ‘“ (Â·) depend"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "on the specific network structure."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "In our module (seen in Fig. 2(e)), we set 1D convolution operation and an average-pooling"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "operation with kernel of size 1 to conduct linear embedding as"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "(10)\nğ‘”(ğ‘¥ ğ‘— ) = ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™ (ğ‘Šğ‘”ğ‘¥ ğ‘— )."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "For pairwise function ğ‘“ (Â·), there exists various formulations to the relationship between long-range"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "indexes like Embedded Gaussian, Concatenation, or, etc. Wang ğ‘’ğ‘¡ ğ‘ğ‘™ . [59] has validated that the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "performance of nonlocal operation is insensitive to the instantiations of function ğ‘“ (Â·) and ğ‘”(Â·)."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "Hence, we choose the commonly used Embedded Gaussian function [58] and define ğ‘“ (Â·) as"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:9": "(11)\nğ‘“ (ğ‘¥ğ‘–, ğ‘¥ ğ‘— ) = ğ‘’ğœƒ (ğ‘¥ğ‘– )ğ‘‡ ğœ™ (ğ‘¥ ğ‘— ),"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:10\nYin and Zhang, et al.": "3.2.4\nMultimodal Fusion Classification. Compared with image- or speech-recognition tasks, the"
        },
        {
          "111:10\nYin and Zhang, et al.": "complicated shapes of physiological curves show that physiological signal-based emotional recog-"
        },
        {
          "111:10\nYin and Zhang, et al.": "nition is a tough problem. It is not intuitive to perceive the relationship between the emotional"
        },
        {
          "111:10\nYin and Zhang, et al.": "state and the input curve. Moreover, personal specificity determines the intricate difference of"
        },
        {
          "111:10\nYin and Zhang, et al.": "nonemotional individual contexts between people [32], especially in datasets with large-scale sub-"
        },
        {
          "111:10\nYin and Zhang, et al.": "jects. Towards this end, we conduct multimodal fusion by adding some static benchmarks, which"
        },
        {
          "111:10\nYin and Zhang, et al.": "are accessible and have a great relationship with an emotional state. For the current multimodal"
        },
        {
          "111:10\nYin and Zhang, et al.": "emotion dataset, the media, such as music or video, is widely used to raise emotional states with"
        },
        {
          "111:10\nYin and Zhang, et al.": "affection involvement. The current largest PMEmo dataset used music that can be conveniently"
        },
        {
          "111:10\nYin and Zhang, et al.": "used as the prior benchmark. We simply applied the open-source toolkit openSMILE to extract"
        },
        {
          "111:10\nYin and Zhang, et al.": "coarse music features because the fine EDA features will give us more information about subjective"
        },
        {
          "111:10\nYin and Zhang, et al.": "experience. As Fig. 1 illustrates, the EDA feature vector and music feature vector are concatenated"
        },
        {
          "111:10\nYin and Zhang, et al.": "and forwarded to the multi-linear classification layers. The classifier consists of three linear layers,"
        },
        {
          "111:10\nYin and Zhang, et al.": "followed by a ReLU function. The output channels are 256, 128, and the class number, respectively."
        },
        {
          "111:10\nYin and Zhang, et al.": "Finally, the output vector is normalized by the softmax function, and the maximum one is chosen"
        },
        {
          "111:10\nYin and Zhang, et al.": "as the predicted result."
        },
        {
          "111:10\nYin and Zhang, et al.": "4\nDATASET ANALYSES"
        },
        {
          "111:10\nYin and Zhang, et al.": "As the performance of the classifier is relevant to the data distribution. In this section, we will"
        },
        {
          "111:10\nYin and Zhang, et al.": "briefly introduce and analyze the characteristics of three affective datasets, including DEAP [34],"
        },
        {
          "111:10\nYin and Zhang, et al.": "AMIGOS [12], and PMEmo [63], which can explain the experimental results in Section 5.2. It should"
        },
        {
          "111:10\nYin and Zhang, et al.": "be emphasized that the music signals are only available on PMEmo. The DEAP and AMIGOS are"
        },
        {
          "111:10\nYin and Zhang, et al.": "mainly used to evaluate the RTCAG for EDA feature extraction, and we compare RTCAG with the"
        },
        {
          "111:10\nYin and Zhang, et al.": "existing single EDA-based classifier on these two datasets. The multimodel evaluation by fusing"
        },
        {
          "111:10\nYin and Zhang, et al.": "music and EDA is conducted on the PMEmo dataset."
        },
        {
          "111:10\nYin and Zhang, et al.": "4.1\nDatasets Introduction"
        },
        {
          "111:10\nYin and Zhang, et al.": "DEAP. The database for emotion analysis using physiological signals (DEAP) is one of the most"
        },
        {
          "111:10\nYin and Zhang, et al.": "explored datasets for affective computing. The 32 participants (19-37 years, 50% females) separately"
        },
        {
          "111:10\nYin and Zhang, et al.": "watched 40 one-minute long videos. Meanwhile, the central nervous signal EEG and peripheral"
        },
        {
          "111:10\nYin and Zhang, et al.": "nervous signals including electromyogram (EMG), electrooculogram (EOG), skin temperature"
        },
        {
          "111:10\nYin and Zhang, et al.": "(TMP), galvanic skin response (GSR), blood volume pulse (BVP), and respiration (RSP) were recorded"
        },
        {
          "111:10\nYin and Zhang, et al.": "at a 512 Hz sampling rate and later down-sampled to 256Hz. Finally, the subjects performed the"
        },
        {
          "111:10\nYin and Zhang, et al.": "assessment of float subjective ratings on arousal, valence, liking, and dominance scales. From the"
        },
        {
          "111:10\nYin and Zhang, et al.": "dataset server, we downloaded â€œdata_preprocessed_python.zip\", which has been downsampled to"
        },
        {
          "111:10\nYin and Zhang, et al.": "128 Hz and segmented into 60-second trials. The 3-second pretrial baseline has been removed."
        },
        {
          "111:10\nYin and Zhang, et al.": "AMIGOS. The dataset for affect, personality, and mood research on individuals and groups"
        },
        {
          "111:10\nYin and Zhang, et al.": "(AMIGOS) consists of the short videos and long videos experiments: (1) the 16 clips of short videos"
        },
        {
          "111:10\nYin and Zhang, et al.": "were watched by 40 subjects; (2) the 37 subjects were separated to watch 4 long videos where 17"
        },
        {
          "111:10\nYin and Zhang, et al.": "subjects performed in an individual setting and 5 groups of 4 people did in a group setting. During"
        },
        {
          "111:10\nYin and Zhang, et al.": "two experiments, physiological signals (EEG, ECG, GSR) were captured, where GSR signals are"
        },
        {
          "111:10\nYin and Zhang, et al.": "recorded at 128 HZ. Each participant rated each video in valence, arousal, dominance, familiarity,"
        },
        {
          "111:10\nYin and Zhang, et al.": "and linking and selected from 7 basic emotions. In our paper, we downloaded the pre-processed"
        },
        {
          "111:10\nYin and Zhang, et al.": "and segmented version from the â€œdata_preprocessed_matlab.zip.\""
        },
        {
          "111:10\nYin and Zhang, et al.": "PMEmo. The popular music dataset with emotional annotations (PMEmo)"
        },
        {
          "111:10\nYin and Zhang, et al.": "work [63]. To the best of our knowledge, it is the current largest emotion recognition dataset with"
        },
        {
          "111:10\nYin and Zhang, et al.": "EDA and music signal. The chorus excerpts clipped from 794 pop songs are collected as the emotion"
        },
        {
          "111:10\nYin and Zhang, et al.": "elicitation. After the relaxing procedure, the subjects listened to music, and the EDA signals were"
        },
        {
          "111:10\nYin and Zhang, et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": "Dataset"
        },
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": "PMEmo"
        },
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": ""
        },
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": "DEAP"
        },
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": ""
        },
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": "AMIGOS"
        },
        {
          "Table 1. The statistics of binary relabeled annotations generated by four-steps procedure in valence and": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:12": "Table 2. The statistics of correlation between the original annotated arousal and valence scores in three",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "datasets.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "Dataset\nCorrelation\nP-Value",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "AMIGOS\nğ‘Ÿ = 0.56 [53]\nğ‘ â‰¤ .001 [53]",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "DEAP\nğ‘Ÿ = 0.15\nğ‘ â‰¤ .001",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "PMEmo\nğ‘Ÿ = 0.52\nğ‘ â‰¤ .001",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "by the strategy in [17] for binary classification, we calculate the subjectâ€™s emotion threshold with",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "discrete annotations as the four-steps procedure. The example of one subject (Subject-ID: 10031",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "from PMEmo) is shown in Fig. 3. The statistics of each class after label recreation is shown in",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "Table 1. It validates the relabeled processing will not cause imbalanced label distribution.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "4.3\nAnnotation Correlation",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "Previous works point out the EDA signal is relevant for arousal classification [53, 57]. However, the",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "results reported by [12, 18, 53] show that the classification performance for the valence and arousal",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "dimensions does not diverge considerably. Shukla et al. [53] reveals that the subject preferences of",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "V/A annotation scores greatly influence the gap of V/A performance. In the specific dataset, the",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "subject tends to annotate the valance score associated with arousal score, where the correlations of",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "V/A scores are high. To systematically analyze the experiment results in Section 5.2, we should first",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "analyze the annotation correlations in DEAP, AMIGOS, and PMEmo datasets. Following [53], we",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "calculate the Spearman correlation coefficient ğ‘Ÿ between the valence and arousal annotations. The",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "significant test of p-Value is also conducted. The statistics are listed in Table 2. Compared with DEAP",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "dataset (ğ‘Ÿ = 0.15), there are relatively higher correlations (ğ‘Ÿ = 0.56/0.52) between the annotated",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "arousal and valence scores of AMIGOS and PMEmo with the strong p-value evidence (ğ‘ â‰¤ .001). It",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "",
          "Yin and Zhang, et al.": "means the valance annotations in those two datasets have been influenced by arousal annotations."
        },
        {
          "111:12": "For DEAP dataset, the subject has no preference to annotate the valence scores associated with",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "arousal annotations. The characteristics of annotation can explain that our RTCAG with single EDA",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "delivers the accurate valence recognition with AMIGOS and PMEmo datasets, and the performance",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "of valence variable is much lower than arousal one in DEAP dataset. For the multimodel fusion",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "of RTCAN-1D, the involvement of music overcomes the defect of EDA signals to achieve high",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "performance in both the valence and the arousal dimensions.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "5\nEXPERIMENTS",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "In this section we introduce the training settings briefly. Then, systemic multimodal analyses",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "are conducted. Because the music and EDA signals from large-scale subjects are only available",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "",
          "Yin and Zhang, et al.": "from PMEmo, the analyses with single music and multimodal inputs are conducted on PMEmo."
        },
        {
          "111:12": "The DEAP [34] and AMIGOS [12] with fewer subjects are used for single EDA-based emotion",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "recognition. Because of\nthe individual specificity of",
          "Yin and Zhang, et al.": "large-scale subjects, we also conduct\nthe"
        },
        {
          "111:12": "incremental subject experiments with single EDA inputs on PMEmo to observe the overfitting",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "of CNN models. Moreover, we carry out\nthe ablation study to present",
          "Yin and Zhang, et al.": "the effects of different"
        },
        {
          "111:12": "components of RTCAN-1D. The comparisons with previous solutions prove that our proposed",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "",
          "Yin and Zhang, et al.": "multimodal framework can efficiently solve those problems. More details are presented as follows."
        },
        {
          "111:12": "5.1\nTraining Settings",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "For fair comparisons with other existing methods, the weights of the whole RTCAN-1D in both",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:12": "",
          "Yin and Zhang, et al.": "DEAP, AMIGOS and PMEmo datasets were initialized from the Gaussian distribution ğ‘ (0, 0.01)."
        },
        {
          "111:12": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.",
          "Yin and Zhang, et al.": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "Table 3. The multimodal analyses with different models, input signals, and different scale subjects in DEAP,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "AMIGOS, and PMEmo datasets."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "Dataset"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "DEAP"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "AMIGOS"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo-10%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo-40%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo-70%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo-10%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo-40%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo-70%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "PMEmo"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "256. The Stochastic Gradient Descent (SGD) was applied as the optimizer. Moreover, we set the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "initial learning rate to 0.001, which was decreased by 0.9 at every 15 epoches."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "The model suggested was assessed with 10-fold cross-validation. The dataset was divided into"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "10 disjoint subsets with subject-independent scenarios. At each fold, one subset was set to the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "validation data for fine-tuning hyperparameters, one subset was set to the test set, and the rest"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "were used for training. Repeated 10 times, all the subsets were tested, and the average metrics of all"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "the 10 folds were calculated as the results. The performance of binary classification is evaluated"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "with average accuracy and F1-score (the harmonic mean of precision and recall)."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "Before fed into a deep neural network, the EDA and music signals will be pre-processed. Processed"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "by z-score normalization and CvxEDA, the EDA signal was split into tonic, phasic components. The"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "signal denoizing could be accomplished by convex optimization. Then, the z-score normalization"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "was also conducted for three EDA and music signals. Moreover, we just used EDA signals after"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "3-second because annotators need some preliminary to evoke their emotion [3]. For input size"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:13": "alignment, linear interpolation was also conducted to rescale EDA inputs."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:14": "(a) Arousal ave-accuracies\n(b) Arousal F1-scores",
          "Yin and Zhang, et al.": "(c) Valence ave-accuracies\n(d) Valence F1-scores"
        },
        {
          "111:14": "Fig. 4. The recognition results with subject incremental settings for arousal and valence dimensions in PMEmo",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "dataset. The subjects are gradually increased from 10% to 40%, 70%, and 100%. Itâ€™s clear that there is an",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "significant drop-off from 10%to 40% subject percent and the decrease becomes gentle from 40% to 100%.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "",
          "Yin and Zhang, et al.": "The results of our RTCAG in DEAP, AMIGOS, and PMEmo are listed on the 1st, 2nd, and 14th"
        },
        {
          "111:14": "rows of Table 3. It is clear that our model has accomplished accurate recognition in DEAP and",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "AMIGOS, which proves the EDA feature extraction of RTCAG is excellent. Moreover, because of",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "the annotation correlations, the results for V/A dimensions in AMIGOS and PMEmo are relatively",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "close, and the performance in DEAP diverges considerably, which is aligned with the dataset",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "characteristics in Section 4.3. However, the performance in the large-scale PMEmo decreases 10%",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "with the increase in the number of participants, which intrigued us. Previous works of neural",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "networks [38, 39, 55] reveal the DNN models, especially deeper models with large parameters, may",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "suffer overfitting when the inputs are complicated from various different domains. Considering the",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "various subject specificity, we assume the RTCAG may suffer overfitting.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "",
          "Yin and Zhang, et al.": "To validate our assumption, we conduct an incremental subject experiment. The subjects from"
        },
        {
          "111:14": "PMEmo are gradually increased from 10% to 40%, 70%, and 100%. The benefit of hand-crafted",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "features is that they have been shown to be stable by a variety of studies [53]. To systematically",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "",
          "Yin and Zhang, et al.": "evaluate our model we add the traditional SVM classifier with time domain, frequency domain,"
        },
        {
          "111:14": "and time-frequency domain features hand-crafted The 128 statistical features are calculated with",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "the methods of [53]. And we refer to [12] to set the regularization parameter ğ¶ of the linear SVM",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "as 0.25. Because the RTCAG is well-designed for 3-channel EDA signals, we wonder if the simple",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "CNN with EDA feature engineering may suffer less over-fitting. Hence, we add another model in",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "which the 128 statistical features are fed into a 3 layers CNN. The basic convolution layer of simple",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "CNN keeps the same with RTCAG. Keeping the same training settings, the results of three models",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "are shown on the 3rd row to 14th row of Table 3 and illustrated in Fig 4.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "",
          "Yin and Zhang, et al.": "According to the results with single EDA input, we can make some analyses: (1) for RTCAG, when"
        },
        {
          "111:14": "the number of subjects (PMEmo-10%) is small, the recognition accuracies of RTCAG are competitive",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "to DEAP and AMIGOS. When the involved subjects increase to hundreds, the recognition results",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "decrease drastically. More individual specificity cannot help the model to converge but cause the",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "",
          "Yin and Zhang, et al.": "RTCAG to overfit. (2) For 3 layers CNN with hand-crafted features, the model also suffers overfitting,"
        },
        {
          "111:14": "but the decrease is not as sharp as RTCAG. It reveals that the EDA signals from hundreds of subjects",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "are indeed complicated to cause overfitting even processed with feature engineering. On the other",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "hand, the performance of 3 layers CNN is poorer than RTCAG, which means simpler architecture",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "and fewer parameters limit the performance of CNN model. (3) As shown in Fig.4, the advantage",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "",
          "Yin and Zhang, et al.": "of traditional SVM classifier with hand-crafted features is stable. When increasing the subjects,"
        },
        {
          "111:14": "the SVM only shows a slight performance drop. However,",
          "Yin and Zhang, et al.": "it suffers underfitting and produces"
        },
        {
          "111:14": "the worst recognition accuracy limited by the poor representation ability. (4) Although there is a",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "slightly better performance for arousal, the recognition performance for the two variables in four",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:14": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.",
          "Yin and Zhang, et al.": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "subject scales does not diverge considerably, which is aligned with the high annotation correlation"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "of PMEmo dataset as we have described in Section 4.3. According to the analyses, the defect of the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "single EDA input motivates us to include additional information through multimodal fusion."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "5.2.2\nSingle Music Recognition. This situation is common for user-independent emotion recogni-"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "tion: the same song listened by different people will stimulate the different emotional states, then"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "get the opposite labels. The parameter optimization of deep learning model is strongly dependent"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "on the loss between the predicted output and ground-truth label for backpropagation. When the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "same input data maps the total different ground-truth labels, the network optimization becomes"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "very hard. If there exists many such data, the network training may even get corrupted. Therefore,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "for single music recognition, we directly used the result (SVM + Music) in our previous work [62]."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "The results are presented in the 7th row of Table 3. We can make some analyses: (1) compared with"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "single EDA input, the performance with single music input get improved for valence recognition"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "because the EDA is less relevant to the valance state [53, 57] but the music has a strong relationship"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "with subjectâ€™s valence/arousal moods [41]. (2) For single music features,\nif considering subject"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "specificity, the same music might lead to a completely opposite emotional state, which means"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "the same music input maps to label 0 and 1 with different subject. (3) The single music input can"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "reveal peopleâ€™s general emotion states at a certain extent. But it is not enough for user-independent"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "emotion recognition. Although the SVM classifier with single music feature has achieved 70%"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "accuracies,\nit has limited ability to recognize the cases which map to different emotion labels"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "because of individual differences. Towards this, we add the music feature as the static emotion"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "benchmark to help the convergence of our RTCAN-1D."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "5.2.3\nFusing Music and EDA Recognition. As shown in the last row of Table 3, when fusing the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "music and EDA features,\nthe proposed RTCAN-1D achieves the best performance. For single"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "music recognition, the classifier could not recognize the individual specificity because the music"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "feature had no relationships with a specific human. For single EDA recognition, the traditional"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "classifier with feature engineering suffers underfitting, and our well-designed RTCAG model will"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "get overfitting when large-scale subjects involve. The significant\nimprovement of RTCAN-1D"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "validates that the fusion of external static features from music and individual EDA features is a"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "solution for large-scale emotional recognition."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "5.3\nAblation Analyses"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "To avoid the less-effective experiment repetition, we carry out most of the ablation studies in"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "PMEmo. For some experiments of fitting ability, since the network structure has been slightly"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "changed for different datasets, we present the results in DEAP, AMIGOS and PMEmo datasets."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "5.3.1\nRes-block Configuration. As Fig. 1 and Fig. 2 illustrate, the residual feature extraction (RFE) is"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "the principal part of the whole RTCAN-1D while the attention module, shallow feature extraction,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "and linear classifier are designed with the simplest structures. Therefore, the structure of RFE is the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "major determinants of computing cost. As explained in Section 3.2.3, we apply the Resnet-18 as the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "backbone and replace all 2D convolution operations with 1D version. Keeping the basic backbone"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "of Resnet, the key problem addresses the depth of res-block (the stacking 1D convolutional layer,"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "batch normalization, and ReLU) and the number of internal channels. The original settings of"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "ResNet-18 are ğ‘‘ = 2 and ğ‘ = [64, 128, 256, 512]. We design res-block with different configurations."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "The trade-offs between each structure are shown in Table 4."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "In practice, we find that the best performance is given by the simplest structure. By using a deeper"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "convolutional layer and more internal channels, the performance of RTCAN-1D get drop-off, and"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:15": "the network can be easily overfitting, which is more obvious in smaller scale DEAP and AMIGOS"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": "V/A"
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": "V"
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": "A"
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        },
        {
          "we applied the simplest structure ğ‘‘ = 1 and ğ‘ = [64, 64, 64, 64] for our res-block.": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "Base"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "SCA_Res"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "SCA"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "RTNA"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "Ave-Acc"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "F1-score"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "Ave-Acc"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "F1-score"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "Ave-Acc"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": "F1-score"
        },
        {
          "Table 5. Comparison of different attention modules in DEAP, AMIGOS and PMEmo datasets.": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:18": "",
          "Yin and Zhang, et al.": "(c) Channel-temporal attention"
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": "1"
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "attention weights in Fig. 5.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "weights in the 3-channel EDA input.",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:18": "",
          "Yin and Zhang, et al.": ""
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "(a) Channel attention\n(b) Temporal attention"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "Fig. 7. The visualization of the curves of three signals and the column of attention weight in arousal recognition"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "origin, phasic and tonic signals, respectively. The black columns represent the inverse mapping of attention"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "0"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "(a) Channel attention"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "(b) Temporal attention"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "(c) Channel-temporal attention"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "Fig. 8. The arousal spectrogram visualization. The spectrograms in (a), (b), (c) separately correspond to the"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "The quantitative experiment results in Section 5.3.2 and qualitative visualization comparisons"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "in this subsection can show that the channel and temporal attention modules guide the proposed"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "Comparison with previous methods"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "To validate our proposed networkâ€™s superiority, we provide extensive comparisons in three emotion"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "datasets. The single EDA signals from the small-scale DEAP and AMIGOS datasets are used to"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "compare our RTCAG with SOTA EDA-based methods. And large-scale PMEmo with music and EDA"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "signals is applied to prove the effectiveness of our multimodal fusion. Those SOTA models include"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "both the hand-crafted feature-based conventional classifiers and deep learning-based frameworks."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "For single EDA, previous works applied various models to classify the hand-craft features such"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "as Support Vector Machine (SVM) [12, 34], Multi-Layer Perceptron (MLP) [52]. The CNN-based"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "methods [40, 50, 62] with auto feature extraction were also proposed. We notice that the EDA"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "decomposition is gradually being used for emotion recognition task [52]. And our previous work [62]"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": "has also combined CvxEDA [20] and ResNet [23] for emotion recognition. Recently, Machot et"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:19": ""
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 6: liststhecomparisonsbetweenourmodelandpreviousstudies.OurRTCAGoutperformsexisting",
      "data": [
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "Ave-Acc\nF1-Score"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 52.80%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "-"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 54.10%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 75.00%\nV: 71.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 71.00%\nA: 67.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 75.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "-"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 76.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 75.87%\nV: 74.48%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 78.20%\nA: 77.56%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 62.70%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "-"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 57.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 69.80%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "-"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 79.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 82.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 83.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 82.00%\nA: 83.00%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 77.15%\nV: 79.18%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 83.42%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 83.77%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 78.24%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 87.30%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 80.06%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 85.50%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 55.92%\nV: 58.83%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 57.24%\nA: 60.12%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 64.52%\nV: 63.31%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 65.19%\nA: 65.84%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 73.43%\nV: 77.54%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 73.65%\nA: 78.56%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "V: 79.68%\nV: 82.45%"
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": ""
        },
        {
          "111:20": "",
          "Yin and Zhang, et al.": "A: 83.76%\nA: 86.12%"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": "6"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": "The individual differences in the large-scale emotion recognition task is a tough problem, which"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": "cannot alleviate this problem. Towards this, we make an attempt with multimodal"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": ""
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": "EDA-based emotion recognition will promote a lot of practical applications."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": "REFERENCES"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:21": "[1]"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "EDA-based emotion recognition will promote a lot of practical applications."
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "REFERENCES"
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[1]"
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[6]"
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[9] Wolfram Boucsein. 2012. Electrodermal activity. Springer Science & Business Media."
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[10] Antoni Buades, Bartomeu Coll, and J-M Morel. 2005. A non-local algorithm for image denoising. In 2005 IEEE Computer"
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[11] Paulo Chiliguano and Gyorgy Fazekas. 2016. Hybrid music recommender using content-based and social information."
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[12]"
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": ""
        },
        {
          "also further improve the ability of CNNâ€™s end-to-end frameworks. We believe the research of": "[13] Antonio R. Damasio. 1994. Descartesâ€™ Error: Emotion, Reason, and the Human Brain."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "111:22\nYin and Zhang, et al.": "[14] Michael E Dawson, Anne M Schell, and Diane L Filion. 2007. The electrodermal system. Handbook of psychophysiology"
        },
        {
          "111:22\nYin and Zhang, et al.": "2 (2007), 200â€“223."
        },
        {
          "111:22\nYin and Zhang, et al.": "[15] Paul Ekman. 1992. An argument for basic emotions. Cognition & emotion 6, 3-4 (1992), 169â€“200."
        },
        {
          "111:22\nYin and Zhang, et al.": "[16] Moataz El Ayadi, Mohamed S Kamel, and Fakhri Karray. 2011.\nSurvey on speech emotion recognition: Features,"
        },
        {
          "111:22\nYin and Zhang, et al.": "classification schemes, and databases. Pattern Recognition 44, 3 (2011), 572â€“587."
        },
        {
          "111:22\nYin and Zhang, et al.": "[17] Christos A Frantzidis, Charalampos Bratsas, Manousos A Klados, Evdokimos Konstantinidis, Chrysa D Lithari, Ana B"
        },
        {
          "111:22\nYin and Zhang, et al.": "Vivas, Christos L Papadelis, Eleni Kaldoudi, Costas Pappas, and Panagiotis D Bamidis. 2010. On the classification of"
        },
        {
          "111:22\nYin and Zhang, et al.": "emotional biosignals evoked while viewing affective pictures: an integrated data-mining-based approach for healthcare"
        },
        {
          "111:22\nYin and Zhang, et al.": "applications.\nIEEE Transactions on Information Technology in Biomedicine 14, 2 (2010), 309â€“318."
        },
        {
          "111:22\nYin and Zhang, et al.": "[18] Nagarajan Ganapathy, Yedukondala Rao Veeranki, and Ramakrishnan Swaminathan. 2020. Convolutional Neural"
        },
        {
          "111:22\nYin and Zhang, et al.": "Network based Emotion Classification using Electrodermal Activity Signals and Time-Frequency Features. Expert"
        },
        {
          "111:22\nYin and Zhang, et al.": "Systems with Applications (2020), 113571."
        },
        {
          "111:22\nYin and Zhang, et al.": "[19] Alberto Greco, Gaetano Valenza, Luca Citi, and Enzo Pasquale Scilingo. 2016. Arousal and valence recognition of"
        },
        {
          "111:22\nYin and Zhang, et al.": "affective sounds based on electrodermal activity.\nIEEE Sensors Journal 17, 3 (2016), 716â€“725."
        },
        {
          "111:22\nYin and Zhang, et al.": "[20] Alberto Greco, Gaetano Valenza, Antonio Lanata, Enzo Pasquale Scilingo, and Luca Citi. 2015. cvxEDA: A convex"
        },
        {
          "111:22\nYin and Zhang, et al.": "optimization approach to electrodermal activity processing.\nIEEE Transactions on Biomedical Engineering 63, 4 (2015),"
        },
        {
          "111:22\nYin and Zhang, et al.": "797â€“804."
        },
        {
          "111:22\nYin and Zhang, et al.": "[21] Rui Guo, Shuangjiang Li, Li He, Wei Gao, Hairong Qi, and Gina Owens. 2013. Pervasive and unobtrusive emotion sensing"
        },
        {
          "111:22\nYin and Zhang, et al.": "for human mental health. In 2013 7th International Conference on Pervasive Computing Technologies for Healthcare and"
        },
        {
          "111:22\nYin and Zhang, et al.": "Workshops. IEEE, 436â€“439."
        },
        {
          "111:22\nYin and Zhang, et al.": "[22]\nStephan Hamann. 2012. Mapping discrete and dimensional emotions onto the brain: controversies and consensus."
        },
        {
          "111:22\nYin and Zhang, et al.": "Trends in cognitive sciences 16, 9 (2012), 458â€“466."
        },
        {
          "111:22\nYin and Zhang, et al.": "[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In"
        },
        {
          "111:22\nYin and Zhang, et al.": "Proceedings of the IEEE conference on computer vision and pattern recognition. 770â€“778."
        },
        {
          "111:22\nYin and Zhang, et al.": "[24]\nJie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer"
        },
        {
          "111:22\nYin and Zhang, et al.": "vision and pattern recognition. 7132â€“7141."
        },
        {
          "111:22\nYin and Zhang, et al.": "[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. 2017. Densely connected convolutional"
        },
        {
          "111:22\nYin and Zhang, et al.": "networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4700â€“4708."
        },
        {
          "111:22\nYin and Zhang, et al.": "[26] Carroll E. Izard. 2007. Basic emotions, natural kinds, emotion schemas, and a new paradigm. Perspectives on Psychological"
        },
        {
          "111:22\nYin and Zhang, et al.": "Science 2, 3 (2007), 260â€“280."
        },
        {
          "111:22\nYin and Zhang, et al.": "[27] Alejandro Jaimes and Nicu Sebe. 2007. Multimodal humanâ€“computer interaction: A survey. Computer vision and"
        },
        {
          "111:22\nYin and Zhang, et al.": "image understanding 108, 1-2 (2007), 116â€“134."
        },
        {
          "111:22\nYin and Zhang, et al.": "S Jerritta, M Murugappan, R Nagarajan, and Khairunizam Wan. 2011. Physiological signals based human emotion\n[28]"
        },
        {
          "111:22\nYin and Zhang, et al.": "recognition: a review. In 2011 IEEE 7th International Colloquium on Signal Processing and its Applications. IEEE, 410â€“415."
        },
        {
          "111:22\nYin and Zhang, et al.": "[29] Christos D Katsis, Nikolaos Katertsidis, George Ganiatsas, and Dimitrios I Fotiadis. 2008. Toward emotion recognition"
        },
        {
          "111:22\nYin and Zhang, et al.": "in car-racing drivers: A biosignal processing approach.\nIEEE Transactions on Systems, Man, and Cybernetics-Part A:"
        },
        {
          "111:22\nYin and Zhang, et al.": "Systems and Humans 38, 3 (2008), 502â€“512."
        },
        {
          "111:22\nYin and Zhang, et al.": "[30] Malia Kelsey, Murat Akcakaya, Ian R. Kleckner, Richard Vincent Palumbo, Lisa Feldman Barrett, Karen S. Quigley, and"
        },
        {
          "111:22\nYin and Zhang, et al.": "Matthew S. Goodwin. 2018. Applications of sparse recovery and dictionary learning to enhance analysis of ambulatory"
        },
        {
          "111:22\nYin and Zhang, et al.": "electrodermal activity data. Biomedical Signal Processing and Control 40 (2018), 58â€“70."
        },
        {
          "111:22\nYin and Zhang, et al.": "[31]\nJonghwa Kim. 2007. Bimodal emotion recognition using speech and physiological changes. Robust speech recognition"
        },
        {
          "111:22\nYin and Zhang, et al.": "and understanding 265 (2007), 280."
        },
        {
          "111:22\nYin and Zhang, et al.": "[32]\nJonghwa Kim and Elisabeth AndrÃ©. 2008. Emotion recognition based on physiological changes in music listening.\nIEEE"
        },
        {
          "111:22\nYin and Zhang, et al.": "transactions on pattern analysis and machine intelligence 30, 12 (2008), 2067â€“2083."
        },
        {
          "111:22\nYin and Zhang, et al.": "[33] Yelin Kim and Emily Mower Provost. 2015. Emotion recognition during speech using dynamics of multiple regions of"
        },
        {
          "111:22\nYin and Zhang, et al.": "the face. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 12, 1s (2015), 1â€“23."
        },
        {
          "111:22\nYin and Zhang, et al.": "[34]\nSander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry"
        },
        {
          "111:22\nYin and Zhang, et al.": "Pun, Anton Nijholt, and Ioannis Patras. 2011. Deap: A database for emotion analysis; using physiological signals.\nIEEE"
        },
        {
          "111:22\nYin and Zhang, et al.": "transactions on affective computing 3, 1 (2011), 18â€“31."
        },
        {
          "111:22\nYin and Zhang, et al.": "[35]\nSander Koelstra and Ioannis Patras. 2013. Fusion of facial expressions and EEG for implicit affective tagging.\nImage"
        },
        {
          "111:22\nYin and Zhang, et al.": "and Vision Computing 31, 2 (2013), 164â€“174."
        },
        {
          "111:22\nYin and Zhang, et al.": "[36] Peter J Lang, Mark K Greenwald, Margaret M Bradley, and Alfons O Hamm. 1993. Looking at pictures: Affective, facial,"
        },
        {
          "111:22\nYin and Zhang, et al.": "visceral, and behavioral reactions. Psychophysiology 30, 3 (1993), 261â€“273."
        },
        {
          "111:22\nYin and Zhang, et al.": "[37] Peter Langhorne, Julie Bernhardt, and Gert Kwakkel. 2011. Stroke rehabilitation. The Lancet 377, 9778 (2011), 1693â€“1702."
        },
        {
          "111:22\nYin and Zhang, et al.": "[38]\nSteve Lawrence and C Lee Giles. 2000. Overfitting and neural networks: conjugate gradient and backpropagation. In"
        },
        {
          "111:22\nYin and Zhang, et al.": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing:"
        },
        {
          "111:22\nYin and Zhang, et al.": "New Challenges and Perspectives for the New Millennium, Vol. 1. IEEE, 114â€“119."
        },
        {
          "111:22\nYin and Zhang, et al.": "J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[39]\nSteve Lawrence, C Lee Giles, and Ah Chung Tsoi. 1997. Lessons in neural network training: Overfitting may be harder"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "than expected. In AAAI/IAAI. Citeseer, 540â€“545."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[40] Wenqian Lin, Chao Li, and Shouqian Sun. 2017. Deep convolutional neural network for emotion recognition using"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "EEG and peripheral physiological signal. In International Conference on Image and Graphics. Springer, 385â€“394."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[41] Yu-Ching Lin, Yi-Hsuan Yang, and Homer H Chen. 2011. Exploiting online music tags for music emotion classification."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 7, 1 (2011), 1â€“16."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[42]\nJiamin Liu, Yuanqi Su, and Yuehu Liu. 2017. Multi-modal emotion recognition with temporal-band attention based on"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "lstm-rnn. In Pacific Rim Conference on Multimedia. Springer, 194â€“204."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[43] Aditya Shekhar Nittala, Arshad Khan, Klaus Kruttwig, Tobias Kraus, and JÃ¼rgen Steimle. 2020. PhysioSkin: Rapid"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Fabrication of Skin-Conformal Physiological Interfaces. In Proceedings of the 2020 CHI Conference on Human Factors in"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Computing Systems. 1â€“10."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[44] Wenjie Pei, Tadas Baltrusaitis, David MJ Tax, and Louis-Philippe Morency. 2017. Temporal attention-gated model"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "for robust sequence classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "6730â€“6739."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[45] Rosalind W Picard. 2000. Affective computing. MIT press."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "intelligence: Analysis of\n[46] Rosalind W. Picard, Elias Vyzas, and Jennifer Healey. 2001. Toward machine emotional"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "affective physiological state.\nIEEE Transactions on Pattern Analysis & Machine Intelligence 10 (2001), 1175â€“1191."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[47] Robert Plutchik. 1982. A psychoevolutionary theory of emotions. Social Science Information 21 (1982), 529â€“553."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[48]\nJames A Russell. 1980. A circumplex model of affect. Journal of personality and social psychology 39, 6 (1980), 1161."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[49] Akane Sano and Rosalind W Picard. 2013.\nStress recognition using wearable sensors and mobile phones. In 2013"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Humaine Association Conference on Affective Computing and Intelligent Interaction. IEEE, 671â€“676."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[50] Luz Santamaria-Granados, Mario Munoz-Organero, Gustavo Ramirez-Gonzalez, Enas Abdulhay, and NJIA Arunkumar."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "2018. Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS)."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "IEEE Access 7 (2018), 57â€“67."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[51] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "International Conference on Computer Vision. 618â€“626."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[52] Vivek Sharma, Neelam R. Prakash, and Parveen Kalra. 2019. Audio-video emotional response mapping based upon"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Electrodermal Activity. Biomedical Signal Processing and Control 47 (2019), 324 â€“ 333."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[53]\nJainendra Shukla, Miguel Barreda-Angeles, Joan Oliver, GC Nandi, and Domenec Puig. 2019. Feature Extraction and"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Selection for Emotion Recognition from Electrodermal Activity.\nIEEE Transactions on Affective Computing (2019)."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[54] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "arXiv preprint arXiv:1409.1556 (2014)."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[55] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929â€“1958."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[56] Nattapong Thammasan, Ken-ichi Fukui, and Masayuki Numao. 2017. Multimodal fusion of eeg and musical features in"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "music-emotion recognition. In Thirty-First AAAI Conference on Artificial Intelligence."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[57] Cristian A Torres, Ãlvaro A Orozco, and Mauricio A Ãlvarez. 2013. Feature selection for multimodal emotion recognition"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "in the arousal-valence space. In 2013 35th Annual International Conference of the IEEE Engineering in Medicine and"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Biology Society (EMBC). IEEE, 4330â€“4333."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998â€“6008."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[59] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In Proceedings of"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "the IEEE Conference on Computer Vision and Pattern Recognition. 7794â€“7803."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[60] Wilhelm Max Wundt. 1921. Vorlesungen Ã¼ber die menschen- und tierseele. American Journal of Psychology 32, 1"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "(1921), 151."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[61] Ning Xiong and Per Svensson. 2002. Multi-sensor management\nfor information fusion:\nissues and approaches."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Information fusion 3, 2 (2002), 163â€“186."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[62] Guanghao Yin, Shouqian Sun, Hui Zhang, Dian Yu, Chao Li, Kejun Zhang, and Ning Zou. 2019. User Independent"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Emotion Recognition with Residual Signal-Image Network. In 2019 IEEE International Conference on Image Processing"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "(ICIP). IEEE, 3277â€“3281."
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "[63] Kejun Zhang, Hui Zhang, Simeng Li, Changyuan Yang, and Lingyun Sun. 2018. The PMEmo Dataset for Music Emotion"
        },
        {
          "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals111:23": "Recognition. In Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval. ACM, 135â€“142."
        }
      ],
      "page": 23
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Deep-Learning Model for Subject-Independent Human Emotion Recognition Using Electrodermal Activity Sensors",
      "authors": [
        "Al Fadi",
        "Ali Machot",
        "Mouhannad Elmachot",
        "Elyan Ali",
        "Kyandoghere Al Machot",
        "Kyamakya"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "2",
      "title": "Separating individual skin conductance responses in a short interstimulus-interval paradigm",
      "authors": [
        "Chris David M Alexander",
        "P Trengove",
        "Tim Johnston",
        "Cooper",
        "Evian August",
        "Gordon"
      ],
      "year": "2005",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "3",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "Anna Aljanaki",
        "Yi-Hsuan Yang",
        "Mohammad Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "4",
      "title": "Dry electrode optimization for wrist-based electrodermal activity monitoring",
      "authors": [
        "As Anusha",
        "Tony Preejith",
        "Jayaraj Akl",
        "Mohanasankar Joseph",
        "Sivaprakasam"
      ],
      "year": "2018",
      "venue": "IEEE International Symposium on Medical Measurements and Applications (MeMeA). IEEE"
    },
    {
      "citation_id": "5",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal machine learning: A survey and taxonomy"
    },
    {
      "citation_id": "6",
      "title": "Deep listeners: Music, emotion, and trancing",
      "authors": [
        "Judith Becker"
      ],
      "year": "2004",
      "venue": "Deep listeners: Music, emotion, and trancing"
    },
    {
      "citation_id": "7",
      "title": "A continuous measure of phasic electrodermal activity",
      "authors": [
        "Mathias Benedek",
        "Christian Kaernbach"
      ],
      "year": "2010",
      "venue": "Journal of neuroscience methods"
    },
    {
      "citation_id": "8",
      "title": "Decomposition of skin conductance data by means of nonnegative deconvolution",
      "authors": [
        "Mathias Benedek",
        "Christian Kaernbach"
      ],
      "year": "2010",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "9",
      "title": "Electrodermal activity",
      "authors": [
        "Wolfram Boucsein"
      ],
      "year": "2012",
      "venue": "Electrodermal activity"
    },
    {
      "citation_id": "10",
      "title": "A non-local algorithm for image denoising",
      "authors": [
        "Antoni Buades",
        "Bartomeu Coll",
        "J-M Morel"
      ],
      "year": "2005",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)"
    },
    {
      "citation_id": "11",
      "title": "Hybrid music recommender using content-based and social information",
      "authors": [
        "Paulo Chiliguano",
        "Gyorgy Fazekas"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Amigos: a dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "Juan Abdon",
        "Miranda Correa",
        "Mojtaba Khomami Abadi",
        "Niculae Sebe",
        "Ioannis Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Descartes' Error: Emotion, Reason, and the Human Brain",
      "authors": [
        "Antonio Damasio"
      ],
      "year": "1994",
      "venue": "Descartes' Error: Emotion, Reason, and the Human Brain"
    },
    {
      "citation_id": "14",
      "title": "The electrodermal system. Handbook of psychophysiology",
      "authors": [
        "Anne Michael E Dawson",
        "Diane Schell",
        "Filion"
      ],
      "year": "2007",
      "venue": "The electrodermal system. Handbook of psychophysiology"
    },
    {
      "citation_id": "15",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "16",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "On the classification of emotional biosignals evoked while viewing affective pictures: an integrated data-mining-based approach for healthcare applications",
      "authors": [
        "Christos Frantzidis",
        "Charalampos Bratsas",
        "Manousos Klados",
        "Evdokimos Konstantinidis",
        "Chrysa Lithari",
        "Ana Vivas",
        "Christos Papadelis",
        "Eleni Kaldoudi",
        "Costas Pappas",
        "Panagiotis Bamidis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "18",
      "title": "Convolutional Neural Network based Emotion Classification using Electrodermal Activity Signals and Time-Frequency Features",
      "authors": [
        "Nagarajan Ganapathy",
        "Yedukondala Veeranki",
        "Ramakrishnan Swaminathan"
      ],
      "year": "2020",
      "venue": "Convolutional Neural Network based Emotion Classification using Electrodermal Activity Signals and Time-Frequency Features"
    },
    {
      "citation_id": "19",
      "title": "Arousal and valence recognition of affective sounds based on electrodermal activity",
      "authors": [
        "Alberto Greco",
        "Gaetano Valenza",
        "Luca Citi",
        "Enzo Scilingo"
      ],
      "year": "2016",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "20",
      "title": "cvxEDA: A convex optimization approach to electrodermal activity processing",
      "authors": [
        "Alberto Greco",
        "Gaetano Valenza",
        "Antonio Lanata",
        "Enzo Pasquale Scilingo",
        "Luca Citi"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "21",
      "title": "Pervasive and unobtrusive emotion sensing for human mental health",
      "authors": [
        "Rui Guo",
        "Shuangjiang Li",
        "Li He",
        "Wei Gao",
        "Hairong Qi",
        "Gina Owens"
      ],
      "year": "2013",
      "venue": "2013 7th International Conference on Pervasive Computing Technologies for Healthcare and Workshops"
    },
    {
      "citation_id": "22",
      "title": "Mapping discrete and dimensional emotions onto the brain: controversies and consensus",
      "authors": [
        "Stephan Hamann"
      ],
      "year": "2012",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "23",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Basic emotions, natural kinds, emotion schemas, and a new paradigm",
      "authors": [
        "Carroll Izard"
      ],
      "year": "2007",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "27",
      "title": "Multimodal human-computer interaction: A survey",
      "authors": [
        "Alejandro Jaimes",
        "Nicu Sebe"
      ],
      "year": "2007",
      "venue": "Computer vision and image understanding"
    },
    {
      "citation_id": "28",
      "title": "Physiological signals based human emotion recognition: a review",
      "authors": [
        "M Jerritta",
        "R Murugappan",
        "Khairunizam Nagarajan",
        "Wan"
      ],
      "year": "2011",
      "venue": "2011 IEEE 7th International Colloquium on Signal Processing and its Applications"
    },
    {
      "citation_id": "29",
      "title": "Toward emotion recognition in car-racing drivers: A biosignal processing approach",
      "authors": [
        "Christos Katsis",
        "Nikolaos Katertsidis",
        "George Ganiatsas",
        "Dimitrios Fotiadis"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans"
    },
    {
      "citation_id": "30",
      "title": "Applications of sparse recovery and dictionary learning to enhance analysis of ambulatory electrodermal activity data",
      "authors": [
        "Malia Kelsey",
        "Murat Akcakaya",
        "Ian Kleckner",
        "Richard Vincent Palumbo",
        "Lisa Barrett",
        "Karen Quigley",
        "Matthew Goodwin"
      ],
      "year": "2018",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "31",
      "title": "Bimodal emotion recognition using speech and physiological changes. Robust speech recognition and understanding",
      "authors": [
        "Jonghwa Kim"
      ],
      "year": "2007",
      "venue": "Bimodal emotion recognition using speech and physiological changes. Robust speech recognition and understanding"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "Jonghwa Kim",
        "Elisabeth AndrÃ©"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition during speech using dynamics of multiple regions of the face",
      "authors": [
        "Yelin Kim",
        "Emily Provost"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "34",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "35",
      "title": "Fusion of facial expressions and EEG for implicit affective tagging",
      "authors": [
        "Sander Koelstra",
        "Ioannis Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "36",
      "title": "Looking at pictures: Affective, facial, visceral, and behavioral reactions",
      "authors": [
        "J Peter",
        "Mark Lang",
        "Margaret Greenwald",
        "Alfons O Bradley",
        "Hamm"
      ],
      "year": "1993",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "37",
      "title": "Stroke rehabilitation. The Lancet",
      "authors": [
        "Peter Langhorne",
        "Julie Bernhardt",
        "Gert Kwakkel"
      ],
      "year": "2011",
      "venue": "Stroke rehabilitation. The Lancet"
    },
    {
      "citation_id": "38",
      "title": "Overfitting and neural networks: conjugate gradient and backpropagation",
      "authors": [
        "Steve Lawrence",
        "C Lee"
      ],
      "year": "2000",
      "venue": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium"
    },
    {
      "citation_id": "39",
      "title": "Lessons in neural network training: Overfitting may be harder than expected",
      "authors": [
        "Steve Lawrence",
        ", C Lee Giles",
        "Ah Chung"
      ],
      "year": "1997",
      "venue": "AAAI/IAAI"
    },
    {
      "citation_id": "40",
      "title": "Deep convolutional neural network for emotion recognition using EEG and peripheral physiological signal",
      "authors": [
        "Wenqian Lin",
        "Chao Li",
        "Shouqian Sun"
      ],
      "year": "2017",
      "venue": "International Conference on Image and Graphics"
    },
    {
      "citation_id": "41",
      "title": "Exploiting online music tags for music emotion classification",
      "authors": [
        "Yu-Ching Lin",
        "Yi-Hsuan Yang",
        "Homer H Chen"
      ],
      "year": "2011",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "42",
      "title": "Multi-modal emotion recognition with temporal-band attention based on lstm-rnn",
      "authors": [
        "Jiamin Liu",
        "Yuanqi Su",
        "Yuehu Liu"
      ],
      "year": "2017",
      "venue": "Pacific Rim Conference on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "PhysioSkin: Rapid Fabrication of Skin-Conformal Physiological Interfaces",
      "authors": [
        "Aditya Shekhar Nittala",
        "Arshad Khan",
        "Klaus Kruttwig",
        "Tobias Kraus",
        "JÃ¼rgen Steimle"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "44",
      "title": "Temporal attention-gated model for robust sequence classification",
      "authors": [
        "Wenjie Pei",
        "Tadas Baltrusaitis",
        "David Tax",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "46",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "Rosalind Picard",
        "Elias Vyzas",
        "Jennifer Healey"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "47",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1982",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "48",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "49",
      "title": "Stress recognition using wearable sensors and mobile phones",
      "authors": [
        "Akane Sano",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "50",
      "title": "Using deep convolutional neural network for emotion detection on a physiological signals dataset (AMIGOS)",
      "authors": [
        "Luz Santamaria-Granados",
        "Mario Munoz-Organero",
        "Gustavo Ramirez-Gonzalez",
        "Enas Abdulhay",
        "Arunkumar"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "51",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "52",
      "title": "Audio-video emotional response mapping based upon Electrodermal Activity",
      "authors": [
        "Vivek Sharma",
        "Neelam Prakash",
        "Parveen Kalra"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "53",
      "title": "Feature Extraction and Selection for Emotion Recognition from Electrodermal Activity",
      "authors": [
        "Jainendra Shukla",
        "Miguel Barreda-Angeles",
        "Joan Oliver",
        "G Nandi",
        "Domenec Puig"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "55",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "56",
      "title": "Multimodal fusion of eeg and musical features in music-emotion recognition",
      "authors": [
        "Nattapong Thammasan",
        "Ken-Ichi Fukui",
        "Masayuki Numao"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "57",
      "title": "Feature selection for multimodal emotion recognition in the arousal-valence space",
      "authors": [
        "A Cristian",
        "Ãlvaro Torres",
        "Mauricio Orozco",
        "Ãlvarez"
      ],
      "year": "2013",
      "venue": "2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "58",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "59",
      "title": "Abhinav Gupta, and Kaiming He",
      "authors": [
        "Xiaolong Wang",
        "Ross Girshick"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Vorlesungen Ã¼ber die menschen-und tierseele",
      "authors": [
        "Wilhelm Max"
      ],
      "year": "1921",
      "venue": "American Journal of Psychology"
    },
    {
      "citation_id": "61",
      "title": "Multi-sensor management for information fusion: issues and approaches",
      "authors": [
        "Ning Xiong",
        "Per Svensson"
      ],
      "year": "2002",
      "venue": "Information fusion"
    },
    {
      "citation_id": "62",
      "title": "User Independent Emotion Recognition with Residual Signal-Image Network",
      "authors": [
        "Guanghao Yin",
        "Shouqian Sun",
        "Hui Zhang",
        "Dian Yu",
        "Chao Li",
        "Kejun Zhang",
        "Ning Zou"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "63",
      "title": "The PMEmo Dataset for Music Emotion Recognition",
      "authors": [
        "Kejun Zhang",
        "Hui Zhang",
        "Simeng Li",
        "Changyuan Yang",
        "Lingyun Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval"
    }
  ]
}