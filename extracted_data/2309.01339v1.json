{
  "paper_id": "2309.01339v1",
  "title": "Unisa: Unified Generative Framework For Sentiment Analysis",
  "published": "2023-09-04T03:49:30Z",
  "authors": [
    "Zaijing Li",
    "Ting-En Lin",
    "Yuchuan Wu",
    "Meng Liu",
    "Fengxiao Tang",
    "Ming Zhao",
    "Yongbin Li"
  ],
  "keywords": [
    "sentiment analysis",
    "multimodal information",
    "unified framework"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sentiment analysis is a crucial task that aims to understand people's emotional states and predict emotional categories based on multimodal information. It consists of several subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). However, unifying all subtasks in sentiment analysis presents numerous challenges, including modality alignment, unified input/output forms, and dataset bias. To address these challenges, we propose a Task-Specific Prompt method to jointly model subtasks and introduce a multimodal generative framework called UniSA. Additionally, we organize the benchmark datasets of main subtasks into a new Sentiment Analysis Evaluation benchmark, SAEval. We design novel pre-training tasks and training methods to enable the model to learn generic sentiment knowledge among subtasks to improve the model's multimodal sentiment perception ability. Our experimental results show that UniSA performs comparably to the state-ofthe-art on all subtasks and generalizes well to various subtasks in sentiment analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sentiment analysis is a discipline that leverages multimodal data to extract human opinions and comments, as well as comprehend and categorize human emotions. Generalized sentiment analysis encompasses a plethora of subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). Initially, research focused solely on individual subtasks; nevertheless, it has become evident that there is an interrelated sentiment knowledge among these subtasks. Hence, integrating all subtasks into a single model to enhance the sentiment understanding ability of the model has emerged as a significant objective. Following the lead of unified multi-task modeling in other domains, recent studies have explored the potential of jointly modeling some subtasks, e.g., Hu et al.  [18]  jointly modeled the ERC and MSA to boost the performance of both tasks, Yan et al.  [70]  converted all ABSA subtasks a unified generative formulation, which yielding encouraging results. Nevertheless, no research has yet been conducted on the joint modeling of all sentiment analysis subtasks (ERC, MSA, ABSA, etc.) as a single research object.\n\nThe unified modeling of all subtasks of sentiment analysis presents three primary challenges: 1) Format. The input formats and analysis views of each subtask vary. For example, MSA analyzes emotional tendencies based on single-turn utterances, ERC comprehensively assesses speaker emotions through contextual information in dialogues, and ABSA extracts attribute words from utterances and judges emotional tendencies based on those words. Jointly training these subtasks, each with different input and output formats, is the first challenge. 2) Alignment. Some subtasks use multimodal data (e.g., ERC), while others use single-modal data (e.g., speech emotion recognition). The data formats and representations for different modalities (text, acoustic, visual, etc.) differ, and each modality expresses emotional information in its unique way. For example, text modality uses sentiment words, attribute words, modal words, and negative words to express sentiments, while the acoustic modality mainly uses acoustic parameters such as intensity, pitch, speaking rate, and pauses to express the speaker's emotional fluctuations.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Sentiment Analysis",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Sentiment Analysis",
      "text": "The drinks are well made , and the service is great.\n\nThe drinks are well made , and the service is great.\n\nThe drinks are well made , and the service is great.\n\nYesterday, a policeman visited the old man with the lost money, and told him that the thief was caught. The old man was very happy.\n\nI feel so tired.\n\nWhat's going on?\n\nSo many work to do.\n\nIts their character not their color that matter.\n\nThey generously gave us a day off.\n\nHe tried to be sworn in as president from prison. The visual modality expresses human emotion through facial expressions, body posture, and eye gaze. Aligning the emotional information across modalities presents the second challenge. 3) Bias.\n\nSentiment analysis is a highly subjective task, and ensuring that the model learns universal human sentiment knowledge while being less affected by subjectivity bias is the third challenge. Additionally, dataset annotation bias may affect the quality of multimodal data with high-quality annotations, making it difficult to train models that can generalize across different datasets.\n\nIn response to the challenges of sentiment analysis, we reorganize the sentiment analysis subtasks into two categories, namely main tasks and downstream tasks, based on their relevance to sentiment. As shown in Figure  1 , the main tasks, which are the subtasks most correlated with human emotional representation, include ABSA, MSA, ERC, and Comment Analysis (CA). Downstream tasks include tasks related to sentiment analysis but not necessarily detecting human emotion categories, such as irony detection, humor detection, and emoji prediction. Meanwhile, we propose a novel multimodal sentiment analysis framework, named UniSA 1 , which takes the first step toward unified modeling of all sentiment analysis main tasks and generalizes to downstream tasks.\n\nSpecifically, to tackle the first challenge of unifying input and output forms across different subtasks, we introduce the task-specific prompt method which treats all subtasks as generation tasks and jointly trains them. The second challenge of aligning emotional information across different modalities is addressed by extending the generative Transformer  [64]  architecture to process multimodal data and proposing the modal mask training method to learn the inter-modality relationship. The third challenge of learning the 1 The UniSA is available at : https://github.com/dawn0815/UniSA difference between subtasks is tackled by introducing dataset embedding to bridge the annotation bias between different datasets. In order to evaluate the performance of our proposed framework, UniSA, we collate benchmark datasets for each subtask and construct a new sentiment analysis benchmark, SAEval, as illustrated in Table  1 . The details of this benchmark are presented in Section 3.\n\nThe contributions of this paper can be summarized as follows:\n\n‚Ä¢ We advance a novel approach to sentiment analysis, UniSA, which unifies all subtasks under a single generative framework. This represents a significant advancement in the field, as no previous work has taken such a comprehensive approach to sentiment analysis. ‚Ä¢ We propose novel sentiment-related pre-training tasks that allow the model to learn generic sentiment knowledge across subtasks. Extensive experimental results demonstrate that UniSA performs comparably to the state-of-the-art on all subtasks. ‚Ä¢ We curate a benchmark dataset, SAEval, which comprises benchmark datasets for various sentiment analysis subtasks in a unified format, enabling comprehensive and fair evaluation of sentiment analysis models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work 2.1 Sentiment Analysis",
      "text": "We provide a brief overview of the recent advancements in various subfields of sentiment analysis. Aspect-based sentiment analysis (ABSA) is a task that aims to identify the sentiment polarity associated with aspect terms in a single-turn utterance. Previous works   [33, 37, 66] . Recent research on ABSA has explored the application of sequence-to-sequence learning and pre-trained language models to achieve promising results  [26, 36] . Multimodal sentiment analysis involves identifying the speaker's emotion from a single-turn utterance by considering multiple modalities. Early research in this area primarily focused on geometric manipulation in feature spaces  [75, 76] . Recent research on multimodal sentiment analysis has emphasized the importance of modal consistency and difference through multi-task joint learning  [74]  or modal translation  [39] , leveraging cross-modality and multi-scale modality representation to implement the modal alignment  [35, 63] .\n\nComment analysis involves identifying the user's emotion from one or more sentences in a comment. Recent works  [57, 68, 72]  in this field have used pre-trained language models, such as BERT  [5] , RoBERTa  [34] , and XLNet  [72] , to fine-tune comment datasets and achieve promising results.\n\nEmotion recognition in conversation aims to identify the speaker's emotion from multiple utterances in a conversation  [8, 31, 51, 60] . Early research focused on context modeling using GRU  [6]  models to extract context information and judge the emotion category of the utterance based on the context information  [9, 12, 17, 40, 49] . More recent research has introduced GCN models  [11]  into conversation scene modeling, where each utterance in the conversation  [32, 80]  is regarded as a node in the graph, and the relationship between utterances constitutes the edge connecting the nodes  [10, 19, 25, 59 ]. The most recent works in this area employ Transformer architecture and self-attention mechanisms to capture contextual information of utterances and achieve state-of-the-art performance in emotion recognition in conversations  [20, 27, 30, 41, 58] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multi-Task Unified Framework",
      "text": "In recent years, multi-task unified architectures have shown great potential and achieved impressive results across various domains  [13-15, 45, 52, 73] . Bao et al.  [1]  presented a unified vision-language pre-trained model that utilizes a modular Transformer network to jointly learn a dual encoder and a fusion encoder. Li et al.  [28]  proposed a unified pre-training architecture that can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Zhang et al.  [81]  proposed a Unified framework for multimodal summarization. In the Named Entity Recognition domain, some works designed unified architectures for various subtasks  [24, 71] . Recently, large multimodal models such as ERNIE Bot  [65]  and GPT-4  [46]  have achieved remarkable results and have attracted attention from researchers in various fields.\n\nIn the sentiment analysis field, Yan et al.  [70]  employed an improved BART  [23]  architecture to solve all ABSA subtasks in an end-to-end framework. Hu et al.  [18]  proposed a multimodal sentiment knowledge-sharing framework that unifies MSA and ERC tasks from features, labels, and models. These multi-task unified works in various fields support the feasibility of unified modeling for all sentiment analysis subtasks. However, there is currently no end-to-end architecture that can model all subtasks of sentiment analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Saeval: The Benchmark",
      "text": "To better evaluate the performance of the model on various sentiment analysis tasks, we formalized the benchmark dataset for the main task and constructed a new benchmark, SAEval  2  . This section provides a description of the datasets that constitute the SAEval benchmark and the evaluation metrics used.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "As presented in Table  1 , the SAEval benchmark includes several datasets from different subtasks of sentiment analysis:\n\n‚Ä¢ SemEval-2014  [48]  and SemEval-2016  [47]  are subtasks of the Semeval Aspect-based Sentiment Analysis challenge. The goal of these subtasks is to identify the sentiment polarity (positive, negative, neutral, conflict) corresponding to all attribute words contained in each sentence.\n\n‚Ä¢ MOSI  [77]  and MOSEI  [78]   datasets is to predict the sentiment score, which is a continuous value ranging from -3 to +3, of single-turn utterances by incorporating multiple modalities.\n\n‚Ä¢ IEMOCAP  [4]  and MELD  [50]  are both datasets for emotion recognition in conversations using multimodal information.\n\nThe SAEval benchmark uses these datasets to identify the emotion category of each utterance based on the multimodal information and context available.\n\n‚Ä¢ EmoryNLP  [79] , DailyDialog  [29] , and EmoWOZ  [7]  are datasets for textual emotion recognition in conversation. The goal of SAEval for these datasets is to identify the emotion category of utterances based on textual information and context. ‚Ä¢ SST-2  [61] , IMDB  [38] , and Amazon Review  [44]  are datasets for comment analysis. The goal of SAEval for these datasets is to identify the sentiment polarity of comments. It is important to note that Amazon Review is only used for the pre-training phase and does not have a test set for evaluation in SAEval.\n\nAll datasets are unified and stored in a dictionary format 3 . The dictionary includes keywords, such as \"Task Type\", \"Dataset ID\", \"Text\", \"Audio\", and \"Image\". For ERC datasets, additional information such as \"Context\", \"Speaker ID\", and \"Utterance index\" are included to determine the conversation information of the current query.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "SAEval uses the same evaluation metrics as the original tasks for each subtask. Weighted Accuracy (WA) is used for aspect-based sentiment analysis and comment analysis. Mean Absolute Error (MAE), 7-category Accuracy (ACC-7), and 2-category Accuracy (ACC-2) 3 Please see Appendix A.1 for formatted samples are used for multimodal sentiment analysis. WA and weighted F1 score (WF1) are used for emotion recognition in conversation.\n\nAs the percentage of neutral categories in the DailyDialog dataset is more than 90%, this dataset is usually measured with neutral categories removed by default, and macro-averaged F1 (MF1) is used as the measure.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "The architecture of our UniSA is depicted in Figure  2 . We adopt a generative Transformer architecture to unify all subtasks of sentiment analysis into generation tasks. Concretely, to handle the cross-modality inputs of visual, acoustic, and text, we modify the original Transformer encoder to a multimodal encoder and introduce a Modal Mask Training method. This method enables the model to learn the relationship between different modalities effectively. We also propose a Task-Specific Prompt method to standardize the input format of all subtasks. Furthermore, to address the bias between datasets, we incorporate a dataset embedding in the input to differentiate between different datasets. This technique helps the model to better understand the characteristics of each dataset and improves its performance on all tasks. We will elaborate on each of them sequentially.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Formulation",
      "text": "The unified sentiment analysis subtasks aim to process arbitrary data from different modalities, such as text, acoustic, and visual, and output emotion predict results corresponding to a specific subtask. The subtask set includes ABSA, MSA, ERC, and CA, while the modality set includes ùëö ùë° , ùëö ùëé , and ùëö ùë£ , corresponding to text, acoustic, and visual modalities, respectively. The task involves both multimodal situations, such as MSA and ERC, as well as unimodal situations, such as ABSA and CA. Moreover, the task must also consider multimodal situations where one or more modalities are missing. Our target is to build a multimodal emotion-aware frame-",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task-Specific Prompt",
      "text": "To jointly model different subtasks, we propose Task-Specific Prompt to unify the input streams of all subtasks, and transform all subtasks into generative tasks to unify the output form of subtasks. Task-Specific Prompt comprises three components: task identifier ùëç , answer set ùëå , and input streams ùëã . The template ùêø can be represented as:\n\nAs illustrated in Figure  3 , the task identifier ùëç is made up of special tokens, including < ùë°ùëéùë†ùëò_ùë°ùë¶ùëùùëí >, < ùëëùëéùë°ùëé_ùëñùëë >, < ùë†ùëùùëíùëéùëòùëíùëü _ùëñùëë >, etc., which distinguish different subtasks, datasets, and speakers (in conversation). The answer set ùëå is a specific set of labels for each dataset that guides the model in generating the expected results. The input streams ùëã indicate the text, acoustic, visual, and context of the inputs. Task-Specific Prompt standardizes the input form and guides the model to generate task-specific results according to the answer set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modal Mask Training",
      "text": "As the text modality data is abundant in each subtask of sentiment analysis, while the multimodal data is limited, we propose a Modal Mask Training method to enhance the model's multimodal emotion perception capability. Specifically, given a multimodal input signal ùêº ùëñ = ùêº ùë° ùëñ , ùêº In this way, multimodal data has four different input forms in the training phase, which extends the modal diversity of the training data and can effectively cope with the absence of some modalities in multiple data in real scenarios. This method allows the model to learn the relationship between different modalities effectively and improve its performance on multimodal sentiment analysis tasks where the data is limited.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset Embedding",
      "text": "To reduce the impact of dataset bias on the model performance, we propose the use of dataset embedding. This technique transforms the dataset indexes into one-hot embeddings, allowing the model to distinguish between different datasets. As sentiment analysis is a subjective task, people's emotional responses to the same sentence can vary. Additionally, different datasets may be labeled by different annotators, introducing subjective bias between datasets. The dataset embedding helps mitigate this bias, allowing the model to better generalize across different datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Transformer Encoder",
      "text": "The encoder of our model is based on a multi-layer bidirectional Transformer, which is similar to the architecture used by Xing et al.  [69] . We adopt a single-stream architecture to jointly train the inputs of different modalities. This architecture allows the model to effectively capture the interactions and dependencies between different modalities and improve its overall performance on all tasks. For visual modality, we use pre-trained MobileNet  [16]  extracted features as visual embedding, for acoustic modality, we use librosa  [42]  extracted fbank features as acoustic embedding; and for text modality, we follow the setting of the BART model  [23] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pre-Training Tasks",
      "text": "To improve the emotional perception ability of the model, we propose novel pre-training tasks and divide the pre-training into two stages to guide the model to learn general emotional knowledge gradually.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mask Context Modeling.",
      "text": "We extend the Masked Language Modeling  [5]  to a multimodal input and context mask for the current query, named Mask Context Modeling (MCM).MCM can randomly mask tokens from the input text, acoustic, visual, and context modalities to encourage the model to learn to predict the masked tokens. To promote better generalization, we increase the mask probability to 50%. We denote the mask indices by 1 ‚â§ ùëö ‚â§ ùëÄ, where ùëÄ is the number of masked tokens. We denote the masked token by ùë§ ùëö , and the remaining tokens that are not masked by ùë§ ùëõ . The loss function for MCM is defined as:\n\nwhere ùëÉ ùúÉ denotes the output distribution of the model, and ùúÉ represents model parameters to be optimized.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sentiment Polarity Prediction.",
      "text": "To encourage the model to learn to distinguish between different sentiment categories, we transform the fine-grained emotion labels of the datasets into sentiment polarity labels by mapping them to positive, negative, and neutral categories. Then, in the Sentiment Polarity Prediction (SPP) task, we train the model to predict the sentiment polarity category of the input. The loss function for SPP is defined as: In this task, we take the output of the encoder as a representation of each sample and compute the Euclidean distance between samples with the same sentiment label in a batch.\n\nWe then maximize the similarity between samples with the same sentiment label. The loss function for CCL is defined as:\n\nwhere ùëè denotes the batch size, ùëëùëñùë†ùë°ùëéùëõùëêùëí ( ùëó, ùëò) denotes the Euclidean distance between sample ùëó and sample ùëò, and ùëöùëéùë†ùëò ( ùëó, ùëò) is 1 when sample ùëó and sample ùëò have the same sentiment label, and 0 otherwise.\n\n4.6.4 Cross-task Emotion Prediction. In the Cross-task Emotion Prediction (CEP) task, we first take the output of the encoder as the representation of each sample and cluster the samples for each subtask. Then, for each sample, we calculate the distance between its representation and each label cluster of each subtask. We take the label corresponding to the cluster with the smallest distance as the pseudo label of the sample for each subtask. Given a subtask set ùê∑ = {ùê¥ùêµùëÜùê¥, ùëÄùëÜùê¥, ùê∏ùëÖùê∂, ùê∂ùê¥}, each sample will have four labels: one for the original label and three for the cross-task pseudo labels. The loss function for CEP is defined as:\n\nwhere ùê∏ ùëë denotes the emotion label in subtask ùëë ‚àà ùê∑.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments 5.1 Baseline",
      "text": "In this section, we reported the current SOTA models for each dataset in the SAEval benchmark, which we use as baselines to compare with the performance of UniSA on each subtask. These SOTA models are described as follows:\n\n‚Ä¢ UniMSE  [18] : UniMSE is a multimodal sentiment knowledgesharing framework that unifies MSA and ERC tasks from features, labels, and models. It is the current SOTA model for MOSI  [77]  and MOSEI  [78] .\n\n‚Ä¢ EmoCaps  [30] : EmoCaps is a multimodal framework for conversational emotion recognition, which proposes the multimodal emotion vector to characterize the emotional tendencies of the utterance itself. It is the current SOTA model for IEMOCAP  [4] . ‚Ä¢ SPCL-CL-ERC  [62] : SPCL-CL-ERC is a model that combines Supervised Prototypical Contrastive Learning and curriculum learning strategy to address imbalanced classification problem in conversational emotion recognition. It is the current SOTA model for EmoryNLP  [79]  and MELD  [50] .\n\n‚Ä¢ CoMPM  [22] : CoMPM is a model for conversational emotion recognition, which combines the speaker's pre-trained memory with the context model and finds that the pre-trained In addition, the pre-trained models fine-tuned on specific datasets have achieved promising results: BERT  [5]  for EmoWoz  [7] ; T5  [54]  for SST-2  [61] ; XLNet  [72]  for IMDB  [38] ; InstructABSA  [72]  for SemEval-2014  [48]  and SemEval-2016  [47] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We explored three generative architectures, GPT-2  [53] , T5  [54] , and BART  [23] , as the backbone of our UniSA, and determined the optimal model through comparative experiments. The acoustic and visual representations have a hidden dimension of 64, while the textual embedding size is 768. The batch size is set to 64, and the learning rate is 5e-6 for BART-base, while it is 5e-5 for T5-base and GPT2-medium. Further details can be found in Table  2 .\n\nTo evaluate the performance of the model on the primary tasks of sentiment analysis, we took the pre-trained UniSA, which undergoes two pre-training phases and conducted joint fine-tuning on the datasets in SAEval. During multi-task joint fine-tuning, the model may overfit on some tasks and underfit on others due to the varying learning difficulties across subtasks and inconsistent gradient directions between subtasks. To mitigate this issue, we proposed task-average sampling during the fine-tuning phase, which enables the model to learn the gradient of each task equally during each iteration. Specifically, we split the dataset into various task pools based on the task type, shuffled them, and distributed the same number of samples from each task pool equally for each step.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Analysis",
      "text": "The experimental results of our UniSA and the existing SOTA models on the SAEval benchmark are presented in Table  3 , where vacant cells indicate that the models were not fine-tuned on the corresponding datasets. We initially fine-tuned all datasets without pre-training stages using GPT2, T5, and BART as backbones. The results show that GPT-2 with a multimodal encoder (ùëà ùëõùëñùëÜùê¥ ùê∫ùëÉùëá 2 ) has limited performance. Furthermore, we observed that BART (ùëà ùëõùëñùëÜùê¥ ùêµùê¥ùëÖùëá ) As shown in Table  3 , our proposed UniSA model performs comparably to the existing SOTA models for each dataset. Although UniSA is unable to outperform existing SOTA models on various benchmark datasets, it demonstrates the feasibility of uniformly modeling all sentiment analysis subtasks. Moreover, constrained by specific tasks and modalities, these SOTA models cannot be effectively generalized to other subtasks, whereas UniSA is an allin-one model that can perform all sentiment analysis subtasks with a relatively small number of parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We used BART as the backbone of UniSA and conducted a series of ablation studies on the MOSI, MOSEI, IEMOCAP, and MELD datasets. Our ablation experiments aimed to investigate the effectiveness of our proposed methods and are as follows: 1) We replaced the proposed task specifical prompt method with task tokens to verify its effects on model performance. 2) We removed the modal mask training method to verify its impact on model performance. 3) To explore the impact of different input forms on model performance, we introduced three additional LSTMs as encoders for audio, image, and context inputs. 4) We validated the effectiveness of each pre-training stage. 5) We eliminated the acoustic and visual modalities from the multimodal signals to investigate their effects on model performance.\n\nThe experimental results are shown in Table  4 , where S denotes the sequences of all modalities input to a single encoder, while E denotes extended LSTMs as encoders for acoustic and visual modalities. P denotes Task-Specific Prompt, F denotes Modal Mask Training, C denotes additional LSTMs as an encoder for context, and T1 and T2 denote the first and second stages of pre-training, respectively. The results of the ablation experiments demonstrate the effectiveness of our proposed methods.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Few-Shot Generalization",
      "text": "To demonstrate the generalizability of our proposed UniSA on various subtasks, we conducted experiments on several downstream tasks under the few-shot setting. We reported the SOTA scores of several downstream datasets, such as the SemEval2019 Hateval challenge  [3] , Sarcasmania  [21] , Semeval2017 Sentiment Analysis Challenge  [56] , Semeval2018 Emotion Recognition  [43] , Semeval2018 Emoji Prediction challenge  [2] , and performed few-shot experiments with a criterion of 150 samples/category.\n\nThe experimental results in Table  5  demonstrate that our UniSA, under low-resource settings, performs well on each downstream dataset. The results reveal that UniSA has learned emotional knowledge common across subtasks during the pre-training stages and thus shows good generalization on various sentiment analysis subtasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitation Discussion",
      "text": "Error analysis in Appendix 4  reveals that the subjective bias among datasets is one of the factors limiting UniSA's performance. However, our experiments also suggest that there are other reasons for UniSA's limitations. One of these reasons is the performance limitations of the backbone models, which can significantly improve with more parameters and learned data  [67] . Additionally, the lack of multimodal datasets poses another challenge for UniSA's performance. To address this, we encourage researchers to add more baseline datasets into SAEval and promote the development of multi-task unified modeling for sentiment analysis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper, we have proposed a new benchmark for Sentiment Analysis Evaluation, SAEval, and developed a multimodal generative framework named UniSA to recouple various subtasks of sentiment analysis. To overcome the challenges of unifying multitasks, we have introduced the Task Specifical Prompt method and proposed novel pre-training tasks and training methods to improve the model's multimodal sentiment perception ability. Our extensive experiments have demonstrated the good generalizability of UniSA. We have also analyzed the bias between datasets and identified the limited performance of unified modeling for sentiment analysis subtasks.\n\nIn the future, we plan to expand our experiments by applying UniSA to more benchmark datasets and emotion-related tasks. We also intend to explore larger architectures as backbones for UniSA and introduce more pre-training data to improve its performance in various sentiment analysis tasks. Our work represents a step forward in the unified modeling of sentiment analysis subtasks, and we hope that it will inspire future research in this direction.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Subtasks of sentiment analysis are categorized into main tasks and downstream tasks based on their relevance to the",
      "page": 2
    },
    {
      "caption": "Figure 1: , the main tasks, which are the",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of UniSA: a novel approach for unified sentiment analysis modeling.",
      "page": 4
    },
    {
      "caption": "Figure 2: We adopt a",
      "page": 4
    },
    {
      "caption": "Figure 3: , the task identifier ùëçis made up of spe-",
      "page": 5
    },
    {
      "caption": "Figure 3: An example of our Task-Specifical Prompt method.",
      "page": 6
    },
    {
      "caption": "Figure 4: An example of the MELD data in SAEval.",
      "page": 11
    },
    {
      "caption": "Figure 5: An example of the IMDB data in SAEval.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MOSI\nMAE ‚Üì\nACC-7 ‚Üë\nACC-2 ‚Üë": "0.691\n48.68\n85.85\n1.79\n20.69\n44.75\n1.06\n28.86\n69.67\n0.9192\n41.39\n78.86\n0.9439\n29.59\n74.63\n0.9034\n41.25\n78.13\n0.8908\n40.52\n77.55\n0.8527\n42.85\n78.42\n0.9441\n37.17\n78.71\n0.7422\n48.54\n84.11",
          "MOSEI\nMAE ‚Üì\nACC-7 ‚Üë\nACC-2 ‚Üë": "0.523\n54.39\n85.86\n1.006\n41.31\n71.02\n0.6226\n47.82\n84.71\n0.6232\n47.77\n82.20\n0.5712\n52.58\n84.88\n0.5706\n52.65\n85.85\n0.5716\n52.17\n85.16\n0.5544\n53.12\n85.81\n0.5436\n52.62\n85.59\n0.5866\n50.03\n84.93",
          "MELD\nWA ‚Üë\nWF1 ‚Üë": "67.85\n66.71\n46.59\n31.37\n61.72\n59.49\n63.90\n62.32\n66.20\n64.19\n63.83\n63.12\n63.98\n63.45\n62.60\n61.03\n62.64\n61.14\n62.34\n62.22",
          "IEMOCAP\nWA ‚Üë\nWF1 ‚Üë": "70.56\n71.77\n20.22\n60.27\n60.11\n59.64\n56.47\n53.06\n63.74\n63.60\n57.45\n56.68\n54.87\n53.68\n57.15\n56.97\n65.10\n64.67\n64.24\n64.46"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vlmo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts",
      "authors": [
        "Hangbo Bao",
        "Wenhui Wang",
        "Li Dong",
        "Qiang Liu",
        "Owais Khan Mohammed",
        "Kriti Aggarwal",
        "Subhojit Som",
        "Songhao Piao",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "Semeval 2018 Task 2: Multilingual Emoji Prediction",
      "authors": [
        "Francesco Barbieri",
        "Jose Camacho-Collados",
        "Francesco Ronzano",
        "Luis Espinosa-Anke",
        "Miguel Ballesteros",
        "Valerio Basile",
        "Viviana Patti",
        "Horacio Saggion"
      ],
      "year": "2018",
      "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "3",
      "title": "SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter",
      "authors": [
        "Cristina Valerio Basile",
        "Elisabetta Bosco",
        "Debora Fersini",
        "Viviana Nozza",
        "Francisco Patti",
        "Manuel Rangel",
        "Paolo Pardo",
        "Manuela Rosso",
        "Sanguinetti"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive Emotional Dyadic Motion Capture Database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "6",
      "title": "Gate-Variants of Gated Recurrent Unit Neural Networks",
      "authors": [
        "Rahul Dey",
        "M Fathi",
        "Salem"
      ],
      "year": "2017",
      "venue": "2017 IEEE 60th International Midwest Symposium on Circuits and Systems"
    },
    {
      "citation_id": "7",
      "title": "EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems",
      "authors": [
        "Shutong Feng",
        "Nurul Lubis",
        "Christian Geishauser",
        "Hsien-Chin Lin",
        "Michael Heck",
        "Carel Van Niekerk",
        "Milica Gasic"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised Dialogue Topic Segmentation with Topic-aware Contrastive Learning",
      "authors": [
        "Haoyu Gao",
        "Rui Wang",
        "Ting-En Lin",
        "Yuchuan Wu",
        "Min Yang",
        "Fei Huang",
        "Yongbin Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "9",
      "title": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "10",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Inductive Representation Learning on Large Graphs",
      "authors": [
        "Will Hamilton",
        "Zhitao Ying",
        "Jure Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding",
      "authors": [
        "Wanwei He",
        "Yinpei Dai",
        "Binyuan Hui",
        "Min Yang",
        "Zheng Cao",
        "Jianbo Dong",
        "Fei Huang",
        "Luo Si",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Unified Dialog Model Pre-Training for Task-Oriented Dialog Understanding and Generation",
      "authors": [
        "Wanwei He",
        "Yinpei Dai",
        "Min Yang",
        "Jian Sun",
        "Fei Huang",
        "Luo Si",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "15",
      "title": "Galaxy: A Generative Pre-Trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection",
      "authors": [
        "Wanwei He",
        "Yinpei Dai",
        "Yinhe Zheng",
        "Yuchuan Wu",
        "Zheng Cao",
        "Dermot Liu",
        "Peng Jiang",
        "Min Yang",
        "Fei Huang",
        "Li Si",
        "Luo"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "authors": [
        "Menglong Andrew G Howard",
        "Bo Zhu",
        "Dmitry Chen",
        "Weijun Kalenichenko",
        "Tobias Wang",
        "Marco Weyand",
        "Hartwig Andreetto",
        "Adam"
      ],
      "year": "2017",
      "venue": "Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "17",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "EmoBerta: Speaker-aware Emotion Recognition in Conversation with Roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "EmoBerta: Speaker-aware Emotion Recognition in Conversation with Roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "21",
      "title": "ChatGPT: Jack of all trades, master of none",
      "authors": [
        "Jan Koco≈Ñ",
        "Igor Cichecki",
        "Oliwier Kaszyca",
        "Mateusz Kochanek",
        "Dominika Szyd≈Ço",
        "Joanna Baran",
        "Julita Bielaniewicz",
        "Marcin Gruza",
        "Arkadiusz Janz",
        "Kamil Kanclerz"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion"
    },
    {
      "citation_id": "23",
      "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Veselin Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Unified Named Entity Recognition as Word-Word Relation Classification",
      "authors": [
        "Jingye Li",
        "Hao Fei",
        "Jiang Liu",
        "Shengqiong Wu",
        "Meishan Zhang",
        "Chong Teng",
        "Donghong Ji",
        "Fei Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2022",
      "venue": "GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition",
      "arxiv": "arXiv:2207.12261"
    },
    {
      "citation_id": "26",
      "title": "Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation",
      "authors": [
        "Kun Li",
        "Chengbo Chen",
        "Xiaojun Quan",
        "Qing Ling",
        "Yan Song"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Contrast and Generation Make Bart a Good Dialogue Emotion Recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning",
      "authors": [
        "Wei Li",
        "Can Gao",
        "Guocheng Niu",
        "Xinyan Xiao",
        "Hao Liu",
        "Jiachen Liu",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Dailydialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A Manually Labelled Multi-turn Dialogue Dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "30",
      "title": "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "31",
      "title": "Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems",
      "authors": [
        "Yuchuan Ting-En Lin",
        "Fei Wu",
        "Luo Huang",
        "Jian Si",
        "Yongbin Sun",
        "Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "32",
      "title": "Discovering New Intents via Constrained Deep Adaptive Clustering with Cluster Refinement",
      "authors": [
        "Hua Ting-En Lin",
        "Hanlei Xu",
        "Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Attention Modeling for Targeted Sentiment",
      "authors": [
        "Jiangming Liu",
        "Yue Zhang"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference of the European Chapter"
    },
    {
      "citation_id": "34",
      "title": "Roberta: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "35",
      "title": "Scalevlad: Improving Multimodal Sentiment Analysis via Multi-scale Fusion of Locally Descriptors",
      "authors": [
        "Huaishao Luo",
        "Lei Ji",
        "Yanyong Huang",
        "Bin Wang",
        "Shenggong Ji",
        "Tianrui Li"
      ],
      "year": "2021",
      "venue": "Scalevlad: Improving Multimodal Sentiment Analysis via Multi-scale Fusion of Locally Descriptors",
      "arxiv": "arXiv:2112.01368"
    },
    {
      "citation_id": "36",
      "title": "Exploring Sequence-to-Sequence Learning in Aspect Term Extraction",
      "authors": [
        "Dehong Ma",
        "Sujian Li",
        "Fangzhao Wu",
        "Xing Xie",
        "Houfeng Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification",
      "authors": [
        "Dehong Ma",
        "Sujian Li",
        "Xiaodong Zhang",
        "Houfeng Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Learning Word Vectors for Sentiment Analysis",
      "authors": [
        "Andrew Maas",
        "Raymond Daly",
        "Peter Pham",
        "Dan Huang",
        "Andrew Ng",
        "Christopher Potts"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "39",
      "title": "Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "41",
      "title": "DialogueTRM: Exploring the Intra-and Inter-modal Emotional Behaviors in the Conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "DialogueTRM: Exploring the Intra-and Inter-modal Emotional Behaviors in the Conversation",
      "arxiv": "arXiv:2010.07637"
    },
    {
      "citation_id": "42",
      "title": "Librosa: Audio and Music Signal Analysis in Python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Eric Mcvicar",
        "Oriol Battenberg",
        "Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th Python in Science Conference"
    },
    {
      "citation_id": "43",
      "title": "Semeval-2018 Task 1: Affect in Tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez",
        "Mohammad Salameh",
        "Svetlana Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "44",
      "title": "Justifying Recommendations Using Distantly-Labeled Reviews and Fine-Grained Aspects",
      "authors": [
        "Jianmo Ni",
        "Jiacheng Li",
        "Julian Mcauley"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "45",
      "title": "Search-oriented micro-video captioning",
      "authors": [
        "Liqiang Nie",
        "Leigang Qu",
        "Dai Meng",
        "Min Zhang",
        "Qi Tian",
        "Alberto Bimbo"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "47",
      "title": "Semeval-2016 Task 5: Aspect based Sentiment Analysis",
      "authors": [
        "Maria Pontiki",
        "Dimitris Galanis",
        "Haris Papageorgiou",
        "Ion Androutsopoulos",
        "Suresh Manandhar",
        "Al-Smadi Mohammed",
        "Mahmoud Al-Ayyoub",
        "Yanyan Zhao",
        "Bing Qin",
        "Orph√©e De Clercq"
      ],
      "year": "2016",
      "venue": "ProWorkshop on Semantic Evaluation"
    },
    {
      "citation_id": "48",
      "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis",
      "authors": [
        "Maria Pontiki",
        "Dimitris Galanis",
        "John Pavlopoulos",
        "Harris Papageorgiou",
        "Ion Androutsopoulos",
        "Suresh Manandhar"
      ],
      "year": "2014",
      "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "49",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "50",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "51",
      "title": "Empathetic Response Generation via Emotion Cause Transition Graph",
      "authors": [
        "Yushan Qian",
        "Bo Wang",
        "Ting-En Lin",
        "Yinhe Zheng",
        "Ying Zhu",
        "Dongming Zhao",
        "Yuexian Hou",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2023",
      "venue": "Empathetic Response Generation via Emotion Cause Transition Graph",
      "arxiv": "arXiv:2302.11787"
    },
    {
      "citation_id": "52",
      "title": "Dynamic modality interaction modeling for image-text retrieval",
      "authors": [
        "Leigang Qu",
        "Meng Liu",
        "Jianlong Wu",
        "Zan Gao",
        "Liqiang Nie"
      ],
      "year": "2021",
      "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "53",
      "title": "Language Models Are Unsupervised Multitask Learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "54",
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "55",
      "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
      "authors": [
        "Marco Tulio Ribeiro",
        "Tongshuang Wu",
        "Carlos Guestrin",
        "Sameer Singh"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "56",
      "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter",
      "authors": [
        "Sara Rosenthal",
        "Noura Farra",
        "Preslav Nakov"
      ],
      "year": "2017",
      "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "57",
      "title": "Revisiting LSTM Networks for Semi-supervised Text Classification via Mixed Objective Function",
      "authors": [
        "Devendra Singh Sachan",
        "Manzil Zaheer",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "58",
      "title": "DialogXL: Allin-one XLNet for Multi-party Conversation Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "59",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "60",
      "title": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains",
      "authors": [
        "Shuzheng Si",
        "Wentao Ma",
        "Yuchuan Wu",
        "Yinpei Dai",
        "Haoyu Gao",
        "Hangyu Ting-En Lin",
        "Rui Li",
        "Fei Yan",
        "Yongbin Huang",
        "Li"
      ],
      "year": "2023",
      "venue": "SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue in Multiple Domains",
      "arxiv": "arXiv:2305.13040"
    },
    {
      "citation_id": "61",
      "title": "Recursive Deep Models for Semantic Compositionality over A Sentiment Treebank",
      "authors": [
        "Richard Socher",
        "Alex Perelygin",
        "Jean Wu",
        "Jason Chuang",
        "Christopher Manning",
        "Andrew Ng",
        "Christopher Potts"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "62",
      "title": "Supervised Prototypical Contrastive Learning for Emotion Recognition in Conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "63",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "64",
      "title": "Attention is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "65",
      "title": "Exploring Larger-Scale Knowledge Enhanced Pre-Training for Language Understanding and Generation",
      "authors": [
        "Shuohuan Wang",
        "Yu Sun",
        "Yang Xiang",
        "Zhihua Wu",
        "Siyu Ding",
        "Weibao Gong",
        "Shikun Feng",
        "Junyuan Shang",
        "Yanbin Zhao",
        "Chao Pang"
      ],
      "year": "2021",
      "venue": "Exploring Larger-Scale Knowledge Enhanced Pre-Training for Language Understanding and Generation",
      "arxiv": "arXiv:2112.12731"
    },
    {
      "citation_id": "66",
      "title": "Attention-based LSTM for Aspect-level Sentiment Classification",
      "authors": [
        "Yequan Wang",
        "Minlie Huang",
        "Xiaoyan Zhu",
        "Li Zhao"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "67",
      "title": "Emergent Abilities of Large Language Models",
      "authors": [
        "Jason Wei",
        "Yi Tay",
        "Rishi Bommasani",
        "Colin Raffel",
        "Barret Zoph",
        "Sebastian Borgeaud",
        "Dani Yogatama",
        "Maarten Bosma",
        "Denny Zhou",
        "Donald Metzler"
      ],
      "year": "2022",
      "venue": "Emergent Abilities of Large Language Models",
      "arxiv": "arXiv:2206.07682"
    },
    {
      "citation_id": "68",
      "title": "Unsupervised Data Augmentation for Consistency Training",
      "authors": [
        "Qizhe Xie",
        "Zihang Dai",
        "Eduard Hovy",
        "Thang Luong",
        "Quoc Le"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "69",
      "title": "KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation",
      "authors": [
        "Yiran Xing",
        "Zai Shi",
        "Zhao Meng",
        "Gerhard Lakemeyer",
        "Yunpu Ma",
        "Roger Wattenhofer"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "70",
      "title": "A Unified Generative Framework for Aspect-based Sentiment Analysis",
      "authors": [
        "Hang Yan",
        "Junqi Dai",
        "Tuo Ji",
        "Xipeng Qiu",
        "Zheng Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "71",
      "title": "A Unified Generative Framework for Various NER Subtasks",
      "authors": [
        "Hang Yan",
        "Tao Gui",
        "Junqi Dai",
        "Qipeng Guo",
        "Zheng Zhang",
        "Xipeng Qiu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "72",
      "title": "Xlnet: Generalized Autoregressive Pretraining for Language Understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "73",
      "title": "Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment",
      "authors": [
        "Tianshu Yu",
        "Haoyu Gao",
        "Ting-En",
        "Min Lin",
        "Yuchuan Yang",
        "Wentao Wu",
        "Chao Ma",
        "Fei Wang",
        "Yongbin Huang",
        "Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "74",
      "title": "Learning Modality-specific Representations with Self-supervised Multi-task Learning for Multimodal Sentiment Analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "75",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "76",
      "title": "Memory Fusion Network for Multi-view Sequential Learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "77",
      "title": "Mosi: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "78",
      "title": "Multimodal Language Analysis in the Wild: Cmu-Mosei Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "79",
      "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2017",
      "venue": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "80",
      "title": "A Slot Is Not Built in One Utterance: Spoken Language Dialogs with Sub-Slots",
      "authors": [
        "Sai Zhang",
        "Yuwei Hu",
        "Yuchuan Wu",
        "Jiaman Wu",
        "Yongbin Li",
        "Jian Sun",
        "Caixia Yuan",
        "Xiaojie Wang"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "81",
      "title": "UniMS: A Unified Framework for Multimodal Summarization with Knowledge Distillation",
      "authors": [
        "Zhengkun Zhang",
        "Xiaojun Meng",
        "Yasheng Wang",
        "Xin Jiang",
        "Qun Liu",
        "Zhenglu Yang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}