{
  "paper_id": "2507.12889v1",
  "title": "Camera-Based Implicit Mind Reading By Capturing Higher-Order Semantic Dynamics Of Human Gaze Within Environmental Context",
  "published": "2025-07-17T08:17:35Z",
  "authors": [
    "Mengke Song",
    "Yuge Xie",
    "Qi Cui",
    "Luming Li",
    "Xinyu Liu",
    "Guotao Wang",
    "Chenglizhao Chen",
    "Shanchen Pang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition, as a step toward mind reading, seeks to infer internal states from external cues. Most existing methods rely on explicit signals -such as facial expressions, speech, or gestures -that reflect only bodily responses and overlook the influence of environmental context. These cues are often voluntary, easy to mask, and insufficient for capturing deeper, implicit emotions. Physiological signal-based approaches offer more direct access to internal states but require complex sensors that compromise natural behavior and limit scalability. Gaze-based methods typically rely on static fixation analysis and fail to capture the rich, dynamic interactions between gaze and the environment, and thus cannot uncover the deep connection between emotion and implicit behavior. To address these limitations, we propose a novel camera-based, user-unaware emotion recognition approach that integrates gaze fixation patterns with environmental semantics and temporal dynamics. Leveraging standard HD cameras, our method unobtrusively captures users' eye appearance and head movements in natural settings -without the need for specialized hardware or active user participation. From these visual cues, the system estimates gaze trajectories over time and space, providing the basis for modeling the spatial, semantic, and temporal dimensions of gaze behavior. This allows us to capture the dynamic interplay between visual attention and the surrounding environment, revealing that emotions are not merely physiological responses but complex outcomes of human-environment interactions. The proposed approach enables user-unaware, real-time, and continuous emotion recognition, offering high generalizability and low deployment cost. Experimental results demonstrate that our method improves accuracy by 13% over traditional gaze-based techniques and exceeds physiological signal-based approaches by 2.7% in specific scenarios, validating the powerful potential of gaze-environment interaction modeling for implicit emotion recognition in real-world applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A Novel Paradigm For Emotion Recognition",
      "text": "Our groundbreaking contribution to the field is a new paradigm that transforms ordinary HD cameras into mind-reading devices. To address the challenges in emotion recognition, we propose a method that combines the strengths of existing approaches while overcoming their limitations. Ordinary HD cameras, which are inexpensive, easy to deploy, and widely available, could offer a promising alternative. Thus, we envision that by enabling users to interact freely with their environment, can we leverage external human-environment interactions to gain insights into internal emotions (Figure  2 -a)? To implement this, we must first identify what information an HD camera can capture from the user. While facial expressions are often unreliable for interpreting ambiguous or deceptive emotions, gaze information presents a valuable alternative. Studies have shown that gaze patterns are closely linked to emotional states, revealing indicators such as interest, stress, and cognitive engagement. This makes gaze a useful, though indirect, signal for understanding emotions. Despite its advantages -particularly its \"user-unaware\" nature, meaning no wearable devices or active user participation are required -gaze (eye movement)based emotion recognition has limitations. Gaze patterns primarily capture the user's line of sight and fixation behavior, but they do not directly reflect deeper emotional responses or motivations. For instance, the duration or frequency of gazing at an object may not accurately correlate with emotional intensity, as cognitive processes and external factors can influence gaze. Consequently, the relationship between gaze and emotion remains indirect, making it challenging to draw precise emotional conclusions from gaze data alone.\n\nBuilding on these theoretical insights, we present a revolutionary emotion recognition framework that effectively \"reads minds\" through camera-based fixations that combines eye gaze patterns with environmental context (Figure  1-b-Method 4 ). This approach leverages the dynamic interplay between a user's visual attention and their surroundings to provide deeper insights into emotional states. To implement this, we developed a \"userunaware (users are not required to wear any devices and remain unaware that their data is being collected or that they are being monitored)\" gaze tracking method that eliminates the need for specialized eye-tracking devices (Figure  2-b ). Using commonly available HD cameras, this method captures gaze points in natural, unconstrained settings and maps them onto a gaze fixation scanpath through multi-angle observations of eye appearance and head movements (Figure  2-c ). Crucially, it ensures that users remain unaware of the monitoring process, making it ideal for unobtrusive and continuous emotion monitoring. A major challenge, however, is that eye appearance is highly subjective (Figure  2-d ). Factors such as variations in gaze angles, individual differences in eye features like sclera visibility and iris size, and the complexity of 3D eye movements make it difficult to achieve consistent and accurate tracking. Our solution to this challenge -an online personalized calibration method -represents a significant ad-vance in making mind-reading technology practical and accurate in real-world environments. We incorporate this approach (see Figure  3 -a & Methods -Online Personalized Calibration) that integrates subjective fixation (user-specific gaze tendencies) with objective fixation (scene-based salient points). This adaptive approach dynamically adjusts to individual differences, significantly enhancing gaze mapping accuracy and adaptability.\n\nThe core innovation of our work is the Semantic Interactive Orders (SIO) framework, which decodes the language of human gaze to reveal internal emotional states. Using the newly developed gaze tracking method (Figure  2-(c )), we collect raw eye fixation points. Unlike existing approaches that rely solely on gaze coordinates, which have demonstrated low accuracy in emotion recognition (Figure  1    3 )). By mapping fixation patterns to meaningful objects and their contextual interactions over time, this framework provides a richer representation of gaze behavior, significantly enhancing the accuracy and robustness of emotion recognition. Emphasizing a user-centric approach, the method is designed to ensure data security and anonymity, aligning with privacy regulations and addressing concerns in sensitive contexts.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Statistics For Validating The Proposed Method'S Effectiveness",
      "text": "We evaluate the performance of our proposed method on three datasets.\n\nOur 2D and 360-degree screen datasets. The variability in modalities across current datasets poses challenges for direct performance comparisons. To address this, we developed a new dataset EmoGaze2D-50 inspired by the SEED-IV dataset  47  , which includes EEG and eye movement data, but with an expanded fo- Comparatively, EmoGazeNet's overall performance is only marginally less than that of EEG-based approaches, with a slight 1.26% deficit. However, when it comes to recognizing the emotions of \"Happy\", \"Surprise\", and\"Sad\", our method actually surpasses EEG techniques in terms of cawF1 performance. c The confusion matrix reflects the high accuracy of emotion state prediction, with minimal misclassification across different emotional states. d PR curve shows that Happy and Sad have higher precision maintained even at higher recall levels, indicating that the model performs better on these two emotions compared to others. e Scanpath visualization of different genders. \"Positive Emotion\": Happy, Surprise; \"Negative Emotion\": Fear, Sad, Disgust and Angry.\n\ncus. In our dataset, we captured multimodal information -EEG, facial expressions, eye movement data, precise fixation points, and environmental context -as participants watched 50 videos under different emotional states. Since our proposed model, EmoGazeNet, processes Equirectangular Projection (ERP) images from panoramic environments, the EmoGaze2D-50 dataset, which contains only 2D images, is not suitable. Therefore, we created an another new dataset, EmoGaze360-1K, specifically for EmoGazeNet. This dataset comprises 1,000 panoramic images -800 indoor and 200 outdoor -spanning 52 categories, sourced from platforms like YouTube and Vimeo. EmoGaze360-1K also includes emotional annotations for six emotional states across various modalities, including EEG, facial expressions, eye movement, precise fixation points, and environmental context. Six distinct emotions were recorded in both genuine and deceptive conditions, following strict modalityspecific standards to ensure consistency and high fidelity. The dataset is split into training and testing sets with a 70/30 ratio, supporting robust ten-fold cross-validation. This dual-context approach enables fairer comparisons across emotion recognition models and offers deeper insights into how emotions are expressed when authentic or intentionally concealed (see Supplementary \"Methods -EmoGaze360-1K\" section -EmoGaze360-1K and EmoGaze2D-50 datasets construction). Our experiments show that training on the entire EmoGaze360-1K dataset yields the best performance (Supplementary Figure  6 ).\n\nOur 360-degree real-world indoor scene dataset. To evaluate the performance of our gaze acquisition method, we collected eye appearance data from subjects using our proposed \"user-unaware\" gaze tracking method across four distinct indoor environments, one public outdoor scene, and one driving scenario (called \"Real360\"). Utilizing sophisticated post-processing techniques mentioned before, we first predict eye gaze coordinates, then transform these eye gaze coordinates into object-level regions. By analyzing the objects and corresponding environments, we can infer emotional states with greater accuracy.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Performance Comparison Between Proposed Deep Model Emogazenet With Existing Emotion Recognition Methods On 2D Screen Dataset",
      "text": "Most mainstream emotion recognition methods are trained on 2D videos where emotions are induced in participants. To ensure consistency and fairness in evaluating our proposed deep model, we used the comprehensive multimodal EmoGaze2D-50 dataset. This dataset, also based on 2D videos, incorporates a diverse array of data types, including EEG readings, facial expressions, eye movement data, precise visual fixation points, and contextual environmental information. This dataset is particularly distinctive as it encompasses six emotional states under both deceptive and real emotional states. We conducted a comparative analysis against several state-of-the-art methods, including electroencephalography (EEG)-based method (ACTNN  48  ), facialbased method (Toisoul  49  ), gaze-based method (CCER  50  ), and our context gaze-based method EmoGazeNet.\n\nBased on the data shown in Figure  4 -a, we can clearly see the significant advantage of the context gaze-based method in distinguishing between real and deceptive emotions. The EEG-based method performed well in real emotion detection (90.74%) with an overall accuracy of 88.94%, though its accuracy in deceptive emotion detection dropped slightly to 87.13%. The facial-based method achieved the highest accuracy in real emotion detection (94.36%), but dropped sharply to 43.91% for deceptive emotions, highlighting its limitation in handling deceptive states. In comparison, the gaze-based method achieved accuracies of 79.43% and 70.26% in real and deceptive emotion detection, respectively, with an overall accuracy of 74.85%. It showed balanced performance but still lagged behind other methods. Our context gaze-based method showed significant improvement over the traditional gazebased method, with an accuracy of 89.82% for deceptive emotions and 87.65% for real emotions, achieving an overall accuracy of 88.74%, close to the EEG-based method (88.94%). Additionally, in deceptive emotion detection, our method even surpassed the EEG-based method, demonstrating its robustness and reliability across different emotional contexts. A two-sample t-test was conducted to compare the accuracy of our method and the traditional EEG-based method, facial-based method, Gaze-based method in deceptive emotion detection. The calculated p-value was 0.031, 0.023, 0.035 (p < 0.05), indicating a statistically significant difference, which strongly validates the superiority of our method in this aspect.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Fine-Grained Performance Comparison Between Proposed Deep Model Emogazenet With Existing Emotion Recognition Methods On 360-Degree Screen Dataset",
      "text": "We also assessed our proposed emotion recognition model EmoGazeNet on the comprehensive multimodal 360-degree screen image dataset, known as EmoGaze360-1K. We reported the cawF1 performance of various emotion recognition methods -physiological signal (EEG)-based, facial-based, gaze (eye movement)based, and our fixation-environment integration method -across six fine-grained emotion states. As illustrated in Figure  4-b , \"Happy\" emotion achieved the highest cawF1 score, with the context gaze-based and EEG-based methods scoring 84.03% and 80.14%, respectively. This high score may result from the distinct and consistent patterns associated with happiness, which makes classification easier. The \"Sad\" emotions followed with slightly lower cawF1 score across the context gaze-based method, but is still higher than physiological signal (EEG)-based methods. The \"Fear\" emotion showed the lowest recognition scores, especially in the facial-based (58.12%) and gaze (eye movement)based (60.77%) methods. The context gaze-based method demonstrated the second highest overall performance, with an average cawF1 score of 78.14%, surpassing both the facial-based and gaze (eye movement)-based methods, and only 1.26% lower in overall performance compared to the EEG-based approach. This suggests that the context gaze-based method is highly effective in recognizing a range of emotional states. A one-way ANOVA test confirmed that the differences between EmoGazeNet and physiological signal (EEG)-based method, facial-based method, gaze (eye movement)-based method were statistically significant (p = 0.015, 0.038, 0.027 < 0.05).\n\nThe confusion matrix in Figure  4 -c illustrates the classification performance of our model on screen panorama data across various emotional categories. The values on the diagonal represent the model's accuracy in correctly classifying each emotion, indicating strong performance in identifying \"Happy\" (0.892), \"Sad\" (0.879), \"Surprise\" (0.887), \"Disgust\" (0.829), \"Fear\" (0.792), and \"Angry\" (0.795). \"Happy\" is primarily misclassified as \"Surprise\" (0.074), suggesting occasional confusion between these emotions, possibly due to similar facial expressions. Similarly, \"Sad\" tends to be misclassified as \"Surprise\" (0.083), reflecting challenges in distinguishing subtle emotional expressions. For the \"Surprise\" emotion, the model occasionally misclassifies it as \"Happy\" (0.044) and \"Sad\" (0.036), indicating some difficulty in distinguishing between neutral and related emotions. \"Disgust\" is more likely to be confused with \"Angry\" (0.092) and \"Fear\" (0.063), while \"Fear\" is primarily misclassified as \"Angry\" (0.113), likely due to overlapping facial cues among these emotions. Finally, \"Angry\" is mainly confused with \"Fear\" (0.071) and \"Disgust\" (0.084), which is consistent with the misclassification patterns observed for \"Fear\".\n\nWe also provided PR (Precision-Recall) curve to evaluate the performance of a model. In a PR curve, the closer the curve is to the top right corner, the better the model's performance. As shown in Fig  4-d , the PR curves for Fear and Disgust show high precision at low recall levels (close to 0.2 to 0.6), but precision quickly decreases as recall increases (0.6). This suggests that the model might struggle with these two emotions. The PR curves for Surprise and Angry are more balanced, with no distinct areas of high precision, but overall, there is a good balance between precision and recall, and the curves are relatively smooth. Happy This radar chart illustrates that females and extroverts, with high emotional sensitivity and adaptability, are well-suited for dynamic tasks requiring quick responses, whereas males and introverts, characterized by greater emotional stability and sustained attention, are better equipped for long-term monitoring in stable environments. c Utilizing an improved eye gaze collection method, we've resolved three key limitations, which has significantly enhanced the performance of the naive version. For the first limitation (limitation 1)low quality eye appearance images -we've employed super-resolution and object-level regions to enhance clarity. To address the eye appearance variations in different users (limitation 2), we've integrated a online personality calibration process. Additionally, for the limited angles of eye appearance images (limitation 3), we developed a 3D reconstruction method to generate eye images from various perspectives. d Compared to segmenting the entire image based on gaze coordinates, mapping these coordinates to object-level regions and leveraging the sequence of these regions for emotion recognition yields better results. e Our research shows that using object-level regions for emotion recognition based on gaze coordinates is more effective than traditional methods. The average accuracy of correct correspondence between gaze coordinates and objects is over 94.67%, with the \"Fear\" emotion achieving the highest accuracy at 97.63%. and Sad have more prominent PR curves, with higher precision maintained even at higher recall levels, indicating that the model performs better on these two emotions compared to others.\n\nFor 360-degree indoor scenes (Figure  4 -e(A)), under positive emotions, males tend to focus first on salient objects like the artwork on the blue wall, then quickly glance at windows, and finally notice the sofa and table. Females start with details on the coffee table, gradually expanding their focus to the sofa and table, and eventually to the view outside the window or door. Under negative emotions, males usually focus on windows or doors first to seek a sense of security. Then they will assess the layout of the room and pay a little attention to some bright areas. Finally, they will fix their eyes on items like sofas and coffee tables. Females begin by focusing on dark corners, then notice details on the sofa and coffee table, with their gaze primarily staying in the dark areas. For outdoor scenes (Figure  4 -e(B)), under positive emotions, males first focus on the wedding scene, then quickly scan surrounding pedestrians and buildings. Females tend to first pay attention to details like the wedding dress and the expressions of the newlyweds, before expanding their focus to the entire scene, including pedestrians and buildings. Under negative emotions, males' gaze tends to be continuous, while females' line of sight is relatively shorter. Males' gaze often focuses directly on darker buildings or crowded areas in the background, and they have a shorter line of sight. Females first notice shadowy areas or details in the crowd and pay less attention to the bright wedding dress.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Fine-Grained Emotion Recognition Based On Our Eye Gaze Collection Method In Real-World Indoor Scene",
      "text": "We have analyzed the performance of our proposed methodology in the domain of fine-grained emotion recognition, focusing on its cawF1 score across a spectrum of real-world indoor scenarios dataset Real360 that encompass diverse lighting conditions and dynamic environments. As shown in Figure  5 -a, \"Happy\" has the highest cawF1 score in all scenarios, especially in the dynamic scenario where it reaches 80.24%, indicating robust recognition across different conditions. In contrast, \"Angry\" and \"Fear\" show relatively lower cawF1 score, with \"Angry\" in the low-light scenario being the lowest (59.85%), suggesting that lighting conditions significantly impact the recognition of these emotions. The high-light scenario has a higher overall cawF1 score (average 73.86%), while the low-light scenario shows lower values (average 65.96%), highlighting that sufficient lighting aids emotion recognition. Notably, the cawF1 score for \"Surprise\" in the dynamic scenario (78.53%) exceeds that in the static scenario (74.84%), which may indicate that dynamic environments facilitate better recognition of certain emotions like \"Surprise\", suggesting that scene dynamics contribute positively to the prediction of specific emotions. A paired t-test for the comparison between high-light and low-light scenarios showed a significant difference (p = 0.042 < 0.05). We have also provided scanpaths of different genders in the four real-world scenarios under different emotion states (Supplementary Figure  7  & 8 ).\n\nFigure  5 -c illustrates the performance improvements of our proposed eye gaze collection method (based on the static scene) as three key limitations are progressively resolved, i.e., (1) lowquality eye appearance images, (2) eye appearance variations in different users, and (3) limited angles of eye appearance images. The baseline version (i.e., with the three key limitations), represented by the blue line, shows the lowest cawF1 scores across all emotions. After addressing limitation 1 (low-quality eye appearance images), shown by the yellow line, the performance improves slightly. Further enhancements are seen with the resolution of limitation 2 (eye appearance variations in different users), indicated by the green line. Finally, resolving all three limitations, including limitation 3 (limited angles of eye appearance images), results in the highest performance across all emotions, as depicted by the red line. The improved version consistently achieves better scores, particularly for \"Happy\" and \"Sad\" emotions, with a notable improvement for \"Disgust\" and \"Angry\".\n\nWe further compared two methods for eye gaze collection: gaze point-based (blue line) and object-level region-based (red line) based on the static scene. As shown in Figure  5-d , the object-level region method consistently outperforms the gaze point method across all emotions, particularly for \"Happy\", \"Sad\", and \"Surprise\" emotions, where it achieves higher cawF1 scores. Both methods perform the worst on \"Fear\", with object-level regions still providing better results. Overall, object-level region mapping shows superior performance, aligning with the study's finding that accurate gaze-to-object correspondence (i.e., the gaze coordinates are precisely within the area occupied by the object) significantly improves emotion recognition, with an average correctness above 94.67%, peaking at 97.63% for \"Fear\" (Figure  5-e ). A pairedsamples t-test was carried out to compare the cawF1 scores of the object-level region-based method and the gaze point-based method. The p-value was calculated to be 0.018 (p < 0.05), demonstrating a significant difference in performance between the two methods, and validating the superiority of the object-level region-based method.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion-Environment Interactions Across Various Gender And Personality",
      "text": "We also explored the differences in emotion-environment interactions across various gender and personality types in Real360 dataset. We quantified the performance of participants across six indicators -emotion sensitivity, emotion stability, real-time response capture, emotion salience focus, context adaptability, and sustained attention preference -using a 1-10 rating scale (1 being low, 10 being high). Detailed explanations of these six indicators can be found in the Supplementary \"Methods\". The experimental results are shown in Figure  5-b .\n\nMales scored high in emotion sensitivity (8) and emotion saliency focus (9), making them responsive to emotionally rich or significant cues and well-suited for complex, dynamic environments due to high context adaptability (8). However, with lower scores in emotion stability and sustained attention preference (5), they show more frequent emotional shifts and shorter focus spans. Females, with high stability and sustained attention (8), exhibit steady emotions and prolonged focus, ideal for stable, low-dynamic contexts. Lower sensitivity and real-time response scores (5) indicate slower reactions to diverse, emotionally intense situations.\n\nIntroverts excel in emotion stability (  9 ) and sustained attention (8), focusing well in stable, low-stimulus settings but scoring lower in sensitivity (4) and salience focus (5), making them less responsive to subtle emotional cues in dynamic environments. Extroverts score high in sensitivity (8), real-time response (9), and salience focus (9), making them quick to engage with emotional cues in varied settings. Their high adaptability (9) contrasts with lower stability and sustained attention scores (4), favoring frequent shifts in focus and making them ideal for fast-paced, interactive contexts.\n\nThis analysis shows that males and extroverts, with their strong emotional sensitivity and adaptability, are well-suited for dynamic emotion recognition tasks, while females and introverts, with greater stability and focus, are better suited for steady, long-term monitoring in single-context environments. These insights support security and driver monitoring. In security, males and extroverts' heightened emotional responsiveness and adaptability aid in identifying sudden changes in high-risk individuals, while females and introverts' stability helps in detecting abnormal behavior over longer periods. For driver monitoring, extroverts benefit from real-time alerts in complex conditions to maintain focus, whereas introverts and stable drivers, more prone to fatigue in extended sessions, can be monitored for declining attention. This approach enhances both public safety and driver security by integrating emotional and environmental interactions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Field Experiment To Evaluate The Practical Application Of Our Eye Gaze Collection Method",
      "text": "To evaluate the practical application of the proposed eye gaze collection method, we designed two field experiment scenarios: a campus environment and a driving simulator environment. These scenarios, with their unique environmental characteristics and emotional demands, provide a comprehensive assessment of the method's robustness and effectiveness in real-world settings.\n\nCampus Scenario Experiment Setup. The experiment was conducted in an open campus area (e.g., a campus square) to simulate a dynamic and varied social and natural environment. Fifteen university students (6 females, 9 males, aged 22-28) participated and were asked to simulate six different emotional states (such as Happy, Angry, and Surprise) induced by videos or images. During the experiment, eight high-definition cameras captured participants' eye appearance from different angles, which was then combined with gaze-environment interactions for emotion recognition. Additionally, participants wore Apple Watches to monitor skin conductance response (GSR) and had their facial expressions recorded.\n\nResults in Figure  6 -d showed that in the campus scenario, our context gaze-based method, which incorporates gaze-environment interactions, outperformed the other methods, achieving an accuracy of 81.45%, F1 score of 0.81, and cawF1 score of 0.73. This was significantly higher than the facial-based method (71.42%, 0.62, and 0.62, respectively) and the physiological signal-based method (79.62%, 0.74, and 0.65). One-way ANOVA was per- In the driving simulator, it again achieved the highest accuracy, F1 score, adaptability, and comfort, while facial methods suffered from angle and lighting issues, resulting in the lowest performance. Dynamic adaptability refers to the system's responsiveness to changing contexts; user comfort reflects ease of use without extra devices. Both metrics are normalized to [0-1]: high=1, medium=0.5, low=0.\n\nformed to compare the accuracy of the three methods. The p-value was 0.025 (p < 0.05), indicating significant differences among the methods. Post-hoc tests showed that the differences between our context gaze-based method and the facial-based method, as well as the physiological signal-based method, were statistically significant (p < 0.05 for both comparisons). Our context gaze-based method also scored highest in dynamic adaptability and user comfort (both 1), demonstrating its suitability for complex, open environments without requiring additional wearable equipment. In contrast, the facial-based and physiological signal-based methods showed lower dynamic adaptability and physiological signal-based methods showed lower user comfort, with facial expression recognition especially affected by lighting and angle, leading to less stable results. Driving Simulator Scenario Experiment Setup. To simulate driving conditions and collect relevant data, we used a driving simulator that presented various traffic scenarios (e.g., emergency braking, traffic congestion). Ten drivers (7 males, 3 females, aged 20-29) participated and were tasked with handling different driving challenges and emotional stimuli. The experiment used three screens to simulate a realistic driving view, displaying front, left, and right window perspectives to replicate real driving conditions. Only one high-definition camera was used in front of the simulator to capture eye appearance, and facial expressions and GSR data were recorded simultaneously.\n\nIn the driving scenario, as shown in Figure  6 -e, our context gaze-based method also outperformed the other methods, with an accuracy of 83.85%, F1 score of 0.81, and cawF1 score of 0.73. By comparison, the facial-based method achieved lower scores (72.62%, 0.62, and 0.58), while the physiological signalbased method scored slightly higher than the facial-based method (82.15%, 0.79, and 0.71). A one-way ANOVA was performed to compare the accuracy of the three methods. The calculated p-value was 0.032 (p < 0.05), indicating significant differences among the methods. Post-hoc tests further revealed that the differences between our context gaze-based method and the facial-based method were significant with a p-value of 0.021 (p < 0.05), and the difference between our method and the physiological signalbased method also reached statistical significance with a p-value of 0.045 (p < 0.05). In terms of dynamic adaptability and user comfort, our context gaze-based method scored the highest (both 1). While the facial-based method had high user comfort (1), it showed lower adaptability (0.5), indicating it was more affected by head movements and changes in lighting within the vehicle. The physiological signal-based method scored low in both dynamic adaptability and user comfort (0.5).\n\nOverall, our context gaze-based method incorporating gazeenvironment interactions demonstrated higher accuracy, F1 scores, and adaptability in both the campus and driving simulator scenarios, enabling more accurate emotion recognition in complex environments. Additionally, it provided superior user comfort by avoiding the need for additional wearable devices. These results suggest that the gaze-based method is particularly advantageous for emotion recognition tasks requiring high adaptability and user comfort.\n\nFuture Research Directions in Mental Health and Security. The positive results from our experiments suggest several promising future research areas. First, psychological state assessment could benefit from our method, enabling continuous, user-unaware monitoring of emotional shifts in real-time. This would be particularly valuable in environments like campus settings, where emotional fluctuations in students could be tracked seamlessly as they move through various activities (Figure  1-d-left ). Second, early psychological disorder screening could be facilitated by identifying subtle emotional cues that indicate mental health issues such as anxiety or depression, even before they become fully apparent. This application could extend to sensitive settings, such as monitoring pilots in an airplane cockpit, where early detection of emotional distress is critical (Figure  1-d-middle ). Third, our method could enhance smart classroom environments by monitoring students' emotional states in real-time, providing valuable insights into their engagement, stress levels, and overall well-being (Figure  1-d-right ). These research directions underscore the broad potential of gaze-based emotion recognition in advancing mental health monitoring and enhancing public security.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Robustness Evaluation",
      "text": "To prove the robustness of the proposed method, we conducted three experiments.\n\nWe conducted a long-term stability monitoring experiment designed to track the emotional recognition results of the same group of users over different time periods, e.g., from day to day in a real-world static scenario, where emotional recognition tests are conducted every six hours, twice a day, over a span of multiple days to sufficiently capture the daily changes in the user's emotional state. Specifically, the experiment design involves testing with two people per group, which may help reduce the impact of individual differences on the results. As shown in Figure  6 -a, the data shows the fluctuation in cawF1 scores. The data displays the cawF1 score of emotional recognition from Day 1 to Day 17. The cawF1 score fluctuate around the 70% to 73% mark, indicating that the proposed method maintains relatively stable performance over the long-term monitoring period. Despite daily fluctuations, there is no significant overall decline or upward trend in cawF1 scores, suggesting that the proposed method has good long-term stability. On Day 9 in Group 1, the cawF1 score reached its lowest point at 70.01%, while on Day 13, it reached its highest point at 72.86%. These peaks and troughs may be related to various factors, such as changes in the user's state, environmental factors, or minor changes in test conditions. In consecutive tests, the cawF1 score showed only minor changes. From the 1st to 5th day in Group 3, it rose slightly from 70.22% to 71.14%. From the 13th to 17th day, it dropped marginally from 71.96% to 71.51%, indicating overall stability. Such short-term fluctuations may be due to random errors or minor changes in the user's emotional state. Looking at the overall data from Day 1 to Day 17, the cawF1 score seems to fluctuate around a central value of approximately 71.5% to 72%. This indicates that during the long-term monitoring period, the proposed method can continuously provide a relatively consistent level of accuracy in emotional recognition.\n\nWe implemented emotional enhancement experiment aimed to compare the impact of playing or not playing corresponding emotional audio on emotion recognition when collecting gaze points in real static scenes and when viewing 360-degree images on a screen. As shown in Figure  6 -b, for viewing 360 images on a screen, without playing emotional audio, the average cawF1 score is 77.14%; with playing emotional audio, the average cawF1 score increases to 78.14%. For collecting gaze points in real scenes, without playing emotional audio, the average cawF1 score is 70.72%; with playing emotional audio, the average cawF1 score increases to 72.22%. Playing emotional audio had a certain enhancing effect on the recognition cawF1 score of most of emotions, although the extent of improvement varied.\n\nWe conducted personalized model training experiment aimed to train an individualized emotion recognition model for each user and assess the effectiveness of personalized models in enhancing emotion recognition for individual users based on the static scene in real world. The experiment also compared the performance differences between personalized and general models. Data was collected every two hours, seven times a day, for the training of the personalized model. As shown in Figure  6-c , personalized models show varying average cawF1 scores across different users (numbered 1 to 10), roughly ranging from 73.37% to 74.73%. The general model has an average cawF1 scores of 72.22%, serving as the baseline for comparison with the performance of personalized models. In most cases, personalized models have higher average cawF1 score than the general model, indicating that personalized training can enhance the performance of emotion recognition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Component Evaluation And Proposal Evaluation Metric",
      "text": "We compared the impact of different visual stimuli on emotion recognition by examining 2D images, 360-degree panoramic images, and 360-degree panoramic videos on Real360 dataset As shown in Figure  7 -a, 360 videos demonstrated superior performance across most emotional dimensions. Specifically, 360 videos achieved the highest recognition rates in \"Surprise\" (76.81%), \"Disgust\" (71.27%), \"Fear\" (66.33%), and \"Angry\" (69.06%), and also attained the highest overall mean score of 72.64%. On the other hand, 360 images excelled in conveying \"Happy\" (77.78%) and \"Sad\" (76.29%) emotions, outperforming both 360 videos and 2D images. This indicates that 360 images have a distinct advantage in eliciting both positive and negative emotions. In contrast, 2D images showed slightly lower scores across all emotional dimensions, with \"Happy\" at 76.62% and \"Sad\" at 75.19%. However, they still maintained relatively high recognition rates in \"Surprise\" (75.87%) and an overall mean of 71.39%. To validate our method, we compared gaze coordinates collected by our approach and an eye tracker using 360 images on Real360 dataset. As shown in Figure  7 -b, the eye tracker showed slightly higher accuracy (83.79%) compared to our method (80.22%), with an F1 score of 80.65% for the tracker and 78.86% for our approach. The eye tracker also had a higher cawF1 score (73.69% v.s. 72.22%). These results suggest that while the eye tracker is more accurate in collecting gaze data, our method performs comparably and is practical for resource-limited or rapid deployment scenarios.\n\nFigure  7 -c shows the performance variations when using different number of HD cameras during eye appearance acquisition. The use of 8 cameras appears to strike an optimal balance between capturing comprehensive eye appearance data and managing the computational complexity and potential data redundancy. Eight cameras provide sufficient coverage of the eye region to capture the necessary details for accurate gaze prediction without overwhelming the system with excessive data. This balance helps maintain high accuracy and F1 scores, as the model can efficiently process the captured data without being hindered by unnecessary information.\n\nWe also evaluated each component of the proposed emotion recognition model EmoGazeNet (see Supplementary Table  1 ). The evaluation matrix clearly demonstrates that the system reaches its peak performance in terms of ACC (80.22%), F1 Score (78.86%), and cawF1 (72.22%) when all key components, such as Scanpathguided Region Generation, Primary Classification Branch, Auxiliary Classification Branch, and Scanpath-guided Classification Branch, are engaged, highlighting the synergistic impact of these elements on overall system performance. We also compared different scanpath prediction methods and different choices of base encoder in the Generator (Supplementary Figure  2  & 3 ).\n\nWe conducted extensive experiments on EmoGaze360-1K dataset to demonstrate that the proposed metric, cawF1, is more effective than existing metrics like accuracy and F1 score. As shown in Figure  7 -d, the quantitative results reveal that cawF1 scores are generally lower than the other two metrics. For instance, in the \"Fear\" emotional state, participants' gaze tended to focus more on the safe bed, a specific area the model failed to capture accurately, leading to a lower cawF1 score (Figure  7-e ). Additionally, when recording participants' gaze scanpaths while they viewed complex scene images under different emotional states, the model successfully predicted the emotional states but struggled to account for the variations in gaze scanpaths. Consequently, cawF1 scored lower, highlighting its stricter evaluation metric (Figure  7-f ).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "This study introduces a novel framework for emotion recognition that deeply integrates gaze behavior with environmental semantics and temporal dynamics, enabling continuous and non-intrusive monitoring of internal emotional states. By moving beyond the reliance on explicit cues such as facial expressions, speech, and gestures, our approach leverages the wealth of information embedded in natural gaze shifts and attention allocation during everyday human-environment interactions. Extensive empirical evidence demonstrates strong generalizability and robustness across a wide range of scenarios (see Fig.  4 , Fig.  5 ), laying a solid technical foundation for emotion sensing and decoding in real-world contexts.\n\nAt the theoretical level, our work resonates with recent advances in neuroscience concerning the interplay among emotion, attention, and environmental context. The results reveal that the generation and regulation of emotion are not isolated internal processes but are fundamentally embedded in the continuous interaction between individuals and their environment (see Fig.  1 , Fig.  2 , Fig.  3 ). By tracking spatial transitions in gaze, semantic targets of attention, and their temporal evolution, we show that emotional states can manifest in subtle and complex patterns of perception and behavior. These findings provide not only new theoretical insights for affective computing but also empirical evidence for understanding the functional mechanisms of emotion in social interaction and cognitive regulation (see Supplementary Fig.  7  and Supplementary Fig.  8 ).\n\nWe constructed multimodal datasets in a variety of settings and systematically compared our environment-and gaze-based approach with traditional methods relying on facial cues, gaze points, or physiological signals. The results show clear advantages in both accuracy and applicability, especially in challenging environments involving emotional concealment or complex attentional shifts (see Fig.  4 -a,b and Fig.  5-a ). Notably, modeling attention at the level of semantic objects allows the system to capture the natural flow of emotional states in real-world scenarios, moving beyond static gaze coordinates or single perceptual features. This is crucial for enabling emotion recognition systems to operate effectively in complex, real-life applications.\n\nIndividual differences also play a significant role in emotionenvironment interaction (see Fig.  5-b ). Our analyses indicate that factors such as gender and personality significantly influence gaze patterns, emotional sensitivity, and stability, providing a solid empirical basis for the development of more refined and personalized emotion recognition systems (see Supplementary \"Methods\"). Furthermore, our long-term and dynamic environment experiments confirm the stability and adaptability of the method (see Fig.  6 a), suggesting broad applicability in domains such as education, public safety, and intelligent driving (see Fig.  6-d,e ).\n\nDespite these advances, several limitations remain. Under conditions of extreme lighting, occlusion, or rapid head movement, the accuracy of gaze estimation and emotion recognition still needs improvement. Moreover, the complexity of emotion -shaped by culture, age, and psychological factors -calls for larger and more diverse datasets to enhance model generalizability and fairness. From an ethical and privacy standpoint, although our method is non-intrusive and low-observability by design, largescale deployment requires rigorous standards for data collection and usage to safeguard user consent and data security.\n\nIn summary, this work not only extends the theoretical and methodological foundations of emotion recognition but also provides a new path for interdisciplinary research and the development of emotional intelligence systems in real-world scenarios. As foundational theories and technologies continue to evolve, emotion recognition based on dynamic modeling of environment, attention, and emotion is poised to have far-reaching impact in areas such as mental health monitoring, human-computer interaction, intelligent education, and public safety.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Camera-Based Gaze Tracking Method",
      "text": "Collection setting. We strategically position eight HD cameras around a designated area, ensuring full coverage and eliminating blind spots. This setup allows individuals to move freely within the space while continuously capturing images from all angles (see Figure  3-a ). For each position within the space, a segment of the panoramic image corresponding to the individual's field of view (FOV) is projected onto a 2D plane. Detailedly, we use a third-person multi-camera panoramic modeling approach to ensure a \"user-unaware\" solution by generating a panoramic model of the scene from any location, capturing the user's gaze interaction with the environment (see Methods -Third-person multi-camera panoramic modeling section).\n\nTo achieve this, we recruited 30 annotators (18 females and 12 males, aged 18 to 28) to collect data on their eye appearance and regions of focus. Prior to data collection, participants underwent emotion induction through video and image stimuli. Each participant collected data six times in the same scene, under six distinct emotional states. Based on Paul Ekman's basic emotion theory  51  , we also categorize emotions into six types, i.e., \"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", and \"Surprised\". To avoid memory residual interference (i.e., carryover effects from one emotion affecting the regions of focus in subsequent emotions), there was a two-day interval between each data collection session for different emotions within the same scene. In total, we collected data from one static indoor scenes, two high/low light indoor scene, and one dynamic outdoor scenes.\n\nGaze mapping. By analyzing the visual appearance of the eyes in this 2D projection, our system predicts the coordinates of the gaze point using existing advanced gaze prediction algorithms (see Supplementary Figure  4 ). This projection and prediction process occurs at short intervals, resulting in a comprehensive dataset of gaze coordinates mapped onto the 360-degree panoramic image over time. Notably, our findings indicate that a projection interval of 0.1 seconds optimizes the accuracy of gaze point collection (see Supplementary Figure  5 ). Since the current advanced gaze prediction algorithms learn the mapping between eye appearance and coordinates directly, ignoring the subjectivity of gaze data and the variability in eye appearance across individuals. This results in reduced generalization and accuracy of the mapping. To improve, we then employ an online personalized calibration method to reduce the interference caused by individual differences in eye appearance using our collected data (see Figure  3 -a and Methods section). Further, due to variations in camera angles, lighting, and other factors, collected eye images may lack clarity, so we enhance them with super-resolution. Additionally, most existing datasets for gaze estimation include only limited head angles, restricting the range of eye appearances. Our method provides greater flexibility in gaze angles, capturing diverse viewpoints. To bridge this gap, we perform 3D reconstruction on the facial data from the existing gaze prediction dataset (e.g., ShanghaiTechGaze  52  ), generating eye appearance data from various angles. This data then retrains the model to better fit our requirements. The details of collection and post-processing of eye appearance data in real-world environments can be seen in Supplementary \"Methods -Gaze Point Collection Process\" section and Supplementary Algorithm 1 and Algorithm 2.\n\nThis innovative approach allows for precise and continuous tracking of eye gaze across a wide area, overcoming the limitations and discomfort associated with traditional eye tracking devices. The collected gaze coordinates are then integrated with the 360-degree panoramic images and fed into our proposed emotion recognition model, EmoGazeNet, enabling robust and accurate emotion detection. Furthre, to assess the accuracy of gaze point collection accuracy, we have introduced an object-box-based evaluation metric: if the gaze coordinates fall within the object box, it is considered accurate; otherwise, it is deemed to have a significant error. This metric provides a straightforward way to evaluate the precision of gaze estimation systems, ensuring that the predicted gaze points are closely aligned with the actual areas of interest within the environment (see Methods section).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Context Gaze-Based Deep Model",
      "text": "Directly mapping eye appearance, gaze coordinates, and environmental context can lead the model to learn superficial visual patterns, associating environment with emotional states without understanding deeper gaze-related correlations. To address this, we introduce EmoGazeNet, a novel GAN-based model designed to capture meaningful, context-aware interactions between gaze behavior, emotional states, and environmental cues. EmoGazeNet takes two primary inputs: (1) a panoramic environment represented by an ERP (Equirectangular Projection) image, and (2) sequential gaze coordinates reflecting human-environment interactions. These gaze coordinates form a scanpath used to segment the ERP image into distinct object patches, ordered according to the sequence of viewing. Different arrangements of these patches effectively reflect variations in emotional states.\n\nAs shown in Figure  3 -c & Supplementary Figure  1 , EmoGazeNet, based on Generative Adversarial Networks (GANs), is designed with two main components: the Generator (Main Net) and the Discriminator (Main Net's Twins). The Generator is responsible for generating the probability distribution of emotion categories, while the Discriminator's task is to distinguish between real and generated data. Through adversarial training, both components continuously improve, with the Generator becoming better at generating realistic emotional state predictions, and the Discriminator sharpening its ability to differentiate between true and synthesized data. This enables the model to not only learn the direct relationships between gaze and emotional states but also refine its understanding of the deeper, context-aware correlations between eye movements and the surrounding environment. Details of EmoGazeNet are shown in the Supplementary \"Methods -EmoGazeNet model achitecture\".",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Online Personalized Calibration",
      "text": "Gaze data is inherently subjective, as individuals exhibit widely varying gaze patterns in identical situations. For example, when observing the same artwork, some may focus on the main character, while others may be drawn to background details or color contrasts. These variations stem from personal interests, preferences, and observation habits, complicating the adaptability and generalizability of emotion recognition models. Furthermore, the mapping between eye appearance and gaze coordinates varies significantly among individuals, making it challenging for traditional regression models to achieve high accuracy in gaze tracking. Given that gaze is a fine-grained external expression, even small tracking errors can significantly interfere with emotion detection, emphasizing the need for high precision.\n\nTo address these challenges, we propose an online personalized calibration method that integrates subjective fixation (user-specific gaze tendencies) and objective fixation (scene-based salient points) to enhance gaze mapping accuracy and adaptability (Figure  2-c  and Figure  3-a ). This method focuses on leveraging two key factors influencing gaze behavior: head motion and gaze state transitions. First, when the head is stationary, head movement data has minimal influence on gaze accuracy. However, during head movement initiation or cessation, visual inertia causes the gaze to align roughly with the head's direction, offering a valuable reference point. A multi-camera system captures images from multiple angles, and head pose estimation algorithms calculate pitch, yaw, and roll. By monitoring changes in head angles, the system identifies movement start and stop points. At these moments, a \"strong hint\" mechanism provides an initial gaze range, reducing errors caused by individual differences. Second, gaze transitions between two states: \"scanning\" and \"fixation\". In the scanning state, the gaze moves rapidly over a wide area, while in the fixation state, it focuses on a specific object. Distinguishing these states enables more precise gaze tracking. An initial gaze mapping model, combined with head pose data, estimates the approximate gaze position. For static objects, saliency detection identifies the most prominent object as the gaze coordinate, aligning with the objective fixation. For dynamic objects, motion detection techniques like optical flow pinpoint the movement's starting point as the precise gaze coordinate. This integration of head motion and gaze states ensures robust, individualized gaze tracking across diverse scenarios.\n\nBuilding on this foundation, the calibration process adapts dynamically to user behavior and environmental changes, starting with a global initialization and continuing through ongoing fine-tuning.\n\nAt system initialization (timestamp t=1), a global calibration process aligns subjective and objective fixation (Figure  3-a ). During this phase, the system collects eye appearance data (e.g., pupil shape, gaze direction) and head movement data (pitch, yaw, roll) using a multi-camera setup. This data forms the basis for aligning the subjective and objective gaze references. To achieve this, a teacher-student model framework is used, inspired by knowledge distillation. The teacher model analyzes the scene to identify salient objects, such as static targets (e.g., cars, trees) or dynamic movement starting points, establishing an objective fixation reference. The student model, which is personalized to the user, predicts gaze points based on subjective fixation tendencies and compares them with the teacher's outputs. This comparison serves to refine the student model through continuous learning, gradually adjusting it to better align with both the scene's characteristics and the user's preferences, enhancing the system's adaptability and accuracy over time.\n\nDuring significant head movements or scene transitions (e.g., yaw or pitch exceeding thresholds at timestamps t=S1+1 and t=S2+1), dynamic calibration is triggered to adjust gaze predictions. Visual inertia temporarily aligns gaze with head direction, allowing the \"strong hint\" mechanism to narrow the gaze range. Simultaneously, the teacher model updates salient object detection, particularly for dynamic regions, and the student model is fine-tuned by integrating motion starting points and salient targets. This process distinguishes between scanning and fixation states, offering broad gaze ranges during scanning and precise targets during fixation.\n\nFinally, during scene transitions, the system performs online fine-tuning within the first 200-300 milliseconds -a critical win-dow when visual attention is primarily driven by objective saliency rather than cognitive or emotional factors  53, 54  . The system reacquires eye appearance data and head movement updates, while the teacher model reanalyzes salient objects in the new scene. This enables rapid calibration of the student model to reflect new scene characteristics. By aligning subjective fixation with the most prominent static or dynamic features (objective fixation), the system ensures precise gaze tracking even in complex and dynamic environments.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Third-Person Multi-Camera Panoramic Modeling",
      "text": "Gaze often exhibits distinct \"first-person\" characteristics 1  in its interaction with the environment. To represent this perspective, conventional methods typically rely on wearable devices, such as head-mounted cameras. However, while these devices can effectively capture the user's field of view, they also increase the user's burden and reduce the overall user experience. To solve this challenge, we propose a \"third-person multi-camera panoramic modeling\" approach, using a multi-camera approach from a third-person perspective to generate a panoramic model of the scene from any location, ensuring a \"user-unaware\" solution (see Figure  2-c  & Supplementary \"Methods -Third-Person Multi-Camera Panoramic Modeling\").\n\nWe adopt a problem decomposition strategy, breaking down the complex panoramic sphere generation task into three subproblems: static background fine reconstruction, local foreground object appearance generation, and foreground-background highrealism fusion. Since the static background is relatively stable, we use computationally expensive methods (such as ReconFusion) to reconstruct the base background of the panoramic sphere. For dynamic foreground objects, a \"lightweight\" approach is needed for high-quality generation and fusion. The specific solution consists of two parts:\n\nFirst, the foreground semantic skeleton captures key information about movable objects in the scene, such as spatial coordinates, size, appearance, and semantics, using multiple complementary camera views. Due to the differences in object representation across various viewpoints, we need to achieve \"common alignment\" and \"differential complement\" of object-level information in a lightweight manner through a \"weakly supervised\" model. Specifically, a subspace clustering approach is used to establish initial mappings of foreground objects from different camera angles, and through self-iteration, the local structure matching is optimized to create a \"sparsely structured\" and \"semantically rich\" foreground semantic skeleton. This method significantly reduces the complexity of panoramic sphere generation and meets realtime requirements.\n\nSecond, to reduce panoramic sphere generation's computational overhead, we simplify processing by utilizing prereconstructed backgrounds and foreground semantic skeletons. Our approach generates target object appearances from desired viewpoints using semantic skeletons, then fuses them with backgrounds. Camera parameters and object poses from the semantic skeleton enable efficient local-to-global fusion with enhanced realism. We further optimize through \"weight-sharing, alternating training\", using a single model for both viewpoint generation and fusion, improving quality without additional computational costs.\n\nThe advantage of this method lies in problem decomposition, ensuring high-quality generation while reducing the demand for computational resources. By using multi-camera joint generation to create high-quality \"first-person perspective\" panoramic spheres, we can represent the interaction between the viewpoint and the environment in a \"user-unaware\" manner, laying a crucial foundation for subsequent research.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Object-Box-Based Evaluation Metric For Gaze Point Collection Accuracy",
      "text": "To evaluate the accuracy of the collected gaze coordinates, we propose a method based on the object's bounding box. This method assumes that every object in the scene is labeled with a bounding box, and the model's predicted gaze coordinates should fall within the bounding box of an object. We judge the accuracy of the prediction based on whether the gaze coordinates fall within the bounding box. If the coordinates fall inside the object's bounding box, the prediction is considered accurate; if they fall outside the bounding box, the prediction is considered to have a large error.\n\nTo describe this process specifically, we assume that the model's predicted gaze coordinates are (x p , y p ), while the bounding box of the closest object is represented by the coordinates of its top-left and bottom-right corners, (x min , y min ) and (x max , y max ), respectively.\n\nFirst, we check if the gaze coordinates satisfy the following conditions to confirm whether they fall inside the object's bounding box:\n\nif these conditions hold, the gaze coordinates (x p , y p ) are within the object's bounding box, and the prediction is considered accurate.\n\nSecond, if the gaze coordinates do not satisfy the above conditions, i.e.:\n\nx p < x min or x p > x max or y p < y min or y p > y max , (2) then the gaze coordinates (x p , y p ) are outside the object's bounding box, and the prediction is considered to have a large error.\n\nThird, we can define an accuracy evaluation function Accuracy, which takes a value of 1 (accurate) or 0 (inaccurate), using the following formula:\n\nx max and y min  y p  y max , 0, otherwise .\n\nHere, A = 1 indicates that the prediction is accurate, meaning the gaze coordinates fall within the object's bounding box; A = 0 indicates that the prediction is inaccurate, meaning the gaze coordinates are outside the bounding box.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Implementation Details",
      "text": "The EmoGazeNet model is developed and implemented using Py-Torch in Python with CUDA. Model training is performed on an NVIDIA Geforce RTX 3090 graphics processing unit (GPU). We use the Adam optimizer with the learning rate of 0.001 to train the EmoGazeNet model for 1000 epochs with batch size of 16. The complement training process takes around 17 hours. The model has 50 GFLOPs and 17.84 million parameters.\n\nTo evaluate the performance of EmoGazeNet, we report three key metrics: Accuracy (Acc), F1 score, and Contextual Attention Weighted F1 Score (cawF1). Acc measures the overall correctness of the model's predictions, calculated as the ratio of correct predictions (both true positives and true negatives) to the total number of predictions made. F1 score provides a balanced measure of precision and recall, which is particularly useful in scenarios with imbalanced class distributions. Contextual Attention Weighted F1 Score (cawF1) is our proposed evaluation metric tailored specifically for emotion recognition tasks. This metric not only assesses classification performance but also incorporates fixation-context consistency. By doing so, cawF1 evaluates the model's ability to correctly classify emotions while simultaneously understanding the relationship between eye fixations and the environmental context, thereby providing a more comprehensive assessment of the model's performance.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Proposed Evaluation Metric",
      "text": "Traditional multi-classification evaluation metrics such as precision, recall, and F1 score are usually used for basic performance measurement, but they may not be sufficient to capture the complexity of emotion recognition tasks, particularly when considering the interaction between emotional states and visual attention. These conventional metrics focus solely on the accuracy of emotion classification without taking into account the specific areas of the environment that individuals might focus on under different emotional states. As a result, models evaluated using these metrics might appear to perform well, even when they fail to accurately predict the gaze patterns or fixation points that are crucial for understanding the emotional context.\n\nTargeting at this issue, we propose a comprehensive evaluation metric that addresses these limitations by integrating both classification performance and fixation-context consistency into a single evaluation metric -Contextual Attention Weighted F1 Score (cawF1). Unlike traditional metrics, cawF1 not only assesses the model's ability to correctly classify emotions but also evaluates how well the model can predict the areas of the environment that are most relevant to the observed emotional state. This makes the metric more rigorous and reflective of the model's true understanding of the interplay between emotion and attention. By incorporating gaze patterns into the evaluation, cawF1 ensures that models are held to a higher standard, where successful emotion recognition is closely tied to accurate environmental context interpretation. The metric can be defined as:\n\nwhere n is the number of samples, bF1 i is the balanced F1 score for the i-th sample. FCC i is the fixation-context consistency score for the i-th sample, used to measure the consistency of the model between the detected viewpoints and the context of the environment. FCC can be calculated by:\n\nwhere n is the number of samples, v local The features of fixation and environment context regarding local and global conditions can be extracted by pre-trained convolutional neural networks (e.g., ResNet, VGG, etc.) For local features v local i and e local i , we extract features within a certain area around the gaze point. For example, features within a fixed-size window around the point of gaze and corresponding environmental information and may be extracted. For global features v global i and e global i , we extract global features from the entire image to capture overall fixations and environmental information.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Principles For Emotion Category Selection",
      "text": "In selecting emotion categories, we followed principles of theoretical representativeness and experimental feasibility. Six basic emotions -\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", and \"Surprised\" -were chosen based on Ekman's theory, covering the core spectrum of human affect. These emotions are easily elicited and annotated in controlled settings, supporting reliable multimodal data collection and enabling direct comparison with previous studies. This choice also ensures compatibility and gener-alizability within existing affective computing and psychological research frameworks.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Privacy Protection And Ethical Considerations",
      "text": "When applying gaze-based emotion recognition methods, privacy protection is crucial. Although this approach uses a \"user-unaware\" monitoring system, it is important to ensure users' privacy is not compromised. To address this, we anonymize all collected emotional data, ensuring it cannot be traced to specific individuals. Additionally, data is encrypted during storage and transmission, and sensitive information is anonymized to remove personal identifiers, enhancing security. We also comply with relevant laws and regulations to safeguard user privacy. Future work will further explore and refine privacy protection mechanisms, ensuring ethical application in fields like public safety and mental health.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Accuracy Improvement: Object Area Detection",
      "text": "Viewpoint coordinates are used as signal input, combined with object detection algorithms, to detect the object area where the viewpoint coordinates are located. By converting relatively coarse viewpoint predictions into precise object-level detection, this compensates for viewpoint errors caused by camera distance. This approach not only improves positioning accuracy but also precisely identifies the object of gaze, which is beneficial for emotional recognition and environmental interaction applications.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Viewpoint Coordinate Prediction Accuracy Evaluation",
      "text": "To evaluate the accuracy of viewpoint coordinate predictions, an evaluation criterion is set: if the predicted coordinates fall outside the object box, the prediction is considered to have a significant error; if they fall within the box, it is considered relatively accurate, based on the principle that eye gaze focuses on the object area. The closer the predicted result is to the object area, the more accurate it is considered to be.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Multiple Trials And Optimization",
      "text": "Multiple trials are conducted to verify the stability and accuracy of the algorithm, adjusting camera positions and numbers to maximize coverage. Continuous optimization is carried out to gradually improve the accuracy of viewpoint predictions, ultimately achieving stable and high-precision viewpoint coordinate predictions. Place C i at designated corner to maximize room coverage, ensuring each corner is within view to minimize gaze point loss.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "4:",
      "text": "Perform calibration using a checkerboard pattern to obtain intrinsic parameters (focal length f i , distortion coefficient k i ) and extrinsic parameters (position t i and orientation R i ).",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "5:",
      "text": "Calculate camera matrix\n\n6: end for 7:\n\nStep 2: Human Position and Gaze Range Estimation 8: Capture image set {I i } from each C i . 9: Using multi-view geometry, apply triangulation on key points {p i } across views to compute the 3D coordinates P of the human position. 10: Compute P as:\n\n11: Store P as the initial reference point for subsequent gaze range estimation. 12: Step 3: Third-Person Panoramic Modeling (Centered on Human Position P) 13: Divide the modeling task into three parts: background reconstruction, foreground generation, and fusion. 14: 1. Background Reconstruction: Use high-precision algorithms (e.g., ReconFusion) to reconstruct the static background around P, resulting in the background map B. 15: 2. Foreground Generation: For each dynamic object F j around P, extract semantic skeleton S j using multi-view analysis. 16: 3. Foreground-Background Fusion: Integrate B and {S j } using weight-sharing and alternate training methods to reduce computational load. Finalize the panoramic model  as:\n\nStep 4: Field of View (FOV) Extraction and 2D Projection 18: Use head orientation angles ( ,  , ) (pitch, yaw, roll) to determine gaze direction g. 19: Set FOV angle  (e.g., 120), with gaze direction g as the central vector. 20: Map  to a spherical coordinate system, then extract the FOV region   centered at g. 21: Project   to a 2D plane for display as  2D  .",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "19/32",
      "text": "Algorithm 2 Gaze Point Collection and Prediction Process -Part 2 1:\n\nStep 5: Personalized Gaze Calibration and Data Collection 2: For detected head movements h, adjust FOV center g based on the \"strong online hint\" mechanism, ensuring precise gaze alignment. Adjust camera positions or parameters to maximize spatial coverage and precision.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "21:",
      "text": "Log accuracy metrics  t and update M Gaze as needed. 22: end for 23: Return final optimized model M * Gaze .",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "20/32",
      "text": "EmoGaze360-1K and EmoGaze2D-50 datasets construction To create the EmoGaze360-1K dataset, we collected 1,000 panoramic images from platforms like 360cities and Flickr, encompassing a wide range of indoor and outdoor settings. When selecting these images, following PANDORA  55  , we focused on three key criteria: 1) they depict real-world scenes, 2) they contain multiple instances of people and objects per image, and 3) they represent a diverse mix of indoor and outdoor environments. This approach ensures that the dataset aligns with real-world scenarios and applications. All images have a resolution of 1,920  960, providing high-quality visual information. Unlike previous emotion recognition datasets that rely solely on visual stimuli (e.g., images and videos) to evoke emotions but don't incorporate them into their training or testing sets, EmoGaze360-1K includes both eye fixation data and contextual panoramic images. This dual-focus design allows for a deeper understanding of the relationship between visual attention and emotional states, providing a comprehensive dataset for emotion recognition research. Additionally, EmoGaze360-1K includes emotional annotations for six distinct emotional states across multiple modalities, including EEG signals, facial expressions, eye-tracking data, precise visual fixation points, and environmental context. This multimodal design offers more detailed and varied input data for more robust emotion recognition analysis.\n\nTo facilitate the collection of eye fixation data, we compiled a set of 500 emotion-inducing images and 50 emotion-inducing videos from open emotion database such as AffectNet and IAPS (International Affective Picture System) and Youtube, resulting in a total of 2,500 images and 250 videos for each emotional state. These resources are designed to evoke corresponding emotional responses from users before the eye fixation data collection stage.\n\nBased on the panoramic fixation collection approach (i.e., WinDB  56  , a head-mounted display (HMD)-free approach, which is more comfortable than the HMD-based one), we recruited 20 users, including 8 females and 12 males aged between 19-26. All users were completely unfamiliar with the fixation collection process, and none of the images from our pool had been shown to them previously. Note that with the WinDB approach, each user only needs to view the images (with a resolution of 1,920  960) on a PC, with a standard eye tracker set up to record the data. EEG and facial expression data were also collected in parallel during each session to capture additional emotional responses, ensuring that the multimodal annotations reflect both the eye-tracking and physiological aspects of emotion.\n\nTo ensure accurate fixation data collection, each user viewed 100 images corresponding to a single emotion in each session, with the entire fixation process lasting approximately 40 minutes. After a day's break, a second session was conducted to annotate a different emotion. To maintain emotional consistency, an emotional stimulus was administered every 20 images. The process could be paused at any time if the user experienced fatigue or discomfort. However, an emotional stimulus was applied before each annotation to ensure consistent emotional responses.\n\nAfter collecting the eye fixation points, we generated scanpaths for each image using established Scanpath annotation methods  57, 58  . These scanpaths were combined with EEG signals, facial expression data, and visual fixation points to create a rich, multimodal dataset that captures both cognitive and emotional responses to panoramic scenes. Note that the SEED-V-Multimodal dataset is also collected and annotated in this manner.\n\nWe now discuss the features of the proposed dataset and its advantages. It contains 1,000 panoramic images, including 800 indoor scenes and 200 outdoor scenes, spanning 52 categories. The dataset is divided into training and testing sets with a 70/30 ratio, allowing for robust ten-fold training. Here are the key advantages of EmoGaze360-1K:\n\n1) Comprehensive Integration of Fixation Trajectories: Unlike existing datasets that focus on specific eye movement signals like pupil size or diameter, EmoGaze360-1K includes fixation trajectories that reflect interaction with the environment. This comprehensive approach allows for a deeper understanding of visual attention in relation to environmental context, resulting in more accurate emotion recognition. By combining these visual cues with EEG and facial expression data, EmoGaze360-1K provides an even richer understanding of how emotional states are expressed and perceived.\n\n2) Non-Intrusive and Cost-Effective Data Collection: EmoGaze360-1K adopts an HMD-free approach, reducing the discomfort often associated with traditional methods like EEG and surface sensors. This design makes data collection more user-friendly and applicable in real-world scenarios where comfort and acceptance are crucial. Furthermore, using widely available eye-tracking equipment alongside EEG and facial expression recognition, this dataset minimizes the need for expensive, specialized hardware, making it more accessible for broader research applications.\n\nThe EmoGaze2D-50 dataset shares the same collection setup as EmoGaze360-1K, including its multimodal structure, emotional stimuli, and viewpoint tracking methodologies. Like EmoGaze360-1K, EmoGaze2D-50 incorporates data from multiple modalities, such as EEG signals, facial expressions, eye-tracking data, visual fixation points, and emotional annotations across six distinct emotional states. These data provide a rich foundation for studying the interplay between emotional responses and visual attention. The primary difference lies in the type of visual content used for data collection. EmoGaze360-1K utilizes immersive 360-degree panoramic images viewed on flat-screen displays to simulate real-world environments, whereas EmoGaze2D-50 focuses on traditional 2D videos displayed on the same medium.\n\nPrimary Classification. We consider the classification branch in the Generator as the Primary Classification Branch, whose output serves as the final emotion recognition results. The Primary Classification Branch consists of a Transformer encoder and a classification head. We adopt the Transformer encoder and MLP head of ViT 59 as our Transformer encoder and classification head. Specifically, the Transformer encoder takes patch embeddings with positional information as input. As emotion recognition task can be considered as multi-classification problem, different from the MLP head in ViT, which directly predicts the category-specific labels, instead, our classification head transfer to predict the category probability distribution. In order to make the emotion recognition more accurate, we predicted both duration and dispersion as well. Note that this Auxiliary Regression Branch is independent of the generative adversarial network.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Discriminator Of Emogazenet Model",
      "text": "Typically, after the Generator generates the emotion states probability distribution, a simple Discriminator is needed to distinguish the generated emotion states probability distribution and the real emotion states probability distribution. However, in this emotion recognition task, simply doing so poses the problem that the model may mechanically learn the ordering of different patches instead of performing deeper semantic learning, which will eventually lead to incorrect emotion recognition. In emotion recognition tasks, simply relying on visual features may not be sufficient to accurately capture emotional states, as temporal and spatial information of viewpoint trajectories plays a key role in emotion recognition. To address this issue, we design an auxiliary classification branch and a scanpath-guided classification branch, and distancing the distance between ordinary classification features and scanpath-guided temporal and spatial features through an adversarial backward inhibition process to ensure that the adversarial learning process pays more attention to the temporal and spatial dependencies between the image patches in different emotional states.\n\nAuxiliary Classification. The auxiliary classification branch aims to enhance the discriminative power of the model by adding additional supervised signals to prevent model overfitting. The architecture of auxiliary classification branch is the same as primary classification branch. The difference is that the auxiliary classification branch has an additional auxiliary classification neck to extract higher level features, thus enhancing the discriminative power of the features. The features output by the auxiliary classification neck access an auxiliary classification header, which maps the features to the category space through a number of fully connected layers. The last layer uses a Softmax activation function to output a probability distribution for each category.\n\nScanpath-guided Classification. The Scanpath-guided classification branch aims to improve the accuracy of emotion recognition by capturing the temporal and spatial relationships between image patches by utilizing the coordinates of viewpoints in different emotional states to ensure that the model understands the contextual and sequential dependencies inherent in the data. The Scanpath-guided classification branch consists of two parts: Scanpath-guided Classification and Scanpath Prediction.\n\nBy introducing the Scanpath information, the model not only relies on the visual features of the image patches themselves, but also incorporates the information of the eye movement trajectories, making the emotion classification more accurate and reliable. First, we use a Scanpath encoder (e.g., RNN or LSTM) to encode the sequence of viewpoint coordinates. The role of the Scanpath encoder is to form a time sequence representing the movement Scanpath that capture the temporal and spatial relationships of viewpoint movement in different emotional states. The extracted Scanpath features are fed into a classification network, which consists of a neck (e.g., multiple convolutional) and a head (e.g., a fully-connected layer plus a Softmax activation function). The role of the neck is to further extract the high-level features, whereas the head maps the features to the category space and outputs a probability distribution for each category.\n\nScanpath Prediction. The main role of Scanpath Prediction is to reconstruct the Scanpath of the gaze point. By adding the Scanpath prediction task, the model is able to learn both emotion recognition and Scanpath prediction tasks simultaneously. This multi-task learning approach ensures that the adversarial learning process focuses more on temporal and spatial dependencies between image patches in different emotional states, and enhances the generalization ability of the model. The extracted Scanpath features by the Scanpath encoder are fed into the Scanpath prediction decoder to reconstruct gaze point trajectories. Scanpath reconstruction loss (e.g., mean square error loss) is used to measure the difference between the reconstructed Scanpath and the true Scanpath. By minimizing the Scanpath reconstruction loss, the model can gradually learn more accurate Scanpath patterns. Joint training of the sentiment classification task and the Scanpath prediction task allows the two tasks to be mutually reinforcing by sharing some of the network layers, improving the overall model performance.\n\nAdversarial Reverse Suppression. Emotion recognition requires not only considering the visual features of the image, but also understanding the temporal and spatial information of the viewpoint Scanpath, which is crucial for accurately capturing the emotional state. If the ordinary classification features and Scanpath-guided features are too similar, the model may not be able to effectively differentiate between visual features and temporal and spatial features, leading to insufficient understanding of emotion recognition. Through the adversarial backward inhibition process, the distance between the ordinary classification features and the Scanpath-guided features is distanced to avoid the two feature representations from being too similar, thus ensuring that the model pays more attention to the temporal and spatial dependencies between the image patches in different emotional states during the adversarial learning process.\n\nTo achieve, we propose Adversarial Reverse Suppression, which is implemented by an Adversarial Reverse Suppression loss, which includes Adversarial Feature Suppression and Adversarial Classification Suppression.\n\nFor Adversarial Feature Suppression, it aims to pull out the features distance between the output features of Scanpath-guided Classification neck and Auxiliary Classification neck. This is achieved by Mutual information (MI), which is a measure used to quantify the dependency between two random variables. In deep neural networks, mutual information loss can be introduced to adjust the training objective of the network, thereby achieving the suppression or separation of specific information between features. Mutual information is defined as follows:\n\nwhere X and Y are two output features, p(x, y) is the joint probability density function, and p(x) and p(y) are the marginal probability density functions of X and Y , respectively. I(X;Y ) is the mutual information of X and Y . Assuming we use approximation methods to estimate mutual information, the mutual information loss can be defined as:\n\nFor Adversarial Classification Suppression, we first calculate the KL divergence between the category probability distributions from the Auxiliary Classification Branch and the Scanpath-Guided Classification Branch. The KL divergence measures the distance between two distributions, given by the formula:\n\nwhere P and Q represent the category probability distributions of the Auxiliary Classification Branch and the Scanpath-Guided Classification Branch, respectively. Then, we design two suppression terms: adversarial suppression term and reverse suppression term. Add the KL divergence as an adversarial suppression term Q to the loss of the Auxiliary Classification Branch to suppress the learning of Scanpath-guided features, while add the KL divergence P as a reverse suppression term to the loss of the Scanpath-Guided Classification Branch to suppress the learning of common classification features.\n\nThe total loss of the Auxiliary Classification Branch is the sum of its classification loss (L cls , here we use Cross-Entropy Loss) and the adversarial suppression term D KL (PQ):\n\nwhere  is a balancing parameter that adjusts the weight between the classification loss and the adversarial suppression term. The total loss of the Scanpath-Guided Classification Branch is the sum of its classification loss L cls , Scanpath reconstruction loss L rec , and the reverse inhibition term D KL (QP):\n\nwhere  is a balancing parameter that adjusts the weight between the classification loss, Scanpath reconstruction loss, and the reverse suppression term.\n\nCombine the total losses of the Auxiliary Classification Branch and the Scanpath-Guided Classification Branch to form the Adversarial Reverse Suppression loss:\n\nOverall Loss Function\n\nIn addition to the network structure, the design of suitable loss functions is also essential for deep learning models. In this work, the loss functions are divided into two parts: Generator-related loss and Discriminator-related loss.\n\nWe propose two different loss functions for the Generator, including the regression loss (L reg ), and the minmax loss. Unlike the common deep learning models, the GAN framework is optimized by an adversarial training procedure. The effect of generating real data is achieved by optimizing the adversarial relationship between the Generator and the Discriminator using the minmax loss L adv .\n\nThe total loss function of the Generator is the weighted sum of each of the above losses:\n\nWe propose two different loss functions for the Discriminator, including the adversarial suppression loss (L ars_total , Supplementary Equation  7 ) and the categorical cross-entropy loss (L cat_ce ).\n\nTo ensure accurately reconstructing the viewpoint Scanpath, we use a mean squared error Loss (MSE) and dynamic time warping (DTW). Specifically, DTW is used to measure the similarity between two time series (e.g., trajectories) that can handle nonlinear deformations on the time axis, MSE is used to directly measure the error between reconstructed and true Scanpath points. Assuming that the scanpath x = [x 1 , x 2 , ..., x n ] is the true scanpath, x = [ x1 , x2 , ..., xn ] is the reconstructed scanpath and the total Scanpath reconstruction loss function can be written in the following form:\n\nwhere xi is the reconstructed scanpath point and x i is the true Scanpath point.  and  are weighting coefficients to balance the effects of the two components of the loss. The mean squared error Loss can be formulated by:\n\nSpecifically, DTW looks for a path P = [p 1 , p 2 , ..., p L ] where p l = (i l , j l ) means that the i l -th point in x matches the j l -jth point in x such that the total distance:\n\nwhere d( x jl , x il ) is the distance between the matching points in x and x, which is usually used as the Euclidean distance.\n\nIn this work, the Discriminator is trained to distinguish between real and fake data by using a categorical cross-entropy loss. In this case, the output layer of the discriminator will usually be a softmax layer that outputs a probability distribution for each category. The loss function can be defined as:\n\nwhere N is the number of samples, C is the number of categories, y ic is the true label of sample i for class c, and p ic is the predicted probability that sample i belongs to class c. The total loss function of the Discriminator is the weighted sum of each of the above losses:\n\nThe total loss function of the Generator and the Discriminator is as follows:",
      "page_start": 23,
      "page_end": 25
    },
    {
      "section_name": "Detailed Explanations Of Six Indicators In The Experiment Of Emotion-Environment Interactions Across Various Gender And Personality",
      "text": "(1) Emotion Sensitivity: This measures the system's responsiveness to slight emotional changes in users. Higher scores indicate greater sensitivity, making it especially suited for capturing the frequent emotional shifts seen in extroverts and females.\n\n(2) Emotion Stability: This reflects the continuity and consistency of emotional states. Higher scores indicate greater stability in similar environments, as typically seen in males and introverts, who tend to exhibit steadier emotional responses.\n\n(3) Real-Time Response Capture: This measures the system's speed in detecting rapid emotional changes. Higher scores indicate quicker responses, especially effective for capturing emotional shifts in dynamic situations, often seen in extroverts and females.\n\n(4) Emotion Saliency Focus: This indicates the system's focus on emotionally significant targets (such as highly emotional objects). Higher scores mean the system better identifies emotionally salient objects, with extroverts and females often displaying stronger responsiveness in this area.\n\n(5) Context Adaptability: This measures the system's adaptability across different environmental contexts. Higher scores indicate stronger adaptability, making it well-suited for extroverts and females who, due to their emotional sensitivity, are more compatible with such adaptive system features.\n\n(6) Sustained Attention Preference: This assesses the system's sensitivity to users' preference for prolonged focus on emotionally salient objects. Higher scores indicate that the system can more accurately capture sustained attention behaviors, with males and introverts often displaying longer attention spans on specific objects.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "25/32",
      "text": "",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Supplementary Results",
      "text": "",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Quantitative Component Studies Of Emogazenet Model",
      "text": "We evaluated each component of the proposed emotion recognition model. As shown in Supplementary Table  1 , the evaluation matrix clearly demonstrates that the system reaches its peak performance in terms of ACC (80.22), F1 Score (78.86), and cawF1 score (72.22) when all key components, such as STPE and PC, are engaged, highlighting the synergistic impact of these elements on overall system performance. Omitting certain components, like STPE, leads to a notable decrease in system performance, indicating that they play an essential role in the emotion recognition process. Components such as AR and ACH, when incorporated, can moderately enhance the system's accuracy and F1 score, yet their contribution is less pronounced compared to the core components like PC and BEn. Varied combinations of components result in significant performance fluctuations. For instance, the contrast between row 8 and row 11 illustrates that employing all adversarial suppression modules (AFS, ACS) results in superior system performance compared to most other configurations, underscoring the significance of adversarial suppression in curbing overfitting and bolstering the model's generalizability. The multiplicity of these branches fortifies the system's predictive capabilities, particularly within intricate emotion recognition tasks. Scanpath visualization of real 360-degree scenes For real 360-degree static image scenes, as illustrated in Supplementary Figure  14-A , under positive emotions, males primarily focus on prominent objects such as computer screens or slogans on the wall, then scan the workspace, and finally notice the light from outside the window. Females start with details on the desk, such as clutter or office supplies, then gradually observe the layout of the entire room, and finally pay attention to the slogans on the wall and the light from outside the window. Under negative emotions, males' gazes concentrate on the dark or cluttered areas on the floor and desk, paying less attention to the bright parts. Females start from the corners or cluttered areas of the room, then notice the details on the desk, but their gaze lingers more in the dark parts of the room.\n\nFor dynamic scenes, such as two people moving along a fixed route in this corridor, as illustrated in Supplementary Figure  14-B , under positive emotions, males prioritize focusing on the moving figures, then turn to observe the overall layout and lighting of the corridor. Females start observing details of the figures (such as gait, attire), then focus on the posters on the wall and the scenery outside the window, and finally examine the layout of the entire scene. Under negative emotions, males concentrate their attention on the corners or inconspicuous places of the corridor, paying less attention to the dynamic figures. Females start from the shadowy or darker areas, then notice the moving figures, but their gaze may be brief.\n\nIn high-light scenes, under positive emotions, as illustrated in Supplementary Figure  15 -A, males focus on prominent objects on the display screen or desk, then scan the main areas of the meeting room, such as seats and slogans. Females start observing details on the desk, such as wires or documents, then gradually expand to the entire room's layout, including slogans on the wall and the window. Under negative emotions, males mainly focus on the dark parts of the meeting room or clutter on the floor, paying less attention to the bright areas. Females start from the corners or cluttered areas of the room, then notice the slogans on the wall, but their gaze lingers more on the untidy parts.\n\nIn low-light scenes, as illustrated in Supplementary Figure  15 -B, under positive emotions, males' gazes concentrate on the brighter areas, such as the window, then quickly scan the conference table and display screen. Females start with the well-lit window, gradually observe the details on the desk, and finally pay attention to the walls and slogans. Under negative emotions, males mainly focus on the shadows and darker areas of the room, paying less attention to the bright parts. Females start from the shadowy areas or clutter on the desk, then notice the window, but their gaze lingers more in the dark areas.",
      "page_start": 26,
      "page_end": 32
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: -a, Figure 1-b and",
      "page": 1
    },
    {
      "caption": "Figure 1: -b-Method 2) like EEG (electroencephalography)3840",
      "page": 1
    },
    {
      "caption": "Figure 1: -b-Method 3), as a non-contact and naturally occurring",
      "page": 1
    },
    {
      "caption": "Figure 1: -a-right). Using only standard HD cameras, our",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison of existing emotion recognition methods. a Comparing traditional explicit-centric and novel",
      "page": 2
    },
    {
      "caption": "Figure 3: -b(2)) that",
      "page": 2
    },
    {
      "caption": "Figure 2: Human-environment interaction for contextual gaze-based emotion recognition. (a) illustrates the concept of leveraging",
      "page": 3
    },
    {
      "caption": "Figure 3: -b(3)). In other words, we understand emotion as an",
      "page": 3
    },
    {
      "caption": "Figure 1: -d). Notably, our method",
      "page": 3
    },
    {
      "caption": "Figure 2: -a)? To implement this, we must first identify",
      "page": 3
    },
    {
      "caption": "Figure 3: Calibration and semantic-aware modeling for improved contextual gaze-based emotion recognition. a shows an online",
      "page": 4
    },
    {
      "caption": "Figure 1: -b-Method 4). This",
      "page": 4
    },
    {
      "caption": "Figure 2: -b). Using commonly",
      "page": 4
    },
    {
      "caption": "Figure 2: -c). Crucially, it ensures that users remain",
      "page": 4
    },
    {
      "caption": "Figure 2: -d). Factors",
      "page": 4
    },
    {
      "caption": "Figure 3: -a & Methods  Online Personalized Calibration) that",
      "page": 4
    },
    {
      "caption": "Figure 2: -(c)), we collect raw eye",
      "page": 4
    },
    {
      "caption": "Figure 1: -b-Method 3 and Figure 3-b(2)), SIO",
      "page": 4
    },
    {
      "caption": "Figure 3: -b(3)). By mapping fixation",
      "page": 4
    },
    {
      "caption": "Figure 4: Experimental validation of our proposed method against other methods and its performance on screen scenes.",
      "page": 5
    },
    {
      "caption": "Figure 4: -a, we can clearly see the",
      "page": 6
    },
    {
      "caption": "Figure 4: -c illustrates the classification",
      "page": 6
    },
    {
      "caption": "Figure 4: -d, the PR curves for Fear and Disgust show high",
      "page": 6
    },
    {
      "caption": "Figure 5: Experimental validation of our proposed method on different settings and its performance on real scenes. a For real",
      "page": 7
    },
    {
      "caption": "Figure 4: -e(A)), under positive",
      "page": 7
    },
    {
      "caption": "Figure 4: -e(B)), under positive emotions, males",
      "page": 7
    },
    {
      "caption": "Figure 5: -a, Happy has the",
      "page": 8
    },
    {
      "caption": "Figure 5: -c illustrates the performance improvements of our",
      "page": 8
    },
    {
      "caption": "Figure 5: -e). A paired-",
      "page": 8
    },
    {
      "caption": "Figure 6: -d showed that in the campus scenario, our",
      "page": 8
    },
    {
      "caption": "Figure 6: Robustness validation of our proposed method and component evaluation. a Long-term stability monitoring experiment",
      "page": 9
    },
    {
      "caption": "Figure 6: -e, our context",
      "page": 10
    },
    {
      "caption": "Figure 1: -d-left). Second,",
      "page": 10
    },
    {
      "caption": "Figure 1: -d-middle). Third, our",
      "page": 10
    },
    {
      "caption": "Figure 1: -d-right). These research directions underscore the broad",
      "page": 10
    },
    {
      "caption": "Figure 6: -b, for viewing 360 images on a screen, with-",
      "page": 10
    },
    {
      "caption": "Figure 6: -c, personalized",
      "page": 10
    },
    {
      "caption": "Figure 7: -a, 360 videos demonstrated superior perfor-",
      "page": 10
    },
    {
      "caption": "Figure 7: Ablation studies on Real360 dataset. a When collecting users gaze points in the real-world scene of the Real360 dataset,",
      "page": 11
    },
    {
      "caption": "Figure 7: -b, the eye tracker",
      "page": 11
    },
    {
      "caption": "Figure 7: -c shows the performance variations when using dif-",
      "page": 11
    },
    {
      "caption": "Figure 7: -d, the quantitative results reveal that cawF1 scores are",
      "page": 12
    },
    {
      "caption": "Figure 7: -e). Additionally, when",
      "page": 12
    },
    {
      "caption": "Figure 7: and Supplementary Fig. 8).",
      "page": 12
    },
    {
      "caption": "Figure 3: -a). For each position within the space, a segment",
      "page": 12
    },
    {
      "caption": "Figure 4: ). This projection and prediction pro-",
      "page": 12
    },
    {
      "caption": "Figure 5: ). Since the current advanced gaze",
      "page": 12
    },
    {
      "caption": "Figure 3: -a and Methods",
      "page": 13
    },
    {
      "caption": "Figure 3: -c & Supplementary Figure 1,",
      "page": 13
    },
    {
      "caption": "Figure 3: -a). This method focuses on leveraging two key",
      "page": 13
    },
    {
      "caption": "Figure 2: -c & Supplementary Methods  Third-Person",
      "page": 14
    },
    {
      "caption": "Figure 8: Pipeline of the proposed EmoGazeNet model. The proposed EmoGazeNet model consists of a Generator and a",
      "page": 22
    },
    {
      "caption": "Figure 8: , the design of EmoGazeNet consists of two main components: the Generator and the Discriminator. The Generator is",
      "page": 22
    },
    {
      "caption": "Figure 9: & 10). Our findings indicate that employing HAT for scanpath prediction significantly enhances the cawF1 score of emotion recognition,",
      "page": 27
    },
    {
      "caption": "Figure 9: Performance comparison of different scanpath prediction methods (HAT57, IndivScan58, ScanGan36060, and ScanDMM61)",
      "page": 27
    },
    {
      "caption": "Figure 10: Performance comparison of different choices of base encoders (GNN62, CNN63, LSTM64, and Transformer65) in the",
      "page": 28
    },
    {
      "caption": "Figure 11: shows a performance comparison across various gaze point prediction methods: ShanghaiTechGaze, L2CS-Net,",
      "page": 29
    },
    {
      "caption": "Figure 11: Performance comparison of different gaze point prediction methods (ShanghaiTechGaze66, L2CS-Net67, GazeTR68, and",
      "page": 29
    },
    {
      "caption": "Figure 12: shows the performance comparison of different projection intervals (T=0.1, T=0.2, T=0.3) across three metrics:",
      "page": 30
    },
    {
      "caption": "Figure 12: Performance comparison of different projection intervals (T=0.1, T=0.2, T=0.3) during eye appearance acquisition and",
      "page": 30
    },
    {
      "caption": "Figure 13: shows the performance variations when different proportions (20%, 40%, 60%, 80%, and 100%) of the",
      "page": 31
    },
    {
      "caption": "Figure 13: Performance comparison of different number of EmoGaze360-1K dataset in training EmoGazeNet model.",
      "page": 31
    },
    {
      "caption": "Figure 14: -A, under positive emotions, males primarily focus on",
      "page": 32
    },
    {
      "caption": "Figure 15: -A, males focus on prominent objects on the",
      "page": 32
    },
    {
      "caption": "Figure 15: -B, under positive emotions, males gazes concentrate on the brighter",
      "page": 32
    },
    {
      "caption": "Figure 14: Scanpath visualization of real 360-degree static (A) and dynamic (B) scenes in different genders and emotion states.",
      "page": 32
    },
    {
      "caption": "Figure 15: Scanpath visualization of real 360-degree high-light (A) and low-light (B) scenes in different genders and emotion states.",
      "page": 32
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ScanGAN360": "ScanDMM\nIndivScan"
        },
        {
          "ScanGAN360": "HAT"
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GNN": "CNN\nLSTM"
        },
        {
          "GNN": "Transformer"
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "L2CS-Net": "GazeTR\nAFF-Net"
        },
        {
          "L2CS-Net": "ShanghaiTechGaze"
        }
      ],
      "page": 29
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Dimensions underlying the representational alignment of deep neural networks with humans",
      "authors": [
        "F Mahner",
        "L Muttenthaler",
        "U Gl",
        "M Hebart"
      ],
      "year": "2024",
      "venue": "Dimensions underlying the representational alignment of deep neural networks with humans",
      "arxiv": "arXiv:2406.19087"
    },
    {
      "citation_id": "2",
      "title": "Human-ai collaboration enables more empathic conversations in text-based peer-to-peer mental health support",
      "authors": [
        "A Sharma",
        "I Lin",
        "A Miner",
        "D Atkins",
        "T Althoff"
      ],
      "year": "2023",
      "venue": "Nat. Mach. Intell"
    },
    {
      "citation_id": "3",
      "title": "Homeostasis and soft robotics in the design of feeling machines",
      "authors": [
        "K Man",
        "A Damasio"
      ],
      "year": "2019",
      "venue": "Nat. Mach. Intell"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "5",
      "title": "Lstm-based emotion detection using physiological signals: Iot framework for healthcare and distance learning in covid-19",
      "authors": [
        "M Awais"
      ],
      "year": "2020",
      "venue": "IEEE Internet Things J"
    },
    {
      "citation_id": "6",
      "title": "Natural emotion vocabularies as windows on distress and well-being",
      "authors": [
        "V Vine",
        "R Boyd",
        "J Pennebaker"
      ],
      "year": "2020",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "7",
      "title": "Associations between mental health, blood pressure and the development of hypertension",
      "authors": [
        "H Schaare"
      ],
      "year": "2023",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "8",
      "title": "The brain structure, inflammatory, and genetic mechanisms mediate the association between physical frailty and depression",
      "authors": [
        "R Jiang"
      ],
      "year": "2024",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "9",
      "title": "Curriculum cyclegan for textual sentiment domain adaptation with multiple sources",
      "authors": [
        "S Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the Web Conference 2021"
    },
    {
      "citation_id": "10",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Modulating emotional states of rats through a rat-like robot with learned interaction patterns",
      "authors": [
        "G Jia"
      ],
      "year": "2024",
      "venue": "Nat. Mach. Intell"
    },
    {
      "citation_id": "12",
      "title": "A probabilistic map of emotional experiences during competitive social interactions",
      "authors": [
        "J Heffner",
        "O Feldmanhall"
      ],
      "year": "2022",
      "venue": "Nat. communications"
    },
    {
      "citation_id": "13",
      "title": "Automatic ecg-based emotion recognition in music listening",
      "authors": [
        "Y.-L Hsu",
        "J.-S Wang",
        "W.-C Chiang",
        "C.-H Hung"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "14",
      "title": "esee-d: Emotional state estimation based on eye-tracking dataset",
      "authors": [
        "V Skaramagkas"
      ],
      "year": "2023",
      "venue": "Brain Sci"
    },
    {
      "citation_id": "15",
      "title": "Toward label-efficient emotion and sentiment analysis",
      "authors": [
        "S Zhao",
        "X Hong",
        "J Yang",
        "Y Zhao",
        "G Ding"
      ],
      "year": "2023",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "16",
      "title": "Measuring emotions during epistemic activities: the epistemically-related emotion scales",
      "authors": [
        "R Pekrun",
        "E Vogl",
        "K Muis",
        "G Sinatra"
      ],
      "year": "2017",
      "venue": "Cogn. Emot"
    },
    {
      "citation_id": "17",
      "title": "Measures of emotion",
      "authors": [
        "J Russell"
      ],
      "year": "1989",
      "venue": "The Measurement of Emotions"
    },
    {
      "citation_id": "18",
      "title": "The level of expressed emotion scale: A useful measure of expressed emotion in adolescents?",
      "authors": [
        "S Nelis",
        "G Rae",
        "C Liddell"
      ],
      "year": "2011",
      "venue": "J. Adolesc"
    },
    {
      "citation_id": "19",
      "title": "Encoding of multi-modal emotional information via personalized skin-integrated wireless facial interface",
      "authors": [
        "J Lee"
      ],
      "year": "2024",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "20",
      "title": "Multimodal emotion classification with multi-level semantic reasoning network",
      "authors": [
        "T Zhu",
        "L Li",
        "J Yang",
        "S Zhao",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Multimed"
    },
    {
      "citation_id": "21",
      "title": "A circularstructured representation for visual emotion distribution learning",
      "authors": [
        "J Yang",
        "J Li",
        "L Li",
        "X Wang",
        "X Gao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "22",
      "title": "Emotional attention detection and correlation exploration for image emotion distribution learning",
      "authors": [
        "Z Xu",
        "S Wang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "23",
      "title": "Progressive visual content understanding network for image emotion classification",
      "authors": [
        "J Pan",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Weakly supervised video emotion detection and prediction via cross-modal temporal erasing network",
      "authors": [
        "Z Zhang",
        "L Wang",
        "J Yang"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Smg: A micro-gesture dataset towards spontaneous body gestures for emotional stress state analysis",
      "authors": [
        "H Chen",
        "H Shi",
        "X Liu",
        "X Li",
        "G Zhao"
      ],
      "year": "2023",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "26",
      "title": "Inferring internal states across mice and monkeys using facial features",
      "authors": [
        "A Tlaie"
      ],
      "year": "2025",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "27",
      "title": "Professional actors demonstrate variability, not stereotypical expressions, when portraying emotional states in photographs",
      "authors": [
        "Le Mau"
      ],
      "year": "2021",
      "venue": "Nat. Commun"
    },
    {
      "citation_id": "28",
      "title": "Brain-machine coupled learning method for facial emotion recognition",
      "authors": [
        "D Liu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "29",
      "title": "Class activation regularization-based facial emotion recognition network and its application in students' emotional engagement assessment",
      "authors": [
        "L Xu",
        "Y Gan",
        "Y Jin"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "30",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "32",
      "title": "Analyzing continuous-time and sentence-level annotations for speech emotion recognition",
      "authors": [
        "L Martinez-Lucas",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "33",
      "title": "Secure human action recognition by encrypted neural network inference",
      "authors": [
        "M Kim",
        "X Jiang",
        "K Lauter",
        "E Ismayilzada",
        "S Shams"
      ],
      "year": "2022",
      "venue": "Nat. communications"
    },
    {
      "citation_id": "34",
      "title": "Leveraging spatio-temporal convolutions for gait-based emotion recognition on videos",
      "authors": [
        "M Lima",
        "W De Lima Costa",
        "E Martnez",
        "V Teichrieb",
        "St-Gait++"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "See your emotion from gait using unlabeled skeleton data",
      "authors": [
        "H Lu",
        "X Hu",
        "B Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "37",
      "title": "Exploring eeg features in cross-subject emotion recognition",
      "authors": [
        "X Li"
      ],
      "year": "2018",
      "venue": "Front. Neurosci"
    },
    {
      "citation_id": "38",
      "title": "Fbstcnet: A spatiotemporal convolutional network integrating power and connectivity features for eeg-based emotion decoding",
      "authors": [
        "W Huang",
        "W Wang",
        "Y Li",
        "W Wu"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "39",
      "title": "Fine-grained interpretability for eeg emotion recognition: Concat-aided grad-cam and systematic brain functional network",
      "authors": [
        "B Liu",
        "J Guo",
        "C Chen",
        "X Wu",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "40",
      "title": "Beyond mimicking underrepresented emotions: deep data augmentation with emotional subspace constraints for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "41",
      "title": "Vreed: Virtual reality emotion recognition dataset using eye tracking & physiological measures",
      "authors": [
        "L Tabbaa"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "42",
      "title": "Student engagement detection using emotion analysis, eye tracking and head movement with machine learning",
      "authors": [
        "P Sharma"
      ],
      "year": "2022",
      "venue": "International Conference on Technology and Innovation in Learning, Teaching and Education"
    },
    {
      "citation_id": "43",
      "title": "The influence of service environments on customer emotion and service outcomes",
      "authors": [
        "J.-S Lin",
        "H.-Y Liang"
      ],
      "year": "2011",
      "venue": "Manag. Serv. Qual. An Int. J"
    },
    {
      "citation_id": "44",
      "title": "Emotion and the environment: the forgotten dimension",
      "authors": [
        "M Farshchi",
        "N Fisher"
      ],
      "year": "1999",
      "venue": "Creating the Productive Workplace"
    },
    {
      "citation_id": "45",
      "title": "Social anxiety and romantic relationships: The costs and benefits of negative emotion expression are context-dependent",
      "authors": [
        "T Kashdan",
        "J Volkmann",
        "W Breen",
        "S Han"
      ],
      "year": "2007",
      "venue": "J. Anxiety Disord"
    },
    {
      "citation_id": "46",
      "title": "Encoding of emotional memories depends on amygdala and hippocampus and their interactions",
      "authors": [
        "M Richardson",
        "B Strange",
        "R Dolan"
      ],
      "year": "2004",
      "venue": "Nat. Neurosci"
    },
    {
      "citation_id": "47",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Ment. Dev"
    },
    {
      "citation_id": "48",
      "title": "Eeg emotion recognition using attention-based convolutional transformer neural network",
      "authors": [
        "L Gong",
        "M Li",
        "T Zhang",
        "W Chen"
      ],
      "year": "2023",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "49",
      "title": "Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "A Toisoul",
        "J Kossaifi",
        "A Bulat",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2021",
      "venue": "Nat. Mach. Intell"
    },
    {
      "citation_id": "50",
      "title": "Cross-cultural emotion recognition with eeg and eye movement signals based on multiple stacked broad learning system",
      "authors": [
        "X Gong",
        "C Chen",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Comput. Soc. Syst"
    },
    {
      "citation_id": "51",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "J. Pers. Soc. Psychol"
    },
    {
      "citation_id": "52",
      "title": "Multiview multitask gaze estimation with deep convolutional neural networks",
      "authors": [
        "D Lian"
      ],
      "year": "2018",
      "venue": "IEEE TNNLS"
    },
    {
      "citation_id": "53",
      "title": "Top-down versus bottom-up attentional control: A failed theoretical dichotomy",
      "authors": [
        "E Awh",
        "A Belopolsky",
        "J Theeuwes"
      ],
      "year": "2012",
      "venue": "Trends Cogn. Sci"
    },
    {
      "citation_id": "54",
      "title": "Top-down and bottom-up control of visual selection",
      "authors": [
        "J Theeuwes"
      ],
      "year": "2010",
      "venue": "Acta Psychol"
    },
    {
      "citation_id": "55",
      "title": "Pandora: A panoramic detection dataset for object with orientation",
      "authors": [
        "H Xu"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "Hmd-free and distortion-free panoptic video fixation learning",
      "authors": [
        "G Wang",
        "C Chen",
        "A Hao",
        "H Qin",
        "D.-P Fan",
        "Windb"
      ],
      "year": "2023",
      "venue": "Hmd-free and distortion-free panoptic video fixation learning"
    },
    {
      "citation_id": "57",
      "title": "Unifying top-down and bottom-up scanpath prediction using transformers",
      "authors": [
        "Z Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Beyond average: Individualized visual scanpath prediction",
      "authors": [
        "X Chen",
        "M Jiang",
        "Q Zhao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "60",
      "title": "Scangan360: A generative model of realistic scanpaths for 360 images",
      "authors": [
        "D Martin",
        "A Serrano",
        "A Bergman",
        "G Wetzstein",
        "B Masia"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Vis. Comput. Graph"
    },
    {
      "citation_id": "61",
      "title": "Scandmm: A deep markov model of scanpath prediction for 360deg images",
      "authors": [
        "X Sui",
        "Y Fang",
        "H Zhu",
        "S Wang",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "Improving rgb-d salient object detection via modality-aware decoder",
      "authors": [
        "M Song",
        "W Song",
        "G Yang",
        "C Chen"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "63",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "64",
      "title": "Long-term traffic prediction based on lstm encoder-decoder architecture",
      "authors": [
        "Z Wang",
        "X Su",
        "Z Ding"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Intell. Transp. Syst"
    },
    {
      "citation_id": "65",
      "title": "Rethinking object saliency ranking: A novel whole-flow processing paradigm",
      "authors": [
        "M Song",
        "L Li",
        "D Wu",
        "W Song",
        "C Chen"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Image Process"
    },
    {
      "citation_id": "66",
      "title": "Multiview multitask gaze estimation with deep convolutional neural networks",
      "authors": [
        "D Lian"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst"
    },
    {
      "citation_id": "67",
      "title": "L2cs-net: Fine-grained gaze estimation in unconstrained environments",
      "authors": [
        "A Abdelrahman",
        "T Hempel",
        "A Khalifa",
        "A Al-Hamadi"
      ],
      "year": "2022",
      "venue": "L2cs-net: Fine-grained gaze estimation in unconstrained environments",
      "arxiv": "arXiv:2203.03339"
    },
    {
      "citation_id": "68",
      "title": "Gaze estimation using transformer",
      "authors": [
        "Y Cheng",
        "F Lu"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "69",
      "title": "Adaptive feature fusion network for gaze tracking in mobile tablets",
      "authors": [
        "Y Bao",
        "Y Cheng",
        "Y Liu",
        "F Lu"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    }
  ]
}