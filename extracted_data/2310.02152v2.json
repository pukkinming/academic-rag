{
  "paper_id": "2310.02152v2",
  "title": "Graph Neural Network-Based Eeg Classification: A Survey",
  "published": "2023-10-03T15:40:03Z",
  "authors": [
    "Dominik Klepl",
    "Min Wu",
    "Fei He"
  ],
  "keywords": [
    "graph neural network",
    "classification",
    "EEG",
    "neuroscience",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Graph neural networks (GNN) are increasingly used to classify EEG for tasks such as emotion recognition, motor imagery and neurological diseases and disorders. A wide range of methods have been proposed to design GNN-based classifiers. Therefore, there is a need for a systematic review and categorisation of these approaches. We exhaustively search the published literature on this topic and derive several categories for comparison. These categories highlight the similarities and differences among the methods. The results suggest a prevalence of spectral graph convolutional layers over spatial. Additionally, we identify standard forms of node features, with the most popular being the raw EEG signal and differential entropy. Our results summarise the emerging trends in GNN-based approaches for EEG classification. Finally, we discuss several promising research directions, such as exploring the potential of transfer learning methods and appropriate modelling of cross-frequency interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Electroencephalography (EEG) is a non-invasive technique used for recording electrical brain activity with a wide range of applications in cognitive neuroscience  [1] , clinical diagnosis  [2, 3] , and brain-computer interfaces  [4, 5] . However, analysing EEG signals poses several challenges, including a low signal-to-noise ratio, nonstationarity resulting from brain dynamics, and the multivariate nature of the signals  [6, 7] . In this review, we focus on the classification of EEG, such as emotion recognition, motor imagery recognition or neurological disorders and diseases.\n\nTraditional feature extraction methods for EEG classification, such as common spatial patterns  [6] , wavelet transform  [8] , and Hilbert-Huang transform  [9] , have been commonly employed. These methods aim to extract meaningful features from EEG signals  [10, 11] , with key features like power spectral density (PSD)  [7]  to characterise brain states. However, relying on such manually defined features to train machine learning classifiers has several limitations. Subjectivity and biases in feature selection, along with time-consuming engineering and selection processes, limit scalability and generalisation  [7, 12] . Automated feature extraction methods are needed to overcome these limitations, improve efficiency, reduce bias, and enhance classifier adaptability to different EEG datasets.\n\nDeep learning architectures, such as convolutional neural networks (CNN) and long short-term memory (LSTM) networks, have also been explored for EEG analysis  [13, 14] . However, they face challenges in effectively capturing the spatial dependencies between electrodes and handling the temporal dynamics of EEG signals  [7] . Modelling the complex sequential and spatial relation- * Correspondence to: fei.he@coventry.ac.uk ships in EEG data is crucial for more accurate classification and analysis.\n\nNetwork neuroscience offers an alternative approach to EEG modelling by framing the signals as a graph. The brain exhibits a complex network structure, with neurons forming connections and communicating with each other  [15] . Analysing EEG data as a graph enables the study of network properties, including functional connectivity, providing insights into brain function and dysfunction  [12, 16, 17] . Graph-based analysis facilitates the examination of network features, node importance, community structure, and information flow, offering insights into brain organisation and dynamics. Such graph-theorybased features were shown to be powerful predictive features for EEG classification  [12, [17] [18] [19] [20] [21] [22] . However, these features have the same limitations as manually defined features based on traditional EEG analysis methods introduced above.\n\nGraph Neural Networks (GNNs) emerge as a powerful tool for modelling neurophysiological data  [23] , such as EEG, within the network neuroscience framework  [7, 24] . GNNs are specifically designed to operate on graphstructured data. They can effectively leverage the spatial structure within EEG data to extract features, uncover patterns and make predictions based on the complex interactions between different electrodes. Designing GNN models for EEG classification will likely improve classification tasks and potentially uncover new insights in neuroscience.\n\nMotivated by the potential of GNNs and an increasing number of recent papers proposing GNN for various EEG classification tasks, there is an urgent need for a comprehensive review of GNN models for EEG classification. The main contributions of this paper include:\n\n• Identifying emerging trends of GNN models tailored for EEG classification.\n\n• Reviewing popular graph convolutional layers and their applicability to EEG data. • Providing a unified overview of node feature and brain graph structure definitions in the context of EEG analysis.\n\n• Examining techniques for transforming sets of node feature embeddings into a single graph embedding for graph classification tasks.\n\nBy addressing these essential aspects, this review paper will provide a comprehensive and in-depth analysis of the application of Graph Neural Network (GNN) models for EEG classification. The findings and insights gained from this review will serve as a resource to navigate this emerging field and identify promising future research directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Overview Of Graph Neural Networks",
      "text": "Graphs are widely used to capture complex relationships and dependencies in various domains, such as social networks, biological networks, and knowledge graphs. The problem of graph classification, which aims to assign a label to an entire graph, has gained attention in recent years. GNNs offer a promising solution to this problem by extending the concept of convolution from Euclidean inputs to graph-structured data. GNNs have been successfully applied in a wide range of fields, such as biology  [23] , bioinformatics  [25] , network neuroscience  [26] , chemistry  [27, 28] , drug design and discovery  [29, 30] , natural language processing  [31, 32] , recommendation systems  [33, 34] , traffic prediction  [35, 36]  and finance  [37] .\n\nIn graph classification problems, the input is a set of graphs, each with its own set of nodes, edges, and node Compared to other deep learning models, GNNs offer several advantages. First, GNNs were specifically designed for graph-structured inputs. This means that GNNs can adapt to irregularly structured inputs, i.e. graphs with varying numbers of nodes, compared to traditional deep learning, such as CNN, that require fixedsize inputs. Next, GNNs can simultaneously learn information from node features and the graph structure by accepting two inputs: node feature matrix and graph structure. Such simultaneous integration is not possible with traditional deep learning methods.\n\nMultiple types of GNNs have been well introduced in  [38, 39] . In this survey, we briefly introduce the two main branches of GNNs, namely, spatial and spectral GNNs (Fig.  2 ). Other types of GNNs, such as attention GNNs  [40] , recurrent GNNs  [41] , and graph transformers  [42] , can be viewed as special cases of spatial GNNs, and thus we will not provide detailed discussion in this survey. Both spatial and spectral GNNs aim to extend the convolution mechanism to graph data. For a detailed review of their similarities and differences, see  [43] . Moreover, for a comparison of GNNs in terms of computational complexity, see  [38] .\n\nSpatial GNNs aggregate information from neighbour-ing nodes, similar to traditional convolution applied to image data aggregating information from adjacent pixels. Stacking multiple spatial GNN layers leads to information aggregation from various scales going from local to global patterns being captured in early and later layers, respectively. In contrast, spectral GNNs perform information aggregation in the graph frequency domain, with low-frequency and high-frequency components capturing global and local patterns, respectively. However, both approaches learn to capture local and global patterns within the graph, i.e. high and low-frequency information in the spectral domain. The advantage of spectral GNNs is their connection to graph signal processing, allowing for interpretation from the perspective of graph filters. However, spectral GNNs do not generalise well to large graphs since they depend on the eigendecomposition of graph Laplacian. In contrast, spatial GNNs can be applied to large graphs since they perform only local message-passing. On the other hand, spatial GNNs may be challenging to interpret and prone to overfitting because of over-smoothing, where embeddings of all nodes become similar.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Spatial Gnns",
      "text": "Spatial GNNs directly operate on the graph structure via the adjacency matrix operator. Given a set of nodes and associated features, spatial GNNs perform neighbourhood aggregation to derive node embeddings. This process is referred to as message passing. Intuitively, nodes connected by edges should have similar node embeddings, i.e. local node similarity. Message passing implements this idea by updating node embeddings with aggregated information collected from the node's neighbourhood. Formally, the node update equation in l th layer of spatial GNN with L layers is defined as follows:\n\nwhere h i is the node embedding vector, or when l = 1, this is the input node feature vector, σ is the activation function, is the aggregation function, N (v i ) is the neighbourhood of node v i , W ∈ R d1×d2 is a learnable parameter matrix projecting node embeddings from input dimension d 1 to hidden dimension d 2 and e ji is the edge weight (e ji = 1 for unweighted graphs).\n\nA single spatial GNN layer aggregates information from the 1-hop neighbourhood. Thus, to increase the reception field of the model, L spatial GNN layers can be stacked to aggregate information from up to L-hop neighbourhoods. A disadvantage of spatial GNNs is the difficulty of training deep models with many layers. With an increasing number of layers, the node embeddings become increasingly smooth, i.e. variance among embeddings of all nodes decreases. This happens when the messages already contain aggregated information from the whole graph; continual message passing of such saturated messages leads to oversmoothing, i.e., all node embeddings becoming essentially identical.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Spectral Gnns",
      "text": "Spectral GNNs can also be applied to EEG classification tasks by leveraging the spectral domain analysis of graph-structured data. The EEG graph is transformed into the spectral domain using the Graph Fourier Transform (GFT) and Graph Signal Processing (GSP) techniques. For a detailed review of spectral GNN methods, please refer to  [44] .\n\nThe graph spectrum is defined as the eigendecomposition of the graph Laplacian matrix. The GFT is then defined as Ĥ = U T H, its inverse as H = U Ĥ, where U is the orthonormal matrix of eigenvectors of the graph Laplacian L and H ∈ R N ×D is the matrix of node feature vectors with N and D being the number of nodes and dimensionality of node features, respectively. The graph Laplacian is defined as L = D -A, but often the normalised version is preferred:\n\n(A and D are the adjacency and degree matrices, respectively).\n\nSpectral GNN is then typically defined as the convolution ( * ) of a signal defined on graph H and a spatial kernel g in the spectral domain, thus becoming an element-wise multiplication (⊙):\n\nGenerally, U T g is defined as a learnable diagonal matrix G = diag(g 1 , ..., g V ) spectral filter  [44] .\n\nHowever, the full spectral graph convolution can be computationally expensive. A popular approximation is the Chebyshev GNN (ChebConv)  [45] , which performs localised spectral filtering on the graph. The node embedding update equation of a ChebConv is defined as:\n\nwhere\n\nis the largest eigenvalue of L, often approximated as λ max = 2). The K parameter controls the size of the Chebyshev filter.\n\nHowever, spectral GNNs are limited to input graphs with a fixed number of nodes. This is because of the explicit use of the graph Laplacian. This is in contrast to spatial GNNs, which do not rely on explicitly materialising the adjacency matrix.\n\nFIG.  2 : Illustration of core mechanisms of spatial and spectral GNNs. A) An undirected featured graph is given as an example input graph with node features shown as node labels and colours. B) Spatial GNNs operate in the graph domain directly using message passing to update node embeddings. 1) Messages, i.e. transformed node features or embeddings, are sent along edges. For simplicity, we show only one direction of the flow of messages. 2) The collected messages at each node are aggregated using a permutation-invariant function and are fused with the original node embedding to form an updated node embedding. Thus, one spatial GNN layer results in node embeddings containing information about the 1-hop neighbourhood of a given node. Thus, L layers are required for node embeddings to access the information from the L-hop neighbourhood. C) In contrast, spectral GNNs operate in the graph spectral domain. 1) Node features are treated as signals on top of a graph and are deconstructed into graph frequencies given by the eigendecomposition of the graph Laplacian. Graph frequencies can be interpreted as variations of the signals.\n\n2) The contribution of each graph frequency is weighted by the set of learnable kernels G that effectively function as graph filters. 3) Node embeddings are then obtained by aggregating the filtered graph frequencies and transforming them back to the spatial graph domain. Thus, full spectral GNNs can access information from N -hop neighbourhoods where N is the number of nodes of a given graph. However, in practice, approximations such as Chebyshev graph convolution restrict this to the chosen hop size.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Survey Results",
      "text": "This survey is based on a review of 63 articles. These articles were selected by title and abstract screening from a search on Google Scholar and ScienceDirect queried on November 1st, 2022. The search query for collecting the articles was defined as: (\"Graph neural network\" OR \"Graph convolutional network\") AND (\"Electroencephalography\" OR \"EEG\"). Both peer-reviewed articles and preprints were searched and utilised. All types of EEG classification tasks were included. We summarise the various types of EEG classification tasks identified in the surveyed papers in Fig 3 . The most common classification tasks are emotion recognition, epilepsy diagnosis and detection and motor imagery. However, the type of classification task should have a relatively minor effect on the GNN architecture design. Thus, we do not analyse and discuss this in detail. Instead, we survey the various GNN-based methods for EEG classification, intending to systematically categorise the types of GNN modules and identify emerging trends in this field independent of the specific classification task.\n\nIn the remaining portion of this paper, we report the categories of comparisons we identified in the surveyed papers. These are based on the different modules of the proposed GNN-based models. Specifically, these are: The following sections will provide further details on these categories, and the paper will conclude by discussing trends and proposing plausible directions for future research.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Definition Of Brain Graph Structure",
      "text": "The first part of the input to a GNN model is the brain graph structure inferred from the EEG data itself (Fig.  1A ). We summarise the methods for defining the brain graphs in Table  I . These methods can be generally categorised as learnable or pre-defined.\n\nAn alternative categorisation of the brain graph structures is the functional (FC) and the \"structural\" connectivity (SC). Generally, SC graphs are pre-defined, whereas FC graphs can be both pre-defined and learnable. SC in the classical sense of physical connections between brain regions is not possible to obtain using EEG signals since these are recorded at the scalp surface. Instead, we use the term to describe methods that construct brain graphs based on the physical distance between EEG electrodes. In contrast, FC refers to pairwise statistical relationships between EEG signals.\n\nSC graph is pre-defined such that electrodes are connected by an edge in the following way:\n\nwhere e ij is the edge weight connecting nodes i and j, d ij is a measure of distance between EEG electrodes, and t is a manually defined threshold controlling the graph sparsity. Such an approach offers several advantages. First, the SC graph is insensitive to any noise effects of EEG recording since it is independent of the actual signals. Second, all data samples share an identical graph structure, provided the same EEG montage was utilised during the recording. This offers explainability advantages when combined with spectral GNN since the graph frequency components defined by the eigenvectors of graph Laplacian are fixed. On the other hand, the SC graph is limited to short-range relationships. Thus, it might not accurately represent the underlying brain network. Some papers propose to overcome this limitation by manually inserting global  [53, [56] [57] [58] 62]  or inter-hemispheric edges  [46, 54, 87] .\n\nIn contrast, an FC graph can be obtained from either classical FC measures (FC measure in Table  I  or learnable methods (e.g. feature concatenation/distance and attention methods in Table  I ). We refer to all of these methods as FC because they all measure the degree of interaction between two nodes, thus falling within the traditional definition of FC. Unlike SC, the FC graph is unique for each data sample and can contain both shortand long-range edges. On the other hand, since it is derived directly from EEG signals, it might be sensitive to noise.\n\nLearnable FC based on node feature distance or feature concatenation are generally computed as:\n\n(5)\n\nrespectively, where θ 1 (•) and θ 2 (•) are neural networks with input-output dimensions of R : d → 1 and R : 2 × d → 1, respectively; | • | denotes absolute value; ∥ denotes concatenation and h i is the node feature/embedding of node i. We discuss the attention-based graphs together with the types of graph convolutional layers in Section VI and thus skip these methods in this section. Special cases of brain graph definition are the sharedmask methods. These methods defined a matrix of learnable parameters with the same shape as the adjacency matrix of the input graphs that acts as a mask/filter by multiplying it with the adjacency matrix. This learnable matrix is a part of the model. Thus, the same mask is applied to all input graphs. However, a shared mask limits the size of the input graphs, i.e. the number of nodes must remain fixed so that the adjacency matrix can be multiplied with the shared mask. In the current stage, which method should be preferred for brain graph classification tasks is unclear. Some authors attempt to avoid this issue by combining multiple methods. However, we instead suggest that the researchers carefully consider each of the presented methods in the context of the given classification task, as each method poses its unique set of strengths and weaknesses.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Node Feature Definitions",
      "text": "The second part of the input to a GNN model is the node feature matrix (Fig.  1A ). We summarise the various definitions of node features in Table  II . We categorise these definitions based on which domain they are computed, i.e. time, frequency and graph domains.\n\nThe time-domain methods are the most commonly used in the current literature. In particular, these are the differential entropy (DE) and raw signal methods. The popularity of DE is given by the fact that many of the open EEG datasets include this feature, such as the SEED  [108]  emotion recognition dataset. DE describes the complexity of a continuous variable and is defined as:\n\nwhere X is a random continuous variable and f (x) is the probability density function. Many papers define the node feature as the raw EEG signal. However, the raw signal can be too long for a GNN to process effectively. Thus, it is often coupled with node feature pre-processing module and spatio-temporal GNNs (See V A and VI respectively) to either reduce the dimensionality or to extract the temporal patterns contained within the signal effectively. An alternative to the raw signal node feature is descriptive statistics, such as mean, median or standard deviation.\n\nFrequency-domain node features are usually defined as the Fourier frequency components obtained by the Fourier transform or the power spectral density. Both of these methods attempt to quantify the strength of various frequency components within the EEG signal. An advantage of these representations is their relatively low dimensionality compared to the raw signal described previously.\n\nFinally, graph-theoretical features can be utilised to describe the nodes, e.g. mean node weight  [65]  and be- tweenness centrality  [65, 73] . A severe limitation of this method is that the graph structure needs to be defined prior to node feature extraction. Thus, this node feature type is incompatible with learnable brain graph methods.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Node Feature Preprocessing",
      "text": "An optional next step after node features construction is some kind of node feature pre-processing module (NFP) (Fig.  1B ). We summarise the types of NFPs in Table  III .\n\nMost of the NFPs are integrated within the GNN architecture, thus allowing the model to be trained in an end-to-end manner. The exceptions are methods that utilise a pre-trained feature extraction neural network implemented as a bidirectional LSTM  [76]  or a CNN  [64] .\n\nThe surveyed NFPs are all based on a neural network. In most cases, these are variants of a CNN and multilayer perceptron (MLP). These modules aim to (1) reduce the dimensionality of the node features and (2) enhance the node features, including potentially suppressing noise or redundant information.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Type Of Graph Convolutional Layer",
      "text": "A core part of a GNN model are the graph convolutional layers (GCN) (Fig.  1C ). We summarise the utilised types of GCNs in Table  IV . We further categorise them based on the type of GNN as introduced in Section II, i.e. spatial, spectral. Additionally, we add the temporal category, which is not a type of standalone GCN layer but must be combined with spatial or spectral GCN.\n\nInterestingly, ChebConv is used in the majority of the surveyed papers (counting both ChebConv and spectral spatio-temporal GNN in Table  IV ). Since EEG typically uses 128 electrodes in high-density montages, the size of the brain graphs is relatively small. In such cases, even a full spectral GNN would not be too computationally expensive for EEG classification. Therefore, it remains unclear why many authors opt for the ChebConv approximation of spectral GNN. We speculate that the influence of classical signal processing tools in EEG analysis might also serve as a sufficient argument for using spectral GNNs for EEG classification.\n\nOn the other hand, the other half of the surveyed papers experiment with a wide range of spatial GNNs. The (simplified) GCN is a popular method amongst these, which is equivalent to a 1st-order ChebConv (K = 1). A special case of spatial GNN is the graph attention network (GAT). GAT allows for adjusting the graph by reweighting the edges using an attention mechanism. Generally, the attention mechanism for computing the new softmax-normalised edge weight e ij is defined as follows:\n\nwhere w and W are the learnable parameters of the model, σ is an activation function, h is the node feature vector/embedding, and N (i) is the set of nodes connected to node i. The resulting edge weights can then be passed to Equation  1 .\n\nNext, the spatio-temporal GNNs were tested for EEG classification in several instances. A spatio-temporal block consists of one GCN layer and one 1D-CNN applied temporally. This structure allows the model to extract both spatial (i.e. graph) and temporal patterns. There are both spatial and spectral variants of spatio-temporal GNN, and there is no indication as to which one should be preferred as no comparative study exists to date.\n\nFinally, several papers adopt multi-branch architectures. These methods utilise multiple GCN layers applied in parallel to allow the model to focus on various aspects (also views) of the input graph. An example of such a model utilises two-branch GNN to learn from both FC-and SC-based brain graph structure  [63] . Alternatively, the individual frequency bands of EEG signals can be used to construct various graph views  [85] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vii. Node Pooling Mechanisms",
      "text": "In some instances, reducing the number of nodes in the graph might be desirable. This can be achieved with a node pooling module (Fig.  1D ). We summarise the node pooling modules utilised in the surveyed papers in Table  V .\n\nThere are both learnable and non-learnable node pooling modules in the literature. Please see the corresponding papers for a detailed description of these methods (Table  V ). Node pooling modules remain a relatively unexplored topic in the EEG-GNN classification models. Node pooling can (1) remove redundant nodes, (2) reduce the size of the graph embedding in a setting where the concatenation of node embeddings forms it, and (3) aid in the explainability of the model by identifying node importance with respect to the classification task.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Viii. From Node Embeddings To Graph Embedding",
      "text": "The output of the graph convolutions is a set of learned node embeddings. Node embeddings in this form are suitable for tasks such as node classification and link prediction. However, for graph classification, the set of node features needs to be transformed into a unified graph representation (Fig.  1E ). We summarise the methods for this transformation in Table  VI .\n\nThe most straightforward method to form a graph embedding is to simply concatenate the node features. This approach poses a few limitations. First, the resulting graph embedding grows with the number of nodes, thus, the classification layer requires a large number of parameters. Second, all input graphs need to have the same number of nodes, limiting the model's generalisation to other datasets. Finally, such an approach is likely to include redundant or duplicated information in the graph embedding since GNN produces node embeddings by aggregating information from neighbouring nodes.\n\nA readout function is one of the methods to form a graph embedding that addresses these issues. A readout forms the embedding by passing the node features through a permutation-invariant function. A general definition of a readout to obtain graph embedding of a graph G i from a set of V node embeddings H = [h 1 , ..., h V ] is given by:\n\nwhere can be any permutation-invariant function. In the surveyed papers, these functions were sum, average and maximum. A few papers also experiment with attention-weighted sum to attenuate the role of unimportant nodes within the graph embedding  [88] . An interesting alternative is to apply CNN-style average or maximum pooling node-wise  [105] .\n\nAlternatively, researchers explored various neural network models to obtain graph embeddings, such as CNN  [52, 69, 78] , (bi-)LSTM  [51, 83, 84, 99, 100] , Transformer  [89]  and capsule networks  [73] . Additionally, graph pooling methods, such as DiffPool  [109] , SAGPool  [110] , iPool  [111] , TAP  [112]  and HierCorrPool  [113]  can be used for this purpose.  This survey categorises the proposed GNN models in terms of their inputs and modules. Specifically, these are brain graph structure, node features and their preprocessing, GCN layers, node pooling mechanisms, and formation of graph embeddings. This categorisation allows us to provide a quick and simple overview of the different methods presented in the EEG-GNN literature, appreciate the current state of the art in this field and identify promising future directions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "A. Limitations Of Surveyed Papers",
      "text": "Surprisingly, we have identified the least variety and innovation in the category of GCN layers (Table  IV ). A significant proportion of the surveyed papers utilise either ChebConv or \"vanilla\" spatial GCN. This might be due to the relative novelty of the EEG-GNN field, and thus, many papers explore other areas of model design, such as node features and brain graph definitions. A few papers seem to successfully experiment with more complex types of GCN layers  [47, 50, 91]  and multi-branch architectures  [58, 63, 80, 92, 97, 100] .\n\nA major limitation of most surveyed papers is the lack of generalisability to external datasets that might use a different number of EEG signals. This is caused by (1) the use of ChebConv and (2) forming graph embedding by node feature concatenation  [47, 55-60, 64, 66, 67, 70, 74, 77, 80, 81, 86, 87, 90-93, 98, 100-102, 104] . (1) can be addressed by utilising spatial GCN layers as suggested above, and (2) can be solved by using a readout function or a suitable node pooling mechanism, which coarsens the graph to a fixed number of nodes. Additionally, there is a general lack of transfer learning experiments for EEG-GNN models, which might be a promising direction for future research.\n\nFinally, we have identified an interesting gap in EEG-GNN research: the lack of utilising frequency band information in a more complex way. A few papers train separate models for each frequency band in isolation  [46, 47, 65] . Alternatively, they propose concatenating the graph embeddings generated from the frequencyband-GNN branches  [52, 87, 101] .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Future Directions",
      "text": "Several promising directions can be identified in the rapidly evolving landscape of EEG-GNN research. First, a comprehensive comparison of the various GCN layers (e.g. spatial GNN, ChebConv, GAT and graph transformer) with respect to their influence on classification performance should be carried out to address this crucial design question in a systematic manner.\n\nSecond, enhancing the generalisability of models by addressing issues related to the varying number of EEG signals/electrodes and exploring transfer learning approaches can open new avenues for research. For instance, pre-trained GNN models on cheap-to-obtain large datasets, such as open databases for emotion recognition or BCI applications, would allow the application of complex GNN architectures to problems with limited data availability due to the high costs or small populations (e.g. clinical data, rare diseases and disorders). Focusing on these issues would likely improve the generalisability of the models when evaluated on a diverse set of EEG datasets and different classification tasks.\n\nLastly, the rich frequency information of EEG signals should be explored more. For instance, we suggest a plausible utility of integrating cross-frequency coupling (CFC) approaches into EEG-GNN models. There is growing evidence in the literature concerning the advanced brain functions (e.g. learning, memory) enabled by CFC  [114] . Thus, integrating findings from neuroscience research into the EEG-GNN design promises both performance and explainability gains.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Limitations Of Our Survey",
      "text": "It is worth noting that this paper does not follow a systematic review methodology; therefore, we do not assert that our findings are exhaustive. Instead, our objective is to offer a succinct and cohesive overview of the current research on EEG-GNN models to facilitate the development of innovative approaches and assist researchers new to this field.\n\nOne of the major parts of EEG-GNN models we omit in this survey is the model explainability. We suggest that a survey paper is not well suited for comprehensively covering this aspect of research. Instead, we suggest a comparative experimental study to be better suited to explore the various explainability options of GNN explainability. However, to maintain the comprehensiveness of this survey, we list the papers that report the use of certain methods of model explainability:  [50, 55, 89, 105, 106] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "X. Conclusion",
      "text": "In conclusion, this survey examined the current research on EEG-GNN models for classifying EEG signals.\n\nVarious GNN-based methods have been proposed for tasks such as emotion recognition, brain-computer interfaces, and psychological and neurodegenerative disorders. The surveyed papers were categorised based on inputs and modules, including brain graph structure, node features, GCN layers, node pooling mechanisms, and graph embeddings.\n\nGNNs offer a unique method for analysing and classifying EEG in the graph domain, thus allowing the exploitation of complex spatial information in brain networks that other neural networks do not. Additionally, GNNs can be easily extended with CNN and recurrent network-based modules at various stages of the GNN architecture, such as for node feature pre-processing, node embedding post-processing and graph embedding formation.\n\nHowever, limitations and areas for improvement were identified. There is a lack of variety and innovation in GCN layers, with many papers utilising ChebConv or \"simple\" spatial GCN without clear justification. Generalisability to external datasets with varying numbers of EEG electrodes is limited. Transfer learning experiments and integration of cross-frequency coupling approaches are potential future research to enhance the performance and explainability of GNN.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: General architecture of a graph neural network model for classification of EEG. (A) The input to the model consists",
      "page": 2
    },
    {
      "caption": "Figure 1: Compared to other deep learning models, GNNs of-",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Other types of GNNs, such as attention GNNs",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of core mechanisms of spatial and spectral GNNs. A) An undirected featured graph is given as an",
      "page": 4
    },
    {
      "caption": "Figure 3: The most common classi-",
      "page": 4
    },
    {
      "caption": "Figure 3: Classification tasks presented in the current",
      "page": 5
    },
    {
      "caption": "Figure 1: A). We summarise the methods for defining the",
      "page": 5
    },
    {
      "caption": "Figure 1: A). We summarise the vari-",
      "page": 6
    },
    {
      "caption": "Figure 1: B). We summarise the types of NFPs in",
      "page": 7
    },
    {
      "caption": "Figure 1: C). We summarise the utilised",
      "page": 7
    },
    {
      "caption": "Figure 1: D). We summarise the node",
      "page": 7
    },
    {
      "caption": "Figure 1: E). We summarise the methods for",
      "page": 8
    },
    {
      "caption": "Figure 3: ). This recent rise in popularity of",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Distance between electrode\npositions\nFunctional connectivity\nmeasure\nManually defined\nShared learnable mask\nFeature similarity\nFeature distance\nTransformer-style attention\nConcatenation attention\nDense projection\nLSTM-based\nMultiple/Combined graph\ndefinitions",
          "Learnable Pre-defined": "✓\n✓\n✓\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n-",
          "Papers": "[46–64]\n[7, 47, 49, 50, 55, 59, 61, 65–86]\n[46, 53, 54, 57, 58, 62, 87]\n[46, 63, 69, 72, 82, 88–92]\n[56, 63, 72, 87, 92–94]\n[64, 95–97]\n[98, 99]\n[81, 100]\n[101–103]\n[104]\n[47, 49, 53, 54, 57–59, 61–64, 67, 69, 72, 79, 81, 82, 87, 92, 102]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Differential\nentropy\nRaw signal\nFourier Transform\nPower Spectral\nDensity/Band\nPower\nGraph theory\nmetrics\nDescriptive\nstatistics",
          "Time\ndomain": "✓\n✓\n✗\n✗\n✗\n✓",
          "Frequency\ndomain": "✗\n✗\n✓\n✓\n✗\n✗",
          "Graph\nDomain": "✗\n✗\n✗\n✗\n✓\n✗",
          "Papers": "[46, 51, 53, 55, 57, 71, 72, 75, 78, 81, 82, 87, 89,\n90, 92, 93, 95–99, 101, 102]\n[48, 52, 56, 60, 62–64, 66, 67, 69, 72, 76, 77, 79,\n80, 83, 84, 86, 91, 94, 100, 104–106]\n[50, 88, 107]\n[7, 49, 55, 57–59, 61, 78, 85, 90]\n[65, 73]\n[47, 58, 61]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "1D CNN\nFeature-wise\nattention\nweighting\nbidirectional\nLSTM\nTemporal CNN\nWaveletCNN\nSincCNN\nMLP\nCNN Feature\nExtractor",
          "Trained\nseparately": "✗\n✓\n✗\n✗\n✗\n✗\n✓",
          "Papers": "[48, 53, 60, 93, 94, 100,\n104, 106]\n[73]\n[76]\n[56, 91, 94]\n[91]\n[60]\n[85]\n[64]"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Graph Isomorphism Network\n(Simplified) Graph\nConvolution Network\nChebyshev Graph Convolution\nGraph Attention Network\nDiffusion recurrent gated\nSpatio-temporal GNN\n(Spectral)\nSpatio-temporal GNN (Spatial)\nPowers of Adjacency Matrix\nGNN\nGraphSAGE\nSpectral GNN\nB-Spline Kernel GCN\nResidual GCN\nMultibranch architectures",
          "Spatial": "✓\n✓\n✗\n✓\n✗\n✗\n✓\n✓\n✓\n✗\n✓\n✓\n-",
          "Spectral Temporal": "✗\n✗\n✗\n✗\n✗\n✓\n✓\n✗\n✗\n✗\n✗\n✗\n-",
          "Papers": "[48, 65, 79]\n[7, 46, 53, 54, 56, 58, 61, 70, 72, 75, 83, 89, 106]\n[49, 51, 55, 57, 59, 66, 67, 69, 71, 74, 76–\n78, 80, 82, 85, 90, 97, 99, 104]\n[60, 62, 73, 84, 88, 94, 98, 100]\n[50]\n[52, 81, 86, 95, 96, 107]\n[63, 105]\n[101, 102]\n[48, 107]\n[87, 93]\n[47]\n[91]\n[58, 63, 80, 92, 97, 100]"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "TopK\nHierarchical tree pooling\nSortPool\nEdgePool\nSAGPool\nSet2Set\nManual Clustering\nGraclus Clustering",
          "Learnable": "✓\n✓\n✓\n✓\n✓\n✓\n✗\n✗",
          "Papers": "[62, 67]\n[65]\n[48]\n[48]\n[48, 54]\n[48]\n[101, 102]\n[77, 80]"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Sum readout\nAverage readout\nMaximum readout\nConcatenate node\nembeddings\nCNN-like\nAverage/Maximum\nPooling\nSortPool\nAttention weighted\nCNN\nLSTM\nCapsule Network\nTransformer\nBidirectional LSTM",
          "Learnable": "✗\n✗\n✗\n✗\n✗\n✓\n✓\n✓\n✓\n✓\n✓\n✓",
          "Papers": "[46, 65, 82]\n[49, 54, 61, 62, 72, 85, 107]\n[7, 54, 62, 76, 106]\n[47, 55–60, 64, 66, 67, 70, 74, 77, 80, 81, 86, 87, 90–93, 98, 100–\n102, 104]\n[83, 105]\n[68]\n[63, 88, 97]\n[52, 69, 78]\n[51, 99]\n[73]\n[89]\n[83, 84, 100]"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Timefrequency analysis methods and their application in developmental EEG data",
      "authors": [
        "Santiago Morales",
        "Maureen Bowers"
      ],
      "year": "2022",
      "venue": "Developmental Cognitive Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "EEG in the diagnosis, classification, and management of patients with epilepsy",
      "authors": [
        "S Smith"
      ],
      "year": "2005",
      "venue": "Neurosurgery & Psychiatry"
    },
    {
      "citation_id": "3",
      "title": "Clinical Utility of EEG in Attention Deficit Hyperactivity Disorder",
      "authors": [
        "Sandra Loo",
        "Russell Barkley"
      ],
      "year": "2005",
      "venue": "Applied Neuropsychology",
      "doi": "10.1207/s15324826an12022"
    },
    {
      "citation_id": "4",
      "title": "EEG-based brain-computer interfaces",
      "authors": [
        "D Mcfarland",
        "J Wolpaw"
      ],
      "year": "2017",
      "venue": "Current Opinion in Biomedical Engineering"
    },
    {
      "citation_id": "5",
      "title": "A review of classification algorithms for EEG-based brain-computer interfaces: a 10 year update",
      "authors": [
        "F Lotte",
        "L Bougrain",
        "A Cichocki",
        "M Clerc",
        "M Congedo",
        "A Rakotomamonjy",
        "F Yger"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "6",
      "title": "Stationary common spatial patterns for brain-computer interfacing",
      "authors": [
        "Wojciech Samek",
        "Carmen Vidaurre",
        "Klaus-Robert Müller",
        "Motoaki Kawanabe"
      ],
      "year": "2012",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "7",
      "title": "EEG-based Graph Neural Network Classification of Alzheimer's Disease: An Empirical Evaluation of Functional Connectivity Methods",
      "authors": [
        "Dominik Klepl",
        "Fei He",
        "Min Wu",
        "Daniel Blackburn",
        "Ptolemaios Sarrigiannis"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "8",
      "title": "Classification of human emotion from EEG using discrete wavelet transform",
      "authors": [
        "Murugappan Murugappan",
        "Nagarajan Ramachandran",
        "Yaacob Sazali"
      ],
      "year": "2010",
      "venue": "Journal of Biomedical Science and Engineering"
    },
    {
      "citation_id": "9",
      "title": "Seizure classification in EEG signals utilizing Hilbert-Huang transform",
      "authors": [
        "J Rami",
        "Enas Oweis",
        "Abdulhay"
      ],
      "year": "2011",
      "venue": "Biomedical Engineering Online"
    },
    {
      "citation_id": "10",
      "title": "A Review on Machine Learning for EEG Signal Processing in Bioengineering",
      "authors": [
        "Mohammad-Parsa Hosseini",
        "Amin Hosseini",
        "Kiarash Ahi"
      ],
      "year": "2021",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "11",
      "title": "Machine Learning for Predicting Epileptic Seizures Using EEG Signals: A Review",
      "authors": [
        "Khansa Rasheed",
        "Adnan Qayyum",
        "Junaid Qadir",
        "Shobi Sivathamboo",
        "Patrick Kwan",
        "Levin Kuhlmann",
        "O' Terence",
        "Adeel Brien",
        "Razi"
      ],
      "year": "2021",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "12",
      "title": "Cross-frequency multilayer network analysis with bispectrum-based functional connectivity: a study of Alzheimer's disease. Neuroscience",
      "authors": [
        "Dominik Klepl",
        "Fei He",
        "Min Wu",
        "Daniel Blackburn",
        "Ptolemaios Sarrigiannis"
      ],
      "year": "2023",
      "venue": "Cross-frequency multilayer network analysis with bispectrum-based functional connectivity: a study of Alzheimer's disease. Neuroscience"
    },
    {
      "citation_id": "13",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "14",
      "title": "Deep long short-term memory structures model temporal dependencies improving cognitive workload estimation",
      "authors": [
        "Brett Ryan G Hefron",
        "James Borghetti",
        "Christine Christensen",
        "Schubert Kabban"
      ],
      "year": "2017",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "15",
      "title": "Network neuroscience",
      "authors": [
        "Danielle Bassett",
        "Olaf Sporns"
      ],
      "year": "2017",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "EEG functional connectivity underlying emotional valance and arousal using minimum spanning trees",
      "authors": [
        "Rui Cao",
        "Yan Hao",
        "Xin Wang",
        "Yuan Gao",
        "Huiyu Shi",
        "Shoujun Huo",
        "Bin Wang",
        "Hao Guo",
        "Jie Xiang"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Brain network analysis for the discrimination of dementia disorders using electrophysiology signals: A systematic review",
      "authors": [
        "T Abdulyekeen",
        "Kalyana Adebisi",
        "Veluvolu"
      ],
      "year": "2023",
      "venue": "Frontiers in Aging Neuroscience"
    },
    {
      "citation_id": "18",
      "title": "Classification of contrasting discrete emotional states indicated by EEG based graph theoretical network measures",
      "authors": [
        "Berke Kılıç",
        "Serap Aydın"
      ],
      "year": "2022",
      "venue": "Neuroinformatics"
    },
    {
      "citation_id": "19",
      "title": "Graph theoretical analysis of Alzheimer's disease: Discrimination of AD patients from healthy subjects",
      "authors": [
        "Mahdi Jalili"
      ],
      "year": "2017",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "20",
      "title": "Graph theory analysis of directed functional brain networks in major depressive disorder based on EEG signal",
      "authors": [
        "Fatemeh Hasanzadeh",
        "Maryam Mohebbi",
        "Reza Rostami"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "21",
      "title": "Classification methods based on complexity and synchronization of electroencephalography signals in Alzheimer's disease",
      "authors": [
        "Sou Nobukawa",
        "Teruya Yamanishi",
        "Shinya Kasakawa",
        "Haruhiko Nishimura",
        "Mitsuru Kikuchi",
        "Tetsuya Takahashi"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychiatry"
    },
    {
      "citation_id": "22",
      "title": "Epilepsy Detection From EEG Using Complex Network Techniques: A Review",
      "authors": [
        "Supriya Supriya",
        "Siuly Siuly",
        "Hua Wang",
        "Yanchun Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "23",
      "title": "Graph Signal Processing, Graph Neural Network and Graph Learning on Biological Data: A Systematic Review",
      "authors": [
        "Rui Li",
        "Xin Yuan",
        "Mohsen Radfar",
        "Peter Marendy",
        "Wei Ni",
        "Terrence O'brien",
        "Pablo Casillas-Espinosa"
      ],
      "year": "2023",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "24",
      "title": "Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data",
      "authors": [
        "Dominik Klepl",
        "Fei He",
        "Min Wu",
        "Daniel Blackburn",
        "Ptolemaios Sarrigiannis"
      ],
      "year": "2023",
      "venue": "Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data",
      "arxiv": "arXiv:2304.05874"
    },
    {
      "citation_id": "25",
      "title": "Graph Neural Networks and Their Current Applications in Bioinformatics",
      "authors": [
        "Xiao-Meng Zhang",
        "Li Liang",
        "Lin Liu",
        "Ming-Jing Tang"
      ],
      "venue": "Frontiers in Genetics"
    },
    {
      "citation_id": "26",
      "title": "Graph Neural Networks in Network Neuroscience",
      "authors": [
        "Alaa Bessadok",
        "Mohamed Ali Mahjoub",
        "Islem Rekik"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "A compact review of molecular property prediction with graph neural networks",
      "authors": [
        "Oliver Wieder",
        "Stefan Kohlbacher",
        "Mélaine Kuenemann",
        "Arthur Garon",
        "Pierre Ducrot",
        "Thomas Seidel",
        "Thierry Langer"
      ],
      "year": "2020",
      "venue": "Drug Discovery Today: Technologies"
    },
    {
      "citation_id": "28",
      "title": "Graph neural networks for materials science and chemistry",
      "authors": [
        "Patrick Reiser",
        "Marlen Neubert",
        "André Eberhard",
        "Luca Torresi",
        "Chen Zhou",
        "Chen Shao",
        "Houssam Metni",
        "Clint Van Hoesel",
        "Henrik Schopmans",
        "Timo Sommer",
        "Pascal Friederich"
      ],
      "year": "2022",
      "venue": "Communications Materials"
    },
    {
      "citation_id": "29",
      "title": "Graph neural networks for automated de novo drug design",
      "authors": [
        "Jiacheng Xiong",
        "Zhaoping Xiong",
        "Kaixian Chen",
        "Hualiang Jiang",
        "Mingyue Zheng"
      ],
      "year": "2021",
      "venue": "Drug Discovery Today"
    },
    {
      "citation_id": "30",
      "title": "Graph convolutional networks for computational drug development and discovery",
      "authors": [
        "Mengying Sun",
        "Sendong Zhao",
        "Coryandar Gilvary",
        "Olivier Elemento",
        "Jiayu Zhou",
        "Fei Wang"
      ],
      "year": "2020",
      "venue": "Briefings in Bioinformatics"
    },
    {
      "citation_id": "31",
      "title": "Review of Graph Neural Network in Text Classification",
      "authors": [
        "Masoud Malekzadeh",
        "Parisa Hajibabaee",
        "Maryam Heidari",
        "Samira Zad",
        "Ozlem Uzuner",
        "James Jones"
      ],
      "year": "2021",
      "venue": "2021 IEEE 12th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)"
    },
    {
      "citation_id": "32",
      "title": "Graph Neural Networks for Natural Language Processing: A Survey. Foundations and Trends® in Machine Learning",
      "authors": [
        "Lingfei Wu",
        "Yu Chen",
        "Kai Shen",
        "Xiaojie Guo",
        "Hanning Gao",
        "Shucheng Li",
        "Jian Pei",
        "Bo Long"
      ],
      "year": "2023",
      "venue": "Graph Neural Networks for Natural Language Processing: A Survey. Foundations and Trends® in Machine Learning"
    },
    {
      "citation_id": "33",
      "title": "A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions",
      "authors": [
        "Chen Gao",
        "Yu Zheng",
        "Nian Li",
        "Yinfeng Li",
        "Yingrong Qin",
        "Jinghua Piao",
        "Yuhan Quan",
        "Jianxin Chang",
        "Depeng Jin",
        "Xiangnan He",
        "Yong Li"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Recommender Systems"
    },
    {
      "citation_id": "34",
      "title": "Graph Neural Networks in Recommender Systems: A Survey",
      "authors": [
        "Shiwen Wu",
        "Fei Sun",
        "Wentao Zhang",
        "Xu Xie",
        "Bin Cui"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "35",
      "title": "Graph neural network for traffic forecasting: A survey",
      "authors": [
        "Weiwei Jiang",
        "Jiayun Luo"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "36",
      "title": "Temporal Multi-Graph Convolutional Network for Traffic Flow Prediction",
      "authors": [
        "Mingqi Lv",
        "Zhaoxiong Hong",
        "Ling Chen",
        "Tieming Chen",
        "Tiantian Zhu",
        "Shouling Ji"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "37",
      "title": "A Review on Graph Neural Network Methods in Financial Applications",
      "authors": [
        "Jianian Wang",
        "Sheng Zhang",
        "Yanghua Xiao",
        "Rui Song"
      ],
      "year": "2022",
      "venue": "A Review on Graph Neural Network Methods in Financial Applications",
      "arxiv": "arXiv:2111.15367"
    },
    {
      "citation_id": "38",
      "title": "A Comprehensive Survey on Graph Neural Networks",
      "authors": [
        "Zonghan Wu",
        "Shirui Pan",
        "Fengwen Chen",
        "Guodong Long",
        "Chengqi Zhang",
        "Philip Yu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "39",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "Jie Zhou",
        "Ganqu Cui",
        "Shengding Hu",
        "Zhengyan Zhang",
        "Cheng Yang",
        "Zhiyuan Liu",
        "Lifeng Wang",
        "Changcheng Li",
        "Maosong Sun"
      ],
      "year": "2020",
      "venue": "AI Open"
    },
    {
      "citation_id": "40",
      "title": "Graph Attention Networks",
      "authors": [
        "Petar Veličković",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "Graph Attention Networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "41",
      "title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks",
      "authors": [
        "Youngjoo Seo",
        "Michaël Defferrard",
        "Pierre Vandergheynst",
        "Xavier Bresson"
      ],
      "year": "2018",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "42",
      "title": "Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification",
      "authors": [
        "Yunsheng Shi",
        "Zhengjie Huang",
        "Shikun Feng",
        "Hui Zhong",
        "Wenjin Wang",
        "Yu Sun"
      ],
      "year": "2021",
      "venue": "Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification",
      "arxiv": "arXiv:2009.03509"
    },
    {
      "citation_id": "43",
      "title": "Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph Neural Networks",
      "authors": [
        "Zhiqian Chen",
        "Fanglan Chen",
        "Lei Zhang",
        "Taoran Ji",
        "Kaiqun Fu",
        "Liang Zhao",
        "Feng Chen",
        "Lingfei Wu",
        "Charu Aggarwal",
        "Chang-Tien Lu"
      ],
      "year": "2021",
      "venue": "Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph Neural Networks",
      "arxiv": "arXiv:2002.11867"
    },
    {
      "citation_id": "44",
      "title": "A Survey on Spectral Graph Neural Networks",
      "authors": [
        "Deyu Bo",
        "Xiao Wang",
        "Yang Liu",
        "Yuan Fang",
        "Yawen Li",
        "Chuan Shi"
      ],
      "year": "2023",
      "venue": "A Survey on Spectral Graph Neural Networks"
    },
    {
      "citation_id": "45",
      "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering",
      "authors": [
        "Michaël Defferrard",
        "Xavier Bresson",
        "Pierre Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "EEGbased emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Fatigue driving recognition based on deep learning and graph neural network",
      "authors": [
        "Zhiqiang Lin",
        "Taorong Qiu",
        "Ping Liu",
        "Lingyun Zhang",
        "Siwei Zhang",
        "Zhendong Mu"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "48",
      "title": "EEG-GNN: Graph Neural Networks for Classification of Electroencephalogram (EEG) Signals",
      "authors": [
        "Andac Demir",
        "Toshiaki Koike-Akino",
        "Ye Wang",
        "Masaki Haruna",
        "Deniz Erdogmus"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "49",
      "title": "A GRAPH CONVOLUTIONAL NEURAL NET-WORK FOR THE AUTOMATED DETECTION OF SEIZURES IN THE NEONATAL EEG",
      "authors": [
        "Khadijeh Raeisi",
        "Mohammad Khazaei",
        "Pierpaolo Croce",
        "Gabriella Tamburro",
        "Silvia Comani",
        "Filippo Zappasodi"
      ],
      "year": "2022",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "50",
      "title": "Self-Supervised Graph Neural Networks for Improved Electroencephalographic Seizure Analysis",
      "authors": [
        "Siyi Tang",
        "Jared Dunnmon",
        "Khaled Kamal Saab",
        "Xuan Zhang",
        "Qianying Huang",
        "Florian Dubost",
        "Daniel Rubin",
        "Christopher Lee-Messer"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "51",
      "title": "EEG emotion recognition using fusion model of graph convolutional neural networks and LSTM",
      "authors": [
        "Yongqiang Yin",
        "Xiangwei Zheng",
        "Bin Hu",
        "Yuang Zhang",
        "Xinchun Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "52",
      "title": "Adaptive spatiotemporal graph convolutional networks for motor imagery classification",
      "authors": [
        "Biao Sun",
        "Han Zhang",
        "Zexu Wu",
        "Yunyan Zhang",
        "Ting Li"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "53",
      "title": "A Multi-Dimensional Graph Convolution Network for EEG Emotion Recognition",
      "authors": [
        "Guanglong Du",
        "Jinshao Su",
        "Linlin Zhang",
        "Kang Su",
        "Xueqian Wang",
        "Shaohua Teng",
        "Peter Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "54",
      "title": "DAGAM: A Domain Adversarial Graph Attention Model for Subject Independent EEG-Based Emotion Recognition",
      "authors": [
        "Tao Xu",
        "Wang Dang",
        "Jiabao Wang",
        "Yun Zhou"
      ],
      "year": "2022",
      "venue": "DAGAM: A Domain Adversarial Graph Attention Model for Subject Independent EEG-Based Emotion Recognition",
      "arxiv": "arXiv:2202.12948"
    },
    {
      "citation_id": "55",
      "title": "EEG-based video identification using graph signal modeling and graph convolutional neural network",
      "authors": [
        "Soobeom Jang",
        "Seong-Eun Moon",
        "Jong-Seok Lee"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "56",
      "title": "LGGNet: learning from Local-global-graph representations for brain-computer interface",
      "authors": [
        "Yi Ding",
        "Neethu Robinson",
        "Qiuhao Zeng",
        "Cuntai Guan"
      ],
      "year": "2021",
      "venue": "LGGNet: learning from Local-global-graph representations for brain-computer interface",
      "arxiv": "arXiv:2105.02786"
    },
    {
      "citation_id": "57",
      "title": "EEG Emotion Recognition based on Hierarchy Graph Convolution Network",
      "authors": [
        "Fa Zheng",
        "Bin Hu",
        "Shilin Zhang",
        "Yalin Li",
        "Xiangwei Zheng"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "58",
      "title": "Hierarchy graph convolution network and tree classification for epileptic detection on electroencephalography signals",
      "authors": [
        "Difei Zeng",
        "Kejie Huang",
        "Cenglin Xu",
        "Haibin Shen",
        "Zhong Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "59",
      "title": "CR-GCN: Channel-Relationships-Based Graph Convolutional Network for EEG Emotion Recognition",
      "authors": [
        "Jingjing Jia",
        "Bofeng Zhang",
        "Hehe Lv",
        "Zhikang Xu",
        "Shengxiang Hu",
        "Haiyan Li"
      ],
      "year": "2022",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "60",
      "title": "Affect recognition from scalp-EEG using channel-wise encoder networks coupled with geometric deep learning and multi-channel feature fusion",
      "authors": [
        "Darshana Priyasad",
        "Tharindu Fernando"
      ],
      "year": "2022",
      "venue": "Simon Denman, Sridha Sridharan, and Clinton Fookes"
    },
    {
      "citation_id": "61",
      "title": "Efficient graph convolutional networks for seizure prediction using scalp EEG",
      "authors": [
        "Manhua Jia",
        "Wenjian Liu",
        "Junwei Duan",
        "Long Chen",
        "Qun Cl Philip Chen",
        "Zhiguo Wang",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "62",
      "title": "Exploring Self-Attention Graph Pooling With EEG-Based Topological Structure and Soft Label for Depression Detection",
      "authors": [
        "Tao Chen",
        "Yanrong Guo",
        "Shijie Hao",
        "Richang Hong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "An Attention-Guided Spatiotemporal Graph Convolutional Network for Sleep Stage Classification",
      "authors": [
        "Menglei Li",
        "Hongbo Chen",
        "Zixue Cheng"
      ],
      "year": "2022",
      "venue": "Life"
    },
    {
      "citation_id": "64",
      "title": "Multi-view spatial-temporal graph convolutional networks with domain generalization for sleep stage classification",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Xiaojun Ning",
        "Yuanlai He",
        "Ronghao Zhou",
        "Yuhan Zhou",
        "H Lehman"
      ],
      "year": "1977",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "65",
      "title": "Minimum spanning tree based graph neural network for emotion classification using EEG",
      "authors": [
        "Hanjie Liu",
        "Jinren Zhang",
        "Qingshan Liu",
        "Jinde Cao"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "66",
      "title": "Graph Neural Network with Multilevel Feature Fusion for EEG based Brain-Computer Interface",
      "authors": [
        "Youngchul Kwak",
        "Woo-Jin",
        "Seong-Eun Song",
        "Kim"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)"
    },
    {
      "citation_id": "67",
      "title": "Classification of first-episode schizophrenia, chronic schizophrenia and healthy control based on brain network of mismatch negativity by graph neural network",
      "authors": [
        "Qi Chang",
        "Cancheng Li",
        "Qing Tian",
        "Qijing Bo",
        "Jicong Zhang",
        "Yanbing Xiong",
        "Chuanyue Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "68",
      "title": "Multi-domain fusion deep graph convolution neural network for EEG emotion recognition",
      "authors": [
        "Jinying Bi",
        "Fei Wang",
        "Xin Yan",
        "Jingyu Ping",
        "Yongzhao Wen"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "69",
      "title": "Accurate emotion recognition using Bayesian model based EEG sources as dynamic graph convolutional neural network nodes",
      "authors": [
        "Shiva Asadzadeh",
        "Tohid Yousefi Rezaii",
        "Soosan Beheshti",
        "Saeed Meshgini"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "70",
      "title": "EEG-Based Seizure detection using linear graph convolution network with focal loss",
      "authors": [
        "Yanna Zhao",
        "Changxu Dong",
        "Gaobo Zhang",
        "Yaru Wang",
        "Xin Chen",
        "Weikuan Jia",
        "Qi Yuan",
        "Fangzhou Xu",
        "Yuanjie Zheng"
      ],
      "year": "2021",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "71",
      "title": "Bayesian Graph Neural Networks for EEG-Based Emotion Recognition",
      "authors": [
        "Jianhui Chen",
        "Hui Qian",
        "Xiaoliang Gong"
      ],
      "venue": "Clinical Image-Based Procedures, Distributed and Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning"
    },
    {
      "citation_id": "72",
      "title": "A multidomain adaptive graph convolutional network for EEGbased emotion recognition",
      "authors": [
        "Rui Li",
        "Yiting Wang",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "73",
      "title": "Decoding Subjective Creativity Skill from Visuo-Spatial Reasoning Ability Using Capsule Graph Neural Network",
      "authors": [
        "Sayantani Ghosh",
        "Amit Konar",
        "Atulya Nagar"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "74",
      "title": "Developing an efficient functional connectivity-based geometric deep network for automatic EEG-based visual decoding",
      "authors": [
        "Nastaran Khaleghi",
        "Tohid Yousefi Rezaii",
        "Soosan Beheshti",
        "Saeed Meshgini"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "75",
      "title": "Identification of Depression with a Semi-supervised GCN based on EEG Data",
      "authors": [
        "Dixin Wang",
        "Chang Lei",
        "Xuan Zhang",
        "Hongtong Wu",
        "Shuzhen Zheng",
        "Jinlong Chao",
        "Hong Peng"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "76",
      "title": "Deep feature mining via attention-based BiLSTM-GCN for human motor imagery recognition",
      "authors": [
        "Yimin Hou",
        "Shuyue Jia",
        "Xiangmin Lun",
        "Yan Shi",
        "Yang Li"
      ],
      "year": "2020",
      "venue": "Deep feature mining via attention-based BiLSTM-GCN for human motor imagery recognition",
      "arxiv": "arXiv:2005.00777"
    },
    {
      "citation_id": "77",
      "title": "GCNs-net: a graph convolutional neural network approach for decoding time-resolved eeg motor imagery signals",
      "authors": [
        "Yimin Hou",
        "Shuyue Jia",
        "Xiangmin Lun",
        "Ziqian Hao",
        "Yan Shi",
        "Yang Li",
        "Rui Zeng",
        "Jinglei Lv"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "78",
      "title": "Causal Graph Convolutional Neural Network For Emotion Recognition",
      "authors": [
        "Wanzeng Kong",
        "Min Qiu",
        "Menghang Li",
        "Xuanyu Jin",
        "Li Zhu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "79",
      "title": "Seizure detection by brain-connectivity analysis using dynamic graph isomorphism network",
      "authors": [
        "Tian-Li Tao",
        "Liang-Hu Guo",
        "Qiang He",
        "Han Zhang",
        "Lin Xu"
      ],
      "year": "2022",
      "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "80",
      "title": "Graph Learning with Co-Teaching for EEG-Based Motor Imagery Recognition",
      "authors": [
        "Bo Wang",
        "Hui Shen",
        "Gai Lu",
        "Yingxin Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "81",
      "title": "Mutualgraphnet: a novel model for motor imagery classification",
      "authors": [
        "Yan Li",
        "Ning Zhong",
        "David Taniar",
        "Haolan Zhang"
      ],
      "year": "2021",
      "venue": "Mutualgraphnet: a novel model for motor imagery classification",
      "arxiv": "arXiv:2109.04361"
    },
    {
      "citation_id": "82",
      "title": "Linking Multi-Layer Dynamical GCN With Style-Based Recalibration CNN for EEG-Based Emotion Recognition",
      "authors": [
        "Guangcheng Bao",
        "Kai Yang",
        "Li Tong",
        "Jun Shu",
        "Rongkai Zhang",
        "Linyuan Wang",
        "Bin Yan",
        "Ying Zeng"
      ],
      "year": "2022",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "83",
      "title": "EEG-Based Emotion Recognition Using Spatial-temporal Graph Convolutional LSTM with Attention Mechanism",
      "authors": [
        "Lin Feng",
        "Cheng Cheng",
        "Mingyan Zhao",
        "Huiyuan Deng",
        "Yong Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "84",
      "title": "Spatial-temporal seizure detection with graph attention network and bidirectional LSTM architecture",
      "authors": [
        "Jiatong He",
        "Jia Cui",
        "Gaobo Zhang",
        "Mingrui Xue",
        "Dengyu Chu",
        "Yanna Zhao"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "85",
      "title": "A Novel Complex Network-Based Graph Convolutional Network in Major Depressive Disorder Detection",
      "authors": [
        "Xinlin Sun",
        "Chao Ma",
        "Peiyin Chen",
        "Mengyu Li",
        "He Wang",
        "Weidong Dang",
        "Chaoxu Mu",
        "Zhongke Gao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "86",
      "title": "Spatial-temporal graph convolutional network for Alzheimer classification based on brain functional connectivity imaging of electroencephalogram",
      "authors": [
        "Xiaocai Shan",
        "Jun Cao",
        "Shoudong Huo",
        "Liangyu Chen",
        "Ptolemaios Sarrigiannis",
        "Yifan Zhao"
      ],
      "year": "2022",
      "venue": "Human Brain Mapping"
    },
    {
      "citation_id": "87",
      "title": "EEG Emotion Recognition Based on Dynamically Organized Graph Neural Network",
      "authors": [
        "Hanyu Li",
        "Xu Zhang",
        "Ying Xia"
      ],
      "year": "2022",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "88",
      "title": "EEG Emotion Recognition Based on Self-attention Dynamic Graph Neural Networks",
      "authors": [
        "Chao Li",
        "Yong Sheng",
        "Haishuai Wang",
        "Mingyue Niu",
        "Peiguang Jing",
        "Ziping Zhao",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "89",
      "title": "A Dual-Branch Dynamic Graph Convolution Based Adaptive Transformer Feature Fusion Network for EEG Emotion Recognition",
      "authors": [
        "Mingyi Sun",
        "Weigang Cui",
        "Shuyue Yu",
        "Hongbin Han",
        "Bin Hu",
        "Yang Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "90",
      "title": "GCB-Net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "Tong Zhang",
        "Xuehan Wang",
        "Xiangmin Xu",
        "Chen Philip"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "91",
      "title": "Spatio-temporal-spectral hierarchical graph convolutional network with semisupervised active learning for patient-specific seizure prediction",
      "authors": [
        "Yang Li",
        "Yu Liu",
        "Yu-Zhu Guo",
        "Xiao-Feng Liao",
        "Bin Hu",
        "Tao Yu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "92",
      "title": "Adaptive Hierarchical Graph Convolutional Network for EEG Emotion Recognition",
      "authors": [
        "Yunlong Xue",
        "Wenming Zheng",
        "Yuan Zong",
        "Hongli Chang",
        "Xingxun Jiang"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "93",
      "title": "Crosssubject EEG emotion recognition with self-organized graph neural network",
      "authors": [
        "Jingcong Li",
        "Shuqi Li",
        "Jiahui Pan",
        "Fei Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "94",
      "title": "PearNet: A Pearson Correlationbased Graph Attention Network for",
      "authors": [
        "Jianchao Lu",
        "Yuzhe Tian",
        "Shuang Wang",
        "Michael Sheng",
        "Xi Zheng"
      ],
      "year": "2022",
      "venue": "Sleep Stage Recognition",
      "arxiv": "arXiv:2209.13645"
    },
    {
      "citation_id": "95",
      "title": "GraphSleep-Net: Adaptive Spatial-Temporal Graph Convolutional Networks for Sleep Stage Classification",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Jing Wang",
        "Ronghao Zhou",
        "Xiaojun Ning",
        "Yuanlai He",
        "Yaoshuai Zhao"
      ],
      "year": "2020",
      "venue": "IJCAI"
    },
    {
      "citation_id": "96",
      "title": "Emotion Recognition from Physiological Channels Using Graph Neural Network",
      "authors": [
        "Tomasz Wierciński",
        "Mateusz Rock",
        "Robert Zwierzycki",
        "Teresa Zawadzka",
        "Micha Zawadzki"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "97",
      "title": "Siam-GCAN: a Siamese Graph Convolutional Attention Network for EEG Emotion Recognition",
      "authors": [
        "Hong Zeng",
        "Qi Wu",
        "Yanping Jin",
        "Haohao Zheng",
        "Mingming Li",
        "Yue Zhao",
        "Hua Hu",
        "Wanzeng Kong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "98",
      "title": "Locally temporal-spatial pattern learning with graph attention mechanism for EEG-based emotion recognition",
      "authors": [
        "Yiwen Zhu",
        "Kaiyu Gan",
        "Zhong Yin"
      ],
      "year": "2022",
      "venue": "Locally temporal-spatial pattern learning with graph attention mechanism for EEG-based emotion recognition",
      "arxiv": "arXiv:2208.11087"
    },
    {
      "citation_id": "99",
      "title": "Attention-based Spatio-Temporal Graphic LSTM for EEG Emotion Recognition",
      "authors": [
        "Xiaoxu Li",
        "Wenming Zheng",
        "Yuan Zong",
        "Hongli Chang",
        "Cheng Lu"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "100",
      "title": "Emotion Recognition from Multi-channel EEG Data through A Dual-pipeline Graph Attention Network",
      "authors": [
        "Xiang Li",
        "Jing Li",
        "Yazhou Zhang",
        "Prayag Tiwari"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "101",
      "title": "Instance-adaptive graph for EEG emotion recognition",
      "authors": [
        "Tengfei Song",
        "Suyuan Liu",
        "Wenming Zheng",
        "Yuan Zong",
        "Zhen Cui"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "102",
      "title": "Variational instance-adaptive graph for EEG emotion recognition",
      "authors": [
        "Tengfei Song",
        "Suyuan Liu",
        "Wenming Zheng",
        "Yuan Zong",
        "Zhen Cui",
        "Yang Li",
        "Xiaoyan Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "103",
      "title": "Learning graph in graph convolutional neural networks for robust seizure prediction",
      "authors": [
        "Qi Lian",
        "Yu Qi",
        "Gang Pan",
        "Yueming Wang"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "104",
      "title": "Geometric Deep Learning for Subject Independent Epileptic Seizure Prediction Using Scalp EEG Signals",
      "authors": [
        "Theekshana Dissanayake",
        "Tharindu Fernando",
        "Simon Denman",
        "Sridha Sridharan",
        "Clinton Fookes"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "105",
      "title": "Classify EEG and reveal latent graph structure with spatio-temporal graph convolutional neural network",
      "authors": [
        "Xiaoyu Li",
        "Buyue Qian",
        "Jishang Wei",
        "An Li",
        "Xuan Liu",
        "Qinghua Zheng"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "106",
      "title": "Investigating Brain Connectivity with Graph Neural Networks and GNNExplainer",
      "authors": [
        "Maksim Zhdanov",
        "Saskia Steinmann",
        "Nico Hoffmann"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "107",
      "title": "Temporal graph convolutional networks for automatic seizure detection",
      "authors": [
        "Ian Covert",
        "Balu Krishnan",
        "Imad Najm",
        "Jiening Zhan",
        "Matthew Shore",
        "John Hixson",
        "Ming Po"
      ],
      "year": "2019",
      "venue": "Machine Learning for Healthcare Conference"
    },
    {
      "citation_id": "108",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "109",
      "title": "Hierarchical Graph Representation Learning with Differentiable Pooling",
      "authors": [
        "Zhitao Ying",
        "Jiaxuan You",
        "Christopher Morris",
        "Xiang Ren",
        "Will Hamilton",
        "Jure Leskovec"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "110",
      "title": "Self-Attention Graph Pooling",
      "authors": [
        "Junhyun Lee",
        "Inyeop Lee",
        "Jaewoo Kang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning"
    },
    {
      "citation_id": "111",
      "title": "iPool-Information-Based Pooling in Hierarchical Graph Neural Networks",
      "authors": [
        "Xing Gao",
        "Wenrui Dai",
        "Chenglin Li",
        "Hongkai Xiong",
        "Pascal Frossard"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "112",
      "title": "Topology-Aware Graph Pooling Networks",
      "authors": [
        "Hongyang Gao",
        "Yi Liu",
        "Shuiwang Ji"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "113",
      "title": "Multivariate Time Series Representation Learning via Hierarchical Correlation Pooling Boosted Graph Neural Network",
      "authors": [
        "Yucheng Wang",
        "Min Wu",
        "Xiaoli Li",
        "Lihua Xie",
        "Zhenghua Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "114",
      "title": "Cross-frequency coupling in real and virtual brain networks",
      "authors": [
        "Viktor Jirsa",
        "Viktor Müller"
      ],
      "year": "2013",
      "venue": "Frontiers in Computational Neuroscience"
    }
  ]
}