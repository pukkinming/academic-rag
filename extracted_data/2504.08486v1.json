{
  "paper_id": "2504.08486v1",
  "title": "Plugselect: Pruning Channels With Plug-And-Play Flexibility For Electroencephalography-Based Brain Computer Interface",
  "published": "2025-04-11T12:24:06Z",
  "authors": [
    "Xue Yuan",
    "Keren Shi",
    "Ning Jiang",
    "Jiayuan He"
  ],
  "keywords": [
    "electroencephalography (EEG)",
    "brain-computer interface (BCI)",
    "channel optimization",
    "plug-and-play",
    "wearable device"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic minimization and optimization of the number of the electrodes is essential for the practical application of electroencephalography (EEG)-based brain computer interface (BCI). Previous methods typically require additional training costs or rely on prior knowledge assumptions. This study proposed a novel channel pruning model, plug-and-select (PlugSelect), applicable across a broad range of BCI paradigms with no additional training cost and plug-and-play functionality. It integrates gradients along the input path to globally infer the causal relationships between input channels and outputs, and ranks the contribution sequences to identify the most highly attributed channels. The results showed that for three BCI paradigms, i.e., auditory attention decoding (AAD), motor imagery (MI), affective computation (AC), PlugSelect could reduce the number of channels by at least half while effectively maintaining decoding performance and improving efficiency. The outcome benefits the design of wearable EEG-based devices, facilitating the practical application of BCI technology.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "lectroencephalography (EEG) is the electrical manifestation of brain activities and contain neural information of the central nervous system (CNS)  [1] . It is commonly employed as source signal for the application of brain computer interface (BCI), which provides the possibility of directly controlling machines based on the human intentions decoded from neural signals  [2] . There are multiple types of EEG-based BCI paradigms, such as auditory attention decoding (AAD), motor imagery (MI), affective computation (AC), etc. AAD is a task which is designed to decode which voice the participant is listening from EEG signals in a multi-speaker condition  [3] . MI is to decode the type of the imagined movements from EEG signals  [4] . AC is to obtain the emotion of the participant from EEG signals  [5] . It is suggested that multiple types of information could be decoded from EEG signals. The EEG-based BCI is a practical noninvasive technology of restoring and augmenting the functions of the humans.\n\nThough promising, there were several challenges for practical use of EEG-based BCI system. One key challenge involves the automatic optimization of the number of the electrodes, or channels, placed on the scalp. For most current EEG tasks, tens of the electrodes are employed for achieving high decoding performance. However, excessive electrodes increased the complexity and decreased the portability of the system. As such, channel pruning, i.e., automatic minimization and optimization of the number of the electrodes, is crucial for the practical application of the system  [6] ,  [7] ,  [8] .\n\nThe optimal electrode placement is subject to the BCI paradigm, and the number of the electrodes required. Current EEG channel pruning approaches mainly include end-to-end deep learning (DL) algorithms, machine learning methods, and statistical techniques  [8] . Iteratively applying DL classifiers to eliminate channels with minimal contribution to the decoding performance for a specific task is the most direct approach for channel selection. Kashefi et al. applied shallow convolutional neural networks (CNNs) iteratively to perform channel selection for MI  [8] , while Mirkovic et al. set up an offline iterative process for channel selection in auditory attention tasks  [9] . In addition,  Xu et al.  proposed an end-to-end DL model based on a weighted residual structure to identify a subset of invariant channels relevant to auditory tasks at the group level  [10] . Sun et al. incorporated a channel selection module into an end-to-end MI decoding model  [11] . However, channel selection schemes that rely on iterative classifiers and training processes not only increase training and learning costs but also complicate physiological interpretation. Similar to iterative processes, Narayanan et al. employed a greedy algorithm to search for channels relevant to auditory attention  [12] . Moreover, some researchers have utilized the weights of intermediate layers in DL algorithms for channel selection, thereby reducing computational costs. Wang et al. used the 2norm of the spatial convolution layer weights to determine the MI classification contribution  [7] , Lin et al. considered attention scores as indicators of relevance for AC tasks  [13] , and Cai et al. attempted to quantify the importance of EEG channels for AAD tasks using graph attention weights  [14] . However, the weights of a given layer may not directly reflect the causal relationship between input and output and often exhibit poor stability. Therefore, some researchers have adopted machine learning or statistical methods that more directly capture causal relationships for channel selection. Wang et al.  [15]  selected four optimal channels using weight vectors derived from common spatial patterns (CSP), while Yong et al. further enhanced the sparsity of CSP weight vectors by introducing ‚Ñì1-norm regularization  [16] ,  [17] . Li et al. selected channels by calculating the statistical significance of power spectral parameters in relation to the task  [18] , and Jing et al. proposed a correlation-based channel selection method  [19] ,  [20] . These methods typically rely on linear assumptions, which may fail to fully capture the complex nonlinear features present in EEG signals. Furthermore, all of the aforementioned methods are tailored to specific BCI paradigms, and their portability and effectiveness across different platforms remain uncertain.\n\nTo address these challenges, we propose plug-and-select (PlugSelect), a framework for channel selection based on outcome attribution designed for the EEG-based BCI paradigms. PlugSelect could work with different neural networks and requires no additional training costs, prior knowledge, or assumptions for channel selection. Specifically, it consists of two modules: integrated gradients (IG) and ranking strategy (RS). (  1 ) IG integrates the gradients along the input path to assess the global contribution of input channels to the prediction outcome, thereby providing a direct interpretation of the decision-making process in complex neural networks. This approach addresses the issue of traditional weight-based schemes' inability to stably capture the causal relationship between input and output, all while requiring no additional training costs. (2) RS, building upon the personalized channel selection scheme provided by IG, introduces a tasklevel channel ranking strategy aimed at addressing subject heterogeneity, thereby achieving a more stable and reliable subset of channels.\n\nIn addition, for verifying the plug-and-play functionality and broad effectiveness of the framework, three different types of EEG-based BCI paradigms were applied: 1) the new cuemasked AAD task, of which one is the two-class orientation attention (OA) decoding, and another is the two-class timbre attention (TA) decoding; 2) the traditional four-class MI task; 3) the three-class AC task. Using the PlugSelect framework, we then calculated the average classification contribution of each channel from the input multichannel EEG signal assessing its impact on prediction outcomes. And evaluated the model's performance under varying channel densities using metrics such as accuracy (ACC). Ultimately, we aimed to balance the number of channels, model decoding performance, and computational efficiency (number of samples processed per second) by selecting a task-relevant subset of channels. This provides guidance for designing more efficient, low-channel EEG caps suitable for different EEG-based BCI paradigms.\n\nIn conclusion, our main contributions are:  (1)  We propose PlugSelect as a plug-and-play systematic channel selection framework, which has been efficiently ported and validated across multiple data platforms (MI task and AC task). (  2 ) PlugSelect preserves decoding efficiency while significantly reducing redundant channels. (3) 15 AAD-related channels are identified, and the channel pruning results demonstrate a strong correlation with downstream task performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Channel Pruning Framework",
      "text": "The proposed PlugSelect consists of two key modules: IG and RS, as shown in Fig.  1 . The IG is designed to estimate the contribution of each channel for the downstream BCI tasks. The RS is used to process the difference among the subjects to mitigate the effects of subject variability and make the results generalized. The details of the two modules are described as follows: a). IG For a specific BCI task, IG employs linear interpolation and gradient summation along the path between a reference baseline and the EEG data to measure the contribution of each channel. Its input is raw EEG data, denoted as ùúë ‚àà ‚Ñù !√ó# with the number of the channels as ùê∂ and the sample points in the ùë† $% decision window as ùëá, and a pre-trained neural network model for the downstream BCI tasks with full channels, denoted as ‚àÜ & . For the ùëñ $% channel, the attribution ùúô ' ( at the ùë† $% decision window is calculated as follows:\n\nwhere ùúë ' ) is the reference baseline for the data of the ùëñ $% channel ùúë ' ‚àà ‚Ñù 1√óùëá with a zero-baseline in this study.\n\nùëÄ represents the number of steps in the Riemannian path integral. ùí¢ denotes the gradient integral over the path, which is defined as follows:\n\nwhere ‚Ñé(‚Ä¢) denotes the mapping function of the pre-trained model ‚àÜ ùë• , which is responsible for transforming the input ùúå % into a specific representation in the output space. ùúå % is a scaling variable at step ùëö. IG employs an integral method to express the impact of each incremental change from the zerobaseline to the complete input on the output in a fine-grained manner, rather than merely focusing on the singular effect of the complete input on the model output. This makes it apply to the nonlinear model. Additionally, IG integrates the cumulative contribution along the entire path from the reference baseline to the actual input, providing guidance in the channel pruning process by retaining key channels and pruning redundant ones. Suppose there are ùëõ decision windows for each subject, the contribution of the ùëñ $% channel is the summation of the contributions across all the windows, which is calculated as follows:\n\nThe contribution value ranged from -1 to 1. If it is greater than zero, it indicates that the channel positively contributes to the identification of the downstream task. If it is less than zero, it indicates a negative contribution. The larger the absolute value of IG, the stronger the channel's influence on the model's classification.\n\nAlgorithm 1 shows the pseudo-code for the reasoning component of the proposed PlugSelect method.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B). Rs",
      "text": "As the results of channel selection might be varied among the subjects, two strategies, i.e., averaging and voting, were employed on the results of the subjects available for generalization. For averaging, it considers shared statistics. For each channel, attribution of each subject from IG were averaged, and then the average values were ranked from highest to lowest for channel selection. For voting, it considers sample specificity. For each subject, channel attribution from IG is ranked, and then the occurrence of each channel in the top rank were calculated with the data of all the subjects. The electrodes with high occurrence were selected.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Bci Paradigms",
      "text": "For investigating the feasibility of the proposed framework, three BCI paradigms were employed in this study, including AAD, MI, and AC. The dataset of AAD was self-collected, and the datasets of the other two paradigms were from the open data repository, which was BCI Competition IV-2a for MI and SEED for AC. The three EEG datasets varied in acquisition devices, experimental paradigms, subject numbers, and sample sizes, providing a comprehensive basis for fairly validating the multi-platform portability and effectiveness of the proposed method. Due to the difference in the number of channels among the datasets, the electrode positions from the 64-lead system were used as the foundational canvas for the analysis, as shown in Fig.  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A) Aad",
      "text": "The goal of AAD was to decode auditory attention from EEG. There were 30 healthy right-handed participants (aged 17-32 years, 15 females and 15 males with subject IDs S1 to S30) recruited in this study. Before the experiment, the procedures were provided and written informed consent was obtained from all the participants. The experimental protocol was in accordance with the Declaration of Helsinki, and approved by the Research Ethics Committee of West China Hospital, Sichuan University (# 2024582).\n\nThe experimental protocol was illustrated in Fig.  3 . The experiment was conducted in a soundproof room where the participants' field of vision was limited to white walls. During the experiment, the participants were exposed to mixed male and female audio stimuli, and the EEG signals were recorded with a commercial device (Enobio EEG systems, NE Neuroelectrics, Spain). Based on the international 10-20 system, 32 electrode positions were selected across the entire scalp, i.e., P8, T8, CP6, FC6, F8, F4, C4, P4, AF4, Fp2, Fp1, AF3, Fz, FC2, Cz, CP2, PO3, O1, Oz, O2, PO4, Pz, CP1, FC1, P3, C3, F3, F7, FC5, CP5, T7, P7. The sampling frequency was 500 Hz.\n\nThere were two AAD tasks, OA and TA. In the mixed speech ), ùúô 8 ‚àà ‚Ñù ùê∂√óùëá Compute all channel's attribution ùúô ùúô = ùëöùëíùëéùëõ(ùúô 8 , ùëéùë•ùëñùë† = 1) return ùúô end stimulus trials, the target timbre and spatial location were randomized, but the number of trials was nearly equal. Each participant completed 12 mixed speech stimulus trials, with each trial having a stimulus duration of 70 seconds.\n\nFor signal preprocessing, the collected data underwent successively average referencing and bandpass filtering (0.4-32 Hz). Then independent component analysis (ICA) was employed to eliminate artifacts such as eye movements and muscle activities. The data was segmented into 0.5-second windows with no overlap, resulting in 1,656 windows per participant.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B) Mi",
      "text": "The goal of MI was to decode the imagined movements from EEG. An open dataset, BCI Competition IV-2a dataset provided by Graz University of Technology, was adopted in this study. It contains EEG data from 9 subjects. The paradigm involved MI tasks for four movement categories: left hand (L), right hand (R), feet (F), and tongue (T), as illustrated in Fig.  3  (C ). Each subject completed two sessions on separate days, with each session comprising 72 EEG trials for each of the four MI tasks, recorded at a sampling rate of 250 Hz. A total of 22 electrodes, placed according to the 10-20 system, were used: Fz, FC3, FC1, FCz, FC2, FC4, C5, C3, C1, Cz, C2, C4, C6, CP3, CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused on the 0-4 second window after cue onset, corresponding to  [2, 6]  seconds per trial.\n\nFor signal preprocessing, a band-pass filter was applied to the EEG data in the  [4, 40]  Hz range, as described in  [21] . In this study, a 6th-order Chebyshev filter was employed to preserve task-relevant rhythms. However, the time-consuming nature of MI-EEG acquisition and the limited size of the dataset increase the risk of underfitting. As such, following previous studies  [22] ,  [23] , the strategy of segmentation and reconstruction (S&R) in the time domain was adopted to generate additional data. Additionally, Z-score normalization was performed on the EEG data from Datasets MI, AC to mitigate fluctuations and non-stationarity. The normalization is as follows:\n\nwhere ùúá and ùúé denote the mean and standard deviation of the training set, respectively. c) AC The goal of AC was to decode emotion from EEG. An open dataset, SEED provided by Shanghai Jiao Tong University, was adopted in this study. It contains emotion-based EEG signals from 15 subjects. Each session involved 15 movie clips designed to evoke positive, neutral, and negative moods, with the paradigm illustrated in Fig.  3 (D) . Data were collected across three sessions, spaced approximately one week apart. EEG signals were recorded from 62 electrodes at a sampling rate of 1000 Hz, and subsequently down sampled to 200 Hz. The 62 channels, placed according to the 10-20 system, included: Fp1, Fpz, Fp2, AF3, AF4, F7, F5, F3, F1, Fz, F2, F4, F6, F8, FT7, FC5, FC3, FC1, FCz, FC2, FC4, FC6, FT8, T7, C5, C3, C1, Cz, C2, C4, C6, T8, TP7, CP5, CP3, CP1, CPz, CP2, CP4, CP6, TP8, P7, P5, P3, P1, Pz, P2, P4, P6, P8, PO7, PO5, PO3, Poz, PO4, PO6, PO8, CB1, O1, Oz, O2, and CB2. Experiments were conducted based on the shortest trial length, consisting of 37,000 sample points (185 seconds).\n\nFor preprocessing, we applied a 6th-order Chebyshev bandpass filter to the data in the  [4, 47]  Hz range. Each sample was then segmented into non-overlapping one-second time windows, resulting in 2775 trials from a single session. Z-score normalization was adopted to mitigate fluctuations and nonstationarity, which was the same as MI.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Neural Network",
      "text": "The implementation of the proposed framework needed a pre-trained neural network with effective decoding capability.\n\nIn this study, the decoding model for the AAD task is AADNet  [24] , which demonstrated high classification accuracy for AAD tasks, as shown in Fig.  4 . It comprises three main blocks: temporal convolution, spatial convolution, and hybrid decoding modules. The number of 2D convolution kernels for the three blocks is set to  [32, 64, 64] , with kernel sizes of [(1, 64),  (32, 1) ,  (1, 16) ], and the number of kernels is kept consistent within each block.\n\nFor MI and AC, EEGConformer was adopted as the decoding model for the good performance in  [23] . EEGConformer consists of two main modules: the spatio-temporal convolution module and the multi-head self-attention module. The spatiotemporal convolution module contains 40 2D convolution kernels, with kernel sizes of [  (1, 25) , ( ùëê‚Ñé , 1)], where ùëê‚Ñé represents the number of channels for the corresponding task. Both the MI task and the AC task execute the multi-head attention module six times, but the number of attention heads differs, which is 10 and 5, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Performance Evaluation",
      "text": "The decoding performance of three BCI paradigms with different channels was calculated with different strategies, including two RSs (averaging and voting) and one randomly selecting five sets of channels. For the two-class AAD task, ACC, area under the receiver operating characteristic curve (AUC), specificity (SPE), sensitivity (SEN), and F1 score, was employed for model performance evaluation. For the multiclass MI and AC tasks, only ACC was used as the evaluation metric. Besides, effective decoding ACC and computational efficiency (CE) was combined to select the optimal number of channels relevant to each task. CE is defined as the throughput of the neural network per second, measured in FPS (frames per second). The effective decoding ACC is defined as the decoding accuracy that must exceed the chance level for a specific task category.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Decoding Performance With Channel Pruning A) Aad Task",
      "text": "The performance of AAD task with different number of the channels is displayed in Tables  I  and II . The decoding performance declines as the number of the channels decreasing under all the strategies, with a significant drop observed when channels are reduced from 10 to 5. Furthermore, compared to the random method, the two proposed RSs (averaging and voting) consistently demonstrated a significant advantage in mean decoding ACC across different channel counts, with this advantage becoming more pronounced as the number of channels decreased. Even with only 5 channels retained, the proposed RSs maintained decoding accuracy above 80% for the OA task and above 75% for the TA task, while the random method exhibited a further decrease of approximately 10%. Additionally, compared to the voting RS, the averaging RS demonstrated superior decoding accuracy, particularly when the channel count was 15.\n\nFig.  5  (A) and (B) clearly highlight the advantage of the averaging RS in decoding ACC and compare its stability with that of the random selection, to emphasize the effectiveness of the proposed approach. The results indicate that, in contrast to random channel selection, the averaging RS exhibits remarkable stability and reproducibility. Ultimately, due to its superior ability to maintain decoding performance, the averaging RS was chosen for channel pruning analysis in the AAD task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B) Mi Task",
      "text": "The performance of the 4-class MI task with different channel counts is shown in Table  III . Under all strategies, the decoding performance decreases as the number of channels reduces. Notably, when the number of selected channels was 5, the random method exhibited a significantly lower ability to select effective channels compared to the two proposed RSs. With the proposed RSs, the decoding ACC decreased by approximately 12%, whereas the random method saw a more substantial decline of about 19%. Additionally, the averaging RS demonstrated a more pronounced advantage in decoding performance compared to the voting RS.\n\nFig.  5  (C) more clearly illustrates the advantage of the averaging RS in maintaining decoding ACC and compares its stability with the random selection method. The results indicate that, compared to random channel selection, the averaging RS shows superior subject stability. Ultimately, due to its superior ability to maintain decoding performance, the averaging RS was applied in the MI task for channel pruning analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C) Ac Task",
      "text": "In the AC task with more acquisition channels, the performance with different number of channels is shown in Table  IV . Under all strategies, decoding performance decreased as the number of channels decreased. However, when the channel density ùúÇ was only 8%, all strategies still maintained a decoding accuracy of approximately 70%. Compared to the averaging RS, the voting RS demonstrated better retention of decoding accuracy, especially when the number of channels was 20, which differs from the results observed in the AAD and MI tasks. Nevertheless, when the channel density ùúÇ was less than 10%, the voting RS showed weaker ability to control the   divergence in decoding accuracy compared to the averaging RS. Furthermore, although the random method showed slightly higher average decoding accuracy than the averaging RS, it was still inferior to the voting RS. Analysis of individual subject stability revealed that the performance repeatability of the random method was notably poor, with a maximum performance difference of over 20% across five random selections, as shown in Fig.  5  (D). Therefore, considering all factors, the voting RS was applied for channel pruning analysis in the AC task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Channel Selection",
      "text": "Subject heterogeneity results in performance variability in BCI decoding task; however, the sensitivity to the optimal channel count remains relatively consistent across subjects. The decoding ACC variation curves for subjects in Fig.  5  indicate that, in the AAD task, the turning point for decoding accuracy change occurs at 15 channels; in the MI task, it occurs at 10 channels; and in the AC task, it occurs at 20 channels.\n\nIn addition, the balance curves between computational efficiency and decoding accuracy indicate that the optimal balance point for the AAD task is around 15 channels, for the MI task it is between 5 and 10 channels, and for the AC task it is around 10 channels, as shown in Fig.  6 .\n\nTaking into account factors such as decoding ACC, computational efficiency, electrode density, and positions (AAD task, Fig.  7 ; MI task, Fig.  8 ; AC task, Fig.  9 ), the final channel selection results are as follows: In the AAD task, considering the final OA and TA results, the selected channels ùê∂ ::; are the top 15 electrodes ranked by classification contribution after RS ranking: Fp1, Fp2, F7, F8, AF4, AF3, F3,",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv Discussion",
      "text": "This study proposes PlugSelect, a channel selection framework for BCI tasks, aimed at practical applications such as portable neuro-steered hearing aids. The framework automatically selects channels through data-driven attentional weight assignment while ensuring high decoding efficiency. We validated and demonstrated the effectiveness of the proposed PlugSelect in channel selection, as well as its multiplatform portability and broad applicability, using the multiattribute AAD dataset collected from 30 subjects, along with widely used MI (BCI Competition IV 2a) and AC datasets (SEED).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Plug-And-Play Plugselect, Multi-Platform Compatibility",
      "text": "The main inference component of PlugSelect employs the IG algorithm to interpret the pre-trained model and automatically assign weights to each channel. Unlike end-toend channel selection models  [10] , which add deployment and training costs, or channel selection modules  [11]  that may skew the learning direction of the classification model and reduce decoding performance, PlugSelect simplifies the process. And it avoids the issues associated with multiple iterations to find optimal channel combinations  [8] , which can increase noise interference and search difficulty. while channel sparsification methods like CSP reduce costs to some extent, they largely rely on prior selection or knowledge such as filters  [15] . In contrast, PlugSelect streamlines system deployment by requiring only access to the pre-trained model and raw task data, and eliminates the need for classifier iteration or training, making it truly plug-and-play, as shown in Fig.  1 . For this reason, as described in this paper, we can easily apply PlugSelect to BCI Competition IV-2a dataset, SEED, and other data platforms with different paradigms, categorization goals, and target quantities.\n\nAdditionally, due to individual differences among subjects,  [25] ,  [26]  the optimal EEG channels vary from person to person (Fig.  5 ). PlugSelect can automatically select the most suitable sub-channel for each subject by using a subject-specific pretraining model, thus facilitating the personalized design of BCI devices for real-world applications.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Maintain Decoding Efficiency, Automate Channel Decoupling",
      "text": "The optimal number of channels depends on the specific BCI paradigm and requirements. We also compared the impact of channel density on decoding efficiency. PlugSelect maintains a decoding efficiency similar to that of the full channel configuration when the channel density is greater than 65%, especially in the AAD task. When the number of channels is reduced from 32 to 15, TA decoding performance decreases by 2%, while OA performance decreases by less than 1%, and OA task performance remains more stable with a reduction in the number of channels. In the 4-class MI task, when the channel density is less than 50%, decoding accuracy decreases by less than 5%. In the AC task, when the channel density ùúº is 0.32, emotional decoding accuracy remains above 80%. This indicates that PlugSelect can effectively reduce the number of channels unrelated to the task. Moreover, with this number of  electrodes, mobile EEG recordings are feasible outside the laboratory and in everyday life settings  [9] .\n\nDue to subject heterogeneity, we investigated the impact of different channel sequencing strategies on overall decoding performance to obtain a more stable RS and identify common activation patterns for specific BCI tasks. Firstly, compared to the random selection strategy, the two proposed RSs exhibit more stable performance and enhanced reproducibility. And Tables I, II, III, and IV indicate that the performance difference between the averaging strategy, which incorporates statistical sharing, and the voting strategy, which emphasizes individual differences, is around 1%. This suggests that PlugSelect's attribution calculation is insensitive to different ordering strategies and exhibits stronger stability. Additionally, we found that when the number of channels is larger, the voting RS outperforms the averaging RS. Notably, the decoding performance of subjects shows similar changes as the number of channels decreases, indicating that our model can effectively achieve automatic channel decoupling, identifying a set of invariant channels for specific BCI tasks and alleviating subject heterogeneity to some extent.\n\nThis evidence supports the notion that PlugSelect achieves channel decoupling and helps maintain decoding efficiency.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. High Relevance To Downstream Tasks",
      "text": "PlugSelect explains the channel selection results by calculating the contribution levels. For the AAD task, the top 5 channels extracted by PlugSelect were Fp1, Fp2, F7, F8, and AF4, which are distributed within the prefrontal lobe. This is likely because the AAD task requires subjects to focus intently on attentional targets in a noisy environment, and the prefrontal lobe is linked to transient decision-making and attention allocation  [27] ,  [28] .\n\nThe top three channels extracted by PlugSelect in the MI task were C3, CP4, and C4, which align with the findings of  [7] . These channels are located in the primary sensorimotor cortex. Numerous EEG studies have confirmed that motor imagery activates primary sensorimotor areas  [29] ,  [30]  and have observed significant ERD/ERS phenomena in the C3 and C4 regions  [31] .\n\nIn the AC task, the top five channels identified by PlugSelect were T7, T8, TP7, FT7 and FT8, consistent with the results from  [32] ,  [33] ,  [34] . These channels are primarily situated in the temporal and frontal lobes. The temporal lobe is associated not only with auditory stimuli in this paradigm but also with audiovisual emotion comprehension  [35] . And emotion cognition results from integrated processing across various brain regions  [34] ,  [36] .\n\nMoreover, since the multi-attribute AAD task paradigm used in this study is less commonly discussed, we can only analyze the plausibility of the channels extracted by PlugSelect from the perspective of functional brain regions. In contrast, the MI and ER paradigms and datasets are well-established, and the channels identified by PlugSelect are consistent with findings reported in existing literature, which serves as an additional validation of the reasonableness of the AAD channel results.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Research Gap And Future Work",
      "text": "While cross-subject automatic channel selection helps balance individual heterogeneity and group characteristics, this study primarily relied on statistical results from the best subchannels within single-subject domain to identify taskrelevant invariant channels. In addition, the proposed PlugSelect relies on an effective pre-trained decoding model. Channels identified solely based on model inference may be limited in scope, and validating the effectiveness of the subset of channels extracted by PlugSelect through the functional properties of brain regions may lack sufficient accuracy. In future studies, we will further explore unsupervised algorithms to overcome labeling constraints and comprehensively evaluate the subset of channels associated with BCI task from multiple physiological perspectives.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V Conclusion",
      "text": "In this study, we proposed a novel plug-and-play framework, PlugSelect, for efficient channel selection in BCI tasks. PlugSelect requires no additional training and can directly infer and interpret channels that are highly correlated with cortical electrical activity patterns through model result attribution. It is also efficiently portable across multiple platforms, such as AAD, MI and AC, addressing the limitations of existing algorithms. Furthermore, PlugSelect enables automatic channel decoupling while preserving decoding performance, providing a subset of channels closely related to specific BCI tasks, supporting the development and application of portable wearable devices.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The IG is designed to estimate the",
      "page": 2
    },
    {
      "caption": "Figure 1: The framework of PlugSelect, contains two modules: IG and RS. IG, integrated gradients. RS, ranking strategy. IG evaluates the global contribution",
      "page": 2
    },
    {
      "caption": "Figure 3: (C). Each subject completed two sessions on separate days, with",
      "page": 4
    },
    {
      "caption": "Figure 3: (D). Data were collected",
      "page": 4
    },
    {
      "caption": "Figure 3: Experimental paradigms for each task. (A) Experimental",
      "page": 4
    },
    {
      "caption": "Figure 2: Electrode position. (A) 64-lead position based on 10-20 system",
      "page": 4
    },
    {
      "caption": "Figure 4: It comprises three main",
      "page": 5
    },
    {
      "caption": "Figure 5: (A) and (B) clearly highlight the advantage of the",
      "page": 5
    },
    {
      "caption": "Figure 5: (C) more clearly illustrates the advantage of the",
      "page": 5
    },
    {
      "caption": "Figure 4: Decision capability projection of AADNet in OA and TA tasks. (A)",
      "page": 5
    },
    {
      "caption": "Figure 5: (D). Therefore, considering all",
      "page": 7
    },
    {
      "caption": "Figure 5: indicate",
      "page": 7
    },
    {
      "caption": "Figure 6: Taking into account factors such as decoding ACC,",
      "page": 7
    },
    {
      "caption": "Figure 7: ; MI task, Fig. 8; AC task, Fig. 9), the final",
      "page": 7
    },
    {
      "caption": "Figure 5: The task decoding accuracy changes curves for all subjects at different channel counts (light lines), along with a comparison of the average",
      "page": 7
    },
    {
      "caption": "Figure 6: Balance curves of computational efficiency and decoding accuracy",
      "page": 7
    },
    {
      "caption": "Figure 7: (C); in the MI task, the selected",
      "page": 8
    },
    {
      "caption": "Figure 8: (B); and in the AC task, the",
      "page": 8
    },
    {
      "caption": "Figure 1: For this reason, as",
      "page": 8
    },
    {
      "caption": "Figure 5: ). PlugSelect can automatically select the most suitable",
      "page": 8
    },
    {
      "caption": "Figure 7: AAD task channel selection results. Top (A) 5, (B) 10, (C) 15, (D)",
      "page": 8
    },
    {
      "caption": "Figure 8: MI task channel selection results. Top (A) 5, (B) 10, (C) 15",
      "page": 8
    },
    {
      "caption": "Figure 9: AC task channel selection results. Top (A) 5, (B) 10, (C) 20, (D)",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Abstract‚ÄîAutomatic  minimization  and  optimization  of \nthe"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "number of the electrodes is essential for the practical application"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "of electroencephalography (EEG)-based brain computer interface"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "(BCI).  Previous  methods  typically  require  additional  training"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "costs or rely on prior knowledge assumptions. This study proposed"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "a  novel  channel  pruning  model,  plug-and-select \n(PlugSelect),"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "applicable  across  a  broad  range  of  BCI  paradigms  with  no"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "additional \ntraining \ncost  and  plug-and-play \nfunctionality. \nIt"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "integrates  gradients  along  the  input  path  to  globally  infer  the"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "causal  relationships  between  input  channels  and  outputs,  and"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "ranks  the  contribution  sequences  to \nidentify  the  most  highly"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "attributed  channels.  The  results  showed \nthat \nfor \nthree  BCI"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "paradigms, \ni.e., \nauditory \nattention  decoding \n(AAD),  motor"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "imagery \n(MI),  affective  computation \n(AC),  PlugSelect  could"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "reduce  the  number  of  channels  by  at  least  half  while  effectively"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "maintaining decoding performance and improving efficiency. The"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "outcome  benefits  the  design  of  wearable  EEG-based  devices,"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "facilitating the practical application of BCI technology."
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "I\nndex  Terms‚Äîelectroencephalography \n(EEG),  brain-computer"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "interface  (BCI),  channel  optimization,  plug-and-play,  wearable"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "device"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "I. INTRODUCTION"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "lectroencephalography \n(EEG) \nis \nthe \nelectrical"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "E"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "manifestation  of  brain \nactivities \nand \ncontain  neural"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "information  of  the  central  nervous  system  (CNS)  [1].  It  is"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "commonly  employed  as  source  signal  for  the  application  of"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "brain computer interface (BCI), which provides the possibility"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "of directly controlling machines based on the human intentions"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "decoded  from  neural  signals  [2].  There  are  multiple  types  of"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "EEG-based BCI paradigms, such as auditory attention decoding"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "(AAD), motor imagery (MI), affective computation (AC), etc."
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "AAD  is  a  task  which  is  designed  to  decode  which  voice  the"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "participant  is  listening  from  EEG  signals  in  a  multi-speaker"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "condition  [3].  MI \nis \nto  decode \nthe \ntype  of \nthe \nimagined"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "movements from EEG signals [4]. AC is to obtain the emotion"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "of  the  participant  from  EEG  signals  [5].  It  is  suggested  that"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "This  work  was  supported  by  the  Fundamental  Research  Funds  for  the"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Central  Universities  under  Grant  YJ202373,  Science  and  Technology  Major"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Project \nof \nTibetan \nAutonomous \nRegion \nof \nChina \nunder \nGrant"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "XZ202201ZD0001G, 1.3.5 project for Disciplines of 1435 Excellence Grant"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "from  West  China  Hospital  under  Grant  ZYYC22001,  and  key  project  from"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Med-X \nCenter \nfor \nManufacturing \nunder \nGrant \n0040206107007."
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "(Corresponding author: Jiayuan He; Email: jiayuan.he@wchscu.cn)."
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Xue  Yuan,  Keren  Shi,  Ning  Jiang,  and  Jiayuan  He  are  with  National"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Clinical  Research  Center \nfor  Geriatrics,  West  China  Hospital,  Sichuan"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": ""
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "University, Chengdu, Sichuan 610017, China, and also with Med-X Center for"
        },
        {
          "Xue Yuan, Keren Shi, Ning Jiang, Senior Member, IEEE, Jiayuan He, Member, IEEE": "Manufacturing, Sichuan University, Chengdu, Sichuan 610017, China."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "EEG-based  BCI  paradigms  were  applied:  1)  the  new  cue-"
        },
        {
          "2": "masked  AAD  task,  of  which  one  is  the  two-class  orientation"
        },
        {
          "2": "attention  (OA)  decoding,  and  another  is  the  two-class  timbre"
        },
        {
          "2": "attention (TA) decoding; 2) the traditional four-class MI task;"
        },
        {
          "2": "3) the three-class AC task. Using the PlugSelect framework, we"
        },
        {
          "2": "then calculated the average classification contribution of each"
        },
        {
          "2": "channel from the input multichannel EEG signal assessing its"
        },
        {
          "2": "impact  on  prediction  outcomes.  And  evaluated  the  model's"
        },
        {
          "2": "performance  under  varying  channel  densities  using  metrics"
        },
        {
          "2": "such as accuracy (ACC). Ultimately, we aimed to balance the"
        },
        {
          "2": "number \nof \nchannels,  model \ndecoding \nperformance, \nand"
        },
        {
          "2": "computational  efficiency  (number  of  samples  processed  per"
        },
        {
          "2": "second)  by  selecting  a  task-relevant  subset  of  channels.  This"
        },
        {
          "2": "provides  guidance  for  designing  more  efficient,  low-channel"
        },
        {
          "2": "EEG caps suitable for different EEG-based BCI paradigms."
        },
        {
          "2": "In  conclusion,  our  main  contributions  are:  (1)  We  propose"
        },
        {
          "2": "PlugSelect  as  a  plug-and-play  systematic  channel  selection"
        },
        {
          "2": "framework,  which  has  been  efficiently  ported  and  validated"
        },
        {
          "2": "across  multiple  data  platforms  (MI  task  and  AC  task).  (2)"
        },
        {
          "2": "PlugSelect  preserves  decoding  efficiency  while  significantly"
        },
        {
          "2": "reducing redundant channels. (3) 15 AAD-related channels are"
        },
        {
          "2": "identified, and the channel pruning results demonstrate a strong"
        },
        {
          "2": "correlation with downstream task performance."
        },
        {
          "2": ""
        },
        {
          "2": "II METHOD"
        },
        {
          "2": ""
        },
        {
          "2": "A. Channel Pruning Framework"
        },
        {
          "2": ""
        },
        {
          "2": "The  proposed  PlugSelect  consists  of  two  key  modules:  IG"
        },
        {
          "2": "and RS, as shown in Fig. 1. The IG is designed to estimate the"
        },
        {
          "2": "contribution of each channel for the downstream BCI tasks. The"
        },
        {
          "2": "RS  is  used  to  process  the  difference  among  the  subjects  to"
        },
        {
          "2": "mitigate the effects of subject variability and make the results"
        },
        {
          "2": "generalized.  The  details  of  the  two  modules  are  described  as"
        },
        {
          "2": "follows:"
        },
        {
          "2": ""
        },
        {
          "2": "a). IG"
        },
        {
          "2": ""
        },
        {
          "2": "For a specific BCI task, IG employs linear interpolation and"
        },
        {
          "2": ""
        },
        {
          "2": "gradient summation along the path between a reference baseline"
        },
        {
          "2": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "Its  input  is  raw  EEG  data,  denoted  as  ùúë ‚àà ‚Ñù!√ó#\t with  the",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "number  of",
          "nd the EEG data to measure the contribution of each channel.": "the  channels  as  ùê∂   and"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "the\tùë†$%\tdecision window as  ùëá, and a pre-trained neural network",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "model \nfor \nthe  downstream  BCI",
          "nd the EEG data to measure the contribution of each channel.": "tasks  with"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "denoted  as",
          "nd the EEG data to measure the contribution of each channel.": "‚àÜ& .  For  the \tùëñ$%   channel,  the  attribution \tùúô‚Äô"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "the\tùë†$%\tdecision window\tis calculated as follows:",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "ùúô‚Äô\n((‚Ñé, ùúë‚Äô, ùúë‚Äô",
          "nd the EEG data to measure the contribution of each channel.": ")) √ó ùí¢ √ó\n)) = (ùúë‚Äô ‚àí ùúë‚Äô"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "where  ùúë‚Äô",
          "nd the EEG data to measure the contribution of each channel.": ")\t is  the  reference  baseline  for  the  data  of  the"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "channel\n  with \n\tùúë‚Äô ‚àà ‚Ñù1√óùëá",
          "nd the EEG data to measure the contribution of each channel.": "a \nzero-baseline"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "ùëÄ\t represents  the  number  of  steps  in  the  Riemannian  path",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "integral.  ùí¢\tdenotes the gradient integral over the path, which is",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "defined as follows:",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "ùëÄ",
          "nd the EEG data to measure the contribution of each channel.": "ùúï %‚Ñé&ùúåùëö, \t‚àÜùë•‚Äô("
        },
        {
          "a": "ùí¢ = $",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": "ùëö=1\nùúïùúëùëñ"
        },
        {
          "a": "",
          "nd the EEG data to measure the contribution of each channel.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Compute all channel‚Äôs attribution\tùúô": "ùúô = ùëöùëíùëéùëõ(ùúô8, ùëéùë•ùëñùë† = 1)"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "return  ùúô"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "end"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "calculated with the data of all the subjects. The electrodes with"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "high occurrence were selected."
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "B. BCI Paradigms"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "For investigating the feasibility of the proposed framework,"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "three  BCI  paradigms  were  employed  in  this  study,  including"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "AAD, MI, and AC. The dataset of AAD was self-collected, and"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "the datasets of the other two paradigms were from the open data"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "repository,  which  was  BCI  Competition  IV-2a  for  MI  and"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "SEED  for  AC.  The  three  EEG  datasets  varied  in  acquisition"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "devices, experimental paradigms, subject numbers, and sample"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "sizes, providing a comprehensive basis for fairly validating the"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "multi-platform  portability  and  effectiveness  of  the  proposed"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "method. Due to the difference in the number of channels among"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "the  datasets,  the  electrode  positions  from  the  64-lead  system"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "were used as the foundational canvas for the analysis, as shown"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "in Fig. 2."
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "a) AAD"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "The goal of AAD was to decode auditory attention from EEG."
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "There  were  30  healthy  right-handed  participants  (aged  17-32"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "years,  15  females  and  15  males  with  subject  IDs  S1  to  S30)"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "recruited in this study. Before the experiment, the procedures"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "were provided and written informed consent was obtained from"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "all \nthe \nparticipants.  The \nexperimental \nprotocol  was \nin"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "accordance with the Declaration of Helsinki, and approved by"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "the  Research  Ethics  Committee  of  West  China  Hospital,"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "Sichuan University (# 2024582)."
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "The  experimental  protocol  was  illustrated  in  Fig.  3.  The"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "experiment  was  conducted  in  a  soundproof  room  where  the"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "participants' field of vision was limited to white walls. During"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "the  experiment,  the  participants  were  exposed  to  mixed  male"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "and female audio stimuli, and the EEG signals were recorded"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "with \na \ncommercial \ndevice \n(Enobio  EEG \nsystems,  NE"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "Neuroelectrics, Spain). Based on the international 10-20 system,"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "32 electrode positions were selected across the entire scalp, i.e.,"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "P8, T8, CP6, FC6, F8, F4, C4, P4, AF4, Fp2, Fp1, AF3, Fz, FC2,"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "Cz, CP2, PO3, O1, Oz, O2, PO4, Pz, CP1, FC1, P3, C3, F3, F7,"
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "FC5, CP5, T7, P7. The sampling frequency was 500 Hz."
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": ""
        },
        {
          "Compute all channel‚Äôs attribution\tùúô": "There were two AAD tasks, OA and TA. In the mixed speech"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "on the 0-4 second window after cue onset, corresponding to [2,"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "6] seconds per trial."
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "For  signal  preprocessing,  a  band-pass  filter  was  applied  to"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "the EEG data in the [4, 40] Hz range, as described in [21]. In"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "this  study,  a  6th-order  Chebyshev  filter  was  employed \nto"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": ""
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "preserve task-relevant rhythms. However, the time-consuming"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": ""
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "nature of MI-EEG acquisition and the limited size of the dataset"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": ""
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "increase  the  risk  of  underfitting.  As  such,  following  previous"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "studies \n[22], \n[23], \nthe \nstrategy \nof \nsegmentation \nand"
        },
        {
          "CP1, CPz, CP2, CP4, P1, Poz, Pz, and P2. Our analysis focused": "reconstruction  (S&R) \nin \nthe \ntime  domain  was  adopted \nto"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "was  performed  on  the  EEG  data  from  Datasets  MI,  AC  to"
        },
        {
          "4": "mitigate fluctuations and non-stationarity. The normalization is"
        },
        {
          "4": "as follows:"
        },
        {
          "4": "ùëã ‚àí ùúá"
        },
        {
          "4": "(5) \n \n \nùëã\n. ="
        },
        {
          "4": "ùúé"
        },
        {
          "4": "where\tùúá\tand\tùúé\tdenote  the  mean  and  standard  deviation  of  the"
        },
        {
          "4": "training set, respectively."
        },
        {
          "4": "c) AC"
        },
        {
          "4": "The goal of AC was to decode emotion from EEG. An open"
        },
        {
          "4": "dataset, SEED provided by Shanghai Jiao Tong University, was"
        },
        {
          "4": "adopted  in  this  study.  It  contains  emotion-based  EEG  signals"
        },
        {
          "4": "from  15  subjects.  Each  session \ninvolved  15  movie  clips"
        },
        {
          "4": "designed to evoke positive, neutral, and negative moods, with"
        },
        {
          "4": "the  paradigm  illustrated  in  Fig.  3  (D).  Data  were  collected"
        },
        {
          "4": "across  three  sessions,  spaced  approximately  one  week  apart."
        },
        {
          "4": "EEG  signals  were  recorded  from  62  electrodes  at  a  sampling"
        },
        {
          "4": "rate  of  1000  Hz,  and  subsequently  down  sampled  to  200  Hz."
        },
        {
          "4": "The  62  channels,  placed  according \nto \nthe  10-20  system,"
        },
        {
          "4": "included: Fp1, Fpz, Fp2, AF3, AF4, F7, F5, F3, F1, Fz, F2, F4,"
        },
        {
          "4": "F6, F8, FT7, FC5, FC3, FC1, FCz, FC2, FC4, FC6, FT8, T7,"
        },
        {
          "4": "C5, C3, C1, Cz, C2, C4, C6, T8, TP7, CP5, CP3, CP1, CPz,"
        },
        {
          "4": ""
        },
        {
          "4": "CP2, CP4, CP6, TP8, P7, P5, P3, P1, Pz, P2, P4, P6, P8, PO7,"
        },
        {
          "4": ""
        },
        {
          "4": "PO5, PO3, Poz, PO4, PO6, PO8, CB1, O1, Oz, O2, and CB2."
        },
        {
          "4": "Experiments were conducted based on the shortest trial length,"
        },
        {
          "4": ""
        },
        {
          "4": "consisting of 37,000 sample points (185 seconds)."
        },
        {
          "4": ""
        },
        {
          "4": "For preprocessing, we applied a 6th-order Chebyshev band-"
        },
        {
          "4": ""
        },
        {
          "4": "pass filter to the data in the [4, 47] Hz range. Each sample was"
        },
        {
          "4": ""
        },
        {
          "4": "then \nsegmented \ninto \nnon-overlapping \none-second \ntime"
        },
        {
          "4": ""
        },
        {
          "4": "windows, resulting in 2775 trials from a single session. Z-score"
        },
        {
          "4": ""
        },
        {
          "4": "normalization  was  adopted  to  mitigate  fluctuations  and  non-"
        },
        {
          "4": ""
        },
        {
          "4": "stationarity, which was the same as MI."
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "Fig.  3.  Experimental  paradigms \nfor \neach \ntask. \n(A)  Experimental"
        },
        {
          "4": "conditions  for  the  AAD,  illustrating  the  manner  in  which  participants"
        },
        {
          "4": "focus  their  attention  and  the  presentation  of  auditory  stimuli  during  the"
        },
        {
          "4": ""
        },
        {
          "4": "task.  Paradigms  for  (B)  AAD,  (C)  MI,  (D)  AC.  Each  experimental"
        },
        {
          "4": ""
        },
        {
          "4": "paradigm  provides  a  detailed  explanation  of  the  task  design  and  the"
        },
        {
          "4": "presentation of stimuli aimed at enhancing the decoding of brain signals"
        },
        {
          "4": "in different experimental contexts."
        },
        {
          "4": ""
        },
        {
          "4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": "TA  task,  projection  of  the  original  EEG  data  distribution.  (B)  TA  task,"
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": ""
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": "projection of features extracted by AADNet. (C) OA task, projection of"
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": "the  original  EEG  data  distribution.  (D)  OA  task,  projection  of  features"
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": "extracted  by  AADNet.  The  purpose  of  this  figure  is  to  demonstrate  the"
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": "effective"
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": "employed in the study."
        },
        {
          "Fig. 4. Decision capability projection of AADNet in OA and TA tasks. (A)": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "neural  network  per  second,  measured \nin  FPS  (frames  per"
        },
        {
          "5": "second).  The  effective  decoding  ACC \nis  defined  as \nthe"
        },
        {
          "5": ""
        },
        {
          "5": "decoding  accuracy  that  must  exceed  the  chance  level  for  a"
        },
        {
          "5": ""
        },
        {
          "5": "specific task category."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "III RESULTS"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "A. Decoding performance with channel pruning"
        },
        {
          "5": ""
        },
        {
          "5": "a) AAD task"
        },
        {
          "5": ""
        },
        {
          "5": "The performance of AAD task with different number of the"
        },
        {
          "5": ""
        },
        {
          "5": "channels \nis  displayed \nin  Tables \nI  and \nII.  The  decoding"
        },
        {
          "5": ""
        },
        {
          "5": "performance declines as the number of the channels decreasing"
        },
        {
          "5": ""
        },
        {
          "5": "under all the strategies, with a significant drop observed when"
        },
        {
          "5": ""
        },
        {
          "5": "channels are reduced from 10 to 5. Furthermore, compared to"
        },
        {
          "5": ""
        },
        {
          "5": "the  random  method,  the  two  proposed  RSs  (averaging  and"
        },
        {
          "5": ""
        },
        {
          "5": "voting)  consistently  demonstrated  a  significant  advantage  in"
        },
        {
          "5": ""
        },
        {
          "5": "mean decoding ACC across different channel counts, with this"
        },
        {
          "5": ""
        },
        {
          "5": "advantage  becoming  more  pronounced  as \nthe  number  of"
        },
        {
          "5": ""
        },
        {
          "5": "channels  decreased.  Even  with  only  5  channels  retained,  the"
        },
        {
          "5": ""
        },
        {
          "5": "proposed RSs maintained decoding accuracy above 80% for the"
        },
        {
          "5": ""
        },
        {
          "5": "OA  task  and  above  75%  for  the  TA  task,  while  the  random"
        },
        {
          "5": ""
        },
        {
          "5": "method  exhibited  a  further  decrease  of  approximately  10%."
        },
        {
          "5": "Additionally,  compared  to  the  voting  RS,  the  averaging  RS"
        },
        {
          "5": "demonstrated  superior  decoding  accuracy,  particularly  when"
        },
        {
          "5": "the channel count was 15."
        },
        {
          "5": "Fig.  5  (A)  and  (B)  clearly  highlight  the  advantage  of  the"
        },
        {
          "5": "averaging RS in decoding ACC and compare its stability with"
        },
        {
          "5": "that of the random selection, to emphasize the effectiveness of"
        },
        {
          "5": "the proposed approach. The results indicate that, in contrast to"
        },
        {
          "5": "random \nchannel \nselection, \nthe \naveraging \nRS \nexhibits"
        },
        {
          "5": "remarkable stability and reproducibility. Ultimately, due to its"
        },
        {
          "5": "superior \nability \nto  maintain \ndecoding \nperformance, \nthe"
        },
        {
          "5": "averaging RS was chosen for channel pruning analysis in the"
        },
        {
          "5": "AAD task."
        },
        {
          "5": "b) MI task"
        },
        {
          "5": "The  performance  of \nthe  4-class  MI \ntask  with  different"
        },
        {
          "5": "channel counts is shown in Table III. Under all strategies, the"
        },
        {
          "5": "decoding  performance  decreases  as  the  number  of  channels"
        },
        {
          "5": "reduces. Notably, when the number of selected channels was 5,"
        },
        {
          "5": "the  random  method  exhibited  a  significantly  lower  ability  to"
        },
        {
          "5": "select  effective  channels  compared  to  the  two  proposed  RSs."
        },
        {
          "5": ""
        },
        {
          "5": "With \nthe  proposed  RSs, \nthe  decoding  ACC  decreased  by"
        },
        {
          "5": ""
        },
        {
          "5": "approximately 12%, whereas the random method saw a more"
        },
        {
          "5": ""
        },
        {
          "5": "substantial decline of about 19%. Additionally, the averaging"
        },
        {
          "5": "RS  demonstrated  a  more  pronounced  advantage  in  decoding"
        },
        {
          "5": ""
        },
        {
          "5": "performance compared to the voting RS."
        },
        {
          "5": "Fig.  5  (C)  more  clearly  illustrates  the  advantage  of  the"
        },
        {
          "5": "averaging RS in maintaining decoding ACC and compares its"
        },
        {
          "5": "stability with the random selection method. The results indicate"
        },
        {
          "5": ""
        },
        {
          "5": "that, compared to random channel selection, the averaging RS"
        },
        {
          "5": ""
        },
        {
          "5": "shows superior subject stability. Ultimately, due to its superior"
        },
        {
          "5": ""
        },
        {
          "5": "ability  to  maintain  decoding  performance,  the  averaging  RS"
        },
        {
          "5": ""
        },
        {
          "5": "was applied in the MI task for channel pruning analysis."
        },
        {
          "5": ""
        },
        {
          "5": "c) AC task"
        },
        {
          "5": ""
        },
        {
          "5": "In \nthe  AC \ntask  with  more \nacquisition \nchannels, \nthe"
        },
        {
          "5": ""
        },
        {
          "5": "performance  with  different  number  of  channels  is  shown  in"
        },
        {
          "5": ""
        },
        {
          "5": "Table IV. Under all strategies, decoding performance decreased"
        },
        {
          "5": ""
        },
        {
          "5": "as \nthe  number  of  channels  decreased.  However,  when \nthe"
        },
        {
          "5": ""
        },
        {
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "AUC"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "0.976(¬±1.89%)"
        },
        {
          "TABLE I": "0.972(¬±2.69%)"
        },
        {
          "TABLE I": "0.970(¬±3.20%)"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "0.950(¬±3.57%)"
        },
        {
          "TABLE I": "0.886(¬±6.14%)"
        },
        {
          "TABLE I": "0.974(¬±2.18%)"
        },
        {
          "TABLE I": "0.967(¬±2.54%)"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "0.952(¬±3.56%)"
        },
        {
          "TABLE I": "0.891(¬±6.07%)"
        },
        {
          "TABLE I": "0.960(¬±2.61%)"
        },
        {
          "TABLE I": "0.934(¬±3.76%)"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "0.892(¬±5.22%)"
        },
        {
          "TABLE I": "0.781(¬±7.10%)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "AUC"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "0.962(¬±2.38%)"
        },
        {
          "TABLE II": "0.958(¬±2.50%)"
        },
        {
          "TABLE II": "0.945(¬±3.84%)"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "0.916(¬±5.10%)"
        },
        {
          "TABLE II": "0.844(¬±6.19%)"
        },
        {
          "TABLE II": "0.953(¬±3.17%)"
        },
        {
          "TABLE II": "0.943(¬±3.86%)"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "0.918(¬±4.98%)"
        },
        {
          "TABLE II": "0.836(¬±5.85%)"
        },
        {
          "TABLE II": "0.936(¬±3.20%)"
        },
        {
          "TABLE II": "0.901(¬±4.75%)"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "0.857(¬±5.19%)"
        },
        {
          "TABLE II": "0.724(¬±5.75%)"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "Channel",
          "TABLE IV": "Channel"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "ùúº",
          "TABLE IV": "ùúº"
        },
        {
          "TABLE III": "1.000",
          "TABLE IV": "1.000"
        },
        {
          "TABLE III": "0.68",
          "TABLE IV": "0.65"
        },
        {
          "TABLE III": "0.45",
          "TABLE IV": "0.32"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "0.23",
          "TABLE IV": "0.16"
        },
        {
          "TABLE III": "0.68",
          "TABLE IV": "0.08"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "0.45",
          "TABLE IV": "0.65"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.32"
        },
        {
          "TABLE III": "0.23",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.16"
        },
        {
          "TABLE III": "0.68",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.08"
        },
        {
          "TABLE III": "0.45",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.65"
        },
        {
          "TABLE III": "0.23",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.32"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.16"
        },
        {
          "TABLE III": "",
          "TABLE IV": "0.08"
        },
        {
          "TABLE III": "",
          "TABLE IV": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "performance  of  different  strategies  (dark  lines).  The  small  box  plots  represent  the  performance  comparison  between  the  selected  RS  strategies  and  the"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "Random strategy for the last subject in each task. (A) OA task. (B) TA task. (C) MI task. (D) AC task. The aim is to demonstrate the positive impact of the"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "considering \nthe \nfinal  OA \nand  TA \nresults, \nthe \nselected"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "channels\tùê∂::;\tare the top 15 electrodes ranked by classification"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "contribution after RS ranking: Fp1, Fp2, F7, F8, AF4, AF3, F3,"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "Fig. 6. Balance curves of computational efficiency and decoding accuracy"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "for the AAD task (OA, TA), MI task, and AC task models. The left y-axis"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "(in blue) represents the ACC at each channel count relative to the ACC"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "with \nall \nchannels,  while \nthe \nright  y-axis \n(in \nred) \nrepresents \nthe"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "computational efficiency at each channel count relative to the maximum"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "computational efficiency. (A) OA task, (B) TA task, (C) MI task, (D) AC"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "task.  The  purpose  of  the  figure  is  to  illustrate  the  balance  between"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "decoding  performance  and  computational  efficiency,  with  the  optimal"
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": "channel count for achieving this balance varying across different tasks."
        },
        {
          "Fig.  5.  The  task  decoding  accuracy  changes  curves  for  all  subjects  at  different  channel  counts  (light  lines),  along  with  a  comparison  of  the  average": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "channels in terms of scores. This figure aims to illustrate the distribution"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "of channel subsets with varying densities selected by PlugSelect for the"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "four-class MI task, highlighting its ability to sensitively identify sensory-"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "motor regions strongly associated with the task."
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "FC5, T8, T7, F4, CP6, C3, FC6, P8, and the electrode placement"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "channels in terms of scores. This figure aims to illustrate the distribution"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "of channel subsets with varying densities selected by PlugSelect for the"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "four-class MI task, highlighting its ability to sensitively identify sensory-"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "motor regions strongly associated with the task."
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "FC5, T8, T7, F4, CP6, C3, FC6, P8, and the electrode placement"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "locations are shown in Fig. 7 (C); in the MI task, the selected"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "channels\tùê∂<=\tare the top 10 electrodes: C3, CP4, C4, CP3, CP2,"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "C1,  CPz,  CP1,  Cz,  and  POz,  and  the  electrode  placement"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "locations  are  shown  in  Fig.  8  (B);  and  in  the  AC  task,  the"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "selected  channels\tùê∂:!\tare  the  top  10  electrodes:  T7,  T8,  TP7,"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "FT7, FT8, C5, Oz, C1, P7, P5, the electrode placement locations"
        },
        {
          "Fig.  8.  MI  task  channel  selection  results.  Top  (A)  5,  (B)  10,  (C)  15": "are shown in Fig. 9 (B)."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IV DISCUSSION": "This \nstudy \nproposes \nPlugSelect, \na \nchannel \nselection"
        },
        {
          "IV DISCUSSION": "framework for BCI tasks, aimed at practical applications such"
        },
        {
          "IV DISCUSSION": "as \nportable \nneuro-steered \nhearing \naids.  The \nframework"
        },
        {
          "IV DISCUSSION": "automatically selects channels through data-driven attentional"
        },
        {
          "IV DISCUSSION": "weight  assignment  while  ensuring  high  decoding  efficiency."
        },
        {
          "IV DISCUSSION": "We  validated  and  demonstrated \nthe  effectiveness  of \nthe"
        },
        {
          "IV DISCUSSION": "proposed PlugSelect in channel selection, as well as its multi-"
        },
        {
          "IV DISCUSSION": "platform  portability  and  broad  applicability,  using  the  multi-"
        },
        {
          "IV DISCUSSION": "attribute  AAD  dataset  collected  from  30  subjects,  along  with"
        },
        {
          "IV DISCUSSION": "widely  used  MI  (BCI  Competition  IV  2a)  and  AC  datasets"
        },
        {
          "IV DISCUSSION": "(SEED)."
        },
        {
          "IV DISCUSSION": "A. Plug-and-play PlugSelect, Multi-platform compatibility"
        },
        {
          "IV DISCUSSION": "The main inference component of PlugSelect employs the"
        },
        {
          "IV DISCUSSION": "IG \nalgorithm \nto \ninterpret \nthe \npre-trained  model \nand"
        },
        {
          "IV DISCUSSION": "automatically assign weights to each channel. Unlike end-to-"
        },
        {
          "IV DISCUSSION": "end channel selection models [10], which add deployment and"
        },
        {
          "IV DISCUSSION": "training costs, or channel selection modules [11] that may skew"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "the  learning  direction  of  the  classification  model  and  reduce"
        },
        {
          "IV DISCUSSION": "decoding performance, PlugSelect simplifies the process. And"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "it avoids the issues associated with multiple iterations to find"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "optimal  channel  combinations  [8],  which  can  increase  noise"
        },
        {
          "IV DISCUSSION": "interference and search difficulty. while channel sparsification"
        },
        {
          "IV DISCUSSION": "methods like CSP reduce costs to some extent, they largely rely"
        },
        {
          "IV DISCUSSION": "on prior selection or knowledge such as filters [15]. In contrast,"
        },
        {
          "IV DISCUSSION": "PlugSelect  streamlines  system  deployment  by  requiring  only"
        },
        {
          "IV DISCUSSION": "access \nto \nthe  pre-trained  model  and \nraw \ntask  data,  and"
        },
        {
          "IV DISCUSSION": "eliminates the need for classifier iteration or training, making it"
        },
        {
          "IV DISCUSSION": "truly  plug-and-play,  as  shown  in  Fig.  1.  For  this  reason,  as"
        },
        {
          "IV DISCUSSION": "described in this paper, we can easily apply PlugSelect to BCI"
        },
        {
          "IV DISCUSSION": "Competition  IV-2a  dataset,  SEED,  and  other  data  platforms"
        },
        {
          "IV DISCUSSION": "with  different  paradigms,  categorization  goals,  and \ntarget"
        },
        {
          "IV DISCUSSION": "quantities."
        },
        {
          "IV DISCUSSION": "Additionally, due to individual differences among subjects,"
        },
        {
          "IV DISCUSSION": "[25], [26] the optimal EEG channels vary from person to person"
        },
        {
          "IV DISCUSSION": "(Fig. 5). PlugSelect can automatically select the most suitable"
        },
        {
          "IV DISCUSSION": "sub-channel  for  each  subject  by  using  a  subject-specific  pre-"
        },
        {
          "IV DISCUSSION": "training model, thus facilitating the personalized design of BCI"
        },
        {
          "IV DISCUSSION": "devices for real-world applications."
        },
        {
          "IV DISCUSSION": "B. Maintain decoding efficiency, Automate channel"
        },
        {
          "IV DISCUSSION": "decoupling"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "The optimal number of channels depends on the specific BCI"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "paradigm and requirements. We also compared the impact of"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "channel density on decoding efficiency. PlugSelect maintains a"
        },
        {
          "IV DISCUSSION": "decoding \nefficiency \nsimilar \nto \nthat  of \nthe \nfull \nchannel"
        },
        {
          "IV DISCUSSION": "configuration  when  the  channel  density  is  greater  than  65%,"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "especially in the AAD task. When the number of channels is"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "reduced from 32 to 15, TA decoding performance decreases by"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "2%, while OA performance decreases by less than 1%, and OA"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "task performance remains more stable with a reduction in the"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "number of channels. In the 4-class MI task, when the channel"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "density is less than 50%, decoding accuracy decreases by less"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "than 5%. In the AC task, when the channel density  ùúº  is 0.32,"
        },
        {
          "IV DISCUSSION": ""
        },
        {
          "IV DISCUSSION": "emotional \ndecoding \naccuracy \nremains \nabove \n80%.  This"
        },
        {
          "IV DISCUSSION": "indicates that PlugSelect can effectively reduce the number of"
        },
        {
          "IV DISCUSSION": "channels unrelated to the task. Moreover, with this number of"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "The top three channels extracted by PlugSelect in the MI task"
        },
        {
          "9": "were  C3,  CP4,  and  C4,  which  align  with  the  findings  of  [7]."
        },
        {
          "9": "These channels are located in the primary sensorimotor cortex."
        },
        {
          "9": "Numerous  EEG  studies  have  confirmed  that  motor  imagery"
        },
        {
          "9": "activates  primary  sensorimotor  areas \n[29], \n[30]  and  have"
        },
        {
          "9": "observed  significant  ERD/ERS  phenomena  in  the  C3  and  C4"
        },
        {
          "9": "regions [31]."
        },
        {
          "9": "In the AC task, the top five channels identified by PlugSelect"
        },
        {
          "9": "were  T7,  T8,  TP7,  FT7  and  FT8,  consistent  with  the  results"
        },
        {
          "9": "from [32], [33], [34]. These channels are primarily situated in"
        },
        {
          "9": "the temporal and frontal lobes. The temporal lobe is associated"
        },
        {
          "9": "not  only  with  auditory  stimuli  in  this  paradigm  but  also  with"
        },
        {
          "9": "audiovisual \nemotion \ncomprehension \n[35].  And \nemotion"
        },
        {
          "9": "cognition  results  from  integrated  processing  across  various"
        },
        {
          "9": "brain regions [34], [36]."
        },
        {
          "9": "Moreover, since the multi-attribute AAD task paradigm used"
        },
        {
          "9": "in this study is less commonly discussed, we can only analyze"
        },
        {
          "9": "the plausibility of the channels extracted by PlugSelect from the"
        },
        {
          "9": "perspective of functional brain regions. In contrast, the MI and"
        },
        {
          "9": "ER  paradigms  and  datasets  are  well-established,  and \nthe"
        },
        {
          "9": ""
        },
        {
          "9": "channels identified by PlugSelect are consistent with findings"
        },
        {
          "9": ""
        },
        {
          "9": "reported  in  existing  literature,  which  serves  as  an  additional"
        },
        {
          "9": "validation of the reasonableness of the AAD channel results."
        },
        {
          "9": ""
        },
        {
          "9": "D. Research gap and future work"
        },
        {
          "9": "While \ncross-subject \nautomatic \nchannel \nselection \nhelps"
        },
        {
          "9": ""
        },
        {
          "9": "balance individual heterogeneity and group characteristics, this"
        },
        {
          "9": ""
        },
        {
          "9": "study  primarily \nrelied  on  statistical \nresults \nfrom \nthe  best"
        },
        {
          "9": ""
        },
        {
          "9": "subchannels  within  single-subject  domain \nto \nidentify \ntask-"
        },
        {
          "9": ""
        },
        {
          "9": "relevant \ninvariant \nchannels. \nIn \naddition, \nthe \nproposed"
        },
        {
          "9": ""
        },
        {
          "9": "PlugSelect  relies  on  an  effective  pre-trained  decoding  model."
        },
        {
          "9": ""
        },
        {
          "9": "Channels  identified  solely  based  on  model  inference  may  be"
        },
        {
          "9": ""
        },
        {
          "9": "limited in scope, and validating the effectiveness of the subset"
        },
        {
          "9": ""
        },
        {
          "9": "of  channels  extracted  by  PlugSelect  through  the  functional"
        },
        {
          "9": ""
        },
        {
          "9": "properties  of  brain  regions  may  lack  sufficient  accuracy.  In"
        },
        {
          "9": ""
        },
        {
          "9": "future studies, we will further explore unsupervised algorithms"
        },
        {
          "9": ""
        },
        {
          "9": "to overcome labeling constraints and comprehensively evaluate"
        },
        {
          "9": ""
        },
        {
          "9": "the subset of channels associated with BCI task from multiple"
        },
        {
          "9": ""
        },
        {
          "9": "physiological perspectives."
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "V CONCLUSION"
        },
        {
          "9": ""
        },
        {
          "9": "In this study, we proposed a novel plug-and-play framework,"
        },
        {
          "9": "PlugSelect, \nfor  efficient  channel \nselection \nin  BCI \ntasks."
        },
        {
          "9": "PlugSelect requires no additional training and can directly infer"
        },
        {
          "9": "and  interpret  channels  that  are  highly  correlated  with  cortical"
        },
        {
          "9": "electrical activity patterns through model result attribution. It is"
        },
        {
          "9": "also  efficiently  portable  across  multiple  platforms,  such  as"
        },
        {
          "9": "AAD,  MI  and  AC,  addressing \nthe \nlimitations  of  existing"
        },
        {
          "9": "algorithms. Furthermore, PlugSelect enables automatic channel"
        },
        {
          "9": ""
        },
        {
          "9": "decoupling while preserving decoding performance, providing"
        },
        {
          "9": ""
        },
        {
          "9": "a  subset  of  channels  closely  related  to  specific  BCI  tasks,"
        },
        {
          "9": ""
        },
        {
          "9": "supporting \nthe \ndevelopment \nand \napplication \nof \nportable"
        },
        {
          "9": ""
        },
        {
          "9": "wearable devices."
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "REFERENCES"
        },
        {
          "9": ""
        },
        {
          "9": "[1] \nA. \nBiasiucci, \nB. \nFranceschiello, \nand \nM. \nM. \nMurray,"
        },
        {
          "9": "‚ÄúElectroencephalography,‚Äù Current Biology, vol. 29, no. 3, pp. R80‚Äì"
        },
        {
          "9": "R85, Feb. 2019, doi: 10.1016/j.cub.2018.11.052."
        },
        {
          "9": "[2] \nG.  Pfurtscheller  et  al.,  ‚ÄúGraz-BCI:  State  of \nthe  art  and  clinical"
        },
        {
          "9": "applications,‚Äù \nIEEE \nTransactions \non \nNeural \nSystems \nand"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "Y.  Park \nand  W.  Chung, \n‚ÄúOptimal  Channel  Selection  Using"
        },
        {
          "10": "Correlation  Coefficient  for  CSP  Based  EEG  Classification,‚Äù  IEEE"
        },
        {
          "10": "Access, \nvol. \n8, \npp. \n111514‚Äì111521, \n2020, \ndoi:"
        },
        {
          "10": "10.1109/ACCESS.2020.3003056."
        },
        {
          "10": "J. Jin, Y. Miao, I. Daly, C. Zuo, D. Hu, and A. Cichocki, ‚ÄúCorrelation-"
        },
        {
          "10": "based channel selection and regularized feature optimization for MI-"
        },
        {
          "10": "based BCI,‚Äù Neural Networks, vol. 118, pp. 262‚Äì270, Oct. 2019, doi:"
        },
        {
          "10": "10.1016/J.NEUNET.2019.07.008."
        },
        {
          "10": "K. K. Ang, Z. Y. Chin, C. Wang, C. Guan, and H. Zhang, ‚ÄúFilter bank"
        },
        {
          "10": "common spatial pattern algorithm on BCI competition IV datasets 2a"
        },
        {
          "10": "and 2b,‚Äù Front Neurosci, vol. 6, no. MAR, p. 21002, Mar. 2012, doi:"
        },
        {
          "10": "10.3389/FNINS.2012.00039/BIBTEX."
        },
        {
          "10": "F.  Lotte,  ‚ÄúSignal  processing  approaches  to  minimize  or  suppress"
        },
        {
          "10": "calibration \ntime \nin \noscillatory \nactivity-based \nbrain-computer"
        },
        {
          "10": "interfaces,‚Äù Proceedings of the IEEE, vol. 103, no. 6, pp. 871‚Äì890,"
        },
        {
          "10": "Jun. 2015, doi: 10.1109/JPROC.2015.2404941."
        },
        {
          "10": "Y.  Song,  Q.  Zheng,  B.  Liu,  and  X.  Gao,  ‚ÄúEEG  Conformer:"
        },
        {
          "10": "Convolutional  Transformer  for  EEG  Decoding  and  Visualization,‚Äù"
        },
        {
          "10": "IEEE \nTransactions \non \nNeural \nSystems \nand \nRehabilitation"
        },
        {
          "10": "Engineering, \nvol. \n31, \npp. \n710‚Äì719, \n2023, \ndoi:"
        },
        {
          "10": "10.1109/TNSRE.2022.3230250."
        },
        {
          "10": "K. Shi et al., ‚ÄúAADNet: Exploring EEG Spatiotemporal Information"
        },
        {
          "10": "for Fast and Accurate Orientation and Timbre Detection of Auditory"
        },
        {
          "10": "Attention Based on A Cue-Masked Paradigm,‚Äù Jan. 2025, Accessed:"
        },
        {
          "10": "Jan. \n10, \n2025. \n[Online]. \nAvailable:"
        },
        {
          "10": "https://arxiv.org/abs/2501.03571v1"
        },
        {
          "10": "M. Arvaneh, C. Guan, K. K. Ang, and H. C. Quek, ‚ÄúEEG Channel"
        },
        {
          "10": "Selection Using Decision Tree in Brain-Computer Interface,‚Äù pp. 14‚Äì"
        },
        {
          "10": "17, 2010."
        },
        {
          "10": "H.  Varsehi  and  S.  M.  P.  Firoozabadi,  ‚ÄúAn  EEG  channel  selection"
        },
        {
          "10": "method \nfor  motor \nimagery  based  brain‚Äìcomputer \ninterface  and"
        },
        {
          "10": "neurofeedback using Granger causality,‚Äù Neural Networks, vol. 133,"
        },
        {
          "10": "pp. 193‚Äì206, Jan. 2021, doi: 10.1016/J.NEUNET.2020.11.002."
        },
        {
          "10": "L.  J√§ncke  and  N.  J.  Shah,  ‚ÄúDoes  dichotic  listening  probe  temporal"
        },
        {
          "10": "lobe functions?,‚Äù Neurology, vol. 58, no. 5, pp. 736‚Äì743, Mar. 2002,"
        },
        {
          "10": "doi: 10.1212/WNL.58.5.736."
        },
        {
          "10": "R. T. Knight, D. Scabini, and D. L. Woods, ‚ÄúPrefrontal cortex gating"
        },
        {
          "10": "of auditory transmission in humans,‚Äù Brain Res, vol. 504, no. 2, pp."
        },
        {
          "10": "338‚Äì342, Dec. 1989, doi: 10.1016/0006-8993(89)91381-4."
        },
        {
          "10": "W. Lang, D. Cheyne, P. H√∂llinger, W. Gerschlager, and G. Lindinger,"
        },
        {
          "10": "‚ÄúElectric  and  magnetic  fields  of  the  brain  accompanying  internal"
        },
        {
          "10": "simulation of movement,‚Äù Cognitive Brain Research, vol. 3, no. 2, pp."
        },
        {
          "10": "125‚Äì129, Mar. 1996, doi: 10.1016/0926-6410(95)00037-2."
        },
        {
          "10": "R. Beisteiner, P. H√∂llinger, G. Lindinger, W. Lang, and A. Berthoz,"
        },
        {
          "10": "‚ÄúMental  representations  of  movements.  Brain  potentials  associated"
        },
        {
          "10": "with imagination of hand movements,‚Äù Electroencephalography and"
        },
        {
          "10": "Clinical Neurophysiology/Evoked Potentials Section, vol. 96, no. 2,"
        },
        {
          "10": "pp. 183‚Äì193, Mar. 1995, doi: 10.1016/0168-5597(94)00226-5."
        },
        {
          "10": "G. \nPfurtscheller \nand \nC. \nNeuper, \n‚ÄúMotor \nimagery \ndirect"
        },
        {
          "10": "communication,‚Äù Proceedings of the IEEE, vol. 89, no. 7, pp. 1123‚Äì"
        },
        {
          "10": "1134, 2001, doi: 10.1109/5.939829."
        },
        {
          "10": "J. Y. Guo et al., ‚ÄúA Transformer based neural network for emotion"
        },
        {
          "10": "recognition and visualizations of crucial EEG channels,‚Äù Physica A:"
        },
        {
          "10": "Statistical Mechanics and its Applications, vol. 603, p. 127700, Oct."
        },
        {
          "10": "2022, doi: 10.1016/J.PHYSA.2022.127700."
        },
        {
          "10": "G.  Qu  et  al.,  ‚ÄúA  Hybrid  Critical  Channel  Selection  Framework  for"
        },
        {
          "10": "EEG Emotion Recognition,‚Äù IEEE Sens J, vol. 24, no. 9, pp. 14881‚Äì"
        },
        {
          "10": "14893, May 2024, doi: 10.1109/JSEN.2024.3380749."
        },
        {
          "10": "C. Chen, Z. Li, F. Wan, L. Xu, A. Bezerianos, and H. Wang, ‚ÄúFusing"
        },
        {
          "10": "Frequency-Domain  Features  and  Brain  Connectivity  Features  for"
        },
        {
          "10": "Cross-Subject Emotion Recognition,‚Äù IEEE Trans Instrum Meas, vol."
        },
        {
          "10": "71, 2022, doi: 10.1109/TIM.2022.3168927."
        },
        {
          "10": "B.  Metternich  et  al.,  ‚ÄúDynamic  facial  emotion  recognition  and"
        },
        {
          "10": "affective prosody recognition are associated in patients with temporal"
        },
        {
          "10": "lobe  epilepsy,‚Äù  Sci  Rep,  vol.  14,  no.  1,  p.  3935,  Dec.  2024,  doi:"
        },
        {
          "10": "10.1038/S41598-024-53401-9."
        },
        {
          "10": "P.  Li  et  al.,  ‚ÄúEEG  Based  Emotion  Recognition  by  Combining"
        },
        {
          "10": "Functional  Connectivity  Network  and  Local  Activations,‚Äù  IEEE"
        },
        {
          "10": "Trans Biomed Eng, vol. 66, no. 10, pp. 2869‚Äì2881, Oct. 2019, doi:"
        },
        {
          "10": "10.1109/TBME.2019.2897651."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Current Biology",
      "authors": [
        "A Biasiucci",
        "B Franceschiello",
        "M Murray",
        "\" Electroencephalography"
      ],
      "year": "2019",
      "venue": "Current Biology",
      "doi": "10.1016/j.cub.2018.11.052"
    },
    {
      "citation_id": "2",
      "title": "Graz-BCI: State of the art and clinical applications",
      "authors": [
        "G Pfurtscheller"
      ],
      "year": "2003",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2003.814454"
    },
    {
      "citation_id": "3",
      "title": "A Neural-Inspired Architecture for EEG-Based Auditory Attention Detection",
      "authors": [
        "S Cai",
        "P Li",
        "E Su",
        "Q Liu",
        "L Xie"
      ],
      "year": "2022",
      "venue": "IEEE Trans Hum Mach Syst",
      "doi": "10.1109/THMS.2022.3176212"
    },
    {
      "citation_id": "4",
      "title": "MI-DAGSC: A domain adaptation approach incorporating comprehensive information from MI-EEG signals",
      "authors": [
        "D Zhang",
        "H Li",
        "J Xie",
        "D Li"
      ],
      "year": "2023",
      "venue": "Neural Networks",
      "doi": "10.1016/J.NEUNET.2023.08.008"
    },
    {
      "citation_id": "5",
      "title": "Adolescent major depressive disorder: Neuroimaging evidence of sex difference during an affective Go/No-Go task",
      "authors": [
        "J Chuang"
      ],
      "year": "2017",
      "venue": "Front Psychiatry",
      "doi": "10.3389/FPSYT.2017.00119"
    },
    {
      "citation_id": "6",
      "title": "EEG channel selection using Gramian Angular Fields and spectrograms for energy data visualization",
      "authors": [
        "O Kucukler",
        "A Amira",
        "H Malekmohamadi"
      ],
      "year": "2024",
      "venue": "Eng Appl Artif Intell",
      "doi": "10.1016/j.engappai.2024.108305"
    },
    {
      "citation_id": "7",
      "title": "MI-BMInet: An Efficient Convolutional Neural Network for Motor Imagery Brain-Machine Interfaces With EEG Channel Selection",
      "authors": [
        "X Wang",
        "M Hersche",
        "M Magno",
        "L Benini"
      ],
      "year": "2024",
      "venue": "IEEE Sens J",
      "doi": "10.1109/JSEN.2024.3353146"
    },
    {
      "citation_id": "8",
      "title": "Motor imagery electroencephalography channel selection based on deep learning: A shallow convolutional neural network",
      "authors": [
        "H Kashefi Amiri",
        "M Zarei",
        "M Daliri"
      ],
      "year": "2024",
      "venue": "Eng Appl Artif Intell",
      "doi": "10.1016/j.engappai.2024.108879"
    },
    {
      "citation_id": "9",
      "title": "Decoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications",
      "authors": [
        "B Mirkovic",
        "S Debener",
        "M Jaeger",
        "M Vos"
      ],
      "year": "2015",
      "venue": "J Neural Eng",
      "doi": "10.1088/1741-2560/12/4/046007"
    },
    {
      "citation_id": "10",
      "title": "AN END-TO-END EEG CHANNEL SELECTION METHOD WITH RESIDUAL GUMBEL SOFTMAX FOR BRAIN-ASSISTED SPEECH ENHANCEMENT",
      "authors": [
        "Q Xu",
        "J Zhang",
        "Z Ling"
      ],
      "year": "2024",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP48485.2024.10446263"
    },
    {
      "citation_id": "11",
      "title": "Graph Convolution Neural Network Based End-to-End Channel Selection and Classification for Motor Imagery Brain-Computer Interfaces",
      "authors": [
        "B Sun",
        "Z Liu",
        "Z Wu",
        "C Mu",
        "T Li"
      ],
      "year": "2023",
      "venue": "IEEE Trans Industr Inform",
      "doi": "10.1109/TII.2022.3227736"
    },
    {
      "citation_id": "12",
      "title": "Analysis of Miniaturization Effects and Channel Selection Strategies for EEG Sensor Networks With Application to Auditory Attention Detection",
      "authors": [
        "A Narayanan",
        "A Bertrand"
      ],
      "year": "2020",
      "venue": "IEEE Trans Biomed Eng",
      "doi": "10.1109/TBME.2019.2911728"
    },
    {
      "citation_id": "13",
      "title": "EEG emotion recognition using improved graph neural network with channel selection",
      "authors": [
        "X Lin",
        "J Chen",
        "W Ma",
        "W Tang",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Comput Methods Programs Biomed",
      "doi": "10.1016/J.CMPB.2023.107380"
    },
    {
      "citation_id": "14",
      "title": "Brain Topology Modeling With EEG-Graphs for Auditory Spatial Attention Detection",
      "authors": [
        "S Cai",
        "T Schultz",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Trans Biomed Eng",
      "doi": "10.1109/TBME.2023.3294242"
    },
    {
      "citation_id": "15",
      "title": "Common Spatial Pattern Method for Channel Selelction in Motor Imagery Based Brain-computer Interface",
      "authors": [
        "Y Wang",
        "S Gao",
        "X Gao"
      ],
      "year": "2005",
      "venue": "2005 IEEE Engineering in Medicine and Biology 27th Annual Conference",
      "doi": "10.1109/IEMBS.2005.1615701"
    },
    {
      "citation_id": "16",
      "title": "Sparse spatial filter optimization for EEG channel reduction in brain-computer interface",
      "authors": [
        "X Yong",
        "R Ward",
        "G Birch"
      ],
      "year": "2008",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing -Proceedings",
      "doi": "10.1109/ICASSP.2008.4517635"
    },
    {
      "citation_id": "17",
      "title": "Automated selecting subset of channels based on CSP in motor imagery brain-computer interface system",
      "authors": [
        "J Meng",
        "G Liu",
        "G Huang",
        "X Zhu"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Robotics and Biomimetics",
      "doi": "10.1109/ROBIO.2009.5420462"
    },
    {
      "citation_id": "18",
      "title": "EEG power spectra parameterization and adaptive channel selection towards semi-supervised seizure prediction",
      "authors": [
        "H Li",
        "J Liao",
        "H Wang",
        "C Zhan",
        "F Yang"
      ],
      "year": "2024",
      "venue": "Comput Biol Med",
      "doi": "10.1016/J.COMPBIOMED.2024.108510"
    },
    {
      "citation_id": "19",
      "title": "Optimal Channel Selection Using Correlation Coefficient for CSP Based EEG Classification",
      "authors": [
        "Y Park",
        "W Chung"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3003056"
    },
    {
      "citation_id": "20",
      "title": "Correlationbased channel selection and regularized feature optimization for MIbased BCI",
      "authors": [
        "J Jin",
        "Y Miao",
        "I Daly",
        "C Zuo",
        "D Hu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "Neural Networks",
      "doi": "10.1016/J.NEUNET.2019.07.008"
    },
    {
      "citation_id": "21",
      "title": "Filter bank common spatial pattern algorithm on BCI competition IV datasets 2a and 2b",
      "authors": [
        "K Ang",
        "Z Chin",
        "C Wang",
        "C Guan",
        "H Zhang"
      ],
      "year": "2012",
      "venue": "Front Neurosci",
      "doi": "10.3389/FNINS.2012.00039/BIBTEX"
    },
    {
      "citation_id": "22",
      "title": "Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces",
      "authors": [
        "F Lotte"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/JPROC.2015.2404941"
    },
    {
      "citation_id": "23",
      "title": "EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization",
      "authors": [
        "Y Song",
        "Q Zheng",
        "B Liu",
        "X Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering",
      "doi": "10.1109/TNSRE.2022.3230250"
    },
    {
      "citation_id": "24",
      "title": "AADNet: Exploring EEG Spatiotemporal Information for Fast and Accurate Orientation and Timbre Detection of Auditory Attention Based on A Cue-Masked Paradigm",
      "authors": [
        "K Shi"
      ],
      "year": "2025",
      "venue": "AADNet: Exploring EEG Spatiotemporal Information for Fast and Accurate Orientation and Timbre Detection of Auditory Attention Based on A Cue-Masked Paradigm"
    },
    {
      "citation_id": "25",
      "title": "EEG Channel Selection Using Decision Tree in Brain-Computer Interface",
      "authors": [
        "M Arvaneh",
        "C Guan",
        "K Ang",
        "H Quek"
      ],
      "year": "2010",
      "venue": "EEG Channel Selection Using Decision Tree in Brain-Computer Interface"
    },
    {
      "citation_id": "26",
      "title": "An EEG channel selection method for motor imagery based brain-computer interface and neurofeedback using Granger causality",
      "authors": [
        "H Varsehi",
        "S Firoozabadi"
      ],
      "year": "2021",
      "venue": "Neural Networks",
      "doi": "10.1016/J.NEUNET.2020.11.002"
    },
    {
      "citation_id": "27",
      "title": "Does dichotic listening probe temporal lobe functions?",
      "authors": [
        "L J√§ncke",
        "N Shah"
      ],
      "year": "2002",
      "venue": "Neurology",
      "doi": "10.1212/WNL.58.5.736"
    },
    {
      "citation_id": "28",
      "title": "Prefrontal cortex gating of auditory transmission in humans",
      "authors": [
        "R Knight",
        "D Scabini",
        "D Woods"
      ],
      "year": "1989",
      "venue": "Brain Res",
      "doi": "10.1016/0006-8993(89)91381-4"
    },
    {
      "citation_id": "29",
      "title": "Electric and magnetic fields of the brain accompanying internal simulation of movement",
      "authors": [
        "W Lang",
        "D Cheyne",
        "P H√∂llinger",
        "W Gerschlager",
        "G Lindinger"
      ],
      "year": "1996",
      "venue": "Cognitive Brain Research",
      "doi": "10.1016/0926-6410(95)00037-2"
    },
    {
      "citation_id": "30",
      "title": "Mental representations of movements. Brain potentials associated with imagination of hand movements",
      "authors": [
        "R Beisteiner",
        "P H√∂llinger",
        "G Lindinger",
        "W Lang",
        "A Berthoz"
      ],
      "year": "1995",
      "venue": "Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section",
      "doi": "10.1016/0168-5597(94)00226-5"
    },
    {
      "citation_id": "31",
      "title": "Motor imagery direct communication",
      "authors": [
        "G Pfurtscheller",
        "C Neuper"
      ],
      "year": "2001",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/5.939829"
    },
    {
      "citation_id": "32",
      "title": "A Transformer based neural network for emotion recognition and visualizations of crucial EEG channels",
      "authors": [
        "J Guo"
      ],
      "year": "2022",
      "venue": "Physica A: Statistical Mechanics and its Applications",
      "doi": "10.1016/J.PHYSA.2022.127700"
    },
    {
      "citation_id": "33",
      "title": "A Hybrid Critical Channel Selection Framework for EEG Emotion Recognition",
      "authors": [
        "G Qu"
      ],
      "year": "2024",
      "venue": "IEEE Sens J",
      "doi": "10.1109/JSEN.2024.3380749"
    },
    {
      "citation_id": "34",
      "title": "Fusing Frequency-Domain Features and Brain Connectivity Features for Cross-Subject Emotion Recognition",
      "authors": [
        "C Chen",
        "Z Li",
        "F Wan",
        "L Xu",
        "A Bezerianos",
        "H Wang"
      ],
      "venue": "IEEE Trans Instrum Meas",
      "doi": "10.1109/TIM.2022.3168927"
    },
    {
      "citation_id": "35",
      "title": "Dynamic facial emotion recognition and affective prosody recognition are associated in patients with temporal lobe epilepsy",
      "authors": [
        "B Metternich"
      ],
      "year": "2024",
      "venue": "Sci Rep",
      "doi": "10.1038/S41598-024-53401-9"
    },
    {
      "citation_id": "36",
      "title": "EEG Based Emotion Recognition by Combining Functional Connectivity Network and Local Activations",
      "authors": [
        "P Li"
      ],
      "year": "2019",
      "venue": "IEEE Trans Biomed Eng",
      "doi": "10.1109/TBME.2019.2897651"
    }
  ]
}