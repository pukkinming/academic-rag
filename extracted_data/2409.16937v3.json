{
  "paper_id": "2409.16937v3",
  "title": "Semi-Supervised Cognitive State Classification From Speech With Multi-View Pseudo-Labeling",
  "published": "2024-09-25T13:51:19Z",
  "authors": [
    "Yuanchao Li",
    "Zixing Zhang",
    "Jing Han",
    "Peter Bell",
    "Catherine Lai"
  ],
  "keywords": [
    "Emotion Recognition",
    "Dementia Detection",
    "Semi-Supervised Learning",
    "Fréchet Audio Distance",
    "LLMs"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multiview pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Fréchet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. Highconfidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech classification tasks, especially those related to cognitive states, are crucial for various applications, including human-computer interaction, health monitoring, and clinical diagnosis. However, human annotation for these tasks is often expensive, time-consuming, and requires extensive subjective assessment. This data scarcity issue hinders the progress of these classifications in real-world applications. To address this challenge, Semi-Supervised Learning (SSL) offers a promising approach by leveraging both limited labeled data and a larger amount of unlabeled data for various classification tasks, such as hate speech detection, emotion recognition, sound event detection, sleepiness detection, and gender classification  [1] -  [3] .\n\nGenerally, SSL methods can be categorized into two main types: generating reliable pseudo-labels and building reliable models using limited data. For example, pseudo-labeling involves creating labels for unlabeled data based on predictions from an iteratively trained model  [4] . Consistency regularization ensures that the model produces consistent predictions for augmented versions of the same input (e.g., with added noise)  [5] . These two types are often complementary and are commonly used together in existing SSL frameworks  [6] . Moreover, the advancements in self-supervised learning have further improved SSL by utilizing large amounts of unlabeled data to pretrain SSL models  [7] ,  [8] .\n\nAmong the literature, the most relevant studies to our work focus on pseudo-label generation. D'Sa et al.  [1]  used label propagation, transducing labels from labeled data to unlabeled data with probabilistic transitions for hate speech classification. Zhang et al.  [2]  added unlabeled data with high confidence levels to the training set and resampled the originally labeled data for sound event classification. They also proposed dividing acoustic features into two views, selecting high-confidence instances in each view, and aggregating them with their predictions into the initial training sets per iteration  [3] . Zhu et al.  [4]  applied noisy student training  [9]  to emotion recognition, using a teacher model trained on labeled data to infer soft labels for unlabeled data. Feng et al.  [6]  proposed incorporating federated learning, utilizing both labeled and unlabeled data at each local client in a multi-view pseudo-labeling approach.\n\nDespite these advances, selecting high-confidence pseudo-labeled data remains challenging. In this work:\n\n• We propose a novel SSL framework, integrating multi-view pseudo-labeling that leverages both acoustic and linguistic characteristics to select the most confident data for model training.\n\n• We employ Fréchet Audio Distance (FAD) as a reference-free method to cluster unlabeled data based on acoustic similarity.\n\n• We use task-specific prompts to predict labels from ASR transcripts, learning insights from acoustics, linguistics, and psychology.\n\n• We examine multiple fusion methods in the context of SSL to build the bimodal classifier.\n\nOur proposed SSL framework is evaluated on emotion recognition and dementia detection for classifying short-term and long-term cognitive states, demonstrating competitive performance using only 30% of the labeled data compared to fully supervised learning, and showing greater effectiveness than the selected baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Methodology",
      "text": "As illustrated in Fig.  1 , the proposed SSL framework with multiview pseudo-labeling consists of two paths: acoustic and linguistic. The acoustic path utilizes the similarity between labeled and unlabeled data based on diverse audio embeddings, while the linguistic path employs LLMs to predict class labels from ASR transcriptions using task-specific knowledge. If the generated pseudo-labels from both paths align, we consider them as high-confidence data for training a bimodal classifier. Otherwise, the data are treated as lowconfidence and will be further predicted using the trained bimodal classifier. The semi-supervised training of the bimodal classifier will iterate until a predefined criterion is met.\n\nA. Multi-View Pseudo-Labeling 1) Acoustic Path: We extract acoustic features from multiple audio encoders trained with different objectives to reduce the bias of relying on a single one, inspired by a previous study in the music field  [10] . We use the following four audio encoders, resulting in four sets of embeddings for calculating the respective FAD scores:\n\n• VGGish: convolutional embeddings  [11]  • EnCodec: low-rate audio codecs  [12]  • Wav2vec 2.0: self-supervised acoustic embeddings  [13]  • CLAP: contrastive audio-text embeddings  [14]  Given the embeddings of labeled data X l and unlabeled data X u , the FAD score is calculated using multivariate Gaussians from two embedding sets X l (µ l , Σ l ) and X u (µu, Σu) as follows:\n\narXiv:2409.16937v3 [eess.AS] 30 Apr 2025  where tr is the trace of a matrix. Compared to traditional similarity metrics, such as cosine similarity and Euclidean distance, FAD is specifically designed for audio assessment, reflecting the perceptual similarity between two audio embedding distributions  [15] . It has proven effective in distinguishing real and synthetic audio  [16] , classifying audio with different emotions  [10] , and measuring acoustic similarity between emotional music and speech  [17] . Therefore, we use FAD to measure the acoustic similarity between labeled and unlabeled data. The four FAD scores, calculated based on four sets of embeddings, are averaged to obtain the final FAD score. The unlabeled data with the smallest FAD score relative to the labeled class will be assigned that class label as the acoustic pseudo-labels. An example of this process for emotion pseudo-labeling is shown in Table  I . Note that the scores are not comparable across different audio encoders, as they are calculated based on different embeddings.\n\n2) Linguistic Path: Previous speech classification tasks that used textual information typically relied on ground-truth text. However, in real-world applications, ASR is the only text source, and its transcriptions are usually noisy and contain errors, which can lead to incorrect classifications or pseudo-labels. We argue that it is more challenging to prompt LLMs for classification tasks based on ASR transcriptions compared to human transcriptions due to the presence of word errors.\n\nTo address this, we use the REVISE-REASON-RECOGNIZE (R3) prompting pipeline to perform speech classification with ASR Error Correction (AEC) and reasoning on ASR transcriptions  [18] . The R3 pipeline involves three steps: REVISE, where ASR errors are corrected based on N-best hypotheses; REASON, where the LLMs self-explain based on the corrected transcriptions and task-specific knowledge; and RECOGNIZE, where the label is identified.\n\nFor the ASR systems, we adopt the following ten models, the same as  [19] , to generate diverse transcriptions and form 10-best ASR hypotheses:\n\n• Wav2Vec2-base-{100h,960h}\n\n• Whisper-{tiny, base, small, large-v2}.en To perform AEC, we follow an AEC-specific Alpaca template  [20] , which uses the \"You are an ASR error corrector\" instruction, guiding the LLMs to perform error correction. As LLMs have demonstrated their ability in both AEC and emotion recognition  [21] , we expect that this capability can be extended to dementia detection from ASR transcriptions as well. The revised ASR transcriptions will be used for subsequent text feature extraction to train the bimodal classifier. For reasoning, we design task-specific knowledge that incorporates acoustics, linguistics, and psychology as in Fig.  1 .\n\nWe first apply Parameter-Efficient Fine-Tune (PEFT) on the following three LLMs with the LoRA adapter  [22]  using the labeled data with the R3 prompt:\n\nThe learning rate, weight decay, and number of epochs are set to 1e-4, 1e-5, and 5, respectively, with AdamW optimizer used. The three fine-tuned LLMs are then prompted with the R3 prompt to predict class labels from ASR transcriptions of the unlabeled data.\n\nFinally, majority voting is applied to the predicted labels of the three LLMs to generate the linguistic pseudo-labels.\n\n3) Data Selection: The acoustic and linguistic pseudo-labels generated from the two paths are combined to select the most confident data for semi-supervised training. Data with matching pseudo-labels from both paths are selected as high-confidence, while data with differing pseudo-labels are considered low-confidence. Together with the labeled data, the high-confidence data will be used to train the bimodal classifier in the first iteration, ensuring robust initial training.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "B. Semi-Supervised Training",
      "text": "1) Bimodal Classifier: The bimodal classifier consists of pretrained feature encoders-HuBERT  [23]  and RoBERTa  [24] -that extract audio and text features, respectively, and a classification model that uses these features to generate a prediction label. For PEFT the encoders and training the classification model, the learning rate, weight decay, number of epochs, and batch size are set as 1e-4, 1e-5, 30, and 64, respectively. The AdamW optimizer is used. For the classification model, we examine the following four fusion methods:\n\n• Early fusion: text and audio features are concatenated at the embedding level • Cross-attention fusion: text and audio features are attended to each other via attention and then concatenated  [25]  • Tensor fusion: unimodal information and bimodal interactions are learned explicitly and then aggregated  [26]  • Modality-gated fusion: primary modality is dynamically adjusted in each training step  [27]  2) Iteration: After training the bimodal classifier, low-confidence data are predicted and labeled using the trained classifier. In most previous SSL studies, model-labeled data are fully trusted and incorporated into the training set in the next iteration. However, as training progresses, mislabeled data (noise) may accumulate, leading to a cycle of erroneous learning  [28] . To address this issue, we choose not to fully trust the model-labeled data. Instead, the pseudo-label generated by the bimodal classifier is compared with pseudo-labels from multi-view pseudo-labeling. If the model pseudo-label matches either the acoustic or linguistic pseudo-label, the data are added to the training set for the next iteration. Otherwise, they remain lowconfidence and will be predicted in the next iteration.\n\nIn each iteration, we update the training set by adding modellabeled data and randomly removing 20% of the initial highconfidence data to avoid over-reliance on multi-view pseudo-labeling. The model is reinitialized in every iteration to reduce overfitting and bias. The maximum number of iterations is set to 40. However, if there is no performance improvement on the validation set for two consecutive iterations, the iteration will be terminated. The process is summarized in Algorithm 1.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Datasets And Experimental Settings",
      "text": "We use IEMOCAP  [29]  for emotion recognition and ADReSSo  [30]  for dementia detection. For IEMOCAP, we focus on the Big Four emotion classes and exclude utterances with blank transcriptions, resulting in 5,500 utterances (1,103 angry, 1,615 happy+excited, 1,704 neutral, 1,078 sad). For ADReSSo, since there are no human labels in the test set to verify our approach, we use only the training set and focus on binary classes: Alzheimer's Dementia (AD) and Cognitively Normal (CN). Due to the long duration of each audio file and the presence of interviewer's speech, we segment all files using the official segmentation information, extracting participants' speech, which results in 2,268 utterances  (1, 200   For both tasks, we use an 80/10/10 split for training, validation, and testing, applying the ratio equally to each class to ensure a balanced distribution. Additionally, the ground-truth labeling rates for the training data are compared at 20%, 25%, and 30%, with the remaining data labeled using our method. All results are measured using Unweighted Accuracy (UA). Random seeds are kept consistent across all experiments. Other settings have been detailed in the previous section (Code available).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Results And Discussions",
      "text": "Four baselines are used for comparison:\n\n• Supervised full: the classification model is trained on the entire training data (i.e., the 80% split) • Supervised limited: the classification model is trained on the limited labeled data without pseudo-labeling the unlabeled data • Decision merging: two classification models are trained using audio and text, respectively, and their probability distribution are merged to select high-confidence data for the next iteration • Co-training: two classification models are trained using audio and text, respectively. High-confidence data selected by each model are added to the training set for the other model in the next iteration  [31] ,  [32]  The comparison of our method with supervised full and supervised limited are shown in Table  II . It can be observed that 1) With only 30% labeled data, our method achieves performance competitive with the supervised full baseline. 2) Our proposed method outperforms the supervised limited baseline, which lacks multi-view pseudo-labeling and semi-supervised training to augment the labeled data, particularly in dementia detection.\n\n3) The less ground-truth labeled data available, the more effective our method is in dementia detection, likely because binary classes are easier for pseudo-labeling. 4) When ground-truth labels are limited, classification performance of the supervised limited baseline for both tasks drops significantly compared to the supervised full training. Our proposed method, however, mitigates this drop by more than 2% in emotion recognition and 3%-7% in dementia detection. 5) Modality-gated fusion performs best  among the four fusion methods. This is reasonable as it dynamically selects the primary modality contributing most to the classification tasks, thereby reducing the impact of ASR errors in the text modality. As bimodal fusion does not apply to the decision merging and cotraining baseline settings, we select our best-performing results for comparison. Note that the principle of decision merging is the same as that of a fusion method-late fusion (or decision-level fusion)  [33] . The difference is that we further use the merged probability to determine the high-confidence data for iteration, whereas late fusion directly outputs results based on the highest probability. Here, we refer to it as decision merging to avoid confusion with late fusion, as we have used fusion techniques in the previous experiment. For both decision merging and co-training, we set the threshold at 0.5 for emotion recognition and 0.7 for dementia detection. For example, the fourth emotion class will be selected as the pseudo-label for an unlabeled data with a probability distribution of {0.1, 0.1, 0.3, 0.5}, and the unlabeled data will be added to the training set as highconfidence data.\n\nFrom Table  III , we can observe that: 1) Our method significantly outperforms the two baselines, likely for two reasons: first, it generates more high-confidence unlabeled data with pseudo-labels, which iteratively trains the classifier for performance improvement; second, our bimodal classifier takes both modalities as input during training. In contrast, although the two baselines also consider two modalities, their classifiers are separate, each relying on a single modality and ignoring the interrelatedness between them. 2) Although both decision merging and co-training consist of two classifiers that output respective labels, the latter performs better than the former, especially in emotion recognition. This result is plausible since decision merging could potentially weaken the better prediction if the other probability is significantly incorrect. On the contrary, by incorporating high- Its relatively lower effectiveness in dementia detection is likely due to the low-quality audio, which makes one of the models less powerful.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Effect Of Multi-View Pseudo-Labeling Components",
      "text": "We explore the contributions of each audio encoder and LLM in multi-view pseudo-labeling by keeping either the acoustic or linguistic path unchanged (i.e., full path) while adding an individual encoder from the other path. For brevity, Table  IV  presents the results of early fusion with 30% labeled data omitting the other conditions. The results show that 1) The acoustic path contributes more than the linguistic path. 2) CLAP and Falcon perform best among the acoustic and linguistic encoders, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this work, we propose a novel semi-supervised learning framework that introduces a multi-view pseudo-labeling method leveraging both acoustic and linguistic characteristics. This method utilizes Fréchet audio distance and large language models to select the most reliable unlabeled data for augmenting the training set. Multiple fusion techniques have been compared to utilize multi-view knowledge for further enhancement of the framework. We evaluate our method on emotion recognition and dementia detection tasks, demonstrating that it outperforms fully-supervised, limited-supervised, and two SSL baselines. Our method achieves competitive performance compared to fully supervised learning while using less than 30% of human-labeled data. In future work, we plan to explore the effects of additional audio encoders and large language models on multi-view pseudolabeling and investigate more efficient fusion methods for the bimodal classifier.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed SSL framework with multi-",
      "page": 1
    },
    {
      "caption": "Figure 1: Framework of our proposed semi-supervised speech classification with pseudo-labeling and task-specific knowledge.",
      "page": 2
    },
    {
      "caption": "Figure 1: We first apply Parameter-Efficient Fine-Tune (PEFT) on the fol-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Angry": "4.12\n35.33\n54.66\n45.46\n34.64",
          "Happy": "3.98\n42.56\n58.49\n182.65\n71.42",
          "Neutral": "6.87\n57.24\n88.78\n141.75\n73.41",
          "Sad": "12.20\n89.65\n109.02\n230.39\n110.57"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Label propagation-based semi-supervised learning for hate speech classification",
      "authors": [
        "Ashwin Geet D'sa",
        "Irina Illina",
        "Dominique Fohr",
        "Dietrich Klakow",
        "Dana Ruiter"
      ],
      "year": "2020",
      "venue": "Insights from Negative Results Workshop"
    },
    {
      "citation_id": "2",
      "title": "Semi-supervised learning helps in sound event classification",
      "authors": [
        "Zixing Zhang",
        "Björn Schuller"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Co-training succeeds in computational paralinguistics",
      "authors": [
        "Zixing Zhang",
        "Jun Deng",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using semisupervised learning with efficient labeling strategies",
      "authors": [
        "Zhi Zhu",
        "Yoshinao Sato"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "5",
      "title": "Semi-supervised audio classification with consistency-based regularization",
      "authors": [
        "Kangkang Lu",
        "Chuan-Sheng Foo",
        "Kah Kuan Teh",
        "Huy Dat Tran",
        "Vijay Chandrasekhar"
      ],
      "year": "2019",
      "venue": "Semi-supervised audio classification with consistency-based regularization"
    },
    {
      "citation_id": "6",
      "title": "Semi-fedser: Semi-supervised learning for speech emotion recognition on federated learning using multiview pseudo-labeling",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2022",
      "venue": "Semi-fedser: Semi-supervised learning for speech emotion recognition on federated learning using multiview pseudo-labeling"
    },
    {
      "citation_id": "7",
      "title": "Censer: Curriculum semi-supervised learning for speech recognition based on self-supervised pre-training",
      "authors": [
        "Bowen Zhang",
        "Songjun Cao",
        "Xiaoming Zhang",
        "Yike Zhang",
        "Long Ma",
        "Takahiro Shinozaki"
      ],
      "year": "2022",
      "venue": "Censer: Curriculum semi-supervised learning for speech recognition based on self-supervised pre-training"
    },
    {
      "citation_id": "8",
      "title": "Semi-supervised spoken language understanding via selfsupervised speech and language model pretraining",
      "authors": [
        "Cheng-I Lai",
        "Yung-Sung Chuang",
        "Hung-Yi Lee",
        "Shang-Wen Li",
        "James Glass"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Selftraining with noisy student improves imagenet classification",
      "authors": [
        "Qizhe Xie",
        "Minh-Thang Luong",
        "Eduard Hovy",
        "Quoc V Le"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Rethinking emotion bias in music via frechet audio distance",
      "authors": [
        "Yuanchao Li",
        "Azalea Gui",
        "Dimitra Emmanouilidou",
        "Hannes Gamper"
      ],
      "year": "2024",
      "venue": "Rethinking emotion bias in music via frechet audio distance",
      "arxiv": "arXiv:2409.15545"
    },
    {
      "citation_id": "11",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "High fidelity neural audio compression",
      "authors": [
        "Alexandre Défossez",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Yossi Adi"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "13",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Natural language supervision for general-purpose audio representations",
      "authors": [
        "Benjamin Elizalde",
        "Soham Deshmukh",
        "Huaming Wang"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms",
      "authors": [
        "Dominik Roblek",
        "Kevin Kilgour",
        "Matt Sharifi",
        "Mauricio Zuluaga"
      ],
      "year": "2019",
      "venue": "Fréchet audio distance: A reference-free metric for evaluating music enhancement algorithms"
    },
    {
      "citation_id": "16",
      "title": "Adapting frechet audio distance for generative music evaluation",
      "authors": [
        "Azalea Gui",
        "Hannes Gamper",
        "Sebastian Braun",
        "Dimitra Emmanouilidou"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Revisiting acoustic similarity in emotional speech and music via self-supervised representations",
      "authors": [
        "Yujia Sun",
        "Zeyu Zhao",
        "Korin Richmond",
        "Yuanchao Li"
      ],
      "year": "2024",
      "venue": "Revisiting acoustic similarity in emotional speech and music via self-supervised representations",
      "arxiv": "arXiv:2409.17899"
    },
    {
      "citation_id": "18",
      "title": "Revise, reason, and recognize: Llm-based emotion recognition via emotion-specific prompts and asr error correction",
      "authors": [
        "Yuanchao Li",
        "Yuan Gong",
        "Chao-Han Huck",
        "Peter Yang",
        "Catherine Bell",
        "Lai"
      ],
      "year": "2024",
      "venue": "Revise, reason, and recognize: Llm-based emotion recognition via emotion-specific prompts and asr error correction",
      "arxiv": "arXiv:2409.15551"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition with ASR transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "20",
      "title": "Generative speech recognition error correction with large language models and task-activating prompting",
      "authors": [
        "Chao-Han Huck",
        "Yile Yang",
        "Yi-Chieh Gu",
        "Shalini Liu",
        "Ivan Ghosh",
        "Andreas Bulyko",
        "Stolcke"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "21",
      "title": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "authors": [
        "Chao-Han Huck",
        "Taejin Yang",
        "Yuan Park",
        "Yuanchao Gong",
        "Zhehuai Li",
        "Yen-Ting Chen",
        "Chen Lin",
        "Yuchen Chen",
        "Kunal Hu",
        "Piotr Dhawan",
        "Żelasko"
      ],
      "year": "2024",
      "venue": "Large language model based generative error correction: A challenge and baselines for speech recognition, speaker tagging, and emotion recognition",
      "arxiv": "arXiv:2409.09785"
    },
    {
      "citation_id": "22",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "23",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "24",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Liu"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "25",
      "title": "Fusing ASR outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "27",
      "title": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "authors": [
        "Yaoting Wang",
        "Yuanchao Li",
        "Paul Liang",
        "Louis-Philippe Morency",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2023",
      "venue": "Cross-attention is not enough: Incongruity-aware dynamic hierarchical fusion for multimodal affect recognition",
      "arxiv": "arXiv:2305.13583"
    },
    {
      "citation_id": "28",
      "title": "Semi-supervised learning literature survey",
      "authors": [
        "Jerry Xiaojin",
        "Zhu"
      ],
      "year": "2005",
      "venue": "Semi-supervised learning literature survey"
    },
    {
      "citation_id": "29",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "30",
      "title": "Detecting cognitive decline using speech only: The adresso challenge",
      "authors": [
        "Saturnino Luz",
        "Fasih Haider",
        "Sofia De La Fuente",
        "Davida Fromm",
        "Brian Macwhinney"
      ],
      "year": "2021",
      "venue": "Detecting cognitive decline using speech only: The adresso challenge"
    },
    {
      "citation_id": "31",
      "title": "Combining labeled and unlabeled data with co-training",
      "authors": [
        "Avrim Blum",
        "Tom Mitchell"
      ],
      "year": "1998",
      "venue": "Proceedings of the eleventh annual conference on Computational learning theory"
    },
    {
      "citation_id": "32",
      "title": "Leveraging unlabeled data for emotion recognition with enhanced collaborative semi-supervised learning",
      "authors": [
        "Zixing Zhang",
        "Jing Han",
        "Jun Deng",
        "Xinzhou Xu",
        "Fabien Ringeval",
        "Björn Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "33",
      "title": "Early versus late fusion in semantic video analysis",
      "authors": [
        "G Cees",
        "Marcel Snoek",
        "Arnold Worring",
        "Smeulders"
      ],
      "year": "2005",
      "venue": "Proceedings of the 13th annual ACM international conference on Multimedia"
    }
  ]
}