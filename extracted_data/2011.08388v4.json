{
  "paper_id": "2011.08388v4",
  "title": "Interpretable Image Emotion Recognition: A Domain Adaptation Approach Using Facial Expressions",
  "published": "2020-11-17T02:55:16Z",
  "authors": [
    "Puneet Kumar",
    "Balasubramanian Raman"
  ],
  "keywords": [
    "Interpretable AI",
    "Transfer Learning",
    "Domain Adaptation",
    "Image Emotion Recognition",
    "Discrepancy Loss"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a feature-based domain adaptation technique for identifying emotions in generic images, encompassing both facial and non-facial objects, as well as non-human components. This approach addresses the challenge of the limited availability of pre-trained models and well-annotated datasets for Image Emotion Recognition (IER). Initially, a deep-learning-based Facial Expression Recognition (FER) system is developed, classifying facial images into discrete emotion classes. Maintaining the same network architecture, this FER system is then adapted to recognize emotions in generic images through the application of discrepancy loss, enabling the model to effectively learn IER features while classifying emotions into categories such as 'happy,' 'sad,' 'hate,' and 'anger.' Additionally, a novel interpretability method, Divide and Conquer based Shap (DnCShap), is introduced to elucidate the visual features most relevant for emotion recognition. The proposed IER system demonstrated emotion classification accuracies of 61.86% for the IAPSa dataset, 62.47% for the ArtPhoto dataset, 70.78% for the FI dataset, and 59.72% for the EMOTIC dataset. The system effectively identifies the important visual features that lead to specific emotion classifications and also provides detailed embedding plots explaining the predictions, enhancing the understanding and trust in AI-driven emotion recognition systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Humans predominantly portray emotion-related information visually. The images and videos depicting various emotions may contain facial contents, human activities, different backgrounds, and non-human objects. There is a need to develop computational systems that can identify the emotional information expressed in them. These systems study people's emotional responses to visual information  [22]  and are used in animation, gaming, marketing, entertainment, graphics, and lie detection  [22, 27] . The semantic-level Image Emotion Recognition (IER) analysis  [47]  has been explored by the researchers; however, affective level analysis is more difficult  [27, 50, 60] . The recent progress of deep learning has caused an enormous performance boost for image recognition and object classification  [29, 47] . Deep learning-based approaches have been successfully used for Facial Expression Recognition (FER); however, emotion analysis in generic images is a very complex task as they may include human, nonfacial components, and non-human objects  [31] . The emotional expression in images is sometimes attributed to high-level visual features such as background, and facial structure, whereas, sometimes, low-level image features such as texture, edge, color, and shape determine the emotional portrayal in images.\n\nIn the context of emotion recognition in the visual domain, while Facial Expression Recognition (FER) has been extensively studied  [40, 53, 65] , its methodologies cannot be directly transferred to Image Emotion Recognition (IER) for generic images without adaptation. The absence of sufficient pre-trained models and well-annotated datasets for IER compared to FER presents significant challenges. To bridge this gap, our proposed system employs a domain adaptation strategy, a special case of transfer learning where the source and target domains share identical feature spaces but have different data distributions. This approach utilizes the advances in FER to enhance IER by adapting successful FER methodologies to broader emotional contexts in images, ensuring the system effectively learns IER features while maintaining robustness in FER applications. By maintaining the same architecture for both FER and IER and employing discrepancy loss, we align the target domain's distribution without altering the network architecture, allowing for seamless adaptation across both domains  [54, 56, 63] . This feature-based strategy enhances the model's ability to generalize across facial and generic images, which is inspired by the understanding that emotions, whether captured through facial expressions or generic images, share underlying affective properties that can be modeled similarly.\n\nThe proposed system classifies an image into discrete categories using an adapted FER model, constructed using VGG16, residual blocks, convolution, max pooling, and dense layers, trained with the Adam optimizer. This main FER model and the adapted IER model are trained simultaneously. The discrepancy loss minimizes differences between the models, enabling the adapted IER model to effectively learn the IER domain's distribution. The DnCShap interpretability approach, detailed later in Section 3.4.1, enhances this model by identifying the key features contributing most towards recognizing specific emotion classes. While deep learning methods have achieved good performance in IER, they often function as \"black-box\" solutions with limited transparency  [37] . In the context of IER, interpretability is particularly crucial because emotional cues can be subjective and multifaceted, making it essential for practitioners and end-users to understand how decisions are being made. This lack of interpretability poses challenges in real-world applications where accountability and user trust are paramount  [7, 35] . The proposed DnCShap interpretability approach addresses these concerns by highlighting the salient features that most strongly influence model predictions, thereby bridging the gap between high accuracy and user-centric transparency in IER systems. By revealing the internal decisionmaking pathways, DnCShap ensures that stakeholders can comprehend the rationale behind specific emotion classifications, ultimately enhancing trust and facilitating broader adoption in sensitive or safety-critical domains.\n\nThe proposed domain adaptation approach minimizes discrepancy loss, enabling an effective transfer of learned representations from FER to broader IER task. This is achieved by systematically aligning source (facial) and target (generic) distributions and constraining the output probabilities of these two tasks to be consistent through the use of a discrepancy loss. Compared to existing methods, which typically tackle face-only or highly constrained image data, the proposed system preserves robust features pertinent to emotion classification. Furthermore, the newly introduced DnC-Shap interpretability module provides explanations at both feature and layer-wise levels, detailing what parts of an image drive the model's decisions-an aspect rarely addressed in prior domain adaptation setups. By uniting robust domain adaptation with detailed interpretability, this results in an interpretable emotion recognition system, advancing the state-of-the-art in IER  [47, 60] .\n\nThe experimental results on the International Affective Picture System Subset a (IAPSa), ArtPhoto, Flicker & Instagram, and The Emotions In Context (EMOTIC) datasets demonstrate emotion recognition accuracies of 61.86%, 62.47%, 70.78% and 59.72% respectively, which significantly exceed those reported in previous studies, validating the effectiveness of our approach. The architecture of the proposed IER system and the corresponding domain adaptation strategy have been refined through these studies, highlighting the critical link between the identified gaps in existing models and our contributions to addressing these challenges. The key contributions of this paper are listed as follows.\n\n1. A novel IER system based on domain adaptation techniques has been proposed to classify generic images containing facial, non-facial, and non-human components into discrete emotion classes. 2. A domain adaptation strategy for simultaneous training of FER and IER models using discrepancy loss has been introduced. This strategy enables the FER models to adapt for IER and to learn the distribution characteristics of IER datasets effectively.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Dncshap Interpretability Technique Has Been Incorporated Into The Ier",
      "text": "framework, which identifies the input features that most significantly contribute to the recognition of specific emotional classes, enhancing the model's transparency and interpretability. 4. A novel strategy for interpreting the layer-by-layer learning process and predictions of the IER system has been developed, providing detailed insights into the neural network's decision-making pathways.\n\n5. The proposed system has been validated across diverse datasets, demonstrating superior accuracy and efficiency. Comparative analysis shows significant advancements over existing methods. The rest of the manuscript has been organized as follows. Section 2 provides an in-depth review of the existing literature and related research in the field. Section 3 delineates the architecture and methodology of the proposed system, elaborating on the innovative features and domain adaptation techniques employed. Section 4 covers the implementation details and presents the experimental results, demonstrating the proposed model's effectiveness. Section 5 concludes the manuscript, summarizing the key findings, discussing their implications, and suggesting future research directions to advance emotion recognition systems.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "This section reviews the state-of-the-art in FER and IER along with domain adaptation and interpretability for IER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Image Emotion Recognition",
      "text": "Facial expressions are essential for machines to understand human emotions. The domain of FER is robust, encompassing a variety of techniques including microexpression analysis, face localization, landmark points' analysis, face registration, shape feature analysis, face segmentation, and eye gaze prediction  [15, 40] . Contrarily, IER has started gaining attention more recently. Pioneering work by Kim et al.  [27]  developed a neural-based system that synergizes diverse emotional features from images. Additionally, Rao et al.  [60]  extended these concepts into hierarchical emotion recognition frameworks, allowing for a layered understanding of emotions. Recent explorations by Kumar et al.  [32] , Khare et al.  [25] , and Cirneanu et al.  [14]  further expand the scope of IER by integrating innovative methodologies that address both facial and non-facial components. These advancements align closely with the objectives of our research, which introduces an IER system based on domain adaptation designed to classify complex emotional content in images including facial, non-facial, and non-human elements. Our approach utilizes a domain adaptation strategy using discrepancy loss to enhance the FER model's adaptability for the IER task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Expression Recognition",
      "text": "The FER has significantly evolved from traditional manual feature extraction methods to advanced deep learning approaches  [6, 19, 53] . Traditional FER involves a multistep process starting with pre-processing, where techniques like histogram equalization are commonly applied to normalize lighting across images  [19] . This is followed by feature extraction using methods such as local binary patterns, which capture textural information  [12] , and Gabor wavelets, noted for their edge detection capabilities  [5] . These traditional techniques, however, often struggle with the variability of facial expressions within the same category. Transitioning to deep learning, these methods have revolutionized FER by enhancing the ability to handle intra-class variations effectively, without the need for manual intervention  [75] .  Khorrami et al. demonstrated  this with their work on a deep network with zero bias, which showed improved performance on the Cohn-Kanade (CK) dataset  [26] . Further advancements include Aneja et al.'s deep FER model that facilitates animated and human facial expression mapping  [3] , highlighting the versatility of deep learning models.\n\nMoreover, the field has seen innovations such as Castellano et al.'s real-time automatic FER system, which is particularly useful in dynamic environments such as during the COVID-19 pandemic  [10] . Malik et al. focused on enhancing FER in lowresolution images, crucial for surveillance contexts  [51] , and Chowdary et al. have worked on improving the nuances of interaction dynamics within human-computer interfaces  [13] . Additionally, Jain et al. have developed a hyperparameter-tuned deep learning model for autonomous vehicles, ensuring driver safety through effective emotion recognition  [21] . Vignesh et al. introduced a novel model using segmentation and VGG-19 architecture to achieve high accuracy in emotion detection  [77] , and Talaat has implemented a real-time system for children with autism, combining deep learning with IoT technologies for behavioural therapy  [73] .\n\nThese technological enhancements not only demonstrate significant strides in accuracy and real-time processing but also point to a gap in the understanding of the internal decision-making processes of FER models, as discussed by Pedro et al. and Singh et al.  [17, 71] . Our research aims to bridge this gap by enhancing the explainability and transparency of FER systems, making them more accessible and comprehensible to users.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Generic Image Emotion Recognition",
      "text": "The IER has evolved from relying solely on low-level features such as shape, colour, and edge  [18]  to incorporating mid-level features like composition and optical balance, which enhance the aesthetic interpretation of images  [22] . Pioneering works by Zhao et al.  [85]  and Machajdik et al.  [50]  have furthered the use of mid-level visual features and semantic visual content, respectively, to deepen emotional analysis. While IER methodologies are often divided into Dimensional Emotion Space (DES) approaches that analyze emotional states through arousal and valence  [27, 88] , and Categorical Emotion States (CES) approaches that classify emotions into discrete classes like happiness, sadness, anger, and hate  [60, 84, 85] , our research enhances the CES framework by identifying complex emotions in images containing a mix of facial, non-facial, and non-human components.\n\nTraditional IER approaches generally utilize a feature-based semantic analysis, often limited by the manual crafting of low-level features, which fails to capture the full semantic potential of images  [18] . Although advances by Zhao et al.  [85]  and Machajdik et al.  [50]  have expanded the use of mid-level features, there remains a gap in fully integrating all semantic layers effectively. Our work addresses these challenges by implementing a domain adaptation strategy that significantly improves the system's ability to interpret a broad spectrum of visual features cohesively and accurately.\n\nMoreover, the advent of deep learning has revolutionized IER by enabling the extraction and analysis of features without manual intervention through end-to-end methodologies  [61, 82, 85] . These approaches, utilizing Convolutional Neural Network (CNN)-based transfer learning, adapt pre-trained models to new IER tasks efficiently. However, they often struggle with the precise extraction and utilization of both low and mid-level features essential for accurate emotion recognition  [60] , compounded by the scarcity of large-scale, well-annotated IER datasets and the inherent subjectivity of emotion perception. Our system overcomes these hurdles by incorporating a novel discrepancy loss function within our domain adaptation framework, ensuring that both FER and IER models effectively learn from and enhance each other, improving their performance across diverse datasets and complex emotional scenarios.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Domain Adaptation For Image Emotion Recognition",
      "text": "Domain Adaptation (DA), a specialized form of Transfer Learning, is crucial for enhancing the versatility and accuracy of emotion recognition systems across diverse domains. It specifically addresses the challenge of applying models trained on the source domain to effectively perform on the target domains. Advanced techniques such as deep cross-domain transfer facilitate emotion recognition via joint learning  [54] , and multisource marginal distribution adaptation has been particularly effective in crosssubject and cross-session EEG emotion recognition  [11] . Additionally, multi-view domain adaptive representation learning has shown promise in EEG-based emotion recognition  [38] . Strategies such as spectral  [81] , multi-domain  [39] , and adversarial domain adaptation  [8, 45]  push the boundaries of adapting emotion recognition models to new challenges.\n\nHistorically, emotion recognition has relied heavily on facial images  [53] , with systems often utilizing attention models to enhance FER  [17] . Domain adaptation in this field encompasses various approaches including 'feature-based adaptation' where features are transformed or augmented to bridge the gap between source and target domains  [41, 64] , 'instance-based adaptation' which focuses on selecting relevant training samples from the source domain  [68] , and 'model-based adaptation' strategies that involve modifying the model architecture or training strategy, such as using adversarial techniques or discrepancy loss  [63] .\n\nRecent works have extended domain adaptation into cross-subject and multi-modal directions specifically targeting emotion recognition. Lin et al.  [44]  introduced a multisource domain adaptation framework for visual sentiment classification, demonstrating how fusing knowledge across multiple labeled domains can significantly enhance transferability. Zhao et al.  [83]  proposed a \"plug-and-play\" approach to domain adaptation for EEG-based emotion recognition, highlighting the modular reusability of adaptation blocks across different subjects. Similarly, Ahn et al.  [1]  investigated crosscorpus speech emotion recognition through few-shot learning and domain adaptation, underscoring the interplay between minimal supervision and acoustic domain shifts. Meanwhile, Zheng et al.  [89]  tackled source-free domain adaptation challenges in visual emotion recognition with a fuzzy-aware loss, effectively mitigating noisy pseudo-label issues in the target domain. Collectively, these advances reinforce the necessity of robust, noise-tolerant adaptation methods in diverse affective computing scenarios.\n\nWe employ a feature-based domain adaptation approach, focusing directly on visual modalities to preserve the integrity of emotional data, thereby avoiding the information loss associated with methods like those using image captions for domain adaptation  [36] . This approach is exemplified by the FaceNet architecture, which learns mappings from facial images into representational spaces based on face-similarity measures  [65] . However, these FER models often falter when applied to non-facial images, necessitating adaptations for broader IER applications. Our system adapts pretrained FER models to effectively recognize emotions in generic non-facial images using this feature-based domain adaptation strategy. Employing discrepancy loss to align the feature distributions of FER and IER models without altering the network architecture ensures optimal transfer and adaptation of learned features across domains  [63] . This method addresses the challenge of varied emotional cues in different image contexts and is capable of handling diverse datasets. It not only enhances the accuracy and applicability of our system but also interprets a wide range of visual features, significantly improving the robustness of emotion recognition.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Interpretability And Explainability",
      "text": "Interpretability and Explainability are increasingly vital in the field of AI and machine learning, particularly within the domain of emotion recognition  [35, 37] . These concepts are essential for building trust in AI systems, as they allow users to understand and validate the decision-making processes of the models  [7] . Interpretable emotion analysis has become a significant trend, with research expanding across both visual  [57, 72]  and other modalities  [33, 42] , underscoring the need for systems that users can understand and interact with confidently.\n\nThe process of making a deep learning-based classifier's operations transparent involves elucidating the mechanisms that lead to specific outputs. Ribeiro et al.  [62]  introduced an influential approach by developing an algorithm that explains predictions by approximating a classifier locally with an interpretable model and identifying the parts of the input most responsible for the output. Similarly, Shrikumar et al.  [69]  presented a technique to decompose the predictions of a neural network by backpropagating the contributions of each neuron to understand their impact on the final decision. These methodologies, while groundbreaking, often fall short of revealing how the neural network weights are trained and adjusted during the learning process, a gap that remains largely unaddressed in current literature.\n\nWith the increasing complexity of emotion recognition systems, the need for clarity in how decisions are made is crucial. Recent efforts by Kang et al.  [24]  have introduced a privacy-preserving adversarial domain adaptation approach, which not only protects sensitive information but also clarifies feature group processing, linking domain adaptation with clear, actionable insights. Asokan et al.  [4]  utilize concept activation vectors to illuminate the contributions of multimodal emotion cues, like facial expressions and vocal tones, to decision-making processes within models. Furthermore, Malik et al.  [51]  have enhanced the transparency of facial emotion recognition systems by localizing and accentuating critical facial regions, significantly boosting user trust and understanding of automated affective assessments.\n\nOur research builds on these foundational works by proposing a system that not only explains emotion recognition outcomes but also provides insights into the training dynamics of network weights. This is crucial for the accurate interpretation of subtle emotional cues, which is essential in effective human-computer interaction. By enhancing transparency and interpretability, we aim to create more reliable AI tools for emotion analysis  [34] . Our findings show that integrating real-time data interpretation and dynamic adjustment of network weights not only meets but exceeds theoretical expectations and outperforms traditional models in initial tests. These results underline the practical benefits of our approach in effectively addressing gaps in existing models.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Methodology",
      "text": "This section proposes a novel approach for transferring the FER model to the IER task using a feature-based domain adaptation strategy. We maintain the same architecture for both FER and IER and employ discrepancy loss to effectively align the target domain's distribution, thereby enhancing the model's capability to learn IER features.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Problem Formulation",
      "text": "The target task T i is defined as IER for feature space X ′ and target domain D i , where D i ⊂ X ′ . The source domain D f and feature space X, where D f ⊂ X, are designated for FER, denoted as T f . The task T i in D i utilizes the information from T f and D f . Given the different distribution of their data points, i.e., images, D i undergoes a statistical adaptation to align with the source domain's feature space X. According to Pan et al.  [56] , this adaptation process involving probabilistic transformations is consistent with Domain Adaptation as both D f and D i perform the same task, share the same feature space, but have different marginal distributions of data points.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Facial Expression Recognition",
      "text": "Fig.  1  illustrates the architecture of the proposed FER system, which has been constructed on top of a pre-trained VGG16 network  [70]  by incorporating four residual layers. Its structure comprises five blocks, B k where k ∈ {1, 2, . . . , 5}. Block B1 includes two convolutional layers with 64 channels, B2 has two convolutional layers with 128 channels, while B3 and B4 contain three convolutional layers with 256 and 512 channels, respectively. B5 is configured identically to B4. Each of these blocks employs filters of size 3 × 3 followed by a max-pool layer with a stride of 2, 2.\n\nThe residual module has been included to mitigate the issues of exploding and vanishing gradients commonly associated with deep neural networks  [20] . It links four residual layers, R l where l ∈ {1, 2, 3, 4}, that process the outputs of blocks B1, B2, B3, and B4 in parallel. R1 comprises four max-pooling layers and one convolutional layer with 512 channels, R2 includes three max-pooling layers and one convolutional layer with 512 channels, R3 and R4 incorporate two and one max-pooling layers, respectively, each followed by a single convolutional layer. The outputs from these individual residual layers (R1 to R4) are then concatenated into a single tensor, which is further concatenated with the output from B5. This concatenated output is then\n\nAnger Disgust",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Fear Happy Neutral",
      "text": "Sad Surprise",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Fer Module",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Residual Module",
      "text": "Interpretability Module\n\nInputs from FER module's blocks\n\nFig.  1  The proposed FER system's architecture contains FER, residual, and interpretability modules. The input is processed through FER module's layers B1 to B5, followed by residual module's parallel branches R1 to R4. The interpretability module uses DnCShap to analyze pixel relevance for emotion recognition.\n\nflattened and passed through three dense layers of sizes  (1, 2048) ,  (1, 2048) , and\n\n. By concatenating outputs from multiple residual layers, the model integrates and preserves information across processing depths, enhancing feature extraction and generalization capabilities. This configuration allows each network section to learn features at varying levels of abstraction, improving its ability to handle complex tasks like emotion recognition from facial expressions. The interpretability module employs the Divide and Conquer based SHAP (DnC-Shap) method to attribute significance to individual pixels in contributing to the model's decision-making process. By decomposing the input image into its constituent features and assessing their impact, this module provides visual insights into which aspects of the image are most relevant for each recognized emotion. The proposed twofold interpretability approach for IER, encompassing both feature-wise and layer-wise analyses, is detailed in Section 3.4.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Image Emotion Recognition",
      "text": "Fig.  2  illustrates the architecture of the proposed IER system, which was refined through ablation studies. This system is aligned with the FER task as detailed in Section 3.2. Both the FER and IER models employ the same VGG16 architecture, which includes additional residual, convolutional, max pooling, and dense layers, ensuring consistency across tasks. The training process begins with the FER model using an Adam optimizer, and the IER model, mirroring the FER model's architecture, is trained in parallel. Discrepancy loss is minimized between the outputs of both models during training to ensure accurate alignment and effective emotion recognition across different types of images. Preprocessing: Prior to training, significant preprocessing was applied to both datasets to ensure consistency and reliability of the input data. Initially, all images were resized to a standard dimension of 224x224 pixels and normalized to have zero mean and unit variance to mitigate any model biases due to variations in image scale and illumination. To address discrepancies in labeling conventions across datasets, a mapping scheme was devised that aligns the emotion labels from different sources into a unified set of categories. Furthermore, given the inherent class imbalance in the datasets, a combination of oversampling the minority classes and undersampling the majority classes was employed to ensure an equitable representation of all classes in the training process. These preprocessing steps are crucial as they significantly influence the model's ability to generalize across diverse inputs and directly impact the performance outcomes discussed in Section 4.4.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Domain Adaptation Strategy:",
      "text": "We employ a feature-based domain adaptation strategy using the same network architecture for both FER and IER tasks. This approach utilizes discrepancy loss to effectively align the target domain's distribution with that of the source. The discrepancy loss, which is minimized after each training epoch, helps the adapted IER model accurately learn the IER domain's distribution, despite the differing marginal distributions of data points between the FER and IER domains. This alignment is critical for the model's ability to perform consistently across both domains. The overall loss function is detailed in Eq. 1.\n\nwhere L overall , L classif ier , and L discrepancy are overall loss, cross-entropy loss and discrepancy loss. The third term is the regularization term, where ω and λ are the weights of the fully connected layers and the regularization weight, which are calculated experimentally by observing the variance in the output. The computation of classifier loss and discrepancy loss has been explained as follows. The cross-entropy loss has been used to find the output probability of the main FER model and the adapted IER model. For an image sample v, distribution p 1 in the FER domain and distribution p 2 in the IER domain, classifier loss is calculated using Eq. 2.\n\nwhere v is an image sample whereas p 1 and p 2 denote the actual and predicted distributions of v, respectively. Further, the absolute difference between the two classifiers' probabilistic outputs has been utilized as discrepancy loss. For the image samples' distribution for the FER classifier, p 1 and the predicted distribution for the IER classifier, p 2 , the discrepancy loss is calculated as per Eq. 3.\n\nwhere o 1k and o 2k denote the probabilistic outputs for the distributions p 1 and p 2 , and K is the number of classes. This discrepancy loss is key to ensuring that the adapted model effectively learns the unique distribution of the IER domain, enhancing its predictive accuracy and generalizability across diverse datasets. This strategy significantly improves the adaptability and effectiveness of the existing FER systems for the broader and more complex task of IER, demonstrating the model's enhanced capability to interpret emotional cues in a wide array of image contexts.\n\nTheoretical Underpinnings of Discrepancy Loss: Discrepancy loss plays a critical role in our domain adaptation framework by quantitatively assessing and minimizing the disparity between feature distributions of the source (FER) and target (IER) domains. It measures the divergence between the probability distributions of features extracted from the FER and IER domains, specifically at the last hidden layer of the network. As incorporated in the overall loss function (Eq 1), L discrepancy is instrumental in aligning these domains. The alignment facilitated by discrepancy loss is not merely a statistical alignment of distributions but a strategic enforcement of feature space homogenization. This process reduces the model's sensitivity to the domain-specific nuances of emotional expression, which are prevalent when transitioning from FER to IER. By minimizing the discrepancy loss, we effectively guide the model to find a common feature space, where both domains' statistical properties are not just aligned but are functionally indistinguishable in terms of their contribution to the model's output. This ensures that the learned representations are robust, maintaining high accuracy and consistency across varied emotional datasets and contexts. Moreover, the use of discrepancy loss aids in overcoming overfitting to the domain-specific traits of the training data by promoting domain-invariant feature learning. This is critical for applications in real-world scenarios where the diversity of emotional expressions and the contexts in which they are presented can vary significantly from the conditions seen during training. By ensuring that our model learns to generalize across these domains effectively, we enhance its usability and reliability in practical deployments where domain variability is the norm.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Impact Of Discrepancy Loss Function:",
      "text": "The role of the discrepancy loss in our domain adaptation strategy is multifaceted. It is instrumental in training the model to reduce the divergence in output probabilities between the FER and IER domains for similar emotional inputs, facilitating a smoother transition from FER to IER tasks. Additionally, it allows the model to generalize more effectively across varied image contexts where emotional expressions differ significantly from the training data. This enhanced adaptability is crucial for deploying the model in real-world applications where it must perform reliably across different scenarios and datasets.\n\nIn the proposed domain adaptation strategy, each component is strategically crafted to optimize the model's performance in the new IER domain. The cross-entropy loss ensures that the fundamental task of emotion recognition is maintained by effectively training the model to minimize the difference between the actual and predicted emotion classifications in the FER domain. This forms the baseline upon which the domain adaptation builds. The discrepancy loss, on the other hand, is critical for aligning the model to the new IER domain. It specifically addresses the challenge of different marginal distributions between the FER and IER domains. By minimizing this loss, the model is trained to reduce the divergence in output probabilities between the domains for similar emotional inputs. This not only facilitates a smoother transition of the model from FER to IER tasks but also enhances the model's ability to generalize across varied image contexts where emotional expressions differ significantly from the training data.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Interpretability",
      "text": "A two-fold interpretability technique has been proposed to explain the working of the proposed method. It first visualizes the important features of the input image responsible for emotion classification. Then, implementing the proposed layer-wise explainability technique results in the intersection matrices and cluster distances.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Feature-Wise Interpretability",
      "text": "This section introduces DnCShap, which integrates the Divide and Conquer approach into the SHAP method to expedite Shapley values' computation. While the exact Shapley value calculation is NP-hard  [67] , SHAP approximates it in quadratic time. DnCShap further reduces the time complexity to linear by dividing the feature set into manageable subsets, computing Shapley values independently, and aggregating them efficiently.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Mathematical Proof Of Efficiency In Dncshap:",
      "text": "The Divide and Conquer method implemented in DnCShap divides the feature set into smaller, manageable subsets, upon which Shapley values are independently computed and then aggregated. This division reduces the computational complexity from quadratic to linear time as follows:\n\nGiven a feature set F , it is divided into subsets F 1 and F 2 . For a feature f in F 1 , Shapley values are calculated using Eq. 4.\n\nThe same process is applied to F 2 and other subsets. As depicted in Eq. 5, the final Shapley value for f is approximated by averaging these individual contributions.\n\nThis method ensures that each subset computation can be handled independently, dramatically reducing the computational burden and enabling parallel processing.\n\nCalculating Shapley values: The Shapely values' computation has been demonstrated with an example shown in Fig.  3 . The Node1 has no feature, Node4 has two features (f 1 and f 2 ) and the rest of the nodes contain one feature each (i.e., f 1 and f 2 , respectively). The marginal contribution of a feature differentiates the predictions of two nodes connected by an edge. The feature f 1 's marginal contribution for label c for the model with f 1 feature only is computed as per Eq. 6.\n\nM C f1,{f1} = score {f1} -score {ϕ}  (6)  where score {f1} is the prediction for label c using the model with feature f 1 . The overall contribution is computed as per Eq. 7.\n\nwhere w 12 and w 34 are the coefficient weights. The coefficient weight, w i is computed using Eq. 8.\n\nwhere |S| denotes no. of features and F shows the no. of weights. The values of w 12 and w 34 are calculated as per Eq. 8 and Eq. 9 computes the Shapely values. For image X with width w and height h, it is divided into two parts x 1 , x 2 where width is w/2 and height is h. The two parts are considered as different features, and Eq 7 is used to find their Shapley values. The divide and conquer is continuously applied, and Shapely values are found until the image is divided into a pre-specified (hyperparameter) number of parts. The time complexity is of the order of T (n) = 2T (n/2) + O(1), i.e., of the order of O(n), which comes out to be linear time complexity for n number of features.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Layer-Wise Interpretability",
      "text": "This section proposes a novel technique to explain the predictions of the proposed deep learning-based system. We have defined the term Intersection Score, which depicts the correctness of network weights layer by layer. It first visualizes the important features of the input image responsible for emotion classification. The convergence of the emotion clusters is then visualized for various network layers. The intersection score is calculated using the following steps. a) For each emotion class i, calculate the principle components (x i , y i , z i ) of p th last layer's embedding. For each emotion class i and all three components (x i , y i , z i ), mean m i and standard deviation σ mi are calculated using Eq. 10, where n i is the number of data points for i th emotion-class and k is the k th data point.\n\nTo get the principle components of a layer's embeddings, first, the embedding vector is extracted and flattened. Then the principal component analysis (PCA) procedure is applied, which involves computing eigenvectors; sorting them by decreasing eigenvalues and choosing k eigenvectors with the largest eigenvalues and using this d × k eigenvector matrix for transformation where d dimensions.\n\nb) For each emotion class i, the range for every principle component is defined using the above parameters with an assumption that most of the values will be within two times the standard deviations of the mean. Range for i th emotion is defined as\n\nwhere L i (m) and R i (m) are the left and right extreme points of m th component's spread for emotion class i. The extreme points are defined in Eq. 11. The choice of using the double of the standard deviation value to define the range is done under the assumption that the distribution has a symmetric tail on both sides.\n\nc) The intersection between emotion classes i and j is calculated as per Eq. 12 and denoted as I i,j (m). Here, four emotion classes, i.e., angry, happy, sad, and neutral, are considered.\n\nHere, I i,j (m) denotes the intersection between the spread of m th component's data for emotion classes i and j.\n\nd) The total intersection between two emotion classes i and j is denoted as I i,j . As shown in Eq. 13, it is calculated as the product of all component-wise intersections between emotion classes i and j. The values for I i,j are in the range [0,1]. It has a maximum value of one when i = j, i.e. when the spread of m th component's data is the same for two emotion classes. Here, I i,j denotes the total intersection between the emotion classes i and j. I i,j = I i,j (x) * I i,j (y) * I i,j (z)",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Comparison Of Interpretability Approaches",
      "text": "The DnCShap technique compares with other interpretability techniques such as SHAP  [49] , LIME  [62] , and Grad-CAM  [66]  as follows.\n\nComputational Efficiency: DnCShap enhances computational efficiency by incorporating a Divide & Conquer strategy to compute Shapley values, reducing the time complexity from exponential, as is typical with traditional SHAP, to linear. Our method computes approximate Shapley values in linear time, compared to the quadratic or higher time complexity of SHAP and LIME.\n\nInterpretability Depth: Unlike Grad-CAM, which provides a coarse localization map indicative of relevant regions in the image, DnCShap offers detailed attributions at a pixel level, enabling finer interpretations of the contributions of specific image features Interpretable Image Emotion Recognition using Domain Adaptation to the emotion recognition task. This depth is particularly crucial in understanding subtle emotional cues in facial expressions.\n\nApplicability to FER: The ability of DnCShap to provide detailed and computationally efficient interpretations makes it particularly suited for real-time FER systems, where understanding the decision-making process of the model is as important as its accuracy. The observations, presented in Table  1 , demonstrate that DnCShap not only outperforms existing methods in terms of computational efficiency but also provides deeper insights, making it an ideal choice for advanced FER applications where both speed and interpretability are crucial.\n\nComputational Efficiency and Scalability: DnCShap enhances computational efficiency by incorporating a Divide & Conquer strategy, which significantly reduces the computational overhead associated with calculating Shapley values. Typically, computing exact Shapley values in traditional SHAP has exponential time complexity. In contrast, our method manages to approximate these values in linear time, which is particularly advantageous when scaling to more complex models or larger datasets. To demonstrate the scalability and runtime efficiency, we conducted benchmark tests across datasets of varying sizes and complexity, which show that DnCShap maintains efficient performance without compromising interpretability, even as dataset size increases. These tests help substantiate DnCShap's practical utility in real-world applications where both computational resources and time are limiting factors.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Experimental Results",
      "text": "This section presents the implementation setup and experimental results along with ablation studies and a discussion of the important observations.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Datasets",
      "text": "The datasets utilized in this study are characterized by their large scale and diversity, encompassing a broad range of emotions and scenarios. These qualities make them exceptionally well-suited for the demands of both facial and image emotion recognition.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Fer Datasets:",
      "text": "The FER datasets used in this study are widely recognized in the research community for their robust and varied annotations of facial expressions, which are critical for the development and evaluation of FER systems.\n\n• FER13  [9] : Includes 35,887 emotion-labeled images, utilized extensively for its diversity in facial expressions and occlusions, facilitating robust model training. • FERG  [3] : Comprises 55,767 annotated images known for their animated representations, which help in understanding the subtleties of facial expressions in animated contexts. • JAFFE  [23] : Contains 213 images, often used for its high-quality depictions of basic emotional expressions by Japanese female models. • CK+  [48] : Features 927 images with labeled emotions, widely used for its sequential imaging that captures the onset and apex of facial expressions.\n\nIER Datasets: These datasets are crucial for advancing research in image emotion recognition by providing a diverse range of contexts and settings.\n\n• IAPSa dataset  [52] : Comprising 395 images, this dataset is pivotal for its role in psychophysiological studies and is frequently used to validate emotion recognition models due to its reliable emotional annotations.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Emotion Class Maping And Hyperparameter Setting",
      "text": "",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Emotion Class Mapping",
      "text": "In this work, we consolidate all emotion annotations to four discrete classes, i.e., happy, sad, hate, and anger. We chose these four because they are the most frequently used categories in state-of-the-art Image Emotion Recognition (IER) studies involving the same benchmark datasets: IAPSa, ArtPhoto, Flicker&Instagram (FI), and EMOTIC. As discussed in Section 4.4.2, many prior methods on these datasets also focus on these four dominant categories for consistency and reliable sample sizes.\n\nRationale for Merging: Although some datasets originally contain up to seven labeled emotions (e.g., amusement, contentment, disgust, fear, surprise, etc.), not all of these are well-represented or consistently annotated across the four datasets. For instance, IAPSa and ArtPhoto include additional labels like amusement or contentment, but each has relatively few samples. Likewise, disgust is labeled in some, but not all.\n\nTo avoid excessive class imbalance and enhance cross-dataset alignment, we adopt the following merges aligned with Plutchik's emotion wheel  [59] : contentment and amusement are merged to happy whereas disgust is merged to hate. In this manner, each of the four datasets has sufficient examples in the four selected emotion categories, facilitating robust training and evaluation.\n\nImpact on Model Performance: By consolidating underrepresented labels into these four commonly used categories, we mitigate the risk of overfitting to sparsely populated classes and reduce confusion between closely related affective states (e.g., disgust vs. hate). Empirically, we observe more stable training curves and higher overall accuracy compared to attempts at using six or seven fine-grained categories.\n\nIn particular, merging disgust into hate and grouping contentment/amusement with happy provides clearer class boundaries, resulting in better cross-dataset generalization. While we focus on these four discrete classes for the present work, the underlying domain adaptation framework could be extended to additional discrete labels if future datasets offer consistent and balanced annotations.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Training Strategy And Hyperparameter Setting",
      "text": "The model training is implemented in PyTorch, utilizing a training-testing data split of 80% and 20%, 5-fold cross-validation, with a standard batch size of 16, and a learning rate ranging from 8 × 10 -5 to 8 × 10 -4 for up to 100 epochs. The FER and IER models were initially trained using cross-entropy loss and subsequently together using the discrepancy loss, with the Adam optimizer utilized for both networks. Accuracy, defined as the proportion of true results (both true positives and true negatives) among the total number of cases examined, has been used as the evaluation metric.\n\nHyperparameter Tuning: We employed Ray Tune  [43]  for hyperparameter optimization, allowing for a systematic and efficient search through the hyperparameter space. This approach facilitated the adjustment of learning rates dynamically, refinement of batch sizes to balance computational efficiency and model accuracy, and tuning of Adam optimizer parameters to optimize training dynamics. Different hyperparameter settings, such as beta values and epsilon for the Adam optimizer, were methodically varied to explore their impacts on the loss oscillations and the stability of model updates. Extended training durations beyond the standard 100 epochs with early stopping based on validation loss were also tested to prevent overfitting while ensuring robust model performance. Inspired by other advanced hyperparameter optimization tools such as Optuna  [2]  and Keras Tuner  [55] , which are used extensively in various machine learning platforms, we incorporated similar strategies within our Ray Tune implementation to fine-tune the balance between cross-entropy and discrepancy loss. These modifications aided in refining the model's capability to learn generalized features while effectively minimizing domain-specific discrepancies.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Models",
      "text": "The following baselines and proposed system's architectures have been decided according to the ablation studies discussed in Section 4.4.7.\n\n• Baseline#1: It trains AlexNet  [30]  simultaneously for FER and IER.\n\n• Baseline#2: The VGG16  [70]  based IER and FER models are implemented and trained using discrepancy loss.\n\n• Baseline#3: It trains ResNet  [20]  models simultaneously for FER and IER.\n\n• Baseline#4: It generates the captions for the given images and trains the TER model for emotion recognition. The captions' emotion labels are considered the images' labels, as there is a one-to-one mapping between them.\n\n• Baseline#5: It adapts a pre-trained FER model, Deep Emotion  [53]  and re-trains it simultaneously for FER and IER using the discrepancy loss.\n\n• Baseline#6: uses EfficientNet  [74] . It is a scalable architecture that aims to achieve superior efficiency through compound scaling of depth, width, and resolution.\n\n• Baseline#7: It is based on the Vision Transformer (ViT)  [16]  model that applies attention mechanisms to segment the images into patches and processes them sequentially.\n\n• Baseline#8 uses Swin Transformer  [46]  which is a hierarchical transformer whose representation is computed with shifted windows, facilitating efficient modeling of image data at multiple scales to handle complex visual tasks.\n\n• Proposed system: The proposed IER system adapts the FER model proposed in section 3.2 using the proposed domain adaptation scheme.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Results",
      "text": "This section discusses the IER results along with the interpretable feature maps.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Accuracy And Confusion Matrices",
      "text": "The proposed IER system shows 61.86%, 62.47%, 70.78%, and 59.72% accuracies for IAPSa, ArtPhoto, FI, and EMOTIC datasets, respectively. Fig.  4  shows the corresponding confusion matrices.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Comparison With Existing Methods And Baselines",
      "text": "Table  2  compares the results of the proposed IER system with the existing methods and baseline models. Several research works utilize diverse evaluation metrics including Mean Average Precision (mAP), Area Under the Curve (AUC), and accuracy percentages. For consistency, this analysis focuses on studies that use accuracy as their evaluation metric, ensuring a uniform standard for comparing results across various IER methods and datasets. These datasets predominantly feature four common emotion classes ('happy,' 'anger,' 'hate,' and 'sad'), aligning with the typical categorizations found in existing IER research.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Feature-Wise Interpretability",
      "text": "Fig.  5  demonstrates the feature maps for various emotion classes. The maps highlighted in red illustrate the most significant visual features that contribute to emotion recognition. These maps are instrumental in identifying both facial and non-facial cues that are pivotal for accurately classifying emotions. For images containing human faces, the feature maps effectively capture distinct facial features such as eyebrows, eyes, and mouth, which are crucial for recognizing emotions like happiness or anger.\n\nIn contrast, for images without human faces, the feature maps help in pinpointing other relevant areas, such as body posture or contextual elements within the scene, that are significant in conveying the underlying emotion of the image. This detailed visualization aids in understanding how different components of an image contribute to emotion recognition, providing valuable insights into the robustness and adaptability of the IER system in handling diverse types of images.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Layer-Wise Interpretability",
      "text": "Fig.  6  illustrates the embeddings from the trained IER model, projected onto a threedimensional hyperplane. The principal components of the embeddings are spread along the x, y, and z axes, representing distinct dimensions of the data's variance for various emotion classes. This visualization highlights how different layers of the model capture varying levels of abstraction in the data. Notably, as we progress from the intermediate to the final layers of the network, the separation between the embeddings of different emotion classes becomes more pronounced. This enhanced separation indicates that the deeper layers of the network are more effective at distinguishing between complex emotional states, suggesting that these layers capture more discriminative features essential for accurate emotion classification. This layer-wise analysis provides valuable insights into the model's ability to generalize and its interpretability, demonstrating how different network depths contribute to the overall decision-making process. We observe that smaller datasets (IAPSa and ArtPhoto) require only a few seconds per epoch, whereas larger datasets (FI and EMOTIC, each containing over 20,000 images) demand correspondingly more time. Despite the discrepancy-loss domain adaptation overhead, it remains only a modest fraction (∼20-40%) of baseline training. DnCShap analysis takes a few seconds at most, owing to its linear-time complexity. PCA computations are also minimal. Overall, even with additional domain adaptation and interpretability steps, our method remains computationally tractable for practical use, completing training in a reasonable time (on the order of minutes per epoch) for the smaller datasets and under a minute per epoch for the larger FI and EMOTIC datasets.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Computational Efficiency Analysis",
      "text": "",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Human Annotation Study",
      "text": "To evaluate how well our model's interpretations align with human perception of nonfacial or generic images, we conducted a small-scale annotation study on a subset of each dataset (IAPSa, ArtPhoto, FI, and EMOTIC). We randomly selected 100 images per dataset that primarily depict scenes, objects, or contexts without clear human faces.\n\nAnnotation Procedure: Three volunteers (postgraduate students with familiarity in affective computing) independently labeled each image with one of the four emotions (happy, sad, hate, anger) and indicated the visual cues driving their label choice. They received no information about model outputs.\n\nInter-Annotator Agreement: We used Cohen's Kappa (κ)  [76]  to quantify pairwise agreement. Table  4  shows the average κ across annotator pairs. The moderate agreement for FI and EMOTIC underscores the subjective nature of emotional cues in real-world or context-heavy scenes.\n\nComparison with Model: We compared the model's predicted labels to the majority vote among annotators for each image. In 78-83% of cases, the model matched the humans' dominant label. Additionally, DnCShap-highlighted regions overlapped with human-indicated cues (e.g., color or foreground objects) in 70% of instances. Overall, this study indicates that our model's predictions and saliency outputs provide a reasonable correspondence with human judgments in generic, non-facial images. However, the moderate agreement levels among annotators also highlight the inherent subjectivity in emotional perception. Future extensions could expand the subset size and include a broader pool of annotators or alternative emotion categories.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Ablation Studies",
      "text": "This section discusses the experimental studies performed to determine the appropriate domain adaptation approach. a) Inter-Modality Domain Adaptation: This trial transforms the visual information into textual and performs the emotion recognition task. This transformation has been performed using image captions generated using an attention-based captioning system  [78] . Following this, a BERT based pre-trained Text Emotion Recognition (TER) model has been re-trained using these captions, and TER has been performed. The predicted TER labels have been considered the IER labels because the images and captions have one-to-one relations as they verbalize the same information visually and textually. The adapted TER model showed 33.89% accuracy on being tested without re-training on image captions. On the other hand, re-training it with image captions demonstrated 53.07% accuracy. The above-described approach has been implemented in Baseline#4. b) Intra-Modality Adaptation: The datasets and pre-trained models for visual object recognition and FER tasks are more abundant. The following approaches have been considered to leverage them for IER.\n\n• The pre-trained object recognition models such as AlexNet, ResNet, and VGG16 were used for IER without re-training. However, it resulted in poor performance of around 30% emotion recognition accuracy. • The pre-trained FER models were trained further and used for recognizing emotions in images using transfer learning. Taking inspiration from the existing feature transformation-based domain adaptation approaches  [64] , we have mixed a fraction of IER data with FER data while training the FER model proposed in section 3.2. It enables the FER model to learn the distribution of IER data gradually. Table  5  shows the IER performance on mixing a fraction of IER data along with FER data to train the FER models and test them for IER data. It has been observed that mixing 40-80% IER data while training the FER model resulted in the best possible IER accuracies by this approach. However, there is a scope to improve it further. • This approach trains a network for FER and IER simultaneously and minimizes the loss between them during the training. Further, the following networks have been considered while formulating the baseline models and proposed system -AlexNet (Baseline#1), VGG16 (Baseline#2), ResNet (Baseline#3), DeepEmotion  [53]  (Baseline#5), and the FER model proposed in section 3.2 (Proposed Method). Table  6  summarizes their respective performances. The rationale for choosing DeepEmotion is its state-of-the-art performance for FERG, JAFFE and CK+ datasets  [51] . The individual FER datasets and their combination (denoted as 'All') have been used during the implementation. In most cases, the experiments combining all FER datasets have performed better than the individual FER datasets. Overall, simultaneous training of the FER model proposed in section 3.2 on FER and IER datasets resulted in better performance. The proposed system implements it.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Discussion",
      "text": "As more pre-trained models and labelled large-scale datasets are available for FER, we have proposed a domain adaptation-based system to use them for IER. Our domain adaptation approach utilizes a feature-based strategy, retraining the same architecture for IER as was developed for FER, coupled with discrepancy loss. This method ensures the effective transfer of learned behaviours from FER to IER, even when applied to small-scale IER datasets. By maintaining the same architecture, we facilitate a seamless domain adaptation process, optimizing the model's performance through a nuanced understanding of both facial and non-facial emotion indicators. The use of discrepancy loss helps in aligning the target domain's distribution with the source, further enhancing the model's efficacy. Accuracy has been adopted as the primary performance measure for its direct representation of the proportion of true results within the total number of cases examined. This metric has been specifically chosen because it provides a clear and understandable measure of model performance across varied datasets. In the presentation of our results in Table  2 , we have included only those existing works for which accuracy values are available. While many studies employ different metrics such as mAP or AUC, focusing on accuracy allows for a consistent and straightforward comparison across different approaches.\n\nThe proposed interpretability approach, DnCShap, has effectively highlighted important features within images that contribute to emotion recognition. Observations from Fig.  5  show that while the feature maps for the 'happy' and 'anger' classes are more precise, those for the 'sad' and 'hate' classes appear more scattered. This variance provides valuable insights into how different emotions are processed and recognized by the system, suggesting areas for further fine-tuning. Moreover, extending interpretability to other modalities may yield additional improvements and insights, enhancing the system's overall robustness and applicability. An intriguing aspect of the results presented in Fig.  4  is the variability in how similar emotions are recognized across different datasets. This could be attributed to the inherent differences in dataset composition and the contexts within which the images were captured. Each dataset may capture unique emotional expressions influenced by cultural nuances and the specific circumstances of image capture, affecting the generalizability of the results. Further, the dependency of performance on specific datasets, as evidenced in Table  6 , underscores the impact of each dataset's unique characteristics on emotion recognition efficacy. Datasets with artistic images might encapsulate subtler emotional expressions compared to those with more expressive images from controlled settings. Understanding these characteristics is crucial for refining the training and adaptation processes of IER systems.\n\nGeneralizability Across Datasets: Our proposed method has demonstrated robust performance across standard datasets used in facial emotion recognition. However, the scalability and generalizability of the system to other datasets, especially those that might contain more complex or abstract emotional cues, warrant further exploration. The architecture and methodologies employed in our system are designed to be adaptable and flexible, allowing for potential extensions to datasets encompassing a broader range of emotional expressions and cultural variations. For example, the inclusion of domain adaptation techniques such as discrepancy loss ensures that our model can learn domain-invariant features, which is crucial for applications involving diverse emotional datasets. Future work will focus on testing the model's performance on datasets like the Emotionet, which includes a wide array of subtle and complex emotional expressions, often captured in less controlled environments. Additionally, we plan to explore the integration of multimodal data sources, such as audio and textual information, to enrich the emotional context and enhance the model's interpretability and adaptability to real-world scenarios where emotional cues are not solely visual.\n\nFailure Case Analysis: Despite achieving competitive accuracy across multiple datasets, our model exhibits certain failure cases that warrant further analysis. A key challenge arises in the misclassification of visually similar emotions, such as confusion between fear and surprise or sadness and neutral expressions, particularly in the FER domain. In the IER setting, errors frequently occur in images where emotional cues are highly context-dependent rather than explicitly conveyed through facial expressions. Additionally, domain adaptation limitations contribute to errors when transferring knowledge from FER to IER, especially when the dataset distributions differ significantly. Low-quality images, occlusions, and dataset biases further affect the model's performance, leading to occasional inconsistencies in predictions. To mitigate these issues, potential improvements include incorporating uncertainty-aware learning, improving dataset balance, and leveraging multi-modal cues beyond visual information to enhance emotion recognition reliability. This analysis highlights the importance of refining adaptation strategies and dataset curation to address these failure cases effectively.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper has introduced a novel feature-based domain adaptation strategy that effectively uses discrepancy loss to align the IER domain with the FER domain, employing the same network architecture for both tasks. Our approach not only demonstrates robust emotion recognition across diverse image contexts but also achieves notable accuracy metrics across multiple datasets. The integration of the DnCShap interpretability method enriches our understanding of key visual features influencing emotion recognition, enhancing transparency and trust in AI-driven systems. Looking ahead, the method's potential applicability in areas lacking large-scale datasets invites further exploration, particularly in real-world scenarios where emotional contexts are highly variable. Future work should focus on refining the domain adaptation techniques to meet the nuanced demands of these applications and expanding the interpretability framework to include more complex emotional scenarios and diverse datasets. Additionally, exploring the integration of multimodal data, such as audio and textual cues, could provide a more comprehensive understanding of emotional states and further improve the accuracy and robustness of emotion recognition systems. This ongoing research will aim to enhance the model's scalability, adaptability, and efficiency, moving toward more sophisticated and universally applicable emotion recognition systems.",
      "page_start": 29,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the architecture of the proposed FER system, which has been con-",
      "page": 8
    },
    {
      "caption": "Figure 1: The proposed FER system’s architecture contains FER, residual, and interpretability modules. The",
      "page": 9
    },
    {
      "caption": "Figure 2: illustrates the architecture of the proposed IER system, which was refined",
      "page": 9
    },
    {
      "caption": "Figure 2: Architecture of the proposed IER system, adapted from the pre-trained FER model. The convolutional",
      "page": 10
    },
    {
      "caption": "Figure 3: The Node1 has no feature, Node4 has two features (f1",
      "page": 13
    },
    {
      "caption": "Figure 3: Illustration of Shapley value calculation. Nodes (Node1 to Node4) represent different configurations",
      "page": 13
    },
    {
      "caption": "Figure 4: shows the",
      "page": 19
    },
    {
      "caption": "Figure 5: demonstrates the feature maps for various emotion classes. The maps high-",
      "page": 19
    },
    {
      "caption": "Figure 4: Confusion matrices for IAPSa, ArtPhoto, FI, and EMOTIC datasets.",
      "page": 20
    },
    {
      "caption": "Figure 6: illustrates the embeddings from the trained IER model, projected onto a three-",
      "page": 20
    },
    {
      "caption": "Figure 5: Feature-wise Interpretability for various emotion classes. Areas marked in red highlight the most",
      "page": 22
    },
    {
      "caption": "Figure 6: Layer-wise Interpretability for various IER datasets, projecting the embeddings from the trained",
      "page": 23
    },
    {
      "caption": "Figure 5: show that while the feature maps for the ‘happy’ and ‘anger’ classes",
      "page": 26
    },
    {
      "caption": "Figure 4: is the variability in how sim-",
      "page": 28
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1/n": "1/n"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cross-Corpus Speech Emotion Recognition based on Few-Shot Learning and Domain Adaptation",
      "authors": [
        "Youngdo Ahn",
        "Sung Lee",
        "Jong Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "2",
      "title": "Optuna: A Next-Generation Hyperparameter Optimization Framework",
      "authors": [
        "Takuya Akiba",
        "Shotaro Sano",
        "Toshihiko Yanase",
        "Takeru Ohta",
        "Masanori Koyama"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "3",
      "title": "Modeling Stylized Character Expressions via Deep Learning",
      "authors": [
        "Deepali Aneja",
        "Alex Colburn",
        "Gary Faigin",
        "Linda Shapiro",
        "Barbara Mones"
      ],
      "year": "2016",
      "venue": "The Asian Conference on Computer Vision (ACCV)"
    },
    {
      "citation_id": "4",
      "title": "Interpretability for multimodal emotion recognition using concept activation vectors",
      "authors": [
        "Ashish Ramayee Asokan",
        "Nidarshan Kumar",
        "Anirudh Ragam",
        "Shylaja"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "5",
      "title": "Recognizing Facial Expression: Machine Learning and Application to Spontaneous Behavior",
      "authors": [
        "Marian Stewart",
        "Gwen Littlewort",
        "Mark Frank",
        "Claudia Lainscsek",
        "Ian Fasel",
        "Javier Movellan"
      ],
      "year": "2005",
      "venue": "The IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "6",
      "title": "POSER: POsed vs Spontaneous Emotion Recognition using fractal encoding",
      "authors": [
        "Carmen Bisogni",
        "Lucia Cascone",
        "Michele Nappi",
        "Chiara Pero"
      ],
      "year": "2024",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "7",
      "title": "Psychological Foundations of Explainability and Interpretability in Artificial Intelligence",
      "authors": [
        "A David",
        "Broniatowski"
      ],
      "venue": "Psychological Foundations of Explainability and Interpretability in Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Exploiting Adaptive Adversarial Transfer Network for Cross Domain Teacher's Speech Emotion Recognition",
      "authors": [
        "Ting Cai",
        "Shengsong Wang",
        "Yu Xiong",
        "Xin Zhong"
      ],
      "year": "2023",
      "venue": "International Conference on Computer Science and Education"
    },
    {
      "citation_id": "9",
      "title": "FER-2013 Face Database",
      "authors": [
        "Pierre-Luc Carrier",
        "Aaron Courville",
        "Ian Goodfellow",
        "Medhi Mirza",
        "Yoshua Bengio"
      ],
      "year": "2013",
      "venue": "FER-2013 Face Database"
    },
    {
      "citation_id": "10",
      "title": "Automatic facial emotion recognition at the covid-19 pandemic time",
      "authors": [
        "Giovanna Castellano",
        "Berardina De Carolis",
        "Nicola Macchiarulo"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "11",
      "title": "MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Ming Hao Chen",
        "Zhunan Jin",
        "Cunhang Li",
        "Jinpeng Fan",
        "Huiguang Li",
        "He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "12",
      "title": "Facial Expression Recognition based on Facial Components Detection and HOG Features",
      "authors": [
        "Junkai Chen",
        "Zenghai Chen",
        "Zheru Chi",
        "Hong Fu"
      ],
      "year": "2014",
      "venue": "The International Workshop on Electrical and Computer Engineering Subfields"
    },
    {
      "citation_id": "13",
      "title": "Deep Learning Based Facial Emotion Recognition for Human-Computer Interaction Applications",
      "authors": [
        "Chowdary Kalpana",
        "Tu Nguyen",
        "Jude Hemanth"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "14",
      "title": "New Trends in Emotion Recognition Using Image Analysis by Neural Networks, A Systematic Review",
      "authors": [
        "Andrada-Livia Cîrneanu",
        "Dan Popescu",
        "Iordache Dragos"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-related Applications",
      "authors": [
        "Adrian Ciprian",
        "Marc Corneanu",
        "Jeffrey Oliu Simón",
        "Sergio Cohn",
        "Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
      "citation_id": "16",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy"
      ],
      "year": "2020",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "17",
      "title": "FERAtt: Facial Expression Recognition with Attention Net",
      "authors": [
        "Pedro D Marrero Fernandez",
        "Fidel Guerrero Peña",
        "Tsang Ing Ren",
        "Alexandre Cunha"
      ],
      "year": "2019",
      "venue": "The IEEE/CVF International Conference on Computer Vision and Pattern Recognition-workshops (CVPRw)"
    },
    {
      "citation_id": "18",
      "title": "Extracting Moods from Pictures and Sounds: Towards Personalized TV",
      "authors": [
        "Alan Hanjalic"
      ],
      "year": "2006",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "19",
      "title": "Automatic Facial Expression Recognition Using Features of Salient Facial Patches",
      "authors": [
        "Happy Sl",
        "Aurobinda Routray"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing (TAFFC)"
    },
    {
      "citation_id": "20",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "21",
      "title": "An Automated Hyperparameter Tuned Deep Learning Model Enabled Facial Emotion Recognition for Autonomous Vehicle Drivers",
      "authors": [
        "Deepak Kumar Jain",
        "Ashit Kumar Dutta",
        "Elena Verdú",
        "Shtwai Alsubai",
        "Abdul Rahaman Wahab Sait"
      ],
      "year": "2023",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "22",
      "title": "Aesthetics and Emotions in Images",
      "authors": [
        "Dhiraj Joshi",
        "Ritendra Datta",
        "Elena Fedorovskaya",
        "Quang-Tuan Luong",
        "James Wang",
        "Jia Li",
        "Jiebo Luo"
      ],
      "year": "2011",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "23",
      "title": "The Japanese Female Facial Expression (JAFFE) Database",
      "authors": [
        "Miyuki Kamachi",
        "Michael Lyons",
        "Jiro Gyoba"
      ],
      "year": "1998",
      "venue": "The Japanese Female Facial Expression (JAFFE) Database"
    },
    {
      "citation_id": "24",
      "title": "Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability",
      "authors": [
        "Yan Kang",
        "Yuanqin He",
        "Jiahuan Luo",
        "Tao Fan",
        "Yang Liu",
        "Qiang Yang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Big Data"
    },
    {
      "citation_id": "25",
      "title": "Emotion Recognition and Artificial Intelligence: A Systematic Review (2014-2023) and Research Recommendations",
      "authors": [
        "Smith K Khare"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "26",
      "title": "Do Deep Neural Networks Learn Facial Action Units when Doing Expression Recognition?",
      "authors": [
        "Pooya Khorrami",
        "Thomas Paine",
        "Thomas Huang"
      ],
      "year": "2015",
      "venue": "The IEEE/CVF International Conference on Computer Vision-workshop (ICCVw)"
    },
    {
      "citation_id": "27",
      "title": "Building Emotional Machines: Recognizing Image Emotions through Deep Neural Networks",
      "authors": [
        "Hye-Rin Kim",
        "Yeong-Seok Kim",
        "Seon Joo Kim",
        "In-Kwon Lee"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia (TMM)"
    },
    {
      "citation_id": "28",
      "title": "Context based Emotion Recognition using EMOTIC Dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
      "citation_id": "29",
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems (NeurIPS)"
    },
    {
      "citation_id": "30",
      "title": "ImageNet Classification with Deep Convolutional Neural Networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2017",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "31",
      "title": "Affective Feedback Synthesis Towards Multimodal Text and Image Data",
      "authors": [
        "Puneet Kumar",
        "Gaurav Bhatt",
        "Omkar Ingle",
        "Daksh Goyal",
        "Balasubramanian Raman"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "32",
      "title": "Hybrid Fusion Based Approach For Multimodal Emotion Recognition With Insufficient Labeled Data",
      "authors": [
        "Puneet Kumar",
        "Vedanti Khokher",
        "Yukti Gupta",
        "Balasubramanian Raman"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "33",
      "title": "Interpretable Multimodal Emotion Recognition Using Facial Features And Physiological Signals",
      "authors": [
        "Puneet Kumar",
        "Xiaobai Li"
      ],
      "year": "2023",
      "venue": "Interpretable Multimodal Emotion Recognition Using Facial Features And Physiological Signals",
      "arxiv": "arXiv:2306.02845"
    },
    {
      "citation_id": "34",
      "title": "Interpretable multimodal emotion recognition using hybrid fusion of speech and image data",
      "authors": [
        "Puneet Kumar",
        "Sarthak Malik",
        "Balasubramanian Raman"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "35",
      "title": "Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data",
      "authors": [
        "Puneet Kumar",
        "Sarthak Malik",
        "Balasubramanian Raman",
        "Xiaobai Li"
      ],
      "year": "2024",
      "venue": "Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image Data",
      "arxiv": "arXiv:2402.07640"
    },
    {
      "citation_id": "36",
      "title": "Domain Adaptation Based Technique For Image Emotion Recognition Using Image Captions",
      "authors": [
        "Puneet Kumar",
        "Balasubramanian Raman"
      ],
      "year": "2020",
      "venue": "Computer Vision and Image Processing: 5th International Conference"
    },
    {
      "citation_id": "37",
      "title": "Measuring Non-Typical Emotions for Mental Health: A Survey of Computational Approaches",
      "authors": [
        "Puneet Kumar",
        "Alexander Vedernikov",
        "Xiaobai Li"
      ],
      "year": "2024",
      "venue": "Measuring Non-Typical Emotions for Mental Health: A Survey of Computational Approaches",
      "arxiv": "arXiv:2403.08824"
    },
    {
      "citation_id": "38",
      "title": "Multi View Domain Adaptive Representation Learning for EEG-based Emotion Recognition",
      "authors": [
        "Chao Li",
        "Ning Bian",
        "Ziping Zhao",
        "Haishuai Wang",
        "Björn Schuller"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "39",
      "title": "MS-FTSCNN: An EEG Emotion Recognition Method from the Combination of Multi-Domain Features",
      "authors": [
        "Feifei Li"
      ],
      "year": "2024",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "40",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing (TAFFC)"
    },
    {
      "citation_id": "41",
      "title": "Learning With Augmented Features For Supervised And Semi-Supervised Heterogeneous Domain Adaptation",
      "authors": [
        "Wen Li",
        "Lixin Duan",
        "Dong Xu",
        "Ivor Tsang"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)"
    },
    {
      "citation_id": "42",
      "title": "An Explanation Framework and Method for AI-based Text Emotion Analysis and Visualisation",
      "authors": [
        "Yuming Li",
        "Johnny Chan",
        "Gabrielle Peko",
        "David Sundaram"
      ],
      "year": "2024",
      "venue": "Decision Support Systems"
    },
    {
      "citation_id": "43",
      "title": "Tune: A Research Platform for Distributed Model Selection and Training",
      "authors": [
        "Richard Liaw",
        "Eric Liang",
        "Robert Nishihara",
        "Philipp Moritz",
        "Joseph Gonzalez",
        "Ion Stoica"
      ],
      "year": "2018",
      "venue": "Tune: A Research Platform for Distributed Model Selection and Training",
      "arxiv": "arXiv:1807.05118"
    },
    {
      "citation_id": "44",
      "title": "Multi-source Domain Adaptation for Visual Sentiment Classification",
      "authors": [
        "Chuang Lin",
        "Sicheng Zhao",
        "Lei Meng",
        "Tat-Seng Chua"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "45",
      "title": "DA-CapsNet: A Multi-branch Capsule Network based on Adversarial Domain Adaption for Cross-Subject EEG Emotion Recognition",
      "authors": [
        "Shuaiqi Liu",
        "Zeyao Wang",
        "Yanling An",
        "Bing Li",
        "Xinrui Wang",
        "Yudong Zhang"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "46",
      "title": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "47",
      "title": "Fully Convolutional Networks for Semantic Segmentation",
      "authors": [
        "Jonathan Long",
        "Evan Shelhamer",
        "Trevor Darrell"
      ],
      "year": "2015",
      "venue": "IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "48",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A Complete Dataset for Action Unit and Emotion Specified Expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "The IEEE/CVF International Conference on Computer Vision and Pattern Recognition-workshops (CVPRw)"
    },
    {
      "citation_id": "49",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "M Scott",
        "Su-In Lundberg",
        "Lee"
      ],
      "year": "2017",
      "venue": "The 31st International Conference on Neural Information Processing Systems (NeuroIPS)"
    },
    {
      "citation_id": "50",
      "title": "Affective Image Classification using Features Inspired by Psychology and Art Theory",
      "authors": [
        "Jana Machajdik",
        "Allan Hanbury"
      ],
      "year": "2010",
      "venue": "18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Towards Interpretable Facial Emotion Recognition",
      "authors": [
        "Sarthak Malik",
        "Puneet Kumar",
        "Balasubramanian Raman"
      ],
      "year": "2021",
      "venue": "The 12th Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "52",
      "title": "Emotional Category Data on Images from the International Affective Picture System. Behavior research methods",
      "authors": [
        "Barbara Joseph A Mikels",
        "Fredrickson",
        "Casey Gregory R Larkin",
        "Sam Lindberg",
        "Patricia Maglio",
        "Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Emotional Category Data on Images from the International Affective Picture System. Behavior research methods"
    },
    {
      "citation_id": "53",
      "title": "Deep-Emotion: Facial Expression Recognition using Attentional Convolutional Network",
      "authors": [
        "Shervin Minaee",
        "Mehdi Minaei",
        "Amirali Abdolrashidi"
      ],
      "year": "2021",
      "venue": "MDPI Sensors"
    },
    {
      "citation_id": "54",
      "title": "Deep Cross-Domain Transfer for Emotion Recognition via Joint Learning",
      "authors": [
        "Dung Nguyen",
        "Thanh Duc",
        "Sridha Nguyen",
        "Mohamed Sridharan",
        "Simon Abdelrazek",
        "Son Denman",
        "Rui Tran",
        "Clinton Zeng",
        "Fookes"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "55",
      "title": "",
      "authors": [
        "O' Tom",
        "Elie Malley",
        "James Bursztein",
        "Long",
        "Haifeng Franc ¸ois Chollet",
        "Luca Jin",
        "Invernizzi"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "56",
      "title": "A Survey on Transfer Learning",
      "authors": [
        "Jialin Sinno",
        "Qiang Pan",
        "Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "57",
      "title": "Luc Van Gool, and Danda Paudel. A Unified and Interpretable Emotion Representation and Expression Generation",
      "authors": [
        "Reni Paskaleva",
        "Mykyta Holubakha",
        "Andela Ilic",
        "Saman Motamed"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "58",
      "title": "Affect-DML: Context-Aware One-Shot Recognition of Human Affect using Deep Metric LearningDML",
      "authors": [
        "Kunyu Peng",
        "Alina Roitberg",
        "David Schneider",
        "Marios Koulakis",
        "Kailun Yang",
        "Rainer Stiefelhagen"
      ],
      "year": "2021",
      "venue": "16th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "59",
      "title": "The Nature of Emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "Journal Storage (JSTOR) Digital Library's American scientist Journal"
    },
    {
      "citation_id": "60",
      "title": "Learning Multi-level Deep Representations for Image Emotion Classification",
      "authors": [
        "Tianrong Rao",
        "Xiaoxu Li",
        "Min Xu"
      ],
      "year": "2019",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "61",
      "title": "Multi-Scale Blocks based Image Emotion Classification using Multiple Instance Learning",
      "authors": [
        "Tianrong Rao",
        "Min Xu",
        "Huiying Liu",
        "Jinqiao Wang",
        "Ian Burnett"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "62",
      "title": "Explaining the Predictions of Any Classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD)"
    },
    {
      "citation_id": "63",
      "title": "Residual Parameter Transfer for Deep Domain Adaptation",
      "authors": [
        "Artem Rozantsev",
        "Pascal Mathieu Salzmann",
        "Fua"
      ],
      "year": "2018",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "64",
      "title": "Adapting Visual Category Models to New Domains",
      "authors": [
        "Kate Saenko",
        "Brian Kulis",
        "Mario Fritz",
        "Trevor Darrell"
      ],
      "year": "2010",
      "venue": "European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "65",
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "66",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Ramprasaath Selvaraju"
      ],
      "year": "2017",
      "venue": "The IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "67",
      "title": "A Value for n-Person Games, Contributions to the Theory of Games ii",
      "authors": [
        "Ls Shapley"
      ],
      "year": "1953",
      "venue": "A Value for n-Person Games, Contributions to the Theory of Games ii"
    },
    {
      "citation_id": "68",
      "title": "Generalized Domain-Adaptive Dictionaries",
      "authors": [
        "Sumit Shekhar",
        "M Vishal",
        "Hien Patel",
        "Rama Nguyen",
        "Chellappa"
      ],
      "year": "2013",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "69",
      "title": "Learning Important Features Through Propagating Activation Differences",
      "authors": [
        "Avanti Shrikumar",
        "Peyton Greenside",
        "Anshul Kundaje"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "70",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "71",
      "title": "Hierarchical Interpretations for Neural Network Predictions",
      "authors": [
        "Chandan Singh",
        "James Murdoch",
        "Bin Yu"
      ],
      "year": "2018",
      "venue": "The 6th International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "72",
      "title": "Emotional Video Captioning With Vision-based Emotion Interpretation Network",
      "authors": [
        "Peipei Song",
        "Dan Guo",
        "Xun Yang",
        "Shengeng Tang",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "73",
      "title": "Real-Time Facial Emotion Recognition System Among Children With Autism Based On Deep Learning And IoT",
      "authors": [
        "M Fatma",
        "Talaat"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "74",
      "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "75",
      "title": "EXPERTNet: Exigent Features Preservative Network for Facial Expression Recognition",
      "authors": [
        "Monu Verma",
        "Jaspreet Kaur Bhui",
        "Santosh Vipparthi",
        "Girdhari Singh"
      ],
      "year": "2018",
      "venue": "The 11th Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "76",
      "title": "Cohen's Kappa Coefficient as a Performance Measure for Feature Selection",
      "authors": [
        "Susana Vieira",
        "Uzay Kaymak",
        "João Mc Sousa"
      ],
      "year": "2010",
      "venue": "The 18th IEEE International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "77",
      "title": "A Novel Facial Emotion Recognition Model Using Segmentation VGG-19 Architecture",
      "authors": [
        "M Vignesh",
        "M Savithadevi",
        "Rajeswari Sridevi",
        "Sridhar"
      ],
      "year": "2023",
      "venue": "International Journal of Information Technology"
    },
    {
      "citation_id": "78",
      "title": "Attend and Tell: Neural Image Caption Generation with Visual Attention",
      "authors": [
        "Kelvin Xu",
        "Jimmy Ba",
        "Ryan Kiros",
        "Kyunghyun Cho",
        "Aaron Courville",
        "Ruslan Salakhudinov",
        "Rich Zemel",
        "Yoshua Bengio",
        "Show"
      ],
      "year": "2015",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "79",
      "title": "Deep cocktail network: Multi-source unsupervised domain adaptation with category shift",
      "authors": [
        "Ruijia Xu",
        "Ziliang Chen",
        "Wangmeng Zuo",
        "Junjie Yan",
        "Liang Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "80",
      "title": "Retrieving And Classifying Affective Images Via Deep Metric Learning",
      "authors": [
        "Jufeng Yang",
        "Dongyu She",
        "Yu-Kun Lai",
        "Ming-Hsuan Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "81",
      "title": "Spectral-Spatial Attention Alignment for Multi-Source Domain Adaptation in EEG-Based Emotion Recognition",
      "authors": [
        "Yi Yang",
        "Ze Wang",
        "Wei Tao",
        "Xucheng Liu",
        "Ziyu Jia",
        "Boyu Wang",
        "Feng Wan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "82",
      "title": "Building a Large Scale Dataset for Image Emotion Recognition: the Fine Print and the Benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "Association for the Advancement of Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "83",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "Li-Ming Zhao",
        "Xu Yan",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "84",
      "title": "Discrete Probability Distribution Prediction of Image Emotions with Shared Sparse Learning",
      "authors": [
        "Sicheng Zhao",
        "Guiguang Ding",
        "Yue Gao",
        "Xin Zhao",
        "Youbao Tang",
        "Jungong Han",
        "Hongxun Yao",
        "Qingming Huang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing (TAFFC)"
    },
    {
      "citation_id": "85",
      "title": "Exploring Principles-of-art Features for Image Emotion Recognition",
      "authors": [
        "Sicheng Zhao",
        "Yue Gao"
      ],
      "year": "2014",
      "venue": "22nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "86",
      "title": "CycleEmotionGAN: Emotional Semantic Consistency Preserved CycleGAN for Adapting Image Emotions",
      "authors": [
        "Sicheng Zhao",
        "Chuang Lin",
        "Pengfei Xu",
        "Sendong Zhao",
        "Yuchen Guo",
        "Ravi Krishna",
        "Guiguang Ding",
        "Kurt Keutzer"
      ],
      "year": "2019",
      "venue": "AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "87",
      "title": "Guiguang Ding, and Tat-Seng Chua. Predicting personalized image emotion perceptions in social networks",
      "authors": [
        "Sicheng Zhao",
        "Hongxun Yao",
        "Yue Gao"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "88",
      "title": "Continuous Probability Distribution of Image Emotions via Multitask Shared Sparse Regression",
      "authors": [
        "Sicheng Zhao",
        "Hongxun Yao",
        "Yue Gao",
        "Rongrong Ji",
        "G Ding"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Multimedia (TMM)"
    },
    {
      "citation_id": "89",
      "title": "Fuzzy-aware Loss for Source-free Domain Adaptation in Visual Emotion Recognition",
      "authors": [
        "Ying Zheng",
        "Yiyi Zhang",
        "Yi Wang",
        "Lap-Pui Chau"
      ],
      "year": "2025",
      "venue": "Fuzzy-aware Loss for Source-free Domain Adaptation in Visual Emotion Recognition",
      "arxiv": "arXiv:2501.15519"
    },
    {
      "citation_id": "90",
      "title": "Unpaired imageto-image translation using cycle-consistent adversarial networks",
      "authors": [
        "Jun-Yan Zhu",
        "Taesung Park",
        "Phillip Isola",
        "Alexei Efros"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision (ICCV)"
    }
  ]
}