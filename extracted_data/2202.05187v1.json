{
  "paper_id": "2202.05187v1",
  "title": "Adults As Augmentations For Children In Facial Emotion Recognition With Contrastive Learning",
  "published": "2022-02-10T17:43:11Z",
  "authors": [
    "Marco Virgolin",
    "Andrea De Lorenzo",
    "Tanja Alderliesten",
    "Peter A. N. Bosman"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in children can help the early identification of, and intervention on, psychological complications that arise in stressful situations such as cancer treatment. Though deep learning models are increasingly being adopted, data scarcity is often an issue in pediatric medicine, including for facial emotion recognition in children. In this paper, we study the application of data augmentation-based contrastive learning to overcome data scarcity in facial emotion recognition for children. We explore the idea of ignoring generational gaps, by adding abundantly available adult data to pediatric data, to learn better representations. We investigate different ways by which adult facial expression images can be used alongside those of children. In particular, we propose to explicitly incorporate within each mini-batch adult images as augmentations for children's. Out of 84 combinations of learning approaches and training set sizes, we find that supervised contrastive learning with the proposed training scheme performs best, reaching a test accuracy that typically surpasses the one of the second-best approach by 2% to 3%. Our results indicate that adult data can be considered to be a meaningful augmentation of pediatric data for the recognition of emotional facial expression in children, and open up the possibility for other applications of contrastive learning to improve pediatric care by complementing data of children with that of adults.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The course of vital treatments such as cancer therapy can expose the patient to high levels of stress and depression, in adults as well as in children  [Linden et al., 2012; Compas et al., 2014] . In particular for young children, whose ability to disclose inner feelings is still under development  [Sprung et al., 2015] , being able to recognize non-verbal emotion signals can be a critical factor for the monitoring of, and intervention on, their well-being. Here we consider the problem of automating emotion recognition from images of facial expressions of children.\n\nState-of-the-art methods to tackle facial emotion recognition are based on deep learning, which is notoriously datahungry: tens of thousands of images may be needed to obtain good accuracy on a given task  [Li and Deng, 2020] . Unfortunately, in many medical applications, the available data are limited. This problem is typically exacerbated in the case of pediatric data. Reasons for the lack of pediatric data include, e.g., certain pathologies being less likely to occur in younger people  [Li et al., 2008]  and regulations to protect children's privacy  [Tikkinen-Piri et al., 2018] . For emotion recognition from facial expressions, the relatively up-to-date repository \"Awesome-FER\"  [Fan, 2021]  lists only one children-specific data set out of the 20 reported, while the Wikipedia page \"facial expression databases\" 1  lists none out of the 19 reported.\n\nWhen only limited data are available, data augmentation can be of great help. Typically, data augmentation on images is carried out by randomly applying modifications such as cropping, flipping, and color jittering, which, crucially, do not change the semantics of the original image 2  [Shorten and  Khoshgoftaar, 2019] . In this paper, we study the effect of data augmentation for facial emotion recognition in children, and in particular whether introducing adult facial expressions as augmentations for child facial expressions is better than solely relying on repeated augmentations of child facial expressions. The hypotheses that motivate taking this approach are, first, that the extent of information that can be learned from inflating a dataset with classic augmentations has diminishing returns; and second, that facial features that convey an emotion are orthogonal to age, hence using data for emotion recognition of adults can be considered to be but another form of augmentation for data regarding children.\n\nTo conduct this study, we rely on contrastive learning, a recent class of methods that uses data augmentations to learn a latent representation where similar data points are close in some metric space and dissimilar ones are far (e.g., in terms of inner product). We particularly build upon Super-vised Contrastive learning (SupCon)  [Khosla et al., 2020] , by proposing a scheme where images of adults are explicitly paired with images of children that share the same emotion. For comparison, we include standard SupCon, the unsupervised approach Simple framework for Contrastive Learning of visual Representations (SimCLR)  [Chen et al., 2020] , and traditional supervised learning, with and without expanding the original child emotion data set with an adult one.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Contrastive learning concerns learning a function that maps data points that are similar (resp., dissimilar) into latent representations that are near (resp., far), in terms of a certain metric space (e.g., the Euclidean one). These latent representations can be useful to perform supervised learning tasks (if labels are available), as well as to reason about the data. There exist several approaches to contrastive learning, some dating back to the early the 2000s (e.g.,  [Chopra et al., 2005] ), which differ in how the training process works, what loss is used, and whether the approach is supervised (i.e., labels are used) or unsupervised (no labels are used, e.g., because they are unavailable)  [Jaiswal et al., 2021; Weng, 2021] . For example, the method proposed by  [Schroff et al., 2015]  for face identification proposed a triplet loss, which trains a neural network to minimize the L2-distance between an anchor (i.e., an image of a person) with its positives (i.e., other images of the same person) and maximize it with its negatives (i.e., images of other people), using labels to define what images portrait the same person. A different and label-free (i.e., unsupervised or self-supervised) approach can typically be taken when knowing how modifications of the original data that do not alter their semantics can be generated. A well-known unsupervised contrastive approach for images is SimCLR  [Chen et al., 2020] , whereby augmentations are used to generate different versions of a same image that carry the same meaning (e.g., by cropping, rotating, color jittering, and so on). The loss function of SimCLR is designed to learn latent representations such that the latent representations of augmented images that share a common origin are similar with respect to the metric space. SupCon  [Chen et al., 2020]  is similar to SimCLR but, besides using augmentations, also (requires and) uses the labels of the images, to learn better latent representations.\n\nIn this work, we focus on facial emotion recognition in children. While there exist many works in literature about facial emotion recognition in general  [Ko, 2018] , the number of studies that specifically focus on children are limited. Of these, recent works normally rely on deep convolutional neural networks, since these methods represent the state-of-theart in image classification in general  [Rawat and Wang, 2017; Anwar et al., 2018] ; and also on data sets such as CAFE (1192 images of 2 to 8 years old children)  [LoBue and Thrasher, 2015] , the Dartmouth database of children's faces  (640 images, 6-16 y.o.) [Dalrymple et al., 2013] , NIMH-ChEFS (482 images, 10-17 y.o.)  [Egger et al., 2011] , and EmoReact (1102 audio-visual clips, 4-14 y.o.)  [Nojavanasghari et al., 2016] . Since these data sets are relatively small for training convolutional neural networks, research works typically involve pre-training, e.g., on a larger data set of adults  [Lopez-Rincon, 2019] , or feature reduction, e.g., by using facial landmarks rather than the entire image as inputs for the network  [Rao et al., 2020] . To the best of our knowledge, this is the first paper that considers contrastive learning for facial emotion recognition in children.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "Fig.  1  summarizes, at high level, the contrastive learning methods considered here. Given a data set (e.g., of children's facial emotions), a neural network is trained with minibatches that contain augmentations of the original images, to learn to project images that are similar into respective latent representations that are close. A label may or may not be present and used by the contrastive learning method. Different from the traditional use of a single data set (e.g., as would be the case for images 1 and 2 in the mini-batch of Fig.  1 ), here we juxtapose a second data set (of adults) that shares images that have the same set of (possibly unknown) labels as the first data set (of children), and study how this can be exploited to improve the network's capability to learn a good latent representation.\n\nIn the next section, we describe SupCon and SimCLR in more detail. Next, in Sec. 3.2 we describe the approaches that we investigate to leverage adult facial expressions. Details on the adopted data sets and experimental setup are then provided in Sec. 3.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Contrastive Learning: Simclr And Supcon",
      "text": "We begin by describing the principles behind [Chen et al., 2020]'s SimCLR. SimCLR does not require labels, and is thus an unsupervised or self-supervised method. Let b be the size of a mini-batch. SimCLR operates by (i) randomly sampling b/2 images from the data set; (ii) generating two augmentations for each image; (iii) filling the mini-batch with such augmentations; and (iv) computing the loss:\n\n, where z ∈ R K is a latent representation that is produced by the network, I is the set of indices that identifies the augmentations in the mini-batch, i is the index of an augmentation, o(i) is the other augmentation for the same image for which the i th augmentation was made, s is a similarity function (here, cosine similarity), and τ is a hyper-parameter. Essentially, L SimCLR increases the similarity of latent representations generated for those elements of the mini-batch that are augmentations of the same image.\n\nOne can easily see that SimCLR relies on the mini-batch containing mostly (augmentations of) images with different (unknown) labels: if two images are retrieved from the data set that share the same meaning, their respective augmentations will be treated as dissimilar by L SimCLR . Differently from SimCLR,  [Khosla et al., 2020] 's SupCon assumes that labels are available and uses them to tackle this limitation, by:\n\n, where P (i) is the set of indices different from i that share the same label of the i-th augmentation. Note that, in principle, SupCon does not need to rely on augmentations, and could rely entirely on the images that are present in the data set. However,  [Khosla et al., 2020]  propose to use the same procedure that is used for SimCLR, i.e., to craft two augmentations of the same image to populate a mini-batch.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Considered Approaches",
      "text": "We intend to adopt two data sets, namely a data set A for which the latent representations are ultimately needed (here, for facial expressions of children); and a data set B that acts as a supplement of information (here, facial expressions of adults). We consider two approaches to use these data sets: 1. Simply extend data set A with images from data set B, de facto adopting the union of the two as a new data set. Each mini-batch will then contain augmentations originating from both data sets. We sample images from the two data sets with equal proportion. 2. Explicitly use augmentations of data points from data set B as if they were augmentations of points of data set A. Specifically, to populate a mini-batch, firstly we sample b/2 images from data set A; next, for each image from data set A, we insert in the mini-batch one augmentation of that image, and one augmentation of a random image from data set B, that shares the same label. The two approaches are depicted in Fig.  2 . On the one hand, the two approaches may seem similar because the amount of data points that are taken from data sets A and B are the same in expectation (fifty-fifty in both cases). On the other hand, the first approach results in mini-batches containing two augmentations of the same image, while the second does not. Consequently, one can expect that the amount of information within a mini-batch (informally, how much different the images will be on average) will be larger with the second approach than with the first. In other words, let x A i be the i th image from data set A, x B i be the i th image from data set B, and a(x, ξ) be the augmentation produced for image x using the collection of random variables ξ, which represents what random transformations are applied. Then, for a meaningful similarity function σ between images (e.g., one could attempt to measure the proximity of locality sensitive hashing systems such as Apple's NeuralHash [Apple Inc., 2021]), it is reasonable to expect that:\n\nSince with the second approach mini-batches contain images that are more dissimilar from one another, these mini-batches may carry a richer training signal than those obtained with the first approach.\n\nWe remark that the second approach cannot be used with SimCLR, as one needs to know the labels to pick images from data set B for the images of data set A.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "We set up our experiments according to the settings used by  [Khosla et al., 2020] , with adaptations for the task and data at hand. We begin by describing the data sets used and their pre-processing. Next, we provide details on the possible augmentations, followed by information on the network, the training process, and the validation steps.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Sets",
      "text": "We consider two data sets: the Child Affective Facial Expression (CAFE) data set  [LoBue and Thrasher, 2015] , and the Facial Expression Recognition challenge (FER) data set  [Goodfellow et al., 2013] . CAFE contains 1192 pictures of 2 to 8 years old-children, in color and portrait format, for the following emotions: anger, disgust, fear, happiness, neutral, sadness, surprise. FER contains 28 000 images for the same set of emotions, in gray scale and squared format. FER is mostly populated by images of adults, however some pictures of children are also present. By visual estimation of a random sample of 1050 images (150 per class), we estimate that approximately 15% of the images in FER represent children up to ten years old (some of whom are younger than the children represented in CAFE, i.e., < 2 y.o.).\n\nWe pre-process CAFE to have a same picture format of FER, i.e., the top and the bottom of the images is appropri-ately cut (based on visual inspection) to obtain a 1 : 1 widthheight ratio, resolution is set to 48 pixels per dimension, and images are converted to gray scale.\n\nThroughout the experiments, CAFE is used to train, validate, and test the considered approaches. We assess how the results change based on how much data are available for training, by using training splits of 50%, 70%, and 90%; the validation and test sets are created by partitioning the remaining data points in equal amounts. We use balanced training sets, assigning the number of data points for training by multiplying the training split percentage with the number of data points available in the minority class. We repeat each experiment ten times due to the stochastic nature of data splitting and neural training. For FER no splits are used, as it is only used to supply random images of adults during training.\n\nNeural architecture, training process, and augmentations Architecture The neural architecture used for contrastive learning (SupCon and SimCLR) is composed of two parts: an encoder network and a projection network. The encoder network takes an image x and produces a latent representation r ∈ R K . Like  [Khosla et al., 2020] , we use a ResNet  [He et al., 2016]  and in particular a gray-scale version of ResNet-18, with K = 512. The projection network is a multi-layer perceptron stacked on top of the encoder network, to map r to z ∈ R K , which is a smaller projection (i.e., K < K ). The contrastive loss is computed upon the representations z 1 , . . . , z b obtained by iteratively processing (by the encoder network and the projection network) the respective x 1 , . . . , x b , where b is the size of the mini-batches. We use K = 128 and b = 32. The projection network is discarded after constrastive training. To perform classification, a linear classifier is stacked on top of the encoder and trained with a cross-entropy loss. We refer to the work by  [Khosla et al., 2020]  for more details.\n\nTraining and validation settings Our experimental settings for training are essentially those used by  [Khosla et al., 2020] , adapted to the smaller size of the data sets at play here. In particular, we train for n epochs = 250, as this is abundantly sufficient to make the contrastive losses of SupCon and SimCLR converge in all our experiments 3  . For stochastic gradient descent, we set the initial learning rate to 0.05, momentum to 0.9, and learning rate decay to 0.1 at epochs n epochs -0.1 × n epochs × {3, 2, 1}. The linear layer is then trained for 50 epochs with similar settings (except for using an initial learning rate of 10 -4 ), which we found to be sufficient to for the loss to converge. During these epochs, we record the parameters of the network that lead to the best top-1 validation accuracy, and use those parameters for testing.\n\nWe also include a comparison with classic supervised learning training, where a network composed of the encoder and the linear classifier is trained directly with the crossentropy loss for n epochs = 250. Again, the best-found validation accuracy is tracked to determine which parameters to use for testing.\n\nAugmentations We follow  [Khosla et al., 2020]  and adopt cropping of up to 20% of the image; horizontal flipping with 0.5 probability; and color jittering of ±0.4 for brightness, contrast, saturation, and ±0.1 for hue, with 0.8 probability.\n\nWe formulate the use of augmentations in a different way than how typically done in literature. Normally, one or two augmentations are generated per image on the fly, to enter a mini-batch  [Khosla et al., 2020; Fort et al., 2021] . As a consequence, the longer the training process takes, the more information is fed to the network. Here, we re-frame this phenomenon in an equivalent setting whereby training has a fixed duration (of n epochs = 250), but the size of the training set varies based on how many augmentations calls are made. This allows us to directly think in terms of how much data is seen by the network with respect to the size of the training set. In the results below, we will use the term augmentation ratio to refer to the multiplicity by which the original training set is used to craft augmentations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "We present two types of results: accuracy (on validation and test sets), and a qualitative comparison using class activation maps. From now onwards, we use the following acronyms: SC for supervised contrastive learning, i.e., SupCon; UC for unsupervised contrastive learning, i.e., SimCLR; and SL for traditional supervised learning, i.e., the direct training of the encoder and linear classifier, without the intermediate contrastive training. Furthermore, we indicate whether the approach is trained on CAFE alone with the notation (c) for \"children\"; on CAFE extended with an equal number of random images from FER with (c+a) for \"children plus adults\"; and on CAFE using images from FER as augmentations for the images in CAFE with (c←a) for \"children augmented by adults\". The latter approach is only possible when using SC, as explained before in Sec. 3.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Child Emotion Recognition Accuracy",
      "text": "Figure  3  shows the validation and test accuracy for the considered learning approaches, for different sizes of the original training set, and augmentation ratios.\n\nThe general trend suggests that using a larger augmentation ratio leads to equal or better validation and test accuracy. This trend is more pronounced when the training split is small, and less when it is large. A first evident finding is that the UC approaches perform substantially worse than the others, meaning that the latent representations they learn are poor. This can be expected because the training data sets are relatively small (UC approaches are normally used on very large, unlabelled data sets) and the number of classes for the problem at hand is also small, hence L SimCLR often puts in contrast augmentations of different images of the same class.\n\nThe other approaches, i.e., the variants of SL and SC, perform similarly. In particular, no evident differences can be observed for SC when CAFE is used alone, i.e., SC (c), or in conjunction with FER, i.e., SC (c+a). For SL, (c+a) often leads to worse results than (c). Conversely, the performance obtained by SC (c←a) is notable, as this approach obtains the best distributions of validation and test accuracies  in most cases, except when the training split is large (90%).\n\nThere are two (non-mutually exclusive) explanations for this. First, with a training split of 90%, the training data may already be sufficient to learn a latent representation that is (almost) optimal. Second, as the validation and test sets are each composed of half the data left over from the training split, the recordings of validation and test accuracy may be noisy.\n\nLastly, for SC (c) and SC (c+a) with a training split of 90% and the larger augmentation ratios, the best-found validation accuracy drops, which indicates over-fitting of the contrastive training process.\n\nTable  1  shows results for the best and second-best approaches in terms of test accuracy, for a ×20 augmentation ratio and across the different training splits. SC (c←a) is always the best method, whereas the second-best varies. The Mann-Whitney-U test is used to report the p-values of the null hypothesis that the best approach is not significantly better than the second-best approach  [Mann and Whitney, 1947] . The improvements in test accuracy vary between 2 to 3%, and are significant with p-value < 0.1 for all cases, and with p-value < 0.05 when the training split is 50% and 70%.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Qualitative Analysis With Class Activation Maps",
      "text": "We provide additional insights by comparing the class activation maps that are obtained by requiring the networks to identify the areas of the image that prompt its prediction to provide the correct label. We consider SC (c←a) (the best performing approach that makes the best use of adult faces) and SC (c) (the respective approach that uses only children's Figure  4 : Class activation maps for three children's facial expression images. The more the color of a pixel is red (as opposed to blue), the more the pixel is deemed to be responsible for the activation of the true class (reported above each pair of rows). For a pair of rows, the top row shows the class activation maps obtained by the ten networks trained with SC (c←a), while the bottom row shows the ten networks trained with SC (c). When the prediction of a network is incorrect, the border of the corresponding image is red (instead of green), and the class activation map often appears to be unreasonable. faces) for a training split of 70% and augmentation ratio of 5 (an intermediate training configuration). Because the images of CAFE are protected by copyright, we use images of the authors and their family (similarly to previous figures). Note that these images can be considered to be out-ofdistribution because they do not belong to CAFE (nor FER, for SC (c←a)). Class activation maps are computed with the implementation by  [Gildenblat, Jacob and contributors, 2021]  of Grad-CAM  [Selvaraju et al., 2017] .\n\nFig.  4  shows the class activation maps obtained for the true class with the networks trained by ten repetitions of the training process (columns). Images for which the network predicts the correct (respectively, incorrect) class have a green (red) border. The ten networks trained with SC (c←a) are, on average, more accurate than those trained with SC (c). Interestingly, the area highlighted for correct predictions (green border images) corresponding to different networks from the same approach (a same row) can differ, due to the randomness of the data splitting and training process. For example, sometimes the eyes are deemed to be important, and sometimes they are not. When the prediction is incorrect (red border images), the pixels that are deemed to be responsible by the class activation map tend to be scattered around the image, at times on parts that can be considered not helpful to predict the emotion (e.g., part of the hair or the chin). For SC (c), one of such \"unreasonable\" maps is also produced for a correct classification of the happy boy, on the 7 th column.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "In the introduction of this work, we made two hypotheses. First, that training a neural network with more and more augmentations of the same data has diminishing returns. Our results confirm this hypothesis. In fact, this finding can also be observed in the recent work by  [Fort et al., 2021]  for much bigger data sets than the one used here (CAFE), namely CIFAR-100 and ImageNet  [Krizhevsky, 2009; Deng et al., 2009] . However, our results also show that diminishing returns are less pronounced when the training data set, i.e., images of children, is supplemented with extra data, i.e., images of adults, provided that the training set is relatively small and an appropriate learning approach is used. Thus, in settings where the data for the problem at hand is scarce (e.g., as often the case in pediatric medicine), augmentations can only help up to some point. However, integrating a different but related data source can bring additional improvements.\n\nOur second hypothesis was that using adult data can be beneficial for recognizing the emotions of children. While the best approach, SC (c←a), does in fact make use of adult data, that was not always the case for the second-best approach, indicating that how the adult data are used matters. Thus, our results support this hypothesis only in part.\n\nRelated to this work,  [Fort et al., 2021]  proposed to use multiple augmentations of a few images when populating a mini-batch, to diminish the variance of the training process. They found that using larger augmentation multiplicities can improve both training speed and generalization performance, when dealing with relatively large data sets (compared to CAFE), namely CIFAR-100 and ImageNet. Interestingly, such an approach is more similar to our SC (c) and SC (c+a), which use two augmentations of the same image per minibatch, than to SC (c←a), which uses a single augmentation of an image per mini-batch (see Fig.  2 ). Yet, SC (c←a) was the best-performing approach in our experiments: this might seem counter-intuitive. Actually, the results by  [Fort et al., 2021]  in regimes where over-fitting is likely (i.e., when the number of training epochs is relatively large with respect to the size of the training set) show that increasing the multiplicity of augmentations of the same image increases the speed by which generalization performance drops because of overfitting. As CAFE is approximately 50 and 1000 times smaller than CIFAR-100 and ImageNet, respectively, CAFE is eas-ier to over-fit. In this light, our results suggest that for small data sets such as those concerning children, using multiple augmentations of a few images may not be the best approach. Rather, one may wish to enrich the informational content of mini-batches (and thus variance in the training process).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We compared contrastive and supervised learning approaches for training deep neural networks for the problem of facial emotion recognition in children, when the available data are scarce. We proposed an approach that uses supervised contrastive learning to use abundantly-available data of adults as augmentations for data of children, and found this approach to perform better than baseline approaches, especially when the training set is particularly small. Our approach obtained +2 to 3% test accuracy compared to the second-best method. We further showed examples of class activation maps, which are useful tools to explain the behavior of black-box models like deep neural networks. This provided additional indication that appropriately incorporating adult data can lead to learning better representations than solely relying on data of children. This work will hopefully serve as an inspiration for other applications in pediatric care where data is scarce but other sources may be exploited to learn good representations.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: summarizes, at high level, the contrastive learning",
      "page": 2
    },
    {
      "caption": "Figure 1: Contrastive learning methods train a network to encode",
      "page": 3
    },
    {
      "caption": "Figure 2: On the one",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of the two approaches we consider for popu-",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the validation and test accuracy for the con-",
      "page": 4
    },
    {
      "caption": "Figure 3: Validation (left columns) and test (right columns) accuracy for an increasingly larger training set (training split X%) and multiplici-",
      "page": 5
    },
    {
      "caption": "Figure 4: Class activation maps for three children’s facial expression",
      "page": 5
    },
    {
      "caption": "Figure 4: shows the class activation maps obtained for the true",
      "page": 5
    },
    {
      "caption": "Figure 2: ). Yet, SC (c←a) was",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3Leiden University Medical Center": "4Delft University of Technology"
        },
        {
          "3Leiden University Medical Center": "{marco.virgolin, peter.bosman}@cwi.nl, andrea.delorenzo@units.it, t.alderliesten@lumc.nl"
        },
        {
          "3Leiden University Medical Center": "Abstract"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "Emotion recognition in children can help the early"
        },
        {
          "3Leiden University Medical Center": "identiﬁcation of, and intervention on, psychologi-"
        },
        {
          "3Leiden University Medical Center": "cal complications that arise in stressful situations"
        },
        {
          "3Leiden University Medical Center": "such\nas\ncancer\ntreatment.\nThough\ndeep\nlearn-"
        },
        {
          "3Leiden University Medical Center": "ing models\nare\nincreasingly being adopted,\ndata"
        },
        {
          "3Leiden University Medical Center": "scarcity is often an issue\nin pediatric medicine,"
        },
        {
          "3Leiden University Medical Center": "including for\nfacial\nemotion recognition in chil-"
        },
        {
          "3Leiden University Medical Center": "dren. In this paper, we study the application of data"
        },
        {
          "3Leiden University Medical Center": "augmentation-based contrastive\nlearning to over-"
        },
        {
          "3Leiden University Medical Center": "come data scarcity in facial emotion recognition for"
        },
        {
          "3Leiden University Medical Center": "children. We explore the idea of ignoring genera-"
        },
        {
          "3Leiden University Medical Center": "tional gaps, by adding abundantly available adult"
        },
        {
          "3Leiden University Medical Center": "data to pediatric data,\nto learn better\nrepresenta-"
        },
        {
          "3Leiden University Medical Center": "tions.\nWe\ninvestigate\ndifferent ways\nby which"
        },
        {
          "3Leiden University Medical Center": "adult\nfacial expression images can be used along-"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "side those of children.\nIn particular, we propose to"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "explicitly incorporate within each mini-batch adult"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "images as augmentations for children’s. Out of 84"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "combinations of\nlearning approaches and training"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "set sizes, we ﬁnd that supervised contrastive learn-"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "ing with the proposed training scheme performs"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "best,\nreaching a\ntest\naccuracy that\ntypically sur-"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "passes the one of the second-best approach by 2%"
        },
        {
          "3Leiden University Medical Center": "to 3%. Our results indicate that adult data can be"
        },
        {
          "3Leiden University Medical Center": "considered to be a meaningful augmentation of pe-"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "diatric data for the recognition of emotional facial"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "expression in children, and open up the possibil-"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "ity for other applications of contrastive learning to"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "improve pediatric care by complementing data of"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "children with that of adults."
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "1\nIntroduction"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "The course of vital treatments such as cancer therapy can ex-"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "pose the patient\nto high levels of\nstress and depression,\nin"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "adults as well as in children [Linden et al., 2012; Compas et"
        },
        {
          "3Leiden University Medical Center": "al., 2014].\nIn particular for young children, whose ability to"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "disclose inner feelings is still under development [Sprung et"
        },
        {
          "3Leiden University Medical Center": ""
        },
        {
          "3Leiden University Medical Center": "al., 2015], being able to recognize non-verbal emotion sig-"
        },
        {
          "3Leiden University Medical Center": "nals can be a critical factor for the monitoring of, and inter-"
        },
        {
          "3Leiden University Medical Center": "vention on,\ntheir well-being. Here we consider the problem"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "by proposing a scheme where images of adults are explicitly",
          "typically involve pre-training,\ne.g., on a larger data set of": "adults [Lopez-Rincon, 2019], or\nfeature reduction, e.g., by"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "paired with images of children that share the same emotion.",
          "typically involve pre-training,\ne.g., on a larger data set of": "using facial\nlandmarks rather than the entire image as inputs"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "For comparison, we include standard SupCon,\nthe unsuper-",
          "typically involve pre-training,\ne.g., on a larger data set of": "for the network [Rao et al., 2020]. To the best of our knowl-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "vised approach Simple framework for Contrastive Learning",
          "typically involve pre-training,\ne.g., on a larger data set of": "edge, this is the ﬁrst paper that considers contrastive learning"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "of visual Representations (SimCLR) [Chen et al., 2020], and",
          "typically involve pre-training,\ne.g., on a larger data set of": "for facial emotion recognition in children."
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "traditional supervised learning, with and without expanding",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "the original child emotion data set with an adult one.",
          "typically involve pre-training,\ne.g., on a larger data set of": "3\nMethods"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "Fig. 1 summarizes,\nat high level,\nthe\ncontrastive\nlearning"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "2\nRelated work",
          "typically involve pre-training,\ne.g., on a larger data set of": "methods considered here.\nGiven a data set\n(e.g., of chil-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "dren’s facial emotions), a neural network is trained with mini-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "Contrastive learning concerns learning a function that maps",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "batches that contain augmentations of the original images, to"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "data points that are similar (resp., dissimilar) into latent rep-",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "learn to project\nimages that are similar into respective latent"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "resentations\nthat are near\n(resp.,\nfar),\nin terms of a certain",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "representations that are close. A label may or may not be"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "metric\nspace\n(e.g.,\nthe Euclidean one).\nThese\nlatent\nrep-",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "present and used by the contrastive learning method. Differ-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "resentations\ncan be useful\nto perform supervised learning",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "ent from the traditional use of a single data set (e.g., as would"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "tasks (if labels are available), as well as to reason about\nthe",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "be the case for images 1 and 2 in the mini-batch of Fig. 1),"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "data. There exist several approaches to contrastive learning,",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "here we juxtapose a second data set\n(of adults)\nthat shares"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "some dating back to the early the 2000s\n(e.g.,\n[Chopra et",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "images that have the same set of (possibly unknown) labels"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "al., 2005]), which differ\nin how the training process works,",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "as the ﬁrst data set (of children), and study how this can be"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "what\nloss\nis used, and whether\nthe approach is\nsupervised",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "exploited to improve the network’s capability to learn a good"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "(i.e.,\nlabels are used) or unsupervised (no labels are used,",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "latent representation."
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "et\ne.g.,\nbecause\nthey\nare\nunavailable)\n[Jaiswal\nal.,\n2021;",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "In the next section, we describe SupCon and SimCLR in"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "Weng, 2021]. For example, the method proposed by [Schroff",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "more detail. Next,\nin Sec. 3.2 we describe the approaches"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "et al., 2015]\nfor\nface identiﬁcation proposed a triplet\nloss,",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "that we investigate to leverage adult facial expressions. De-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "which trains a neural network to minimize the L2-distance be-",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "tails on the adopted data sets and experimental setup are then"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "tween an anchor (i.e., an image of a person) with its positives",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "provided in Sec. 3.3."
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "(i.e., other images of the same person) and maximize it with",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "its negatives\n(i.e.,\nimages of other people), using labels\nto",
          "typically involve pre-training,\ne.g., on a larger data set of": "3.1\nContrastive learning: SimCLR and SupCon"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "deﬁne what images portrait the same person. A different and",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "We begin by describing the principles behind [Chen et al.,"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "label-free (i.e., unsupervised or self-supervised) approach can",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "2020]’s SimCLR. SimCLR does not require labels, and is thus"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "typically be taken when knowing how modiﬁcations of\nthe",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "an unsupervised or self-supervised method. Let b be the size"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "original data that do not alter\ntheir semantics can be gener-",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "of a mini-batch. SimCLR operates by (i) randomly sampling"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "ated. A well-known unsupervised contrastive approach for",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "b/2 images from the data set;\n(ii) generating two augmen-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "images is SimCLR [Chen et al., 2020], whereby augmenta-",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "tations for each image;\n(iii) ﬁlling the mini-batch with such"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "tions are used to generate different versions of a same image",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "augmentations; and (iv) computing the loss:"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "that carry the same meaning (e.g., by cropping, rotating, color",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "jittering, and so on).\nThe loss\nfunction of SimCLR is de-",
          "typically involve pre-training,\ne.g., on a larger data set of": "exp (cid:0)s(zi, zo(i))/τ (cid:1)"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "LSimCLR = −\nlog\n,"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "signed to learn latent representations such that the latent rep-",
          "typically involve pre-training,\ne.g., on a larger data set of": "(cid:88) i\n(cid:80)"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "q∈I\\{i} exp (s(zi, zq)/τ )"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "∈I"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "resentations of augmented images that share a common origin",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "are similar with respect to the metric space. SupCon [Chen et",
          "typically involve pre-training,\ne.g., on a larger data set of": "where z ∈ RK is a latent representation that\nis produced by"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "al., 2020] is similar to SimCLR but, besides using augmen-",
          "typically involve pre-training,\ne.g., on a larger data set of": "the network, I\nis the set of\nindices that\nidentiﬁes the aug-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "tations, also (requires and) uses the labels of the images,\nto",
          "typically involve pre-training,\ne.g., on a larger data set of": "mentations in the mini-batch, i is the index of an augmenta-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "learn better latent representations.",
          "typically involve pre-training,\ne.g., on a larger data set of": "tion, o(i) is the other augmentation for\nthe same image for"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "In this work, we focus on facial emotion recognition in",
          "typically involve pre-training,\ne.g., on a larger data set of": "which the ith augmentation was made, s is a similarity func-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "children. While there exist many works in literature about",
          "typically involve pre-training,\ne.g., on a larger data set of": "tion (here, cosine similarity), and τ is a hyper-parameter. Es-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "facial emotion recognition in general [Ko, 2018], the number",
          "typically involve pre-training,\ne.g., on a larger data set of": "sentially, LSimCLR increases the similarity of latent represen-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "of studies that speciﬁcally focus on children are limited. Of",
          "typically involve pre-training,\ne.g., on a larger data set of": "tations generated for those elements of the mini-batch that are"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "these, recent works normally rely on deep convolutional neu-",
          "typically involve pre-training,\ne.g., on a larger data set of": "augmentations of the same image."
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "ral networks, since these methods represent\nthe state-of-the-",
          "typically involve pre-training,\ne.g., on a larger data set of": "One can easily see that SimCLR relies on the mini-batch"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "art in image classiﬁcation in general [Rawat and Wang, 2017;",
          "typically involve pre-training,\ne.g., on a larger data set of": "containing mostly (augmentations of)\nimages with different"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "Anwar et al., 2018];\nand also on data sets\nsuch as CAFE",
          "typically involve pre-training,\ne.g., on a larger data set of": "(unknown) labels:\nif two images are retrieved from the data"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "(1192\nimages\nof\n2\nto\n8\nyears\nold\nchildren)\n[LoBue\nand",
          "typically involve pre-training,\ne.g., on a larger data set of": "set\nthat share the same meaning,\ntheir respective augmenta-"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "Thrasher, 2015],\nthe Dartmouth database of children’s faces",
          "typically involve pre-training,\ne.g., on a larger data set of": "tions will be treated as dissimilar by LSimCLR.\nDifferently"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "(640 images, 6–16 y.o.)\n[Dalrymple et al., 2013], NIMH-",
          "typically involve pre-training,\ne.g., on a larger data set of": "from SimCLR, [Khosla et al., 2020]’s SupCon assumes that"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "ChEFS (482 images, 10–17 y.o.)\n[Egger et al., 2011], and",
          "typically involve pre-training,\ne.g., on a larger data set of": "labels are available and uses them to tackle this limitation, by:"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "EmoReact\n(1102 audio-visual clips, 4–14 y.o.)\n[Nojavanas-",
          "typically involve pre-training,\ne.g., on a larger data set of": ""
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "−1\nexp (s(zi, zp)/τ )\n(cid:88)"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "ghari et al., 2016]. Since these data sets are relatively small",
          "typically involve pre-training,\ne.g., on a larger data set of": "log\n,\nLSupCon ="
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "(cid:88) i\n(cid:80)"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "for\ntraining convolutional neural networks,\nresearch works",
          "typically involve pre-training,\ne.g., on a larger data set of": "|P (i)|\nq∈I\\{i} exp (s(zi, zq)/τ )"
        },
        {
          "vised Contrastive learning (SupCon)\n[Khosla et al., 2020],": "",
          "typically involve pre-training,\ne.g., on a larger data set of": "∈I\np∈P (i)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "lating a mini-batch using two data sets. In the ﬁrst approach (a), two"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "augmentations are produced for each image taken from a data set. In"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "the second approach (b),\nthe label of the image taken from the ﬁrst"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "data set (of children) is used to fetch an image from the second data"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "set\n(of adults). An augmentation is produced for each of\nthe two"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "images, and the augmentation of the image from the second data set"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "(of adults) is explicitly used as another augmentation of the image"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "from the ﬁrst data set (of children)."
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "what random transformations are applied. Then, for a mean-"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "ingful similarity function σ between images (e.g., one could"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "attempt to measure the proximity of locality sensitive hashing"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "systems such as Apple’s NeuralHash [Apple Inc., 2021]),\nit"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "is reasonable to expect that:"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ", ξ), a(xA\n, ξ), a(xB\n, ξ))].\nEξ,i,j[σ(a(xA\nj , ξ))] > Eξ,i,j[σ(a(xA"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "j"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "Since with the second approach mini-batches contain images"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "that are more dissimilar from one another, these mini-batches"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "may carry a richer training signal than those obtained with the"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "ﬁrst approach."
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "We remark that\nthe second approach cannot be used with"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "SimCLR, as one needs to know the labels to pick images from"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "data set B for the images of data set A."
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "3.3\nExperimental setup"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "We set up our experiments according to the settings used"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "by [Khosla et al., 2020], with adaptations for\nthe task and"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "data at hand. We begin by describing the data sets used and"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "their pre-processing. Next, we provide details on the possible"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "augmentations, followed by information on the network,\nthe"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "training process, and the validation steps."
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "Data sets"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "We consider\ntwo data sets:\nthe Child Affective Facial Ex-"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "pression (CAFE) data set\n[LoBue and Thrasher, 2015], and"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": ""
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "the\nFacial Expression Recognition\nchallenge\n(FER)\ndata"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "set [Goodfellow et al., 2013]. CAFE contains 1192 pictures"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "of 2 to 8 years old-children,\nin color and portrait format, for"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "the following emotions: anger, disgust, fear, happiness, neu-"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "tral, sadness, surprise.\nFER contains 28 000 images for\nthe"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "same set of emotions, in gray scale and squared format. FER"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "is mostly populated by images of adults, however some pic-"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "tures of children are also present. By visual estimation of a"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "random sample of 1050 images (150 per class), we estimate"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "that approximately 15% of the images in FER represent chil-"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "dren up to ten years old (some of whom are younger than the"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "children represented in CAFE, i.e., < 2 y.o.)."
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "We pre-process CAFE to have a same picture format of"
        },
        {
          "Figure 2:\nIllustration of the two approaches we consider for popu-": "FER,\ni.e.,\nthe top and the bottom of the images is appropri-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "height ratio, resolution is set to 48 pixels per dimension, and",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "cropping of up to 20% of the image; horizontal ﬂipping with"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "images are converted to gray scale.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "0.5 probability;\nand color\njittering of ±0.4 for brightness,"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "Throughout\nthe experiments, CAFE is used to train, val-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "contrast, saturation, and ±0.1 for hue, with 0.8 probability."
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "idate, and test\nthe considered approaches. We assess how",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "We formulate the use of augmentations in a different way"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "the results change based on how much data are available for",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "than how typically done in literature. Normally, one or two"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "training, by using training splits of 50%, 70%, and 90%;\nthe",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "augmentations are generated per\nimage on the ﬂy,\nto enter"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "validation and test sets are created by partitioning the remain-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "a mini-batch [Khosla et al., 2020; Fort et al., 2021]. As a"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "ing data points in equal amounts. We use balanced training",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "consequence,\nthe longer the training process takes,\nthe more"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "sets, assigning the number of data points for training by mul-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "information is\nfed to the network.\nHere, we re-frame this"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "tiplying the training split percentage with the number of data",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "phenomenon in an equivalent setting whereby training has a"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "points available in the minority class. We repeat each exper-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "the size of the training\nﬁxed duration (of nepochs = 250), but"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "iment\nten times due to the stochastic nature of data splitting",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "set varies based on how many augmentations calls are made."
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "and neural\ntraining. For FER no splits are used, as it\nis only",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "This allows us to directly think in terms of how much data is"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "used to supply random images of adults during training.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "seen by the network with respect\nto the size of\nthe training"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "set.\nIn the results below, we will use the term augmentation"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "Neural architecture, training process, and augmentations",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "ratio to refer to the multiplicity by which the original training"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "set is used to craft augmentations."
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "Architecture\nThe neural architecture used for contrastive",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "learning (SupCon and SimCLR) is composed of two parts: an",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "encoder network and a projection network. The encoder net-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "4\nResults"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "work takes an image x and produces a latent representation",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "We present two types of results: accuracy (on validation and"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "r ∈ RK(cid:48)\n. Like [Khosla et al., 2020], we use a ResNet [He",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "test sets), and a qualitative comparison using class activation"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "et al., 2016] and in particular a gray-scale version of ResNet-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "maps. From now onwards, we use the following acronyms:"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "18, with K (cid:48) = 512. The projection network is a multi-layer",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "SC for supervised contrastive learning, i.e., SupCon; UC for"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "perceptron stacked on top of the encoder network,\nto map r",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "unsupervised contrastive learning,\ni.e., SimCLR; and SL for"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "to z ∈ RK, which is a smaller projection (i.e., K < K (cid:48)). The",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "traditional supervised learning,\ni.e.,\nthe direct training of the"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "contrastive loss is computed upon the representations z1, . . . ,",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "encoder and linear classiﬁer, without\nthe intermediate con-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "zb obtained by iteratively processing (by the encoder network",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "trastive training.\nFurthermore, we indicate whether\nthe ap-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "and the projection network) the respective x1, . . . , xb, where",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "proach is\ntrained on CAFE alone with the notation (c)\nfor"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "b is the size of the mini-batches. We use K = 128 and b = 32.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "“children”; on CAFE extended with an equal number of ran-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "The projection network is discarded after constrastive train-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "dom images from FER with (c+a) for “children plus adults”;"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "ing. To perform classiﬁcation, a linear classiﬁer is stacked on",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "and on CAFE using images from FER as augmentations for"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "top of the encoder and trained with a cross-entropy loss. We",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "the images in CAFE with (c←a) for “children augmented by"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "refer to the work by [Khosla et al., 2020] for more details.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "adults”. The latter approach is only possible when using SC,"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "as explained before in Sec. 3.1."
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "Training and validation settings\nOur\nexperimental\nset-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "tings for training are essentially those used by [Khosla et al.,",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "4.1\nChild emotion recognition accuracy"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "2020], adapted to the smaller size of the data sets at play here.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": ""
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "as\nthis\nis abun-\nIn particular, we train for nepochs = 250,",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "Figure 3 shows the validation and test accuracy for the con-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "dantly sufﬁcient\nto make the contrastive losses of SupCon",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "sidered learning approaches, for different sizes of the original"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "and SimCLR converge in all our experiments3. For stochas-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "training set, and augmentation ratios."
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "tic gradient descent, we set\nthe initial\nlearning rate to 0.05,",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "The general\ntrend suggests that using a larger augmenta-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "momentum to 0.9, and learning rate decay to 0.1 at epochs",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "tion ratio leads\nto equal or better validation and test accu-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "layer\nis then\nnepochs − 0.1 × nepochs × {3, 2, 1}. The linear",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "racy. This trend is more pronounced when the training split"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "trained for 50 epochs with similar settings (except for using",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "is small, and less when it\nis large. A ﬁrst evident ﬁnding is"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "an initial\nlearning rate of 10−4), which we found to be suf-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "that the UC approaches perform substantially worse than the"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "ﬁcient\nto for the loss to converge. During these epochs, we",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "others, meaning that\nthe latent representations they learn are"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "record the parameters of the network that lead to the best top-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "poor. This can be expected because the training data sets are"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "1 validation accuracy, and use those parameters for testing.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "relatively small\n(UC approaches are normally used on very"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "We\nalso\ninclude\na\ncomparison with\nclassic\nsupervised",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "large, unlabelled data sets) and the number of classes for the"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "learning training, where a network composed of the encoder",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "problem at hand is also small, hence LSimCLR often puts in"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "and the\nlinear\nclassiﬁer\nis\ntrained directly with the\ncross-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "contrast augmentations of different images of the same class."
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "the best-found vali-\nentropy loss for nepochs = 250. Again,",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "The other approaches, i.e., the variants of SL and SC, per-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "dation accuracy is tracked to determine which parameters to",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "form similarly.\nIn particular, no evident differences can be"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "use for testing.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "observed for SC when CAFE is used alone,\ni.e., SC (c), or"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "in conjunction with FER,\ni.e., SC (c+a).\nFor SL,\n(c+a) of-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "3Training to convergence is often done in contrastive learning be-",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "ten leads to worse results than (c). Conversely,\nthe perfor-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "cause one lacks a classiﬁer (or, for SimCLR, even labels) to perform",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "mance obtained by SC (c←a) is notable, as this approach ob-"
        },
        {
          "ately cut (based on visual inspection) to obtain a 1 : 1 width-": "early stopping.",
          "Augmentations\nWe follow [Khosla et al., 2020] and adopt": "tains the best distributions of validation and test accuracies"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: shows results for the best and second-best ap- trainedwithSC(c). Whenthepredictionofanetworkisincorrect,",
      "data": [
        {
          "Figure 3: Validation (left columns) and test (right columns) accuracy for an increasingly larger training set (training split X%) and multiplici-": "ties by which augmentations are sampled from the training set (augmentation ratio). The panels in the top (bottom) row include (exclude) UC"
        },
        {
          "Figure 3: Validation (left columns) and test (right columns) accuracy for an increasingly larger training set (training split X%) and multiplici-": "and share (do not share) the same vertical axis. Dots are means, shaded areas are 95% conﬁdence intervals, and bars are standard deviations,"
        },
        {
          "Figure 3: Validation (left columns) and test (right columns) accuracy for an increasingly larger training set (training split X%) and multiplici-": "computed for ten repetitions."
        },
        {
          "Figure 3: Validation (left columns) and test (right columns) accuracy for an increasingly larger training set (training split X%) and multiplici-": "Table 1: Mean test results for ×20 augmentation ratios of the best"
        },
        {
          "Figure 3: Validation (left columns) and test (right columns) accuracy for an increasingly larger training set (training split X%) and multiplici-": "and runner up (R.U.) approaches across training split sizes (Tr.S.)."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: shows results for the best and second-best ap- trainedwithSC(c). Whenthepredictionofanetworkisincorrect,",
      "data": [
        {
          "There are two (non-mutually exclusive) explanations for this.": "First, with a training split of 90%,\nthe training data may al-"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "ready be sufﬁcient\nto learn a latent representation that\nis (al-"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "most) optimal. Second, as the validation and test sets are each"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "composed of half\nthe data left over\nfrom the training split,"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "the recordings of validation and test accuracy may be noisy."
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "Lastly, for SC (c) and SC (c+a) with a training split of 90%"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "and the larger augmentation ratios,\nthe best-found validation"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "accuracy drops, which indicates over-ﬁtting of the contrastive"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "training process."
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "Table 1 shows\nresults\nfor\nthe best\nand second-best\nap-"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "proaches in terms of\ntest accuracy,\nfor a ×20 augmentation"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "ratio and across the different\ntraining splits. SC (c←a) is al-"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "ways the best method, whereas the second-best varies. The"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "Mann-Whitney-U test\nis used to report\nthe p-values of\nthe"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "null hypothesis that the best approach is not signiﬁcantly bet-"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "ter than the second-best approach [Mann and Whitney, 1947]."
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "The improvements in test accuracy vary between 2 to 3%,"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "and are signiﬁcant with p-value < 0.1 for all cases, and with"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "p-value < 0.05 when the training split is 50% and 70%."
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "4.2\nQualitative analysis with class activation maps"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "We provide additional\ninsights by comparing the class acti-"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": ""
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "vation maps that are obtained by requiring the networks to"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "identify the areas of\nthe image that prompt\nits prediction to"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "provide the correct\nlabel. We consider SC (c←a)\n(the best"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "performing approach that makes the best use of adult faces)"
        },
        {
          "There are two (non-mutually exclusive) explanations for this.": "and SC (c) (the respective approach that uses only children’s"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "multiple augmentations of a few images when populating a",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "mini-batch,\nto diminish the variance of the training process.",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "References"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "They found that using larger augmentation multiplicities can",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "[Anwar et al., 2018] Syed Muhammad Anwar, Muhammad"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "improve both training speed and generalization performance,",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "Majid, Adnan Qayyum, Muhammad Awais, Majdi Al-"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "when dealing with relatively large data\nsets\n(compared to",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "nowami, and Muhammad Khurram Khan. Medical image"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "CAFE), namely CIFAR-100 and ImageNet.\nInterestingly,",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "analysis using convolutional neural networks: A review."
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "such an approach is more similar to our SC (c) and SC (c+a),",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "Journal of Medical Systems, 42(11):1–13, 2018."
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "which use two augmentations of\nthe same image per mini-",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "batch,\nthan to SC (c←a), which uses a single augmentation",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "[Apple Inc., 2021] Apple Inc. CSAM detection — technical"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "of an image per mini-batch (see Fig. 2). Yet, SC (c←a) was",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "summary, 2021. Accessed January 2022."
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "the best-performing approach in our experiments:\nthis might",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "[Chen et al., 2020] Ting Chen, Simon Kornblith, Moham-"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "seem counter-intuitive. Actually,\nthe results by [Fort et al.,",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "mad Norouzi, and Geoffrey Hinton. A simple framework"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "2021]\nin regimes where over-ﬁtting is likely (i.e., when the",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "for contrastive learning of visual representations.\nIn Inter-"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "number of training epochs is relatively large with respect\nto",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "national Conference on Machine Learning, pages 1597–"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "the size of the training set) show that increasing the multiplic-",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "",
          "M´axima Center for Pediatric Oncology.": "1607. PMLR, 2020."
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "ity of augmentations of the same image increases the speed",
          "M´axima Center for Pediatric Oncology.": ""
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "by which generalization performance drops because of over-",
          "M´axima Center for Pediatric Oncology.": "[Chopra et al., 2005] Sumit Chopra, Raia Hadsell, and Yann"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "ﬁtting. As CAFE is approximately 50 and 1000 times smaller",
          "M´axima Center for Pediatric Oncology.": "LeCun. Learning a similarity metric discriminatively, with"
        },
        {
          "Related to this work,\n[Fort et al., 2021] proposed to use": "than CIFAR-100 and ImageNet,\nrespectively, CAFE is eas-",
          "M´axima Center for Pediatric Oncology.": "application to face veriﬁcation. In IEEE Computer Society"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "on average, more accurate than those trained with SC (c). In-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "data sets such as those concerning children, using multiple"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "terestingly, the area highlighted for correct predictions (green",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "augmentations of a few images may not be the best approach."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "border images) corresponding to different networks from the",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Rather, one may wish to enrich the informational content of"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "same approach (a same row) can differ, due to the random-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "mini-batches (and thus variance in the training process)."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "ness of the data splitting and training process. For example,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "sometimes the eyes are deemed to be important, and some-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "6\nConclusion"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "times they are not. When the prediction is incorrect (red bor-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "We compared contrastive and supervised learning approaches"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "der images),\nthe pixels that are deemed to be responsible by",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "for\ntraining deep neural networks for\nthe problem of\nfacial"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "the class activation map tend to be scattered around the im-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "emotion recognition in children, when the available data are"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "age, at\ntimes on parts that can be considered not helpful\nto",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "scarce. We proposed an approach that uses supervised con-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "predict the emotion (e.g., part of the hair or the chin). For SC",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "trastive learning to use abundantly-available data of adults as"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "(c), one of such “unreasonable” maps is also produced for a",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "augmentations for data of children, and found this approach"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "correct classiﬁcation of the happy boy, on the 7th column.",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "to perform better than baseline approaches, especially when"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "the training set\nis particularly small. Our approach obtained"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "5\nDiscussion",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "+2 to 3% test accuracy compared to the second-best method."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "We further showed examples of class activation maps, which"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "In the introduction of\nthis work, we made two hypotheses.",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "are useful\ntools to explain the behavior of black-box models"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "First, that training a neural network with more and more aug-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "like deep neural networks. This provided additional\nindica-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "mentations of the same data has diminishing returns. Our re-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "tion that appropriately incorporating adult data can lead to"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "sults conﬁrm this hypothesis.\nIn fact,\nthis ﬁnding can also",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "learning better representations than solely relying on data of"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "be observed in the recent work by [Fort et al., 2021]\nfor",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "children. This work will hopefully serve as an inspiration for"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "much bigger data sets than the one used here (CAFE), namely",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "other applications in pediatric care where data is scarce but"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "CIFAR-100 and ImageNet\n[Krizhevsky, 2009; Deng et al.,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "other sources may be exploited to learn good representations."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "2009]. However, our\nresults also show that diminishing re-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "turns are less pronounced when the training data set, i.e., im-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "ages of children, is supplemented with extra data, i.e., images",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Acknowledgments"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "of adults, provided that the training set is relatively small and",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "The authors thank Jamal Toutouh for insightful discussions."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "an appropriate learning approach is used.\nThus,\nin settings",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Improving Childhood Can-\nThis work is part of\nthe project"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "where the data for the problem at hand is scarce (e.g., as of-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "cer Care when Parents Cannot be There – Reducing Medical"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "ten the case in pediatric medicine), augmentations can only",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Traumatic Stress in Childhood Cancer Patients by Bonding"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "help up to some point. However,\nintegrating a different but",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "with a Robot Companion (with project number #15198) of"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "related data source can bring additional improvements.",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "the research program Technology for Oncology, which is ﬁ-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "Our\nsecond hypothesis was\nthat using adult data can be",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "nanced by the Dutch Research Council\n(NWO),\nthe Dutch"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "beneﬁcial for recognizing the emotions of children. While the",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Cancer Society (KWF), TKI Life Sciences & Health, Asolu-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "best approach, SC (c←a), does in fact make use of adult data,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "tions, Brocacef, Cancer Health Coach, and Focal Meditech."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "that was not always the case for\nthe second-best approach,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "The research consortium consists of Centrum Wiskunde &"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "indicating that how the adult data are used matters. Thus, our",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Informatica, Delft University of Technology, the Amsterdam"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "results support this hypothesis only in part.",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "University Medical Centers location AMC, and the Princess"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "Related to this work,\n[Fort et al., 2021] proposed to use",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "M´axima Center for Pediatric Oncology."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "multiple augmentations of a few images when populating a",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "mini-batch,\nto diminish the variance of the training process.",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "References"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "They found that using larger augmentation multiplicities can",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "[Anwar et al., 2018] Syed Muhammad Anwar, Muhammad"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "improve both training speed and generalization performance,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Majid, Adnan Qayyum, Muhammad Awais, Majdi Al-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "when dealing with relatively large data\nsets\n(compared to",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "nowami, and Muhammad Khurram Khan. Medical image"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "CAFE), namely CIFAR-100 and ImageNet.\nInterestingly,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "analysis using convolutional neural networks: A review."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "such an approach is more similar to our SC (c) and SC (c+a),",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "Journal of Medical Systems, 42(11):1–13, 2018."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "which use two augmentations of\nthe same image per mini-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "batch,\nthan to SC (c←a), which uses a single augmentation",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "[Apple Inc., 2021] Apple Inc. CSAM detection — technical"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "of an image per mini-batch (see Fig. 2). Yet, SC (c←a) was",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "summary, 2021. Accessed January 2022."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "the best-performing approach in our experiments:\nthis might",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "[Chen et al., 2020] Ting Chen, Simon Kornblith, Moham-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "seem counter-intuitive. Actually,\nthe results by [Fort et al.,",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "mad Norouzi, and Geoffrey Hinton. A simple framework"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "2021]\nin regimes where over-ﬁtting is likely (i.e., when the",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "for contrastive learning of visual representations.\nIn Inter-"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "number of training epochs is relatively large with respect\nto",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "national Conference on Machine Learning, pages 1597–"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "the size of the training set) show that increasing the multiplic-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "1607. PMLR, 2020."
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "ity of augmentations of the same image increases the speed",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": ""
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "by which generalization performance drops because of over-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "[Chopra et al., 2005] Sumit Chopra, Raia Hadsell, and Yann"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "ﬁtting. As CAFE is approximately 50 and 1000 times smaller",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "LeCun. Learning a similarity metric discriminatively, with"
        },
        {
          "(red) border. The ten networks trained with SC (c←a) are,": "than CIFAR-100 and ImageNet,\nrespectively, CAFE is eas-",
          "ier to over-ﬁt.\nIn this light, our results suggest\nthat for small": "application to face veriﬁcation. In IEEE Computer Society"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conference on Computer Vision and Pattern Recognition,": "volume 1, pages 539–546. IEEE, 2005.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "tion recognition based on visual\ninformation.\nSensors,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "18(2):401, 2018."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Compas et al., 2014] Bruce E Compas, Leandra Desjardins,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Krizhevsky, 2009] Alex Krizhevsky. Learning multiple lay-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Kathryn Vannatta, Tammi Young-Saleme, Erin M Ro-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "ers of features from tiny images, 2009. Technical report."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "driguez, Madeleine Dunn, Heather Bemis, Sarah Snyder,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "and Cynthia A Gerhardt. Children and adolescents coping",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Li and Deng, 2020] Shan Li and Weihong Deng. Deep fa-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "with cancer: Self-and parent\nreports of coping and anxi-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "IEEE Transactions\ncial expression recognition: A survey."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "ety/depression. Health Psychology, 33(8):853, 2014.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "on Affective Computing, 2020."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Dalrymple et al., 2013] Kirsten A Dalrymple, Jesse Gomez,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Li et al., 2008]\nJun Li, Trevor D Thompson, Jacqueline W"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "and Brad Duchaine. The Dartmouth database of children’s",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Miller, Lori A Pollack,\nand Sherri L Stewart.\nCan-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "faces: Acquisition and validation of a new face stimulus",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "cer\nincidence\namong\nchildren\nand\nadolescents\nin\nthe"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "set. PloS one, 8(11):e79131, 2013.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "United States,\n2001–2003.\nPediatrics,\n121(6):e1470–"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "e1477, 2008."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Deng et al., 2009]\nJia Deng, Wei Dong, Richard Socher, Li-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Linden et al., 2012] Wolfgang Linden, Andrea Vodermaier,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Jia Li, Kai Li, and Li Fei-Fei.\nImageNet: A large-scale",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Regina MacKenzie, and Duncan Greig. Anxiety and de-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "hierarchical image database.\nIn 2009 IEEE Conference on",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "pression after cancer diagnosis: prevalence rates by can-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Computer Vision and Pattern Recognition, pages 248–255.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "cer type, gender, and age.\nJournal of Affective Disorders,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Ieee, 2009.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "141(2-3):343–351, 2012."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Egger et al., 2011] Helen Link Egger, Daniel S Pine, Eric",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[LoBue and Thrasher, 2015] Vanessa\nLoBue\nand\nCat"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Nelson, Ellen Leibenluft, Monique Ernst, Kenneth E Tow-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Thrasher. The Child Affective Facial Expression (CAFE)"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "bin,\nand Adrian Angold.\nThe NIMH Child Emotional",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "set:\nValidity\nand\nreliability\nfrom untrained\nadults."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Faces picture Set (NIMH-ChEFS): A new set of children’s",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Frontiers in Psychology, 5:1532, 2015."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "International Journal of Methods\nfacial emotion stimuli.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "in Psychiatric Research, 20(3):145–156, 2011.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Lopez-Rincon, 2019] Alejandro Lopez-Rincon.\nEmotion"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "recognition using facial expressions in children using the"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Fan, 2021] Evelyn Fan. Awesome-FER (code repository),",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "NAO robot.\nIn International Conference on Electronics,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "2021. Accessed January 2022.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Communications and Computers, pages 146–153.\nIEEE,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "2019."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Fort et al., 2021] Stanislav\nFort, Andrew Brock, Razvan",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Pascanu,\nSoham De,\nand\nSamuel L Smith.\nDraw-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Mann and Whitney, 1947] Henry B Mann\nand Donald R"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "ing multiple\naugmentation\nsamples\nper\nimage\nduring",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Whitney. On a test of whether one of\ntwo random vari-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "arXiv preprint\ntraining efﬁciently decreases\ntest\nerror.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "ables is stochastically larger than the other. The Annals of"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "arXiv:2105.13343, 2021.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Mathematical Statistics, pages 50–60, 1947."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Gildenblat, Jacob and contributors, 2021] Gildenblat, Jacob",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Nojavanasghari et al., 2016] Behnaz Nojavanasghari, Tadas"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "and contributors.\nPyTorch library for CAM methods.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Baltruˇsaitis,\nCharles\nE\nHughes,\nand\nLouis-Philippe"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "https://github.com/jacobgil/pytorch-grad-cam, 2021.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Morency. EmoReact: A multimodal approach and dataset"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "for recognizing emotional responses in children.\nIn Pro-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Goodfellow et al., 2013]\nIan J Goodfellow, Dumitru Erhan,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "ceedings of\nthe 18th ACM International Conference on"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Multimodal Interaction, pages 137–144, 2016."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Hamner, Will Cukierski, Yichuan Tang, David Thaler,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Rao et al., 2020] Adish Rao,\nSiddhanth Ajri,\nAbhishek"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Dong-Hyun Lee, et al. Challenges in representation learn-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Guragol, Rahul Suresh,\nand Shikha Tripathi.\nEmotion"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "ing: A report on three machine learning contests.\nIn In-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "recognition from facial expressions in children and adults"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "ternational Conference on Neural Information Processing,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Intelligent Systems, Technolo-\nusing deep neural network."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "pages 117–124. Springer, 2013.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "gies and Applications, pages 43–51, 2020."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[He et al., 2016] Kaiming He, Xiangyu Zhang,\nShaoqing",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Rawat and Wang, 2017] Waseem\nRawat\nand\nZenghui"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Ren,\nand Jian Sun.\nDeep residual\nlearning for\nimage",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Wang.\nDeep\nconvolutional\nneural\nnetworks\nfor\nim-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "the IEEE Conference on\nrecognition.\nIn Proceedings of",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Neural\nage\nclassiﬁcation:\nA comprehensive\nreview."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Computer Vision and Pattern Recognition, pages 770–778,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Computation, 29(9):2352–2449, 2017."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "2016.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Schroff et al., 2015] Florian Schroff, Dmitry Kalenichenko,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Jaiswal et al., 2021] Ashish Jaiswal, Ashwin Ramesh Babu,",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "and James Philbin. Facenet: A uniﬁed embedding for face"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Mohammad Zaki Zadeh, Debapriya Banerjee, and Fillia",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "the IEEE\nrecognition and clustering.\nIn Proceedings of"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Makedon. A survey on contrastive self-supervised learn-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "ing. Technologies, 9(1):2, 2021.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": ""
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "pages 815–823, 2015."
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "[Khosla et al., 2020] Prannay Khosla, Piotr Teterwak, Chen",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "[Selvaraju et al., 2017] Ramprasaath R Selvaraju, Michael"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "Maschinot, Ce Liu, and Dilip Krishnan. Supervised con-",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "Parikh,\nand Dhruv Batra.\nGrad-CAM: Visual explana-"
        },
        {
          "Conference on Computer Vision and Pattern Recognition,": "trastive learning. arXiv preprint arXiv:2004.11362, 2020.",
          "[Ko, 2018] Byoung Chul Ko. A brief review of facial emo-": "tions from deep networks via gradient-based localization."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the IEEE International Conference on\nIn Proceedings of": "Computer Vision, pages 618–626, 2017."
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "[Shorten and Khoshgoftaar, 2019] Connor\nShorten\nand"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "Taghi M Khoshgoftaar. A survey on image data augmen-"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "tation for deep learning.\nJournal of Big Data, 6(1):1–48,"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "2019."
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "[Sprung et al., 2015] Manuel\nSprung, Hannah M M¨unch,"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "Paul L Harris, Chad Ebesutani, and Stefan G Hofmann."
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "Children’s\nemotion understanding:\nA meta-analysis of"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "training studies. Developmental Review, 37:41–65, 2015."
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "[Tikkinen-Piri et al., 2018] Christina\nTikkinen-Piri,\nAnna"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "Rohunen, and Jouni Markkula. EU general data protection"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "regulation:\nChanges and implications\nfor personal data"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "collecting companies. Computer Law & Security Review,"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "34(1):134–153, 2018."
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "[Weng, 2021] Lilian Weng. Contrastive representation learn-"
        },
        {
          "the IEEE International Conference on\nIn Proceedings of": "ing (blog post), 2021. Accessed January 2022."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Medical image analysis using convolutional neural networks: A review",
      "authors": [
        "Anwar"
      ],
      "year": "2018",
      "venue": "Journal of Medical Systems"
    },
    {
      "citation_id": "2",
      "title": "Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations",
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Children and adolescents coping with cancer: Self-and parent reports of coping and anxiety/depression",
      "authors": [
        "Chopra"
      ],
      "year": "2005",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "Deng"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "The NIMH Child Emotional Faces picture Set (NIMH-ChEFS): A new set of children's facial emotion stimuli",
      "authors": [
        "Egger"
      ],
      "year": "2011",
      "venue": "International Journal of Methods in Psychiatric Research"
    },
    {
      "citation_id": "6",
      "title": "Evelyn Fan. Awesome-FER (code repository)",
      "year": "2021",
      "venue": "Evelyn Fan. Awesome-FER (code repository)"
    },
    {
      "citation_id": "7",
      "title": "Drawing multiple augmentation samples per image during training efficiently decreases test error",
      "authors": [
        "Fort"
      ],
      "year": "2021",
      "venue": "Drawing multiple augmentation samples per image during training efficiently decreases test error",
      "arxiv": "arXiv:2105.13343"
    },
    {
      "citation_id": "8",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "; Gildenblat",
        "Goodfellow"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "9",
      "title": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun",
      "authors": [
        "He"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "A survey on contrastive self-supervised learning",
      "authors": [
        "Jaiswal"
      ],
      "year": "2021",
      "venue": "A survey on contrastive self-supervised learning"
    },
    {
      "citation_id": "11",
      "title": "Ko, 2018] Byoung Chul Ko. A brief review of facial emotion recognition based on visual information",
      "authors": [
        "Khosla"
      ],
      "year": "2018",
      "venue": "Sensors",
      "arxiv": "arXiv:2004.11362"
    },
    {
      "citation_id": "12",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "Alex Krizhevsky",
        "Shan Krizhevsky",
        "Weihong Li",
        "Deng"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Cancer incidence among children and adolescents in the United States",
      "authors": [
        "Li"
      ],
      "year": "2001",
      "venue": "Pediatrics"
    },
    {
      "citation_id": "14",
      "title": "Anxiety and depression after cancer diagnosis: prevalence rates by cancer type, gender, and age",
      "authors": [
        "Linden"
      ],
      "year": "2012",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "15",
      "title": "The Child Affective Facial Expression (CAFE) set: Validity and reliability from untrained adults",
      "authors": [
        "Thrasher Lobue",
        "Vanessa Lobue",
        "Cat Thrasher"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition from facial expressions in children and adults using deep neural network. Intelligent Systems, Technologies and Applications",
      "authors": [
        "Alejandro Lopez-Rincon",
        "; Lopez-Rincon",
        "Whitney Mann",
        "B Henry",
        "Donald R Whitney ; Mann",
        "Nojavanasghari"
      ],
      "year": "1947",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "17",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Schroff"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Grad-CAM: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Selvaraju"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "EU general data protection regulation: Changes and implications for personal data collecting companies",
      "authors": [
        "Tikkinen-Piri"
      ],
      "year": "2018",
      "venue": "Computer Law & Security Review"
    },
    {
      "citation_id": "20",
      "title": "Contrastive representation learning (blog post)",
      "authors": [
        "Lilian Weng",
        "Weng"
      ],
      "year": "2021",
      "venue": "Contrastive representation learning (blog post)"
    }
  ]
}