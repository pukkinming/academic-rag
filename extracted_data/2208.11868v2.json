{
  "paper_id": "2208.11868v2",
  "title": "Interpretable Multimodal Emotion Recognition Using Hybrid Fusion Of Speech And Image Data",
  "published": "2022-08-25T04:43:34Z",
  "authors": [
    "Puneet Kumar",
    "Sarthak Malik",
    "Balasubramanian Raman"
  ],
  "keywords": [
    "Affective Computing",
    "Multimodal Analysis",
    "Speech and Image Processing",
    "Interpretable AI",
    "Information Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a multimodal emotion recognition system based on hybrid fusion that classifies the emotions depicted by speech utterances and corresponding images into discrete classes. A new interpretability technique has been developed to identify the important speech & image features leading to the prediction of particular emotion classes. The proposed system's architecture has been determined through intensive ablation studies. It fuses the speech & image features and then combines speech, image, and intermediate fusion outputs. The proposed interpretability technique incorporates the divide & conquer approach to compute shapely values denoting each speech & image feature's importance. We have also constructed a large-scale dataset (IIT-R SIER dataset) consisting of speech utterances, corresponding images, and class labels, i.e., 'anger,' 'happy,' 'hate,' and 'sad.' The proposed system has achieved 83.29% accuracy for emotion recognition. The enhanced performance of the proposed system advocates the importance of utilizing complementary information from multiple modalities for emotion recognition.",
      "page_start": 1,
      "page_end": 14
    },
    {
      "section_name": "Introduction",
      "text": "The multimedia data has overgrown in the last few years, leading multimodal emotion analysis to emerging as an important research trend  [1] . The need to develop multimodal emotion processing systems capable of recognizing various emotions from images and texts is rapidly increasing. Research in this direction aims to help machines become empathetic as emotion analysis is used in various applications such as cognitive psychology, automated identification, intelligent devices, and humanmachine interface  [27] . The speech and image modalities portray human emotions and intentions very effectively  [12] . Combining complementary information from both of these modalities could increase emotion recognition accuracy  [42] .\n\nResearchers have attempted to identify emotions by processing audio and visual information separately  [18, 28, 39] . However, multimodal emotion recognition, where the emotional context from multiple modalities are analyzed together, performs better than unimodal emotion recognition  [42] . In this context, multimodal emotion recognition from speech & text modalities and image & text modalities have been performed; however, emotion recognition from speech & image modalities has yet to be fully explored. Moreover, most of the existing multimodal approaches do not focus on interpreting the internal working of their emotion recognition systems. It inspired us to develop a multimodal emotion recognition system capable of recognizing emotions portrayed by speech utterances & corresponding images and explaining the importance of each speech segment & visual feature towards emotion recognition.\n\nMultimodal emotion recognition also faces the issue of the unavailability of sufficient labeled datasets for training. Moreover, the real-life multimodal data contains generic images with facial, human, and non-human objects, but most of the existing multimodal datasets contain only facial and human images  [2] . A few multimodal datasets are available that contain generic images; however, they consist of positive, negative, and neutral sentiment labels and do not contain multi-class emotion labels  [5, 37] . A new dataset, 'IIT Roorkee Speech & Image Emotion Recognition (IIT-R SIER) dataset,' has been constructed to address this issue. It contains generic images, corresponding speech utterances, and discrete class labels, i.e., 'anger,' 'happy,' 'hate,' and 'sad.' We used the data instances with identical predicted emotion labels for image and text modalities to construct the dataset. This paper analyses the improvements in SER on combining the complementary information from corresponding images.\n\nThe proposed system, 'ParallelNet,' recognizes emotions in speech utterances and corresponding images. It implements two networks, N 1 and N 2, to fuse the information of speech and image modalities in a hybrid manner of intermediate and late fusion. The architectures for N 1 and N 2 are determined through extensive ablation studies. A technique to interpret the important input features and predictions has also been developed. The proposed system has performed with an accuracy of 83.29% on the IIT-R SIER dataset. The dataset and code for this paper are accessible at https://github.com/MIntelligence-Group/SpeechImg EmoRec.\n\nThe paper makes the following major contributions.\n\n• A hybrid-fusion-based novel system, 'ParallelNet,' has been proposed to classify an input containing speech utterance & corresponding image into discrete emotion classes. It combines the information from speech & image modalities using a hybrid of intermediate and late fusion. • A large-scale dataset, 'IIT-R SIER dataset' containing speech utterances, corresponding images, and emotion labels, has been constructed. • A new interpretability technique has been developed to identify the important parts of the input speech and image that contribute the most to recognizing emotions.\n\nFurther in this paper, the related works have been reviewed in Section 2. The proposed dataset, system, and interpretability technique have been described in Section 3 along with the dataset compilation procedure. Section 4 and 5 discuss the experiments and results. Finally, Section 6 concludes the paper and highlights the directions for future research.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "This Section surveys the existing literature on speech & image emotion recognition and the interpretability of deep neural networks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "The deep learning-based approaches using spectrogram features and attention mechanisms have shown state-of-the-art results for speech emotion recognition (SER)  [4, 13, 40] . In this context, Xu et al.  [39]  generated multiple attention maps, fused and used them for SER. They observed an increased performance as compared to non-fusion-based approaches. In another work, Majumder et al.  [18]  implemented a deep neural network to track speakers' identities showing specific emotions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Image Emotion Recognition",
      "text": "Image Emotion Recognition (IER) research is also an active domain. For instance, Kim et al.  [12]  built a deep feed-forward neural network to combine different levels of emotion features obtained by using the semantic information of the image. In another work, Rao et al.  [28]  prepared hierarchical notations for emotion recognition in the visual domain.\n\nThe human emotions can be expressed in various modalities, out of which speech & image express the emotional intentions most effectively  [12] . Analysis in a single modality may not be able to recognize the emotional context completely, which leads to the need for multimodal emotion recognition approaches that analyze multimodal audio-visual emotional context  [42] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Multimodal emotion analysis from audio-visual data has started getting researchers' attention lately  [6, 8, 15] . For instance, Siriwardhana et al.  [34]  fine-tuned Transformers-based models to improve the performance of multimodal speech emotion recognition. Multimodal emotion recognition has been carried out for text & speech modalities  [14, 19]  and text & image modalities  [5, 15, 37] . However, it has not been fully explored for speech & image modalities. Moreover, most deep learning-based multimodal emotion recognition systems work as a black box where it is difficult to interpret their inside mechanism. It inspired us to develop an interpretable multimodal emotion recognition system for speech & image modalities.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Interpretability Of Deep Neural Networks",
      "text": "The existing interpretability approaches compute each input feature's importance by backpropagating the network or observing the changes in output on changing the input  [17] . In this direction, Riberio et al.  [29]  explained a network based on each input's importance. Researchers have explained the layer-by-layer learning of deep neural networks and the output based on all the neurons' contributions  [14, 32] . There are interpretability methods for visual analysis to compute input pixels' importance  [17, 20, 29] . However, such methods still need to be sufficiently explored for speech modality. It inspired us to develop an interpretability technique for multimodal emotion recognition to explain the importance of each speech segment and each visual feature of the input.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Construction",
      "text": "The 'IIT Roorkee Speech & Image Emotion Recognition (IIT-R SIER) dataset has been constructed using Balanced Twitter for Sentiment Analysis (B-T4SA) dataset  [37] .\n\nThe recent text-to-speech models generate high-quality audio that can be used as a valid approximation of natural audio signals  [21, 22, 25] . A pre-trained state-of-the-art text-to-speech model, DeepSpeech3  [25] , has been used to convert the text from the B-T4SA dataset to speech. The samples are manually cleaned by removing the corrupt and duplicate samples. Further, the following procedure has been followed to generate the ground-truth labels according to the overall emotional context represented by both modalities in combination.\n\nVarious parameters of the SIER dataset have been summarised in Fig.  1  whereas the procedure to construct the same has been described as follows.\n\nThe speech component of each data sample is passed through the SER model trained on The Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [2]  dataset; classification probabilities for each emotion class are obtained, and the maximum among the probabilities for all emotion classes is noted as max 1 . Likewise, each sample's image component is passed through the IER model trained on Flickr & Instagram (FI)  [41]  dataset, and the maximum of classification probabilities for all emotion classes, i.e., max 2 is noted. The higher of max 1 and max 2 is observed, and the corresponding emotion label is assigned as the ground-truth label to the data sample. For example, if IER model returned probabilities 0.1, 0.8, 0.05 and 0.05 for four emotion classes while SER model gave 0.1, 0.1, 0.7 & 0.1 then we assigned second emotion class to the sample considering max(0.8 and 0.7). The samples having max(max 1 , max 2 ) less than a threshold of 0.5 are discarded as the predicted class label must be at least double confident than random prediction (probability 0.25). The samples labeled as 'excitement' & 'disgust' have been re-labeled as 'happy' & 'hate' as per Plutchik's wheel of emotions  [26] . The final dataset contains a total of 80,893 samples with 42,958 labeled as 'happy,' 13621 as 'sad', and 4401 & 19,913 as'hate' and 'anger' respectively.\n\nWe did not take the samples with the same predicted labels by SER and IER, as speech & image modalities might favor different emotion classes in isolation. In contrast, we are interested in the emotion class denoted by both modalities together. Samples for which SER and IER models predicted the same emotion label have been retained to form the IIT-R SIER dataset. The samples having the same predicted labels for SER and IER models denote high confidence in both modalities. They have been retained irrespective of whether the labels are correct. This approach is inspired by the B-T4SA dataset's base paper, where the samples having high confidence in text and image modalities are kept while others are discarded  [37] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Human Evaluation",
      "text": "We had two human readers (one male and one female) who spoke out and recorded the text components of the data samples. The evaluators listened to the machinesynthesized and human speech recorded by the human readers and labeled the emotion classes portrayed by them. The samples have been picked randomly, and the average of the evaluators' scores has been reported in Table  1 . Here, A i denotes the emotion classification accuracy when the human evaluators predicted the emotions considering the image components. Likewise, A ss & A hs are the accuracy values on considering the synthetic and human speech components, and A ss-i & A hs-i are the accuracies on considering both speech and image modalities. The following two major observations can be drawn from Table  1 : i) The similar values of 74.49% for synthetic speech and 78.91% for human speech advocate that the speech component of the data generated through text-to-speech is mature enough and embodies the appropriate emotional context. ii) Considering complementary information from both speech and image modalities led to higher emotion recognition performance. The evaluators also reported that 78.93% of the samples considering machine-synthesized speech along with the corresponding image were in line with the determined emotion label, whereas this is comparable to the value of 80.46% on considering human speech along with the corresponding image with is significantly higher than the accuracies on considering only image or only speech components.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Proposed Multimodal Emotion Recognition System",
      "text": "Fig.  2  depicts the architecture of the proposed multimodal emotion recognition system, which is determined in Section 4.3 through the ablation studies. A hybrid of intermediate and late fusion is implemented where intermediate fusion combines various modalities' information before classifying, while late fusion fuses the results after classification. The input image is in the space domain. The speech has been converted from the time domain to a log-mel spectrogram, i.e., the space domain. The proposed system contains networks N 1 and N 2 and dense, multiply, weighted addition, and softmax layers. N 1 uses convolution & max-pool layers while N 2 uses pre-trained networks VGG16 and VGG19  [33] . Both of these networks contain batch-normalization, flattened, and dense layers.\n\nThe intuition behind our architecture was to include a mechanism somehow So that each modality affects the other while making predictions. Here the two modalities are combined in two ways:-intermediate fusion and late fusion. First of all, to bring both modalities in the same domain audio signal is converted to a log-mel spectrogram to convert it from the time to space domain. Now, let us consider two networks, N1 and N2. N1 consists of a pre-trained network, than a batch normalization layer, a flattening layer, and a dense layer of 512 neurons. While N2 has the following architecture: First, two convolution layers have 64 filters, then a max-pooling layer, then two more convolution layers of 128 filters, then a max-pooling layer again, comes two more",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Intermediate Fusion Phase",
      "text": "Consider the networks fed with the image input to be N 1 i and N 2 i while the networks N 1 s and N 2 s process the speech input. The speech is expressed as a spectrogram of size (128, 128, 1) and passed first to a convolution layer having three convolutional filters of size (1, 1) each and then to N 2 s where a pre-trained VGG16 network is used. The image with size (128, 128, 3) is passed to N 2 i that uses a pre-trained VGG19 network. As shown in Eq. 1, the output of N 1 s is added with the output of N 2 i to get F s . Likewise, the outputs of N 1 i and N 2 s are added to obtain F i . Then F s and F i are element-wise multiplied to obtain F mul .\n\nThe choice of using multiplication instead of weighted addition in Eq. 1 to combine F s and F i in the low-level fusion has been determined experimentally. Moreover, theoretically, if the speech and image modalities predict the same emotion class, they should support each other. However, let's consider a case where one modality predicts i th emotion very strongly while another predicts another emotion j th weakly. We expect that the i th emotion should be predicted weakly. It would not have been the case in the case of using addition, and the i th emotion would have the upper hand. In comparison, the multiplication of both modalities would dilute the assertive behavior of the i th emotion and give us the expected prediction.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Late Fusion Phase",
      "text": "The intermediate outputs F i , F s , and F mul are passed from three dense layers of size 1024, 1024, and 4 to obtain O sp for speech, O img for image, and O mul for multiplied. These outputs are combined using the weighted addition layer as per Eq. 2 in a late fusion manner and passed from a softmax layer to get the final predicted label, ŷ. The weights w 1 , w 2 , and w 3 are randomly initialized and passed to a softmax layer to normalize them to non-negative values. Their final values are learned using the Gradient Descent algorithm. It combines the information from speech & image modalities and the output of intermediate fusion in a hybrid manner.\n\n(2)",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Proposed Interpretability Technique",
      "text": "While making predictions, a deep learning-based classifier is expected to consider the input features that a human would consider. However, it is challenging to look into it and understand what input features it is considering  [29] . To work on this challenge, we have developed an interpretability technique based on 'shapely values'  [17]  that denotes each input feature's importance. Theoretically, shapely values' computation takes exponential time. The computation has been approximated using the divide and conquer approach as shown in Eq. 3. For a model with two features f 1 and f 2 , shapely value S {f1} for feature f 1 denoting its importance is computed as follows.\n\nHere, M C f1,{f1} is feature f 1 's marginal contribution to the model containing only f 1 and given by Eq. 4 where score {f1} denotes the prediction for the ground-truth label using the model with feature f 1 .\n\nThe respective speech and image inputs are segregated and fed into the model while keeping the other as zero to compute the individual contribution of each modality. As depicted in Fig.  3 , each modality's input is divided into two parts for a specified number of times, and the importance of each part towards the model's prediction is computed as per Eq. 2. Moreover, the calculation of the importance score follows the basic requirement of shapely values given by Eq. 5.\n\nThe important image features for the predictions can be directly observed through the shapely values. In contrast, the important speech features are analyzed after transforming them to wave, i.e., time-domain representation. We first applied the shapely values directly and converted the spectrogram to speech; however, the speech reconstructed by this method was not meaningful. Then, we used the method of averaging the shapely values along the frequency axis and reducing them to the time axis to find the features' importance at a given time. The speech segments below a threshold shapely values of 30 percentile have been reduced to zero. The leftover segment with high importance is converted to text using speech-to-text model  [3]  and interpreted to understand how the model classifies each instance. The proposed interpretability technique has been summarised in Algorithm 1.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "The proposed system's network has been trained using Nvidia Quadro P5000 Graphics Card, whereas 64 bit Core(TM) i7-8700 Ubuntu system with 3.70 GHz 16GB RAM has been used for model evaluation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training Strategy",
      "text": "The model has been trained using a batch size of 64, a train-test split of 70-30, 5-fold cross-validation, Adam optimizer, ReLU activation function with a learning rate of 8 × 10 -6 . The baselines and proposed models converged regarding validation loss in 18-23 epochs. The models have been trained for 30 epochs as a safe upper bound. A weighted combination of categorical cross entropy with weights 1 and 0.5 and categorical focal loss  [16]  has been used as the loss function. EarlyStopping and ReduceLROnPlateau have been incorporated with patience values 5 and 2. Accuracy, macro f1  [23] , and CohenKappa  [38]  have been analyzed for evaluation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies And Models",
      "text": "The following studies analyze the effect of using multimodal information and various network configurations.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Effect Of Multiple Modalities",
      "text": "We first worked on SER and IER alone, using only speech samples and images from the IIT-R SIER dataset. Then we combined the information from speech & image modalities and performed multimodal emotion recognition. The IER-only experiments demonstrated high training but low validation accuracy. The convergence of accuracy and f1 score was not in line, and CohenKappa metric's value was low, denoting overfitting for a particular class. The accuracy f1 score converged in line for SER-only experiments, though the accuracy was less.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Effect Of Various Network Configurations",
      "text": "As depicted in Fig.  2 , ParallelNet consists of a family of networks where N 1 and N 2 can be varied in different situations. We first keep N 2 fixed as EfficientNet  [36]  and evaluate three configurations for N 1 -Configuration 1 uses two criss-crosses before and after N 2. A criss-cross is a position combining two different modalities' networks. Configuration 2 & 3 implement single criss-cross before and after N 2. Three baseline models have been implemented in line with these configurations. Configuration 3 was chosen for final implementation as it shows in-line convergence & improved performance.\n\nFurther, keeping Configuration 3 fixed for N 1's configuration, following choices have been evaluated for N 2 -VGG  [33]  (VGG-16, VGG-19), ResNet  [7]  (ResNet-34, ResNet-50, ResNet-101), InceptionNet  [35]  (Inception 3a, Inception 4a), MobileNet  [9]  and DenseNet  [11] . The best performance has been observed with VGG16 as N 2 s and VGG19 as N 1 i , which have finally been implemented by the 'ParallelNet.' The baseline & proposed models determined through the aforementioned studies are listed below, and their performance in terms of validation accuracies have been summarized in Table  2 .\n\n• Baseline 1 -N 1: Two criss-cross, N 2: EfficientNet. It divides N 1 into two parts and uses two criss-crosses before and after N 2. A criss-cross is a position combining two different modalities' networks.\n\n• Baseline 2 -N 1: Criss-cross before N 2; N 2: EfficientNet.\n\n• Baseline 3 -N 1: Criss-cross after N 2; N 2: EfficientNet.\n\n• Proposed -N 1: Criss-cross after N 2; N 2: VGG.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results And Discussion",
      "text": "The emotion classification results have been discussed in this Section, along with their interpretation and a comparison of sentiment classification results with existing methods.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Quantitative Results",
      "text": "The 'ParallelNet' has achieved emotion recognition accuracy of 83.29%. Its class-wise accuracies are shown in Fig.  4 .  Fig.  5 : Sample results; here, 'P', 'GT' and 'Score' denote the predicted label, ground-truth label and softmax score.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Qualitative Results",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Results Comparison",
      "text": "Comparison with existing Sentiment Analysis methods: The emotion recognition results have been reported in Section 5.1. The IIT-R SIER dataset has been constructed from the B-T4SA dataset in this paper; hence, there are no existing emotion recognition results for it. However, sentiment classification (into neutral, negative, and positive classes) results on the B-T4SA dataset are available in the literature, which has been compared with the proposed method's sentiment classification results in Table  3 . Comparison with human evaluation: On considering the multimodal context from image and speech modalities, the human evaluation (See Table  1 ) and automatic evaluation (using ParallelNet. See Table  2 ) resulted in emotion classification accuracies of 80.46% and 89.68% respectively. In both cases, the emotion classification performance improved on considering the multimodal context compared to considering only image or speech modality. It establishes the importance of considering complementary information from multiple modalities for emotion recognition.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Discussion",
      "text": "The proposed system classifies a given multimodal input containing speech & the corresponding image into 'anger,' 'happy,' 'hate,' and 'sad' classes. The proposed interpretability technique identifies the important speech & image features contributing to emotion recognition. An alternate procedure to construct the IITR-SIER dataset was to retain only those samples from the BT4SA dataset for which SER & IER models predicted the same label and discard the rest of the samples. However, it would have caused a bias towards the models used in the first place for creating these labels. The SER & IER models have been retrained on the IITR-SIER dataset instead of using the pre-trained weights of the models used to construct the IITR-SIER dataset. However, suppose somebody uses the pre-trained models of either one of the two modalities (trained on IEMOCAP and Flickr & Instagram datasets, respectively) used during dataset construction. In that case, they will get a 100% accuracy. The closest to them, any other evaluated machine learning model is, the more favorable its evaluation would be. That's why the proposed procedure of considering the prediction probabilities for all emotion classes is more effective in capturing the overall emotional context represented by both modalities in combination. It leads to generating more accurate ground-truth labels.\n\nThe ParallelNet's architecture has been determined through extensive ablation studies. It consists of a family of networks where N 1 and N 2 can be varied in different situations. We've first determined the optimal configuration for N 1 to combine speech & image modalities' information. Further, VGG, ResNet, InceptionNet, MobileNet, and DenseNet have been evaluated for N 2. The best performance has been observed with VGG. The ResNet depicted very slow learning for a lower learning rate, while the learning fluctuated significantly for a higher learning rate. The model converged faster for the Inception Net and Efficient Net; however, the accuracy is lower. MobileNet and DenseNet have also resulted in low performance.\n\nApart from the experimental validation in Table  2 , Fig.  5  qualitatively re-affirms the importance of combining complementary information from multiple modalities for more accurate emotion recognition. In the first & second cases, the image and speech features (shown by yellow parts of the waveform and denoted by corresponding words in blue) contribute to predicting the emotion class 'sad.' In the third & fourth cases, the image features have not been precisely captured, and the images seem neutral. However, the corresponding speech features not filling and killer contribute towards hatred and anger intent, which leads to recognizing the 'hate' and 'anger' classes.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "The importance of utilizing information from multiple modalities has been established for emotion recognition. The proposed system, ParallelNet, has resulted in better performance than SER alone, IER alone, and baseline models. The proposed interpretability technique identifies the important image & speech features contributing to emotion recognition.\n\nFuture research plans include working on emotion recognition in other modalities such as text, videos, and emotion signal data. It is also planned to explore the interpretability of emotion recognition in the aforementioned modalities.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Summary of IIT-R SIER dataset. Left: Class-wise data samples distribution.",
      "page": 5
    },
    {
      "caption": "Figure 2: depicts the architecture of the proposed multimodal emotion recognition system,",
      "page": 6
    },
    {
      "caption": "Figure 2: Architecture of the proposed system (top), N1 (bottom left) and N2 (bottom",
      "page": 7
    },
    {
      "caption": "Figure 3: Proposed interpretability technique’s illustration. Here, each part’s importance",
      "page": 8
    },
    {
      "caption": "Figure 3: , each modality’s input is divided into two parts for a speciﬁed",
      "page": 9
    },
    {
      "caption": "Figure 2: , ParallelNet consists of a family of networks where N1 and N2",
      "page": 11
    },
    {
      "caption": "Figure 4: Confusion matrix showing class-wise accuracies.",
      "page": 12
    },
    {
      "caption": "Figure 5: shows sample emotion classiﬁcation & interpretation results. The important",
      "page": 12
    },
    {
      "caption": "Figure 5: Sample results; here, ‘P’, ‘GT’ and ‘Score’ denote the predicted label,",
      "page": 13
    },
    {
      "caption": "Figure 5: qualitatively re-afﬁrms",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "93.74\n2.86\n0.72\n2.68": "10.56\n82.12\n2.45\n4.87\n12.41\n8.54\n72.55\n6.49"
        },
        {
          "93.74\n2.86\n0.72\n2.68": "21.16\n12.52\n2.42\n63.89"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
      "authors": [
        "Tadas Baltrušaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal Machine Learning: A Survey and Taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive Emotional dyadic MOtion CAPture data. Language Resources and Evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive Emotional dyadic MOtion CAPture data. Language Resources and Evaluation"
    },
    {
      "citation_id": "3",
      "title": "Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition",
      "authors": [
        "William Chan",
        "Navdeep Jaitly",
        "Quoc Le",
        "Oriol Vinyals",
        "Listen"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Learning Discriminative Features from Spectrograms using Center Loss for SER",
      "authors": [
        "Dongyang Dai",
        "Zhiyong Wu",
        "Runnan Li",
        "Xixin Wu",
        "Jia Jia",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "A Multimodal Approach to Image Sentiment Analysis",
      "authors": [
        "António Gaspar",
        "Luís A Alexandre"
      ],
      "year": "2019",
      "venue": "Springer International Conference on Intelligent Data Engineering and Automated Learning (IDEAL)"
    },
    {
      "citation_id": "6",
      "title": "Multimodal Emotion Recognition by Fusing Correlation Features of Speech-Visual",
      "authors": [
        "Chen Guanghui",
        "Zeng Xiaoping"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "7",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "8",
      "title": "Emotion Recognition using Deep Learning Approach from Audio Visual Emotional Big Data",
      "authors": [
        "M Shamim",
        "Ghulam Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "9",
      "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "authors": [
        "Menglong Andrew G Howard",
        "Bo Zhu",
        "Dmitry Chen",
        "Weijun Kalenichenko",
        "Tobias Wang",
        "Marco Weyand",
        "Hartwig Andreetto",
        "Adam"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "10",
      "title": "Multimodal Sentiment Analysis to Explore the Structure of Emotions",
      "authors": [
        "Anthony Hu",
        "Seth Flaxman"
      ],
      "year": "2018",
      "venue": "ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)"
    },
    {
      "citation_id": "11",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "12",
      "title": "Building Emotional Machines: Recognizing Image Emotions through Deep Neural Networks",
      "authors": [
        "Hye-Rin Kim",
        "Yeong-Seok Kim",
        "Seon Joo Kim",
        "In-Kwon Lee"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia (T-MM)"
    },
    {
      "citation_id": "13",
      "title": "End-to-End Triplet Loss based Emotion Embedding System for Speech Emotion Recognition",
      "authors": [
        "Puneet Kumar",
        "Sidharth Jain",
        "Balasubramanian Raman",
        "Partha Roy",
        "Masakazu Iwamura"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "14",
      "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
      "authors": [
        "Puneet Kumar",
        "Vishesh Kaushik",
        "Balasubramanian Raman"
      ],
      "year": "2021",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Hybrid Fusion based Approach for Multimodal Emotion Recognition with Insufficient Labeled Data",
      "authors": [
        "Puneet Kumar",
        "Vedanti Khokher",
        "Yukti Gupta",
        "Balasubramanian Raman"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "16",
      "title": "Focal Loss for Dense Object Detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "17",
      "title": "A Unified Approach to Interpreting Model Predictions",
      "authors": [
        "M Scott",
        "Su-In Lundberg",
        "Lee"
      ],
      "year": "2017",
      "venue": "The 31st International Conference on Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "18",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "19",
      "title": "Multimodal Emotion Recognition with High-level Speech and Text Features",
      "authors": [
        "Mariana Rodrigues Makiuchi",
        "Kuniaki Uto",
        "Koichi Shinoda"
      ],
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "20",
      "title": "Towards Interpretable Facial Emotion Recognition",
      "authors": [
        "Sarthak Malik",
        "Puneet Kumar",
        "Balasubramanian Raman"
      ],
      "year": "2021",
      "venue": "The 12th Indian Conference on Computer Vision, Graphics and Image Processing"
    },
    {
      "citation_id": "21",
      "title": "Wavenet: A Generative Model for Raw Audio",
      "authors": [
        "Deep Mind"
      ],
      "year": "2016",
      "venue": "Wavenet: A Generative Model for Raw Audio"
    },
    {
      "citation_id": "22",
      "title": "Generative Model for Raw Audio",
      "authors": [
        "Aaron Van Den Oord",
        "Sander Dieleman",
        "Heiga Zen",
        "Karen Simonyan",
        "Oriol Vinyals",
        "Alex Graves",
        "Nal Kalchbrenner",
        "Andrew Senior",
        "Koray Kavukcuoglu",
        "Wavenet"
      ],
      "year": "2016",
      "venue": "Generative Model for Raw Audio",
      "arxiv": "arXiv:1609.03499"
    },
    {
      "citation_id": "23",
      "title": "Macro f1 and Macro f1",
      "authors": [
        "Juri Opitz",
        "Sebastian Burst"
      ],
      "year": "2019",
      "venue": "Macro f1 and Macro f1",
      "arxiv": "arXiv:1911.03347"
    },
    {
      "citation_id": "24",
      "title": "Multimodal Multitask Emotion Recognition Using Images, Texts and Tags",
      "authors": [
        "Pagé Mathieu",
        "Brahim Fortin",
        "Chaib-Draa"
      ],
      "year": "2019",
      "venue": "ACM Workshop on Cross-modal Learning and Application"
    },
    {
      "citation_id": "25",
      "title": "DeepVoice 3: Scaling Text-to-Speech with Convolutional Sequence Learning",
      "authors": [
        "Wei Ping",
        "Kainan Peng",
        "Andrew Gibiansky",
        "O Sercan",
        "Ajay Arik",
        "Sharan Kannan",
        "Jonathan Narang",
        "John Raiman",
        "Miller"
      ],
      "year": "2018",
      "venue": "The 6th Int. Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "26",
      "title": "The Nature of Emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "Journal Storage Digital Library's American scientist Journal"
    },
    {
      "citation_id": "27",
      "title": "A Review of Affective Computing: From Unimodal Analysis to Multimodal Fusion. Elsevier Information Fusion Journal",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "A Review of Affective Computing: From Unimodal Analysis to Multimodal Fusion. Elsevier Information Fusion Journal"
    },
    {
      "citation_id": "28",
      "title": "Learning Multi-level Deep Representations for Image Emotion Classification",
      "authors": [
        "Tianrong Rao",
        "Xiaoxu Li",
        "Min Xu"
      ],
      "year": "2019",
      "venue": "Learning Multi-level Deep Representations for Image Emotion Classification"
    },
    {
      "citation_id": "29",
      "title": "Why Should I Trust You? Explaining Predictions of Any Classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "International Conference on Knowledge Discovery & Data mining (KDD)"
    },
    {
      "citation_id": "30",
      "title": "Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification",
      "authors": [
        "Justin Salamon",
        "Juan Bello"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "31",
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "The IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "32",
      "title": "Learning Important Features Through Propagating Activation Differences",
      "authors": [
        "Avanti Shrikumar",
        "Peyton Greenside",
        "Anshul Kundaje"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "33",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "34",
      "title": "Jointly Fine Tuning 'BERT-Like' Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis",
        "Rivindu Weerasekera"
      ],
      "year": "2020",
      "venue": "Jointly Fine Tuning 'BERT-Like' Self Supervised Models to Improve Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "35",
      "title": "Going Deeper with Convolutions",
      "authors": [
        "Christian Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "Pierre Sermanet",
        "Scott Reed",
        "Dragomir Anguelov",
        "Dumitru Erhan",
        "Vincent Vanhoucke",
        "Andrew Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "36",
      "title": "EfficientNet: Rethinking Model Scaling for CNN",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "37",
      "title": "Cross-Media Learning for Image Sentiment Analysis in the Wild",
      "authors": [
        "Lucia Vadicamo",
        "Fabio Carrara",
        "Andrea Cimino",
        "Stefano Cresci",
        "Felice Dell'orletta",
        "Fabrizio Falchi",
        "Maurizio Tesconi"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision Workshops (ICCV-W)"
    },
    {
      "citation_id": "38",
      "title": "Cohen's Kappa Coefficient as a Performance Measure for Feature Selection",
      "authors": [
        "Susana Vieira",
        "Uzay Kaymak",
        "João Mc Sousa"
      ],
      "year": "2010",
      "venue": "International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "39",
      "title": "Improve Accuracy of Speech Emotion Recognition with Attention Head Fusion",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Samee U Khan"
      ],
      "year": "2020",
      "venue": "IEEE Annual Computing and Communication Workshop and Conference (CCWC)"
    },
    {
      "citation_id": "40",
      "title": "Speech Emotion Recognition Using Spectrogram & Phoneme Embedding",
      "authors": [
        "Promod Yenigalla",
        "Abhay Kumar",
        "Suraj Tripathi",
        "Chirag Singh",
        "Sibsambhu Kar",
        "Jithendra Vepa"
      ],
      "year": "2018",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "41",
      "title": "Building a Large Scale Dataset for Image Emotion Recognition: the Fine Print and the Benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "The 30th AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "42",
      "title": "A Survey of Affect Recognition: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)",
      "authors": [
        "Zhihong Zeng",
        "Maja Pantic",
        "Thomas Glenn I Roisman",
        "Huang"
      ],
      "year": "2009",
      "venue": "A Survey of Affect Recognition: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)"
    }
  ]
}