{
  "paper_id": "2206.07593v1",
  "title": "Hicem: A High-Coverage Emotion Model For Artificial Emotional Intelligence",
  "published": "2022-06-15T15:21:30Z",
  "authors": [
    "Benjamin Wortman",
    "James Z. Wang"
  ],
  "keywords": [
    "Modeling human emotion",
    "basic emotions",
    "emotion theory",
    "statistical clustering",
    "natural language"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As social robots and other intelligent machines enter the home, artificial emotional intelligence (AEI) is taking center stage to address users' desire for deeper, more meaningful human-machine interaction. To accomplish such efficacious interaction, the next-generation AEI need comprehensive human emotion models for training. Unlike theory of emotion, which has been the historical focus in psychology, emotion models are a descriptive tools. In practice, the strongest models need robust coverage, which means defining the smallest core set of emotions from which all others can be derived. To achieve the desired coverage, we turn to word embeddings from natural language processing. Using unsupervised clustering techniques, our experiments show that with as few as 15 discrete emotion categories, we can provide maximum coverage across six major languages-Arabic, Chinese, English, French, Spanish, and Russian. In support of our findings, we also examine annotations from two large-scale emotion recognition datasets to assess the validity of existing emotion models compared to human perception at scale. Because robust, comprehensive emotion models are foundational for developing real-world affective computing applications, this work has broad implications in social robotics, human-machine interaction, mental healthcare, and computational psychology.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A S far back as Darwin, researchers have studied the subjectivity and universality of human emotions  [1] . While such research was primarily limited to academic discussions in university psychology departments, with the rise of in-home social robots and other intelligent machines (e.g., Alexa, Astro), has expanded this subject matter into the field of affective computing where developing an accurate model of human emotion is a stepping stone toward artificial emotional intelligence (AEI)  [2] . Here, emotion modeling is a descriptive tool used to ensure that systems being developed have sufficient coverage for a wide range of human-machine or human-robot interactions.\n\nIdeally, a robust model with sufficient coverage means identifying the smallest core set of independent human emotions from which all other emotions can be derived. If the emotion model used in an AEI program has too many components, the AEI may struggle to distinguish among these components. On the other hand, if the model used is too simple, the AEI may not be able to understand human emotion to a level necessary for the intended application. Some researchers used the VAD dimensional model to circumvent this problem  [3] , but such an approach is not suitable for many AEI applications.\n\nDeveloping an ideal emotion model for AEI is a complex problem. Existing models either do not provide enough coverage  [4]  or include excessive, overlapping labels to describe the space  [5] ,  [6] ,  [7] . We provide a visual representation of an emotion model's coverage so the power of different models can be compared (Fig.  1 ). These are generated by taking the FastText word vectors  [9]  for 1,720 emotion concepts and projecting them down to two dimensions using Uniform Manifold Approximation and Projection (UMAP)  [10] . This dimensionality reduction technique is similar to t-SNE  [11] , but UMAP has also been shown to help preserve global relationships. We then generate the heatmaps using the maximum log cosine similarity between the FastText word vectors for the model and our emotion-concepts list.  1 This step allows us to visualize the coverage of each emotion model in relation to our emotion concepts list. More details will be provided later.\n\nTo overcome the limitations in existing models and understand the full range of human emotion, we turn to natural language processing (NLP). Language has evolved to be the principal means of human communication and has been shown to influence our perception of the world  [12] ,  [13] . By examining emotion-related words across cultures, we can identify trends and develop more robust, universal emotion models for next-generation affective computing applications. Previous work has taken similar approaches by having groups manually annotate existing emotions across continuous affective dimensions such as the valence, arousal, and dominance (VAD) space  [14] . However, word embeddings popular in NLP are now able to encode this information automatically. Understanding this advancement, we leverage statistical techniques to identify the minimum number of components with maximum coverage across multiple cultures. We propose a new emotion model, named the HIgh-Coverage Emotion Model (HICEM), to provide higher coverage with fewer components compared with existing emotion models popular in psychology used for affective computing. Using two separate evaluation metrics, Here we visualize the maximum log cosine similarity (with a ceiling of 0.5) between the word vectors of 1,720 emotion concepts and the contents of the model. A higher cosine similarity (yellow) means the contents of the model have a similar semantic meaning to the given concept. Let k denote the number of labels or components in an emotion model. We can see Ekman's basic emotions  [4]  (k=7) are insufficient to describe the space. Plutchik's wheel of emotion  [5]  (k=32) is better, but its labels contain a significant amount of overlap. Using unsupervised techniques, our proposed HICEM with 15 components, named HICEM-15, minimizes the overlap between labels and provides almost the exact same coverage as Plutchik's with half as many components. For comparison we also include Cowen's emotions identified in video  [6]  (k=27), the annotation categories for the GoEmotions Dataset  [7]  (k=28), and the EMOTIC  [8]  dataset annotation scheme (k=27).\n\nwe show HICEM is able to achieve this goal. In support of this assertion, we also analyze the results from recent largescale emotion recognition datasets to assess the validity and coverage of existing discrete and continuous emotion models.\n\nThe main contributions of our work include:",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We provide to the affective computing and AEI communities a new high-coverage emotion model, named HICEM, which contains more information with fewer labels than existing emotion models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We developed a principled framework to quantify the quality of existing emotion models across different languages using two new metrics and a list of 1,720 emotion concepts. We evaluated existing models with six major languages recognized by the UN.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We developed a new data-driven approach, leveraging the annotations of existing large-scale emotion recognition datasets, to assess the validity of existing emotion models in relation to human perception at scale.\n\nThe rest of the paper is organized as follows. We cover related work in emotion modeling in Section 2. Section 3 describes our methods for generating a high-coverage crosscultural model of emotion, the HICEM. The methodology and analysis of existing large-scale emotion recognition datasets are shown in Section 4. We discuss our results and identify future areas of interest in Section 5 and conclude in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "There are three competing schools of thought on emotion: basic emotions, continuous models, and componential models. In this section, we briefly discuss these approaches in relation to affective computing and AEI.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Basic Emotion Theory",
      "text": "Basic emotion theory suggests that humans evolved a set of discrete, independent emotions which when triggered produce a physiological response or action tendency. From these basic emotions, all other human emotions can be derived. As shown in Table  1 , these basic emotions are often used as categorical labels in affective computing datasets. More specifically, Paul Ekman's research into basic universal emotions serves as the foundation for most annotation schemes currently used  [8] ,  [15] ,  [16] ,  [17] ,  [18] ,  [19] . His original research identified six emotions universally recognizable by their facial expression  [4] . They are fear, anger, joy, sadness, disgust, and surprise. However, several studies  [20] ,  [21] ,  [22]  suggest facial expressions alone are insufficient to differentiate emotions. Since it has been demonstrated that body language cues are also universal across cultures  [23] , there may exist a subset of emotions that are universal for body language while being indistinguishable in facial expressions alone  [24] . Although not shown to be cross-cultural, analysis by Cowen et al. on perceived emotions from vocalization  [25] , facial expressions  [26] , and perceived emotion from video  [6]  suggests not six but more than 24 emotion categories are required to adequately map the space. However, this list was limited in that the label space was predetermined by the researchers. In attempting to develop an emotion model for text classification, Demszky et al. expanded upon Cowen et al.'s work by using user-submitted labels to augment their emotion model. These labels were then pruned and refined to generate a more annotator-friendly list of 27 emotions and a neutral category  [7] .\n\nAlthough Ekman's basic emotions are the most commonly used in affective computing, other models do exist. In taking an evolutionary inspired approach, Plutchik proposes an alternative to Ekman's model which consists of eight primary affective states arranged to form a wheel of emotion  [5] . Each of these affective states has varying degrees of intensity and when combined form more complex human emotions. Although a useful tool, this model is criticized as being too simplistic and hasn't been shown to have a strong empirical foundation  [28] . Compared with Plutchik's palette theory, Jaak Panksepp took a biological approach to understanding emotion. His work pioneered the field of affective neuroscience which works to map specific regions of the brain to emotional experience  [29] ,  [30] ,  [31] . In his original work, he describes seven affective systems common across mammalian brains which control specific types of behaviours and generate distinct emotional states  [32] . He describes these structures as the \"core-SELF.\" 2  Despite its neurological underpinnings, Panksepp's model hasn't been widely used in the affective computing community.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Continuous Models",
      "text": "Recognizing the limits of discrete labels for human emotions, some researchers have worked to define continuous dimensions to measure a person's affective state. As shown in Table  1 , annotations along continuous dimensions are often used together with basic emotions. In the simplest case, such annotations simply means labeling a sample based on how positive or negative it is. This dimension is usually described as the sentiment, pleasure, or valence of the sample. Expanding beyond one dimension, the Circumplex of Affect by Russel considers arousal (relaxed vs. aroused) and valence (pleasant vs. unpleasant) as the two fundamental dimensions which together provide a mapping for the discrete emotions  [33] . There is strong support for the two-dimensional approach of the Affective Circumplex. These two dimensions appear across a wide range of studies  [34] ,  [35] ,  [36] . Similar to Panksepp's mapping of discrete emotions, there has also been a considerable amount of work mapping valence and arousal to processes in the human brain  [37] ,  [38] ,  [39] ,  [40] .\n\nFor three dimensions, another popular model comes from the researchers Mehrabian et al. who described the emotion space across pleasure-displeasure, arousalnonarousal, and dominance-submissiveness (PAD 3 )  [14] . This mirrors earlier work by Osgood et al., who considered the closely related concept of control instead of dominance  [41] . Here, control can be thought of in terms of both the feelings of power or weakness in addition to interpersonal dominance or submission. With regards to the PAD model, other proposed dimensions include anticipation-expectation, anxiety-confidence, boredomfascination, frustration-euphoria, terror-enchantment, and intensity (how far the person is from a state of pure, cool rationality)  [42] ,  [43] ,  [44] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Componential Models",
      "text": "In contrast with discrete basic emotions and the previously described continuous models, some work has led to componential models derived from the appraisal theory of emotion  [43] ,  [45] ,  [46] ,  [47] . Under this framework, emotion is a dynamic process thought to result from a person's repeated evaluation (appraisal) of their circumstances  [48] ,  [49] . This perspective has an advantage over both descriptive models leveraging basic emotions and continuous dimensions because it provides an explanation for why an emotion presents itself. Since these componential models rely heavily on subjective experience  [43] ,  [50] , outside of lab constrained experiments  [51] ,  [52] , they have not been widely adopted for use in affective computing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross Cultural Word Embeddings",
      "text": "NLP provides an interesting avenue for research into emotion modeling. By examining how current annotation schemes relate to other emotion-related words, we can take a quantitative approach toward identifying gaps in existing models. In this section, we outline our methods for generating the HIgh Coverage Emotion Model, or HICEM, from NLP word embeddings.\n\n3. Valence and pleasure are often used interchangeably and sometimes referred to as VAD for valence, arousal, and dominance. Then we perform dimensionality reduction and clustering to generate summary words. This process was repeated to create summary words for six major languages. Finally, these were once again clustered to produce our final model, the HICEM.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Generating A List Of Emotion-Concepts",
      "text": "As shown in Fig.  2 , we first compiled a list of emotionrelated concepts from various models  [1] ,  [4] ,  [5] , online sources  [53] ,  [54] ,  [55] , and the Semantic Atlas of Emotional Concepts  [56] . This list is then passed through a pre-trained Word2Vec model  [57] , which encodes the semantic meaning of a word into a 300-dimensional vector and allows us to perform operations across these embeddings to identify relationships between words. A popular example of this process is\n\nThat is, if we take the vector for \"king,\" subtract the vector for \"man,\" and then add the vector for \"woman,\" the resulting output vector is approximately equal to \"queen.\" For each pairwise combination of words in this list, we append synonyms based on the cosine similarity of the average of their Word2Vec vectors. For example, given \"happy\" and \"sad\" we would be able to identify \"bittersweet\" because its vector is approximately the average of the two. That is,\n\nThis expanded set was manually pruned to remove adverbs and words unrelated to a person's emotional state. For example, words such as \"terrorist\" and \"terrorism\" are closely related to the pairwise combination \"terror\" and \"anger,\" but these have little to do with emotion so they are removed from the final expanded list. Likewise, there was a small subset of words typically used in religious contexts ('glee', 'woe', 'joy', 'awe') which had an extremely high cosine similarity between their word vectors. This had an adverse effect on clustering so they were removed. Although, alternate forms used in different contexts remained ('gleeful', 'woeful', 'joyful', 'awed'). Because the objective of this study was to identify gaps in existing emotion models for affective computing tasks, we took an inclusive approach and kept words that may not qualify classically as emotions but are still concepts and dispositions that influence a per-son's expressions  [31] ,  [58] . In total the final list contained 1,720 emotion-related keywords.  4",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Umap Reduction",
      "text": "Using the list of 1,720 emotion concepts, a pre-trained FastText model  [59]  is used to encode the semantic meaning of the word. FastText is a variant of Word2Vec that operates at the n-gram level which allows for the use of subword information to improve the quality of the embeddings  [60] . Trained on Common Crawl and Wikipedia, this model provides a 300-dimensional embedding for each word. FastText was chosen over more advanced techniques such as BERT  [61]  due to its location invariance and its use of the same methodology to generate vectors for multiple languages  [9] . BERT embeddings vary based on the location of the word within the text. In our testing, BERT produced poor results when feeding individual words to the model.\n\nSince word embeddings are generated based on their local context, antonym word pairs (e.g., Happiness/Sadness) which are commonly used in the same context may have high cosine similarities relative to their perceived similarity. To overcome this, we use Uniform Manifold Approximation and Projection (UMAP)  [10] . This dimensionality reduction technique is similar to t-SNE  [11]  but has the added benefit of maintaining the global relationships among samples. Because UMAP relies on a number of neighbors to generate the embedding, as long as a word has more true synonyms than similar-context antonyms, it will close the distance between the synonyms while increasing the distance between antonyms. This outcome is demonstrated in Table  2 , where UMAP increases the similarity between synonyms while pushing antonyms away. In the raw vectors, \"Sadness\" has the highest cosine similarity with \"Happiness\" despite the semantic meaning being quite different. Other dimensionality reduction techniques like principal component analysis (PCA), singular value decomposition (SVD), and tdistributed stochastic neighbor embedding (t-SNE) fail to produce accurate cosine similarities between synonyms and antonyms after reduction. Sentiment Fig.  3 . We compare four different dimensionality reduction methods using sentiment as a heuristic. Unlike t-SNE  [11] , UMAP  [10]  is able to separate and organize positive and negative emotion concepts. Compared with other methods, when we plot Ekman's basic emotions  [4]  in the UMAP embedding space the separation between \"Happiness\" and the negative basic emotions is increased. This result is also shown in Table  2  where we compare the cosine similarity between the raw FastText  [9]  word vectors and UMAP embeddings across synonyms and antonyms for \"Happiness.\" In Fig.  3 , we visualize the different embeddings with respect to the sentiment for each word using the Python Natural Language Toolkit (NLTK)  [62] . Although we don't use any sentiment information when generating the embeddings, all dimensionality reduction techniques are able to create a separation between positive and negative emotions. However, t-SNE is only able to achieve this locally and fails to provide any consistent global separation. Similar to the result in Table  2 , when we plot Ekman's basic emotions, we see much clearer separation between \"Happiness\" and Ekman's negative basic emotions in the UMAP embeddings than in other techniques.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Hierarchical Clustering",
      "text": "Agglomerative clustering  [63] ,  [64]  was used to programmatically select emotion words with the highest coverage. We chose agglomerative clustering over other methods because it mirrors the hierarchical nature of basic emotions  [32] ,  [65] . To do this, we first determine the optimum number of clusters using the elbow method  [66] . This automatically finds the \"elbow\" or \"knee\" which corresponds to point of maximum curvature on the plot of cluster distortion by clusters count  [67]  as shown in Fig.  4 . Finally, we summarize the contents of each group by finding the word embedding closest to the centroid of each cluster. The results of this step are shown in Fig.  5 .\n\nTo develop our cross-cultural model, we repeat this process by translating our list of emotion-related concepts  from English into the other official languages recognized by the United Nations (Arabic, Chinese, French, Spanish, and Russian) using Google's Translation API. 5 We then once again proceeded with Facebook's FastText models which have been trained on Common Crawl and Wikipedia for each of these languages  [9] . The translated UMAP embeddings for both Chinese and Russian needed adjustments due to limitations in translation. For Chinese, the embeddings originally formed two distinct clusters. This separation is caused by the inclusion of the nominalization particle \"的,\" which converts nouns or noun phrases into adjectives (e.g., \"快乐\" or happiness → \"快乐的\" or happy). An equivalent example in English would be the use of the suffix \"-ness\" such as in \"happiness.\" This suffix converts the adjective \"happy\" into a noun. Since FastText uses subword information to generate its word vectors, the semantic meaning of this character influences the final word embedding. When UMAP is run on these embeddings, the inclusion of this character creates enough separation between the word vectors that all words containing this character get clustered separately from the rest of the emotion-concepts list. To resolve this divide, this cluster was excluded from the analysis since the other was significantly larger and already contained a superset of Ekman's basic emotions. In Russian, there was a similar separation caused by a noun-adjective divide. Here, the noun cluster was excluded because it was roughly a quarter the size of the adjective cluster.\n\nWith the translated word vectors, we once again apply agglomerative clustering to each language. Since this process combines the two closest centroids in each iteration, this clustering can handle situations where one-to-one translations do not exist. As shown in Table  3 , we start experiencing diminishing returns in all languages around 15 clusters. This result provides quantitative evidence that all other emotionrelated words can be embedded into the semantic space of roughly 15 dimensions. An interesting finding from this analysis is that these clusters, broadly capture the basic emotions as described by Ekman (i.e., fear, anger, happiness/joy, sadness, disgust, and surprise) in each language. Likewise, there seem to be themes of humor, abnormal behavior, friendliness, anxiety, and confusion. To ensure our model has coverage across cultures, we then took the top 50 cluster summary words from each language and once again performed agglomerative clustering (k=15) to generate a list of cross-cultural summary words. As a final step, we then manually adjusted our emotion model by replacing rare or outdated language with more common English terms. This process provides us with the final list of 15 high coverage emotion concepts in Table  4 . We repeat this final agglomerative clustering step for 25 clusters (named HICEM-25) to provide an additional comparison with similar-sized emotion models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Assessing Performance",
      "text": "We can qualitatively assess the quality of this list, by projecting it into the English embeddings as shown in Fig.  1 . Because this list is uniformly distributed throughout the space, we can assume there is minimal overlap in its labels.\n\nIn addition to this, we also define a new metric \"coverage\" to provide a quantitative assessment of the quality of each model. This metric is based on cosine similarity between the words in the model and our emotion list as follows Here M is the set of FastText word vectors in an emotion model, and W -M is the set of vectors for the words from the set of 1,720 emotion concepts (the set W ) excluding the components of the emotion model (the set M ). We define n as the cardinality of set W -M . We chose to proceed with max cosine similarity since this gives an indication of how strong the top synonym is for a given emotion model and emotion-concept pair. As seen in Table  5  and Fig.  6 , our model provides higher coverage with fewer components compared with previous models.\n\nIn addition to analyzing the coverage, we also conducted an experiment to show how much emotion information is captured in each model. The intuition behind this experiment is that ideally, we should be able to recover a large number of emotional states for a sample using only the annotations from a given model. In contrast with coverage where we only consider the max cosine similarity, here we leverage the additional annotations across all the components to make a prediction on the emotion present. To do Fig.  6 . In this histogram of coverage by cosine similarity across English word vectors, we show that our HICEM-15 model is able to provide higher coverage with fewer labels compared to other models of emotion popular in psychology  [4] ,  [5] ,  [6] ,  [7] ,  [8] ,  [42] . In English, our model achieves an average similarity of 0.45. This result is equivalent to the similarity between \"happiness\" and \"calmness,\" \"merriment,\" or \"euphoria\" (0.45 ± 0.01).\n\nthis, we assume we have an oracle who, when given an emotion model M , is able to generate an embedding X for a given word vector w i using the cosine similarities between the FastText word vectors of each m ∈ M and w i . More formally, X = CosSim(w i , M ) .\n\nwhere G is a function we pass X through in order to make a prediction on the original word vectors ŵi . Then once again using cosine similarity, we compare the original word vector w i with our recovered prediction ŵi as follows:\n\nCosSim(w i , ŵi ) .\n\nIn practice, if we are to annotate the emotional expression of a character in a video clip using a categorical emotion model, this would be equivalent to a human annotator for the clip judging how similar or dissimilar the sample (i.e., the character's emotional expression) is to the labels of the given emotion model and then using those annotations to recover the ground-truth emotion present in the clip. There is often a level of disagreement among annotators in realworld settings which adds noise to the oracle's embedding X. However, the assumption of a perfect oracle provides a theoretical upper bound for the amount of recoverable information which we can use for comparison.\n\nIn our experiment, we use the Ridge regression  [68]  for G and a 50/50 train-test split on the list of 1,720 emotion concepts. We then train on the similarity embeddings for each model to make a prediction on the original embedding. To provide a baseline, we use random subsets of words from the emotion-concept list of varying sizes. As shown in Fig.  7  and Table  6 , HICEM is the only emotion model that performs above a random subset of the same size. There are several possible explanations for this. First, this model's success could be influenced by the presence of redundant  or overlapping labels. Second, because the random subsets were selected using a random uniform distribution, they are more likely to sample across the entire emotion concept space, in turn providing more unique information. Lastly, HICEM benefits from being biased towards the emotionconcept list it is being tested against. Although other models touch on concepts such as craving  [6]  and pain  [8] ,  [15] , these types of concepts have only recently been included in the discussion of emotion  [58] ,  [69] ,  [70] . Since these were included in the list of 1,720 emotion concepts, HICEM is better able to represent them since it is derived from this same list.\n\nIt is worth noting that in practice, there is a trade-off between the size of the emotion model and the amount of information the annotators will give for any sample. As emotion models become more complex and include more abstract concepts, the agreement between annotators decreases substantially. This reduction can be seen in the levels of inter-annotator agreement across several large-scale datasets that reported this information. For example, despite several quality-control measures implemented during data collection and the post-processing done to filter unreliable annotators in the BoLD dataset  [15] , the average Fleiss' Kappa  [71]  across emotion categories is κ = 0.173  [15] . Intuitively less complex emotions like \"Happiness\" have higher levels of agreement comparable to objective tasks performed at the time of data collection like determining age or ethnicity. More abstract concepts, such as \"Yearning\" and \"Sensitivity,\" had almost no agreement among participants. This result mirrors those from the EMOTIC  [8]  and GoEmotions  [7]  datasets. This comparison is important since as the size of the emotion model increases the types of emotion concepts included are bound to become more abstract. Not only do additional components have diminishing returns in terms of the information they provide, but since emotion is subjective, they also suffer an agreement penalty during the annotation process further reducing their effectiveness. One method to help mitigate this effect would be to choose more concrete emotion concepts as the foundation for emotion models. This method wasn't considered when generating the random models in Table  6 . Although they seem to outperform existing models in terms of recoverable information, the abstractness of their labels would severely limit their real-world effectiveness.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Large-Scale Data Analysis",
      "text": "To gain additional insight into how people consciously perceive emotion, we also examined the annotations for two large-scale emotion recognition datasets: EMOTIC  [8]  and BoLD  [15] . These are both in-the-wild datasets annotated for the same 26 categorical emotions as well as valence, arousal, and dominance  [14] . Although the categorical emotions were not based on any pre-existing emotion models, there is enough overlap with these emotion models to assess their validity. In addition to this, since EMOTIC and BoLD are image and video data, respectively, together they offer some insight into how humans process these different modalities.\n\nTo visualize how these emotions relate to each other, they were projected into the VAD space as shown in Fig.  9 . In general, the categorical emotions share similar distributions for both datasets. However, looking at the plots for valence vs. dominance, there is a distinct cluster in the BoLD dataset for high-dominance, low-valence emotions such as \"Anger,\" \"Disapproval,\" \"Aversion,\" and \"Annoyance.\" Because these are all high arousal emotions, this difference Fig.  8 . Categorical emotions plotted against valence and dominance for the BoLD dataset. Horizontal delineation suggests valence and vertical delineation suggests dominance as the discriminant factor. In these scatterplots of categorical emotions according to their valence and dominance ratings, there appears to be a gap (i.e., blank space) for low-dominance, high-valence emotions. Looking at the few samples present in this area; these seem to represent guardian-child type relationships where one subject takes on the role of the protector and the other takes on the role of the child. We suggest the lack of samples in this area is due to some bias in the original data collection and not necessarily due to a limitation of the continuous VAD dimensions.\n\nbetween datasets may be caused by the additional motion information video samples have over static photos. Other than this cluster, there appears to be a strong correlation between dominance and valence. Past work has shown that these dimensions are not completely orthogonal  [73] . The collected annotations from these two datasets seem to confirm this finding. Even with the distinct clustering off the mainline dominance/valence plot, this can be accounted for by their separation in the arousal dimension. The scatter plots in Fig.  8  show that the only categorical emotion outside of the \"Anger\" cluster that really benefits from the inclusion of dominance is confidence as its heatmap appears uniformly distributed across the valence axis while decreasing from high dominance to low dominance. Similar to the linear relationship between valence and dominance, in the raw valence-arousal projections in Fig.  9 , we also observe a collapse along the arousal dimension. This observation suggests changes in valence are the primary discriminative factor for differentiating emotions. This \"valence focus\" mirrors results from previous studies  [72] . Still, when the labels from the BoLD dataset are projected onto the unit circle based on the mean valence and arousal for each emotion, the outcome broadly aligns with the affective circumplex  [33] ,  [74] .\n\nIn addition to the simple VAD embeddings (Fig.  11 -15), we also once again produce UMAP embeddings  [10]  of the space as shown in Fig.  10 . We performed this analysis on both datasets. However, due to the large imbalance in the EMOTIC dataset as well a lack of data cleaning similar to BoLD's annotator reliability analysis, we limit the following analysis strictly to BoLD. First, it is important to note that although the dimensions are not meaningful, we can assign some meaning to them by also looking at the continuous VAD annotations in the same embedding space.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Projections Of Bold Categorical Labels Across Continuous Dimensions",
      "text": "Circumplex Projection Raw Valence-Arousal Projection Raw Dominance-Valence Projection Fig.  9 . Left: When the BoLD labels are projected onto the unit circle for valence and arousal, the locations of the emotions broadly line up with the Affective Circumplex  [33] . Center: The relationship between raw Valence-Arousal dimensions is visualized. These are colored by Dominance. The circle size is based on the number of samples for that label within the BoLD dataset. The compression of emotions along the Arousal dimension is in line with previous experiments  [72] . Right: Similar to previous work  [73] , the Dominance-Valence projection shows a strong linear relationship between these two dimensions.\n\nIn general, valence corresponds to the horizontal axis with arousal on the vertical. Similar to their correlation in the 2D VAD embeddings, dominance closely follows valence by generally increasing from left to right. For each individual categorical emotion, we then generate a heatmap so we can see where it exists in these embeddings. The discreteness of these heatmaps provides an idea of how fundamental each of these emotions is. For example, in Fig.  4  it is clear that \"Happiness\" and \"Anger\" occupy their own distinct clusters away from the main grouping. Likewise, Ekman's other basic emotions (Sadness, Fear, Surprise), also have relatively discrete clusters suggesting they are a fundamental building block for more complex emotions. On the opposite end of the spectrum, \"Anticipation\" and \"Engagement\" appear almost randomly dispersed throughout the embedding space. This spread suggests they are more complex in their expression and present themselves in a wider range of scenarios.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "Using word embeddings, we were able to show how existing models of human emotion popular in psychology either lack sufficient coverage or contain redundant, overlapping labels. Through agglomerative clustering, we were able to identify 15 components that minimized overlap and maximized coverage across cultures. This model, HICEM, was able to provide almost as much coverage with half as many labels as compared with existing emotion models. Interestingly enough, the only basic emotion described by Ekman which was not included in the HICEM was \"Disgust.\" The combination of \"Anger\" and \"Disgust\" in our analysis is similar to previous work on biologically basic emotions  [75] . Another consideration in the construction of comprehensive emotion models is the inclusion of general wellbeing/pain as well as a neutral affective state in the model. Although these are not classically thought of as emotions, the inclusion of these makes sense for HICEM because wellbeing/pain is expressed through facial expressions and body language similar to other emotions, and \"Neutral\" affective states also contain a variety of information useful for filtering out functional movements (e.g., walking) common in everyday life. In addition to this, recent work taking inspiration from componential models of emotion has looked to relate other physiological systems to affective states. Hunger, thirst, sleepiness, and stress have all been shown to be connected to emotion  [58] ,  [76] . These factors can be added to the components found in HICEM to provide complete coverage across both physiological processes and emotion.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Bold Umap Embeddings In The",
      "text": "Although quantitative analysis shows diminishing returns after around 15 clusters, qualitatively the completeness of the model seems to peak between 25 and 30 emotion concepts similar to previous results in earlier studies  [6] ,  [25] . Thus, it may be more beneficial to proceed with more than 15 base components in future dataset creation to ensure completeness. Likewise, if there are certain areas of the emotion space which are more important for a given domain, the hierarchical nature of our model easily allows for additional dimensions while maintaining maximum coverage. An example of this might be the inclusion of abnormal emotional states for mental health diagnoses. If we consider the clustering in Fig.  5  for 15 components, instead of using \"bizarre\" as a label, we can look at the two clusters which formed it and split the label into \"zany\" and \"nonsensical.\"\n\nFinally, similar to previous work, it is recommended that HICEM is used in tandem with continuous affective dimen-  sions (i.e., VAD) to provide a holistic representation of the emotion space. Given the relationship between valence and arousal in existing large-scale datasets, it is recommended to drop dominance as a dimension in future data collection to reduce redundancies and costs associated with the data collection. Instead, alternate dimensions such as certainty or effort may be included. An interesting extension of HICEM would be to examine the latent continuous dimensions of emotion and develop a similar high-coverage model to describe the space.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Limitations",
      "text": "As stated previously, one limitation with using word embeddings has to do with antonyms being used in similar contexts, potentially having a higher cosine similarity than their synonyms. Although UMAP helps in increasing the distance between these antonyms in our analysis, this context issue still influences cluster purity because there are situations where it is difficult to tease out. By taking the median vector for each cluster, we minimize this effect when generating our summary words. Another alternative would be to use word-level emotion distributions as described by Li et al.  [77]  for text classification tasks. In addition to this, the use of Deep Learning (DL) generated embeddings  [61]  from free text annotations provides an interesting alternative. These do not rely on local word context to generate their vectors and have shown higher performance in NLP tasks compared to classical methods.\n\nIn addition to the embeddings, our method also is limited by translation. We once again minimized the influence of this by averaging across several cultures; however, better results for each individual language may be achieved by having native speakers generate localized emotion word lists and then using those lists to fine-tune the models. Similarly, although we can generate a consistent set of labels for use across languages, the way people perceive these different emotion words is unique to their culture. Given that all languages appear to share the same 15 base components, transfer learning and fine-tuning can be performed to better fit machine learning models to their local language context.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Applications",
      "text": "HICEM is primarily a descriptive tool, so its value comes from its ability to describe large numbers of affective states with relatively few labels. This is ideal for dataset annotation in modern data-driven AI because it maximizes the return on investment in terms of the amount of information gathered from each sample labeled. Although limited to 15 components, HICEM still provides high coverage over a wide range of human emotions. This advancement means next-generation affective computing or AEI applications leveraging HICEM will allow for more natural humanmachine or human-robot interactions.\n\nIn addition to HICEM's value as an annotation tool, the process for developing our emotion model can also be used to build a taxonomy of human emotion similar to WordNet  [78] . Such a tool has potential applications in psychological research and can also be used for succinctly describing a patient's emotional state. Because HICEM is limited to discrete emotions, an interesting extension of this would be to use NLP word embeddings to find the equivalent for the continuous emotion dimensions.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusions",
      "text": "There has been a substantial amount of research into computational techniques for recognizing human emotion. However, little work has been done examining the actual emotion models underpinning this research. As our analysis shows, existing models of emotion are insufficient for real-world applications. In affective computing, coverage is much more important than the completeness of the emotion model due to practical limitations in data collection and annotation. Here, using unsupervised techniques we were able to identify 15 components with minimal overlap and maximum coverage across 1,720 emotion concepts. This work provides a more efficient model of human emotion which is a step toward achieving artificial emotional intelligence.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). These are generated by taking",
      "page": 1
    },
    {
      "caption": "Figure 1: Existing emotion models provide incomplete representations across the entire emotion space. Here we visualize the maximum log cosine",
      "page": 2
    },
    {
      "caption": "Figure 2: Our pipeline for generating a cross cultural, high-coverage model of emotion. Starting with our list of emotion-concepts, we generate their",
      "page": 4
    },
    {
      "caption": "Figure 2: , we ﬁrst compiled a list of emotion-",
      "page": 4
    },
    {
      "caption": "Figure 3: We compare four different dimensionality reduction methods using sentiment as a heuristic. Unlike t-SNE [11], UMAP [10] is able to separate",
      "page": 5
    },
    {
      "caption": "Figure 3: , we visualize the different embeddings with",
      "page": 5
    },
    {
      "caption": "Figure 4: Finally, we",
      "page": 5
    },
    {
      "caption": "Figure 5: To develop our cross-cultural model, we repeat this",
      "page": 5
    },
    {
      "caption": "Figure 4: Using the elbow method to identify the number of clusters for",
      "page": 5
    },
    {
      "caption": "Figure 5: Cluster summaries for English for the number of clusters, k, increases from seven to 30. As k increases, we are able to generate more",
      "page": 6
    },
    {
      "caption": "Figure 1: Because this list is uniformly distributed throughout the",
      "page": 7
    },
    {
      "caption": "Figure 6: In this histogram of coverage by cosine similarity across English",
      "page": 7
    },
    {
      "caption": "Figure 7: and Table 6, HICEM is the only emotion model that",
      "page": 7
    },
    {
      "caption": "Figure 7: In this plot of recovered cosine similarity vs. the number of com-",
      "page": 8
    },
    {
      "caption": "Figure 9: In general, the categorical emotions share similar distri-",
      "page": 8
    },
    {
      "caption": "Figure 8: Categorical emotions plotted against valence and dominance for the BoLD dataset. Horizontal delineation suggests valence and vertical",
      "page": 9
    },
    {
      "caption": "Figure 8: show that the only categorical emotion outside",
      "page": 9
    },
    {
      "caption": "Figure 9: , we also observe",
      "page": 9
    },
    {
      "caption": "Figure 10: We performed this analysis",
      "page": 9
    },
    {
      "caption": "Figure 9: Left: When the BoLD labels are projected onto the unit circle for valence and arousal, the locations of the emotions broadly line up with the",
      "page": 10
    },
    {
      "caption": "Figure 4: it is clear",
      "page": 10
    },
    {
      "caption": "Figure 10: Heatmaps for each of the 26 categorical emotions in the BoLD dataset based on their categorical UMAP embeddings. The blue areas",
      "page": 11
    },
    {
      "caption": "Figure 5: for 15 components, instead of using",
      "page": 11
    },
    {
      "caption": "Figure 11: Categorical emotions plotted against dominance and arousal",
      "page": 12
    },
    {
      "caption": "Figure 12: Categorical emotions plotted against valence and arousal for",
      "page": 12
    },
    {
      "caption": "Figure 13: Categorical emotions plotted against dominance and arousal for",
      "page": 12
    },
    {
      "caption": "Figure 14: Categorical emotions plotted against valence and dominance",
      "page": 12
    },
    {
      "caption": "Figure 15: Categorical emotions plotted against valence and arousal for the",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.5\nSerene\nLove\nAdmiration\nEcstasy\nAnticipation\n0.4\nAccepted\nSubmissive\nTrust\nOptimism\nAggressiveness": "Log Similarity\nInterest\n0.3\nVigilance\nJoyful\nSurprise\nFear\nLoathing\nApprehension\nContempt\nDisgust\nAmazement\nDistracted\nTerror\nAnnoyed\nBored\nDisapproval\n0.2\nRemorse\nSadness"
        },
        {
          "0.5\nSerene\nLove\nAdmiration\nEcstasy\nAnticipation\n0.4\nAccepted\nSubmissive\nTrust\nOptimism\nAggressiveness": "Pensiveness\n0.1"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.5\nSatisfaction\nAdoration\nAdmiration\nCraving\nExcitement\n0.4": "Log Similarity\nInterest\n0.3\nCalm\nJoyful\nSurprise\nFear\nRelief\nAnger\nDisgust\nEmpathetic Pain\nAmusement\nConfused\nAwkwardness\nAesthetic Appreciation\n0.2\nSexual Desire\nBoredom\nHorror\nSadness\nRomance"
        },
        {
          "0.5\nSatisfaction\nAdoration\nAdmiration\nCraving\nExcitement\n0.4": "Nostalgia\n0.1"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.5": "Happiness\nAffection\n0.4\nHonest\nAffable"
        },
        {
          "0.5": "Log Similarity\n0.3\nApathetic\nSpiteful\nPlayful\n0.2\nUnhealthy\nStrange\n0.1"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.5": "Happiness\nPeace\nPleasure\nEsteem\nAffection\nYearning\nAnticipation\nSympathy\nEsteemed\n0.4\nConfidence\nEngagement"
        },
        {
          "0.5": "Log Similarity\n0.3\nSensitive\nDoubt\nSurprise\nFear\nAversion\nAnger\nDisquiet\nConfused\nDisapproval\nAnnoyance\nEmbarrassment\nDisconnection\n0.2\nPain\nFatigue\nSuffering\nSadness\n0.1"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: , when we plot Ekman’s basic emotions,",
      "data": [
        {
          "Sentiment\n0.6\nContempt\nAnger\nDisgust\n0.4\nFear\nSadness\n0.2\nHappiness\n0.0\nSurprise\n0.2\n0.4": "0.6"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The Expression of the Emotions in Man and Animals",
      "authors": [
        "C Darwin"
      ],
      "year": "2015",
      "venue": "The Expression of the Emotions in Man and Animals"
    },
    {
      "citation_id": "2",
      "title": "Artificial (emotional) intelligence",
      "authors": [
        "M Krakovsky"
      ],
      "year": "2018",
      "venue": "Comm. ACM"
    },
    {
      "citation_id": "3",
      "title": "On shape and the computability of emotions",
      "authors": [
        "X Lu",
        "P Suryanarayan",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2012",
      "venue": "Proc. 20th ACM Int. Conf. on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "J. Personality and Soc. Psychol"
    },
    {
      "citation_id": "5",
      "title": "The Emotions",
      "authors": [
        "R Plutchik"
      ],
      "venue": "The Emotions"
    },
    {
      "citation_id": "6",
      "title": "America",
      "year": "1991",
      "venue": "America"
    },
    {
      "citation_id": "7",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proc. Natl. Academy Sci"
    },
    {
      "citation_id": "8",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Mtg. Assc. Comput. Linguistics (ACL)"
    },
    {
      "citation_id": "9",
      "title": "EMOTIC: Emotions in context dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conf.Comput. Vis. Pattern Recognit. Workshops (CVPRW)"
    },
    {
      "citation_id": "10",
      "title": "Learning word vectors for 157 languages",
      "authors": [
        "E Grave",
        "P Bojanowski",
        "P Gupta",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2018",
      "venue": "Proc. of the Int. Conf. Lang. Resources Evaluation (LREC)"
    },
    {
      "citation_id": "11",
      "title": "UMAP: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "J Melville"
      ],
      "year": "2018",
      "venue": "UMAP: Uniform manifold approximation and projection for dimension reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "12",
      "title": "Stochastic neighbor embedding",
      "authors": [
        "G Hinton",
        "S Roweis"
      ],
      "year": "2003",
      "venue": "Adv"
    },
    {
      "citation_id": "13",
      "title": "Language as context for the perception of emotion",
      "authors": [
        "L Barrett",
        "K Lindquist",
        "M Gendron"
      ],
      "year": "2007",
      "venue": "Trends Cog. Sci"
    },
    {
      "citation_id": "14",
      "title": "What's in a word? Language constructs emotion perception",
      "authors": [
        "K Lindquist",
        "M Gendron"
      ],
      "year": "2013",
      "venue": "Emotion Rev"
    },
    {
      "citation_id": "15",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "16",
      "title": "ARBEE: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "Int. J. Comput. Vis"
    },
    {
      "citation_id": "17",
      "title": "DFEW: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proc. 28th ACM Int. Conf. on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. 56th Annl"
    },
    {
      "citation_id": "19",
      "title": "The omg-emotion behavior dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 Int. Joint Conf. Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "20",
      "title": "EmoReact: A multimodal approach and dataset for recognizing emotional responses in children",
      "authors": [
        "B Nojavanasghari",
        "T Baltrušaitis",
        "C Hughes",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proc. 18th ACM Int. Conf. Multimodal Interaction, ser. ICMI"
    },
    {
      "citation_id": "21",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "22",
      "title": "Context in emotion perception",
      "authors": [
        "L Barrett",
        "B Mesquita",
        "M Gendron"
      ],
      "year": "2011",
      "venue": "Curr. Dir. Psychol. Sci"
    },
    {
      "citation_id": "23",
      "title": "Professional actors demonstrate variability, not stereotypical expressions, when portraying emotional states in photographs",
      "authors": [
        "T Mau",
        "K Hoemann",
        "S Lyons",
        "J Fugate",
        "E Brown",
        "M Gendron",
        "L Barrett"
      ],
      "year": "2021",
      "venue": "Nature Communications"
    },
    {
      "citation_id": "24",
      "title": "Emotions are understood from biological motion across remote cultures",
      "authors": [
        "C Parkinson",
        "T Walker",
        "S Memmi",
        "T Wheatley"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "25",
      "title": "The recognition of 18 facial-bodily expressions across nine cultures",
      "authors": [
        "D Cordaro",
        "R Sun",
        "S Kamble",
        "N Hodder",
        "M Monroy",
        "A Cowen",
        "Y Bai",
        "D Keltner"
      ],
      "year": "2020",
      "venue": "Emotion"
    },
    {
      "citation_id": "26",
      "title": "Mapping the passions: Toward a high-dimensional taxonomy of emotional experience and expression",
      "authors": [
        "A Cowen",
        "D Sauter",
        "J Tracy",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "Psychol. Sci. Public Interest"
    },
    {
      "citation_id": "27",
      "title": "What the face displays: Mapping 28 emotions conveyed by naturalistic expression",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2020",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "28",
      "title": "Deep affect prediction in-the-wild: Aff-Wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Deep affect prediction in-the-wild: Aff-Wild database and challenge, deep architectures, and beyond",
      "arxiv": "arXiv:1804.10938"
    },
    {
      "citation_id": "29",
      "title": "Critiquing models of emotions",
      "authors": [
        "H Smith",
        "A Schneider"
      ],
      "year": "2009",
      "venue": "Soc. Methods Res"
    },
    {
      "citation_id": "30",
      "title": "Emotion circuits in the brain",
      "authors": [
        "J Ledoux"
      ],
      "year": "2000",
      "venue": "Annl. Rev. Neuroscience"
    },
    {
      "citation_id": "31",
      "title": "Functional neuroanatomy of emotion: A meta-analysis of emotion activation studies in PET and fMRI",
      "authors": [
        "K Phan",
        "T Wager",
        "S Taylor",
        "I Liberzon"
      ],
      "year": "2002",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "32",
      "title": "The Archaeology of Mind: Neuroevolutionary Origins of Human Emotions, 1st ed., ser. A Norton professional book",
      "authors": [
        "J Panksepp",
        "L Biven"
      ],
      "year": "2012",
      "venue": "The Archaeology of Mind: Neuroevolutionary Origins of Human Emotions, 1st ed., ser. A Norton professional book"
    },
    {
      "citation_id": "33",
      "title": "Affective Neuroscience: The Foundations of Human and Animal Emotions",
      "authors": [
        "J Panksepp"
      ],
      "year": "2004",
      "venue": "Affective Neuroscience: The Foundations of Human and Animal Emotions"
    },
    {
      "citation_id": "34",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J. of Personality and Soc. Psychol"
    },
    {
      "citation_id": "35",
      "title": "Multidimensional scaling of facial expressions",
      "authors": [
        "R Abelson",
        "V Sermat"
      ],
      "year": "1962",
      "venue": "J. of Exp. Psychol"
    },
    {
      "citation_id": "36",
      "title": "Multidimensional scaling of emotional facial expressions: Similarity from preschoolers to adults",
      "authors": [
        "J Russell",
        "M Bullock"
      ],
      "year": "1985",
      "venue": "J. of Personality and Soc. Psychol"
    },
    {
      "citation_id": "37",
      "title": "The two general activation systems of affect: Structural findings, evolutionary considerations, and psychobiological evidence",
      "authors": [
        "D Watson",
        "D Wiese",
        "J Vaidya",
        "A Tellegen"
      ],
      "year": "1999",
      "venue": "J. of Personality and Soc. Psychol"
    },
    {
      "citation_id": "38",
      "title": "Hemispheric asymmetry in the expression of positive and negative emotions: neurologic evidence",
      "authors": [
        "H Sackeim"
      ],
      "year": "1982",
      "venue": "Archives of Neurology"
    },
    {
      "citation_id": "39",
      "title": "Dissociated neural representations of intensity and valence in human olfaction",
      "authors": [
        "A Anderson",
        "K Christoff",
        "I Stappen",
        "D Panitz",
        "D Ghahremani",
        "G Glover",
        "J Gabrieli",
        "N Sobel"
      ],
      "year": "2003",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "40",
      "title": "Arousal systems",
      "authors": [
        "B Jones"
      ],
      "year": "2003",
      "venue": "Front. Biosci"
    },
    {
      "citation_id": "41",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Dev. and Psychol"
    },
    {
      "citation_id": "42",
      "title": "Crosscultural Universals of Affective Meaning",
      "authors": [
        "C Osgood",
        "W May",
        "M Miron",
        "M Miron"
      ],
      "year": "1975",
      "venue": "Crosscultural Universals of Affective Meaning"
    },
    {
      "citation_id": "43",
      "title": "An affective model of interplay between emotions and learning: Reengineering educational pedagogy-building a learning companion",
      "authors": [
        "B Kort",
        "R Reilly",
        "R Picard"
      ],
      "year": "2001",
      "venue": "Proc IEEE Int. Conf. Adv. Learning Tech"
    },
    {
      "citation_id": "44",
      "title": "The world of emotions is not two-dimensional",
      "authors": [
        "J Fontaine",
        "K Scherer",
        "E Roesch",
        "P Ellsworth"
      ],
      "year": "2007",
      "venue": "Psychol. Sci"
    },
    {
      "citation_id": "45",
      "title": "The SE-MAINE corpus of emotionally coloured character interactions",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "2010 IEEE Int. Conf. Multimedia and Expo"
    },
    {
      "citation_id": "46",
      "title": "Appraisal Processes in Emotion: Theory, Methods, Research, ser. Series in affective science",
      "year": "2001",
      "venue": "Appraisal Processes in Emotion: Theory, Methods, Research, ser. Series in affective science"
    },
    {
      "citation_id": "47",
      "title": "Conscious emotional experience emerges as a function of multilevel, appraisal-driven response synchronization",
      "authors": [
        "D Grandjean",
        "D Sander",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Consciousness Cogn"
    },
    {
      "citation_id": "48",
      "title": "A computational unification of cognitive behavior and emotion",
      "authors": [
        "R Marinier",
        "J Laird",
        "R Lewis"
      ],
      "year": "2009",
      "venue": "Cog. Syst. Res"
    },
    {
      "citation_id": "49",
      "title": "Emotion and Personality Psychological Aspects",
      "authors": [
        "M Arnold"
      ],
      "year": "1960",
      "venue": "Emotion and Personality Psychological Aspects"
    },
    {
      "citation_id": "50",
      "title": "Psychological Stress and the Coping Process",
      "authors": [
        "R Lazarus"
      ],
      "year": "1966",
      "venue": "Psychological Stress and the Coping Process"
    },
    {
      "citation_id": "51",
      "title": "A systems approach to appraisal mechanisms in emotion",
      "authors": [
        "D Sander",
        "D Grandjean",
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "52",
      "title": "A multi-componential approach to emotion recognition and the effect of personality",
      "authors": [
        "G Mohammadi",
        "P Vuilleumier"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "53",
      "title": "Induction and profiling of strong multi-componential emotions in virtual reality",
      "authors": [
        "B Meuleman",
        "D Rudrauf"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "54",
      "title": "List of emotions: 271 emotion words",
      "authors": [
        "Tchiki Davis"
      ],
      "year": "2022",
      "venue": "List of emotions: 271 emotion words"
    },
    {
      "citation_id": "55",
      "title": "380 High Emotion Words",
      "year": "2014",
      "venue": "380 High Emotion Words"
    },
    {
      "citation_id": "56",
      "title": "List of emotions: a huge list of useful words to describe feelings and emotions",
      "year": "2019",
      "venue": "List of emotions: a huge list of useful words to describe feelings and emotions"
    },
    {
      "citation_id": "57",
      "title": "A Semantic Atlas of Emotional Concepts",
      "authors": [
        "J Averill"
      ],
      "year": "1975",
      "venue": "American Psycholog. Ass., Journal Suppl. Abstract Service"
    },
    {
      "citation_id": "58",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "59",
      "title": "A hybrid cognitive architecture with primal affect and physiology",
      "authors": [
        "C Dancy"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "60",
      "title": "Bag of tricks for efficient text classification",
      "authors": [
        "A Joulin",
        "E Grave",
        "P Bojanowski",
        "T Mikolov"
      ],
      "year": "2016",
      "venue": "Bag of tricks for efficient text classification",
      "arxiv": "arXiv:1607.01759"
    },
    {
      "citation_id": "61",
      "title": "Advances in pre-training distributed word representations",
      "authors": [
        "T Mikolov",
        "E Grave",
        "P Bojanowski",
        "C Puhrsch",
        "A Joulin"
      ],
      "year": "2018",
      "venue": "Proc. Int. Conf. Lang. Rscs. Eval. (LREC)"
    },
    {
      "citation_id": "62",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. 2019 Conf"
    },
    {
      "citation_id": "63",
      "title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit",
      "authors": [
        "S Bird",
        "E Klein",
        "E Loper"
      ],
      "year": "2009",
      "venue": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit"
    },
    {
      "citation_id": "64",
      "title": "Data Mining and Knowledge Discovery Handbook",
      "year": "2005",
      "venue": "Data Mining and Knowledge Discovery Handbook"
    },
    {
      "citation_id": "65",
      "title": "Clustering with UMAP: Why and how connectivity matters",
      "authors": [
        "A Dalmia",
        "S Sia"
      ],
      "year": "2021",
      "venue": "Clustering with UMAP: Why and how connectivity matters",
      "arxiv": "arXiv:2108.05525"
    },
    {
      "citation_id": "66",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cog. Emo"
    },
    {
      "citation_id": "67",
      "title": "Who belongs in the family?",
      "authors": [
        "R Thorndike"
      ],
      "year": "1953",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "68",
      "title": "Finding a \"Kneedle\" in a haystack: detecting knee points in system behavior,\" in 2011 31st Int. Conf",
      "authors": [
        "V Satopaa",
        "J Albrecht",
        "D Irwin",
        "B Raghavan"
      ],
      "year": "2011",
      "venue": "Finding a \"Kneedle\" in a haystack: detecting knee points in system behavior,\" in 2011 31st Int. Conf"
    },
    {
      "citation_id": "69",
      "title": "Ridge regression: Biased estimation for nonorthogonal problems",
      "authors": [
        "A Hoerl",
        "R Kennard"
      ],
      "year": "1970",
      "venue": "Technometrics"
    },
    {
      "citation_id": "70",
      "title": "The evolutionary psychology of hunger",
      "authors": [
        "L Al-Shawaf"
      ],
      "year": "2016",
      "venue": "Appetite"
    },
    {
      "citation_id": "71",
      "title": "What is the relationship between pain and emotion? Bridging constructs and communities",
      "authors": [
        "G Gilam",
        "J Gross",
        "T Wager",
        "F Keefe",
        "S Mackey"
      ],
      "year": "2020",
      "venue": "Neuron"
    },
    {
      "citation_id": "72",
      "title": "Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring the Extent of Agreement Among Raters",
      "authors": [
        "K Gwet"
      ],
      "year": "2014",
      "venue": "Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring the Extent of Agreement Among Raters"
    },
    {
      "citation_id": "73",
      "title": "Valence focus and arousal focus: Individual differences in the structure of affective experience",
      "authors": [
        "L Feldman"
      ],
      "year": "1995",
      "venue": "J. of Personality and Soc. Psychol"
    },
    {
      "citation_id": "74",
      "title": "Norms of valence, arousal, and dominance for 13,915 English lemmas",
      "authors": [
        "A Warriner",
        "V Kuperman",
        "M Brysbaert"
      ],
      "year": "2013",
      "venue": "Behav. Res. Methods"
    },
    {
      "citation_id": "75",
      "title": "The structure of current affect",
      "authors": [
        "L Barrett",
        "J Russell"
      ],
      "year": "1999",
      "venue": "Curr. Dir. in Psychol. Sci"
    },
    {
      "citation_id": "76",
      "title": "Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time",
      "authors": [
        "R Jack",
        "O Garrod",
        "P Schyns"
      ],
      "year": "2014",
      "venue": "Curr. Bio"
    },
    {
      "citation_id": "77",
      "title": "Extraneous factors in judicial decisions",
      "authors": [
        "S Danziger",
        "J Levav",
        "L Avnaim-Pesso"
      ],
      "year": "2011",
      "venue": "Proc. Proc. Natl. Academy Sci"
    },
    {
      "citation_id": "78",
      "title": "Word-level emotion distribution with two schemas for short text emotion classification",
      "authors": [
        "Z Li",
        "H Xie",
        "G Cheng",
        "Q Li"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Syst"
    },
    {
      "citation_id": "79",
      "title": "WordNet: A lexical database for English",
      "authors": [
        "G Miller"
      ],
      "year": "1995",
      "venue": "Comm. ACM"
    }
  ]
}