{
  "paper_id": "2411.18060v1",
  "title": "Oris: Online Active Learning Using Reinforcement Learning-Based Inclusive Sampling For Robust Streaming Analytics System",
  "published": "2024-11-27T05:11:37Z",
  "authors": [
    "Rahul Pandey",
    "Ziwei Zhu",
    "Hemant Purohit"
  ],
  "keywords": [
    "Active Learning",
    "Reinforcement Learning",
    "Human Memory Decay",
    "Human Error",
    "Human-AI Collaboration"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Effective labeled data collection plays a critical role in developing and fine-tuning robust streaming analytics systems. However, continuously labeling documents to filter relevant information poses significant challenges like limited labeling budget or lack of high-quality labels. There is a need for efficient human-in-the-loop machine learning (HITL-ML) design to improve streaming analytics systems. One particular HITL-ML approach is online active learning, which involves iteratively selecting a small set of the most informative documents for labeling to enhance the ML model performance. The performance of such algorithms can get affected due to human errors in labeling. To address these challenges, we propose ORIS, a method to perform Online active learning using Reinforcement learningbased Inclusive Sampling of documents for labeling. ORIS aims to create a novel Deep Q-Network-based strategy to sample incoming documents that minimize human errors in labeling and enhance the ML model performance. We evaluate the ORIS method on emotion recognition tasks, and it outperforms traditional baselines in terms of both human labeling performance and the ML model performance. The code for this research is available at https://github.com/rpandey4/oris.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The exponential growth of online data for various application domains, such as journalism, public health, and crisis management, has presented new challenges in effectively filtering and processing high-volume, high-velocity data streams. These data streams are often characterized by their noisy, sparse, and redundant nature, making it difficult for human annotators to keep pace with the sheer velocity and volume of data  [1] . Further, purely automated systems for streaming analytics face limitations in accurately filtering data and fail to adapt to the dynamic nature of the data.\n\nTo address these challenges, human-in-the-loop machine learning (HITL-ML) methods like online active learning have emerged that combines human labeling and automated classification to achieve accurate and efficient data filtering  [2] ,  [3] . Online active learning methods selectively request labels for informative documents from a human (oracle), reducing the overall labeling cost while maintaining ML model accuracy. * The author is currently with Amazon.\n\nThis approach has shown success in various tasks, such as object detection, image/video/text classification, and machine translation systems  [4] ,  [5] .\n\nHowever, the accuracy and reliability of labeling can get affected by various human factors during the labeling process. For instance, prior research  [6] ,  [7]  has shown the presence of serial ordering-induced human errors like Mistakes and Slips  [8]  in the case of labeling task. Mistakes result from the absence of a correct cognitive representation of a concept, while slips occur despite acquiring the correct cognitive representation of a concept due to memory decay over time. To mitigate the serial ordering-induced slip error, researchers have proposed a heuristic-driven approach to sample the documents that can reduce the error in labeling and increase the annotator reliability, which in turn increases the performance of the ML model  [6] ,  [7] . However, there are certain challenges in using heuristic-driven approaches, like inflexibility to new tasks, increased bias, and limited scalability. Thus, a dynamic datadriven technique like reinforcement learning emerges as an indispensable and urgently required solution to address the challenge of minimizing the slips type of human error for the labeling tasks on data streams.\n\nWe propose ORIS, a method to perform Online active learning using Reinforcement learning-based Inclusive Sampling. ORIS aims to minimize human errors in labeling and enhancing the performance of the ML model. We use a novel Deep Q-Network  [9]  based strategy for reinforcement learning to sample incoming documents in a data stream efficiently that leads to robust active learning. We introduce a novel state representation and reward function, which learns the policy of inclusive sampling-based active learning for streaming data. The learned policy helps in the efficient collection of highquality streaming data that reduces human error in labeling and improves the ML model quality.\n\nOur contributions in this paper are: 1) We formulate the problem of human error in a streaming analytics framework. In this framework, a oracle annotator mimicking memory decay behavior is used for labeling that can pass erroneous labels.\n\n2) We propose a novel online active learning method, ORIS, which is capable of sampling documents that are inclusive and less prone to human error (see Fig.  1 ). 3) With extensive experimentation and evaluation to compare the proposed ORIS method with the baselines on emotion recognition tasks, we achieve up to 38.3% human & 55.7% machine performance improvement on Twitter (now X.com), and up to 44.2% human & 70.1% machine performance improvement on Reddit.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Online Active Learning",
      "text": "Traditional batch-based Active Learning (AL) assumes the existence of a pool of unlabeled data from which to select the most informative documents for labeling  [10] ,  [11] . However, in streaming analytics systems, data comes in real-time streams. Hence, the decision to pick or discard a document must be made in real-time, making batch-based AL impractical. For example, in the case of processing sequential online real-world data such as social media streams, gathering the true label is both costly and time-consuming. Hence, prior work has proposed an online active learning that samples data coming in streams for efficient model training  [12] -  [14] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Deep Reinforcement Learning For Active Learning",
      "text": "Traditional active learning depends on heuristic-driven approaches for coming up with sampling strategies. However, researchers have explored the use of non-heuristic methods in active learning, such as reinforcement learning, to automate the design of deep learning models and active learning query strategies  [3] . For example, Deep Reinforcement Active Learning (DRAL), applies the idea of reinforcement learning to dynamically adjust the acquisition function for specific tasks such as named entity recognition  [15] , person re-identification  [16] , image segmentation  [17] , and multimodal classification  [10] . The DRAL framework selects sequential documents from a gallery pool or streaming input during the active learning process, obtaining manual labels with binary/multi-class feedback. The rewards and the oracle feedback are used to adjust the agent's queries, ensuring the selection of highquality query samples. This approach enables more flexible and efficient active learning processes, improving the accuracy and reliability of labeling while reducing the workload on human annotators.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Human Factors In Data Labeling",
      "text": "Prior works have utilized psychology literature to improve labeling. For example,  [18]  uses social strategies of interactions from psychology literature to improve crowdsourcing participation. By augmenting questions in visual question answering task using the social strategies, they have increased the crowd workers' participation and informative responses. Researchers have also found a direct correlation between the monetization of the crowdsourcing tasks with the labeling speed  [19] . Most recent research studied the memory decay behavior from psychology in the case of labeling quality, explained in detail in Section III-B  [6] ,  [7] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Preliminaries",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Online Active Learning",
      "text": "As discussed in Section II-A, online active learning is a promising approach for minimizing the labeling cost while improving the ML model's performance. In online active learning, the ML model retrains with the new labeled documents added to the training set. This process continues until a stopping criterion is met, such as reaching a certain performance threshold or exhausting the labeling budget. Fig.  2  shows the generic flow of online active learning. It assumes the data is coming in streams in real-time. There is a fixed set of test documents to analyze the performance of the active learning model (ALM ), and the update frequency f decides when to update the ALM model (using the condition b(modf ) = 0). The whole system runs till the maximum budget of B to receive the oracle's feedback. Given the current document, we get the decision to sample document for labeling from an agent. This agent can be heuristic-driven or data-driven. If the agent's decision is to pick, then we request oracle to provide the label. We keep an update of the past selected labels in the oracle memory for mimicking the oracle behavior as discussed in the next Section III-B. Moreover, we update the training set by including the current document-label pair, which will be used for retraining the ALM . At every update frequency f of the budget exhausted, we re-train the ALM model with the training set. Moreover, we analyze the performance on the independent test documents.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Human Memory And Labeling Error",
      "text": "The memory decay behavior of humans is widely studied in psychology literature, which shows that memory retention decreases exponentially with time. Prior research  [8]  provides a human error taxonomy that distinguishes between two classes of errors: mistakes and slips as discussed in the Introduction.\n\nPrior research showed that a similar taxonomy exists in the crowdsourcing domain  [6] ,  [7] . They studied the memory decay behavior of humans in the context of learning and acquiring new knowledge, which results in the serial orderinginduced mistakes and slips  [20] ,  [21] . Psychologists have used an exponential function in the past to model the memory decay of humans  [22] ,  [23] . In the context of labeling, mathematically, the probability of an oracle making an error in labeling a particular class label c is defined using an exponential function over time last seen (∆t c ). Prior work used a parameterized sigmoid function  [6] ,  [7]  to compute the error probability score as defined in Eq 1:\n\nwhere α and β are hyperparameters representing different decay intensities of humans. Finally, we extend a new parameterized exponential decay function in an oracle to further strengthen the efficacy of the proposed method. We use an exponential function to compute the error probability score as defined in Eq 2:\n\nIV. APPROACH\n\nIn this section, we introduce the problem of streaming analytics systems for text document classification in the presence of a error-prone oracle annotator. We then explain the design of our ORIS method. Note that ORIS can be extended to other modalities, such as images & videos, and other streaming analytics tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Problem Formulation",
      "text": "Given a stream of documents (d 1 , d 2 , ..., d t , ...) arriving in real-time, our objective is to sample documents for labeling by the oracle to improve the ML model (ALM ). The maximum budget to pick documents to train is defined by the hyperparameter B. Whenever a document is picked, we request a label from the oracle. The oracle induces the serial order-induced slip error behavior in providing the annotation as described in Section III-B. Once we receive the label, we store it in the training set S for re-training the ML model. We also keep track of the prior labeling memory M to correctly imitate the real-world system where oracle posses memory decay behavior. We (re-)train a BERT-based ML model (ALM ) with the training set S, and the frequency of its retraining is defined by hyperparameter f (f < B). Moreover, after each retraining with frequency f , we observe the ML model's (ALM ) performance (machine performance) on an independent test set (T ). Furthermore, we observe the human performance of the oracle's behavior in providing the correct or erroneous labels. We report both performance metrics at every frequency interval f .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Overview Of Oris",
      "text": "In this section, we describe the proposed ORIS method used for inclusive sampling. Unlike relying on a heuristicbased approach, ORIS uses reinforcement learning to develop the sampling strategy as used in recent prior work on deep reinforcement active learning (DRAL)  [10] ,  [15] -  [17] . However, the objective of ORIS is different from the prior DRALbased approaches. Instead of only focusing on the machine performance of the ML model, the proposed ORIS method tackles both human and machine performance to create a robust active learning sampling strategy.\n\nFig.  1  shows the overall architecture of ORIS. It comprises two components: ORIS sampling using Deep Q-Network and Online Active Learning. To create an optimal sampling strategy, we formulate the problem of sampling the documents coming in streams as a Markov Decision Process (MDP)  [24] . Given the current document d t at time t, we represent using document embedding. Additionally, we model the current memory of the oracle to include it in the state variable along with the document embedding. We initialize Deep Q-Network (DQN)  [9] , which consists of a feed-forward neural network as a Q-network, and acts as a reinforcement learning agent that takes the state as input and outputs the action. The DQN agent takes action on the incoming documents d t till the budget B to pick the documents is exhausted. We store each experience containing the state, action, reward, and next state in the replay buffer R. During training, we utilize these experiences to minimize the mean squared error between the predicted Q-value and the target Q-value. We train the DQN for several episodes with different online real-time streaming till maximum episode E max . Once the DQN is trained, we use it for online active learning's decision-making.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Oris Modeling",
      "text": "In this section, we explain the different components for modeling the proposed ORIS method as an MDP problem. First, we define the formation of state, action, and reward, followed by detailed information on forming the DQN architecture, the training, and the inference.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) State:",
      "text": "The state variables consist of the representation of the environment, which is the input to the DQN to get the optimal action. Given the incoming streaming input text d t , we compute its embeddings emb t as a part of the state variable. We use the pre-trained word embeddings to compute the document embedding representation. Consider the document d t as a set of words [w 1 , w 1 , ..., w n ] of length n, we compute embeddings at time t as emb t = w ϵ d t emb w n where emb w is the pre-trained word embeddings of the word w.\n\nMoreover, we keep track of the prior class labels from the oracle as an additional input of the state variable. This input helps the agent to understand the oracle memory and its effect on the label quality to make an inclusive decision to pick or discard the current document. Given the current step t, we compute the latest time last seen for class c i ∈ C by the oracle as ∆t 1 ci = t-j 1 , where j 1 is the latest step at which c j1 == c i . However, there exists a possibility where an oracle can forget a class and make errors in labeling, and hence the class label used to compute the time last seen may not be valid. Hence, to ensure the reliability of the time last seen value computation, we keep track of the most recent k time last seen values for each class c i ∈ C. Finally, the state variable s t is the concatenation of current input text embedding emb t and the k-averaged time last seen for all the classes as shown in Fig.  1 (1c) :\n\nThe length of the state s t is equal to len(emb t ) (length of embedding) + len(C) (total classes). Fig.  1  illustrates an example of state variable computation. The incoming document is d t with document embedding emb t . There are five classes to label. The table in the figure represents the time last seen ∆t ci corresponding to the past k = 3 last seen for each class c i . The ∆t t is calculated as the k-averaged time last seen for every class and concatenated with the embedding emb t to make the state variable representation s t .\n\n2) Action: Since the ORIS agent aims to sample the input documents, the action has two values: pick (1) if labeling the document and discard (0) if not. If picked, the oracle provides the label, which is used to retrain the ML model (ref. Fig.  2 ).\n\n3) Reward: The reward incentivizes the optimal behavior of the DQN model. Since the objective of ORIS is not to forget any classes by the oracle, we reward the agent if the selected documents are equally diversified along with their class labels. Note that the oracle can make no error in labeling if all the sampled documents belong to the same class. However, that causes risk to both the oracle and ML model. The ML model will not learn well for all classes if all the picked documents for training belong to the same class. Moreover, the oracle will have the highest risk of providing errors if the selected documents belong to any other class besides the frequent one.\n\nHence, the proposed reward function promotes diversity and inclusivity of all classes of recently sampled documents. We note that the diversity in sampling not only reduces the chance of the oracle making fewer serial ordering-induced slips errors but also improves the performance of the ML model, especially for the infrequent class in the imbalanced data distribution, which is the case for many real-time streaming tasks. To achieve this, we introduce a new intermediate metric Inclusivity, which measures the Shannon entropy  [25]  of the annotated classes of past m picked documents in the Memory M as shown in Eq 4.\n\nMoreover, to make the overall reward promote higher inclusivity and penalize marginal or low inclusivity scores, we use a parameterized exponential function as shown in Eq 4. The parameter δ helps to deactivate the marginal inclusivity (entropy) score and amplify the high entropy score. The parameter ρ helps to amplify the reward values to [0, ρ). Fig.  1  (1d, 1e, & 1f) shows the example of computing reward score given the past m labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Oris Training And Inference",
      "text": "Once we model the ORIS as an MDP problem, we train it using the DQN training strategy  [9] . Our goal is to learn an optimal policy such that the oracle makes fewer errors, and the ML model is trained with diverse error-free documents. Algorithm 1 describes the training procedure of our proposed ORIS method. It takes a set of documents D as input, which we use to create different streaming scenarios. Moreover, similar to the original approach of DQN  [9] , we use a replay buffer R to store the state-action transition (s t , a t , r t+1 , s t+1 ). We also initialize two Q-networks: source and target, each with weights θ and θ -, respectively (Line 2). It consists of three dense layers with ReLU activation as in Eq 5.\n\nAn episode is calculated once the agent has picked the documents equivalent to the budget of B. We train the DQN for several episodes until E max . At each episode, we shuffle the dataset D to explore different real-time streaming scenarios (Line 4). We keep track of all picked documents and its label in memory M t . We use M t to compute the time last seen ∆t for state representation (c.f. Eq 3) and to compute the reward (c.f. Eq 4). Once we create real-time streaming data D i for episode i, we extract each document d t and compute the state representation s t for the agent (Line 6). To make the DQN explore different actions, we keep an exploration rate ϵ and exponentially decrease it over time. We use this exploration rate as a probability to choose random action as opposed to greedy action based on the Q-value as shown in Line 6. If the action a t = 1 (pick), we fetch the oracle label for the current input d t (Line 8  it the same as the previous value. We also update the budgetexhausted counter b for the current episode i. Based on the action a t and the current memory M t , we compute the reward using Eq 4 (Line 10). Next, we compute the succeeding state representation to store transition in the replay buffer R (Line 10-11). To train DQN, we sample a minibatch of transitions from R and compute the predicted Q-value (Line 12-13). We update source Q-network weights θ by minimizing the smooth L1 loss between the predicted Q-value and the source Q-value (Line 14). We also soft update the target Q-network weights θ -with the weights of source Q-network θ using the factor τ at every step (Line 15). During inference, we use the trained source Q-network with weights θ as 'Get Sampling Decision' function in Fig.  2  to perform the robust online active learning sampling.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Experiments",
      "text": "In the experiment, we answer four research questions.\n\n• RQ1: How does the ORIS method reduce the impact of error-prone oracle and improve both ML model and human performance? • RQ2: How effective is the ORIS method when transferred across domains from Twitter to Reddit? • RQ3: What is the long-term effect on the performance of the ORIS method as the budget gets exhausted? • RQ4: How quickly and efficiently does the ORIS method perform?\n\nA. Dataset We implement the ORIS method for online active learning in emotion recognition, a well-studied complex natural language understanding task. We use two famous social media datasets. The first dataset comprises of labeled Twitter posts with six emotions  [26] . The second dataset comprises of labeled Reddit posts with manually annotated 27 emotions for (RQ2)  [27] . For our experiments, we filtered out the documents corresponding to only five common emotions classes (sadness, joy, surprise, anger, and fear). The Twitter dataset from the Huggingface Datasets 1  had two groups: split with train/validation/test distinction, and unsplit. We use the unsplit group minus the split data for DQN training. For both Twitter and Reddit data, we use train & test splits for active learning demonstration and machine performance evaluation, respectively. Table  I  shows the dataset distribution. Note that the datasets are highly imbalanced and are prone to memory decay during the labeling process.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Configuration 1) Hyperparameter Tuning:",
      "text": "The ORIS method has several parameters, outlined here. To train the DQN agent, we run for E max = 10, 000 episodes with budget B = 500. We kept the replay buffer R size to 50, 000. The DQN architecture shown in Eq 5 has dense layer sizes of 256, 256, and 2, respectively. The last layer represents the policy for output actions: pick or discard. It takes input from the state defined in Eq 3. We use FastText 2 word embeddings for document representation  [28] ,  [29] . To compute the embeddings, consider the current streaming document d t at time t as set of words [w 1 , w 2 , ..., w n ] of length n, we compute embeddings as emb t = w ϵ d t emb w n . The exploration rate ϵ is initialized with 0.9 and exponentially decays to 0.05 with the decay rate 2 https://fasttext.cc/docs/en/aligned-vectors.html of 0.0005. The discount factor γ is set to 0.99. The mini-batch transition size to sample from the replay buffer R is set to 512. We update the target network with factor = 0.005. The DQN minimizes the smooth L1 loss with a learning rate of 1e-4. To compute the reward in Eq 4, we use the past m = 10 memory to get the Inclusivity score. The small positive reward for exploration λ is set to 0.01. The parameter ρ in Eq 4 is set to 5. During inference, we use two large language models as ML models to fine-tune in an online active learning setting: BERT-Mini  [30] ,  [31]  and mBERT Base  [32] . For both pretrained models, the embedding layer is frozen during fine-tuning. The model updates with update frequency f = 25 till the budget B = 500 is exhausted. For each fine-tuning , we keep the batch size to 8, the learning rate to 2e -5, the number of epochs for training to 5, and weight decay to 0.01.\n\n2) Experimental Setting: We propose two variations of ORIS with the reward activation δ. ORIS (δ = 16): In this experiment, the reward is heavily deactivated for low scores of inclusivity, e.g., the inclusivity score of < 0.8 has a negligible reward. ORIS (δ = 8): In this, the reward quickly activates compared to the previous experiment. However, the lower score of inclusivity has a negligible reward. Furthermore, we mimic two oracles with different parameterized memory decay explained in the section III-B:\n\n-Sigmoid [Slow] Forgetting: We use the sigmoid-based decay function (c.f. Eq 1) with α = 0.3 and β = 9.\n\n-Exponential [Fast] Forgetting: We use the exponential-based decay function (c.f. Eq 2) with α = 0.6 and β = -19. We report the results of five runs with different random ordering to simulate streaming settings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Baseline Techniques",
      "text": "We compared the proposed ORIS method to several traditional online active learning. Since our approach to computing inclusivity is loosely based on diversity sampling idea, we also include additional experiments of offline diversity-samplingbased active learning  [33] ,  [34] . To our knowledge, we could not find any online diversity sampling technique. Here is the list of different baseline methods:\n\n1. [Online] Random Sampling: The agent randomly decides to pick or discard the current instance. It is highly dependent on data distribution.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "[Online]",
      "text": "Uncertainty Sampling: The agent utilizes the ML model's confidence to compute entropy and use a threshold for decision, which we dynamically reduce with the budget utilization  [35] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "[Offline]",
      "text": "Diversity Sampling: The agent uses offline agglomerative clustering  [33]  to create B clusters and sample B documents closest to the center.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Evaluation Metrics",
      "text": "We report f1-macro scores for both machine and human performance. For machine performance, we rely on the independent test data. At every update frequency f , when the ML model is trained with labels provided by the oracle, we compute the f1 scores for each class c as f 1 c and then, f1macro score. For human performance, we rely on the labels provided by the oracle for the selected documents. To compute f1 scores for each class c as f 1 c , true positive means when the oracle labeled the document with class c correctly. Similarly, false positive means when the oracle erroneously labeled the document as class c but originally belonged to another class, and false negative means when the oracle erroneously labeled the document that originally belonged to class c to another class. Finally, we compute the f1-macro score.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Results & Discussion",
      "text": "To answer the RQs stated in Section V, we report the results of both the evaluation metrics for the online active learning implementation for two datasets. Table  II  and III show the final performance when the budget B = 500 is exhausted for each experiment. We divide the results into four parts. In the first part, we explore the final performance of active learning in the error-prone environment for Twitter data, and compare the ORIS method with baselines. In the second part, we focus on the effect of ORIS method when used for active learning on cross-domain Reddit data. In the third part, we focus on long-term effect of the proposed ORIS method. Finally, in the fourth part, we compare the inference speed of the different active learning demonstrations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Performance Comparison With Oris (Rq1)",
      "text": "Table  II  shows the results of experiments conducted with the Twitter dataset. We observe that the final human performance is significantly lower in the baselines compared to the proposed ORIS method. We also observe that there is a good machine performance degradation even in the robust large language model like BERT Mini and mBERT if there is a significant amount of labeling error present. We observe the degradation in f1-macro score as low as 40.4% for random sampling compared to the maximum of 56.2% for the proposed ORIS method with δ = 8 when trained with an equal number of documents (B = 500). Similarly, the difference can reach from 46.8% for random sampling compared to the maximum of 60.6% for the proposed ORIS sampling with δ = 8 when fine-tuning an mBERT model. Moreover, within the baseline strategies, diversity sampling has better human performance than random or uncertaintybased sampling showing that diversity sampling can explore and extract inclusive samples to reduce human errors of the oracle, further improving the machine performance. We observe that random sampling has high variance since it is based on the distribution of the incoming document stream.\n\nFurthermore, both the proposed ORIS methods performed significantly better than the baselines in terms of both the human performance of error-prone oracle and the machine performance of the two BERT models. However, the experiments with δ = 8 have higher and more stable performance than δ = 16. We believe that with δ = 8, the reward values are more informative for robust and efficient DQN training.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Domain Transfer Effect Comparison (Rq2)",
      "text": "Table  III  shows the results of experiments conducted on the Reddit emotion dataset. Since the Reddit dataset is crossdomain for the proposed ORIS method and has reliable manual labels, the results demonstrate the significance of our approach for a more general setting. Similar to Table  II , we observe that the proposed ORIS method improved both BERT Mini and mBERT models compared to all the baselines, including the offline diversity sampling. We observe that the offline diversity sampling had lower human performance compared to other baselines, contrary to what we observe in the Twitter dataset. We believe it could be due to the smaller sample size active learning sampling. Since the Reddit training data is ≈ 4 times less than the Twitter data, the offline diverse sampling may represent diversity among the same class. Hence, the sampled documents were still prone to memory decay-based errors from the error-prone oracle. However, the ORIS method outperformed all baselines in this dataset. The ORIS method with δ = 8 achieved an f1-macro score of 76.0% on the BERT Mini model and 79.0% on the mBERT model. We observe the effect of degradation in human performance in the baselines that do not take inclusivity into consideration. The human performance has reached as low as 66.8% f1-macro score in the baseline uncertainty sampling. While the ORIS method reached 99.9% f1-macro in human performance, which signifies that the oracle, despite being prone to memory decay, was able to make close to zero errors in labeling.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Long Term Effect Comparison (Rq3)",
      "text": "In this section, we look into the performance progress in our experiments through Fig.  3 . Collectively, we observe that the proposed ORIS method starts gaining significant performance improvement in machine performance with budget utilization of as low as 150 documents. However, the human performance gain is visible when the budget utilization is as low as 50 documents. It shows that even if the effect of labeling errors does not directly correlate with machine performance, it will eventually hamper the machine's performance in the long run. Based on the error-prone oracle design as defined in Section 3.2, the human performance is highest at the beginning as the classes are just learned. However, we observe that human performance degrades at a much higher acceleration for baselines as compared to the proposed ORIS method. We observe that uncertainty sampling is most affected by human performance compared to other baselines since it does not consider the distribution of the incoming data. Even though the uncertainty sampling chose informative samples to improve machine performance, due to the high erroneous labels provided by the oracle, it does not improve the machine performance compared to the other baselines. There are several other hyperparameters that can be tweaked for better performance, which we will study in the future. Table  IV  shows the average experiment duration when completing the sampling of B documents and the performance evaluation at every update frequency f . For the diversity sampling, we do not average as there was only one run per experiment. All experiments ran on Linux machine with four CPU cores, 32GB memory, and one Nvidia A100 GPU with 40GB memory for GPU computation (for BERT model finetuning and DQN inference). We observe that the experiments with mBERT fine-tuning took significantly more time than the BERT Mini fine-tuning. It is evident because the size of the mBERT model is larger than the BERT mini. Moreover, since Twitter data exploration was higher than Reddit data, it took a long time to finish the experiments. We observe that both of our proposed ORIS methods are faster than uncertainty or diversity sampling in all experimental settings. It could be because calculating the confidence of each documents in uncertainty sampling can be time-consuming. Moreover, diversity sampling requires the computation of the clustering algorithm for entire data beforehand, which is, again, time-consuming. And the proposed ORIS method performs similarly to random sampling, which makes the sampling decision in near real-time. The only time-consuming part of the experiments was fine-tuning of BERT model, which was common in all experiments. Overall, the results demonstrate that the proposed method, ORIS, is highly effective for online active learning in real-world error-prone settings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Inference Speed Comparison (Rq4)",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion & Future Work",
      "text": "In this paper, we address the challenge of human memory decay that causes errors in data labeling process, by designing a novel and efficient online active learning-based streaming analytics system. We presented ORIS, a novel method that addresses both human and machine performance challenges through a reinforcement learning problem formulation. We introduced a novel Inclusivity factor in designing the reward of a Deep Q-Network. We evaluated the ORIS method on emotion recognition tasks with traditional active learning baselines and analyzed both human and machine performance. We observe that the traditional baselines are prone to human errors when we use the slip-based error-prone oracle. Whereas the ORIS method reduces human errors by inclusively sampling documents and thus, improving the ML model performance. We observe this performance improvement on two datasets: Twitter and Reddit. Moreover, the ORIS method starts gaining significant machine performance improvement with budget utilization of 150 documents, and visible human performance gain with the budget utilization of as low as 50. It shows that the labelling errors eventually hamper the machine's performance in the long run. Finally, the inference speed of the proposed ORIS method is close to real-time, outperforming traditional baselines. Future research could test ORIS for other labeling tasks and study the effect of its different hyperparameters in building a robust and inclusive online active learningbased HITL-ML system for streaming analytics.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed ORIS architecture. The method aims to sample inclusive documents from streams using a DQN-based agent. Components",
      "page": 2
    },
    {
      "caption": "Figure 1: ). 3) With extensive",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the",
      "page": 2
    },
    {
      "caption": "Figure 2: Online Active Learning System Flow.",
      "page": 3
    },
    {
      "caption": "Figure 1: shows the overall architecture of ORIS. It comprises",
      "page": 3
    },
    {
      "caption": "Figure 1: (1a) shows the example of keeping",
      "page": 4
    },
    {
      "caption": "Figure 1: illustrates",
      "page": 4
    },
    {
      "caption": "Figure 1: (1d, 1e, & 1f) shows the example of computing reward score",
      "page": 5
    },
    {
      "caption": "Figure 2: to perform the robust online active learning",
      "page": 5
    },
    {
      "caption": "Figure 3: Machine and Human performance comparison over budget exhausted for both Twitter and Reddit Dataset when using the BERT mini and mBERT",
      "page": 6
    },
    {
      "caption": "Figure 3: Collectively, we observe that the",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Experiment": "",
          "BERT Mini": "Sigmoid\nExponential",
          "mBERT": "Sigmoid\nExponential"
        },
        {
          "Experiment": "",
          "BERT Mini": "Machine\nHuman\nMachine\nHuman",
          "mBERT": "Machine\nHuman\nMachine\nHuman"
        },
        {
          "Experiment": "Random",
          "BERT Mini": "40.4 ± 8.6\n64.9 ± 11.2\n47.3 ± 6.3\n72.1 ± 5.4",
          "mBERT": "47.0 ± 5.8\n68.3 ± 6.5\n46.8 ± 1.2\n68.7 ± 2.9"
        },
        {
          "Experiment": "Uncertainty",
          "BERT Mini": "38.9 ± 2.4\n62.7 ± 3.9\n41.1 ± 2.7\n65.3 ± 2.7",
          "mBERT": "48.8 ± 6.4\n68.4 ± 8.0\n46.4 ± 9.2\n67.7 ± 4.6"
        },
        {
          "Experiment": "Diversity",
          "BERT Mini": "51.0 ± 0\n81.2 ± 0\n36.1 ± 0\n63.5 ± 0",
          "mBERT": "50.4 ± 0\n77.7 ± 0\n50.4 ± 0\n77.7 ± 0"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Experiment": "",
          "BERT Mini": "Sigmoid\nExponential",
          "mBERT": "Sigmoid\nExponential"
        },
        {
          "Experiment": "",
          "BERT Mini": "Machine\nHuman\nMachine\nHuman",
          "mBERT": "Machine\nHuman\nMachine\nHuman"
        },
        {
          "Experiment": "Random",
          "BERT Mini": "59.6 ± 4.5\n81.2 ± 5.3\n59.0 ± 4.9\n80.0 ± 6.1",
          "mBERT": "59.0 ± 2.1\n77.9 ± 3.1\n57.5 ± 3.8\n75.5 ± 5.9"
        },
        {
          "Experiment": "Uncertainty",
          "BERT Mini": "50.2 ± 3.5\n71.6 ± 3.1\n49.9 ± 3.9\n71.7 ± 2.4",
          "mBERT": "57.2 ± 1.6\n67.2 ± 2.1\n55.2 ± 2.5\n66.8 ± 2.4"
        },
        {
          "Experiment": "Diversity",
          "BERT Mini": "51.1 ± 0\n68.5 ± 0\n54.8 ± 0\n77.2 ± 0",
          "mBERT": "51.4 ± 0\n68.3 ± 0\n46.5 ± 0\n69.3 ± 0"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "MACHINE AND HUMAN PERFORMANCE REPRESENT THE MEAN AND STANDARD DEVIATION OF f1-macro SCORE OF FIVE RANDOM RUNS",
      "authors": [
        "B = 500"
      ],
      "venue": "MACHINE AND HUMAN PERFORMANCE REPRESENT THE MEAN AND STANDARD DEVIATION OF f1-macro SCORE OF FIVE RANDOM RUNS"
    },
    {
      "citation_id": "2",
      "title": "TABLE III PERFORMANCE COMPARISON FOR REDDIT DATASET IN AN ACTIVE LEARNING SETTING WITH BUDGET B = 500. MACHINE AND HUMAN PERFORMANCES REPRESENT THE MEAN AND STANDARD DEVIATION OF f1-macro SCORE OF FIVE RANDOM RUNS",
      "venue": "TABLE III PERFORMANCE COMPARISON FOR REDDIT DATASET IN AN ACTIVE LEARNING SETTING WITH BUDGET B = 500. MACHINE AND HUMAN PERFORMANCES REPRESENT THE MEAN AND STANDARD DEVIATION OF f1-macro SCORE OF FIVE RANDOM RUNS"
    },
    {
      "citation_id": "3",
      "title": "Big crisis data: social media in disasters and time-critical situations",
      "authors": [
        "C Castillo"
      ],
      "year": "2016",
      "venue": "Big crisis data: social media in disasters and time-critical situations"
    },
    {
      "citation_id": "4",
      "title": "A survey on concept drift adaptation",
      "authors": [
        "J Gama"
      ],
      "year": "2014",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/2523813"
    },
    {
      "citation_id": "5",
      "title": "A Survey of Deep Active Learning",
      "authors": [
        "P Ren"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys",
      "doi": "10.1145/3472291"
    },
    {
      "citation_id": "6",
      "title": "Engineering Crowdsourced Stream Processing Systems",
      "authors": [
        "M Imran"
      ],
      "year": "2014",
      "venue": "Engineering Crowdsourced Stream Processing Systems",
      "arxiv": "arXiv:1310.5463"
    },
    {
      "citation_id": "7",
      "title": "Design Patterns for Hybrid Algorithmic-Crowdsourcing Workflows",
      "authors": [
        "C Lofi",
        "K Maarry"
      ],
      "year": "2014",
      "venue": "IEEE CBI"
    },
    {
      "citation_id": "8",
      "title": "Modeling human annotation errors to design bias-aware systems for social stream processing",
      "authors": [
        "R Pandey",
        "C Castillo",
        "H Purohit"
      ],
      "year": "2019",
      "venue": "Modeling human annotation errors to design bias-aware systems for social stream processing"
    },
    {
      "citation_id": "9",
      "title": "Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning",
      "authors": [
        "R Pandey",
        "H Purohit",
        "C Castillo",
        "V Shalin"
      ],
      "year": "2022",
      "venue": "IJHCS"
    },
    {
      "citation_id": "10",
      "title": "Human error: models and management",
      "authors": [
        "J Reason"
      ],
      "year": "2000",
      "venue": "BMJ"
    },
    {
      "citation_id": "11",
      "title": "Human-level control through deep reinforcement learning",
      "authors": [
        "V Mnih"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "12",
      "title": "Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach",
      "authors": [
        "O Rudovic",
        "M Zhang",
        "B Schuller",
        "R Picard"
      ],
      "year": "2019",
      "venue": "ICMI",
      "doi": "10.1145/3340555.3353742"
    },
    {
      "citation_id": "13",
      "title": "Cold-start Active Learning through Self-supervised Language Modeling",
      "authors": [
        "M Yuan",
        "H.-T Lin",
        "J Boyd-Graber"
      ],
      "year": "2020",
      "venue": "EMNLP. Online: ACL"
    },
    {
      "citation_id": "14",
      "title": "Feedback-driven multiclass active learning for data streams",
      "authors": [
        "Y Cheng"
      ],
      "year": "2013",
      "venue": "CIKM",
      "doi": "10.1145/2505515.2505528"
    },
    {
      "citation_id": "15",
      "title": "Online active learning with expert advice",
      "authors": [
        "S Hao"
      ],
      "year": "2018",
      "venue": "TKDD"
    },
    {
      "citation_id": "16",
      "title": "Active Learning With Drifting Streaming Data",
      "authors": [
        "I Žliobaitė",
        "A Bifet",
        "B Pfahringer",
        "G Holmes"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "17",
      "title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach",
      "authors": [
        "M Fang",
        "Y Li",
        "T Cohn"
      ],
      "year": "2017",
      "venue": "EMNLP"
    },
    {
      "citation_id": "18",
      "title": "Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification",
      "authors": [
        "Z Liu"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "19",
      "title": "Reinforced active learning for image segmentation",
      "authors": [
        "A Casanova",
        "P Pinheiro",
        "N Rostamzadeh"
      ],
      "year": "2019",
      "venue": "Reinforced active learning for image segmentation"
    },
    {
      "citation_id": "20",
      "title": "AI-Based Request Augmentation to Increase Crowdsourcing Participation",
      "authors": [
        "J Park"
      ],
      "year": "2019",
      "venue": "HCOMP"
    },
    {
      "citation_id": "21",
      "title": "Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research",
      "authors": [
        "J Vaughan"
      ],
      "year": "2018",
      "venue": "JMLR"
    },
    {
      "citation_id": "22",
      "title": "Some tests of the decay theory of immediate memory",
      "authors": [
        "J Brown"
      ],
      "year": "1958",
      "venue": "Quarterly journal of experimental psychology"
    },
    {
      "citation_id": "23",
      "title": "Memory: A Contribution to Experimental Psychology",
      "authors": [
        "H Ebbinghaus"
      ],
      "year": "1913",
      "venue": "Memory: A Contribution to Experimental Psychology"
    },
    {
      "citation_id": "24",
      "title": "Reflections of the environment in memory",
      "authors": [
        "J Anderson",
        "L Schooler"
      ],
      "year": "1991",
      "venue": "Psychological science"
    },
    {
      "citation_id": "25",
      "title": "Evaluating forgetting curves",
      "authors": [
        "G Loftus"
      ],
      "year": "1985",
      "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition"
    },
    {
      "citation_id": "26",
      "title": "A markovian decision process",
      "authors": [
        "R Bellman"
      ],
      "year": "1957",
      "venue": "Journal of mathematics and mechanics"
    },
    {
      "citation_id": "27",
      "title": "A mathematical theory of communication",
      "authors": [
        "C Shannon"
      ],
      "year": "2001",
      "venue": "ACM SIGMOBILE Mobile Computing and Communications Review",
      "doi": "10.1145/584091.584093"
    },
    {
      "citation_id": "28",
      "title": "CARER: Contextualized affect representations for emotion recognition",
      "authors": [
        "E Saravia"
      ],
      "year": "2018",
      "venue": "EMNLP. Brussels, Belgium: ACL"
    },
    {
      "citation_id": "29",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky"
      ],
      "year": "2020",
      "venue": "ACL. Online: ACL"
    },
    {
      "citation_id": "30",
      "title": "Enriching word vectors with subword information",
      "authors": [
        "P Bojanowski",
        "E Grave",
        "A Joulin",
        "T Mikolov"
      ],
      "year": "2017",
      "venue": "TACL"
    },
    {
      "citation_id": "31",
      "title": "Loss in translation: Learning bilingual word mapping with a retrieval criterion",
      "authors": [
        "A Joulin"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "32",
      "title": "Generalization in nli: Ways (not) to go beyond simple heuristics",
      "authors": [
        "P Bhargava",
        "A Drozd",
        "A Rogers"
      ],
      "year": "2021",
      "venue": "Generalization in nli: Ways (not) to go beyond simple heuristics"
    },
    {
      "citation_id": "33",
      "title": "Well-read students learn better: On the importance of pre-training compact models",
      "authors": [
        "I Turc",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2020",
      "venue": "Well-read students learn better: On the importance of pre-training compact models"
    },
    {
      "citation_id": "34",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "35",
      "title": "Active Learning with Clustering",
      "authors": [
        "Z Bodó",
        "Z Minier",
        "L Csató"
      ],
      "year": "2011",
      "venue": "ALED workshop (AISTATS 2010)"
    },
    {
      "citation_id": "36",
      "title": "Off to a good start: Using clustering to select the initial training set in active learning",
      "authors": [
        "R Hu"
      ],
      "year": "2010",
      "venue": "Off to a good start: Using clustering to select the initial training set in active learning"
    },
    {
      "citation_id": "37",
      "title": "Cost-effective active learning for deep image classification",
      "authors": [
        "K Wang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    }
  ]
}