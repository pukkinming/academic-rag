{
  "paper_id": "2504.03707v1",
  "title": "Towards Practical Emotion Recognition: An Unsupervised Source-Free Approach For Eeg Domain Adaptation",
  "published": "2025-03-26T14:29:20Z",
  "authors": [
    "Md Niaz Imtiaz",
    "Naimul Khan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is crucial for advancing mental health, healthcare, and technologies like braincomputer interfaces (BCIs). However, EEG-based emotion recognition models face challenges in cross-domain applications due to the high cost of labeled data and variations in EEG signals from individual differences and recording conditions. Unsupervised domain adaptation methods typically require access to source domain data, which may not always be feasible in real-world scenarios due to privacy and computational constraints. Source-free unsupervised domain adaptation (SF-UDA) has recently emerged as a solution, enabling target domain adaptation without source data, but its application in emotion recognition remains unexplored. We propose a novel SF-UDA approach for EEG-based emotion classification across domains, introducing a multi-stage framework that enhances model adaptability without requiring source data. Our approach incorporates Dual-Loss Adaptive Regularization (DLAR) to minimize prediction discrepancies on confident samples and align predictions with expected pseudo-labels. Additionally, we introduce Localized Consistency Learning (LCL), which enforces local consistency by promoting similar predictions from reliable neighbors. These techniques together address domain shift and reduce the impact of noisy pseudolabels, a key challenge in traditional SF-UDA models. Experiments on two widely used datasets, DEAP and SEED, demonstrate the effectiveness of our method. Our approach significantly outperforms state-of-the-art methods, achieving 65.84% accuracy when trained on DEAP and tested on SEED, and 58.99% accuracy in the reverse scenario. It excels at detecting both positive and negative emotions, making it well-suited for practical emotion recognition applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion, a fundamental mental phenomenon, is deeply connected to human cognition and behavior, shaping daily life experiences. Recognizing emotions is a critical focus in psychology, medicine, and engineering. In psychology, it provides a quantitative foundation for understanding emotion-related behaviors. In medicine, it aids in the diagnosis and treatment of conditions such as autism spectrum disorder (ASD) and depression and helps track patient recovery  [1, 2] . In engineering, emotion recognition advances affective brain-computer interfaces (aBCIs), enabling better integration of emotional states into technology.\n\nPhysiological signals, such as electroencephalogram (EEG), offer a more objective approach to emotion recognition than non-physiological signals like facial expressions, gestures, and voice, which can be influenced by environmental or social factors  [3, 4] . EEG measures brain activity through electrical signals, providing high reliability and precision, while also being portable and cost-effective.\n\nAdvancements in deep learning have greatly improved EEG decoding performance through various neural network models  [5] [6] [7] . However, these models generally perform well on test data with a distribution similar to the training data, but their performance significantly declines when applied to a new domain with a different data distribution, a phenomenon known as domain shift  [8] . The considerable variations in EEG signals across individuals challenge the effectiveness of cross-domain emotion recognition models. Additionally, deep neural networks typically require a vast quantity of labeled data for training. Collecting and annotating EEG signals is often a labor-intensive and costly process.\n\nUnsupervised domain adaptation (UDA) is gaining attention to address these challenges, aiming to enhance model performance on the target domain by utilizing information from the source domain. Recent research on domain adaptation for classifying EEG emotions has largely focused on cross-subject and cross-session scenarios within the same dataset  [9] [10] [11] [12] [13] . However, less attention has been given to the cross-dataset scenario, which presents even greater challenges due to variations in EEG signals not only from individual differences but also from differences in EEG recording settings and stimuli.\n\nAnother limitation of traditional unsupervised domain adaptation methods is that they require access to the source domain data while adapting to the target domain. However, in many real-world applications, accessing the source data of a trained source model is not feasible. For example, when deploying domain adaptation algorithms on mobile devices with constrained computational resources, keeping source domain data on the device is impractical. Additionally, to preserve the privacy of source subjects, their data may be inaccessible during domain adaptation. This is particularly true for healthcare applications, where data sharing is complicated due to ethical concerns. With the lack of source samples, traditional distribution matching techniques become infeasible.\n\nSource-free unsupervised domain adaptation (SF-UDA) focuses on transferring knowledge from a source domain to a target domain without requiring access to the source dataset, thus inherently protecting individual privacy  [14] . Unlike UDA, where labeled source domain data, the trained model, and unlabeled target data are available, SF-UDA relies solely on the trained model and unlabeled target data, with no access to source domain data (Fig.  1 ). SF-UDA faces several challenges. Biases in the source data during pre-training can cause the model to focus on domain-specific features rather than learning fundamental patterns. Additionally, without access to source data, target adaptation often suffers due to the generation of noisy pseudo-labels for the target data.\n\nTo address the challenges outlined above, we propose a novel source-free unsupervised domain adaptation (SF-UDA) method for EEG-based emotion classification. SF-UDA has primarily been applied to image classification tasks  [15] [16] [17] [18] [19] [20] , with only a few studies exploring its use for EEG signal classification, such as seizure and sleep stage classification  [21] [22] [23] . To the best of our knowledge, this is the first application of SF-UDA to EEG-based emotion recognition. Our method comprises four stages: pre-training, computation, target adaptation, and inference. The pre-training and inference stages, as well as the network architecture-a feature extractor followed by two parallel classifiers-are adapted from our previous work  [24] .\n\nDuring the pre-training stage, the model is trained on labeled source samples to learn essential features for emotion recognition. Once pre-training is complete, the source domain data is no longer accessible for the subsequent stages. The computation stage calculates cluster centroids and classifier discrepancies using the pre-trained model's predictions on the target domain data. These calculations lay the foundation for target adaptation.\n\nThe target adaptation stage involves two training steps using unlabeled target data. In the first step, Dual-Loss Adaptive Regularization (DLAR), the model is trained to minimize classifier discrepancies on confident samples while aligning its predictions with the calculated pseudo-labels. This ensures the model produces consistent and reliable predictions. The second step, Localized Consistency Learning (LCL), enforces local consistency by encouraging reliable neighbors to produce similar predictions. To mitigate the influence of unreliable neighbors, we introduce a mechanism for identifying reliable neighbors based on both feature values and softmax probabilities. In the inference stage, we apply our recently proposed Prediction Confidence-aware Test-Time Augmentation (PC-TTA)  [24] , which selectively augments test samples based on predictive confidence, improving model prediction performance while minimizing computational costs.\n\nOur proposed method is evaluated on two widely used emotion recognition datasets, DEAP  [25]  and SEED  [26] . To assess its performance in cross-dataset settings, where distribution discrepancies are more pronounced due to differences in subject demographics and recording conditions, we conduct experiments by training the model on one dataset and evaluating it on the other. Since no SF-UDA methods currently exist for EEG emotion recognition, we compare our approach with four recent, open-source SF-UDA methods  [15, 16, 21, 22]  that are recognized for their high performance. This comparison is carried out by evaluating all methods on the same datasets used in our experiments. Our method outperforms all other methods by a significant margin, achieving accuracies of 58.99% (SEED → DEAP) and 65.84% (DEAP → SEED).\n\nThe key contributions of this paper are as follows:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Dynaseg",
      "text": "(1) A novel source-free unsupervised domain adaptation (SF-UDA) method is introduced for EEG-based emotion recognition, an area where SF-UDA has not previously been applied.\n\n(2) A robust multi-stage framework is proposed to effectively address the challenges of cross-domain emotion recognition, enhancing the model's ability to adapt across diverse datasets.\n\n(3) A novel technique called Dual-Loss Adaptive Regularization (DLAR) is proposed, which minimizes classifier discrepancies on confident samples and aligns predictions with pseudo-labels, ensuring consistent and reliable performance.\n\n(4) Localized Consistency Learning (LCL) is introduced to enforce local consistency in predictions by encouraging similar outcomes from reliable neighbors.\n\nThe structure of the paper is outlined as follows: Section 2 provides an overview of the related literature. Section 3 describes our proposed method in detail, including the model architecture, training and testing processes, and data preprocessing. Section 4 presents the experimental setup, discusses the datasets used, and reports the results of the experiments, including both quantitative and qualitative evaluations. The paper concludes in Section 5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Extensive research has been conducted on classifying EEG signals using machine learning techniques. Commonly used methods include Support Vector Machines  [27] [28] [29] , Decision Trees  [30, 31] , Logistic Regression  [32, 33] , and Linear Discriminant Analysis  [34, 35] . However, traditional shallow learning techniques often struggle to effectively capture the intricate temporal dynamics of EEG signals, thereby limiting their generalizability to new or unseen data. In deep learning, Autoencoders  [36, 37] , Self-Organizing Maps  [38, 39] , and Deep Belief Networks  [40, 41]  have been explored. Moreover, Convolutional Neural Networks and Long Short-Term Memory networks are frequently employed for EEG classification tasks  [42] [43] [44] [45] .\n\nUnsupervised Domain Adaptation (UDA) is a machine learning technique that addresses domain shift by transferring knowledge from a labeled source domain to an unlabeled target domain. Many UDA-based approaches for EEG emotion classification focus on feature-based methods, enhancing model generalization by aligning or transforming features between domains. They reduce domain shift by either learning a shared feature space or minimizing discrepancies between feature distributions. For classifying emotions across subjects, Jiménez-Guarneros et al. proposed a method that aligns subject distributions in the global feature space and fine-tunes the alignment between modalities within each domain's distribution  [10] . She et al.  [11]  developed a method for selecting a source domain with limited labeled target data, using manifold feature mapping to mitigate data drift and minimum redundancy maximum correlation to improve classifier performance.\n\nAdversarial-based methods, another prominent UDA approach, minimize domain shift through adversarial training to learn domain-invariant features. The feature extractor produces features that are indistinguishable between the source and target domains, while the discriminator distinguishes between them. Huang et al.  [12]  introduced a knowledge-DynaSeg free approach to align source domain features with the target domain through adversarial learning while preserving emotional information using EEG content and emotion-related loss functions. Li et al.  [46]  incorporated hemispherespecific features through adversarial learning to address domain shifts.\n\nOnly a few studies have investigated the cross-dataset scenario for EEG emotion recognition, which presents significant challenges. These challenges arise from variations in EEG signals caused by individual differences, as well as differences in recording settings and stimuli. Ni et al.  [47]  presented a domain-adaptive sparse representation classifier that projects source and target domain samples into a shared subspace, learning a domain-invariant dictionary using a local information-preserving criterion. In our previous work  [24] , we introduced a gradual proximity-guided target data selection technique to mitigate the risk of negative transfer by gradually selecting reliable target domain samples. Additionally, we incorporated a cost-effective test-time augmentation strategy to enhance model performance during inference, applying augmentations only when necessary.\n\nConventional UDA methods rely on accessing source data during adaptation, which is not always feasible due to privacy concerns or computational limitations. To overcome this, researchers have recently turned to source-free domain adaptation (SFDA), which facilitates knowledge transfer using a source-pretrained model and unlabeled target data without requiring access to the original source dataset. SFDA has been extensively explored in the field of computer vision  [15] [16] [17] [18] [19] [20] . Based on existing works, SFDA can be categorized into two directions  [48] : data-centric  [16] [17] [18] [19] [49] [50] [51]  and model-centric  [14, 15, 20, [52] [53] [54] .\n\nData-centric methods aim to reconstruct or subdivide the target domain to compensate for the absence of source domain data, facilitating the extension of UDA methods to the SFDA setting. These methods focus on enhancing the target domain's data to refine its representation and improve model generalization. The method proposed by Lee and Lee reduces the domain gap by aligning prediction and feature spaces, incorporating data augmentation and consistency objectives, while also addressing epistemic uncertainty to enhance model adaptation  [17] . Li et al. proposed an approach that designs adversarial examples to attack the training model, leveraging these examples to improve generalization and address the divergence-agnostic learning challenge  [18] . In their method, Du et al. create and expand a pseudo-source domain using target samples to reduce domain discrepancy, utilizing four loss functions  [16] .\n\nModel-centric methods assume that the source-pretrained model generalizes well to the target domain due to shared characteristics. Fine-tuning the model by leveraging its predictions on the target data helps bridge the domain gap, improving adaptability through adjustments in the model's structure, parameters, or training strategy. Liang et al. proposed a method that freezes the source classifier and adapts the feature extractor to the target domain using information maximization and pseudo-labeling  [14] . Ahmed et al. combined model compression with spectral-norm-based loss penalties to remove malicious channels, restoring accuracy through dynamic knowledge transfer during target domain training  [15] . Kothandaraman introduced an approach for adapting a pre-trained network to a target domain with limited annotations and label space shifts, optimizing sample selection and knowledge transfer for task-agnostic, source-free adaptation  [52] .\n\nWhile considerable research on SFDA has been conducted using image data, few studies have explored its application to time series data  [21] [22] [23] . Ragab et al. presented a method that captures temporal information through random masking and a temporal imputer, guiding the target model to align with source domain features while preserving temporal consistency  [21] . Zhao and Peng introduced SS-TrBoosting, a boosting-based SFDA approach for EEG seizure subtype classification, and extended it to U-TrBoosting for unsupervised SFDA  [22] .\n\nDespite remarkable progress in deep learning for EEG-based emotion recognition, challenges remain in cross-domain contexts, particularly when significant domain shifts create substantial disparities between source and target domains. The absence of source samples further complicates the situation, making traditional distribution-matching methods ineffective. While source-free domain adaptation offers a potential solution to these issues, its application in emotion recognition has yet to be explored. To address these gaps, we propose a novel source-free unsupervised domain adaptation method tailored for EEG-based emotion classification. Our approach focuses on mitigating domain discrepancies in cross-dataset scenarios, overcoming the scarcity of labeled data, and handling the unavailability of source datasets during target adaptation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Our proposed approach consists of four stages: pre-training, computation, target adaptation, and inference. The pre-training and inference stages remain consistent with our prior UDA model  [24] . However, we introduce new computation and target adaptation stages to facilitate efficient adaptation to the target domain without relying on data from the source domain.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dynaseg",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Framework",
      "text": "The framework of the proposed method is presented in Fig.  2 . Suppose the source domain consists of N s labeled samples X s = {x i s } Ns i=1 and their corresponding labels Y s = {y i s } Ns i=1 , while the target domain contains N t unlabeled samples X t = {x i t } Nt i=1 . The marginal distributions of the source and target domains are P s (X s ) and P t (X t ), respectively, where P s (X s ) = P t (X t ). This work focuses on solving the source-free domain adaptation challenge, where source data is inaccessible during the adaptation process to maintain data privacy. The aim is to learn a function f that reduces the discrepancy between the marginal distributions P s (X s ) and P t (X t ), ensuring accurate predictions for the target samples.\n\nFigure  2 : Overview of the proposed method. The stages enclosed in red-bordered boxes are adopted from our recent UDA model  [24] , while the stages enclosed in black-bordered boxes represent the original contributions of this research.\n\nThe network architecture follows the structure of our previous model  [24] , which applied cross-dataset EEG emotion recognition in a UDA setting. It consists of a feature extractor (F ) with two fully connected layers, followed by two parallel classifiers (C 1 and C 2 ). Each fully connected layer is followed by batch normalization to standardize features. Previous research  [55, 56]  has shown that simple feed-forward networks with fully connected layers perform well for EEG emotion recognition. Given that more complex networks do not improve performance over simpler ones  [24] , we choose the simpler architecture to reduce complexity and mitigate overfitting issues. As in our previous models  [24, 57] , we employ two parallel classifiers after the feature extractor. This dual-classifier setup helps handle situations where a single classifier may fail, even if the feature extractor produces distinct features. Moreover, we utilize the discrepancy between the classifiers to identify confident samples in the target domain during adaptation and assess the need for augmentation in our PC-TTA method. Each classifier has three fully connected layers, and the predicted emotion category is obtained by averaging the outputs of both classifiers.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Pre-Training",
      "text": "The pre-training stage follows the same approach as in our previous model  [24] . During this phase, the model is trained using labeled source data (X s , Y s ), to learn key features for emotion recognition. The loss function is the weighted sum of classification loss (L cls ) and classifier discrepancy loss (L dis ) (Eq. 1). To compute the classification loss, we apply group distributionally robust optimization (DRO)  [58] , which minimizes the worst-case loss across all data groups, to the weighted cross-entropy loss. The classifier discrepancy loss is calculated as the Euclidean distance between the predictions of the two classifiers.\n\nwhere α is a hyperparameter.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Computation",
      "text": "After the pre-training stage, the pre-trained network becomes available, and source domain data is no longer accessible in subsequent stages. During the computation stage, we calculate the essential properties needed for target adaptation: cluster centroids (CC t ) and mean classifier discrepancy (M dis ), by feeding the unlabeled target data into the pretrained network. Following Liang et al.'s study  [14] , we compute the cluster centroid for each emotion category (CC k t ) in the target domain:\n\nwhere pk (x t ) represents the aggregated softmax value for the sample x t from the two classifiers for the k th emotion category:\n\nwhere δ is the softmax function.\n\nThis technique calculates the centroids by integrating the classifier's predictions with the feature extractor's outputs, ensuring that samples are weighted according to the classifier's confidence. In contrast, methods relying solely on feature extractor outputs may produce less reliable centroids, as they overlook class boundaries and classifier confidence, which can lead to issues with ambiguity and domain shifts.\n\nThe mean classifier discrepancy (M dis ) is calculated as the average difference between the outputs produced by the two classifiers:\n\nwhere D represents the Euclidean distance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Target Adaptation",
      "text": "The target adaptation stage consists of two training steps, using only unlabeled target data with the pre-trained network. In the first step, we introduce the novel Dual-Loss Adaptive Regularization (DLAR) technique to improve the model's consistency and reliability on the unlabeled target data. For each training batch, we generate pseudo-labels by assigning each sample to the nearest cluster, using the cluster centroids computed in the computation stage:\n\nwhere D l represents the Euclidean distance (L2 norm).\n\nWe then calculate the pseudo-label agreement loss (L plal ) using cross-entropy between the pseudo-labels and the model's predictions:\n\nwhere X t,b represents the target samples in a batch, N b denotes the batch size, and K is the total number of emotion categories. This loss enables the model to learn in a self-supervised manner, where the pseudo-labels act as surrogates for true labels. By aligning the model's predictions with the underlying cluster distribution, this loss improves the model's ability to adapt to unseen target data.\n\nAdditionally, we calculate the confident classifier discrepancy loss (L ccdl ) (Eq. 10) by evaluating the discrepancy between classifiers, focusing on samples with confident predictions (CX t ). Confident samples are selected based on low prediction entropy and low classifier discrepancy:\n\nwhere H(x t ) represents the entropy of the predicted distribution for the sample x t , and M H is the average entropy calculated over the training batch:\n\nwhere N c denotes the number of samples in CX t .\n\nThis training provides additional supervision through classifier agreement, leading to better generalization on the target data. The final loss function (L dlar ) is the sum of the pseudo-label agreement loss and the confident classifier discrepancy loss:\n\nIn the second step, we introduce the Localized Consistency Learning (LCL) strategy to enhance local consistency by encouraging similar predictions for reliable neighbors. This strategy brings reliable neighbors closer together (Fig.  3 ). To reduce the influence of unreliable neighbors, we identify trustworthy ones by considering both the feature representations and the softmax probabilities. For each sample x i in the training batch, we first calculate the k-nearest neighbors based on the feature map values (N N f eat,i ) and the softmax outputs (N N cls,i ):\n\nwhere kN N f eat (x i ) and kN N cls (x i ) refer to the k-nearest neighbors of sample x i , based on the feature map values and softmax outputs, respectively. Next, we select the common neighbors (N N f inal,i ) that appear in both N N f eat,i and N N cls,i :\n\nThe model is then trained to minimize the prediction discrepancies among these common neighbors using the localized consistency loss (L lcl ) function:\n\nThis strategy aligns the feature and classifier spaces, ensuring consistent predictions for similar samples, and enables the model to learn robust representations for better generalization.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Inference",
      "text": "During inference, we utilize a Prediction Confidence-aware Test-Time Augmentation (PC-TTA) strategy, as introduced in our previous work  [24] , to improve model performance on target domain test data. TTA typically applies various augmentations to test samples and combines their predictions to enhance robustness. However, this process can be computationally expensive due to the multiple transformations involved. PC-TTA mitigates this issue by selectively applying TTA only to samples with high prediction uncertainty (u). Uncertainty is measured using entropy, calculated DynaSeg from the model's softmax probabilities  [24] . If the uncertainty exceeds a defined threshold (τ ) and classifier discrepancy is higher than the mean classifier discrepancy (M dis ), TTA is performed; otherwise, the model's initial prediction is accepted. For samples requiring TTA, EEG signal segments are augmented using techniques such as Gaussian noise addition and resampling. Features from these transformations are input into the model to generate predictions. A final label is determined by voting among predictions from the original and augmented samples.\n\nAlgorithm 1 summarizes the complete procedure. Accept the model's initial prediction for xt as the final prediction end end Algorithm 1: Steps of the proposed source-free unsupervised domain adaptation approach for cross-dataset EEG emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Preprocessing And Model Input Construction",
      "text": "The data preprocessing is carried out using the procedure described in our previous work  [24] . Power Spectral Density (PSD) and Differential Entropy (DE) are commonly used in EEG-based emotion recognition, with prior studies  [47, 59]  showing that they often outperform other types of EEG features. In this work, we examine both PSD and DE; however, our results demonstrate that PSD yields superior performance. Therefore, we focus on incorporating PSD into our proposed approach.\n\nWe extract EEG data from 32 common channels shared by the DEAP and SEED datasets to maintain a consistent input size for the model. Each EEG trial is segmented into 2-second windows with a 1-second overlap. We compute the PSD features for each segment across five frequency bands: delta (1-3 Hz), theta (4-7 Hz), alpha (8-13 Hz), beta (14-30 Hz), and gamma  (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) . These features are concatenated into a 1-D feature vector, X ∈ R n * i , where n = 32 denotes the number of channels, and i = 5 corresponds to the five frequency bands. Thus, the 1-D input feature vector has a size of 160 (5×32). Finally, the features are normalized to the range [-1, 1].",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dynaseg 4 Experiments",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Details",
      "text": "The proposed model is assessed through experiments conducted on the DEAP and SEED datasets, both of which are publicly available and extensively used in emotion recognition studies. To validate the model's performance, we perform cross-dataset testing by training it on one dataset and evaluating it on another.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Deap Dataset [25]",
      "text": "The DEAP dataset includes EEG and peripheral physiological signals from 32 participants who watched 40 oneminute music videos. Each participant completed 40 trials, each lasting 63 seconds, consisting of a 3-second pre-trial phase followed by 60 seconds of video viewing. After each trial, participants rated their arousal, valence, liking, and dominance on nine-point scales. EEG signals were recorded from 32 electrodes at a sampling rate of 512 Hz, then preprocessed by down-sampling to 128 Hz, removing EOG artifacts, and applying a 4-45 Hz bandpass filter.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Seed Dataset [26]",
      "text": "The SEED dataset includes EEG recordings from 15 Chinese participants across three separate sessions conducted on different days. During each session, participants viewed 15 carefully selected Chinese film clips designed to evoke positive, neutral, and negative emotions. EEG data were recorded using a 62-channel electrode system at a sampling rate of 1,000 Hz, then down-sampled to 200 Hz and filtered within the 0-75 Hz frequency range.\n\nFor our experiments, we maintain the same data preparation process as in our previous work  [24] . We use data from all subjects and trials in both the SEED and DEAP datasets. For DEAP, we exclude the first 3 seconds of each trial, which are pre-trial data. SEED signals are filtered with a bandpass filter (0.3 Hz to 50 Hz) to remove noise, as per previous studies  [13, 24, 56] , while DEAP signals, already filtered between 4 Hz and 45 Hz, undergo no additional filtering. This study focuses on two emotion categories: positive and negative. In DEAP, valence scores above 4.5 are positive, and below 4.5 are negative  [24, 47, 60] . For SEED, only positive and negative samples are kept, excluding neutral ones. After preprocessing, the DEAP dataset has 74,240 samples, and the SEED dataset has 99,630, each with a size of 160.\n\nAll experiments are performed on a Linux platform with Python (version 3.10.12) and the PyTorch library (version 2.5.1+cu121), utilizing an NVIDIA Tesla T4 GPU with 12GB of memory. The learning rate is configured at 0.0001, with a weight decay of 0.0005. We use a batch size of 64, employ the Rectified Linear Unit (ReLU) as the activation function, and optimize the model using the Adam optimizer. The hyperparameter α is set to 0.5, and the threshold τ is set to 0.9, consistent with our previous model  [24] .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Results And Discussion",
      "text": "To assess the model's performance, we conduct cross-dataset testing by training on one dataset (either DEAP or SEED) and testing on the other, considering both directions. As no SF-UDA methods currently exist for EEG emotion recognition, we evaluate our approach by comparing it with four general state-of-the-art methods  [15, 16, 21, 22] , known for their strong performance. We reimplemented these open-source methods and evaluated them on the same datasets used in our experiments to ensure a fair comparison.\n\nTable  1  presents the overall accuracy comparisons across the datasets. Our method outperforms all other approaches, achieving the highest overall accuracy of 58.99% for SEED → DEAP (trained on SEED and tested on DEAP) and 65.84% for DEAP → SEED (trained on DEAP and tested on SEED). These results are 5.44% and 3.86% higher, respectively, than the second-best approach, by Ragab et al. for SEED → DEAP and Zhao et al. for DEAP → SEED. To further validate the significance of these performance differences, we conduct a paired-sample t-test using p-values. The test confirms that the accuracy differences for DEAP → SEED are highly significant (**) when compared to all other approaches. For SEED → DEAP, the difference is significant (*) when compared to the approaches by  Ragab et al. and Zhao et al. , while it remains highly significant compared to the other methods. Fig.  4  displays boxplots illustrating the accuracy distributions across subjects on the test dataset for our method and other SF-UDA approaches. Notably, our method achieves the highest median accuracy among all approaches. In the DEAP → SEED scenario, our method demonstrates superior performance, attaining the highest median accuracy of 65.02%, with minimal variation in accuracies compared to the alternatives. In the SEED → DEAP scenario, although the methods by  Ragab et al. and Ahmed et al.  show slightly lower variability, their overall accuracy remains low. In contrast, our proposed method achieves the highest median accuracy of 57.07%.   Table  2  demonstrates that our proposed method significantly outperforms all other approaches in terms of sensitivity, positive predictive value (PPV), and F1 score. When tested on SEED, our method achieves superior performance in detecting both positive and negative emotions, with an F1 score of 70.34% for positive emotions and 59.73% for negative emotions. When tested on DEAP, all methods face difficulties in accurately identifying negative emotions. While Zhao et al.'s method achieves the highest F1 score for negative emotion recognition (0.61% higher than our method), our approach remains superior in detecting positive emotions. The lower performance in detecting negative emotions on DEAP may be due to the smaller proportion of negative emotion samples (36.87%) compared to positive emotion samples (63.13%), while the SEED dataset has a more balanced distribution (50.36% negative and 49.64% positive).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct an ablation study to evaluate the contribution of each component in our proposed method. This involves systematically removing key components, which are the primary contributions of this study, and assessing their impact  number of neighbors, denoted as k. Table  5  demonstrates that considering both feature representations and softmax probabilities consistently yields better results compared to considering either one alone. This approach helps identify more reliable neighbors while minimizing the influence of unreliable ones. Our findings reveal that the proposed method delivers robust performance across different datasets. Despite encountering substantial distributional differences between the training and testing data and the absence of source domain data, it consistently outperforms other state-of-the-art approaches.",
      "page_start": 10,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "This study introduces a novel source-free unsupervised domain adaptation (SF-UDA) method for EEG-based emotion recognition, addressing critical challenges such as domain discrepancies, limited labeled data, and the absence of source data due to privacy and computational constraints. To the best of our knowledge, this is the first application of SF-UDA in this domain. The proposed approach incorporates Dual-Loss Adaptive Regularization (DLAR) to enhance model consistency and reliability through classifier agreement and alignment with the underlying cluster distributions. Additionally, Localized Consistency Learning (LCL) promotes consistent predictions among reliable neighbors, supported by a mechanism to identify trustworthy neighbors based on feature values and softmax probabilities. Crossdataset results demonstrate that the method significantly outperforms state-of-the-art approaches. Its simplicity and adaptability make it well-suited for resource-constrained environments. This work has substantial industrial potential, enabling empathetic human-computer interactions and reliable mood disorder management in healthcare, while safeguarding data privacy and intellectual property. Future work will focus on refining the pseudo-labeling process and extending the model to support multiple target domains, incorporating domain alignment techniques to better address dataset heterogeneity and dynamic shifts.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). SF-UDA faces",
      "page": 2
    },
    {
      "caption": "Figure 1: Illustration of domain adaptation strategies: (a) Traditional unsupervised domain adaptation; (b) Source-free",
      "page": 3
    },
    {
      "caption": "Figure 2: Suppose the source domain consists of Ns labeled",
      "page": 5
    },
    {
      "caption": "Figure 2: Overview of the proposed method. The stages enclosed in red-bordered boxes are adopted from our recent",
      "page": 5
    },
    {
      "caption": "Figure 3: ). To reduce the inﬂuence of unreliable neighbors, we identify trustworthy ones by considering both the feature",
      "page": 7
    },
    {
      "caption": "Figure 3: Adaptation using Localized Consistency Learning.",
      "page": 7
    },
    {
      "caption": "Figure 4: displays boxplots illustrating the accuracy distributions across subjects on the test dataset for our method and",
      "page": 9
    },
    {
      "caption": "Figure 4: Boxplots depicting the distribution of emotion recognition accuracies on the test dataset across subjects for",
      "page": 10
    },
    {
      "caption": "Figure 5: , and Fig. 6, each component of the proposed model contributes to the overall",
      "page": 11
    },
    {
      "caption": "Figure 5: , and Fig. 6. From the confusion matrices in Fig. 5, it is evident that",
      "page": 11
    },
    {
      "caption": "Figure 6: underscores the contributions of DLAR and",
      "page": 11
    },
    {
      "caption": "Figure 5: Confusion matrices from the ablation study: (a) SEED →DEAP, (b) DEAP →SEED.",
      "page": 12
    },
    {
      "caption": "Figure 7: illustrates the model’s accuracy and",
      "page": 12
    },
    {
      "caption": "Figure 6: Class-speciﬁc accuracy from the ablation study: (a) SEED →DEAP, (b) DEAP →SEED.",
      "page": 13
    },
    {
      "caption": "Figure 7: Model performance for varying values of k: (a) SEED →DEAP, (b) DEAP →SEED.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative\n16740": "Positive\n12559",
          "32685": "36896"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative\n5965": "Positive\n16617",
          "21235": "29911"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative\n8743": "Positive\n15440",
          "18457": "31088"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative\n18007": "Positive\n5519",
          "31418": "43936"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative\n6325": "Positive\n11932",
          "20875": "34596"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Negative\n7498": "Positive\n10531",
          "19702": "35997"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Technology-assisted emotion recognition for autism spectrum disorder (ASD) children: a systematic literature review",
      "authors": [
        "M Rashidan",
        "S Sidek",
        "H Yusof",
        "M Khalid",
        "A Dzulkarnain",
        "A Ghazali",
        "S Zabidi",
        "F Sidique"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "2",
      "title": "Multimodal assistive technologies for depression diagnosis and monitoring",
      "authors": [
        "J Joshi",
        "R Goecke",
        "S Alghowinem",
        "A Dhall",
        "M Wagner",
        "J Epps",
        "G Parker",
        "M Breakspear"
      ],
      "year": "2013",
      "venue": "J. Multimodal User Interfaces"
    },
    {
      "citation_id": "3",
      "title": "Recognition of human emotions using EEG signals: A review",
      "authors": [
        "M Rahman",
        "A Sarkar",
        "M Hossain",
        "M Hossain",
        "M Islam",
        "M Hossain",
        "J Quinn",
        "M Moni"
      ],
      "year": "2021",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "4",
      "title": "A review, current challenges, and future possibilities on emotion recognition using machine learning and physiological signals",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Da",
        "Silva"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Role of machine learning and deep learning techniques in EEG-based BCI emotion recognition system: a review",
      "authors": [
        "P Samal",
        "M Hashmi"
      ],
      "year": "2024",
      "venue": "Artif. Intell. Rev"
    },
    {
      "citation_id": "6",
      "title": "Emotion detection from EEG signals using machine deep learning models",
      "authors": [
        "J Fernandes",
        "A De Alexandria",
        "J Marques",
        "D De Assis",
        "P Motta",
        "B Silva"
      ],
      "year": "2024",
      "venue": "Bioengineering"
    },
    {
      "citation_id": "7",
      "title": "EEG-based epileptic seizure detection using deep learning techniques: A survey",
      "authors": [
        "J Xu",
        "K Yan",
        "Z Deng",
        "Y Yang",
        "J.-X Liu",
        "J Wang",
        "S Yuan"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation",
      "authors": [
        "Y Luo",
        "L Zheng",
        "T Guan",
        "J Yu",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "9",
      "title": "Multisource associate domain adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Q She",
        "C Zhang",
        "F Fang",
        "Y Ma",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "10",
      "title": "CFDA-CSF: A Multi-Modal Domain Adaptation Method for Cross-Subject Emotion Recognition",
      "authors": [
        "M Jiménez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "11",
      "title": "Cross-subject EEG emotion recognition using multi-source domain manifold feature selection",
      "authors": [
        "Q She",
        "X Shi",
        "F Fang",
        "Y Ma",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "12",
      "title": "Generator-based domain adaptation method with knowledge free for crosssubject EEG emotion recognition",
      "authors": [
        "D Huang",
        "S Zhou",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Cogn. Comput"
    },
    {
      "citation_id": "13",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin",
        "C Fan",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "14",
      "title": "Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "J Liang",
        "D Hu",
        "J Feng"
      ],
      "year": "2020",
      "venue": "Proc. Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "15",
      "title": "SSDA: Secure source-free domain adaptation",
      "authors": [
        "S Ahmed",
        "A Arafat",
        "M Rizve",
        "R Hossain",
        "Z Guo",
        "A Rakin"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Int. Conf. Comput. Vis"
    },
    {
      "citation_id": "16",
      "title": "Generation, augmentation, and alignment: A pseudo-source domain-based method for source-free domain adaptation",
      "authors": [
        "Y Du",
        "H Yang",
        "M Chen",
        "H Luo",
        "J Jiang",
        "Y Xin",
        "C Wang"
      ],
      "year": "2024",
      "venue": "Mach. Learn"
    },
    {
      "citation_id": "17",
      "title": "Feature alignment by uncertainty and self-training for source-free unsupervised domain adaptation",
      "authors": [
        "J Lee",
        "G Lee"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "18",
      "title": "Divergence-Agnostic Unsupervised Domain Adaptation by Adversarial Attacks",
      "authors": [
        "J Li",
        "Z Du",
        "L Zhu",
        "Z Ding",
        "K Lu",
        "H Shen"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "19",
      "title": "VDM-DA: Virtual domain modeling for source data-free domain adaptation",
      "authors": [
        "J Tian",
        "J Zhang",
        "W Li",
        "D Xu"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "20",
      "title": "Conmix for source-free single and multi-target domain adaptation",
      "authors": [
        "V Kumar",
        "R Lal",
        "H Patil",
        "A Chakraborty"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis"
    },
    {
      "citation_id": "21",
      "title": "Source-free domain adaptation with temporal imputation for time series data",
      "authors": [
        "M Ragab",
        "E Eldele",
        "M Wu",
        "C.-S Foo",
        "X Li",
        "Z Chen"
      ],
      "year": "2023",
      "venue": "Proc. 29th ACM SIGKDD Conf. Knowl. Discov. Data Min"
    },
    {
      "citation_id": "22",
      "title": "Source-free domain adaptation (SFDA) for privacy-preserving seizure subtype classification",
      "authors": [
        "C Zhao",
        "R Peng",
        "D Wu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng"
    },
    {
      "citation_id": "23",
      "title": "Source-Free Domain Adaptation for Privacy-Preserving Seizure Prediction",
      "authors": [
        "Y Zhao",
        "S Feng",
        "C Li",
        "R Song",
        "D Liang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Ind. Informat"
    },
    {
      "citation_id": "24",
      "title": "Enhanced cross-dataset electroencephalogram-based emotion recognition using unsupervised domain adaptation",
      "authors": [
        "M Imtiaz",
        "N Khan"
      ],
      "year": "2025",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "25",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affective Comput"
    },
    {
      "citation_id": "26",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Ment. Dev"
    },
    {
      "citation_id": "27",
      "title": "EEG-Based Emotion Classification for Verifying the Korean Emotional Movie Clips with Support Vector Machine (SVM)",
      "authors": [
        "G Son",
        "Y Kim"
      ],
      "year": "2021",
      "venue": "Complexity"
    },
    {
      "citation_id": "28",
      "title": "Universum based Lagrangian twin bounded support vector machine to classify EEG signals",
      "authors": [
        "B Kumar",
        "D Gupta"
      ],
      "year": "2021",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "29",
      "title": "Hybrid approach of EEG stress level classification using K-means clustering and support vector machine",
      "authors": [
        "T Wen",
        "S Aris"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "Using simplified EEG-based brain computer interface and decision tree classifier for emotions detection",
      "authors": [
        "R Chalupnik",
        "K Bialas",
        "Z Majewska",
        "M Kedziora"
      ],
      "year": "2022",
      "venue": "Proc. Int. Conf"
    },
    {
      "citation_id": "31",
      "title": "Epilepsy seizures prediction based on nonlinear features of EEG signal and gradient boosting decision tree",
      "authors": [
        "X Xu",
        "M Lin",
        "T Xu"
      ],
      "year": "2022",
      "venue": "Int. J. Environ. Res. Public Health"
    },
    {
      "citation_id": "32",
      "title": "Emotion classification of EEG signals using Logistic Regression classification",
      "authors": [
        "A Singh",
        "G Singh",
        "N Saluja"
      ],
      "year": "2024",
      "venue": "Proc. 3rd Int. Conf. Innov. Technol. (INOCON)"
    },
    {
      "citation_id": "33",
      "title": "EEG-based emotion recognition using logistic regression with Gaussian kernel and Laplacian prior and investigation of critical frequency bands",
      "authors": [
        "C Pan",
        "C Shi",
        "H Mu",
        "J Li",
        "X Gao"
      ],
      "year": "2020",
      "venue": "Appl. Sci"
    },
    {
      "citation_id": "34",
      "title": "Automatic detection of epileptic seizures in EEG using sparse CSP and Fisher linear discrimination analysis algorithm",
      "authors": [
        "R Fu",
        "Y Tian",
        "P Shi",
        "T Bao"
      ],
      "year": "2020",
      "venue": "J. Med. Syst"
    },
    {
      "citation_id": "35",
      "title": "Automated classification of EEG into meditation and non-meditation epochs using common spatial pattern, linear discriminant analysis, and LSTM",
      "authors": [
        "J Panachakel",
        "P Kumar",
        "A Ramakrishnan",
        "K Sharma"
      ],
      "year": "2021",
      "venue": "Proc. TENCON 2021-2021 IEEE Region 10 Conf. (TENCON)"
    },
    {
      "citation_id": "36",
      "title": "Multi-modal domain adaptation variational autoencoder for EEG-based emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "D Li",
        "C Du",
        "B.-L Lu",
        "H He"
      ],
      "year": "2022",
      "venue": "IEEE/CAA J. Automatica Sinica"
    },
    {
      "citation_id": "37",
      "title": "Unsupervised feature extraction with autoencoders for EEG-based multiclass motor imagery BCI",
      "authors": [
        "S Phadikar",
        "N Sinha",
        "R Ghosh"
      ],
      "year": "2023",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "38",
      "title": "Classification of music-evoked emotions from EEG signals using self-organizing maps",
      "authors": [
        "F Beltran-Velandia",
        "J Gómez",
        "M Suarez",
        "A Ojeda",
        "E León"
      ],
      "year": "2022",
      "venue": "Proc. 2022 Int. Conf. Electr"
    },
    {
      "citation_id": "39",
      "title": "Application of self-organizing map to identify nocturnal epileptic seizures",
      "authors": [
        "B Pisano",
        "C Teixeira",
        "A Dourado",
        "A Fanni"
      ],
      "year": "2020",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "40",
      "title": "Classification of EEG signals based on sparrow search algorithm-deep belief network for brain-computer interface",
      "authors": [
        "S Wang",
        "Z Luo",
        "S Zhao",
        "Q Zhang",
        "G Liu",
        "D Wu",
        "E Yin",
        "C Chen"
      ],
      "year": "2023",
      "venue": "Bioengineering"
    },
    {
      "citation_id": "41",
      "title": "Eye state identification utilizing EEG signals: A combined method using self-organizing map and deep belief network",
      "authors": [
        "N Ahmadi",
        "M Nilashi",
        "B Minaei-Bidgoli",
        "M Farooque",
        "S Samad",
        "N Aljehane",
        "W Zogaan",
        "H Ahmadi"
      ],
      "year": "2022",
      "venue": "Scientific Programming"
    },
    {
      "citation_id": "42",
      "title": "HiRENet: Novel convolutional neural network architecture using Hilbert-transformed and raw electroencephalogram (EEG) for subject-independent emotion classification",
      "authors": [
        "M Kim",
        "C.-H Im"
      ],
      "year": "2024",
      "venue": "Comput. Biol. Med"
    },
    {
      "citation_id": "43",
      "title": "Enhancing performance of convolutional neural network-based epileptic electroencephalogram diagnosis by asymmetric stochastic resonance",
      "authors": [
        "Z Shi",
        "Z Liao",
        "H Tabata"
      ],
      "year": "2023",
      "venue": "IEEE J. Biomed. Health Informat"
    },
    {
      "citation_id": "44",
      "title": "Fused CNN-LSTM deep learning emotion recognition model using electroencephalography signals",
      "authors": [
        "M Ramzan",
        "S Dawn"
      ],
      "year": "2023",
      "venue": "Int. J. Neurosci"
    },
    {
      "citation_id": "45",
      "title": "Enhancing the accuracy of electroencephalogram-based emotion recognition through Long Short-Term Memory recurrent deep neural networks",
      "authors": [
        "M Yousefi",
        "A Dehghani",
        "H Taghaavifar"
      ],
      "year": "2023",
      "venue": "Front. Hum. Neurosci"
    },
    {
      "citation_id": "46",
      "title": "A Bi-Hemisphere Domain Adversarial Neural Network Model for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "47",
      "title": "A domain adaptation sparse representation classifier for cross-domain electroencephalogram-based emotion classification",
      "authors": [
        "T Ni",
        "Y Ni",
        "J Xue"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "48",
      "title": "A Comprehensive Survey on Source-Free Domain Adaptation",
      "authors": [
        "J Li",
        "Z Yu",
        "Z Du",
        "L Zhu",
        "H Shen"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "49",
      "title": "Model adaptation: Unsupervised domain adaptation without source data",
      "authors": [
        "R Li",
        "Q Jiao",
        "W Cao",
        "H.-S Wong",
        "S Wu"
      ],
      "year": "2020",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "50",
      "title": "Proxymix: Proxy-based mixup training with label refinery for source-free domain adaptation",
      "authors": [
        "Y Ding",
        "L Sheng",
        "J Liang",
        "A Zheng",
        "R He"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "51",
      "title": "Source data-free unsupervised domain adaptation for semantic segmentation",
      "authors": [
        "M Ye",
        "J Zhang",
        "J Ouyang",
        "D Yuan"
      ],
      "venue": "Proc. 29th ACM Int. Conf. Multimedia, 2021"
    },
    {
      "citation_id": "52",
      "title": "SALAD: Source-free active label-agnostic domain adaptation for classification, segmentation, and detection",
      "authors": [
        "D Kothandaraman",
        "S Shekhar",
        "A Sancheti",
        "M Ghuhan",
        "T Shukla",
        "D Manocha"
      ],
      "year": "2023",
      "venue": "Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis"
    },
    {
      "citation_id": "53",
      "title": "Source-free domain adaptation via distribution estimation",
      "authors": [
        "N Ding",
        "Y Xu",
        "Y Tang",
        "C Xu",
        "Y Wang",
        "D Tao"
      ],
      "year": "2022",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "54",
      "title": "Source-free domain adaptation for semantic segmentation",
      "authors": [
        "Y Liu",
        "W Zhang",
        "J Wang"
      ],
      "year": "2021",
      "venue": "Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit"
    },
    {
      "citation_id": "55",
      "title": "Learning a robust unified domain adaptation framework for cross-subject EEG-based emotion recognition",
      "authors": [
        "M Jiménez-Guarneros",
        "G Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "56",
      "title": "Domain adaptation for EEG emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "57",
      "title": "Cross-database and cross-channel electrocardiogram arrhythmia heartbeat classification based on unsupervised domain adaptation",
      "authors": [
        "M Imtiaz",
        "N Khan"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "58",
      "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "authors": [
        "S Sagawa",
        "P Koh",
        "T Hashimoto",
        "P Liang"
      ],
      "year": "2019",
      "venue": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
      "arxiv": "arXiv:1911.08731"
    },
    {
      "citation_id": "59",
      "title": "EEG-based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "60",
      "title": "Emotion recognition from EEG signals using empirical mode decomposition and second-order difference plot",
      "authors": [
        "N Salankar",
        "P Mishra",
        "L Garg"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}