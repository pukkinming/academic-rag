{
  "paper_id": "2104.06517v1",
  "title": "Comparison And Analysis Of Deep Audio Embeddings For Music Emotion Recognition",
  "published": "2021-04-13T21:09:54Z",
  "authors": [
    "Eunjeong Koh",
    "Shlomo Dubnov"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion is a complicated notion present in music that is hard to capture even with fine-tuned feature engineering. In this paper, we investigate the utility of state-of-the-art pre-trained deep audio embedding methods to be used in the Music Emotion Recognition (MER) task. Deep audio embedding methods allow us to efficiently capture the high dimensional features into a compact representation. We implement several multi-class classifiers with deep audio embeddings to predict emotion semantics in music. We investigate the effectiveness of L 3 -Net and VGGish deep audio embedding methods for music emotion inference over four music datasets. The experiments with several classifiers on the task show that the deep audio embedding solutions can improve the performances of the previous baseline MER models. We conclude that deep audio embeddings represent musical emotion semantics for the MER task without expert human engineering.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "It is an essential step for music indexing and recommendation tasks to understand emotional information in music. Previous Music Emotion Recognition (MER) studies explore sound components that can be used to analyze emotions such as duration, pitch, velocity, and melodic interval. Those representations are high-level acoustic features based on domain knowledge  (Wang, Wang, and Lanckriet 2015; Madhok, Goel, and Garg 2018; Chen et al. 2016; Lin, Chen, and Yang 2013) .\n\nRelying on human expertise to design the acoustic features for pre-processing large amounts of new data is not always feasible. Furthermore, existing emotion-related features are often fine-tuned for the target dataset based on music domain expertise and are not generalizable across different datasets  (Panda, Malheiro, and Paiva 2018b) .\n\nAdvancement in deep neural networks now allows us to learn useful domain-agnostic representations, known as deep audio embeddings, from raw audio input data with no human intervention. Furthermore, it has been reported that deep audio embeddings frequently outperform hand-crafted feature representations in other signal processing problems such as Sound Event Detection (SED) and video tagging task  (Wilkinghoff 2020; DCASE 2019) .\n\nThe power of deep audio embeddings is to automatically identify predominant aspects in the data at scale. Specifically, the Mel-based Look, Listen, and Learn network (L 3 -Net) embedding method recently matched state-of-the-art performance on the SED task  (Cramer et al. 2019) . Using a sufficient amount of training data (around 60M training samples) and carefully designed training choices, Cramer et al. were able to detect novel sound features in each audio clip using the L 3 -Net audio embeddings  (Cramer et al. 2019) .  Cramer et al.  released their optimal pre-trained L 3 -Net model which can now be extended to new tasks.\n\nIn this paper, we compare and analyze the deep audio embeddings, L 3 -Net and VGGish, for representing musical emotion semantics. VGGish is also a type of deep audio embedding method based on a VGG-like structure trained to predict video tags from the Youtube-8M dataset  (Abu-El-Haija et al. 2016; Hershey et al. 2017; Jansen et al. 2017 Jansen et al. , 2018)) . We repurpose the two deep audio embeddings, originally designed for the SED task, to the task of MER. In evaluating the performance of the embedding methods over four different music emotion datasets, we use the embeddings in several classification models and evaluate their efficacy for the MER task on each dataset.\n\nOur results show that the embedding methods provide an effective knowledge transfer mechanism between SED and MER domains without any additional training samples. More importantly, the deep audio embedding does not require expert human engineering of the sound features for the emotion prediction task. Our study reveals that audiodomain knowledge from the SED task can be extended to the MER task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "One of the goals of the MER task is to automatically recognize the emotional information conveyed in music  (Kim et al. 2010 ). Although there are many studies in the MER field  (Soleymani et al. 2013; Yang and Chen 2012; Yang, Dong, and Li 2018) , it is a complex process to compare features and performances of the studies because of the technical differences in data representation, emotion labeling, and feature selection algorithm. In addition, different studies are",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Features Classifiers Emotion",
      "text": "Step 1: select an audio embedding method, then extract a deep audio embedding feature vector\n\nStep 2: select a classification model, then predict an emotion category of each song\n\nFigure  1 : The Proposed Workflow. The figure shows the proposed approach using deep audio embeddings for the MER task.\n\ndifficult to reproduce as many of them use different public datasets or private datasets with small amounts of music clips and different levels of features.\n\nPrevious studies have utilized neural networks to efficiently extract emotional information and analyze the salient semantics of the acoustic features. Recent works explore neural networks given the significant improvements over hand-crafted feature-based methods  (Piczak 2015; Salamon and Bello 2017; Pons and Serra 2019; Simonyan and Zisserman 2014) . Specifically, using Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) based models, several studies attempt to extract necessary parameters for emotion prediction and reduce the dimensionality of the corresponding emotional features  (Cheuk et al. 2020; Thao, Herremans, and Roig 2019; Dong et al. 2019; Liu, Fang, and Huang 2019) . After careful feature engineering, these methods are suitable for a target data set for emotion prediction, however, a considerable amount of training and optimization process is still required.\n\nDeep audio embeddings are a type of audio features extracted by a neural network that take audio data as an input and compute features of the input audio. The advantages of deep audio embedding representations are that they summarize the high dimensional spectrograms into a compact representation. Using deep audio embedding representation, 1) information can be extracted without being limited to specific kinds of data, and 2) it can save time and resources.\n\nSeveral studies have used deep audio embedding methods in music classification tasks. For example, Choi et al. implemented a convnet feature-based deep audio embedding and showed how it can be used in six different music tagging tasks such as dance genre classification, genre classification, speech/music classification, emotion prediction, vocal/non-vocal classification, and audio event classification  (Choi et al. 2017) .  Kim et al.  proposed several statistical methods to understand deep audio embeddings for usage in learning tasks  (Kim et al. 2019) . However, there are currently no studies analyzing the use of deep audio embeddings in the MER task across multiple datasets.\n\nKnowledge transfer is getting increased attention in the Music Information Retrieval (MIR) research as a method to enhance sound features. Recent MIR studies report considerable performance improvements in music analysis, indexing, and classification tasks by using cross-domain knowl-edge transfer  (Hamel and Eck 2010; Van den Oord, Dieleman, and Schrauwen 2013) . For automatic emotion recognition in speech data, Feng and Chaspari used a Siamese neural network for optimizing pairwise differences between source and target data  (Feng and Chaspari 2020) . In the context of SED, where the goal is to detect different sound events in audio streams, Cramer et al.  (Cramer et al. 2019)  propose a new audio analysis method, using deep audio embeddings, based on computer vision techniques. It remains to be seen if knowledge transfer can be successfully applied on deep audio embeddings from the SED domain to the MIR domain for the task of MER.\n\nIn this study, we use deep audio embedding methods designed for the SED task and apply it over four music emotion datasets for learning emotion features in music.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods Downstream Task: Music Emotion Recognition",
      "text": "We employ a two-step experimental approach (see Figure  1 ).\n\nStep 1. Given a song as an input, a deep audio embedding model extracts the deep audio embeddings that indicate the acoustic features of the song.\n\nStep 2. After extracting deep audio embeddings, the selected classification model predicts the corresponding emotion category that indicates the emotion label of the song.  Look, Listen, and Learn network (L 3 -Net) L 3 -Net is an audio embedding method  (Cramer et al. 2019 ) motivated by the original work of Look, Listen, and Learn (L 3 ) (Arandjelovic and Zisserman 2017) that processes Audio-Visual Correspondence learning task in computer vision research. The key differences between the original L 3 (by Arandjelović and Zisserman) and L 3 -Net (by  Cramer et al.)  are (1) input data format (video vs. audio), (2) final embedding dimensionality, and (  3 ) training sample size.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep Audio Embeddings",
      "text": "The L 3 -Net audio embedding method consists of 2D convolutional layers and 2D max-pooling layers, and each convolution layer is followed by batch normalization and a ReLU nonlinearity (see Figure  2 ). For the last layer, a maxpooling layer is performed to produce a single 512 dimension feature vector (L 3 -Net serves as an option for output embedding size such as 6144 or 512, and we choose 512 as our embedding size). The L 3 -Net method is pre-trained on Google AudioSet 60M training samples containing mostly musical performances  (Gemmeke et al. 2017) .\n\nWe follow the design choices of the L 3 -Net study which result in the best performance in their SED task. We use Mel spectrograms with 256 Mel bins spanning the entire audible frequency range, resulting in a 512 dimension feature vector. We revise OpenL3 open-source implementation 1  for our experiments.\n\nVGGish We also verify another deep audio embedding method, VGGish (Simonyan and Zisserman 2014), VGGstructure (VGGNet) based deep audio embedding model. VGGish is a 128-dimensional audio embedding method, motivated by VGGNet  (Simonyan and Zisserman 2014) , and pre-trained on a large YouTube-8M dataset  (Abu-El-Haija et al. 2016 ). Original VGGNet is targeting large scale image classification tasks, and VGGish is targeting extracting acoustic features from audio waveforms. The VGGish audio embedding method consists of 2D convolutional layers and 2D max-pooling layers to produce a single 128 dimension feature vector (see Figure  2 ). We modify a VGGish opensource implementation 2  for our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Music Emotion Classifiers",
      "text": "From the computed deep audio embeddings, we predict an emotion category corresponding to each audio vector as a multi-class classification problem. We employ six different classification models, Support Vector Machine (SVM), Naive Bayes (NB), Random Forest (RF), Multilayer Perceptron (MLP), Convolution Neural Network (CNN), and Recurrent Neural Network (RNN).\n\nFor each classification task, we use 80% of the data for training, 10% for testing, and 10% for validation. All six classification models are implemented in Scikit-learn (Pedregosa et al. 2011), Keras  (Chollet et al. 2015) , and Tensorflow  (Abadi et al. 2016 ). In the case of MLP, CNN, and RNN classification models, we share some implementation details below.\n\n• MLP: We implement the MLP model with two of a single hidden layer with 512 nodes, a ReLU activation function, an output layer with a number of emotion categories, and a softmax activation function. The model is processed using the categorical cross-entropy loss function and we use Adam stochastic gradient descent  (Kingma and Ba 2014) . We fit the model for 1000 training epochs with the default batch size of 32 samples and evaluate the performance at the end of each training epoch on the test dataset.\n\n• CNN: For CNN classification model, we revise the convolutional filter design proposed by Abdoli et al.  (Abdoli, Cardinal, and Koerich 2019) , which includes four 1D convolution layers and a 1D max-pooling operation layer. Each layer processes 64 convolutional filters. The input to the network is a Mel spectrogram, size of 512 feature vector extracted from a deep audio embedding method. This input size is varied depending on the type of embedding methods. For example, in the case of L 3 -Net, the embedding size is 512, VGGish embedding size is 128. ReLU activation functions are applied to the convolutional layers to reduce the backpropagation errors and accelerate the learning process  (Goodfellow, Bengio, and Courville 2016) . The softmax function is used as the output activation function with a number of emotion categories. Adam optimizer, categorical cross-entropy loss function, and the batch size of 32 samples are used. The stopping criterion is set as 1000 epochs with an early-stopping rule if there is no improvement to the score during the last 100 learning epochs.\n\n• RNN: Weninger et al.  (Weninger, Eyben, and Schuller 2014)  propose LSTM-RNN design as an automaton-like structure mapping from an observation sequence to an output feature sequence. We use LSTM networks with a pointwise softmax function based on a number of emotion categories. Adam optimizer, the categorical cross-entropy loss function, and the batch size of 32 samples are used. The same stopping criterion is set as CNNs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Dataset",
      "text": "Four different datasets are selected for computing the emotional features in music data. In Table  1 , we show the number of music files of each dataset by emotion category.\n\n• 4Q Audio Emotion Dataset: This dataset is introduced by Panda et al.  (Panda, Malheiro, and Paiva 2018a) , annotated each music clip into four Arousal-Valence (A-V) quadrants based on Rusell's model  (Russell 2003 ): Q1 (A+V+), Q2 (A+V-), Q3 (A-V-), Q4 (A-V+). Each emotion category has 225 music clips, and each music clip is 30 seconds long. The total music clips for the dataset are 900 files.\n\n• Bi-modal Emotion Dataset: This dataset is introduced by Malheiro et al.  (Malheiro et al. 2016 ) in a context of bimodal analysis in the emotion recognition with audio and lyric information. The emotion category is also annotated into four A-V quadrants by Russell's model. In this dataset, each emotion category has a different number of music clips, Q1: 52 clips; Q2: 45 clips; Q3: 31 clips, and Q4: 34 clips, and each music clip is 30 seconds long. The total music clips for the dataset are 162 files. The size of this dataset is the",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Audio Features",
      "text": "As a baseline feature, we use Mel-Frequency Cepstral Coefficients (MFCCs), which are known to be efficient low-level descriptors for timbre analysis, used as features of music tagging tasks  (Choi et al. 2017; Kim, Lee, and Nam 2018) . MFCCs describe the overall shape of a spectral envelope. We first calculate the time derivatives of the given MFCCs and then take the mean and standard deviation over the time axis. Finally, we concatenate all statistics into one vector. We generate the MFCC features of each music clip into a matrix of 20 x 1500. Librosa is used for MFCCs extraction and audio processing  (McFee et al. 2015) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Performance Measures",
      "text": "For classification problems, classifier performance is typically defined according to the confusion matrix associated with the classifier. We use accuracy measure as a primary evaluation criterion. We also calculate F1-score and r 2 score for comparison with other baseline models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Of Music Emotion Recognition",
      "text": "In Figure  3 , we show the performance of deep audio embeddings over four music emotion datasets. We empirically analyze deep audio embeddings in several settings against baseline MFCC features. The experiments are validated with 20 repetitions of cross-validation where we report the average results. We share key observations in the next sections.\n\nPerformance Analyzed by Features The L 3 -Net embedding has the best performance in all considered cases except for two, CNN classifier accuracy both in Bi-modal Emotion and Emotion in Music dataset (see Figure  3 ). Even though the L 3 -Net embedding is generally not a descriptor for any music-related tasks before, the performance convinces us to use a pre-trained L 3 -Net audio embedding model for the MER task.\n\nSince the direct use of the L 3 -Net embedding shows the better performance, we also investigate more about the different embedding dimension of the L 3 -Net and compare the performance between 512 and 6144. Interestingly, we observe decreasing results with the dimension of 6144 L 3 embeddings. This indicates that those extra features might not be relevant but introducing noise. While the 512 L 3 embeddings show consistent higher performance in many cases, based on our observations, even we increase the depth and number of parameters, 6144 L 3 -Net embeddings perform slightly lower on this MER task. Thus, we have not included the performance in the figure. Note that reported results in Figure  3  are only considered the performance of 512.\n\nComparing between L 3 -Net and VGGish, L 3 -Net outperforms VGGish across the dataset. This could be because L 3 -Net was pre-trained on both visual and audio onto the same embedded space which can include more features. The performance of VGGish is better than MFCC baseline features with the rest of the classification models, even though it has fewer parameters, 128. This justifies our use of L 3 -Net as a main deep audio embedding choice for MER task and VG-Gish is for some cases.\n\nIt is generally known that decision trees and in our case, RF is better than other neural network based classifiers where the data comprises a large set of categories  (Pons and Serra 2019) . It also deals better with dependence between variables, which might increase the error and cause some significant features to become insignificant during training. SVM uses kernel trick to solve non-linear problems whereas decision trees derive hyper-rectangles in input space to solve the problem. This is why decision trees are better for categorical data and it deals with co-linearity better than SVM. We still find that SVM outperforms RF in some cases. The reason can be that SVM deals better with margins and thus better handles outliers. Although these are tangential considerations, it seems to support the overall notion that MER is a higher level recognition problem that first needs to address the division of the data into multiple acoustic categories, also requiring the learning of a rather non-trivial partition structure within these sub-categories.\n\nPerformance Analyzed by Datasets For comparison with prior works studying emotions in audio signals, we ana-lyze the performance of previous studies on each dataset we used. We choose four baseline MER models for our experiments: 1) Panda et al.  (Panda, Malheiro, and Paiva 2018a)  release the 4Q Music Emotion dataset and present the study of musical texture and expressivity features, 2) Malheiro et al.  (Malheiro et al. 2016 ) present novel lyrical features for MER task and release the Bi-modal Emotion dataset, 3) Choi et al.  (Choi et al. 2017 ) present a pre-trained convnet feature for music classification and regression tasks and evaluate the model using Emotion in Music dataset, 4) Arora and Chaspari  (Arora and Chaspari 2018)  present the method of a siamese network for speech emotion classification and evaluate the method using RAVDESS dataset. We compare those baseline models to the performance of our proposed method (see Table  2 ). In the case of the 4Q Audio Emotion dataset, the previous study by Panda et al. obtained its best result of 73.5% F1-score with a high number of 800 features. In Table  3 , Domain Knowledge means a feature set defined by domain knowledge in the study. For achieving the performance of the previous study, the following steps are needed. First, we need to pre-process standard or baseline audio features of each audio clip. The study used Marsyas, MIR Toolbox, and PsySound3 audio frameworks to extract a total of 1702 features. Second, we need to calculate the correlation between the pair of features for normalization. After the preprocessing, the number of features can be decreased to 898 features. Third, after computing these baseline audio features, we also need to compute novel features of each audio clip proposed by the study. Those features were carefully designed based on domain expertise, such as glissando features, vibrato, and tremolo features. Finally, baseline features and extracted novel features are combined for the MER task. For the evaluation, the study conducted post-processing of the features with the ReliefF feature selection algorithm  (Robnik-Šikonja and Kononenko 2003) , ranked the features and evaluated its best-suited features. Since the performance has been evaluated by hyperparameter tuning and feature selection algorithms, these factors may influence the performance of the MER task significantly. Note that in our proposed approach, we show the performance without any postprocessing.\n\nIn the case of the Bi-modal Emotion dataset, the previous study by  Malheiro et al. (Malheiro et al. 2016 ) presented its best classification result of 72.6% F1-score on the dataset which is lower than the performance we have, 88% F1-score from the result of L 3 -Net embedding with SVM classifier.\n\nIn the case of the Emotion in Music dataset, previous studies predicted the time-varying arousal and valence annotation and calculated r 2 score as a performance measure  (Weninger, Eyben, and Schuller 2014; Lee et al. 2019; Kim, Lee, and Nam 2018; Choi et al. 2017 ). We previously map these time-varying annotations into four A-V quadrants based on Rusell's model and show our prediction performance with four emotion categories (see Figure  3-(c )). For a fair comparison, we also verify the original time-varying dynamic annotations from the dataset  (Soleymani et al. 2013)  and compare the result with the baseline model. Using the Emotion in Music dataset, Choi et al. reported its r 2 scores of arousal annotation, 0.656 and valence annotation, 0.462  (Choi et al. 2017) . The best performance of L 3 -Net embeddings achieves 0.671 r 2 score on arousal and 0.556 r 2 score on valence annotation. The result shows that we have a considerable and higher performance on arousal and valence annotation. The result confirms that L 3 -Net embedding method shows favorable performance than the previous embedding features over Emotion in Music data.\n\nIn the case of RAVDESS data, the study by Arora and Chaspari (Arora and Chaspari 2018) reported its best classification accuracy of 63.8% over the dataset which is lower than our accuracy, 71.0%, from the result of L 3 -Net embedding with CNN classifier (see Figure  3-(d) ). Performance Analyzed by A-V Quadrants In Table  3 , we show the results analyzed by each quadrant. This classification report gives us a further understanding of the characteristic of each emotion category in music. The meaning of each quadrant (Q1, Q2, Q3, Q4) information is described in Table  1 .\n\nIn the case of the 4Q Audio Emotion dataset, Q2 and Q3 categories obtain a higher score compared to the Q1 and Q4. This indicates that emotional features in music clips with lower valence components are easier to recognize. Specifically, the Q2 category shows higher performance which is distinctive than others. Based on the dataset  (Panda, Malheiro, and Paiva 2018b) , the study describes music clips of the Q2 category belong to specific genres, such as heavy metal, which have recognizable acoustic features than others. Lower results in Q1 and Q4 categories may also reflect the characteristics of music clips. For instance, the Q1 category indicates happy emotions, which are typically energetic based on positive arousal and positive valence components. Since Q1 and Q4 categories share the same valence axis based on Rusell's model, if the intensity of the song is not intense, the difference between the two quadrants (Q1&Q4 or Q2&Q3) may not be apparent. This aspect results in similar behaviors on the Q2 and Q3 categories' performances as well.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion And Conclusion",
      "text": "In this paper, we evaluate L 3 -Net and VGGish pre-trained deep audio embedding methods for MER task over 4Q Audio Emotion, Bi-modal Emotion, Emotion in Music, and RAVDESS datasets. Even though L 3 -Net has not been intended for emotion recognition, we find that L 3 is the best representation for the MER task. Note that we achieve this performance without any additional domain knowledge feature selection method, feature training process, and finetuning process. Comparing to MFCC baseline features, the empirical analysis shows that L 3 -Net is robust across multiple datasets with favorable performance. Overall, the result using L 3 -Net shows improvement compared to baseline models for Bi-modal Emotion, Emotion in Music, and RAVDESS dataset. In the case of the 4Q Audio Emotion dataset, complex hand-crafted features (over 100 features) still seem to perform better. Specifically, our work does not consider rhythm or specific musical parameters over the time axis that 4Q Audio Emotion had, looking into time-based aspects could be the next step for future research.\n\nIn order to gain deeper insight into the meaning of acoustic features for emotional recognition, we use T-SNE visualization (see Figure  4 ). In both cases of L 3 -Net and VGGish, two main clusters on the left and right side of the figure mean male/female singer groups. We can also see a relatively smooth grouping of samples by emotions with different colors. In the case of L 3 -Net embeddings (top figure of Figure  4 ), multiple small groups in each cluster indicate individual singer which has audio recordings in different emotions. L 3 -Net data seems to cluster into multiple smaller groups according to gender and individual categories, and this shows L 3 -Net outperforms for detecting different timbre information than VGGish. This pattern seems to be consistent in the wild range of T-SNE perplexity parameters. This also shows that our study provides an empirical justification that L 3 -Net outperforms VGGish, with the intuition discussed in the paper based on the clustering shown in Fig-  ure 4 .\n\nAccordingly, for the next step, a possible direction to validate different classifiers is to explore a combination of discrete neural learning methods, such as VQ-VAE, to first solve the categorical problem, and only later learn a more smooth decision surface. VQ-VAE has been recently explored for spectrogram-based music inpainting  (Bazin et al. 2020) . It would be interesting to explore similar high-level parameterization using L 3 -Net embeddings.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Proposed Workﬂow. The ﬁgure shows the",
      "page": 2
    },
    {
      "caption": "Figure 2: Network Architecture of L3-Net and VGGish.",
      "page": 2
    },
    {
      "caption": "Figure 2: ). For the last layer, a max-",
      "page": 3
    },
    {
      "caption": "Figure 2: ). We modify a VGGish open-",
      "page": 3
    },
    {
      "caption": "Figure 3: Performance of Emotion Recognition on the",
      "page": 4
    },
    {
      "caption": "Figure 3: , we show the performance of deep audio embed-",
      "page": 5
    },
    {
      "caption": "Figure 3: ). Even though",
      "page": 5
    },
    {
      "caption": "Figure 3: are only considered the performance of 512.",
      "page": 5
    },
    {
      "caption": "Figure 3: -(c)). For a",
      "page": 6
    },
    {
      "caption": "Figure 4: T-SNE Visualization on RAVDESS dataset. Dif-",
      "page": 6
    },
    {
      "caption": "Figure 4: ). In both cases of L3-Net and VGGish,",
      "page": 7
    },
    {
      "caption": "Figure 4: ), multiple small groups in each cluster indicate in-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , we show the number",
      "page": 3
    },
    {
      "caption": "Table 1: Dataset Details. The number of emotion categories",
      "page": 4
    },
    {
      "caption": "Table 2: Performance Comparison with Baseline MER",
      "page": 5
    },
    {
      "caption": "Table 3: Classiﬁcation Results of Each Quadrant. The",
      "page": 6
    },
    {
      "caption": "Table 1: In the case of the 4Q Audio Emotion dataset, Q2 and Q3",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Tensorflow: A system for large-scale machine learning",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard"
      ],
      "year": "2016",
      "venue": "12th {USENIX} Symposium on Operating Systems Design and Implementation"
    },
    {
      "citation_id": "2",
      "title": "End-to-end environmental sound classification using a 1d convolutional neural network",
      "authors": [
        "S Abdoli",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Youtube-8m: A large-scale video classification benchmark",
      "authors": [
        "S Abu-El-Haija",
        "N Kothari",
        "J Lee",
        "P Natsev",
        "G Toderici",
        "B Varadarajan",
        "S Vijayanarasimhan"
      ],
      "year": "2016",
      "venue": "Youtube-8m: A large-scale video classification benchmark",
      "arxiv": "arXiv:1609.08675"
    },
    {
      "citation_id": "4",
      "title": "Look, listen and learn",
      "authors": [
        "R Arandjelovic",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Exploring siamese neural network architectures for preserving speaker identity in speech emotion classification",
      "authors": [
        "P Arora",
        "T Chaspari"
      ],
      "year": "2018",
      "venue": "Proceedings of the 4th International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction"
    },
    {
      "citation_id": "6",
      "title": "Spectrogram Inpainting for Interactive Generation of Instrument Sounds",
      "authors": [
        "T Bazin",
        "G Hadjeres",
        "P Esling",
        "M Malt"
      ],
      "year": "2020",
      "venue": "Spectrogram Inpainting for Interactive Generation of Instrument Sounds"
    },
    {
      "citation_id": "7",
      "title": "A scheme of MIDI music emotion classification based on fuzzy theme extraction and neural network",
      "authors": [
        "P Chen",
        "L Zhao",
        "Z Xin",
        "Y Qiang",
        "M Zhang",
        "T Li"
      ],
      "year": "2016",
      "venue": "2016 12th International Conference on Computational Intelligence and Security (CIS)"
    },
    {
      "citation_id": "8",
      "title": "Regression-based music emotion prediction using triplet neural networks",
      "authors": [
        "K Cheuk",
        "Y.-J Luo",
        "B Balamurali",
        "G Roig",
        "D Herremans"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "9",
      "title": "Transfer learning for music classification and regression tasks",
      "authors": [
        "K Choi",
        "G Fazekas",
        "M Sandler",
        "K Cho"
      ],
      "year": "2017",
      "venue": "Transfer learning for music classification and regression tasks",
      "arxiv": "arXiv:1703.09179"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "F Chollet"
      ],
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings",
      "authors": [
        "J Cramer",
        "H.-H Wu",
        "J Salamon",
        "J Bello"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Detection and Classification of Acoustic Scenes and Events. Task 4: Sound Event Detection in Domestic Environments",
      "authors": [
        "Dcase"
      ],
      "year": "2019",
      "venue": "Detection and Classification of Acoustic Scenes and Events. Task 4: Sound Event Detection in Domestic Environments"
    },
    {
      "citation_id": "13",
      "title": "Bidirectional Convolutional Recurrent Sparse Network (BCRSN): An Efficient Model for Music Emotion Recognition",
      "authors": [
        "Y Dong",
        "X Yang",
        "X Zhao",
        "J Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "A Siamese Neural Network with Modified Distance Loss For Transfer Learning in Speech Emotion Recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "A Siamese Neural Network with Modified Distance Loss For Transfer Learning in Speech Emotion Recognition",
      "arxiv": "arXiv:2006.03001"
    },
    {
      "citation_id": "15",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Deep learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2016",
      "venue": "Deep learning"
    },
    {
      "citation_id": "17",
      "title": "Learning features from music audio with deep belief networks",
      "authors": [
        "P Hamel",
        "D Eck"
      ],
      "year": "2010",
      "venue": "ISMIR"
    },
    {
      "citation_id": "18",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "19",
      "title": "Large-scale audio event discovery in one million youtube videos",
      "authors": [
        "A Jansen",
        "J Gemmeke",
        "D Ellis",
        "X Liu",
        "W Lawrence",
        "D Freedman"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised learning of semantic audio representations",
      "authors": [
        "A Jansen",
        "M Plakal",
        "R Pandya",
        "D Ellis",
        "S Hershey",
        "J Liu",
        "R Moore",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Are Nearby Neighbors Relatives?: Are Nearby Neighbors Relatives?: Testing Deep Music Embeddings",
      "authors": [
        "J Kim",
        "J Urbano",
        "C Liem",
        "A Hanjalic"
      ],
      "year": "2019",
      "venue": "Frontiers in Applied Mathematics and Statistics"
    },
    {
      "citation_id": "22",
      "title": "Sample-level CNN architectures for music auto-tagging using raw waveforms",
      "authors": [
        "T Kim",
        "J Lee",
        "J Nam"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "23",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Y Kim",
        "E Schmidt",
        "R Migneco",
        "B Morton",
        "P Richardson",
        "J Scott",
        "J Speck",
        "D Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. ISMIR"
    },
    {
      "citation_id": "24",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "25",
      "title": "Enhancing music features by knowledge transfer from user-item log data",
      "authors": [
        "D Lee",
        "J Lee",
        "J Park",
        "K Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Exploration of Music Emotion Recognition Based on MIDI",
      "authors": [
        "Y Lin",
        "X Chen",
        "D Yang"
      ],
      "year": "2013",
      "venue": "ISMIR"
    },
    {
      "citation_id": "27",
      "title": "Music emotion recognition using a variant of recurrent neural network",
      "authors": [
        "H Liu",
        "Y Fang",
        "Q Huang"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Mathematics, Modeling, Simulation and Statistics Application"
    },
    {
      "citation_id": "28",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "SentiMozart: Music Generation based on Emotions",
      "authors": [
        "R Madhok",
        "S Goel",
        "S Garg"
      ],
      "year": "2018",
      "venue": "SentiMozart: Music Generation based on Emotions"
    },
    {
      "citation_id": "30",
      "title": "Bi-modal music emotion recognition: Novel lyrical features and dataset. 9th International Workshop on Music and Machine Learning-MML'2016-in",
      "authors": [
        "R Malheiro",
        "R Panda",
        "P Gomes",
        "R Paiva"
      ],
      "year": "2016",
      "venue": "Bi-modal music emotion recognition: Novel lyrical features and dataset. 9th International Workshop on Music and Machine Learning-MML'2016-in"
    },
    {
      "citation_id": "31",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "32",
      "title": "Musical Texture and Expressivity Features for Music Emotion Recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "IS-MIR"
    },
    {
      "citation_id": "33",
      "title": "Novel audio features for music emotion recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Scikit-learn: Machine learning in Python. the Journal of machine Learning research",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg"
      ],
      "year": "2011",
      "venue": "Scikit-learn: Machine learning in Python. the Journal of machine Learning research"
    },
    {
      "citation_id": "35",
      "title": "Environmental sound classification with convolutional neural networks",
      "authors": [
        "K Piczak"
      ],
      "year": "2015",
      "venue": "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Randomly weighted CNNs for (music) audio classification",
      "authors": [
        "J Pons",
        "X Serra"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Theoretical and empirical analysis of ReliefF and RReliefF",
      "authors": [
        "M Robnik-Šikonja",
        "I Kononenko"
      ],
      "year": "2003",
      "venue": "Machine learning"
    },
    {
      "citation_id": "38",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "J Russell"
      ],
      "year": "2003",
      "venue": "Psychological review"
    },
    {
      "citation_id": "39",
      "title": "Deep convolutional neural networks and data augmentation for environmental sound classification",
      "authors": [
        "J Salamon",
        "J Bello"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "40",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "41",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "M Soleymani",
        "M Caro",
        "E Schmidt",
        "C.-Y Sha",
        "Y.-H Yang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia"
    },
    {
      "citation_id": "42",
      "title": "Multimodal Deep Models for Predicting Affective Responses Evoked by Movies",
      "authors": [
        "H Thao",
        "D Herremans",
        "G Roig"
      ],
      "year": "2013",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)"
    },
    {
      "citation_id": "43",
      "title": "A histogram density modeling approach to music emotion recognition",
      "authors": [
        "J.-C Wang",
        "H.-M Wang",
        "G Lanckriet"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "On-line continuoustime music mood regression with deep recurrent neural networks",
      "authors": [
        "F Weninger",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "On open-set classification with L3-Net embeddings for machine listening applications",
      "authors": [
        "K Wilkinghoff"
      ],
      "year": "2020",
      "venue": "28th European Signal Processing Conference"
    },
    {
      "citation_id": "46",
      "title": "Review of data features-based music emotion recognition methods",
      "authors": [
        "X Yang",
        "Y Dong",
        "J Li"
      ],
      "year": "2018",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "47",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    }
  ]
}