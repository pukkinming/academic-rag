{
  "paper_id": "2306.13592v2",
  "title": "Tacoformer: Token-Channel Compounded Cross Attention For Multimodal Emotion Recognition",
  "published": "2023-06-23T16:28:12Z",
  "authors": [
    "Xinda Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, emotion recognition based on physiological signals has emerged as a field with intensive research. The utilization of multi-modal, multichannel physiological signals has significantly improved the performance of emotion recognition systems, due to their complementarity. However, effectively integrating emotion-related semantic information from different modalities and capturing inter-modal dependencies remains a challenging issue. Many existing multimodal fusion methods ignore either token-to-token or channel-to-channel correlations of multichannel signals from different modalities, which limits the classification capability of the models to some extent. In this paper, we propose a comprehensive perspective of multimodal fusion that integrates channel-level and token-level cross-modal interactions. Specifically, we introduce a unified cross attention module called Token-chAnnel COmpound (TACO) Cross Attention to perform multimodal fusion, which simultaneously models channel-level and token-level dependencies between modalities. Additionally, we propose a 2D position encoding method to preserve information about the spatial distribution of EEG signal channels, then we use two transformer encoders ahead of the fusion module to capture long-term temporal dependencies from the EEG signal and the peripheral physiological signal, respectively. Subject-independent experiments on emotional dataset DEAP and Dreamer demonstrate that the proposed model achieves state-of-the-art performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is critical in human's daily activities and interactions with the real world, affecting our cognition and behavior. The valence-arousal model characterizes emotion as a twodimensional continuous space in which pleasantness or unpleasantness is described as valence, and excitement or calmness is described as arousal. Accurate emotion recognition can contribute to diverse fields like mental health monitoring, Many deep learning models have been built for processing multi-channel time series. But fusing these signals from different modalities remains a critical issue for constructing an emotion recognition model.\n\nThere mainly exist three strategies, which are on feature level, decision level, and model level, as outlined in the literature  [Atrey et al., 2010] . The feature level fusion approach involves concatenation of features from various modalities to obtain a combined representation. Nonetheless, when dealing with signals comprising of numerous channels which corre-sponds to location of signal acquisition, spatial information in a single modality could be disregarded. Furthermore, temporal synchronization information from different modalities may also be overlooked.\n\nThe decision-level fusion methods involve feature extraction from each modality by passing input signal through specific network, embeddings from different networks are concatenated to get a joint representation. then a decision function outputs final results. This approach provides the flexibility to select the most suitable method for each modality. In  [Dar et al., 2020] , Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks were separately employed to process the Electroencephalography (EEG), Galvanic Skin Response (GSR), and Electrocardiogram (ECG) signals. The Majority Vote was subsequently utilized as the decision function to produce predictions from different networks. Nevertheless, this approach has limited capabilities at capturing feature correlations among modalities. Furthermore, designing a heterogeneous network architecture could be time and resourceintensive.\n\nFor model-level fusion, a large amount of studies concentrate on implement inter-modal interactions to model correlations between modalities. For example,  [Liu et al., 2021]  employed a Weighted Sum Fusion method, which employed Deep Canonical Correlation Analysis (DCCA) to obtain coordinated representations of multimodal embeddings.  [Ma et al., 2019]  employed a multimodal residual LSTM network to recognize emotion statements. This network is capable of learning the correlation between the Electroencephalography (EEG) and other physiological signals by sharing weights across modalities in each LSTM layer.  [Hu et al., 2021]  used Graph Convolution Networks (GCNs) to model intramodal and inter-modal dependencies among the audio, video, and text modalities. Cross attention  [Sun et al., 2021; Lu et al., 2022] is a widely adopted multimodal fusion technique that provides an effective approach for learning correlations of different tokens between modalities, which can be defined as token-wise cross attention (TCA). Furthermore, as different channels correspond to different locations for signal acquisition, adaptively fusing sufficient channel-wise features across modalities is advantageous for capturing spatial information. As such, channel-wise cross attention (CCA)  [Wang et al., 2022]  has been utilized to learn the correlation of different channels between two modalities. However, the aforementioned cross attention mechanisms are based on a single perspective at implementing multimodal fusion, either token-wise or channel-wise. Therefore, it is essential to find an effective way to conduct interaction between modalities based on a compounded perspective of channel-wise and token-wise.\n\nIn this paper, we propose a network for emotion recognition based on multimodal psychological data (EEG signals and peripheral psychological signals like  EOG, EMG, and ECG signals) . The proposed multimodal fusion module, namely Token-chAnnel COmpounded(TACO) cross attention, enables us to simultaneously capture long-term tokenwise and channel-wise dependence between two modalities. Figure  1  illustrates TACO cross attention and other two atten-tion mechanism-based fusion methods. With TACO cross attention as the fusion method, we build TACOformer network, which utilizes two separate transformer encoders  [Vaswani et al., 2017]  as temporal extraction module. The main contributions of this paper are summarized as follows:\n\n• We suggest a new perspective of multimodal fusion, i.e. the compound of token-wise and channel-wise cross attention, which can simultaneously capture long-term compounded dependency between two multi-channel modalities on temporal level and channel level.\n\n• We propose a 2D position encoding method to preserve spatial information in the input sequence of twodimensional images, which outperforms 1d position encoding methods.\n\n• Extensive subject-independent experiments are conducted on two benchmark datasets and the experimental results show that our network consistently outperforms other models at valence and arousal classification.\n\nThe rest of paper is organized as below: section 2 introduces preliminary. Section 3 presents the proposed methodology. Section 4 introduces datasets and data preprocess. Section 5 presents experiments and analysis. Section 6 concludes this paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Preliminary",
      "text": "In this paper, we define\n\nthe EEG signal of N electrode channels collected at the time stamp t. E t is then transformed into a 2D matrix E2D ∈ H×W (see Section 4.2 Data preprocess), where H and W denote the height and width of the matrix. Further, we combine 2D matrices at T timestamps then get a 3D spatial-temporal representation\n\nThe EEG emotion recognition problem is defined as: learning a mapping function which maps the input data that includes EEG and Peripheral physiological signals to the corresponding emotion:\n\nwhere F denotes the mapping function, Y class denotes the predicted emotion class.  the temporal extraction module. TACO cross attention models the compounded correlations between channel-to-channel and token-to-token of two modalities to obtain fused embedding. The classification layer utilizes aggregated information represented as classtoken and connects it with a linear layer to get classification output.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Overview",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2D Spatial Position Encoding",
      "text": "Transformer is permutation-invariant when processing sequence, so it is necessary to implement position encoding to retain relative or absolute position information. However, 1D position encoding in  [Vaswani et al., 2017]  ignores spatial distribution of EEG channels. To tackle this problem, we generalize 1D sinusoidal position encoding to 2D position encoding. Made by product of sines in vertical position and cosines in horizontal position, we get 2D position encoding P OS ∈ R T ×H×W , which is depicted as follows: (2) where x and y are the horizontal and vertical position, t is the timestamp of token in sequence, W and H denote width and height of the 2D EEG matrix.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Extraction Module",
      "text": "Due to the capability at capturing long-range dependencies from sequential data of Transformer, we adopt the encoder part of Vanilla Transformer  [Vaswani et al., 2017]  to extract features from EEG and PPS. The key component of Vanilla Transformer is Multi-head attention mechanism. It is depicted as follow: Given the input EEG vector X E ∈ R H×W ×T , where T denotes the length of input sequence, H and W denote the height and width of EEG matrix. it is firstly transposed into shape as T × H × W and 2D position encoding P OS is point-wise added on it. Spatial flattening and Linear projection transforms it into Z E ∈ R T ×d . Following process in  [Devlin et al., 2018] , we concatenate an learnable token CLS ∈ R 1×d as the first token in sequence and use LayerN orm function to normalize embedding, which is depicted as follows:\n\nNext,three embeddings Q(query),K(key),V(value) are generated from linear projections:\n\nThe dot product of Q and K is used to derive a weight matrix, which is rescaled by multiplication with 1/ √ d. The Sof tmax function is applied along rows to generate the attention matrix:\n\n)\n\nwhere n denotes the sequence length, M i,j are normalized alignment scores measuring the similarity between tokens z i and z j .After residual connection and feed-forward network, the final output is defined as:\n\n) where F F N is a token-wise feed forward network, which is implemented by a linear layer with output's dimension as Res.\n\nFor multi-head attention, input embedding Z is splitted into h smaller embeddings, then these embeddings are fed through separate linear projections to generate Query Q i , Key K i and Value V i matrices for each attention head. Then h head attention outputs are computed, concatenated and through a linear projection:\n\n9) where W O ∈ R d×d is the output projection matrix. As illustrated in Figure .2, embeddings of EEG and PPS are obtained from two independent transformer encoders.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Token-Channel Compound Cross Attention",
      "text": "Token-wise cross attention (TCA) and channel-wise cross attention (CCA) can capture long-term dependencies between different tokens and between different channels of two modalities, respectively. However, on one hand, implementation of token-wise cross attention to conduct multimodal fusion could ignore channel-level dependencies, vice versa. On the other hand, simple combination like concatenation of the two cross attention matrix could cause data sparseness, which leads to a larger latent space and requires much more computing resource. Therefore, we propose a novel cross attention mechanism to compound channel-wise and token-wise cross attention, it is depicted as follow:  In T ACO cross attention, token-wise and channel-wise correlations between EEG and PPS are modeled by the left and right part, which can correspond to T CA and CCA depicted as follows:\n\nwhere CCA P -→E ∈ R n×d denotes the channel-wise cross attention matrix, which measures attention scores of channels of modality E in the perspective of channels in modality P, T CA E-→P ∈ R n×d denotes the token-wise cross attention matrix, which measures attention scores of tokens in modality P in the perspective of tokens in modality E.\n\nThe proposed TACO cross attention synthesizes the above two cross attention matrices, thus get a compounded result that measures correlations on token-level and channel-level simultaneously. Given the T ACO , final result is defined as:\n\nwhere F F N is a token-wise feed forward network, which is implemented by a linear layer with output dimension as Res out . The overall structure of TACO cross attention fusion module is illustrated in Figure  2 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Classification Layer",
      "text": "To Further integrate information from the fusion result F out which is composed of many tokens, we utilize linear layer with input as class token which is the first token of F out to generate classification output . Cross-entropy is used as loss function, which is defined as follows: In data preprocessing of DEAP dataset, signal from one subject collected when watching one vidoe is a matrix with shape as 80× 8064, the row of which represents 32 channels of EEG and 8 channels of peripheral physiological signal, the column of which represents 3s' baseline and 60s' stimuli signal recorded in 128Hz. Baseline signal (a 40×384 matrix) is cut into 3 segments (each segment is a 40×128 matrix), the mean value (a 40×128 matrix) of them is calculated. The mean value of baseline signal is subtracted from the stimuli signal along the time dimension to get data with shape as 40× 7680. For each timestamp, the 32-channel EEG signal is mapped into a 9×9 matrix that represents the 2D electrode topological structure  [Li et al., 2017] , which is illustrated in Figure  4 . After that, each 2D 9×9 matrix is normalized with Z-score normalization. Finally, the processed data are cut into 60 segments, each of which includes 1s signal and has the shape of 9×9×128. According to  [Zhao et al., 2020] , 1s is the most suitable time window length of signal for emotion recognition. Data size of EEG signals in DEAP after processing is 76800 instances (32 participants×40 trials×60s),the dimension of each instance is 9×9×128. The peripheral physiological signal of 8 channels is cut into 60 segments with 1s length in each modality, the data size of peripheral physiological signal in each modality after processing is 76800 instances, the dimension of each instance is 8×128. We then randomly split total dataset with 20% as test dataset and 80% as training dataset. Dreamer [Katsigiannis and Ramzan, 2017] used audiovisual stimuli for affect elicitation from 23 subjects. Each subject exposed to 18 different videos of variable length from 65 s-393 s duration where stimuli signals are recorded and a period of 61 s where subjects are in neutral emotion statement and baseline signals are recorded. Each subject was asked to label the valence and arousal values with the scale from 1 to 5 using Self-Assessment Manikins (SAM) after watching one video. Then we use 3 as threshold and separate the assessment scores into a two-class label set. 14 channels of EEG with AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4 channels were recorded using Emotiv-Epoc portable sensor with sampling rate as 128 Hz. ECG with two channels is recorded using the Shimmer sensor with sampling rate of 256 Hz.\n\nFor Dreamer dataset that contains EEG and ECG signals, we first let stimuli signals subtract the mean value of all channels at each timestamp . Then, we utilize bandpass that ranges from 4-45Hz to filter the last 62s of stimuli signals. Moreover, we calculate a mean vector (14×1) of 61s' baseline signal and subtract it from the filtered stimuli signal at each timestamp. After that, at each timestamp, the EEG signal of 14 channels is mapped into a 9×9 matrix to represent 2D electrode topological structure as shown in Figure  5  . Like DEAP dataset, each 9×9 matrix is normalized with Z-score normalization and flatten. Finally, we get EEG signal dataset with 24840 instances (23 participants×18 trials×60s), the dimension of each instance is 9×9×128. And ECG signal after processing also contains 24840 instances, the dimension of each instance is 2× 128. We then randomly split total dataset with 20% as test dataset and 80% as training dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "We train our model on single NVIDIA R5000X GPU with 24GB memory. The Adam optimizer is used to minimize the loss function. The learning rate is set to 0.001. Batch size is set as 120. For each experiment, we randomly shuffle the samples to get training dataset and testing dataset. The ratio of training set to test set is 4:1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Baseline Models",
      "text": "To verify the effectiveness of our proposed model, we compare it with the following models on the emotion recognition task.\n\n•  ECLGCNN[Yin et al., 2021] : it extract entropy features from segmented data, then utilize LSTM and graph convolution neural network as classification model. • 3DFR-DFCN  [Li et al., 2021] : it extracts kurtosis, power, differential entropy from EEG signals in different frequency bands, then rearrange features to get a 3d representation. Then a fully convolution neural network is utilized to process features, a regularization term is used to minimize spectral norm of weight matrices. • FLDNET  [Wang et al., 2021]",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Result Analysis And Comparison",
      "text": "We compare our model with other baseline models on DEAP and DREAMER dataset on subject-independent experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Studies",
      "text": "In this section, we verify the effectiveness and superiority of our proposed modules from two aspect: multimodal fusion module and spatial position encoding method.\n\nAblation on multimodal fusion modules. We conducted experiments on multimodal fusion module. We set \"2d position encoding+ temporal extraction module\" as Backbone model and compare the performance of four models with different fusion module( concatenate, token-wise cross attention, channel-wise cross attention and TACO cross attention). Ablation on position encoding method. We also conducted experiments on position encoding module. We compare our proposed 2d position encoding method with classic 1d position encoding method  [Vaswani et al., 2017] . Table  3  shows that the proposed 2d position encoding achieves better performance than 1d position encoding. It demonstrates that spatial distribution of multi-channel signal should be considered when building spatial-temporal network.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Statistical Test",
      "text": "To measure the significance of improvement of our proposed multimodal fusion method-TACO cross attention, We conduct 5-fold cross validated paired t-test between frameworks that are based on TACO, CCA and TCA. Both p-values are less than 0.05, which demonstrates that there is a statistically significant difference in performance between \"Backbone+TACO\" and the other two models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visualization",
      "text": "To perform a thorough evaluation on the proposed TACO cross attention, we visualize the cross attention fusion matrices in TACO, CCA, TCA. Figure  6  illustrates the attention matrices from different cross attention methods, which are generated from input data corresponding to high and low valence. The horizontal direction denotes the time dimension and vertical direction denotes the channel dimension. Intuitively, we can see that TACO cross attention matrix varies larger on temporal dimension and channel dimension than CCA and TCA. Elements with large scores are distributed throughout different tokens and channels. But in CCA and TCA, same patterns repeats along channel axis and time axis, respectively. This indicates TACO can assign attention scores in the whole latent space.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a network for emotion recognition based on multimodal psychological signal.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of the QKV attention-based multimodal fu-",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates TACO cross attention and other two atten-",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the overall structure of TACOformer. It",
      "page": 2
    },
    {
      "caption": "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The",
      "page": 3
    },
    {
      "caption": "Figure 3: Figure 3: Token-channel compounded cross attention mechanism.",
      "page": 4
    },
    {
      "caption": "Figure 4: , peripheral physiological signals",
      "page": 4
    },
    {
      "caption": "Figure 4: After that, each 2D 9×9 matrix is normalized with",
      "page": 4
    },
    {
      "caption": "Figure 4: (a) The international 10-20 system which describe the lo-",
      "page": 5
    },
    {
      "caption": "Figure 5: . Like DEAP dataset,",
      "page": 5
    },
    {
      "caption": "Figure 5: (a) The international 10-20 system which describe the lo-",
      "page": 5
    },
    {
      "caption": "Figure 6: illustrates the attention",
      "page": 6
    },
    {
      "caption": "Figure 6: Illustration of output of cross attention matrices- TACO,",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "robust predictions of emotion statements than unimodal emo-": ""
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "tion recognition methods."
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "Many deep learning models have been built for processing"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "multi-channel\ntime series. But fusing these signals from dif-"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": ""
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "ferent modalities remains a critical\nissue for constructing an"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "emotion recognition model."
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "There mainly exist\nthree strategies, which are on feature"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "level, decision level, and model level, as outlined in the liter-"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "ature [Atrey et al., 2010]. The feature level fusion approach"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "involves concatenation of features from various modalities to"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "obtain a combined representation. Nonetheless, when dealing"
        },
        {
          "robust predictions of emotion statements than unimodal emo-": "with signals comprising of numerous channels which corre-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Recently, emotion recognition based on physiolog-"
        },
        {
          "Abstract": "ical signals has emerged as a field with intensive"
        },
        {
          "Abstract": "research.\nThe utilization of multi-modal, multi-"
        },
        {
          "Abstract": "channel physiological signals has significantly im-"
        },
        {
          "Abstract": "proved\nthe\nperformance\nof\nemotion\nrecognition"
        },
        {
          "Abstract": "systems, due to their complementarity. However,"
        },
        {
          "Abstract": "effectively integrating emotion-related semantic in-"
        },
        {
          "Abstract": "formation from different modalities and capturing"
        },
        {
          "Abstract": "inter-modal dependencies remains a challenging is-"
        },
        {
          "Abstract": "sue.\nMany existing multimodal\nfusion methods"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ignore either token-to-token or channel-to-channel"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "correlations of multichannel\nsignals\nfrom differ-"
        },
        {
          "Abstract": "ent modalities, which limits\nthe classification ca-"
        },
        {
          "Abstract": "pability of\nthe models\nto some\nextent.\nIn this"
        },
        {
          "Abstract": "paper, we\npropose\na\ncomprehensive\nperspective"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "of multimodal\nfusion that\nintegrates channel-level"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "and token-level cross-modal\ninteractions.\nSpecifi-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "cally, we introduce a unified cross attention module"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "called Token-chAnnel COmpound (TACO) Cross"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Attention to perform multimodal fusion, which si-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "multaneously models channel-level and token-level"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "dependencies between modalities.\nAdditionally,"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "we propose a 2D position encoding method to pre-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "serve information about\nthe spatial distribution of"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "EEG signal channels,\nthen we use two transformer"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "encoders\nahead of\nthe\nfusion module\nto capture"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "long-term temporal dependencies\nfrom the EEG"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "signal and the peripheral physiological signal,\nre-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "spectively.\nSubject-independent\nexperiments\non"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "emotional dataset DEAP and Dreamer demonstrate"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "that\nthe proposed model achieves\nstate-of-the-art"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "performance."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Emotion is critical in human’s daily activities and interactions"
        },
        {
          "Abstract": "with the real world,\naffecting our cognition and behavior."
        },
        {
          "Abstract": "The valence-arousal model characterizes emotion as a two-"
        },
        {
          "Abstract": "dimensional continuous space in which pleasantness or un-"
        },
        {
          "Abstract": "pleasantness is described as valence, and excitement or calm-"
        },
        {
          "Abstract": "ness is described as arousal. Accurate emotion recognition"
        },
        {
          "Abstract": "can contribute to diverse fields like mental health monitoring,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sponds to location of signal acquisition, spatial\ninformation": "in a single modality could be disregarded. Furthermore, tem-",
          "tion mechanism-based fusion methods. With TACO cross at-": "tention as the fusion method, we build TACOformer network,"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "poral synchronization information from different modalities",
          "tion mechanism-based fusion methods. With TACO cross at-": "which utilizes two separate transformer encoders [Vaswani et"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "may also be overlooked.",
          "tion mechanism-based fusion methods. With TACO cross at-": "al., 2017] as temporal extraction module. The main contribu-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "The decision-level fusion methods involve feature extrac-",
          "tion mechanism-based fusion methods. With TACO cross at-": "tions of this paper are summarized as follows:"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "tion from each modality by passing input signal through spe-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "• We\nsuggest\na new perspective of multimodal\nfusion,"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "cific network, embeddings from different networks are con-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "i.e.\nthe compound of token-wise and channel-wise cross"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "catenated to get a joint representation.\nthen a decision func-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "attention, which can simultaneously capture long-term"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "tion outputs final\nresults.\nThis approach provides the flex-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "compounded\ndependency\nbetween\ntwo multi-channel"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "ibility to select\nthe most\nsuitable method for each modal-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "modalities on temporal level and channel level."
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "ity.\nIn [Dar et al., 2020], Convolutional Neural Networks",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "• We propose\na 2D position encoding method to pre-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "(CNNs) and Long Short-Term Memory (LSTM) networks",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "serve spatial\ninformation in the input sequence of two-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "were separately employed to process the Electroencephalog-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "dimensional images, which outperforms 1d position en-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "raphy (EEG), Galvanic Skin Response (GSR), and Electro-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "coding methods."
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "cardiogram (ECG)\nsignals.\nThe Majority Vote was\nsub-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "sequently utilized as\nthe decision function to produce pre-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "• Extensive\nsubject-independent\nexperiments\nare\ncon-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "dictions\nfrom different\nnetworks.\nNevertheless,\nthis\nap-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "ducted on two benchmark datasets and the experimental"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "proach has limited capabilities at capturing feature correla-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "results show that our network consistently outperforms"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "tions among modalities.\nFurthermore, designing a hetero-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "other models at valence and arousal classification."
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "geneous network architecture could be time and resource-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "The rest of paper\nis organized as below:\nsection 2 intro-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "intensive.",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "duces preliminary. Section 3 presents the proposed methodol-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "For model-level fusion, a large amount of studies concen-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "ogy. Section 4 introduces datasets and data preprocess. Sec-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "trate on implement\ninter-modal\ninteractions to model corre-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "tion 5 presents experiments and analysis. Section 6 concludes"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "lations between modalities.\nFor example,\n[Liu et al., 2021]",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "this paper."
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "employed a Weighted Sum Fusion method, which employed",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "Deep Canonical Correlation Analysis (DCCA) to obtain co-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "ordinated representations of multimodal embeddings.\n[Ma et",
          "tion mechanism-based fusion methods. With TACO cross at-": "2\nPreliminary"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "al., 2019] employed a multimodal\nresidual LSTM network",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "In this paper, we define Xe = (E1, E2, · · ·\n, ET ) ∈ RN ×T"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "to recognize emotion statements.\nThis network is capable",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "as an EEG signal sample containing T time stamps, where"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "of learning the correlation between the Electroencephalogra-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": ") ∈\n, eN\nN is\nthe number of electrodes, Et = (e1\nt\nt , · · ·\nt , e2"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "phy (EEG) and other physiological signals by sharing weights",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "RN denotes\nthe EEG signal of N electrode channels col-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "across modalities\nin each LSTM layer.\n[Hu et al., 2021]",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "lected at\nthe\ntime\nstamp t.\nis\nthen transformed into\nEt"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "used Graph Convolution Networks (GCNs)\nto model\nintra-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "a 2D matrix E2D ∈H×W (see Section 4.2 Data prepro-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "modal and inter-modal dependencies among the audio, video,",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "cess), where H and W denote\nthe\nheight\nand width\nof"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "et\nand\ntext modalities.\nCross\nattention\n[Sun\nal.,\n2021;",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "the matrix.\nFurther, we combine 2D matrices at T times-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "Lu et al., 2022]is a widely adopted multimodal fusion tech-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "tamps then get a 3D spatial-temporal\nrepresentation XE ="
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "nique that provides an effective approach for learning corre-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "(E2D1, E2D2, · · ·\n, E2DT ) ∈ RH×W ×T which is the input"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "lations of different\ntokens between modalities, which can be",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "data of EEG signal.\nSimilarly, PPS with K channels is de-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "defined as token-wise cross attention (TCA). Furthermore, as",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "fined as XP = (P1, P2, · · ·\n, PT ) ∈ RK×T , which contains"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "different channels correspond to different\nlocations for sig-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": ", pK\n) ∈ RK denotes the\nT timestamps.\nPt = (p1\nt\nt , · · ·\nt , p2"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "nal acquisition, adaptively fusing sufficient channel-wise fea-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "peripheral physiological signal of K channels at timestamp t."
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "tures across modalities is advantageous for capturing spatial",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "The EEG emotion recognition problem is defined as:\nlearning"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "information. As such, channel-wise cross attention (CCA)",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "a mapping function which maps the input data that\nincludes"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "[Wang et al., 2022] has been utilized to learn the correla-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "EEG and Peripheral physiological signals to the correspond-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "tion of different channels between two modalities. However,",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "ing emotion:"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "the aforementioned cross attention mechanisms are based on",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "(1)\nYclass = F (XE, XP )"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "a single perspective at\nimplementing multimodal fusion, ei-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "ther token-wise or channel-wise. Therefore,\nit\nis essential\nto",
          "tion mechanism-based fusion methods. With TACO cross at-": "where F denotes the mapping function, Yclass denotes the"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "find an effective way to conduct interaction between modali-",
          "tion mechanism-based fusion methods. With TACO cross at-": "predicted emotion class."
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "ties based on a compounded perspective of channel-wise and",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "token-wise.",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "3\nMethodology"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "In this paper, we propose a network for emotion recog-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "",
          "tion mechanism-based fusion methods. With TACO cross at-": "3.1\nModel overview"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "nition based on multimodal psychological data\n(EEG sig-",
          "tion mechanism-based fusion methods. With TACO cross at-": ""
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "nals and peripheral psychological signals like EOG, EMG,",
          "tion mechanism-based fusion methods. With TACO cross at-": "Figure 2 illustrates the overall structure of TACOformer.\nIt"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "and ECG signals). The proposed multimodal fusion module,",
          "tion mechanism-based fusion methods. With TACO cross at-": "consists of\ntemporal extraction module, TACO cross atten-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "namely Token-chAnnel COmpounded(TACO)\ncross\natten-",
          "tion mechanism-based fusion methods. With TACO cross at-": "tion fusion, and classification layer. After data preprocess-"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "tion, enables us to simultaneously capture long-term token-",
          "tion mechanism-based fusion methods. With TACO cross at-": "ing, EEG is represented as a temporal-spatial 3D matrix, PPS"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "wise and channel-wise dependence between two modalities.",
          "tion mechanism-based fusion methods. With TACO cross at-": "is\nrepresented as\na 2D matrix.\nTwo Multi-head attention"
        },
        {
          "sponds to location of signal acquisition, spatial\ninformation": "Figure 1 illustrates TACO cross attention and other two atten-",
          "tion mechanism-based fusion methods. With TACO cross at-": "transformers extract temporal features from EEG and PPS in"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "model consists of temporal extraction module (two independent transformer encoders), TACO cross attenion fusion module and classification"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "layer.\n(b) structure of TACO corss-attention fusion, where we propose a cross attention mechanism based on compounded perspective of"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "token-wise and channel-wise dependency."
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "the temporal extraction module. TACO cross attention mod-\nLinear projection transforms it\ninto ZE ∈ RT ×d. Following"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "els the compounded correlations between channel-to-channel\nprocess in [Devlin et al., 2018], we concatenate an learnable"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "and token-to-token of two modalities to obtain fused embed-\ntoken CLS ∈ R1×d as the first\ntoken in sequence and use"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "ding. The classification layer utilizes aggregated information\nLayerN orm function to normalize embedding, which is de-"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "picted as follows:\nrepresented as classtoken and connects it with a linear layer"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "to get classification output."
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "(3)\nZ = LayerN orm(Concat(CLS, ZE)) ∈ R(T +1)×d"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "3.2\n2D Spatial Position Encoding"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Next,three embeddings Q(query),K(key),V(value) are gen-"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Transformer\nis permutation-invariant when processing se-\nerated from linear projections: Q = Z × W Q, K = Z ×"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "quence,\nso it\nis necessary to implement position encoding\nW K, V = Z × W V where W Q, W K, W V ∈ Rd×d. The dot"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "to retain relative or absolute position information. However,\nproduct of Q and K is used to derive a weight matrix, which"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "√"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "1D position encoding in [Vaswani et al., 2017] ignores spa-"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "d. The Sof tmax func-\nis rescaled by multiplication with 1/"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "tial distribution of EEG channels.\nTo tackle this problem,"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "tion is applied along rows to generate the attention matrix:"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "we generalize 1D sinusoidal position encoding to 2D position"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Q × K T\nencoding. Made by product of sines in vertical position and"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "√\nM = Sof tmax(\n)\n(4)"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "cosines in horizontal position, we get 2D position encoding"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "d"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "P OS ∈ RT ×H×W , which is depicted as follows:"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Q × K T"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "√\nAttn = M × V = Sof tmax(\n) × V\n(5)"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "d"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "sin(t/10000x/W ) · cos(t/10000y/H )\nx=2i,y=2j"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "are normalized\ncos(t/10000x/W ) · cos(t/10000y/H )\nwhere n denotes the sequence length, Mi,j\nx=2i+1,y=2j"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": " \nP OS(t, x, y) ="
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "alignment\nscores measuring the\nsimilarity between tokens\ncos(t/10000x/W ) · sin(t/10000y/H )\nx=2i+1,y=2j+1"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "sin(t/10000x/W ) · sin(t/10000y/H )\nx=2i,y=2j+1\nziand zj.After residual connection and feed-forward network,"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "the final output is defined as:\n(2)"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "where x and y are the horizontal and vertical position, t is the"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Res = Z + Attn\n(6)"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "timestamp of token in sequence, W and H denote width and"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "height of the 2D EEG matrix.\noutput = Res + F F N (LayerN orm(Res))"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "(7)"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "where F F N is a token-wise feed forward network, which"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "3.3\nTemporal Extraction Module"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "is implemented by a linear layer with output’s dimension as"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Due to the capability at capturing long-range dependencies"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Res."
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "from sequential data of Transformer, we adopt\nthe encoder"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "For multi-head attention,\ninput embedding Z is\nsplitted"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "part of Vanilla Transformer\n[Vaswani\net al.,\n2017]\nto ex-"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "into h smaller embeddings,\nthen these embeddings are fed"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "tract\nfeatures\nfrom EEG and PPS. The key component of"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "through separate linear projections to generate Query Qi, Key"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "Vanilla Transformer\nis Multi-head attention mechanism.\nIt"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "for\neach attention head.\nThen\nKi\nand Value Vi matrices"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "is depicted as\nfollow: Given the input EEG vector XE ∈"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "h head attention outputs\nare\ncomputed,\nconcatenated and"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "RH×W ×T , where T denotes the length of input sequence, H"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "through a linear projection:"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "and W denote the height and width of EEG matrix.\nit is firstly"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "T"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "transposed into shape as T × H × W and 2D position encod-\nQi × Ki"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "√\n(8)\n) × Vi ∈ Rn× d\nAttni = Sof tmax("
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "ing P OS is point-wise added on it.\nSpatial flattening and"
        },
        {
          "Figure 2: (a) The whole process of emotion recognition. EEG and PPS signals are represented as 3D and 2D format input into network. The": "d"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "(9)",
          "matrix, which measures attention scores of tokens in modality": "P in the perspective of tokens in modality E."
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "where W O ∈ Rd×d is the output projection matrix. As illus-",
          "matrix, which measures attention scores of tokens in modality": "The proposed TACO cross attention synthesizes the above"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "trated in Figure.2, embeddings of EEG and PPS are obtained",
          "matrix, which measures attention scores of tokens in modality": "two cross attention matrices,\nthus get a compounded result"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "from two independent transformer encoders.",
          "matrix, which measures attention scores of tokens in modality": "that measures correlations on token-level and channel-level"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "simultaneously. Given the T ACO , final result is defined as:"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "3.4\nToken-channel compound Cross Attention",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "(13)\nResout = T ACO + E"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "Token-wise cross attention (TCA) and channel-wise cross at-",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "tention (CCA) can capture long-term dependencies between",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "(14)\nFout = Resout + F F N (LayerN orm(Resout))"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "different tokens and between different channels of two modal-",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "where F F N is a token-wise feed forward network, which"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "ities,\nrespectively.\nHowever, on one hand,\nimplementation",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "is implemented by a linear\nlayer with output dimension as"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "of\ntoken-wise cross attention to conduct multimodal\nfusion",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "Resout. The overall structure of TACO cross attention fusion"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "could ignore channel-level dependencies, vice versa. On the",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "module is illustrated in Figure 2."
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "other hand, simple combination like concatenation of the two",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "cross\nattention matrix could cause data\nsparseness, which",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "3.5\nClassification Layer"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "leads to a larger latent space and requires much more comput-",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "To Further integrate information from the fusion result Fout"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "ing resource. Therefore, we propose a novel cross attention",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "which is composed of many tokens, we utilize linear\nlayer"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "mechanism to compound channel-wise and token-wise cross",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "with input as class token which is the first\ntoken of Foutto"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "attention, it is depicted as follow:",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "generate classification output\n. Cross-entropy is used as loss"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "function, which is defined as follows:"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "QT\nQP × K T",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "E × KP",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "E\n√\n√",
          "matrix, which measures attention scores of tokens in modality": "(cid:88)"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": ")\n(10)\nT ACO = σt(\n) × VE × σc(",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "L = −\n(15)\nyclog(Pc)"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "n\nd",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "c∈classset"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "where d denotes dimension of\ntokens in embedding space,",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "where c denotes a class in class set, yc denotes the binary"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "n denotes length of\nthe token sequence. QE ∈ Rn×d and",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "label (0/1) of class c for certain input signal, Pc denotes the"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "KE ∈ Rn×d are the Query and Key matrix from embedding",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "predicted probability of class c for the input."
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "of EEG E, QP ∈ Rn×d and KP ∈ Rn×d are the Query and",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "Key matrix from embedding of PPS P . σt denotes Sof tmax",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "function along the rows, σc denotes Sof tmax function along",
          "matrix, which measures attention scores of tokens in modality": "4\nExperiments"
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "the columns. It is illustrated in Figure 3.",
          "matrix, which measures attention scores of tokens in modality": ""
        },
        {
          "AttnM ultiHead = Concat(AttnHead1, · · ·\n, Attnheadh)×W O": "",
          "matrix, which measures attention scores of tokens in modality": "4.1\nDataset and Preprocess"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "into 60 segments, each of which includes 1s signal and has"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "the shape of 9×9×128. According to [Zhao et al., 2020], 1s"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "is the most suitable time window length of signal for emotion"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "recognition. Data size of EEG signals in DEAP after process-"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "ing is 76800 instances (32 participants×40 trials×60s),the di-"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "mension of each instance is 9×9×128. The peripheral phys-"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "iological signal of 8 channels is cut\ninto 60 segments with"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "1s length in each modality,\nthe data size of peripheral phys-"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "iological signal\nin each modality after processing is 76800"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "instances, the dimension of each instance is 8×128. We then"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "randomly split total dataset with 20% as test dataset and 80%"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "as training dataset."
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "Figure 4:\n(a) The international 10-20 system which describe the lo-"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "cation of electrodes used for acquiring EEG signal,\nred nodes are"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "used in DEAP dataset.\n(b) The 9×9 matrix which represents 2D"
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": "electrode topological structure."
        },
        {
          "Z-score normalization.\nFinally,\nthe processed data are cut": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "cation of electrodes used for acquiring EEG signal,\nred nodes are"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "used in DEAP dataset.\n(b) The 9×9 matrix which represents 2D"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "electrode topological structure."
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "Dreamer [Katsigiannis and Ramzan, 2017] used audio-"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "visual stimuli\nfor affect elicitation from 23 subjects.\nEach"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "subject exposed to 18 different videos of variable length from"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "65 s–393 s duration where stimuli signals are recorded and a"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "period of 61 s where subjects are in neutral emotion statement"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "and baseline signals are recorded. Each subject was asked to"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "label\nthe valence and arousal values with the scale from 1 to"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "5 using Self-Assessment Manikins (SAM) after watching one"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "video. Then we use 3 as threshold and separate the assess-"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "ment scores into a two-class label set.\n14 channels of EEG"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "with AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4,"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "F8, AF4 channels were recorded using Emotiv-Epoc portable"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "sensor with sampling rate as 128 Hz. ECG with two channels"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "is recorded using the Shimmer sensor with sampling rate of"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "256 Hz."
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "For Dreamer dataset\nthat contains EEG and ECG signals,"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": ""
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "we first let stimuli signals subtract the mean value of all chan-"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "nels at each timestamp . Then, we utilize bandpass that ranges"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "from 4-45Hz to filter the last 62s of stimuli signals. Moreover,"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "we calculate a mean vector (14×1) of 61s’ baseline signal and"
        },
        {
          "Figure 4:\n(a) The international 10-20 system which describe the lo-": "subtract\nit from the filtered stimuli signal at each timestamp."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: demonstrates that “Backbone+ TACO cross",
      "data": [
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": "Valence (%)\nArousal (%)\nValence (%)\nArousal (%)"
        },
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": "ECLGCNN\n84.81\n85.27\n-\n-"
        },
        {
          "Model\nDEAP\nDREAMER": "3DFR-DFCN\n81.03\n79.91\n82.49\n75.97"
        },
        {
          "Model\nDEAP\nDREAMER": "FLDNET\n83.85 ±11.34\n78.82 ±10.34\n87.67 ±10.02\n89.91 ±12.51"
        },
        {
          "Model\nDEAP\nDREAMER": "DCCA\n85.62\n84.33\n88.99\n90.57"
        },
        {
          "Model\nDEAP\nDREAMER": "GA-MLP\n88.28\n90.63\n-\n-"
        },
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": "TACOformer\n91.59 ± 0.51\n92.02 ± 0.73\n94.58 ± 4.73\n94.03 ± 1.71"
        },
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": "Table 1: The comparison of the state-of-the-art models on the DEAP"
        },
        {
          "Model\nDEAP\nDREAMER": "and DREAMER dataset"
        },
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": "4.5\nAblation studies"
        },
        {
          "Model\nDEAP\nDREAMER": ""
        },
        {
          "Model\nDEAP\nDREAMER": "In this section, we verify the effectiveness and superiority of"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: demonstrates that “Backbone+ TACO cross",
      "data": [
        {
          "attention” outperforms the other strategies,": "both token-wise and channel-wise correlation are important,",
          "it": "",
          "indicates that": ""
        },
        {
          "attention” outperforms the other strategies,": "a compounded utilization of both methods is recommended.",
          "it": "",
          "indicates that": ""
        },
        {
          "attention” outperforms the other strategies,": "Model",
          "it": "DEAP",
          "indicates that": ""
        },
        {
          "attention” outperforms the other strategies,": "",
          "it": "Valence (%)",
          "indicates that": "Arousal (%)"
        },
        {
          "attention” outperforms the other strategies,": "Backbone+Concat\n84.05",
          "it": "",
          "indicates that": "86.18"
        },
        {
          "attention” outperforms the other strategies,": "Backbone+TCA\n87.51",
          "it": "",
          "indicates that": "87.40"
        },
        {
          "attention” outperforms the other strategies,": "Backbone+CCA\n88.79",
          "it": "",
          "indicates that": "89.62"
        },
        {
          "attention” outperforms the other strategies,": "Backbone+TACO(TACOformer)\n91.59",
          "it": "",
          "indicates that": "92.72"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Table 4: T-statistics and p-value of paired 5-fold cross validated t-test"
        },
        {
          "TACO and CCA\n13.872": "multimodal fusion module token-channel compound (TACO)",
          "0.0002\n3.154\n0.0343": "[Li et al., 2021] Dongdong Li, Bing Chai, Zhe Wang, Hai"
        },
        {
          "TACO and CCA\n13.872": "cross attention takes a compounded perspective to conduct in-",
          "0.0002\n3.154\n0.0343": "Yang, and Wenli Du.\nEeg emotion recognition based on"
        },
        {
          "TACO and CCA\n13.872": "teractions between modalities. Meanwhile,\nthe utilization of",
          "0.0002\n3.154\n0.0343": "3-d feature representation and dilated fully convolutional"
        },
        {
          "TACO and CCA\n13.872": "transformer as temporal extraction module effectively learns",
          "0.0002\n3.154\n0.0343": "IEEE Transactions on Cognitive and Develop-\nnetworks."
        },
        {
          "TACO and CCA\n13.872": "long-term dependencies and achieves high-accuracy classifi-",
          "0.0002\n3.154\n0.0343": "mental Systems, 13(4):885–897, 2021."
        },
        {
          "TACO and CCA\n13.872": "cation. The subject-independent experiments on DEAP and",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "[Liu et al., 2021] Wei Liu,\nJie-Lin Qiu, Wei-Long Zheng,"
        },
        {
          "TACO and CCA\n13.872": "DREAMER datasets demonstrate TACOformer achieves bet-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "and Bao-Liang Lu. Comparing recognition performance"
        },
        {
          "TACO and CCA\n13.872": "ter performance than the other baselines. In addition, ablation",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "and robustness of multimodal deep learning models\nfor"
        },
        {
          "TACO and CCA\n13.872": "studies show the effectiveness of\nthe proposed modules in-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "IEEE Transactions on\nmultimodal emotion recognition."
        },
        {
          "TACO and CCA\n13.872": "cluding 2d position encoding and TACO cross attention. It is",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Cognitive\nand Developmental\nSystems,\n14(2):715–729,"
        },
        {
          "TACO and CCA\n13.872": "worth noting that the proposed TACO cross attention module",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "2021."
        },
        {
          "TACO and CCA\n13.872": "is a general fusion method for multimodal multivariate time",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "[Lu et al., 2022] Houhong Lu, Yangyang Zhu, Ming Yin,"
        },
        {
          "TACO and CCA\n13.872": "series, which can be further applied in other fields.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Guofu Yin, and Luofeng Xie. Multimodal fusion convolu-"
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "tional neural network with cross-attention mechanism for"
        },
        {
          "TACO and CCA\n13.872": "References",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "internal defect detection of magnetic tile.\nIEEE Access,"
        },
        {
          "TACO and CCA\n13.872": "[Atrey et al., 2010] Pradeep K Atrey, M Anwar Hossain, Ab-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "10:60876–60886, 2022."
        },
        {
          "TACO and CCA\n13.872": "dulmotaleb El Saddik, and Mohan S Kankanhalli. Multi-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "modal fusion for multimedia analysis: a survey. Multime-",
          "0.0002\n3.154\n0.0343": "[Ma et al., 2019]\nJiaxin Ma, Hao Tang, Wei-Long Zheng,"
        },
        {
          "TACO and CCA\n13.872": "dia systems, 16:345–379, 2010.",
          "0.0002\n3.154\n0.0343": "and Bao-Liang Lu. Emotion recognition using multimodal"
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "the 27th ACM\nresidual\nlstm network.\nIn Proceedings of"
        },
        {
          "TACO and CCA\n13.872": "[Dar et al., 2020] Muhammad Najam Dar, Muhammad Us-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "International Conference on Multimedia, MM ’19, page"
        },
        {
          "TACO and CCA\n13.872": "man Akram, Sajid Gul Khawaja, and Amit N. Pujari. Cnn",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "176–183, New York, NY, USA, 2019. Association for"
        },
        {
          "TACO and CCA\n13.872": "and lstm-based emotion charting using physiological sig-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Computing Machinery."
        },
        {
          "TACO and CCA\n13.872": "nals. Sensors, 20(16), 2020.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "[Marjit et al., 2021] Shyam Marjit, Upasana Talukdar,\nand"
        },
        {
          "TACO and CCA\n13.872": "[Devlin et al., 2018]\nJacob Devlin, Ming-Wei Chang, Ken-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Shyamanta M Hazarika.\nEeg-based emotion recognition"
        },
        {
          "TACO and CCA\n13.872": "ton Lee, and Kristina Toutanova.\nBert:\nPre-training of",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "using genetic algorithm optimized multi-layer perceptron."
        },
        {
          "TACO and CCA\n13.872": "deep bidirectional\ntransformers for\nlanguage understand-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "In 2021 International Symposium of Asian Control Asso-"
        },
        {
          "TACO and CCA\n13.872": "ing. arXiv preprint arXiv:1810.04805, 2018.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "ciation on Intelligent Robotics and Industrial Automation"
        },
        {
          "TACO and CCA\n13.872": "[Hu et al., 2021]\nJingwen Hu, Yuchen Liu,\nJinming Zhao,",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "(IRIA), pages 304–309, 2021."
        },
        {
          "TACO and CCA\n13.872": "and Qin Jin. Mmgcn: Multimodal fusion via deep graph",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "[Sun et al., 2021] Licai\nSun, Bin Liu,\nJianhua Tao,\nand"
        },
        {
          "TACO and CCA\n13.872": "convolution network for emotion recognition in conversa-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Zheng Lian. Multimodal cross- and self-attention network"
        },
        {
          "TACO and CCA\n13.872": "the 59th Annual Meeting of\nthe\ntion.\nIn Proceedings of",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "for speech emotion recognition.\nIn ICASSP 2021 - 2021"
        },
        {
          "TACO and CCA\n13.872": "Association for Computational Linguistics and the 11th",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "IEEE International Conference on Acoustics, Speech and"
        },
        {
          "TACO and CCA\n13.872": "International Joint Conference on Natural Language Pro-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Signal Processing (ICASSP), pages 4275–4279, 2021."
        },
        {
          "TACO and CCA\n13.872": "cessing (Volume 1: Long Papers), pages 5666–5675, 2021.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki"
        },
        {
          "TACO and CCA\n13.872": "[Katsigiannis and Ramzan, 2017] Stamos Katsigiannis\nand",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,"
        },
        {
          "TACO and CCA\n13.872": "Naeem Ramzan. Dreamer: A database for emotion recog-",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "Łukasz Kaiser, and Illia Polosukhin. Attention is all you"
        },
        {
          "TACO and CCA\n13.872": "nition through eeg and ecg signals from wireless low-cost",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "IEEE journal of biomedical and",
          "0.0002\n3.154\n0.0343": "need. Advances in neural information processing systems,"
        },
        {
          "TACO and CCA\n13.872": "off-the-shelf devices.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "30, 2017."
        },
        {
          "TACO and CCA\n13.872": "health informatics, 22(1):98–107, 2017.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "[Koelstra et al., 2011] Sander Koelstra, Christian Muhl, Mo-",
          "0.0002\n3.154\n0.0343": "[Wang et al., 2021] Zhe Wang,\nTianhao Gu, Yiwen Zhu,"
        },
        {
          "TACO and CCA\n13.872": "hammad Soleymani,\nJong-Seok Lee, Ashkan Yazdani,",
          "0.0002\n3.154\n0.0343": "Dongdong Li, Hai Yang, and Wenli Du.\nFldnet: Frame-"
        },
        {
          "TACO and CCA\n13.872": "Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioan-",
          "0.0002\n3.154\n0.0343": "level distilling neural network for eeg emotion recogni-"
        },
        {
          "TACO and CCA\n13.872": "nis Patras. Deap: A database for emotion analysis; using",
          "0.0002\n3.154\n0.0343": "tion. IEEE Journal of Biomedical and Health Informatics,"
        },
        {
          "TACO and CCA\n13.872": "physiological signals. IEEE transactions on affective com-",
          "0.0002\n3.154\n0.0343": "25(7):2533–2544, 2021."
        },
        {
          "TACO and CCA\n13.872": "puting, 3(1):18–31, 2011.",
          "0.0002\n3.154\n0.0343": ""
        },
        {
          "TACO and CCA\n13.872": "",
          "0.0002\n3.154\n0.0343": "[Wang et al., 2022] Haonan Wang, Peng Cao,\nJiaqi Wang,"
        },
        {
          "TACO and CCA\n13.872": "[Li et al., 2017] Youjun Li, Jiajin Huang, Haiyan Zhou, and",
          "0.0002\n3.154\n0.0343": "and Osmar R Zaiane.\nUctransnet:\nrethinking the skip"
        },
        {
          "TACO and CCA\n13.872": "Ning Zhong. Human emotion recognition with electroen-",
          "0.0002\n3.154\n0.0343": "connections in u-net from a channel-wise perspective with"
        },
        {
          "TACO and CCA\n13.872": "cephalographic multidimensional features by hybrid deep",
          "0.0002\n3.154\n0.0343": "the AAAI conference on\ntransformer.\nIn Proceedings of"
        },
        {
          "TACO and CCA\n13.872": "neural networks. Applied Sciences, 7:1060, 10 2017.",
          "0.0002\n3.154\n0.0343": "artificial intelligence, volume 36, pages 2441–2449, 2022."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": ""
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": ""
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": "and lstm. Applied Soft Computing, 100:106954, 2021."
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": "[Zhao et al., 2020] Yuxuan Zhao,\nJin Yang,"
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": ""
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": "network for emotion recognition based on eeg signals."
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": ""
        },
        {
          "[Yin et al., 2021] Yongqiang Yin, Xiangwei Zheng, Bin Hu,": "(IJCNN), pages 1–6, 2020."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal fusion for multimedia analysis: a survey. Multimedia systems",
      "authors": [
        "Atrey"
      ],
      "year": "2010",
      "venue": "Multimodal fusion for multimedia analysis: a survey. Multimedia systems"
    },
    {
      "citation_id": "2",
      "title": "Muhammad Najam Dar, Muhammad Usman Akram, Sajid Gul Khawaja, and Amit N. Pujari. Cnn and lstm-based emotion charting using physiological signals",
      "authors": [
        "Dar"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "Devlin"
      ],
      "year": "2011",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "4",
      "title": "Multimodal fusion convolutional neural network with cross-attention mechanism for internal defect detection of magnetic tile",
      "authors": [
        "Li"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "5",
      "title": "Eeg-based emotion recognition using genetic algorithm optimized multi-layer perceptron",
      "authors": [
        "Ma"
      ],
      "year": "2019",
      "venue": "2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA)"
    },
    {
      "citation_id": "6",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "Sun"
      ],
      "year": "2017",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Fldnet: Framelevel distilling neural network for eeg emotion recognition",
      "authors": [
        "Wang"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "8",
      "title": "Uctransnet: rethinking the skip connections in u-net from a channel-wise perspective with transformer",
      "authors": [
        "Wang"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    }
  ]
}