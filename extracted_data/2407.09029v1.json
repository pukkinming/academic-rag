{
  "paper_id": "2407.09029v1",
  "title": "Enhancing Emotion Recognition In Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, And Refinement Framework",
  "published": "2024-07-12T06:44:42Z",
  "authors": [
    "Haoqin Sun",
    "Shiwan Zhao",
    "Shaokai Li",
    "Xiangyu Kong",
    "Xuechen Wang",
    "Aobo Kong",
    "Jiaming Zhou",
    "Yong Chen",
    "Wenjia Zeng",
    "Yong Qin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition systems rely heavily on the full availability of modalities, suffering significant performance declines when modal data is incomplete. To tackle this issue, we present the Cross-Modal Alignment, Reconstruction, and Refinement (CM-ARR) framework, an innovative approach that sequentially engages in cross-modal alignment, reconstruction, and refinement phases to handle missing modalities and enhance emotion recognition. This framework utilizes unsupervised distribution-based contrastive learning to align heterogeneous modal distributions, reducing discrepancies and modeling semantic uncertainty effectively. The reconstruction phase applies normalizing flow models to transform these aligned distributions and recover missing modalities. The refinement phase employs supervised point-based contrastive learning to disrupt semantic correlations and accentuate emotional traits, thereby enriching the affective content of the reconstructed representations. Extensive experiments on the IEMO-CAP and MSP-IMPROV datasets confirm the superior performance of CM-ARR under conditions of both missing and complete modalities. Notably, averaged across six scenarios of missing modalities, CM-ARR achieves absolute improvements of 2.11% in WAR and 2.12% in UAR on the IEMOCAP dataset, and 1.71% and 1.96% in WAR and UAR, respectively, on the MSP-IMPROV dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MMER) entails the analysis of emotional cues across various modalities, including speech, text, and body language, among others. These modalities serve complementary functions in the expression and interpretation of human emotions. However, in practical applications, the availability of these modalities is * Yong Qin is the corresponding author. frequently compromised; specific modalities may be absent or inaccessible due to various factors. For example, text data may be unavailable due to errors in automatic speech recognition systems, speech may be obscured by excessive background noise, and visual data may be impaired by poor lighting or occlusions. These challenges underscore the need for MMER systems to be highly adaptable and robust, capable of effectively functioning even with incomplete modal information.\n\nConventional multimodal learning paradigms, as documented in the literature  (Yoon et al., 2018; Liu et al., 2022; Sun et al., 2024) , typically operate under the assumption of complete modality presence. These approaches are dedicated to constructing fusion models optimized for scenarios where all modalities are fully available, a presumption that can undermine their utility in situations where modalities are partially missing. Illustratively, as depicted in Fig.  1 , an emotion classified as \"angry\" in a fully modal context-primarily due to pronounced tones in the speech modality-may be reinterpreted as \"neutral\" in the absence of the speech component, with the text and video modalities becoming the guide.\n\nThe research community has developed innovative methodologies aimed at enhancing the resilience of MMER systems faced with incomplete modalities, primarily focusing on predicting miss-ing data across modalities. However, the significant distribution gaps between different modalities present substantial challenges. For instance,  Wang et al. (2023)  utilize flow models to map heterogeneous modality representations into a latent space following a Gaussian distribution, aiming to ensure consistency. Nevertheless, this approach does not adequately address the distribution gaps between modalities. Alternatively,  Zuo et al. (2023)  suggest the use of modality-invariant features to aid in reconstructing modality-specific characteristics. While this method is promising, it has been critiqued for its limited effectiveness, particularly due to difficulties in accurately predicting modalityspecific features. These strategies underscore the critical need for effective cross-modal alignment before attempting to predict missing data across modalities.\n\nTo address the aforementioned problems, this paper introduces a novel framework for cross-modal alignment, reconstruction, and refinement, designated as CM-ARR. The initial alignment phase aims to bridge the distributional divergences between modalities, which facilitates subsequent reconstruction efforts. Specifically, inspired by MAP  (Ji et al., 2023) , we employ an unsupervised 1  distribution-based contrastive learning approach that replaces point representations with their Gaussian distributional counterparts. This method can convey richer multimodal semantic information by effectively encoding uncertainty. Next in the reconstruction phase, we deploy a network based on normalizing flow models. This method transforms aligned modality representations into a Gaussian latent space  (Wang et al., 2023) . Gaussian distributions for missing modalities are estimated by transferring characteristics from available modalities. Subsequently, the representation of the missing modality is obtained through inverse normalization of the estimated Gaussian distribution. In the final refinement phase, we refine modality representations to better capture emotional characteristics. The initial phases prioritize semantic alignment, sidelining emotional attributes. Consequently, we implement supervised point-based contrastive learning. This method considers modalities from different instances of the same class as positive samples and thus disrupts the semantic correlation between modalities. Doing so enables the model to capture emotional attributes and related features beyond mere semantics, enhancing the reconstruction of emotional information.\n\nOverall, CM-ARR begins by aligning modalities to harmonize disparate modal distributions, a step that facilitates the effective estimation of missing data in the subsequent reconstruction phase, and concludes with refinement to accentuate emotional traits. The key contributions of this paper are summarized as follows:\n\n‚Ä¢ We introduce the CM-ARR framework, a pioneering approach for cross-modal alignment, reconstruction, and refinement, designed to enhance emotion recognition in scenarios characterized by incomplete data.\n\n‚Ä¢ We present two contrastive learning strategies: unsupervised distribution-based contrastive learning for effective uncertainty modeling and mitigation of distributional disparities, alongside supervised point-based contrastive learning that disrupts strong semantic intermodality correlations, facilitating a deeper understanding of emotional consistency.\n\n‚Ä¢ Our empirical investigations, conducted on the IEMOCAP and MSP-IMPROV datasets under both missing and full modality conditions, affirm the superior performance of our proposed CM-ARR framework.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Incomplete Multimodal Learning",
      "text": "In MMER, there have been remarkable advances in research addressing the modality absence problem. These methods fall into two main categories: missing modality generation  (Cai et al., 2018; Suo et al., 2019; Du et al., 2018)  and multimodal joint representation learning methods  (Pham et al., 2019; Han et al., 2019; Yuan et al., 2021) .\n\nMissing Modality Generation aims to utilize available modalities to predict or reconstruct missing ones.  Tran et al. (2017)  introduce a Cascaded Residual Autoencoder (CRA) to fill in data with missing modalities. This method effectively recovers incomplete data by integrating a series of autoencoders in a cascaded structure and leveraging a residual mechanism to address corrupted data. Similarly,  Cai et al. (2018)  develop a 3D encoderdecoder network that captures the intermodal relationships and compensates for missing modalities through adversarial and classification losses.\n\nMultimodal Joint Representation Learning seeks to learn latent representations in a common feature space from available data that remain robust even when some modalities are missing.  Pham et al. (2019)  introduce a method for learning robust joint representations through cyclic translation between modalities, thereby enhancing the model's capability to comprehend and represent multimodal data.  Zhao et al. (2021)  propose the Missing Modality Imagination Network (MMIN), a unified model designed to address the issue of uncertain missing modalities.  Zeng et al. (2022b)  employ a Tag-Assisted Transformer Encoder (TATE) network, which guides the network to focus on different missing cases by encoding specific tags for the missing modalities. Furthermore, they  (Zeng et al., 2022a)  propose an Ensemble-based Missing Modality Reconstruction (EMMR) framework to detect and recover semantic features of the key missing modality. However, these methods do not consider the effect of heterogeneous modal gaps on missing modality reconstruction and emotion recognition. IF-MMIN  (Zuo et al., 2023)  and DiCMoR  (Wang et al., 2023)  work on this problem. The former learns modality-invariant features, and the latter transfers distributions from available modalities to missing modalities to maintain distribution consistency in the recovered data. Nevertheless, these approaches only partially bridge modal gaps and overlook semantic uncertainties across modalities. To overcome these limitations, we introduce the CM-ARR framework, which leverages Gaussian distributions to both align modalities and model semantic uncertainty.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contrastive Learning",
      "text": "Contrastive learning (CL)  (Khosla et al., 2020; He et al., 2020)  aims to foster efficient data representations by drawing similar samples closer and distancing dissimilar ones. In recent years, CL has become a cornerstone in the field of representation learning  (Radford et al., 2021; Wang et al., 2021; Ghosh et al., 2022; Sun et al., 2023) . Notably,  Ji et al. (2023)  address the heterogeneity between image and text modalities using unsupervised CL.  Pan et al. (2023)  employ supervised contrastive learning to enhance emotional representation learning by clustering similar text and speech modality samples. Building on these principles, our work introduces both unsupervised distribution-based contrastive learning and supervised point-based contrastive learning. These approaches are designed to bridge the gaps between heterogeneous modalities and decipher common emotional patterns for improved prediction accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cm-Arr",
      "text": "In this section, we detail the proposed CM-ARR framework. Fig.  2  illustrates the architecture of CM-ARR, which comprises three main phases: alignment, reconstruction, and refinement. Without loss of generality, we consider a multimodal dataset consisting of three modalities: text, speech, and video.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Alignment Phase",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Extraction",
      "text": "Given a speech signal, video segment and its corresponding transcribed text, we extract high-level features for each modality as follows: Text Representation: For each text sequence, we obtain high-level text features R t using the pre-trained Bert-base model  (Devlin et al., 2018) , which has 12 encoder layers, each with 12 selfattention heads and 768 hidden units. Speech Representation: For each speech signal, we obtain high-level speech features R s using the pre-trained Wav2vec2-base model  (Baevski et al., 2020) , where the pre-trained Wav2vec2-base model has 12 encoder layers, each with 8 self-attention heads and 768 hidden units. Video Representation: For each video segment, we utilize a pre-trained DenseNet model  (Huang et al., 2017)  to extract facial expression features R v , trained on the Facial Expression Recognition Plus (FER+) dataset  (Barsoum et al., 2016) . These features, referred to as \"Denseface,\" are frame-level sequential features derived from detected faces in video frames, with each feature vector comprising 342 dimensions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Unsupervised Distribution-Based Contrastive Learning",
      "text": "In the alignment phase, to mitigate the gaps between heterogeneous modalities while also modeling the uncertainty, we introduce an uncertainty modeling component (UMC) that employs Gaussian distributions to capture semantic uncertainty. This is coupled with unsupervised distributionbased contrastive learning to bring the modal distributions closer. Fig.  3   Figure  2 : The framework of CM-ARR consists of three phases: the alignment phase employs unsupervised distribution-based contrastive learning to semantically align the video, speech, and text modalities (see UMC in Fig.  3 ); the reconstruction phase applies normalizing flow models to each modality; the refinement phase utilizes supervised point-based contrastive learning to accentuate emotional traits. The red arrows denote the inference process assuming the text modality is missing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "R N",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Head Attention",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ln Mlp",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Œún Œ£ N G(‚Ä¢)",
      "text": "Multi-head Attention where n ‚àà {s, v, t}. We specifically employ the M LP and the multi-head attention mechanism to enhance feature-level and sequence-level interactions, respectively. The UMC learns a mean vector ¬µ n and a variance vector Œ£ n for each R n , transforming the point-based modality representations into Gaussian distributions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ln Mlp",
      "text": "We then implement an unsupervised distributionbased contrastive learning approach to align heterogeneous modal distributions effectively, utilizing the 2-Wasserstein distance  (Kim et al., 2021)  to measure the distance between the Gaussian distributions of three modalities:\n\nwhere m, n ‚àà {s, v, t} and n ‚à© m = ‚àÖ. Suppose there are N speech-text, text-video, and speech-video pairs in each batch, where modalities from the same instance are treated as positive samples and those from different instances as negatives.\n\nTaking the example of speech-text pairs, we utilize InfoNCE loss  (He et al., 2020)  to compute the loss L udcl :\n\nwhere œÑ represents a learned temperature parameter. S (‚Ä¢, ‚Ä¢) denotes the similarity between a speech-text pair. a is a negative scale factor and b is a shift value.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Reconstruction Phase",
      "text": "Using representations from different feature spaces directly may lead to inefficacy due to scale, distribution, and semantic inconsistencies. Hence, we initially project each modality's features into a common dimensional space using a 1D convolutional layer, obtaining X s , X v , and X t for further analysis and processing. Subsequently, we define the normalizing flow model for each modality as F n , with n ‚àà {s, v, t}, and F (-1) n representing its inverse transformation. The features of each modality are fed into their respective normalizing flow model, translating the input features from their original complex distribu- tions to a manageable Gaussian distribution Z n .\n\nConversely, the Gaussian distribution Z n can be transformed back into the complex distribution of the input features via the inverse transformation F (-1) n , giving X n . Assuming text modality is missing and speech and video modalities are available, we input X s and X v into respective flow models, F s and F v , to obtain Z s and Z v . Z t for the missing text modality can then be computed as:\n\nwhere N denotes the Gaussian distribution. ¬µ t and Œ£ t represent the mean and covariance of the text Gaussian distribution, respectively. At this point, Z t represents an estimated Gaussian consistent with missing text. X t featuring the original text distribution is then generated through the inverse process of the text-specific flow:\n\nFinally, the text-specific reconstruction module is used to recover the text features Xt . The module consists of multiple residual channel attention blocks  (Wei et al., 2022) , where we replace the 2D convolutional layer with 1D. The reconstruction loss is computed as:\n\nSimilarly, when speech and video modalities is missing, we follow analogous steps, recovering speech and video features by transferring from available text.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Refinement Phase",
      "text": "To further promote the emotional information in the modality representations, we incorporate supervised point-based contrastive learning to refine the modality representations of CM-ARR.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Supervised Point-Based Contrastive Learning",
      "text": "The learned representations still exhibit strong semantic correlations between various modality pairs. To address this, we utilize supervised point-based contrastive learning. Specifically, we treat modalities from different instances but with the same emotion labels (where different instances may have distinct semantics but share similar emotion characteristics) as positive samples, and those from different labels as negatives. This method transforms oneto-one modality relationships into many-to-many, emotion-centric relationships, thereby enabling the network to learn enhanced emotion representations beyond mere semantics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Modal Fusion",
      "text": "Finally, after obtaining the recovered text features Xt and the available speech and video features X s and X v , we fuse them into the multimodal representation H using three cross-modal attention blocks and one self-attention block  (Vaswani et al., 2017)  for the emotion recognition task. The process is as follows:  10 )\n\nwhere",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Optimization Objective",
      "text": "To train our proposed CM-ARR, four loss functions are required: unsupervised distribution-based contrastive learning loss L udcl , supervised point-based contrastive learning loss L spcl , modality reconstruction loss L rec , and emotion recognition loss L cls . In summary, our training loss is defined as:\n\nwhere Œ±, Œ≤, and Œª represent the trade-off factors.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "Our experiments utilize two widely adopted datasets: IEMOCAP  (Busso et al., 2008)  and MSP-IMPROV  (Busso et al., 2016) . IEMOCAP, collected by the University of Southern California, is a multi-modal emotion corpus comprising 10,039 utterances from 10 actors who express a range of specific emotions. MSP-IMPROV features 7,798 utterances from six sessions with 12 actors, focusing on the exploration of emotional behaviors during spontaneous dyadic improvisations. Additional details about these datasets are available in Table  1 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Setup",
      "text": "We evaluate all comparative methods on IEMO-CAP and MSP-IMPROV using 5-fold crossvalidation and 6-fold cross-validation, respectively. We employ evaluation metrics such as weighted average recall (WAR) and unweighted average recall (UAR) to assess the performance. In all experiments, parameters are configured as: Œ±=1.0, Œ≤=0.1, Œª=10, learning rate=1e-5, batch size=16, epochs=100.\n\nWe benchmark CM-ARR against several stateof-the-art (SOTA) frameworks for incomplete multimodal emotion recognition, including CRA  (Tran et al., 2017) , MMIN  (Zhao et al., 2021) , IF-MMIN  (Zuo et al., 2023) , CIF-MMIN  (Liu et al., 2024) , and DiCMoR +  (Wang et al., 2023) . DiCMoR + denotes our enhanced version of the DiCMoR framework, where we substitute the original speech encoder with a pre-trained Wav2vec2 model. This improves reproduction quality, creating a more robust framework. Using Wav2vec2 also enables a fairer comparison to our method, which similarly utilizes Wav2vec2 for speech encoding.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison With Sota Methods",
      "text": "Table  2  presents the performance of CM-ARR against SOTA models in terms of WAR and UAR under full and missing modality testing conditions. Across all evaluation metrics, CM-ARR consistently outperforms the competing models, indicating its superior performance.\n\nFurther analysis of CM-ARR's recognition performance on the IEMOCAP dataset under various missing modality conditions reveals notable findings. Specifically, when compared to the best SOTA model, DiCMoR + , CM-ARR achieves a relative enhancement of 3.07% and 3.06% in WAR and UAR, respectively, on average. Notably, CM-ARR demonstrates exceptional performance when at least one modality is present, particularly with the availability of the speech modality, showing a significant relative improvements of 4.41% and 4.26% across WAR and UAR, respectively. Con- versely, the performance gains are more modest when only the text modality is present. This discrepancy underscores the speech modality's capacity to encapsulate substantial textual information, facilitating the effective reconstruction of textual modality representations from speech. In contrast, the text modality's limited encapsulation of speechrelated information results in less effective speech modality reconstruction. In addition, the performance of our method is also optimized compared to SOTA under full modality testing condition. In conclusion, CM-ARR's ability to leverage available modalities for reconstructing missing modalities significantly mitigates the challenges posed by missing modalities, affirming its effectiveness in addressing the missing modality problem in multimodal emotion recognition.\n\nThe right side of Table  2  shows the performance comparison between CM-ARR and SOTA methods on the MSP-IMPROV corpus. Given the corpus's complexity as a challenging sentiment analysis dataset, the performance of these methods is generally modest. However, experimental results indicate that CM-ARR consistently surpasses SOTA methods across various scenarios, demonstrating its superior effectiveness and robust generalization capabilities.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "In Table  3 , ablation experiments are conducted on each component of the CM-ARR framework. To illustrate the limitations of models trained exclusively on full modalities in addressing missing modality scenarios, we establish a full modality baseline model, denoted as 'Baseline', which includes feature extraction and cross-modal fusion components. Results from the IEMOCAP dataset indicate a significant performance decline in the Baseline model when faced with missing modalities, underscoring its vulnerability to conditions of modality absence, given its training on the presumption of modality completeness.\n\nEffects of Unsupervised Distribution-based Contrastive Learning: To verify the effectiveness of the alignment phase, we perform an ablation experiment (w/o L udcl ) to evaluate the performance, as shown in Table  3 . The results show that the models with unsupervised distributionbased contrastive learning achieve better performance. This suggests that distribution-based representations could learn richer semantic information from modal uncertainty and help bridge the distributional divergences between modalities, which facilitates subsequent reconstruction. Additionally, replacing distribution-based contrast learning with point-based representation (w/ Point) further demonstrates that leveraging modal uncertainty to gather diverse semantic information offers added advantages. Consequently, Gaussian distributionbased representations prove superior to instancebased representations.\n\nEffects of Supervised Point-based Contrastive Learning: To validate the effectiveness of the refinement phase, we present the results of an ablation experiment (w/o L spcl ). The results show that it is helpful to improve the performance by disrupting the semantic correlation between modalities through our point-based supervised contrast learning. This method allows the model to capture more generalized patterns within modal information, reducing the risk of overfitting to specific semantic content. Consequently, this approach emphasizes emotionally significant attributes, thereby enhancing the representation's robustness.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Parameter Analysis",
      "text": "To thoroughly investigate the impact of various parameter settings on model performance, we conduct the comparative analysis focusing on the effects of L udcl , L rec , and L spcl weights. In summary, all three loss weights significantly influence model performance. The optimal weights are 1.0 for L udcl , 10.0 for L rec , and 0.5 for L spcl .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visualization Analysis",
      "text": "In Fig.  5 , we use t-SNE (  Van der Maaten and Hinton, 2008)  to visualize the distribution of the modality representations for Baseline and our CM-ARR. Fig.  5  illustrate the impact of CM-ARR on the reconstructed representation in scenarios of modality absence. In baseline, there is noticeably less overlap between the reconstructed modality and its ground-truth representations, with the distribution shape of the reconstructed representa-tions markedly differing from that of the groundtruth representations. In contrast, ours (CM-ARR) demonstrates that the distributional similarity of reconstructed representations to the ground-truth representations is significantly enhanced, particularly evident in the overlap of clusters and the distribution shapes.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduce the Cross-Modal Alignment, Reconstruction, and Refinement (CM-ARR) framework, designed to improve multimodal emotion recognition in incomplete data scenarios. CM-ARR effectively models uncertainty within the semantic space using unsupervised distribution-based contrastive learning, reducing the distributional gap. The reconstruction phase utilizes a normalizing flow model to transform aligned distributions, while the refinement phase augments the emotional content of the reconstructed representations. Extensive validation on the widely recognized IEMO-CAP and MSP-IMPROV datasets confirms the superior effectiveness of our approach.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "The efficacy of the CM-ARR framework primarily hinges on its ability to leverage available modalities for reconstructing missing ones, thereby mitigating the adverse impacts of modality absence. The experiments with missing modalities show that different modalities contribute to emotion recognition to different degrees. For example, the video modality in the iemocap corpus is weak with a low degree of its contribution to emotion recognition. Therefore, how to deal with the transformation between weak and strong modalities and measure the importance of these modalities is a more interesting issue, which will motivate us to further optimize our proposed CM-ARR.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of missing modalities: when",
      "page": 1
    },
    {
      "caption": "Figure 1: , an emotion classified as ‚Äúan-",
      "page": 1
    },
    {
      "caption": "Figure 2: illustrates the architecture of",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the architecture of the UMC,",
      "page": 3
    },
    {
      "caption": "Figure 2: The framework of CM-ARR consists of three phases: the alignment phase employs unsupervised",
      "page": 4
    },
    {
      "caption": "Figure 3: ); the reconstruction phase applies normalizing flow models to each modality; the refinement phase utilizes",
      "page": 4
    },
    {
      "caption": "Figure 3: The overall structure of the proposed UMC,",
      "page": 4
    },
    {
      "caption": "Figure 4: The effect of weights Œ±, Œ≤, and Œª on performance.",
      "page": 8
    },
    {
      "caption": "Figure 5: Visualization of the representations from different methods on the IEMOCAP corpus test set. Light blue",
      "page": 8
    },
    {
      "caption": "Figure 4: (a), the UAR (orange line) demon-",
      "page": 8
    },
    {
      "caption": "Figure 4: (b) and (c).",
      "page": 8
    },
    {
      "caption": "Figure 5: , we use t-SNE (Van der Maaten and Hin-",
      "page": 8
    },
    {
      "caption": "Figure 5: illustrate the impact of CM-ARR on",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùìïùíó": "",
          "Column_2": "",
          "Column_3": "ùëçùë£",
          "ùìïùíó (‚àíùüè)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "ùëçùë†",
          "ùìïùíî (‚àíùüè)": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùìïùíï (‚àíùüè)": "",
          "Column_2": "~‚Ñí\nùëüùëíùëê"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "UMC": "~‚Ñí\nùë¢ùëëùëêùëô"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Languages": "",
          "Year": "",
          "Subjects": "",
          "Type": "",
          "Access": "",
          "Samplesizeofemotionsused": "ang",
          "Column_7": "hap",
          "Column_8": "neu",
          "Column_9": "sad"
        },
        {
          "Languages": "English\nEnglish",
          "Year": "2008\n2017",
          "Subjects": "5male,\n5female\n6male,\n6female",
          "Type": "Acted\nActed",
          "Access": "Licensed\nLicensed",
          "Samplesizeofemotionsused": "1103\n460",
          "Column_7": "1636\n999",
          "Column_8": "1708\n1733",
          "Column_9": "1084\n627"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "4",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Deep adversarial learning for multi-modality missing data completion",
      "authors": [
        "Lei Cai",
        "Zhengyang Wang",
        "Hongyang Gao",
        "Dinggang Shen",
        "Shuiwang Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "6",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "7",
      "title": "Semi-supervised deep generative modelling of incomplete multi-modality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "Sreyan Ghosh",
        "Utkarsh Tyagi",
        "S Ramaneswaran",
        "Harshvardhan Srivastava",
        "Dinesh Manocha"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "9",
      "title": "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Zhao Ren",
        "Bj√∂rn Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Map: Multimodal uncertaintyaware vision-language pre-training model",
      "authors": [
        "Yatai Ji",
        "Junjie Wang",
        "Yuan Gong",
        "Lin Zhang",
        "Yanru Zhu",
        "Hongfa Wang",
        "Jiaxing Zhang",
        "Tetsuya Sakai",
        "Yujiu Yang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": [
        "Wonjae Kim",
        "Bokyung Son",
        "Ildoo Kim"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Bjorn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Yang Liu",
        "Haoqin Sun",
        "Wenbo Guan",
        "Yuqi Xia",
        "Zhen Zhao"
      ],
      "year": "2022",
      "venue": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework"
    },
    {
      "citation_id": "17",
      "title": "Gemo-clap: Genderattribute-enhanced contrastive language-audio pretraining for speech emotion recognition",
      "authors": [
        "Yu Pan",
        "Yanni Hu",
        "Yuguang Yang",
        "Jixun Yao",
        "Wen Fei",
        "Lei Ma",
        "Heng Lu"
      ],
      "year": "2023",
      "venue": "Gemo-clap: Genderattribute-enhanced contrastive language-audio pretraining for speech emotion recognition",
      "arxiv": "arXiv:2306.07848"
    },
    {
      "citation_id": "18",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnab√°s P√≥czos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "20",
      "title": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and bert for multimodal emotion recognition",
      "authors": [
        "Dekai Sun",
        "Yancheng He",
        "Jiqing Han"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Fine-grained disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Xuechen Wang",
        "Wenjia Zeng",
        "Yong Chen",
        "Yong Qin"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "22",
      "title": "Metric learning on healthcare data with incomplete modalities",
      "authors": [
        "Qiuling Suo",
        "Weida Zhong",
        "Fenglong Ma",
        "Ye Yuan",
        "Jing Gao",
        "Aidong Zhang"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "23",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "Luan Tran",
        "Xiaoming Liu",
        "Jiayu Zhou",
        "Rong Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Pico: Contrastive label disambiguation for partial label learning",
      "authors": [
        "Haobo Wang",
        "Ruixuan Xiao",
        "Yixuan Li",
        "Lei Feng",
        "Gang Niu",
        "Gang Chen",
        "Junbo Zhao"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "Distribution-consistent modal recovering for incomplete multimodal learning",
      "authors": [
        "Yuanzhi Wang",
        "Zhen Cui",
        "Yong Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "28",
      "title": "Prototype-based classifier learning for long-tailed visual recognition",
      "authors": [
        "Xiu-Shen Wei",
        "Shu-Lin Xu",
        "Hao Chen",
        "Liang Xiao",
        "Yuxin Peng"
      ],
      "year": "2022",
      "venue": "Science China Information Sciences"
    },
    {
      "citation_id": "29",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "30",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Wei Li",
        "Hua Xu",
        "Wenmeng Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "2022a. Mitigating inconsistencies in multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Jiandian Zeng",
        "Jiantao Zhou",
        "Tianyi Liu"
      ],
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "32",
      "title": "2022b. Robust multimodal sentiment analysis via tag encoding of uncertain missing modalities",
      "authors": [
        "Jiandian Zeng",
        "Jiantao Zhou",
        "Tianyi Liu"
      ],
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "Haolin Zuo",
        "Rui Liu",
        "Jinming Zhao",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    }
  ]
}