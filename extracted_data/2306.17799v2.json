{
  "paper_id": "2306.17799v2",
  "title": "A Low-Rank Matching Attention Based Cross-Modal Feature Fusion Method For Conversational Emotion Recognition",
  "published": "2023-06-16T16:02:44Z",
  "authors": [
    "Yuntao Shou",
    "Huan Liu",
    "Xiangyong Cao",
    "Deyu Meng",
    "Bo Dong"
  ],
  "keywords": [
    "Attention",
    "Cross Modal Feature Fusion",
    "Low Rank Decomposition",
    "Multimodal Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conversational emotion recognition (CER) is an important research topic in human-computer interactions. Although recent advancements in transformer-based cross-modal fusion methods have shown promise in CER tasks, they tend to overlook the crucial intra-modal and inter-modal emotional interaction or suffer from high computational complexity. To address this, we introduce a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM effectively captures contextual emotional semantic information in conversations while mitigating the quadratic complexity issue caused by the self-attention mechanism. Specifically, by setting a matching weight and calculating inter-modal features attention scores row by row, LMAM requires only one-third of the parameters of self-attention methods. We also employ the lowrank decomposition method on the weights to further reduce the number of parameters in LMAM. As a result, LMAM offers a lightweight model while avoiding overfitting problems caused by a large number of parameters. Moreover, LMAM is able to fully exploit the intra-modal emotional contextual information within each modality and integrates complementary emotional semantic information across modalities by computing and fusing similarities of intra-modal and inter-modal features simultaneously. Experimental results verify the superiority of LMAM compared with other popular cross-modal fusion methods on the premise of being more lightweight. Also, LMAM can be embedded into any existing state-of-the-art CER methods in a plug-and-play manner, and can be applied to other multi-modal recognition tasks, e.g., session recommendation and humour detection, demonstrating its remarkable generalization ability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "With the development of the multi-modal research field, Conversational Emotion Recognition (CER) that utilizes three modal data (i.e., video, audio and text) to identify the speaker's emotional changes during the conversation has become a hot research topic  [6] ,  [29] . Nowadays, CER has shown its promising performance in many practical social media scenarios. For example, in the field of intelligent recommendation, a recommendation system with emotional tendencies can recommend products that users are more interested in by identifying changes in consumers' emotions. Therefore, it is of great importance to accurately identify the speaker's emotional changes during the conversation.\n\nMany existing studies  [33] ,  [38] ,  [40]  have proven that multi-modal feature fusion plays an important role in CER tasks. For example, as shown in Fig.  1 (a), previous work  [2]  achieved multi-modal feature fusion by simply concatenating multi-modal features. Although this method has utilized multimodal data to a certain extent, it often adopts a strategy of treating each modality equally without fully considering the difference in the amount of emotional information between different modalities. The above method has two limitations: 1) the modality with less information may introduce noise during the fusion process, weakening the performance of the overall model; 2) the dominant role of the modality with more information in emotion recognition is not fully utilized. In addition, existing methods cannot capture contextual emotional semantic information within and between modalities, which limits the performance of CER tasks.\n\nTo tackle the above problems, many Transformer-based methods (e.g., CTNet  [24] , and EmoCaps  [23] , etc) have been proposed and have become the current mainstream multimodal feature fusion methods in CER task. Specifically, as shown in Fig.  1 (b), these Transformer-based multi-modal feature fusion methods mainly use the self-attention mechanism to calculate the attention scores of different modal features, and achieve the fusion of intra-modal and inter-modal contextual emotional semantic information based on attention scores. Although the Transformer-based method can more accurately capture and fuse multimodal features in CER tasks, it also has two limitations. 1) This attention-based method has high computational complexity, especially when processing largescale data, the demand for computing resources increases significantly. 2) As the model complexity increases, the risk of overfitting also increases accordingly, especially when the amount of training data is insufficient or the data diversity is limited. This may lead to a decrease in the generalization ability of the model and make it difficult to maintain stable performance in practical applications.\n\nTo solve these problems, existing research proposes lowrank decomposition methods  [19]  to reduce the number of parameters. For example, Zhu et al.  [50]  proposed a Lowrank Tensor Multi-modal Fusion method to achieve multimodal data fusion through a low-rank tensor outer product and attention mechanism. Jin et al.  [17]  proposed a Fine-grained Temporal Low-rank Multi-modal Fusion (FT-LMF) method to fuse multi-modal information in time and space through lowrank tensor outer product. Sahay et al.  [34]  proposed Lowrank fusion based transformers to extract potential interactive information in multi-modal information through low-rank tensor outer product and cross-modal Transformer. Although the above method achieves the fusion of multimodal information through low-rank tensor outer product, this approach has significant limitations. Specifically, this fusion method lacks fine-grained processing capabilities when capturing the potential relationship information between discourses. Although the low-rank tensor outer product method can reduce the computational complexity to a certain extent, due to its inherent structural constraints, it cannot fully express the complex interactions between modalities, resulting in inaccurate capture of potential relationship information.\n\nDifferent from the aforementioned CER methods, we thoroughly leverage the respective advantages of low-rank decomposition and Transformer techniques. Specifically, as shown in Fig.  1 (c), we directly introduce a low-rank weight W into the self-attention, and then perform row-by-row attention calculation on multi-modal features to achieve fine-grained cross-modal information fusion. Our method not only retains the accuracy of Transformer in feature capture, but also effectively reduces the computational complexity through lowrank decomposition, avoiding the overfitting problem that may be caused by traditional methods.\n\nOverall, we introduce a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM effectively captures contextual emotional semantic information in conversations while mitigating the quadratic complexity issue caused by the selfattention mechanism. Different from the existing Transformers with a self-attention mechanism, the number of parameters required by LMAM is less than one-third of the self-attention mechanism. Furthermore, LMAM is much easier to learn and may also reduce the risk of over-fitting. Fig.  1  illustrates a comparison of different cross-modal fusion methods.\n\nSpecifically, we first use the text, video and audio features pre-extracted by BERT, 3D-CNN and openSMILE respectively to obtain the corresponding query feature vector through a linear layer. In order to reduce the amount of parameters required by the network, we introduce low-rank weight decomposition to achieve compression of the learnable parameters W of the linear layer. Then we perform row-by-row attention calculations on the text query feature vector, video query feature vector, and audio query feature vector with the other two modal features (i.e., matching feature vectors). Finally, we fuse the obtained multi-modal features to obtain the final high-level discourse representation with emotional contextual semantic information.\n\nOverall, the contributions of this work are summarized as:\n\n• We introduce a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM effectively captures contextual emotional semantic information in conversations while mitigating the quadratic complexity issue caused by the self-attention mechanism.\n\n• By introducing only one learnable matching feature vectors and further utilizing the low-rank decomposition method, LMAM can significantly reduce the model complexity of existing Transformer-based cross-modal fusion methods. Furthermore, LMAM can also reduce the risk of overfitting.\n\n• Extensive experiments also verify that the proposed LMAM method can be embedded into the existing DLbased CER methods to improve their recognition accuracy in a plug-and-play manner. In addition, LMAM is a general cross-modal feature fusion method and has potential application value in other multi-modal feature fusion tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Conversational Emotion Recognition",
      "text": "Conversational emotion recognition (CER) involves crossfield knowledge such as cognitive science and brain science, and has received extensive attention from researchers  [25] ,  [44] ,  [49] . Current CER research mainly includes three directions, i.e., sequential context-based emotion recognition, distinguishing speaker state-based emotion recognition, and speaker information-based emotion recognition  [22] .\n\nFor the sequential context-based CER approaches, Poria et al.  [31]  proposed a Bidirectional LSTM (bcLSTM), which utilizes recurrent neural units to extract the speaker's context information in the video, audio, and text features, and then uses the attention mechanism to fusion the information. Hazarika et al.  [11]  designed an interactive conversational memory network (ICON) to extract the multi-modal features of different speakers following the idea of hierarchical modeling, and then input them into the global attention network for information fusion. Xing et al.  [43]  introduced an Adapted Dynamic Memory Network (A-DMN) to fine-grainedly model the dependencies between contextual utterances. Shahin et al.  [36]  proposed a dual-channel long short-term memory compressed-CapsNet to improve the hierarchical representation of contextual information.\n\nFor the different speaker states-based CER methods, Majumder et al.  [28]  proposed a DialogueRNN with three gating neural units (i.e., global GRU, party GRU and emotion GRU) to encode and update context and speaker information. Hu et al.  [14]  proposed a Contextual Reasoning Networks (CRN) to distinguish the speaker's emotional changes in the perceptual stage and the cognitive stage.\n\nFor the speaker information-based CER methods, Ghosal et al.  [7]  proposed a DialogueGCN to model the dialogue relationship between speakers by constructing a speaker relationship graph from the concatenated multi-modal feature vectors. Sheng et al.  [37]  designed a summarization and aggregation graph inference network (SumAggGIN) to consider global inferences related to dialogue topics and local inferences with adjacent utterances. Hu et al.  [14]  proposed a dialogue contextual reasoning network (DCRN) to extract contextual information from a cognitive perspective, and designed a multi-round reasoning module to fuse the emotional information. The DCDM proposed by Su et al.  [39]  introduced a causal directed acyclic graph (DAG) structure to establish complex correlations between hidden emotional information and other observed dialogue elements and construct a dynamic time disentanglement model to capture the hidden emotional change information in the dialogue.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Feature Fusion Approaches",
      "text": "In this subsetion, we briefly review the multi-modal feature fusion methods for the CER task. Liu et al.  [26]  designed a low-rank multi-modal fusion method (LFM) to reduce the computational complexity caused by the change of tensor dimensions. Hu et al.  [16]  proposed a multi-modal fused graph convolutional network (MMGCN) to model dialogue relations between speakers and fuse the cross-modal features. Lian et al.  [24]  proposed a Conversational Transformer Network to fuse complementary semantic information from different modalities. Hu et al.  [13]  proposed Multimodal Dynamic Fusion Network (MM-DFN), which performs emotion recognition by eliminating contextual redundant information. Ma et al.  [27]  proposed a transformer-based self-distillation model to dynamically learn the intra-modal and inter-modal information interaction. Li et al.  [21]  proposed a Graph network based Multi-modal Fusion Technology (GraphMFT) for CER. GraphMFT achieves the fusion of intra-modal and inter-modal contextual semantic information by building multiple graph attention networks. The CFN-ESA proposed by Li et al.  [20]  effectively integrated the data distribution between modalities and promotes the interaction of multimodal information by constructing a unimodal encoder, a cross-modal encoder and a sentiment conversion module. Although these multi-modal fusion approaches can obtain discriminative fused feature by exploiting the information of different modalities, they are either computationally expensive or do not fully consider the complementary information of different modals.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Preliminary Information A. Problem Definition",
      "text": "We assume the participants in the dialogue are P = {p 1 , p 2 , . . . , p N }, where N represents the number of participants (N ≥ 2). We define sequential context T = {t 1 , t 2 , . . . , t M }, where M represents the total number of sessions and t i represents the i-th utterance. The task of CER is to identify the discrete emotions (e.g., happy, sad, disgust, neutral, excited, etc.) in each utterance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Multimodal Feature Extraction",
      "text": "In the CER task, three types of modality data are included, namely text, video and audio. The feature extraction method of each modal is different, and the semantic information they contain is also different  [8] . Next, we will briefly introduce their data preprocessing methods.\n\nWord Embedding: To obtain the feature vector representation of characters that computers can understand  [35] , we use the large-scale pre-training model BERT to encode text features. First, we use the Tokenizer method to segment the text to get each word and its index. We then feed them into the BERT model for feature encoding, and use the first 100dimensional features in the BERT model as our text feature vectors.\n\nVisual Feature Extraction: Following Hazarika et al.  [12] , we use 3D-CNN to extract the speaker's facial expression features and gesture change features in video frames, which have an important impact on the model's understanding of the speaker's emotional changes. Specifically, we utilize 3D-CNN and a linear layer with 512 neurons to obtain video feature vectors with rich semantic information.\n\nAudio Feature Extraction: Following Hazarika et al.  [12] , we use openSMILE  [5]  to extract acoustic features in audio (e.g., loudness, Mel-spectra, MFCC). Specifically, we utilize the IS12 ComP arE1 extractor 1  in openSMILE and a linear layer with 100 neurons to obtain speaker audio features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Self-Attention Mechanism",
      "text": "Self-Attention Mechanism is an algorithm used for sequence data processing to calculate the correlation between various positions of the input sequence. Owing to its powerful information capture capabilities, self-attention mechanisms have received considerable research attention in various fields, e.g., natural language understanding and text generation. A typical attention mechanism is to obtain the attention score by calculating the correlation between each element in the input sequence and other elements. For a given input sequence X, query Q, key K and value V are calculated as follows: We add an extra dimension to each modality feature and pad them with 1 to ensure that the intra-modal semantic information is preserved during inter-modal feature fusion.\n\nwhere Q, K,and V are the learnable parameters.\n\nGiven the query Q, key K and value V , the embeddings representation of the sequence are obtained through the dot product and Softmax function as follows:\n\nIn this work, we only use one query Q as our matching feature vectors for self-attention calculation since the high complexity of the model easily leads to the risk of overfitting.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Proposed Lmam Cross-Modal Fusion Method",
      "text": "In this section, we propose a novel cross-modal fusion method, namely Low-rank Matching Attention Mechanism (LMAM).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Matching Attention Layer",
      "text": "We concatenate the row vectors of all 1s with the extracted multi-modal features to obtain ξ as follows:\n\nwhere ξ u represents context utterence features, ξ a represents audio features, and ξ v represents video features.\n\nThe existing CER approaches usually use feature splicing or feature summation to fuse the cross-modal feature  [16] ,  [26] ,  [26] ,  [45] . As introduced in the related work, these crossmodal fusion methods are either computationally expensive or do not fully consider the complementary information of different modals. Therefore, our goal is to construct an efficient and effective fusion method that captures the differences among multimodal features by computing the correlation among the three modalities of text, video and audio and realizes the fusion of complementary semantic information across modalities. Specifically, the computation process of our proposed LMAM fusion method is shown as follows.\n\nFor a given model input K i , V i and M i , where K i = ξ i , V i = ξ i , and M i = ξ i . We first compute the query matrix\n\nlinear transformation from M i as follows: end for 16: end for 17: ξ f usion = {Ψ 1 , Ψ 2 , . . . , Ψ len(ϕ)/32 }. 18: return the enhanced multimodal feature vectors ξ f usion . and\n\nNext, we get the attention score using the following formula:\n\nwhere ⊤ represents the matrix transpose operation. d k represents the sequence length of the modal features, α i represents the attention score, and\n\nSubsequently, we perform matrix multiplication by the attention score and the modality feature I i to obtain the attention output as follows:\n\nwhere A i is the feature vector after attention calculation. As shown in Fig.  3 , we present the fusion process of text features and audio features using a matched attention mechanism. Similarly, the fusion process for text and video, and video and audio follows the same paradigm.\n\nTo prevent the problem of gradient disappearance and information collapse in the model training, we also build a residual connection layer with normalization operation. Finally, we use a linear layer with ReLU activation function to get the final output of the LMAM. The formulas are as follows:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Low-Rank Weight Decomposition",
      "text": "Low-rank weight decomposition technology achieves parameter reduction and model compression by decomposing the original weight matrix into the product of two or more low-rank matrices  [19] . Since it can effectively reduce the complexity of the model, low-rank decomposition technology has received extensive research attention on many tasks, e.g., image processing and multi-modal fusion. Tucker decomposition is a commonly used technique for tensor decomposition, which can decompose a high-order tensor into a smaller core tensor and a set of factor matrices. Unlike other tensor decomposition methods, Tucker decomposition allows the core tensor to maintain complex relationships between multiple modes, thus being able to capture multimodal dependencies in the data.\n\nAs shown in Fig.  2 , the idea behind low-rank decomposition in LMAM is to decompose the weight W into specific factors that match the modal features. For any N -order weight W i , there is always a low-rank decomposition method. The formula is defined as follows:\n\nwhere r represents the rank of the weight W i , ω\n\nis a collection of low-rank decomposition factors, and η is the size of the multimodal feature set ξ = {ξ u , ξ a , ξ v }, i.e., η = 3. W i is computed during training. Therefore, Eq. 4 can be calculated as:\n\nThe whole computational process of the LMAM method is shown in Figure  1 (c) and the pseudocode of the LMAM method is summarized in Algorithm 1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Comparison To Self-Attention",
      "text": "Studies  [4]  have shown that the performance of the selfattention mechanism is lower than CNN, RNN and other methods when the amount of data is small, while its performance can gradually exceed CNN and RNN when the amount of data is very large. The difference in performance may be attributed to that the self-attention mechanism needs to learn the query vectors Q, the key vectors K, and the value vectors V at the same time, which makes the optimization of the model more difficult. Unlike classic self-attention, LMAM only needs a very low rank weight to achieve better performance than selfattention. Specifically, we only set a learnable parameter W Q for cross-modal feature fusion and capture of complementary semantic information. Furthermore, we perform a parallel lowrank decomposition of W Q with modality-specific factors to further reduce the amount of parameters required for W Q . LMAM can reduce the difficulty of network optimization while maintaining performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Network Architecture Of Lmam Module",
      "text": "In this section, we design a network to implement the LMAM method. The overall network architecture of the LMAM module is illustrated in Figure  4(d) . From Figure  4(d) , we can observe that the LMAM module first receives three modal data as input, and then generates two types of vectors (i.e., query feature vector and matched feature vector) by a linear transformation layer. Subsequently, we compute the attention score based on these feature vectors. Finally, we generate the final fusion feature by conducting cross-modal feature fusion followed by a fine-tuning step.   As shown in Figure  4 , there are three ways to use the proposed LMAM module, i.e., early fusion, early fusion with residual connections, and late fusion. For early fusion, we concatenate the three modalities and then input them into the LMAM module for feature fusion. For early fusion with residual connections, the concatenated features vectors of our three modalities are added to the feature vectors after feature fusion through the LMAM module. For late fusion, we extract the contextual semantic information from the model (e.g., EmoCaps) and then input it to the LMAM module for feature fusion. It should be noted that the selection of the LMAM fusion ways depends on the baseline model itself. In the following experiment, we mainly adopt the latter two ways of fusion, i.e., early fusion with residual connections framework and late fusion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experiments",
      "text": "In this section, we conduct several experiments to verify the effectiveness of our proposed LMAM cross-modal fusion method. Specifically, the overall experimental setting is shown as follows. Firstly, we choose seven state-of-the-art DL-based approaches, including TextCNN  [18] , bc-LSTM  [31] , Dia-logueRNN  [28] , DialogueGCN  [7] , MM-DFN  [13] , M2FNet  [3] , and EmoCaps  [23] , as backbones and embed the proposed LMAM fusion method into these approaches. In particular, TextCNN uses CNN, 3D-CNN and openSMILE to extract text, video and audio features respectively and input the obtained multi-modal data into 1D-CNN to complete emotion classification. Secondly, we compare our proposed LMAM method with four popular cross-modal fusion methods, including classical add operation and concatenate operation, and latest lowrank multi-modal fusion (LFM)  [26] , tensor fusion network (TFN)  [45] , Dual Low-Rank Multimodal Fusion (Dual-LMF)  [17] , Low-Rank Multimodal Fusion with Self-Attention (Att-LMF)  [50] . and Low-Rank Multimodal Fusion with multimodal Transformer (LMF-MulT)  [34] . Thirdly, we conduct an ablation study to verify the necessity of considering the multimodal datasets. Finally, we apply the proposed LMAM method to other multi-modal recognition tasks. All the experiments are conducted on a PC with Intel Core i7-8700K CPU, and one GeForce RTX 3090 with 24GB memory.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Datasets And Evaluation Metrics",
      "text": "The IEMOCAP  [1] , MELD  [32]  CMU-MOSI  [47] , and POM  [30]  datasets are widely used for conversational emotion recognition. Therefore, this paper selects these four benchmark datasets to verify the effectiveness of our LMAM fusion method. The IEMOCAP dataset contains three modal data to meet the needs of multimodal research, namely video, text, and audio. The IEMOCAP dataset contains 151 dialogues and 7433 utterances of 5 actors and 5 actresses. The emotional labels of the IEMOCAP dataset were annotated by at least three experts, and they divided the labels into six categories, namely \"happy\", \"neutral\", \"sad\", \"excited\", \"angry\" and \"frustrated\". The MELD also includes video, text, and audio three modal data. The MELD dataset contains 13,708 utterances and 1,433 dialogues by multiple actors for 13.7 hours. The emotional labels of the MELD dataset were annotated by at least five experts, and they divided the labels into seven categories, namely \"fear\", \"neutral\", \"angry\", \"joy\", \"sadness\", \"disgust\" and \"surprise\". The CMU-MOSI data set is a multi-modal sentiment analysis data set, including video, audio, text and other data modalities. The CMU-MOSI dataset is annotated with an emotion label in the range  [-3, 3] . The POM dataset contains 903 movie review videos, each of which is annotated with 16 speaker features. The speaker in each video is annotated with an emotion label including confidence, enthusiasm, and other characteristics.\n\nThe IEMOCAP dataset only contains the training set and the test set, so we divide the test set into a test set and a validation set at a ratio of 8:2. The MELD dataset includes a training set, a test set, and a validation set. Similarly, the",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Performance Verification Experiment",
      "text": "To verify the effectiveness of our designed LMAM module, we first test our method in a plug-and-play way by directly embedding the LMAM module into seven state-of-the-art DLbased CER methods. The experimental results are shown in Table  II , Table  III , Table  IV . From Table  II , Table  III , and Table IV, it can be easily seen that all the seven backbones have a significant performance improvement on the four datasets after using our proposed LMAM module. The performance improvement may attribute to the full interaction and fusion of different modal information in our proposed LMAM method, while the seven backbone networks only make a simple fusion of cross-modal information and thus neglect some complementary semantic information between different modals. Besides, we also compare the emotion recognition results of bc-LSTM+Att and bc-LSTM * (i.e.,bc-LSTM+LMAM), and the performance of bc-LSTM * is significantly better than that of the bc-LSTM+Att, which implies that the proposed LMAM module is better than the self attention module.\n\nSince the above experiment embeds our proposed LMAM module into the backbones, thus it will increase the parameter number of the backbone network. To verify that the performance improvement doesn't come from the increase of model complexity but the reasonable design of our LMAM module, we increase the parameter number of four backbones (i.e., bc-LSTM, MM-DFN, M2FNet, and EmoCaps) to the same as after embedding the LMAM module. The experimental results are shown in Table  V   better than the bc-LSTM, MM-DFN, M2FNet, and EmoCaps models with the same parameter number, which proves that the performance improvement is not due to the increase of parameter number but is brought by our LMAM module.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Parameter Size And Runtime Analysis",
      "text": "To verify that the proposed LMAM is a lightweight plugand-play module, we analyze the changes in parameter size and running time after adding the LMAM method to the baselines on different emotion recognition datasets. In particular, we perform all experiments under the same experimental configuration to ensure the fairness of the experiments. As shown in Table  VI , we observe that after adding our proposed LMAM module to the baselines, the running time and model parameter size only increase slightly, but the performance of emotion recognition can be greatly improved. The experimental results show that through appropriate model optimization or architecture improvement, the computational overhead can be effectively controlled while improving the model performance. The performance improvement is attributed to the fact that the proposed LMAM can more effectively realize feature fusion, so that the model can better capture and utilize the complementary information between different modalities, thereby improving the accuracy of emotion recognition. At the same time, the increase in running time and parameter size is small, indicating that the design of the model successfully avoids excessive growth in resource requirements while retaining complexity and depth.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Statistical Significance",
      "text": "In this section, to further verify the significance level between the LMAM method proposed in this paper and the baseline method, we conducted a paired t-test to ensure the reliability of our experimental conclusions. The significance level results are shown in Tables VIII and VII. We run the baseline model and the baseline model with the LMAM module 5 times and obtained the corresponding F1 value as an evaluation metric of significance statistics. All emotion categories on IEMOCAP and MELD datasets are statistically significant in F1 values under paired t-test (p < 0.05).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Comparison With Other Cross-Modal Fusion Methods",
      "text": "In this section, to further verify the superiority of our proposed LMAM module, we also conduct an experiment to compare our LMAM module with other four typical crossmodal fusion approaches, i.e., classical add operation and concatenate operation, and latest low-rank multi-modal fusion (LFM)  [26] , tensor fusion network (TFN)  [45] , Dual Low-Rank Multimodal Fusion (Dual-LMF)  [17] , Low-Rank Multimodal Fusion with Self-Attention (Att-LMF)  [50] , Low-Rank Multimodal Fusion with multi-modal Transformer (LMF-MulT)  [34] , AuxFormer  [9] , Cross-modality Context fusion and Semantic Refinement Network (CMCF-SRNet)  [48] , and Low-Rank Adaption (LoRA)  [15] . The selected backbone network is EmoCaps  [23]  and the used datasets are also IEMOCAP and MELD.    [18]  9.8 ×10 -10 6.0 ×10 -10 6.1 × 10 -7\n\n1.1 × 10 -6 4.8 × 10 -7 4.2 × 10 -6 bc-LSTM  [31]  4.8 ×10 -3 6.7 × 10 -4 3.1 × 10 -4 5.6 × 10 -5 1.7 × 10 -3 3.9 × 10 -3 bc-LSTM+Att 4.3 × 10 -5 8.9 × 10 -8 5.5 × 10 -6 1.2 × 10 -8 5.9 ×10 -7 3.2 × 10 -3 DialogueRNN  [28]  4.9 ×10 -8 3.4 × 10 -2 2.9 × 10 -5 2.8 × 10 -3 6.4 × 10 -8 1.7 × 10 -9 DialogueGCN  [7]  3.9 ×10 -7 5.2 ×10 -8 5.1 × 10 -7 1.9 × 10 -6 7.3 × 10 -3 9.4 ×10 -8 MM-DFN  [13]  1.0 ×10 -8 4.5 × 10 -5 5.8 × 10 -5  1.4 × 10 -2 6.9 ×10 -4 7.6 ×10 -10 3.0 × 10 -3 2.9 ×10 -2 9.2 × 10 -10 6.1 × 10 -4 Emocaps  [23]  6.9 × 10 -5 1.7 ×10 -3 4.4 ×10 -3 4.7 ×10 -2 8.4 ×10 -7 3.9 × 10 -2 5.7 × 10 -5\n\nThe experimental results are recorded in Table  IX . As shown in the Table  IX , the LMAM method achieves the best experimental results on the IEMOCAP and MELD datasets, with Acc of 73.0% and 65.4%, respectively, and F1 values of 73.0% and 64.9%, respectively. Specifically, compared with the Add method, the Acc and F1 values of the LMAM method on the IEMOCAP dataset are increased by 1.7% and 2.0%, respectively, and the Acc and F1 values on the MELD dataset are increased by 1.1% and 0.9%, respectively. Compared with the Concatenate method, the Acc and F1 values of the LMAM method on the IEMOCAP dataset are increased by 4.1% and 4.9%, respectively, and the Acc and F1 values on the MELD dataset are increased by 2.8% and 3.8%, respectively. We think this is because the Add method and the Concatenate method do not model complementary semantic information within and between modalities. Additionally, compared with the TFN, LFM, Dual-LMF, Att-LMF, LMF-MulT, AuxFormer and CMCF-SRNet methods, the LMAM method has also achieved better performance in the accuracy and F1 value of emotion recognition, which further illustrates the superiority of our designed LMAM fusion method. Moreover, to further illustrate the effectiveness of our proposed LMAM method, we also compare it with LoRA. Specifically, we divide LoRA into two versions, one with randomly initialized pre-trained weights and the other with pre-trained weights using Emo-Caps. Experimental results show that LoRA using EmoCaps' pre-trained weights performs similarly to our proposed LMAM method, but the performance of LoRA is very poor when the pre-trained weights are randomly initialized. We believe this is because the pre-trained weights of LoRA are frozen, so it is very dependent on the prior information of the pretrained weights. Although the LMAM method we proposed randomly initializes the weights, since our method is end-toend and the weights is learnable, it can learn weights with better generalization performance. F. Ablation study 1) Necessity of multi-modal data: To illustrate the necessity of multi-modal research, we used the EmoCaps method equipped with LMAM module as the backbone to conduct a comparative experiment of uni-modal, bi-modal and multimodal on the IEMOCAP and MELD datasets. The experimental results are shown in Table  X . We conducted a unimodal experiment to utilize only one of the three modalities (i.e., text, video, and audio), a bi-modal experiment to use any two modalities and a multi-modal experiment to use all the three modalities. For the uni-modal experiments, we found that the features of the text modality performed best for emotion recognition on both datasets, followed by the features of the audio modality, and the worst performance of the features of the video modality. In the bi-modal experiment, the emotion recognition effect of text + audio is the best, followed by the emotion recognition of text + video, and the emotion recognition of video + audio is the worst. In the multimodal experiment, we can find that the emotion recognition effect of the combination of the three modalities is the best. Experimental results demonstrate that it is necessary to consider the multi-modal study. Furthermore, designing multi-modal feature fusion methods to improve the effect of emotion recognition is also necessary.\n\n2) Comparison of different embedding ways: To compare the performance of the early fusion and early fusion with residual connections embedding ways introduced in Section 4.2, we conduct comparative experiments using the bc-LSTM, MM-DFN, M2FFNet, and EmoCaps algorithms on the IEMOCAP and MELD datasets. The experimental results are shown in Table  XI . As can be seen, the LMAM module with residual connections can obtain better performance compared with the early fusion without residual connection.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "G. Complexity Analysis",
      "text": "We assume that the query vectors W Q , key vector W K , and value vector W V in the self-attention mechanism have the same dimensions as the input multimodal feature d n . Theoretically, the computational complexity of the LMAM method proposed in this paper is 3 of self-attention model. Furthermore, we evaluate the computational complexity and computation time of LMAM and self-attention mechanism. As shown in the Table XII, the training time and parameter of the LMAM method are much smaller than the self-attention mechanism. Here, we set the rank size to 45.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "H. Rank Settings",
      "text": "We verified the impact of different rank parameter settings on the accuracy of emotion recognition on the IEMOCAP dataset. The experimental results are shown in the  Fig 5.  We observe that when rank = 45, the training effect of the model is the best, and the training effect of the model is stable  when the rank is between 30 and 55. When rank > 45, the training result of the model becomes unstable and the effect is poor. Therefore, better experimental results can be obtained by using lower ranks. Therefore, better experimental results can be obtained by using lower ranks.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "I. Potential Applications",
      "text": "In order to verify that our LMAM method has a potential application in other multi-modal recognition tasks, we further apply the proposed LMAM module to session recommendation and humor detection tasks. Specifically, we embed our LMAM method into dual channel hypergraph convolutional network (DHCN)  [42]  for session recommendation task and Contextual Memory Fusion Network (C-MFN)  [46]  for humor detection task, respectively. The session recommendation task is conducted on the Digietica dataset 2  , and the humor detection task is carried out on the UR-FUNNY  [10]  dataset. The experimental results are illustrated in Table  XIII",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a novel cross-modal feature fusion method to enable better cross-modal feature fusion. To capture the complementary emotional contextual semantic information Method Digietica P@5 P@10 P@20 MRR@5 MRR@10 MRR@20 DHCN  [42]   in different modalities, we utilize a low-rank matching attention mechanism (LMAM) to realize the interaction between multimodal features and use low-rank weights to improve efficiency. LMAM is better than the existing fusion methods while has a lower complexity. Extensive experimental results verify that LMAM can be embedded into any existing DL-based CER methods to improve their performance in a plug-and-play manner. We also mathematically prove the effectiveness of our method. Further, LMAM is a general cross-modal feature fusion method and has potential application value in other multi-modal recognition tasks, e.g., session recommendation and humor detection. Although the LMAM method can achieve better performance improvement and increase the running speed of the model, the introduction of low-rank decomposition technology may cause the model to lose contextual semantic information. Therefore, in future research work, we will study how to achieve efficient fusion of multi-modal features without losing contextual semantic information.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Early fusion through multimodal concatenation. (b) Multimodal",
      "page": 1
    },
    {
      "caption": "Figure 1: (a), previous work [2]",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), these Transformer-based multi-modal fea-",
      "page": 2
    },
    {
      "caption": "Figure 1: (c), we directly introduce a low-rank weight W",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates a",
      "page": 2
    },
    {
      "caption": "Figure 2: LMAM achieves information fusion of multimodal features through parallel low-rank decomposition of weight (i.e., ωa, ωv, ωt) and modal features.",
      "page": 4
    },
    {
      "caption": "Figure 3: , we present the fusion process of text features",
      "page": 4
    },
    {
      "caption": "Figure 3: An illustrative example of the proposed LMAM method fusing",
      "page": 5
    },
    {
      "caption": "Figure 2: , the idea behind low-rank decomposition",
      "page": 5
    },
    {
      "caption": "Figure 1: (c) and the pseudocode of the LMAM",
      "page": 5
    },
    {
      "caption": "Figure 4: (d). From Figure 4(d),",
      "page": 5
    },
    {
      "caption": "Figure 4: Three embedding ways for cross-modal fusion using LMAM module. (a) Embedding the LMAM module before model’s feature extraction. (b)",
      "page": 6
    },
    {
      "caption": "Figure 4: , there are three ways to use the",
      "page": 6
    },
    {
      "caption": "Figure 5: We observe that when rank = 45, the training effect of the",
      "page": 11
    },
    {
      "caption": "Figure 5: The impact of different rank parameter settings on the experimental",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "TextCNN [18]\nTextCNN∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "27.73 29.81\n57.14 53.83\n34.36 40.13\n61.12 52.47\n46.11 50.09\n62.94 55.78\n48.89 48.08\n37.24 37.37\n81.41 72.97\n50.57 53.95\n70.28 63.87\n60.35 59.34\n57.21 62.96\n59.01 59.16\n↑9.51 ↑7.56\n↑24.27 ↑19.14\n↑16.21 ↑13.82\n↓3.91 ↑10.49\n↑24.17 ↑13.78\n↓3.87 ↑3.29\n↑11.46 ↑11.26"
        },
        {
          "Methods": "bc-LSTM [31]\nbc-LSTM+Att\n[31]\nbc-LSTM∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "29.16 34.49\n57.14 60.81\n54.19 51.80\n57.03 56.75\n51.17 57.98\n67.12 58.97\n55.19 54.96\n30.56 35.63\n56.73 62.09\n57.55 53.00\n59.41 59.24\n52.84 58.85\n65.88 59.41\n56.31 56.08\n66.43 73.08\n63.09 55.54\n68.08 77.01\n65.55 60.29\n86.67 31.52\n81.51 49.79\n50.39 59.31\n↑56.11 ↓4.11\n↑9.7 ↑10.99\n↑5.54 ↑2.54\n↑22.1 ↓9.45\n↑15.24 ↑18.16\n↓15.49 ↓0.1\n↑10.36 ↑5.33"
        },
        {
          "Methods": "DialogueRNN [28]\nDialogueRNN∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "75.14 78.85\n25.63 33.11\n58.56 59.24\n64.76 65.23\n80.27 71.85\n61.16 58.97\n63.40 62.77\n63.51 66.43\n62.94 59.53\n66.84 65.79\n72.43 78.64\n78.89 55.04\n77.00 77.65\n55.07 59.07\n↑37.88 ↑33.32\n↓2.71 ↓0.21\n↑4.38 ↑0.29\n↑14.13 ↓10.19\n↓3.27 ↑5.8\n↓6.09 ↑0.1\n↑3.44 ↑3.02"
        },
        {
          "Methods": "DialogueGCN [7]\nDialogueGCN∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "89.14 84.45\n64.13 66.90\n40.63 42.71\n61.97 63.54\n67.51 64.14\n65.46 63.08\n65.91 65.62\n62.60 59.69\n72.38 77.93\n66.39 66.12\n76.86 78.98\n62.82 60.06\n74.40 63.48\n56.40 58.30\n↑21.97 ↑16.98\n↓12.28 ↓5.47\n↑0.85 ↓3.48\n↑6.89 ↓0.66\n↑6.92 ↑14.85\n↓7.73 ↓8.6\n↑0.48 ↑0.51"
        },
        {
          "Methods": "MM-DFN [13]\nMM-DFN∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "69.13 66.42\n68.58 66.33\n40.17 42.22\n74.27 78.98\n70.25 69.97\n76.99 75.56\n68.77 68.20\n74.58 69.57\n81.40 80.00\n69.94 69.69\n74.01 81.03\n64.93 63.71\n73.91 66.67\n59.84 61.71\n↑34.41 ↑27.35\n↓0.26 ↑2.05\n↓4.2 ↓2.71\n↑3.66 ↓3.3\n↑4.41 ↑4.44\n↓8.74 ↓4.62\n↑1.16 ↑1.48"
        },
        {
          "Methods": "M2FNet\n[3]\nM2FNet∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "79.18 82.11\n75.37 68.21\n59.87 68.31\n65.92 60.00\n65.80 65.88\n74.84 72.60\n69.11 69.86\n73.35 69.53\n81.57 81.29\n70.31 70.07\n74.37 81.42\n67.26 63.51\n72.86 66.23\n59.64 62.50\n↑7.43 ↑9.53\n↓4.81 ↓0.69\n↑1.46 ↓2.37\n↓2.51 ↓1.98\n↑6.73 ↑8.69\n↓0.23 ↓5.81\n↑1.20 ↑0.21"
        },
        {
          "Methods": "EmoCaps [23]\nEmoCaps∗\n∆",
          "IEMOCAP\nHappy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "70.34 72.86\n77.39 82.45\n64.27 65.10\n71.79 69.14\n84.50 73.90\n63.94 63.41\n71.22 70.06\n82.52 85.47\n68.41 67.03\n84.85 80.14\n73.67 73.01\n69.93 74.31\n79.49 65.26\n63.33 68.38\n↓0.41 ↑1.45\n↑5.13 ↑3.02\n↑4.14 ↑1.93\n↑7.7 ↓3.88\n↑0.35 ↑6.24\n↓0.61 ↑4.97\n↑2.45 ↑2.95"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "TextCNN [18]\nTextCNN∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "43.35 45.51\n4.63 3.71\n8.91 8.36\n71.23 74.91\n18.25 21.17\n46.14 49.47\n35.33 34.51\n52.48 55.09\n24.19 21.19\n50.58 52.46\n43.43 41.68\n53.17 56.55\n70.23 75.79\n36.47 44.78\n0.00 0.00\n0.00 0.00\n↓1.00 ↑0.88\n↓6.88 ↓0.73\n↓4.63 ↓3.71\n↑5.94 ↑0.02\n↑4.44 ↑2.99\n↓8.91 ↓8.36\n↑8.10 ↑7.17\n↑0.69 ↑1.47"
        },
        {
          "Methods": "bc-LSTM [31]\nbc-LSTM+Att\nbc-LSTM∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "3.84 5.46\n4.31 5.23\n71.45 73.84\n46.82 47.71\n22.47 25.19\n51.61 51.34\n36.71 38.44\n54.18 55.89\n70.45 75.55\n46.43 46.35\n0.00 0.00\n21.77 16.27\n49.30 50.72\n0.00 0.00\n41.77 40.71\n53.73 55.82\n52.33 53.11\n43.23 40.92\n54.97 56.85\n70.78 75.46\n47.18 46.47\n0.00 0.00\n26.09 24.58\n0.00 0.00\n↑0.33 ↓0.09\n↑0.75 ↑0.12\n↑0.00 ↑0.00\n↑4.32 ↑8.31\n↑3.03 ↑2.39\n↑0.00 ↑0.00\n↑1.46 ↑0.21\n↑1.24 ↑1.03"
        },
        {
          "Methods": "DialogueRNN [28]\nDialogueRNN∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "46.42 49.47\n72.12 73.54\n1.61 1.23\n23.97 23.83\n52.01 50.74\n1.52 1.73\n41.01 41.54\n55.10 55.97\n3.13 2.77\n2.01 2.58\n42.40 42.21\n55.27 56.93\n71.74 75.76\n45.83 48.23\n31.71 17.93\n49.25 53.04\n↓0.38 ↑2.22\n↓0.59 ↓1.24\n↑1.52 ↑1.54\n↑7.74 ↓5.90\n↓2.76 ↑2.30\n↑0.49 ↑0.85\n↑1.39 ↑0.67\n↑0.17 ↑0.96"
        },
        {
          "Methods": "DialogueGCN [7]\nDialogueGCN∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "5.14 10.09\n54.31 56.08\n11.62 11.27\n75.61 77.45\n51.32 52.76\n30.91 32.56\n42.51 44.65\n58.74 60.55\n78.19 77.82\n52.27 54.11\n35.79 36.43\n48.31 47.22\n60.96 60.98\n2.17 2.31\n54.15 55.07\n4.05 2.12\n↑2.58 ↑0.37\n↑0.95 ↑1.35\n↓2.97 ↓7.78\n↑4.88 ↑3.87\n↓0.16 ↓1.01\n↓7.57 ↓9.15\n↑5.80 ↑2.57\n↑2.22 ↑0.43"
        },
        {
          "Methods": "MM-DFN [13]\nMM-DFN∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "78.17 77.76\n56.19 54.78\n48.31 47.82\n52.15 50.69\n0.00 0.00\n25.77 22.93\n0.00 0.00\n60.30 59.44\n53.79 56.84\n2.07 4.11\n38.10 31.92\n4.23 7.10\n60.65 59.62\n77.08 76.56\n53.63 50.53\n47.99 46.08\n↓1.09 ↓1.20\n↑1.64 ↑6.15\n↑2.07 ↑4.11\n↑12.33 ↑8.99\n↓2.56 ↓4.25\n↑4.23 ↑7.10\n↓0.32 ↓1.74\n↑0.35 ↑0.18"
        },
        {
          "Methods": "M2FNet\n[3]\nM2FNet∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "68.88 67.98\n17.69 25.24\n72.76 58.66\n5.57 3.45\n50.09 47.03\n68.49 65.50\n57.33 55.25\n63.64 60.87\n73.15 60.37\n9.13 11.25\n69.11 65.92\n60.76 57.31\n64.13 60.97\n68.40 67.27\n51.77 46.68\n15.19 17.62\n↓4.48 ↓0.71\n↑0.39 ↑1.71\n↑3.56 ↑7.80\n↑1.68 ↓0.35\n↑0.62 ↑0.42\n↓2.50 ↓7.62\n↑3.43 ↑2.06\n↑0.50 ↑0.10"
        },
        {
          "Methods": "EmoCaps [23]\nEmoCaps∗\n∆",
          "MELD\nNeutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1": "3.45 3.03\n43.78 42.52\n7.01 7.69\n75.24 75.12\n63.57 63.19\n58.34 57.05\n58.79 57.54\n63.52 62.97\n66.57 64.74\n63.33 62.52\n59.45 60.26\n64.93 63.88\n76.37 74.28\n3.11 2.14\n40.17 42.35\n6.21 7.05\n↑1.13 ↓0.84\n↑3.00 ↑1.55\n↓0.34 ↓0.89\n↓3.61 ↓0.17\n↑4.99 ↑5.47\n↓0.80 ↓0.84\n↑0.66 ↑2.72\n↑1.41 ↑0.97"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "CMU-MOSI\nPOM": "Acc-2 (↑)\nF1 (↑)\nMAE (↓)\nCorr\n(↑)\nAcc-7 (↑)"
        },
        {
          "Methods": "TextCNN [18]\nTextCNN∗\n∆",
          "CMU-MOSI\nPOM": "62.2\n62.0\n1.42\n0.41\n23.3\n69.7\n69.6\n1.31\n0.49\n24.6\n↑7.5\n↑7.6\n↑0.11\n↑0.08\n↑1.3"
        },
        {
          "Methods": "bc-LSTM [31]\nbc-LSTM+Att\nbc-LSTM∗\n∆",
          "CMU-MOSI\nPOM": "73.9\n73.9\n1.08\n0.61\n28.7\n75.2\n75.2\n1.05\n0.62\n34.6\n77.0\n77.1\n0.96\n0.64\n34.7\n↑1.8\n↑1.9\n↑0.09\n↑0.02\n↑0.1"
        },
        {
          "Methods": "DialogueRNN [28]\nDialogueRNN∗\n∆",
          "CMU-MOSI\nPOM": "75.7\n75.8\n1.02\n0.62\n34.6\n77.4\n77.0\n1.00\n0.67\n34.9\n↑1.7\n↑1.2\n↑0.02\n↑0.05\n↑0.3"
        },
        {
          "Methods": "DialogueGCN [7]\nDialogueGCN∗\n∆",
          "CMU-MOSI\nPOM": "74.3\n74.0\n1.06\n0.61\n29.6\n74.9\n74.7\n1.04\n0.62\n31.8\n↑0.6\n↑0.7\n↑0.02\n↑0.01\n↑2.2"
        },
        {
          "Methods": "MM-DFN [13]\nMM-DFN∗\n∆",
          "CMU-MOSI\nPOM": "76.7\n76.7\n0.91\n0.68\n33.3\n77.1\n77.3\n0.89\n0.71\n33.9\n↑0.4\n↑0.6\n↑0.02\n↑0.03\n↑0.6"
        },
        {
          "Methods": "M2FNet\n[3]\nM2FNet∗\n∆",
          "CMU-MOSI\nPOM": "78.9\n78.5\n0.87\n0.73\n34.6\n79.6\n79.9\n0.87\n0.75\n35.5\n↑0.7\n↑1.4\n↑0\n↑0.02\n↑0.9"
        },
        {
          "Methods": "EmoCaps [23]\nEmoCaps∗\n∆",
          "CMU-MOSI\nPOM": "80.2\n80.3\n0.86\n0.78\n36.7\n82.7\n82.9\n0.81\n0.89\n38.2\n↑2.5\n↑2.6\n↑0.05\n↑0.11\n↑1.5"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "bc-LSTM [31]\nbc-LSTM⋄\nbc-LSTM∗",
          "IEMOCAP\nMELD\nParams\nAcc.\nF1\nAcc.\nF1": "0.53M\n55.19\n54.96\n54.18\n55.89\n1.15M\n54.37\n52.19\n53.11\n50.93\n65.55\n60.29\n54.97\n56.85\n1.15M"
        },
        {
          "Method": "MM-DFN [13]\nMM-DFN⋄\nMM-DFN∗",
          "IEMOCAP\nMELD\nParams\nAcc.\nF1\nAcc.\nF1": "2.21M\n68.77\n68.20\n60.30\n59.44\n2.83M\n67.33\n67.02\n60.01\n57.65\n69.94\n69.69\n60.65\n59.62\n2.83M"
        },
        {
          "Method": "M2FNet\n[3]\nM2FNet⋄\nM2FNet∗",
          "IEMOCAP\nMELD\nParams\nAcc.\nF1\nAcc.\nF1": "8.47M\n69.11\n69.86\n63.64\n60.87\n9.09M\n69.46\n68.79\n62.49\n60.91\n70.31\n70.07\n64.13\n60.97\n9.09M"
        },
        {
          "Method": "EmoCaps [23]\nEmoCaps⋄\nEmoCaps∗",
          "IEMOCAP\nMELD\nParams\nAcc.\nF1\nAcc.\nF1": "12.74M\n71.22\n71.06\n63.52\n62.97\n13.36M\n71.18\n70.56\n62.17\n62.11\n73.67\n73.01\n64.93\n63.88\n13.36M"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "TextCNN [18]\nTextCNN∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "0.23M\n0.68s\n1.15s\n0.25s\n0.11s\n0.85M\n1.65s\n2.34s\n0.32s\n0.14s"
        },
        {
          "Methods": "bc-LSTM [31]\nbc-LSTM+Att\nbc-LSTM∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "0.53M\n1.21s\n2.17s\n0.27s\n0.19s\n4.7M\n4.13s\n6.63s\n0.83s\n0.44s\n1.15M\n2.18s\n2.89s\n0.46s\n0.25s"
        },
        {
          "Methods": "DialogueRNN [28]\nDialogueRNN∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "14.47M\n47.96s\n76.92s\n12.56s\n7.73s\n15.09M\n48.54s\n78.43s\n13.17s\n8.42s"
        },
        {
          "Methods": "DialogueGCN [7]\nDialogueGCN∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "12.92M\n56.18s\n82.34s\n17.48s\n9.32s\n13.54M\n57.77s\n88.53s\n18.23s\n9.74s"
        },
        {
          "Methods": "MM-DFN [13]\nMM-DFN∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "2.21M\n3.15s\n6.17s\n1.41s\n0.71s\n2.83M\n3.89s\n7.04s\n1.83s\n0.79s"
        },
        {
          "Methods": "M2FNet\n[3]\nM2FNet∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "8.47M\n12.48s\n25.16s\n3.39s\n1.15s\n9.09M\n13.11s\n28.33s\n3.77s\n1.82s"
        },
        {
          "Methods": "EmoCaps [23]\nEmoCaps∗",
          "IEMOCAP\nMELD\nCMU-MOSI\nPOM\nParams\nRunning Time\nRunning Time\nRunning Time\nRunning Time": "12.74M\n16.73s\n31.07s\n4.82s\n2.56s\n13.36M\n17.54s\n34.35s\n5.14s\n3.06s"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: http://cikm2016.cs.iupui.edu/cikm-cup/",
      "data": [
        {
          "Methods": "",
          "IEMOCAP\nMELD": "Acc.\nF1\nAcc.\nF1"
        },
        {
          "Methods": "bc-LSTM∗\nbc-LSTM∗(R)",
          "IEMOCAP\nMELD": "59.49\n59.16\n57.69\n55.47\n61.77\n60.49\n59.56\n57.49"
        },
        {
          "Methods": "MM-DFN∗\nMM-DFN∗(R)",
          "IEMOCAP\nMELD": "67.93\n67.15\n63.28\n61.12\n69.82\n69.68\n68.02\n65.24"
        },
        {
          "Methods": "M2FNet∗\nM2FNet∗(R)",
          "IEMOCAP\nMELD": "68.35\n57.96\n67.47\n66.59\n70.27\n70.07\n68.34\n67.25"
        },
        {
          "Methods": "EmoCaps∗\nEmoCaps∗(R)",
          "IEMOCAP\nMELD": "71.49\n71.01\n65.03\n64.22\n73.67\n73.01\n64.93\n63.88"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1",
      "venue": "Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Textcnn"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "M2fnet"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1",
      "venue": "Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1 Acc. F1"
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "Textcnn"
      ],
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "M2fnet"
      ],
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "Benchmarking multimodal sentiment analysis",
      "authors": [
        "E Cambria",
        "D Hazarika",
        "S Poria",
        "A Hussain",
        "R Subramanyam"
      ],
      "year": "2017",
      "venue": "Computational Linguistics and Intelligent Text Processing: 18th International Conference"
    },
    {
      "citation_id": "9",
      "title": "M2fnet: multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "14",
      "title": "Comma-deer: Common-sense aware multimodal multitask approach for detection of emotion and emotional reasoning in conversations",
      "authors": [
        "S Ghosh",
        "G Singh",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Auxformer: Robust approach to audiovisual emotion recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Ur-funny: A multimodal language dataset for understanding humor",
      "authors": [
        "M Hasan",
        "W Rahman",
        "A Zadeh",
        "J Zhong",
        "M Tanveer",
        "L.-P Morency",
        "M Hoque"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. ACL"
    },
    {
      "citation_id": "18",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "19",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "22",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Dual low-rank multimodal fusion",
      "authors": [
        "T Jin",
        "S Huang",
        "Y Li",
        "Z Zhang"
      ],
      "venue": "Dual low-rank multimodal fusion"
    },
    {
      "citation_id": "24",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL"
    },
    {
      "citation_id": "25",
      "title": "Tensor decompositions and applications",
      "authors": [
        "T Kolda",
        "B Bader"
      ],
      "year": "2009",
      "venue": "SIAM review"
    },
    {
      "citation_id": "26",
      "title": "Cfn-esa: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Y Liu",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Graphmft: A graph network based multimodal fusion technique for emotion recognition in conversation",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao",
        "Y Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "30",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Social image-text sentiment classification with cross-modal consistency and knowledge distillation",
      "authors": [
        "H Liu",
        "K Li",
        "J Fan",
        "C Yan",
        "T Qin",
        "Q Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "33",
      "title": "A transformerbased model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "S Mohammad"
      ],
      "year": "2022",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
      "authors": [
        "S Park",
        "H Shim",
        "M Chatterjee",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "37",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Low rank fusion based transformers for multimodal sequences",
      "authors": [
        "S Sahay",
        "E Okur",
        "S Kumar",
        "L Nachman"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language"
    },
    {
      "citation_id": "41",
      "title": "From bert's point of view: Revealing the prevailing contextual differences",
      "authors": [
        "C Schuster",
        "S Hegelich"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "42",
      "title": "Novel dual-channel long short-term memory compressed capsule networks for emotion recognition",
      "authors": [
        "I Shahin",
        "N Hindawi",
        "A Nassif",
        "A Alhudhaif",
        "K Polat"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "43",
      "title": "Summarize before aggregate: a global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "D Sheng",
        "D Wang",
        "Y Shen",
        "H Zheng",
        "H Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "44",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "45",
      "title": "Dynamic causal disentanglement model for dialogue emotion detection",
      "authors": [
        "Y Su",
        "Y Wei",
        "W Nie",
        "S Zhao",
        "A Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Exploration meets exploitation: Multitask learning for emotion recognition based on discrete and dimensional models",
      "authors": [
        "G Tu",
        "J Wen",
        "H Liu",
        "S Chen",
        "L Zheng",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "47",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "Self-supervised hypergraph convolutional networks for session-based recommendation",
      "authors": [
        "X Xia",
        "H Yin",
        "J Yu",
        "Q Wang",
        "L Cui",
        "X Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "49",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "S Xing",
        "S Mai",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "53",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "54",
      "title": "A cross-modality context fusion and semantic refinement network for emotion recognition in conversation",
      "authors": [
        "X Zhang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "55",
      "title": "Cross-modal credibility modelling for eeg-based multimodal emotion recognition",
      "authors": [
        "Y Zhang",
        "H Liu",
        "D Wang",
        "D Zhang",
        "T Lou",
        "Q Zheng",
        "C Quek"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "56",
      "title": "Yuntao Shou is currently pursuing the M.S. degree in the School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an. His research interests include graph learning and emotion recognition",
      "authors": [
        "H Zhu",
        "Z Wang",
        "Y Shi",
        "Y Hua",
        "G Xu",
        "L Deng"
      ],
      "year": "2020",
      "venue": "Wireless Communications and Mobile Computing"
    },
    {
      "citation_id": "57",
      "title": "He is currently an Associate Professor with the School of Computer Science and Technology, Xi'an Jiaotong Uni versity. His research interests include affective computing, machine learning, and deep learning. (Email: huanliu@xjtu.edu.cn) Xiangyong Cao received the B.Sc. and Ph.D. degrees from Xi'an Jiaotong University",
      "year": "2012",
      "venue": "He is currently an Associate Professor with the School of Computer Science and Technology, Xi'an Jiaotong Uni versity. His research interests include affective computing, machine learning, and deep learning. (Email: huanliu@xjtu.edu.cn) Xiangyong Cao received the B.Sc. and Ph.D. degrees from Xi'an Jiaotong University"
    },
    {
      "citation_id": "58",
      "title": "His research interests in clude model-based deep learning, variational networks, and meta-learning (Email: dymeng@mail.xjtu.edu.cn) Bo Dong received the Ph.D. de gree in computer science and technology from Xi'an Jiaotong University",
      "year": "2001",
      "venue": "His research interests in clude model-based deep learning, variational networks, and meta-learning (Email: dymeng@mail.xjtu.edu.cn) Bo Dong received the Ph.D. de gree in computer science and technology from Xi'an Jiaotong University"
    }
  ]
}