{
  "paper_id": "2210.14977v3",
  "title": "Knowledge Transfer For On-Device Speech Emotion Recognition With Neural Structured Learning",
  "published": "2022-10-26T18:38:42Z",
  "authors": [
    "Yi Chang",
    "Zhao Ren",
    "Thanh Tam Nguyen",
    "Kun Qian",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech emotion recognition",
    "neural structured learning",
    "edge device",
    "lightweight deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has been a popular research topic in human-computer interaction (HCI). As edge devices are rapidly springing up, applying SER to edge devices is promising for a huge number of HCI applications. Although deep learning has been investigated to improve the performance of SER by training complex models, the memory space and computational capability of edge devices represents a constraint for embedding deep learning models. We propose a neural structured learning (NSL) framework through building synthesized graphs. An SER model is trained on a source dataset and used to build graphs on a target dataset. A relatively lightweight model is then trained with the speech samples and graphs together as the input. Our experiments demonstrate that training a lightweight SER model on the target dataset with speech samples and graphs can not only produce small SER models, but also enhance the model performance compared to models with speech samples only and those using classic transfer learning strategies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER), which aims to recognise emotional states from speech, has been a popular research topic in the domain of human-computer interaction (HCI)  [1] . SER has been applied to a range of applications, including call centres, education, mental health, computer games, and many others  [2] . In particular, speech signals provide rich and complementary information to other modalities, e. g., images, biosignals, social media, etc  [3] . SER can not only improve the performance of emotion recognition when combined with other modalities in a multimodal system, but also enable machines to perceive human emotions when other modalities are not available, such as in audio-only call centres  [1] .\n\nDeep learning has been widely applied to SER with many model architectures, including spectrum-based and end-to-end models. Spectrum-based models process spectrum features from speech signals  [4, 5] , while end-to-end models directly process raw speech signals  [6, 7] . Convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their variants (e. g., transformers) have been commonly used to build either end-to-end or spectrum-based models for SER  [8, 9] .\n\nImproving the performance of SER faces two challenges. First, creating emotional speech datasets with high-quality annotations is a time-consuming and potentially biased process, leading to small-scale datasets  [10] . Second, with the increasing demand for SER in Internet-of-Things (IoT) applications, it is essential to train lightweight neural networks for efficient model development. However, directly fine-tuning complex SER models pre-trained on largescale data places a high demand on computing systems  [11] .\n\nMore recently, neural structured learning (NSL) was proposed to add structured signals (e. g., graphs) as the model input in addition to the original data  [12] . In particular, NSL was developed to solve the data labelling problem of semi-supervised learning, and to build adversarial training against adversarial attacks  [12] . Inspired by NSL, it is promising to construct a graph with a pre-trained model to break the bottleneck caused by small-scale labelled data and edge devices.\n\nIn this study, we propose an NSL framework to transfer the knowledge of a large, pre-trained SER model to a smaller model with graph. To the best of the authors' knowledge, there were only few studies using NSL for SER  [13] . The contributions of our work are twofold: i) transferring model knowledge through an NSL-generated graph can improve performance by leveraging multiple databases; ii) the proposed NSL framework can train lightweight neural networks without a high requirement of computing resources. Evaluated on an emotional speech dataset, our NSL framework outperforms models trained on the original data only and those trained with classic transfer learning strategies. Related Works. Deep Learning for SER. As previously mentioned, deep learning is mainly applied for SER with spectrum-based and end-to-end models. Spectrum-based models are typically shallower and more efficient than end-to-end models due to smaller data size of extracted spectrums compared to raw speech signals. On the other hand, end-to-end models save the time of selecting suitable spectrum types and can extract complementary features over fixed spectrums. An end-to-end model was developed based on 1D CNNs for learning spatial features in SER  [14] . Additionally, stacked multiple transformer layers were utilised in  [8]  to better extract global feature dependencies from speech. More recently, wav2vec models, which include CNNs and transformers, were trained with self-supervised learning on unlabelled speech data  [15] . Wav2vec models have been applied to generate speech embeddings for SER  [16, 17] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Arxiv:2210.14977V3 [Cs.Sd] 11 May 2023",
      "text": "Transfer Learning. Transfer learning was designed to transfer knowledge learnt from a large-scale source dataset to a smaller target dataset for better performance in an efficient manner  [18] . Transfer learning has been applied to SER due to limited labelled emotional speech data. For instance, a source dataset with a large number of languages was utilised along with a small amount of target data for training deep belief networks in  [19] . Various approaches to transfer learning aim to gap the data distribution difference between the source dataset and the target dataset  [18] : instance-based transfer learning achieves this by re-weighting samples in the source dataset during training; feature-based transfer learning attempts to learn new feature representations from original ones; parameter-based transfer learning targets to find shared model parameters between the source domain and the target domain; relational knowledge transfer learning works on relational domains with non-i.i.d. data and builds mapping of relational knowledge  [18] . These transfer learning strategies mostly (instance-based, parameter-based, and relational ones) require training one or two models with the same architecture on both source and target data. The proposed knowledge transfer via the NSL framework is an extension of feature-based transfer learning. Compared with the transferred representations, the additional graphs fed into the model on the target data have more regularised and complementary information.\n\nNeural Structured Learning. Neutral Structured Learning (NSL) adds additional structured signals (e. g., graghs) as a regularisation term to maintain the latent structural similarity among input signals.\n\nIn  [13] , Mel frequency cepstrum coefficients and NSL-generated structured signals were combined for SER. However, there was no knowledge transfer in that work and the type of structured signals were not clearly specified.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "As depicted in Figure  1 , with the help of a model trained on a related emotional source speech dataset, embeddings of audio samples in the target dataset are extracted and then used to construct graphs accordingly. Log Mel spectrograms of the audio samples, along with the graphs, are fed into a relatively lightweight model. Afterwards, for each training sample, the loss function has two parts: (1) the neighbor loss, which reduces the distance between the embedding of a sample and the embeddings of its neighbors to maintain structural similarity, and (2) the supervised loss, which is calculated based on the predicted label and the true label in supervised learning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Upstream And Downstream Models",
      "text": "The upstream model denotes a large pre-trained model on a source dataset in SER, while the downstream model on the target data is lightweight and possibly run on edge devices. As noted in Section 1, spectrum-based models can be smaller than end-to-end models due to different inputs. Therefore, the upstream model is an end-to-end model and the downstream one is a spectrum-based model herein.\n\nUpstream. Due to its powerful capability for representations extraction, the upstream model utilised herein is wav2vec 2.0  [15] , which has been widely applied in SER  [16, 20, 21] . Wav2vec 2.0 is an end-to-end model composed of a CNN module as the latent speech feature encoder and a Transformer module for capturing global contextual dependencies. The output of the encoder module is discretised with product quantisation  [22]  for self-supervised training. The pre-trained wav2vec 2.0 model applied in this work is trained on the 960-hour Librispeech corpus  [23] . An audio classification head is then added with two linear layers for the SER task. After fine-tuning",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Neural Structured Learning",
      "text": "With graphs as the structured signals, NSL maintains the structural similarity between the speech samples. In this way, NSL has the potential to improve the model performance with a low requirement of computational resource during training the downstream model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph",
      "text": "The input to the downstream model includes data nodes D and the graph G. Specifically, D = {(xi, yi)}, i = 1, ..., N , where xi is a data sample, yi is its emotional state, and N is the number of training samples; The graph G = (D, E), where E denotes the bi-directional edges connecting similar nodes. A pair of connected nodes in G is a neighbour of each other N = (xi, xj), i = j. The neighbours of xi are computed by\n\nwhere S is the cosine similarity, and h is the feature embedding extracted by the upstream model. The graph G with structured similarity information is then fed into the current lightweight downstream model to achieve the knowledge transfer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Loss Function",
      "text": "The loss function applied in this work is the weighted sum of two components, supervised loss and neighbour loss, as follows:\n\nwhere L supervised is calculated by the cross entropy loss function for classification based on the target y and predicted ŷ; L neighbour reflects the distance between the embeddings p(xi) and p(x k ) from the intermediate layers of the downstream model. We use (1cosine similarity) as the distance function and α is a multiplier.\n\nBy minimising the neighbour loss, the similarity among a sample and its neighbours is maintained. As a result, the transferred knowledge from the source dataset is used on the target dataset.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Databases",
      "text": "RAVDESS. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) is a multi-modal English dataset containing 1,440 speech recordings. It was recorded from 24 professional actors (12 females and 12 males) and labelled into eight emotional classes: angry, calm, disgusted, fearful, happy, neutral, sad, surprised. Each emotional class has eight recordings from each speaker, except for neutral, which has four. We split the dataset in a speaker-independent and gender-balanced manner shown in Table  1 .\n\nBesides its wide applications in SER  [2] , the reasons why we choose RAVDESS as the source dataset are that it is recorded from professional actors in a well-controlled environment, its emotional labels have high levels of validity, and the dataset is gender-balanced. DEMoS. The Database of Elicited Mood in Speech (DEMoS)  [24]  is an Italian speech corpus with 7.7 hours of audio samples recorded from 68 speakers (45 males and 23 females). Besides the 332 neutral speech recordings, there are in total 9, 365 audio samples annotated into seven classes: anger, disgust, fear, guilt, happiness, sadness, and surprise. Similar to prior work  [25]  on the DEMoS data, the minority neutral class is not considered and the remaining 9, 365 emotional speech samples are divided into 40 % train, 30 % validation, and 30 % test sets with speaker-independent strategy. The detailed data distribution of the DEMoS is depicted in Table  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "Evaluations Metrics. Similar to previous works on the DEMoS, the Unweighted Average Recall (UAR) is utilised as the standard evaluation metric to mitigate the class imbalance issue. For the RAVDESS, accuracy is applied to better compare with other works  [26] [27] [28] [29] .\n\nImplementation Details. In our experiments, all audio recordings are re-sampled into 16 kHz. The batch size is 16. For the upstream wav2vec 2.0 model fine-tuned on the RAVDESS, the fine-tuning procedure is optimised by an Adam optimiser with a learning rate of 3e -5, and stopped after 20 epochs. The feature encoder of wav2vec 2.0 is frozen. The outputs of the second last fully connected layer 'FC1' of the wav2vec 2.0 model are averaged over the time frame, resulting in the embeddings h. The dimension of h(xi) is 256, and we set the threshold as 0.99. A lager max number of neighbors n requires more computing resources on edge devices; therefore, we limit n to under 10 considering the dataset size of the DEMoS. For ablation study, we choose 3, 6, and 9 for n. Similarly, we empirically chose 0.01, 0.1 and 1 for the multiplier α.\n\nAs for the downstream lightweight models development, log Mel spectrograms are extracted from the DEMoS audio samples as features. Firstly, we unify the audio length as the maximum one of all audio durations by simply cutting extra signals and selfrepeating shorter samples. To extract log Mel spectrograms, the sliding window, overlap, and Mel bins are set as 512, 256, and 64 time frames, respectively. In this way, the extracted log Mel spectrograms' dimension is (373, 64), where 373 is on the time axis, and 64 describes the number of Mel frequency bins. The model development is also optimised with an Adam optimiser with an initial learning rate of 1e -3 and stopped after 50 epochs. The learning rate decays by a factor of 0.9 after every 5 epochs. As for the comparison with transfer learning, the same embeddings generated from the upstream wav2vec 2.0 model are added / maximised / averaged with the output of the 'FC1' layer of the downstream lightweight models. The code for this study has been made available at https://github.com/glam-imperial/NSL-SER.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Wav2Vec2 Fine-Tuning",
      "text": "To ensure the efficacy of NSL in constructing the graphs, we compare the performance of the fine-tuned wav2vec 2.0 model with state-of-the-art (SOTA) models on the RAVDESS. Accuracy is used to ensure a relatively fair comparison. Moreover, the eight emotional classes are balanced besides neutral, so accuracy is close to UAR. The SOTA models' accuracy scores on the RAVDESS test dataset are 76.08 %  [26] , 78.8 %  [27] , 93.2 %  [28] , and 81.94 %  [29] . Our fine-tuned wav2vec 2.0 model achieves 82.50 % test accuracy, which is better than most works. Please note that the data split is different between SOTA's works and our work. Specifically, data split ratio, strategy, and considered emotional classes are not the same.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Sensitivity Analysis",
      "text": "As shown in Figure  2 , we evaluate the performance of VGG-15 models in different NSL settings, i. e., different maximum neighbours n and two embedding layers ('FC1', 'FC2') for building graphs. Compared with the performance of base VGG-15 (validation UAR 66.4 % , test UAR 77.2 %), VGG-15 with NSL achieves better performance. Specifically, when n is 6, α is 0.1, and the embedding layer is 'FC2', the test UAR obtains significant improvement (p < 0.1 in a one-tailed z-test). To further validate the effectiveness of the proposed NSL framework, we choose the best settings for n, α, and embedding layer based on the test UAR indicated in Figure  2 . Specifically, by averaging and comparing the test UAR grouped by 'FC1' and 'FC2', we observe that 'FC2' tends to obtain better performance, possibly due to more abstract representations learnt after 'FC2'. Afterwards, for different n (i. e.,  3, 6, 9) , we find the best multipliers are 0.01, 0.1, and 0.1, respectively. With more neighbours, stronger structural similarities among inputs are obtained, and the neighbour loss may play a more crucial role during model training with larger multiplier values. It further validates the efficacy of the proposed NSL framework. The following experiments are conducted with lightweight models with these settings.       3 , where the 'Base Model' refers to the model trained solely on log Mel spectrograms. On both accuracy and UAR, NSL models almost outperform base models. Specifically, when n is 3 and α is 0.01, test UARs of ResNet-9 (76.3 %) and CNN-6 (77.0 %) significantly improve compared to their corresponding base models (p < 0.1 and p < 0.002 in one-tailed z-tests, respectively). Notably, for CNN-6, NSL substantially enhances almost all validation and test UARs, indicating significantly better performance. These results demonstrate NSL can help improve simple CNNs' performance via knowledge transfer. Comparing with classic transfer learning strategies, NSL models also perform better. Specifically, test UARs of VGG-15 and CNN-6 are significantly better than those of transfer learning models (p < 0.001 and p < 0.1 in onetailed z-tests, respectively). Figure  3",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "With graphs maintaining the structural similarities among input signals, NSL enables performance improvement of lightweight models. However, it should be noted that some of the results in Table  3  are not stable, such as the validation UARs of ResNet-9 with NSL. This instability may be caused by the latent data difference between RAVDESS and DEMoS. Specifically, the DEMoS dataset contains speech in Italian, whereas samples in RAVDESS are in English; furthermore, the emotion classes of the two datasets are also different. When compared to the SOTA works on DEMoS  [30] , our work's performance is comparable, which can be attributed to data split differences. However, we would like to emphasise that this work focuses on the performance improvement of lightweight models with the help of knowledge transfer by the NSL framework.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This paper proposed a neural structured learning (NSL) framework to transfer the knowledge learnt from the related speech emotion dataset to the target one by maintaining the structural similarities defined in graphs. Specifically, a pre-trained upstream wav2vec 2.0 model was fine-tuned on the RAVDESS emotional speech database for graph construction on DEMoS; further experiments on the DE-MoS emotional speech database with downstream lightweight models validated the effectiveness of our NSL framework.\n\nIn future work, a larger-scale source dataset (e. g., CMU-MOSEI  [31] ) can be applied as the source dataset and more target datasets can be also explored. Moreover, some domain adaptation strategies  [32]  can be explored to bridge the latent data difference between the source dataset and the target dataset.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , with the help of a model trained on a related",
      "page": 2
    },
    {
      "caption": "Figure 1: The framework of the applied NSL for SER. Graphs act",
      "page": 2
    },
    {
      "caption": "Figure 2: , we evaluate the performance of VGG-15 mod-",
      "page": 3
    },
    {
      "caption": "Figure 2: Speciﬁcally, by averaging and comparing the test UAR grouped by",
      "page": 3
    },
    {
      "caption": "Figure 2: Comparison of the performance (UAR [%]) of the VGG-15",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the class-wise analysis",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrix for the best VGG-15 under NSL frame-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#": "Speaker",
          "Train\nVal\nTest": "27\n25\n16",
          "(cid:80)": "68",
          "F / M": "23 /\n45"
        },
        {
          "#": "Anger\nDisgust\nFear\nGuilt\nHappiness\nSadness\nSurprise",
          "Train\nVal\nTest": "586\n531\n360\n666\n608\n404\n461\n404\n291\n453\n395\n281\n561\n471\n363\n606\n543\n381\n396\n358\n246",
          "(cid:80)": "1,477\n1,678\n1,156\n1,129\n1,395\n1,530\n1,000",
          "F / M": "400 /\n729\n596 / 1,082\n524 /\n871\n415 /\n741\n516 /\n961\n349 /\n651\n532 /\n998"
        },
        {
          "#": "(cid:80)",
          "Train\nVal\nTest": "3,729\n3,310\n2,326",
          "(cid:80)": "9,365",
          "F / M": "3,332 / 6,033"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NN": "",
          "# n": "",
          "α": "",
          "VGG-15": "Val",
          "ResNet-9": "Val",
          "CNN-6": "Val"
        },
        {
          "NN": "Base Model",
          "# n": "–",
          "α": "–",
          "VGG-15": "71.0 / 66.4",
          "ResNet-9": "79.4 / 73.9",
          "CNN-6": "68.4 / 65.3"
        },
        {
          "NN": "Transfer Learning (Add)",
          "# n": "–",
          "α": "–",
          "VGG-15": "70.9 / 66.7",
          "ResNet-9": "78.5 / 73.2",
          "CNN-6": "80.6 / 74.7"
        },
        {
          "NN": "Transfer Learning (Max)",
          "# n": "–",
          "α": "–",
          "VGG-15": "76.3 / 70.1",
          "ResNet-9": "80.3 / 74.9",
          "CNN-6": "74.2 / 67.8"
        },
        {
          "NN": "Transfer Learning (Avg)",
          "# n": "–",
          "α": "–",
          "VGG-15": "70.0 / 63.8",
          "ResNet-9": "82.0 / 76.8",
          "CNN-6": "80.5 / 74.8"
        },
        {
          "NN": "NSL",
          "# n": "3\n6\n9",
          "α": "0.01\n0.1\n0.1",
          "VGG-15": "80.3 / 73.6\n74.9 / 70.0\n75.4 / 69.5",
          "ResNet-9": "78.3 / 73.0\n74.5 / 67.8\n74.8 / 68.5",
          "CNN-6": "78.2 / 72.6\n80.7 / 74.9\n77.8 / 73.0"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "76\n75\n77.2\n76.6\n64\n75\n75.8\n78\n77\n71.4\n70.0\n65.7": "",
          "74\n74\n77\n75.7": "68\n78\n72.1\n71.0"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "Taiba Majid"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Mehmet Berkehan",
        "Akc ¸ay",
        "Kaya Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "Ruhul Amin"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "Siqing Wu",
        "H Tiago",
        "Wai-Yip Falk",
        "Chan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "6",
      "title": "Deep spectrum feature representations for speech emotion recognition",
      "authors": [
        "Ziping Zhao",
        "Yiqin Zhao",
        "Zhongtian Bao",
        "Haishuai Wang",
        "Zixing Zhang",
        "Chao Li"
      ],
      "year": "2018",
      "venue": "Proc. ASMMC-MMAC"
    },
    {
      "citation_id": "7",
      "title": "End-to-end triplet loss based emotion embedding system for speech emotion recognition",
      "authors": [
        "Puneet Kumar"
      ],
      "year": "2021",
      "venue": "Proc. ICPR"
    },
    {
      "citation_id": "8",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiehao Zhang",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP, Calgary, AB"
    },
    {
      "citation_id": "9",
      "title": "A novel end-to-end speech emotion recognition network with stacked transformer layers",
      "authors": [
        "Xianfeng Wang"
      ],
      "venue": "Proc. ICASSP, 2021"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "11",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Movement pruning: Adaptive sparsity by fine-tuning",
      "authors": [
        "Victor Sanh",
        "Thomas Wolf",
        "Alexander Rush"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Neural structured learning: Training neural networks with structured signals",
      "authors": [
        "Arjun Gopalan"
      ],
      "year": "2021",
      "venue": "Proc. WSDM"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition using speech and neural structured learning to facilitate edge intelligence",
      "authors": [
        "Zia Uddin",
        "Erik Nilsson"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "MLT-DNet: Speech emotion recognition using 1D dilated CNN based on multi-learning trick approach",
      "authors": [
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "16",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition with coattention based multi-level acoustic information",
      "authors": [
        "Heqing Zou"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "19",
      "title": "A survey on transfer learning",
      "authors": [
        "Jialin Sinno",
        "Qiang Pan",
        "Yang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "20",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Shahzad Younis",
        "Junaid Qadir",
        "Julien Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech, Hyderabad, India"
    },
    {
      "citation_id": "21",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "Mayank Sharma"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "23",
      "title": "Product quantization for nearest neighbor search",
      "authors": [
        "Herve Jégou",
        "Matthijs Douze",
        "Cordelia Schmid"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "25",
      "title": "DEMoS: An Italian emotional speech corpus",
      "authors": [
        "Emilia Parada-Cabaleiro",
        "Giovanni Costantini",
        "Anton Batliner",
        "Maximilian Schmitt",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "DEMoS: An Italian emotional speech corpus"
    },
    {
      "citation_id": "26",
      "title": "Enhancing transferability of black-box adversarial attacks via lifelong learning for speech emotion recognition models",
      "authors": [
        "Jing Zhao Ren",
        "Nicholas Han",
        "Björn Cummins",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "27",
      "title": "An improved convolutional neural network for speech emotion recognition",
      "authors": [
        "Ahmed Sibtain",
        "Butt"
      ],
      "year": "2022",
      "venue": "Proc. SCDM"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition by using complex mfcc and deep sequential model",
      "authors": [
        "Suprava Patnaik"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using time distributed 2D-Convolution layers for CAPSU-LENETS",
      "authors": [
        "Bhanusree Yalamanchili"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "30",
      "title": "Two-way feature extraction for speech emotion recognition using deep learning",
      "authors": [
        "Apeksha Aggarwal"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "31",
      "title": "Generating and protecting against adversarial attacks for deep speech-based emotion recognition models",
      "authors": [
        "Alice Zhao Ren",
        "Jing Baird",
        "Zixing Han",
        "Björn Zhang",
        "Schuller"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "32",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh"
      ],
      "year": "2018",
      "venue": "Proc. AAAI, New Orleans"
    },
    {
      "citation_id": "33",
      "title": "Learning emotion-discriminative and domain-invariant features for domain adaptation in speech emotion recognition",
      "authors": [
        "Qirong Mao",
        "Guopeng Xu",
        "Wentao Xue",
        "Jianping Gou",
        "Yongzhao Zhan"
      ],
      "year": "2017",
      "venue": "Speech Communication"
    }
  ]
}