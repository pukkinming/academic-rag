{
  "paper_id": "2408.12895v1",
  "title": "Ada2I: Enhancing Modality Balance For Multimodal Conversational Emotion Recognition",
  "published": "2024-08-23T07:59:51Z",
  "authors": [
    "Cam-Van Thi Nguyen",
    "The-Son Le",
    "Anh-Tuan Mai",
    "Duc-Trong Le"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Imbalance Modality",
    "Adaptive Feature Weighting",
    "Adaptive Modality Weighting",
    "Disparity ratio"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal learning task in exploiting various data modalities concurrently. Prior studies on effective multimodal ERC encounter challenges in addressing modality imbalances and optimizing learning across modalities. Dealing with these problems, we present a novel framework named Ada2I, which consists of two inseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive Modality Weighting (AMW) for feature-level and modality-level balancing respectively via leveraging both Inter-and Intra-modal interactions. Additionally, we introduce a refined disparity ratio as part of our training optimization strategy, a simple yet effective measure to assess the overall discrepancy of the model's learning process when handling multiple modalities simultaneously. Experimental results validate the effectiveness of Ada2I with state-ofthe-art performance compared to baselines on three benchmark datasets, particularly in addressing modality imbalances. \n CCS CONCEPTS ‚Ä¢ Information systems ‚Üí Sentiment analysis; ‚Ä¢ Computing methodologies ‚Üí Discourse, dialogue and pragmatics.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal learning is an approach to building models that can process and integrate information from multiple heterogeneous data modalities  [2, 20, 21] , including image, text, audio, video, and table. Since numerous tasks in the real world involve multiple modalities, multimodal learning has become increasingly important and attracted widespread attention as an effective way to accomplish these tasks. In recent years, the field of Emotion Recognition in Conversations (ERC) has witnessed a surge in effective models  [8, 27, 30] . Moving beyond unimodal recognition, the utilization of multimodal data offers a multidimensional perspective for more nuanced emotion discernment  [9, 19, 24] . Consequently, the incorporation of multimodal data is a natural evolution for enhancing emotion recognition in conversations. However, the widespread adoption of multimodal learning has revealed underlying challenges, with a primary focus on modality imbalances. These imbalances entail disparities in the contributions of individual modalities to the final decisionmaking process.\n\nAs illustrated in Figure . 1, the text modality quickly addresses the overall model performance and the joint logit scores, whereas the visual and audio modalities remain under-optimized throughout the training process. In addressing modality imbalance, diverse terminologies have emerged to characterize this phenomenon and explore its underlying causes. Terms such as \"greedy nature\"  [39] , \"modality collapse\"  [15] , and \"modality imbalance\"  [6, 22]  have been employed in various studies. These terms are associated with factors such as the \"suppression of dominant modalities\"  [26] , \"different convergence rates\"  [36] , \"diminishing modal marginal utility\"  [37] , or \"modality competition\"  [14] . In essence, two primary perspectives emerge regarding this problem  [37] : firstly, modalities exhibit varying levels of dominance, with models often overly reliant on a dominant modality with the highest convergence speed, thereby impeding the full utilization of other modalities with slower convergence speeds. Secondly, modal encoder optimization varies, necessitating the adoption of multiple strategies. Some approaches  [7, 26]  attempt to modulate the learning rates of different modalities based on the fusion modality. However, these approaches often overlook the impact of intra-modal data enhancement  [46] . For instance, right from the initial representations through the modal encoder, the outputs can lead to misleading final results, resulting in its weakened position across all modalities. Hence, from the outset, it is crucial to enhance representations for each modality, regardless of whether they are weak or strong, as it can affect the imbalance in learning across modalities.\n\nMoreover, current methodologies primarily focus on interactions between pairs of modalities  [6, 22, 26, 40] , resulting in complex computations and inadequate treatment across all modalities. These methods are commonly applied in tasks such as audio-visual learning  [26, 40]  and multimodal affective computing  [46] , often using datasets related to sarcasm detection, sentiment analysis, or humor detection. However, there is a lack of methods explicitly tailored for multimodal ERC tasks, especially for well-known multimodal datasets like IEMOCAP  [3] , MELD  [28] , and CMU-MOSEI  [1] . Additionally, in recent prominent studies  [17, 33] , while overall performance for multimodal ERC tasks has notably increased, a closer examination of the \"importance of modality\" reveals that pairwise modalities consistently fail to achieve satisfactory performance, creating a significant gap compared to leveraging all three modalities simultaneously. Therefore, it is crucial to simultaneously leverage learning from all modalities while also significantly enhancing the capabilities of weaker modalities to improve the overall learning performance of multimodal ERC models in practical applications.\n\nIn this paper, we propose a novel framework named Ada2I that addresses imbalances in learning across audio, text, and visual modalities for multimodal ERC. It consists of two primary modules including Adaptive Feature Weighting (AFW) and Adaptive Modality Weighting (AMW) for feature-level and modality-level balancing respectively in the consideration of Inter-and Intra-modal interactions. Focusing on feature-level balancing using Adaptive Feature Weighting (AFW), we apply tensor contraction to infer feature-aware attention weights for each modality, which aims to produce a feature-level balanced representation for each conversation. As an important component of AFW, Attention Mapping Network controls the balancing via maximizing the alignment between unimodal features and their corresponding attention coefficients. For modality-level balancing using Adaptive Modality Weighting (AMW), we further exploit feature-level balanced representations from the preceding AFW module to generate modality-level balanced ones through modality-wise normalization of features and learning weights before being used to enhance the emotion recognition. Additionally, we utilize the concept of disparity ratio, although with modifications compared to the study by Peng et al.  [26] , called OGM-GE, as a value to supervise the training process and evaluate the model. Specifically, while OGM-GE  [26]  introduced gradient modulation for pairs of modalities, we refine it to handle all three modalities simultaneously-textual, visual, and audio-in the Multimodal emotion recognition in conversation task. This adjustment reduces model complexity and overall processing time, leading to enhanced efficiency. To summarize, our contributions are as follows:\n\n‚Ä¢ We propose an end-to-end framework named Ada2I that addresses the issue of imbalance learning across modalities comprehensively for the multimodal ERC task. It not only considers modality-level imbalances but also leverages feature-level representations to contribute to the balancing step in the learning process.   [3] , MELD  [28] , and CMU-MOSEI  [1] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Multimodal Emotion Recognition",
      "text": "Multimodal Emotion Recognition (ERC) has emerged as a focal point within the affective computing community, garnering significant attention in recent years. The integration of multimodal data provides a multidimensional perspective, enabling a more nuanced understanding of emotions. Moreover, researchers have increasingly turned to multimodal fusion techniques, combining text, audio, and visual cues to enhance multimodal ERC performance  [9, 10, 16, 18, 24, 25] . ICON  [9]  employs two Gated Recurrent Units (GRUs) to capture speaker information, supplemented by global GRUs to track changes in emotional states throughout conversations. Similarly, MMGCN  [38]  utilizes Graph Convolutional Networks (GCNs) to capture contextual information, effectively leveraging multimodal dependencies and speaker information. On the other hand, Multilogue-Net  [31]  introduces a solution utilizing a context-aware RNN and employing pairwise attention as a fusion mechanism. TBJE  [4] , adopts a transformer-based architecture with modular co-attention to jointly encode multiple modalities. Additionally, COGMEN  [16]  is a multimodal context-based graph neural network that integrates both local (speaker information) and global (contextual information) aspects of conversation. Moreover, CORECT  [24]  employs relational temporal Graph Neural Networks (GNNs) with cross-modality interaction support, effectively capturing conversation-level interactions and utterance-level temporal relations. GraphMFT  [18]  utilizes multiple enhanced graph attention networks to capture intra-modal contextual information and intermodal complementary information. More recently, DF-ERC  [17]  emphasizes both feature disentanglement and fusion while taking into account both multimodalities and conversational contexts. Moreover, AdaIGN  [33]  employs the Gumbel Softmax trick to adaptively select nodes and edges, enhancing intra-and cross-modal interactions. While these methods primarily focus on designing model structures, they overlook the challenges posed by modality imbalance during multimodal learning.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Imbalanced Multimodal Learning",
      "text": "Despite the suggestion by  [13]  that integrating multiple modalities could enhance the accuracy of latent space estimations, thereby improving the efficacy of multimodal models, our investigation within the multimodal ERC task reveals a phenomenon contradicting this notion. The problem of modality imbalance persists as a significant challenge in multimodal learning frameworks involving low-quality data  [43] , particularly in tasks such as multimodal ERC. Conventional methods often prioritize one modality over others, assuming that certain types of sensory data are more relevant for a given task. For example, textual cues may receive greater emphasis, while visual or audio cues alone might be prioritized  [16, 24, 38] . Current methodologies for addressing imbalanced multimodal learning primarily focus on tasks such as audio-visual learning with a focus on optimizing pairwise modality learning  [6, 26, 40] , sentiment analysis, and sarcasm detection  [46] . However, these approaches often have task-specific limitations and framework restrictions, limiting their broader applicability. For instance, Wang et al.  [36]  identified that different modalities overfit and generalize at different rates, leading to suboptimal solutions when jointly trained using a unified optimization strategy. Peng et al.  [26]  proposed OGM-ME method where the better-performing modality dominates the gradient update, suppressing the learning process of other modalities. MMCosine  [40]  employs normalization techniques on features and weights to promote balanced and improved fine-grained learning across multiple modalities. Notably, there is a lack of specific approaches tailored for multimodal ERC apart from the work by Wang et al.  [37] . Recently, Wang et al.  [37]  observed a phenomenon referred to as \"diminishing modal marginal utility\" and proposed finegrained adaptive gradient modulation, which was applied to ERC, while I 2 MCL considers both data difficulty and modality balance for multimodal learning based on curriculum learning for affective computing, though not specifically for emotion recognition. To comprehensively address the challenge of modality imbalance in multimodal ERC, we propose an end-to-end model that ensures balance among text, audio, and visual modalities during training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology 3.1 Preliminary",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Tensor Ring Decomposition.",
      "text": "A tensor of order ùêæ (ùêæ dimensions) T ‚àà R ùëë 1 √óùëë 2 √ó...√óùëë ùêæ can be represented as a sequence of core tensors of order 3:\n\n, where the last core tensor has the form G ùêæ ‚àà R ùëë ùêæ √óùëü ùêæ √óùëü 1 . The dimensions ùëü 1 , ùëü 2 , ..., ùëü ùëò are called tensor ranks. In that case, T is represented in the form of a tensor ring ùëáùëü {G 1 , G 2 , ..., G ùëò } as follows:\n\nùëõ denotes the tensor contraction operation with mode-\n\n3.1.2 Problem Definition. In the context of a conversation ùê∂ with ùëÅ utterances {ùë¢ 1 , ùë¢ 2 , . . . , ùë¢ ùëÅ }, the task of Emotion Recognition in Conversations (ERC) is to predict the emotion label for each utterance in the conversation from a predefined emotion category set E. Each utterance is associated with ùëÄ modalities, i.e. textual (t), audio (a), and visual (v) modalities, represented as:\n\nwhere ùë¢ ùëñ ‚àà R ùëÄ √óùëë , ùëë signifies the dimension of modal features. For each modality ùëö, we derive multimodal features {X ùëö } ùëö‚àà {ùë°,ùëé,ùë£ } ‚àà R ùëë ùëö √óùëÅ for the conversation ùê∂. Here, {ùëë ùëö } ùëö‚àà {ùë°,ùëé,ùë£ } is the feature dimension of each modality.\n\nIn the following sub-section, we outline our proposed model Ada2I, including its main sub-modules: (1) Modality Encoder, (2) Adaptive Feature Weighting and (3) Adaptive Modality Weighting. We also refine the disparity ratio metric as part of our Training Optimization Strategy. Figure  2  illustrates architecture of Ada2I.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Modality Encoder",
      "text": "Given a conversation ùê∂, a Transformer  [34]  network is utilized as the encoder to generate a unimodal representation Z ùëö ‚àà R ùëÅ √óùëë ùëö respecting to the modality ùëö as:\n\nwhere the function ùúô (ùúÉ (ùëö) ) is the Transformer network with learnable parameter ùúÉ (ùëö) .\n\n3.3 Adaptive Feature Weighting (AFW) We employ a tensor-ring-based generation function to retrieve the multi-interaction multimodal query tensor Q and key tensor K from the input modality presentations Z ùëö . Specifically, we compute Q and K as follows:  and G ùëö ùêæ for each modality. To perform multimodal attention in the tensor space, we need to compute the attention coefficient matrix, Œò, from the tensorized input. To achive this, we can first compute the Tensor-ring Key representation and Tensor-ring Query representation of input data, G ùëö ùëÑ ‚àà R ùëë ùëö √óùëü ùë† √óùëü ùë§ and G ùëö ùêæ ‚àà R ùëë ùëö √óùëü ùë† √óùëü ùë§ , where ùëö ‚àà {ùë°, ùëé, ùë£ }, the index ùë†, ùë§ ‚àà {1, 2, 3}, and ùë† ‚â† ùë§. The attention coefficient matrix Œò of modality ùëö is formulated as follows:\n\nwhere ‚äô denotes the element-wise product, ‚àöÔ∏Å ùëë ùëò is a scaling factor. More specifically, the modality ùëö core tensor G ùêæ and G ùëÑ are computed using a Linear Transform (Figure  3  ), as expressed below:\n\nwhere ùëö ‚àà {ùë°, ùëé, ùë£ }, ùëä\n\nùêæ ùëö ‚àà R ùëë ùëö √óùëü ùë§ are the linear transformation matrix; ‚äó 1 denotes the mode-1 Khatri-Rao product.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Adaptive Feature Weighting (Afw)",
      "text": ". This module addresses the varying impact of each modality on inter-modality and intramodality interactions using attention mechanism. First, we calculate the attention pooling matrices A (ùëö) ‚àà R ùëü ùë† √óùëü ùë§ by averaging Œò (ùëö) across the modality dimension ùëë ùëö , ùëö ‚àà {ùë°, ùëé, ùë£ }. Inspired by MMT  [32] , the feature-aware attention matrix ùê¥ùë°ùë° ùëö ‚àà R ùëÅ √óùëë ùëö for a given modality ùëö is computed as follows:\n\nwhere √ó 1 3 is the ùëöùëúùëëùëí -(  1 3 ) tensor contraction. The feature-aware balanced representation Z ùëì -ùëéùëëùëéùëùùë° ùëö ‚àà R ùëÅ √óùëë ùëö of the conversation C for a given modality m is computed as:\n\nwhere ùõΩ ‚àà [0, 1] is a balancing parameter to regulate the contribution of the original unimodal feature vector Z ùëö .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Adaptive Modality Weighting (Amw)",
      "text": "Our key focus is to achieve balanced contributions from each modality during the training. Similar to  [40] , we observe the imbalance problem in multimodal ERC through experiments analyzing the modality-wise weight in norm of each label during training. Apparently, the dominant unimodal encoder, e.g., text, tends to have its weight in norm increase much faster than the weaker modalities, i.e., audio and visual, leading to divergent unimodal logit scores and distorting the joint fusion representation. Inspired by  [35, 45] , we propose to incorporate modality-wise L2 normalization to properly weight features, mitigating imbalances arising from differing data distributions and noise levels across modalities. This dynamic adjustment prevents any single modality from dominating the fusion process, thus enhancing overall performance. Therefore, the modality-level balanced representation Z ùëö-ùëéùëëùëéùëùùë° of the given conversation is calculated as follows:\n\nwhere ùëä ùëö ‚àà R ùëë ùëö √ó | E | symbolizes the output matrix of the model pertaining to modality ùëö, and E is the set of emotion classes.\n\nFor emotion recognition, we feed Z ùëö-ùëéùëëùëéùëùùë° , into the mulilayer preceptron (MLP) with ReLU activation function to compute the\n\nThe output ≈∑ùëñ is utilized to predict emotion labels.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Learning",
      "text": "First, we investigate the standard cross-entropy loss for this downstream task, i.e., mutilmodal ERC as:\n\nwhere ùêµ is the batch size. Second, in order to align between the original unimodal representation of modality ùëö and its respective feature-aware attention weights as Eq (6), we employ Attention Mapping Network as follows:\n\n√Ç\n\nwhere Œ¶ ùëö (‚Ä¢) is a feed-forward neural network with the parameter ùúì (ùëö) , √Ç ùë°ùë° ùëö ‚àà R ùëÅ √óùëë ùëö is the feature-aware self-attention weights of the modality ùëö. To enhance feature-level balance across all modalities, we introduce a L1-norm loss L ùëì ùëíùëéùë°ùë¢ùëüùëí as:\n\nAdditionally, we also consider the modality-level balance loss L ùëöùëúùëëùëéùëô , which is computed as:\n\nwhere Z",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "ùëö-ùëéùëëùëéùëùùë° ùëó",
      "text": "represents the output of the ùëó-th class for the ùëñ-th sample. Finally, we combine the all loss functions into a joint objective function, which is used to optimize all trainable parameters in an end-to-end manner:\n\nRecent studies have brought attention to the challenge of handling imbalanced optimization in joint learning models, particularly when dealing with multiple modalities. Peng et al.  [26]  introduce the OGM-GE method to address optimization imbalances encountered during the simultaneous training of dual-modal systems, i.e., visual and audio. However, directly applying the OGM-GE method to our framework is not practical as it only deals with two modalities. In contrast, our framework caters to more than two modalities across different domains, specifically tailored for the multimodal ERC task. Therefore, leanrable parameter of encoder layer is optimized during training process as the following strategy:\n\nwhere ƒù(ùúÉ ùëö ùë°) =\n\nùë° ) denotes the gradient with respect to ùêµ ùë° .\n\nWe adjust the balance of modalities through gradient parameter adjustments. For each output at step ùë°, we compute the discrepancy ratio for each modality using the softmax of the cosine similarity between the output weights and the corresponding feature vectors:\n\nwhere I ùëò=ùë¶ ùëó equals 1 if ùëò = ùë¶ ùëó and 0 otherwise, and softmax(.) estimates the unimodal performance of the multimodal model, ùëÄ denotes the count of modalities. Specifically, for the multimodal ERC task under consideration, we delineate three modalities: text (ùë°), audio (ùëé), and visual (ùë£). The discrepancy ratio is calculated as:\n\nThe learnable parameters are updated according to:\n\nwhere the modulation coefficient ùëò ùëö ùë° is determined by 1tanh(ùõº ‚Ä¢ ùúå ùëö ùë° ) if ùúå ùëö ùë° > 1, and 1 otherwise. Here, ùõº is a hyperparameter controlling the degree of modulation. Additionally, to enhance the adaptability of the modulation process, Gaussian noise ‚Ñé(ùúÉ (ùëñ ) ùë° ) sampled from a distribution N (0, ùë†ùëîùëë (ùúÉ (ùëñ ) ùë° )) is introduced after parameter updates:\n\nTraining Optimization Strategy The training process of Ada2I is illustrated in Algorithm 1.\n\nùë° ) end for end for",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "Datasets: We consider three benchmark datasets for multimodal ERC namely: IEMOCAP  [3] , MELD  [28] , and CMU-MOSEI  [1] . The dataset statistics are illustrated in Table  1 . IEMOCAP. This dataset comprises 12 hours of video recordings of dyadic conversations involving 10 speakers. It includes 151 dialogues, segmented into 7,433 utterances, each annotated with one of six emotion labels: happy, sad, neutral, angry, excited, or frustrated.\n\nMELD. This dataset is based on the TV series Friends, includes 13,709 video clips featuring multi-party conversations, each labeled with one of Ekman's six universal emotions: joy, sadness, fear, anger, surprise, and disgust.\n\nCMU-MOSEI:. This dataset is a prominent resource for sentiment and emotion analysis, comprises 3,228 YouTube videos divided into 23,453 segments, featuring contributions from 1,000 speakers covering 250 topics. It includes six emotion categories: happy, sad, angry, scared, disgusted, and surprised, with sentiment intensity ranging from -3 to 3.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimential Setup 4.1 Baselines And Evaluation Metrics",
      "text": "Baselines: Ada2I is compared against several state-of-the-art (SOTA) baseline approaches for evaluating performance in multimodal ERC, particularly addressing modality imbalance problems. For the IEMOCAP and MELD datasets, we consider baseline models such as DialogueRNN  [23] , DialogueGCN  [8] , MMGCN  [38] , BiD-DIN  [41] , and MM-DFN  [11] . We report the best results obtained from  [37] , which enhanced these models to address modality imbalance. Additionally, we consider other SOTA models for multimodal ERC that do not explicitly address modality imbalance, including COGMEN  [16] , CORECT  [24] , GraphMFT  [18] , DF-ERC  [17] , and AdaIGN  [33] .\n\nFor the CMU-MOSEI dataset, we evaluated various baseline models for sentiment classification tasks, which include both 2-class sentiment, featuring only positive and negative sentiment, and 7-class sentiment, ranging from highly negative (-3) to highly positive (+3). These baseline models include Multilouge-Net  [31] , TBJE  [4] , COG-MEN  [16] , CORECT  [24] , OGM-GE  [26] , and I 2  MCL  [46] . Notably, OGM-GE and I 2 MCL specifically address the issue of imbalanced modalities in multimodal ERC, whereas the others do not.\n\nEvaluation Metrics: Similar to prior studies  [23, 37, 38] , we evaluate the effectiveness of emotion recognition using Accuracy (Acc) and Weighted F1 Score (W-F1) as our primary evalucation metrics.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Settings",
      "text": "We derive multimodal features for each utterance from acoustic, lexical, and visual modalities using a combination of models and pre-trained models, as outlined in Table  2 .\n\nWe employ PyTorch 3  for training our architecture and Comet 4  for logging all experiments, leveraging its Bayesian optimizer for hyperparameter tuning. Additional parameters can be found in Table  2 . We also compare Ada2I with SOTA baseline models for multimodal ERC, particularly those focusing solely on multimodal fusion and architectural design without addressing modality imbalance. Figure  4b  demonstrates that our proposed Ada2I significantly reduces the performance gap in WF1 between learning from all three modalities simultaneously (T+A+V) and pair-wise modality combinations on the MELD dataset. Most notably, with the weaker modality pair (audio+visual) consistently lagging behind in performance compared to the full modality combination (i.e., with AdaIGN, this gap is 23.12%), Ada2I boosts the model and shortens the gap to only 5.22%. Similarly, with the text+audio (T+A) and text+visual (T+V) pairs, this gap is also substantially reduced, indicating that the model has learned in a more balanced manner, leveraging additional useful information from non-dominant modalities. The significant improvement is similarly observed on the IEMOCAP dataset in Figure  4a .\n\nCMU-MOSEI dataset: Table  4  shows that Ada2I outperforms all baseline models. Specifically, when compared to OGM-GE and I 2 MCL, two models proposed for addressing modality imbalance during training, Ada2I demonstrates superior performance across all modality combinations. When compared to other baseline models that do not consider modality balancing, Ada2I also demonstrates significant balancing capabilities, reducing the performance gap between modality pairs. For instance, in the CORECT model, the gap between T+A+V and A+V is 15.09% for 2-class sentiment, and this figure increases to 21.76% for 7-class sentiment. However, with Ada2I, these gaps are significantly reduced to 10.32% and 13.07%, respectively, underscoring the effectiveness of Ada2I in addressing modality imbalances.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "5.2.1 Balancing Interpretation. We conduct ablation studies with the two main modules of the model, AMW and AFW, to assess their impact on the Ada2I model. Additionally, through the Discrepancy Ratio, we interpret the model's balancing by observing its changes. A smaller Discrepancy Ratio indicates a more balanced optimization process. Figure  5  shows that the discrepancy ratios ùúå ùë° , ùúå ùë£ , and ùúå ùëé significantly decrease when both AMW and AFW are combined within Ada2I, with all ratios approaching approximately 1 on the IEMOCAP dataset. In contrast, when one of the modules is ablated, the ratios for audio (ùúå ùëé ) and visual (ùúå ùë£ ) are approximately 1.5, while for text, it increases to around 3. Similarly, on the MELD dataset, our proposed model Ada2I has reduced this discrepancy  ratio of text from over 4 (w/o AFW) to approximately half, reaching around 2, while for audio and visual, it brings them close to the 1 mark. In summary, the combined design of both modules AMW and AFW enhances balanced learning across modalities during training, highlighting the significance and inseparability of feature-level and modality-level balancing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Weight Normalization.",
      "text": "As mentioned earlier, the unimodal weights also directly influence the encoder updating process. The imbalanced weight components induce gradients and subsequently lead to the inconsistent convergence of unimodalities.\n\nHere, we provide a clearer visualization of these unimodal weights before imbalance processing (Only Encoder) and in the Ada2I model in Figure  6  for the IEMOCAP dataset. It is evident that with Only Encoder, the text encoder (dominant modality) weight in norm grows much faster than audio and visual. After balancing, our model exhibits a more balanced optimization process.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effect Of Module.",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we present Ada2I, a framework designed to address modality imbalances and optimize learning in multimodal ERC. We identify and analyze existing issues in current ERC models that overlook the imbalance problem. From there, we propose a solution comprising integral modules: Adaptive Feature Weighting (AFW) and Adaptive Modality Weighting (AMW). The former enhances intra-modal representations for feature-level balancing, while the latter optimizes inter-modal learning weights with the balancing at modality level. Furthermore, we introduce a refined disparity ratio to optimize training, offering a straightforward yet effective measure to evaluate the model's overall discrepancy when handling multiple modalities simultaneously. Extensive experiments on the IEMOCAP, MELD, and CMU-MOSEI datasets validate its effectiveness, showcasing SOTA performance. In the future, we anticipate enhancing the efficiency of the framework and maximizing the utilization of emotional cues.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Weighted F1 scores for the multimodal set-",
      "page": 1
    },
    {
      "caption": "Figure 2: illustrates architecture of Ada2I.",
      "page": 3
    },
    {
      "caption": "Figure 2: Illustration of Ada2I framework",
      "page": 4
    },
    {
      "caption": "Figure 3: ), as expressed below:",
      "page": 4
    },
    {
      "caption": "Figure 3: Linear Transform block to compute core tensor.",
      "page": 5
    },
    {
      "caption": "Figure 4: b demonstrates that our proposed Ada2I significantly re-",
      "page": 7
    },
    {
      "caption": "Figure 4: Performance gap visualizations between the multi-",
      "page": 7
    },
    {
      "caption": "Figure 5: shows that the discrepancy ratios ùúåùë°, ùúåùë£, and ùúåùëé",
      "page": 7
    },
    {
      "caption": "Figure 5: The change of the discrepancy ratio ùúåùë°, ùúåùëé, ùúåùë£on the",
      "page": 8
    },
    {
      "caption": "Figure 6: for the IEMOCAP dataset. It is evident that with Only",
      "page": 8
    },
    {
      "caption": "Figure 6: Modality-wise weights of each label normalized for",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VNU University of Engineering and Technology": "Hanoi, Vietnam"
        },
        {
          "VNU University of Engineering and Technology": "trongld@vnu.edu.vn"
        },
        {
          "VNU University of Engineering and Technology": "\u0000,\u0000(\u00000\u00002\u0000&\u0000$\u00003\n\u00000\u0000(\u0000/\u0000'\n\u0000\u0013\u0000\u0011\u0000\u001a\n\u0000\u0013\u0000\u0011\u0000\u0019"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0019\n\u00007\u0000\u000e\u0000$\u0000\u000e\u00009"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0018"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0018\n\u00007\u0000\u0003\u0000L\u0000Q\u0000\u0003\u00007\u0000\u000e\u0000$\u0000\u000e\u00009"
        },
        {
          "VNU University of Engineering and Technology": "\u0000)\u0000\u0014\u0000\u0003\u0000\u000b\u0000\b\u0000\f\n\u0000)\u0000\u0014\u0000\u0003\u0000\u000b\u0000\b\u0000\f\n\u0000$\u0000\u0003\u0000L\u0000Q\u0000\u0003\u00007\u0000\u000e\u0000$\u0000\u000e\u00009"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0017\n\u00009\u0000\u0003\u0000L\u0000Q\u0000\u0003\u00007\u0000\u000e\u0000$\u0000\u000e\u00009"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0017"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0016"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0015"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\u0000\u0011\u0000\u0016"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\n\u0000\u0014\u0000\u0013\n\u0000\u0015\u0000\u0013\n\u0000\u0016\u0000\u0013\n\u0000\u0017\u0000\u0013\n\u0000\u0018\u0000\u0013\n\u0000\u0013\n\u0000\u0014\u0000\u0013\n\u0000\u0015\u0000\u0013\n\u0000\u0016\u0000\u0013\n\u0000\u0017\u0000\u0013\n\u0000\u0018\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000(\u0000S\u0000R\u0000F\u0000K\n\u0000(\u0000S\u0000R\u0000F\u0000K"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u000b\u0000D\u0000"
        },
        {
          "VNU University of Engineering and Technology": "\u0000,\u0000(\u00000\u00002\u0000&\u0000$\u00003\n\u00000\u0000(\u0000/\u0000'"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0017\u0000\u0013\u0000\u0013\n\u0000\u0014\u0000\u0013\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u001b\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0016\u0000\u0013\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000/\u0000R\u0000J\u0000L\u0000W\u0000V\n\u0000/\u0000R\u0000J\u0000L\u0000W\u0000V\n\u00007\u0000\u0003\u0000L\u0000Q\u0000\u0003\u00007\u0000\u000e\u0000$\u0000\u000e\u00009"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0019\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000$\u0000\u0003\u0000L\u0000Q\u0000\u0003\u00007\u0000\u000e\u0000$\u0000\u000e\u00009"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0015\u0000\u0013\u0000\u0013\n\u00009\u0000\u0003\u0000L\u0000Q\u0000\u0003\u00007\u0000\u000e\u0000$\u0000\u000e\u00009\n\u0000\u0017\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0015\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0014\u0000\u0013\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u0013\n\u0000\u0015\u0000\u0013\u0000\u0013\n\u0000\u0017\u0000\u0013\u0000\u0013\n\u0000\u0013\n\u0000\u0014\u0000\u0013\u0000\u0013\u0000\u0013\n\u0000\u0015\u0000\u0013\u0000\u0013\u0000\u0013\n\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\n\u0000\u0017\u0000\u0013\u0000\u0013\u0000\u0013\n\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0013"
        },
        {
          "VNU University of Engineering and Technology": "\u0000,\u0000W\u0000H\u0000U\u0000V\n\u0000,\u0000W\u0000H\u0000U\u0000V"
        },
        {
          "VNU University of Engineering and Technology": "\u0000\u000b\u0000E\u0000"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "Figure 1:\n(a) Weighted F1 scores for the multimodal set-"
        },
        {
          "VNU University of Engineering and Technology": "ting (T+A+V) compared with each unimodal encoder, and"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "(b) batch-average unimodal-logit scores."
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "1\nINTRODUCTION"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "Multimodal learning is an approach to building models that can pro-"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "cess and integrate information from multiple heterogeneous data"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "modalities [2, 20, 21], including image, text, audio, video, and table."
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "Since numerous tasks in the real world involve multiple modalities,"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "multimodal\nlearning has become increasingly important and at-"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "tracted widespread attention as an effective way to accomplish these"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "tasks. In recent years, the field of Emotion Recognition in Conver-"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "sations (ERC) has witnessed a surge in effective models [8, 27, 30]."
        },
        {
          "VNU University of Engineering and Technology": "Moving beyond unimodal recognition, the utilization of multimodal"
        },
        {
          "VNU University of Engineering and Technology": "data offers a multidimensional perspective for more nuanced emo-"
        },
        {
          "VNU University of Engineering and Technology": "tion discernment [9, 19, 24]. Consequently, the incorporation of"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "multimodal data is a natural evolution for enhancing emotion recog-"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "nition in conversations. However, the widespread adoption of multi-"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "modal learning has revealed underlying challenges, with a primary"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "focus on modality imbalances. These imbalances entail disparities"
        },
        {
          "VNU University of Engineering and Technology": "in the contributions of individual modalities to the final decision-"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "making process."
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "As illustrated in Figure. 1, the text modality quickly addresses"
        },
        {
          "VNU University of Engineering and Technology": ""
        },
        {
          "VNU University of Engineering and Technology": "the overall model performance and the joint logit scores, whereas"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "varying levels of dominance, with models often overly reliant on a",
          "modal emotion recognition in conversation task. This adjustment": "reduces model complexity and overall processing time, leading to"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "dominant modality with the highest convergence speed, thereby",
          "modal emotion recognition in conversation task. This adjustment": "enhanced efficiency. To summarize, our contributions are as follows:"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "impeding the full utilization of other modalities with slower conver-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "‚Ä¢ We propose an end-to-end framework named Ada2I that"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "gence speeds. Secondly, modal encoder optimization varies, necessi-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "addresses the issue of\nimbalance learning across modali-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "tating the adoption of multiple strategies. Some approaches [7, 26]",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "ties comprehensively for the multimodal ERC task.\nIt not"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "attempt to modulate the learning rates of different modalities based",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "only considers modality-level imbalances but also leverages"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "on the fusion modality. However, these approaches often overlook",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "feature-level representations to contribute to the balancing"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "the impact of intra-modal data enhancement [46]. For instance, right",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "step in the learning process."
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "from the initial representations through the modal encoder, the out-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "‚Ä¢ With two modules intricately designed yet inseparable, Adap-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "puts can lead to misleading final results, resulting in its weakened",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "tive Feature Weighting (AFW) is crafted to enhance the rep-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "position across all modalities. Hence, from the outset, it is crucial to",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "resentation of each conversation at the feature level, while"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "enhance representations for each modality, regardless of whether",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "Adaptive Modality Weighting (AMW) is proposed to opti-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "they are weak or strong, as it can affect the imbalance in learning",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "mize the modality-level\nlearning weights during training."
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "across modalities.",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "Additionally, we redefine the disparity ratio, a simple yet"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "Moreover, current methodologies primarily focus on interactions",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "effective measure, to assess the overall discrepancy of the"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "between pairs of modalities [6, 22, 26, 40], resulting in complex com-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "model‚Äôs learning process when simultaneously handling mul-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "putations and inadequate treatment across all modalities. These",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "tiple modalities, rather than just two as in the original ap-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "methods are commonly applied in tasks such as audio-visual learn-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "proach from Peng et al. [26]."
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "ing [26, 40] and multimodal affective computing [46], often using",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "‚Ä¢ Our empirical experiments illustrate the effectiveness and"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "datasets related to sarcasm detection, sentiment analysis, or humor",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "enhancements of Ada2I in comparison to existing state-of-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "detection. However, there is a lack of methods explicitly tailored",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "the-art approaches dealing with modality imbalance across"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "for multimodal ERC tasks, especially for well-known multimodal",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "three prevalent multimodal ERC datasets including IEMO-"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "datasets like IEMOCAP [3], MELD [28], and CMU-MOSEI [1]. Ad-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "CAP [3], MELD [28], and CMU-MOSEI [1]."
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "ditionally, in recent prominent studies [17, 33], while overall per-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "formance for multimodal ERC tasks has notably increased, a closer",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "2\nRELATED WORK"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "examination of the ‚Äúimportance of modality‚Äù reveals that pairwise",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "",
          "modal emotion recognition in conversation task. This adjustment": "2.1\nMultimodal Emotion Recognition"
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "modalities consistently fail to achieve satisfactory performance, cre-",
          "modal emotion recognition in conversation task. This adjustment": ""
        },
        {
          "tives emerge regarding this problem [37]: firstly, modalities exhibit": "ating a significant gap compared to leveraging all three modalities",
          "modal emotion recognition in conversation task. This adjustment": "Multimodal Emotion Recognition (ERC) has emerged as a focal"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": "the visual and audio modalities remain under-optimized through-",
          "Cam-Van Thi Nguyen, et al.": "from the preceding AFW module to generate modality-level bal-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "out the training process. In addressing modality imbalance, diverse",
          "Cam-Van Thi Nguyen, et al.": "anced ones through modality-wise normalization of features and"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "terminologies have emerged to characterize this phenomenon and",
          "Cam-Van Thi Nguyen, et al.": "learning weights before being used to enhance the emotion recogni-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "explore its underlying causes. Terms such as ‚Äúgreedy nature‚Äù [39],",
          "Cam-Van Thi Nguyen, et al.": "tion. Additionally, we utilize the concept of disparity ratio, although"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "‚Äúmodality collapse‚Äù [15], and ‚Äúmodality imbalance‚Äù [6, 22] have been",
          "Cam-Van Thi Nguyen, et al.": "with modifications compared to the study by Peng et al. [26], called"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "employed in various studies. These terms are associated with factors",
          "Cam-Van Thi Nguyen, et al.": "OGM-GE, as a value to supervise the training process and evaluate"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "such as the ‚Äúsuppression of dominant modalities‚Äù [26], ‚Äúdifferent",
          "Cam-Van Thi Nguyen, et al.": "the model. Specifically, while OGM-GE [26] introduced gradient"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "convergence rates‚Äù [36], ‚Äúdiminishing modal marginal utility‚Äù [37],",
          "Cam-Van Thi Nguyen, et al.": "modulation for pairs of modalities, we refine it to handle all three"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "or ‚Äúmodality competition‚Äù [14]. In essence, two primary perspec-",
          "Cam-Van Thi Nguyen, et al.": "modalities simultaneously‚Äîtextual, visual, and audio‚Äîin the Multi-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "tives emerge regarding this problem [37]: firstly, modalities exhibit",
          "Cam-Van Thi Nguyen, et al.": "modal emotion recognition in conversation task. This adjustment"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "varying levels of dominance, with models often overly reliant on a",
          "Cam-Van Thi Nguyen, et al.": "reduces model complexity and overall processing time, leading to"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "dominant modality with the highest convergence speed, thereby",
          "Cam-Van Thi Nguyen, et al.": "enhanced efficiency. To summarize, our contributions are as follows:"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "impeding the full utilization of other modalities with slower conver-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "‚Ä¢ We propose an end-to-end framework named Ada2I that"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "gence speeds. Secondly, modal encoder optimization varies, necessi-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "addresses the issue of\nimbalance learning across modali-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "tating the adoption of multiple strategies. Some approaches [7, 26]",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "ties comprehensively for the multimodal ERC task.\nIt not"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "attempt to modulate the learning rates of different modalities based",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "only considers modality-level imbalances but also leverages"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "on the fusion modality. However, these approaches often overlook",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "feature-level representations to contribute to the balancing"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the impact of intra-modal data enhancement [46]. For instance, right",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "step in the learning process."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "from the initial representations through the modal encoder, the out-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "‚Ä¢ With two modules intricately designed yet inseparable, Adap-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "puts can lead to misleading final results, resulting in its weakened",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "tive Feature Weighting (AFW) is crafted to enhance the rep-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "position across all modalities. Hence, from the outset, it is crucial to",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "resentation of each conversation at the feature level, while"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "enhance representations for each modality, regardless of whether",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "Adaptive Modality Weighting (AMW) is proposed to opti-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "they are weak or strong, as it can affect the imbalance in learning",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "mize the modality-level\nlearning weights during training."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "across modalities.",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "Additionally, we redefine the disparity ratio, a simple yet"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Moreover, current methodologies primarily focus on interactions",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "effective measure, to assess the overall discrepancy of the"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "between pairs of modalities [6, 22, 26, 40], resulting in complex com-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "model‚Äôs learning process when simultaneously handling mul-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "putations and inadequate treatment across all modalities. These",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "tiple modalities, rather than just two as in the original ap-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "methods are commonly applied in tasks such as audio-visual learn-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "proach from Peng et al. [26]."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ing [26, 40] and multimodal affective computing [46], often using",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "‚Ä¢ Our empirical experiments illustrate the effectiveness and"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "datasets related to sarcasm detection, sentiment analysis, or humor",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "enhancements of Ada2I in comparison to existing state-of-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "detection. However, there is a lack of methods explicitly tailored",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "the-art approaches dealing with modality imbalance across"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "for multimodal ERC tasks, especially for well-known multimodal",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "three prevalent multimodal ERC datasets including IEMO-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "datasets like IEMOCAP [3], MELD [28], and CMU-MOSEI [1]. Ad-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "CAP [3], MELD [28], and CMU-MOSEI [1]."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ditionally, in recent prominent studies [17, 33], while overall per-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "formance for multimodal ERC tasks has notably increased, a closer",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "2\nRELATED WORK"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "examination of the ‚Äúimportance of modality‚Äù reveals that pairwise",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "2.1\nMultimodal Emotion Recognition"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "modalities consistently fail to achieve satisfactory performance, cre-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ating a significant gap compared to leveraging all three modalities",
          "Cam-Van Thi Nguyen, et al.": "Multimodal Emotion Recognition (ERC) has emerged as a focal"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "simultaneously. Therefore, it is crucial to simultaneously leverage",
          "Cam-Van Thi Nguyen, et al.": "point within the affective computing community, garnering sig-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "learning from all modalities while also significantly enhancing the",
          "Cam-Van Thi Nguyen, et al.": "nificant attention in recent years. The integration of multimodal"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "capabilities of weaker modalities to improve the overall learning",
          "Cam-Van Thi Nguyen, et al.": "data provides a multidimensional perspective, enabling a more"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "performance of multimodal ERC models in practical applications.",
          "Cam-Van Thi Nguyen, et al.": "nuanced understanding of emotions. Moreover, researchers have"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "In this paper, we propose a novel framework named Ada2I that",
          "Cam-Van Thi Nguyen, et al.": "increasingly turned to multimodal fusion techniques, combining"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "addresses imbalances in learning across audio,\ntext, and visual",
          "Cam-Van Thi Nguyen, et al.": "text, audio, and visual cues to enhance multimodal ERC perfor-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "modalities for multimodal ERC. It consists of two primary mod-",
          "Cam-Van Thi Nguyen, et al.": "mance [9, 10, 16, 18, 24, 25]. ICON [9] employs two Gated Recur-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ules including Adaptive Feature Weighting (AFW) and Adaptive",
          "Cam-Van Thi Nguyen, et al.": "rent Units (GRUs) to capture speaker information, supplemented by"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Modality Weighting (AMW) for feature-level and modality-level bal-",
          "Cam-Van Thi Nguyen, et al.": "global GRUs to track changes in emotional states throughout con-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ancing respectively in the consideration of Inter- and Intra-modal",
          "Cam-Van Thi Nguyen, et al.": "versations. Similarly, MMGCN [38] utilizes Graph Convolutional"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "interactions. Focusing on feature-level balancing using Adaptive",
          "Cam-Van Thi Nguyen, et al.": "Networks (GCNs) to capture contextual\ninformation, effectively"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Feature Weighting (AFW), we apply tensor contraction to infer",
          "Cam-Van Thi Nguyen, et al.": "leveraging multimodal dependencies and speaker information. On"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "feature-aware attention weights for each modality, which aims to",
          "Cam-Van Thi Nguyen, et al.": "the other hand, Multilogue-Net [31] introduces a solution utilizing"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "produce a feature-level balanced representation for each conversa-",
          "Cam-Van Thi Nguyen, et al.": "a context-aware RNN and employing pairwise attention as a fu-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "tion. As an important component of AFW, Attention Mapping Net-",
          "Cam-Van Thi Nguyen, et al.": "sion mechanism. TBJE [4], adopts a transformer-based architecture"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "work controls the balancing via maximizing the alignment between",
          "Cam-Van Thi Nguyen, et al.": "with modular co-attention to jointly encode multiple modalities."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "unimodal features and their corresponding attention coefficients.",
          "Cam-Van Thi Nguyen, et al.": "Additionally, COGMEN [16] is a multimodal context-based graph"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "For modality-level balancing using Adaptive Modality Weighting",
          "Cam-Van Thi Nguyen, et al.": "neural network that integrates both local (speaker information) and"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "(AMW), we further exploit feature-level balanced representations",
          "Cam-Van Thi Nguyen, et al.": "global (contextual information) aspects of conversation. Moreover,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "CORECT [24] employs relational temporal Graph Neural Networks",
          "ACM MM, 2024, Melbourne, Australia": "the form Gùêæ ‚àà Rùëëùêæ √óùëüùêæ √óùëü1 . The dimensions ùëü1, ùëü2, ..., ùëüùëò are called"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "(GNNs) with cross-modality interaction support, effectively captur-",
          "ACM MM, 2024, Melbourne, Australia": "tensor ranks. In that case, T is represented in the form of a tensor"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "... √ó2\nring ùëá ùëü {G1, G2, ..., Gùëò } as follows: T = G1 √ó2"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ing conversation-level interactions and utterance-level temporal re-",
          "ACM MM, 2024, Melbourne, Australia": "Gùëò . In\n3 G2 √ó2\nùëò+2"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "lations. GraphMFT [18] utilizes multiple enhanced graph attention",
          "ACM MM, 2024, Melbourne, Australia": "which √óùëö\ndenotes the tensor contraction operation with mode-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "(ùëö\n‚àà Rùëë2 √óùëü2 √óùëü3"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "networks to capture intra-modal contextual information and inter-",
          "ACM MM, 2024, Melbourne, Australia": "and\nùëõ ). For example, with G1\n‚àà Rùëë1 √óùëü1 √óùëü2 , G2"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "modal complementary information. More recently, DF-ERC [17]",
          "ACM MM, 2024, Melbourne, Australia": "G3 ‚àà Rùëë3 √óùëü3 √óùëü1 , T is represented as G1 √ó2\n3 G2 √ó2\n4 G3 ‚àà Rùëë1 √óùëë2 √óùëë3 ."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "emphasizes both feature disentanglement and fusion while tak-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ing into account both multimodalities and conversational contexts.",
          "ACM MM, 2024, Melbourne, Australia": "In the context of a conversation ùê∂ with\n3.1.2\nProblem Definition."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Moreover, AdaIGN [33] employs the Gumbel Softmax trick to adap-",
          "ACM MM, 2024, Melbourne, Australia": "the task of Emotion Recognition\nùëÅ utterances {ùë¢1, ùë¢2, . . . , ùë¢ùëÅ },"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "tively select nodes and edges, enhancing intra- and cross-modal",
          "ACM MM, 2024, Melbourne, Australia": "in Conversations (ERC)\nis to predict\nthe emotion label\nfor each"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "interactions. While these methods primarily focus on designing model",
          "ACM MM, 2024, Melbourne, Australia": "utterance in the conversation from a predefined emotion category"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "structures, they overlook the challenges posed by modality imbalance",
          "ACM MM, 2024, Melbourne, Australia": "set E. Each utterance is associated with ùëÄ modalities, i.e. textual"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "during multimodal learning.",
          "ACM MM, 2024, Melbourne, Australia": "(t), audio (a), and visual (v) modalities, represented as:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": ", ùë¢ùë£\nùë¢ùëñ = {ùë¢ùë°"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "2.2\nImbalanced multimodal learning",
          "ACM MM, 2024, Melbourne, Australia": "(1)\nùëñ , ùë¢ùëé\nùëñ }, ùëñ ‚àà {1, . . . , ùëÅ }"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Despite the suggestion by [13] that integrating multiple modalities",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "where ùë¢ùëñ ‚àà RùëÄ √óùëë , ùëë signifies the dimension of modal features. For"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "could enhance the accuracy of latent space estimations, thereby im-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "each modality ùëö, we derive multimodal features {Xùëö }ùëö‚àà {ùë°,ùëé,ùë£ } ‚àà"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "proving the efficacy of multimodal models, our investigation within",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "is the feature\nRùëëùëö √óùëÅ for the conversation ùê∂. Here, {ùëëùëö }ùëö‚àà {ùë°,ùëé,ùë£ }"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "the multimodal ERC task reveals a phenomenon contradicting this",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "dimension of each modality."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "notion. The problem of modality imbalance persists as a significant",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "In the following sub-section, we outline our proposed model"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "challenge in multimodal learning frameworks involving low-quality",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Ada2I,\nincluding its main sub-modules: (1) Modality Encoder, (2)"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "data [43], particularly in tasks such as multimodal ERC. Conven-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Adaptive Feature Weighting and (3) Adaptive Modality Weighting."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "tional methods often prioritize one modality over others, assuming",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "We also refine the disparity ratio metric as part of our Training"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "that certain types of sensory data are more relevant for a given",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Optimization Strategy. Figure 2 illustrates architecture of Ada2I."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "task. For example, textual cues may receive greater emphasis, while",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "visual or audio cues alone might be prioritized [16, 24, 38]. Current",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "methodologies for addressing imbalanced multimodal learning pri-",
          "ACM MM, 2024, Melbourne, Australia": "3.2\nModality Encoder"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "marily focus on tasks such as audio-visual\nlearning with a focus",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Given a conversation ùê∂, a Transformer [34] network is utilized as"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "on optimizing pairwise modality learning [6, 26, 40], sentiment",
          "ACM MM, 2024, Melbourne, Australia": "the encoder to generate a unimodal representation Zùëö ‚àà RùëÅ √óùëëùëö"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "analysis, and sarcasm detection [46]. However, these approaches",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "respecting to the modality ùëö as:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "often have task-specific limitations and framework restrictions,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "limiting their broader applicability. For instance, Wang et al. [36]",
          "ACM MM, 2024, Melbourne, Australia": "ùëö), ùëö ‚àà {ùë°, ùëé, ùë£ }\nZ\nùëö = ùúô (ùúÉ (ùëö), X\n(2)"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "identified that different modalities overfit and generalize at different",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "rates, leading to suboptimal solutions when jointly trained using a",
          "ACM MM, 2024, Melbourne, Australia": "where the function ùúô (ùúÉ (ùëö) ) is the Transformer network with learn-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "unified optimization strategy. Peng et al. [26] proposed OGM-ME",
          "ACM MM, 2024, Melbourne, Australia": "able parameter ùúÉ (ùëö) ."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "method where the better-performing modality dominates the gra-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "dient update, suppressing the learning process of other modalities.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "3.3\nAdaptive Feature Weighting (AFW)"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "MMCosine [40] employs normalization techniques on features and",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "3.3.1\nTensor-based Multimodal Interaction Representation. Moti-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "weights to promote balanced and improved fine-grained learning",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "vated by the tensor-ring decomposition method introduced by [44],"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "across multiple modalities. Notably, there is a lack of specific ap-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "we extend the traditional attention mechanism by replacing the"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "proaches tailored for multimodal ERC apart from the work by Wang",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "query (Q) and key (K) representations with tensor-ring decomposition-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "et al. [37]. Recently, Wang et al. [37] observed a phenomenon re-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "based counterparts. This modification results in query tensor-ring"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ferred to as ‚Äúdiminishing modal marginal utility‚Äù and proposed fine-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "representation GùëÑ and key tensor-ring representation Gùêæ , which"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "grained adaptive gradient modulation, which was applied to ERC,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "facilitate the acquisition of more compact modality representa-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "while I2MCL considers both data difficulty and modality balance",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "tions. Additionally, inspired by [32], we integrate a tensor-based"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "for multimodal learning based on curriculum learning for affective",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "multi-way interaction transformer architecture into our model."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "computing,\nthough not specifically for emotion recognition. To",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "This enhancement allows the model to capture multi-way interac-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "comprehensively address the challenge of modality imbalance in",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "tions among modalities, thereby enhancing its capability to discern"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "multimodal ERC, we propose an end-to-end model that ensures",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "intricate multimodal relationships."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "balance among text, audio, and visual modalities during training.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "We employ a tensor-ring-based generation function to retrieve"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "the multi-interaction multimodal query tensor Q and key tensor K"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "3\nMETHODOLOGY",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "from the input modality presentations Zùëö. Specifically, we compute"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "3.1\nPreliminary",
          "ACM MM, 2024, Melbourne, Australia": "Q and K as follows:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "3.1.1\nTensor Ring Decomposition. A tensor of order ùêæ (ùêæ dimen-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": ", G (ùëé)\n, G (ùë£)\n} ‚àà Rùëëùë° √óùëëùëé √óùëëùë£\n(cid:40)Q = Tr{G (ùë° )"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "sions) T ‚àà Rùëë1 √óùëë2 √ó...√óùëëùêæ can be represented as a sequence of core",
          "ACM MM, 2024, Melbourne, Australia": "ùëÑ\nùëÑ\nùëÑ"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "(3)"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "tensors of order 3: Gùëó ‚àà Rùëë ùëó √óùëü ùëó √óùëü ùëó +1 , where the last core tensor has",
          "ACM MM, 2024, Melbourne, Australia": ", G (ùëé)\n, G (ùë£)\n} ‚àà Rùëëùë° √óùëëùëé √óùëëùë£\nK = Tr{G (ùë° )"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "ùêæ\nùêæ\nùêæ"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùëé": "ùëäùëò"
        },
        {
          "ùëé": "ùëé"
        },
        {
          "ùëé": "ùëäùëò"
        },
        {
          "ùëé": ""
        },
        {
          "ùëé": "Loss Function"
        },
        {
          "ùëé": "ùë°"
        },
        {
          "ùëé": "ùëäùëò"
        },
        {
          "ùëé": "ùë°"
        },
        {
          "ùëé": "ùëäùëò"
        },
        {
          "ùëé": ""
        },
        {
          "ùëé": "ùë£\nSoftmax"
        },
        {
          "ùëé": "ùëäùëò"
        },
        {
          "ùëé": "ùë£"
        },
        {
          "ùëé": "ùëäùëò"
        },
        {
          "ùëé": "ùëé"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùë°": "ùëäùëò"
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": "ùë£"
        },
        {
          "ùë°": "ùëäùëò"
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        },
        {
          "ùë°": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "‚äõ": ""
        },
        {
          "‚äõ": ""
        },
        {
          "‚äõ": ""
        },
        {
          "‚äõ": ""
        },
        {
          "‚äõ": ""
        },
        {
          "‚äõ": "Predicted Labels"
        },
        {
          "‚äõ": ""
        },
        {
          "‚äõ": "‡∑ùùíö"
        },
        {
          "‚äõ": ""
        },
        {
          "‚äõ": "Linear"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "ùëö‚àíùëéùëëùëéùëùùë°"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Linear",
          "ACM MM, 2024, Melbourne, Australia": "where Z\nrepresents the output of the ùëó-th class for the ùëñ-th\nùëó"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "reshape",
          "ACM MM, 2024, Melbourne, Australia": "sample. Finally, we combine the all loss functions into a joint objec-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ùí¢(ùëö )\nùëçùëö",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "tive function, which is used to optimize all trainable parameters in"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Linear",
          "ACM MM, 2024, Melbourne, Australia": "an end-to-end manner:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Lùëöùëéùëñùëõ = Lùëöùëúùëëùëéùëô + Lùëì ùëíùëéùë°ùë¢ùëüùëí + Lùëêùëôùë†\n(14)"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Figure 3: Linear Transform block to compute core tensor.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Recent studies have brought attention to the challenge of han-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "dling imbalanced optimization in joint learning models, particularly"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "i.e., audio and visual,\nleading to divergent unimodal\nlogit scores",
          "ACM MM, 2024, Melbourne, Australia": "when dealing with multiple modalities. Peng et al. [26] introduce the"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "and distorting the joint fusion representation. Inspired by [35, 45],",
          "ACM MM, 2024, Melbourne, Australia": "OGM-GE method to address optimization imbalances encountered"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "we propose to incorporate modality-wise L2 normalization to prop-",
          "ACM MM, 2024, Melbourne, Australia": "during the simultaneous training of dual-modal systems, i.e., visual"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "erly weight features, mitigating imbalances arising from differing",
          "ACM MM, 2024, Melbourne, Australia": "and audio. However, directly applying the OGM-GE method to our"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "data distributions and noise levels across modalities. This dynamic",
          "ACM MM, 2024, Melbourne, Australia": "framework is not practical as it only deals with two modalities. In"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "adjustment prevents any single modality from dominating the fu-",
          "ACM MM, 2024, Melbourne, Australia": "contrast, our framework caters to more than two modalities across"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "sion process, thus enhancing overall performance. Therefore, the",
          "ACM MM, 2024, Melbourne, Australia": "different domains, specifically tailored for the multimodal ERC task."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "modality-level balanced representation Zùëö‚àíùëéùëëùëéùëùùë° of the given con-",
          "ACM MM, 2024, Melbourne, Australia": "Therefore, leanrable parameter of encoder layer is optimized during"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "versation is calculated as follows:",
          "ACM MM, 2024, Melbourne, Australia": "training process as the following strategy:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "{ùë°,ùëé,ùë£ }\nùëì ‚àíùëéùëëùëéùëùùë°",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ùëä ùëöZ",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ùëö",
          "ACM MM, 2024, Melbourne, Australia": "= ùúÉ (ùëö)\n‚àí ùúÇ. ÀÜùëî(ùúÉ (ùëö)"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "‚àëÔ∏Å ùëö\n+ ùëè\nùëö‚àíùëéùëëùëéùëùùë° =\nZ\n(8)",
          "ACM MM, 2024, Melbourne, Australia": ")\n(15)\nùë° +1"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ùëì ‚àíùëéùëëùëéùëùùë°",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "‚à•\n‚à•ùëä ùëö ‚à• ‚à•Z\nùëö",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "(cid:205) ùë• ‚àà ùêµùë° ‚àáùúÉùëö\nwhere ÀÜùëî(ùúÉùëöùë°) = 1\n) represents an unbiased\nùë° ùìÅ(ùë•, ùúÉ (ùëñ )"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "where ùëä ùëö ‚àà Rùëëùëö √ó | E | symbolizes the output matrix of the model",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "estimation of the full gradient ‚àáùúÉùëö\n) using a random mini-\nùë° ùìÅ(ùë•, ùúÉ (ùëñ )"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "pertaining to modality ùëö, and E is the set of emotion classes.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "For emotion recognition, we feed Zùëö‚àíùëéùëëùëéùëùùë° , into the mulilayer",
          "ACM MM, 2024, Melbourne, Australia": ")\nbatch ùêµùë° chosen at the ùë°-th step with size ùëú. The term ‚àáùúÉùëö\nùë° ùìÅ(ùë•, ùúÉ (ùëñ )"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "preceptron (MLP) with ReLU activation function to compute the",
          "ACM MM, 2024, Melbourne, Australia": "denotes the gradient with respect to ùêµùë° ."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "output\nùë¶ùëñ ‚àà RùëÅ √ó | E | .",
          "ACM MM, 2024, Melbourne, Australia": "We adjust the balance of modalities through gradient parameter"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "adjustments. For each output at step ùë°, we compute the discrepancy"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ùëö‚àíùëéùëëùëéùëùùë° )\nùë¶ùëñ = MLP(Z\n(9)",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "ratio for each modality using the softmax of the cosine similarity"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ùë¶ùëñ\nThe output\nis utilized to predict emotion labels.",
          "ACM MM, 2024, Melbourne, Australia": "between the output weights and the corresponding feature vectors:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùëñ , ùë•ùëé\nùëñ\nùëñ=1, ùëö ‚àà {ùë°, ùëé, ùë£ }"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùë¶\nÀÜ\nOutput:\nPrediction emotion label"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "for each training epoch do"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùëñ , ùë•ùëé\nùëñ\nùëñ=1 } sampled from D do"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "#Refer to Subsection 3.2"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Encode unimodal feature Xùëö to Zùëö as Eq (2)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "#Refer to Subsection 3.3"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Multimodal feature representation as Eq (3)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Calculate coefficient matrix Œòùëö as Eq (4)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Calculate modality-aware attention ùê¥ùë°ùë°ùëö as Eq (6)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùëì ‚àíùëéùëëùëéùëùùë°"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùëö\nCompute fused feature Z\nwith ùõΩ using Eq (7)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "#Refer to Subsection 3.4"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Compute logit output Zùëö‚àíùëéùëëùëéùëùùë° with modality-wise L2 normal-"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ization as Eq (8)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Produce prediction of multimodal data\nùë¶ùëñ as Eq (9)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "#Refer to Subsection 3.5"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Use cross-entropy loss to calculate Lùëêùëôùë† as Eq (10)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Use ùêø1 to calculate Lùëì ùëíùëéùë°ùë¢ùëüùëí as Eq (12)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Use cross-entropy to calculate Lùëöùëúùëëùëéùëô as Eq (13)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Add Lùëì ùëíùëéùë°ùë¢ùëüùëí , Lùëöùëúùëëùëéùëô and Lùëêùëôùë†\nto compute Lùëöùëéùëñùëõ as Eq (14)"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùë†ùëö"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùë°\n=\nCompute discrepancy ratio ùúåùëö"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùë° )\nminùëö‚àà{ùë°,ùëé,ùë£} (ùë† ùëó"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Compute modulation coefficient ùëòùëö"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ùëî (ùúÉ (ùëñ )\n¬∑ ùëòùëñ\nUpdate using ùúÉ (ùëñ )\nùë° + ùúÇ ¬∑ ‚Ñé (ùúÉ (ùëñ )"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "‚àí ùúÇ ¬∑\n)\n)\nùë°\nùë° +1 = ùúÉ (ùëñ )"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "end for"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "end for"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "3.6\nDatasets"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Datasets: We consider three benchmark datasets for multimodal"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ERC namely: IEMOCAP [3], MELD [28], and CMU-MOSEI [1]. The"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "dataset statistics are illustrated in Table 1."
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Table 1: Data Statistics"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Dialogues\nUtterances"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "Datasets"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "train\nvalid\ntest\ntrain\nvalid\ntest"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "IEMOCAP\n120\n31\n5,810\n1,623"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "MELD\n1,039\n114\n280\n9,989\n1,109\n2,610"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "CMU-MOSEI\n2,248\n300\n676\n16,326\n1,871\n4,659"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "IEMOCAP. This dataset comprises 12 hours of video recordings"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "of dyadic conversations involving 10 speakers. It includes 151 dia-"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "logues, segmented into 7,433 utterances, each annotated with one of"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "six emotion labels: happy, sad, neutral, angry, excited, or frustrated."
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "MELD. This dataset is based on the TV series Friends, includes"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "13,709 video clips featuring multi-party conversations, each labeled"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "with one of Ekman‚Äôs six universal emotions: joy, sadness, fear, anger,"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "surprise, and disgust."
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "CMU-MOSEI:. This dataset is a prominent resource for sentiment"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "and emotion analysis, comprises 3,228 YouTube videos divided"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "into 23,453 segments, featuring contributions from 1,000 speakers"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "covering 250 topics. It includes six emotion categories: happy, sad,"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "angry, scared, disgusted, and surprised, with sentiment intensity"
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": ""
        },
        {
          ", ùë• ùë£\n), ùë¶ùëñ }ùëÅ": "ranging from -3 to 3."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": ""
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "Methods"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": ""
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": ""
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "DialogueRNN‚Ä†"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "DialogueGCN‚Ä†"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "BiDDIN‚Ä†"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "MM-DFN‚Ä†"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "MMGCN‚Ä†"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "Ada2I (Ours)"
        },
        {
          "FAGM [37] (denoted by ‚Ä†). The best performance is indicated in bold, and the second-best performance is underlined.": "Œî(%)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "Ada2I"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "0.43"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "with ‚Äú-‚Äù indicate missing results, and ‚Ä† denotes results repro-",
          "Cam-Van Thi Nguyen, et al.": "0.38"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "0.33\n20\n30\n40\n50"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Methods",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "0.28"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "T+A+V\nT+A",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Multilouge-Net [31]\n82.10\n80.18",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "TBJE [4]\n81.50\n82.40",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "20\n30\n40\n50"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "COGMEN‚Ä† [16]\n82.95\n85.00",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "CORECT‚Ä† [24]\n83.98\n84.28",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "I2MCL [46]\n81.05\n-",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "OGM-GE‚Ä† [26]\n84.58\n84.03",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Ada2I (Ours)\n85.25\n85.08",
          "Cam-Van Thi Nguyen, et al.": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Cam-Van Thi Nguyen, et al.": "20\n30\n40\n50"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Œî(%)\n‚Üë0.67\n‚Üë0.08",
          "Cam-Van Thi Nguyen, et al.": "Epoch"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "the IEMOCAP dataset"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "Ada2I, ensuring model stability. Furthermore, Ada2I with training"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "optimization balances the training across three modalities (text,"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "audio, visual), preventing the text modality from dominating the"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "others."
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "Table 5: Ablation studies of Ada2I on AFW, AMW, and train-"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "ing strategy. The symbol ‚Üì denotes the reduction in perfor-"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "mance of the variants compared to Ada2I."
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "IEMOCAP\nMELD"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "Modules"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "W-F1\nAcc\nW-F1\nAcc"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "w/o AFW\n66.24(‚Üì2.73)\n65.99(‚Üì2.77)\n59.65(‚Üì0.73)\n62.45(‚Üì0.58)"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "w/o AMW\n66.11(‚Üì2.86)\n65.87(‚Üì2.89)\n58.87(‚Üì1.51)\n61.13(‚Üì1.90)"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "w/o traning optimization\n67.95(‚Üì1.02)\n68.08(‚Üì0.68)\n58.13(‚Üì2.25)\n59.92(‚Üì3.11)"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": "Ada2I (Ours)\n68.97\n68.76\n60.38\n63.03"
        },
        {
          "Figure 6: Modality-wise weights of each label normalized for": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "REFERENCES",
          "ACM MM, 2024, Melbourne, Australia": "[18]\nJiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang Zeng. 2023. GraphMFT: A"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "graph network based multimodal fusion technique for emotion recognition in"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[1] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "conversation. Neurocomputing 550 (2023), 126427."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Philippe Morency. 2018. Multimodal Language Analysis in the Wild: CMU-MOSEI",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[19] Zheng Lian, Bin Liu, and Jianhua Tao. 2021. CTNet: Conversational transformer"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Dataset and Interpretable Dynamic Fusion Graph.\nIn Proceedings of\nthe 56th",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "network for emotion recognition.\nIEEE/ACM Transactions on Audio, Speech, and"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Language Processing 29 (2021), 985‚Äì1000."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Papers), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[20]\nPaul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Yu-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Linguistics, Melbourne, Australia, 2236‚Äì2246.\nhttps://doi.org/10.18653/v1/P18-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "fan Chen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. 2021. MultiBench: Multiscale"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "1208",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Benchmarks for Multimodal Representation Learning. In Thirty-fifth Conference"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[2] Tadas Baltru≈°aitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2018. Multi-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "on Neural Information Processing Systems Datasets and Benchmarks Track (Round"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "modal machine learning: A survey and taxonomy.\nIEEE transactions on pattern",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "1)."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "analysis and machine intelligence 41, 2 (2018), 423‚Äì443.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[21]\nPaul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2022.\nFoundations"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[3] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "and Trends in Multimodal Machine Learning: Principles, Challenges, and Open"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Questions. arXiv preprint arXiv:2209.03430 (2022)."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "IEMOCAP:\nInteractive emotional dyadic motion capture database.\nLanguage",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[22] Xun Lin, Shuai Wang, Rizhao Cai, Yizhong Liu, Ying Fu, Zitong Yu, Wenzhong"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "resources and evaluation 42, 4 (2008), 335‚Äì359.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Tang, and Alex Kot. 2024. Suppress and Rebalance: Towards Generalized Multi-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[4]\nJean-Benoit Delbrouck, No√© Tits, Mathilde Brousmiche, and St√©phane Dupont.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Modal Face Anti-Spoofing. arXiv preprint arXiv:2402.19298 (2024)."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "2020. A Transformer-based joint-encoding for Emotion Recognition and Senti-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[23] Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ment Analysis. In Second Grand-Challenge and Workshop on Multimodal Language",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Alexander Gelbukh, and Erik Cambria. 2019. Dialoguernn: An attentive rnn"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "(Challenge-HML), Amir Zadeh, Louis-Philippe Morency, Paul Pu Liang, and Sou-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "for emotion detection in conversations. In Proceedings of the AAAI conference on"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "janya Poria (Eds.). Association for Computational Linguistics, Seattle, USA, 1‚Äì7.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "artificial intelligence, Vol. 33. 6818‚Äì6825."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "https://doi.org/10.18653/v1/2020.challengehml-1.1",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[24] Cam-Van Thi Nguyen, Tuan Mai, Son The, Dang Kieu, and Duc-Trong Le. 2023."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Florian Eyben, Martin W√∂llmer, and Bj√∂rn Schuller. 2010. Opensmile: the munich\n[5]",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Conversation Understanding using Relational Temporal Graph Neural Networks"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "versatile and fast open-source audio feature extractor. In Proceedings of the 18th",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "with Auxiliary Cross-Modality Interaction. In Proceedings of the 2023 Conference"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ACM international conference on Multimedia. 1459‚Äì1462.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[6] Yunfeng Fan, Wenchao Xu, Haozhao Wang, Junxiao Wang, and Song Guo. 2023.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "and Kalika Bali\n(Eds.). Association for Computational Linguistics, Singapore,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "PMR: Prototypical Modal Rebalance for Multimodal Learning.\nIn Proceedings",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "15154‚Äì15167.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.937"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20029‚Äì",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[25] Cam-Van Thi Nguyen, Cao-Bach Nguyen, Duc-Trong Le, and Quang-Thuy Ha."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "20038.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "2024. Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emo-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "tion Recognition.\nIn Proceedings of\nthe 2024 Joint\nInternational Conference on"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "fast networks for video recognition. In Proceedings of the IEEE/CVF international",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Computational Linguistics, Language Resources and Evaluation (LREC-COLING"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "conference on computer vision. 6202‚Äì6211.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "2024), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakri-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[8] Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, and",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "ani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia, 4259‚Äì4265."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Alexander Gelbukh. 2019. DialogueGCN: A Graph Convolutional Neural Network",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "https://aclanthology.org/2024.lrec-main.380"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "for Emotion Recognition in Conversation. In Proceedings of the 2019 Conference",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[26] Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and Di Hu. 2022. Balanced"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "on Empirical Methods in Natural Language Processing and the 9th International",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "multimodal\nlearning via on-the-fly gradient modulation. In Proceedings of the"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8238‚Äì8247."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[27]\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Linguistics, Hong Kong, China, 154‚Äì164.\nhttps://doi.org/10.18653/v1/D19-1015",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Zadeh, and Louis-Philippe Morency. 2017. Context-Dependent Sentiment Analy-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[9] Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria, and Roger",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "sis in User-Generated Videos. In Proceedings of the 55th Annual Meeting of the"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Zimmermann. 2018.\nICON: Interactive Conversational Memory Network for",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Multimodal Emotion Detection. In Proceedings of the 2018 Conference on Empirical",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "and Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hock-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Canada, 873‚Äì883.\nhttps://doi.org/10.18653/v1/P17-1081"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "enmaier, and Jun‚Äôichi Tsujii (Eds.). Association for Computational Linguistics,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik\n[28]"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Brussels, Belgium, 2594‚Äì2604.\nhttps://doi.org/10.18653/v1/D18-1280",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Cambria, and Rada Mihalcea. 2019. MELD: A Multimodal Multi-Party Dataset for"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[10] Devamanyu Hazarika, Soujanya Poria, Amir Zadeh, Erik Cambria, Louis-Philippe",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Emotion Recognition in Conversations. In Proceedings of the 57th Annual Meeting"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Morency, and Roger Zimmermann. 2018. Conversational Memory Network for",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "of the Association for Computational Linguistics, Anna Korhonen, David Traum,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Emotion Recognition in Dyadic Dialogue Videos. In Proceedings of the 2018 Con-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "and Llu√≠s M√†rquez (Eds.). Association for Computational Linguistics, Florence,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ference of the North American Chapter of the Association for Computational Lin-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Italy, 527‚Äì536.\nhttps://doi.org/10.18653/v1/P19-1050"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "guistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[29]\nSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. 2019."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "wav2vec: Unsupervised pre-training for speech recognition.\narXiv preprint"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "New Orleans, Louisiana, 2122‚Äì2132.\nhttps://doi.org/10.18653/v1/N18-1193",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "arXiv:1904.05862 (2019)."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[11] Dou Hu, Xiaolong Hou, Lingwei Wei, Lianxin Jiang, and Yang Mo. 2022. MM-DFN:",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[30] Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan. 2021. Directed Acyclic"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Multimodal dynamic fusion network for emotion recognition in conversations.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Graph Network for Conversational Emotion Recognition. In Proceedings of the"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "59th Annual Meeting of\nthe Association for Computational Linguistics and the"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Processing (ICASSP). IEEE, 7037‚Äì7041.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "11th International Joint Conference on Natural Language Processing (Volume 1:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[12] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Long Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.)."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "2017. Densely Connected Convolutional Networks. In Proceedings of the IEEE",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Association for Computational Linguistics, Online, 1551‚Äì1560.\nhttps://doi.org/"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Conference on Computer Vision and Pattern Recognition (CVPR).",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "10.18653/v1/2021.acl-long.123"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[13]\nYu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[31] Aman Shenoy and Ashish Sardana. 2020. Multilogue-Net: A Context-Aware RNN"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Huang. 2021. What makes multi-modal learning better than single (provably).",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "for Multi-modal Emotion Detection and Sentiment Analysis in Conversation."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Advances in Neural Information Processing Systems 34 (2021), 10944‚Äì10956.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "In Second Grand-Challenge and Workshop on Multimodal Language (Challenge-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[14] Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, and Longbo Huang. 2022.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "HML), Amir Zadeh, Louis-Philippe Morency, Paul Pu Liang, and Soujanya Poria"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Modality competition: What makes joint training of multi-modal network fail",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "(Eds.). Association for Computational Linguistics, Seattle, USA, 19‚Äì28.\nhttps:"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "in deep learning?(provably). In International Conference on Machine Learning.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "//doi.org/10.18653/v1/2020.challengehml-1.3"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "PMLR, 9226‚Äì9259.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[32]\nJiajia Tang, Kang Li, Ming Hou, Xuanyu Jin, Wanzeng Kong, Yu Ding, and Qibin"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[15] Adri√°n Javaloy, Maryam Meghdadi, and Isabel Valera. 2022. Mitigating modality",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Zhao. 2022. MMT: Multi-Way Multi-Modal Transformer for Multimodal Learn-"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "collapse in multimodal VAEs via impartial optimization. In International Confer-",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "ing. In Proceedings of the Thirty-First International Joint Conference on Artificial"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ence on Machine Learning. PMLR, 9938‚Äì9964.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Intelligence, IJCAI-22, LD Raedt, Ed. International Joint Conferences on Artificial"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[16] Abhinav Joshi, Ashwani Bhat, Ayush Jain, Atin Singh, and Ashutosh Modi. 2022.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Intelligence Organization, Vol. 7. 3458‚Äì3465."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "COGMEN: COntextualized GNN based multimodal emotion recognitioN.\nIn",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[33] Geng Tu, Tian Xie, Bin Liang, Hongpeng Wang, and Ruifeng Xu. 2024. Adaptive"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "Proceedings of the 2022 Conference of the North American Chapter of the Association",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Graph Learning for Multimodal Conversational Emotion Detection. In Proceedings"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "for Computational Linguistics: Human Language Technologies. 4148‚Äì4164.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "of the AAAI Conference on Artificial Intelligence, Vol. 38. 19089‚Äì19097."
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "[17] Bobo Li, Hao Fei, Lizi Liao, Yu Zhao, Chong Teng, Tat-Seng Chua, Donghong Ji,",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "[34] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones,"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "and Fei Li. 2023. Revisiting disentanglement and fusion on modality and context",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "in conversational multimodal emotion recognition.\nIn Proceedings of\nthe 31st",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "need. In Proceedings of the 31st International Conference on Neural Information"
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "ACM International Conference on Multimedia. 5923‚Äì5934.",
          "ACM MM, 2024, Melbourne, Australia": ""
        },
        {
          "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition": "",
          "ACM MM, 2024, Melbourne, Australia": "Processing Systems. 6000‚Äì6010."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": "[35]\nFeng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. 2017. Normface:",
          "Cam-Van Thi Nguyen, et al.": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "L2 hypersphere embedding for face verification. In Proceedings of the 25th ACM",
          "Cam-Van Thi Nguyen, et al.": "Processing (ICASSP). IEEE, 1‚Äì5."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "international conference on Multimedia. 1041‚Äì1049.",
          "Cam-Van Thi Nguyen, et al.": "[41] Dong Zhang, Weisheng Zhang, Shoushan Li, Qiaoming Zhu, and Guodong Zhou."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[36] Weiyao Wang, Du Tran, and Matt Feiszli. 2020. What makes training multi-",
          "Cam-Van Thi Nguyen, et al.": "2020. Modeling both intra-and inter-modal influence for real-time emotion de-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "modal classification networks hard?. In Proceedings of the IEEE/CVF conference on",
          "Cam-Van Thi Nguyen, et al.": "tection in conversations. In Proceedings of the 28th ACM International Conference"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "computer vision and pattern recognition. 12695‚Äì12705.",
          "Cam-Van Thi Nguyen, et al.": "on Multimedia. 503‚Äì511."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[37] Yunxiao Wang, Meng Liu, Zhe Li, Yupeng Hu, Xin Luo, and Liqiang Nie. 2023.",
          "Cam-Van Thi Nguyen, et al.": "[42] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Qiao Yu. 2016.\nJoint Face"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Unlocking the Power of Multimodal Learning for Emotion Recognition in Con-",
          "Cam-Van Thi Nguyen, et al.": "Detection and Alignment Using Multitask Cascaded Convolutional Networks."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "versation. In Proceedings of the 31st ACM International Conference on Multimedia.",
          "Cam-Van Thi Nguyen, et al.": "IEEE Signal Processing Letters 23, 10 (2016), 1499‚Äì1503."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "5947‚Äì5955.",
          "Cam-Van Thi Nguyen, et al.": "[43] Qingyang Zhang, Yake Wei, Zongbo Han, Huazhu Fu, Xi Peng, Cheng Deng,"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[38]\nYinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng",
          "Cam-Van Thi Nguyen, et al.": "Qinghua Hu, Cai Xu, Jie Wen, Di Hu, et al. 2024. Multimodal fusion on low-quality"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized",
          "Cam-Van Thi Nguyen, et al.": "data: A comprehensive survey. arXiv preprint arXiv:2404.18947 (2024)."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "recommendation of micro-video. In Proceedings of the 27th ACM international",
          "Cam-Van Thi Nguyen, et al.": "[44] Qibin Zhao, Guoxu Zhou, Shengli Xie, Liqing Zhang, and Andrzej Cichocki. 2016."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "conference on multimedia. 1437‚Äì1445.",
          "Cam-Van Thi Nguyen, et al.": "Tensor ring decomposition. arXiv preprint arXiv:1606.05535 (2016)."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[39] Nan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and Krzysztof J Geras. 2022.",
          "Cam-Van Thi Nguyen, et al.": "[45] Yutong Zheng, Dipan K Pal, and Marios Savvides. 2018.\nRing loss: Convex"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Characterizing and overcoming the greedy nature of learning in multi-modal",
          "Cam-Van Thi Nguyen, et al.": "feature normalization for face recognition. In Proceedings of the IEEE conference"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "deep neural networks. In International Conference on Machine Learning. PMLR,",
          "Cam-Van Thi Nguyen, et al.": "on computer vision and pattern recognition. 5089‚Äì5097."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "24043‚Äì24055.",
          "Cam-Van Thi Nguyen, et al.": "[46] Yuwei Zhou, Xin Wang, Hong Chen, Xuguang Duan, and Wenwu Zhu. 2023."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[40] Ruize Xu, Ruoxuan Feng, Shi-Xiong Zhang, and Di Hu. 2023. MMCosine: Multi-",
          "Cam-Van Thi Nguyen, et al.": "Intra-and Inter-Modal Curriculum for Multimodal Learning. In Proceedings of the"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning. In",
          "Cam-Van Thi Nguyen, et al.": "31st ACM International Conference on Multimedia. 3724‚Äì3735."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "2",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal machine learning: A survey and taxonomy"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "No√© Tits",
        "Mathilde Brousmiche",
        "St√©phane Dupont"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.1"
    },
    {
      "citation_id": "5",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "PMR: Prototypical Modal Rebalance for Multimodal Learning",
      "authors": [
        "Yunfeng Fan",
        "Wenchao Xu",
        "Haozhao Wang",
        "Junxiao Wang",
        "Song Guo"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Slowfast networks for video recognition",
      "authors": [
        "Christoph Feichtenhofer",
        "Haoqi Fan",
        "Jitendra Malik",
        "Kaiming He"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "8",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "9",
      "title": "ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "10",
      "title": "Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "11",
      "title": "MM-DFN: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "13",
      "title": "What makes multi-modal learning better than single (provably)",
      "authors": [
        "Yu Huang",
        "Chenzhuang Du",
        "Zihui Xue",
        "Xuanyao Chen",
        "Hang Zhao",
        "Longbo Huang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably)",
      "authors": [
        "Yu Huang",
        "Junyang Lin",
        "Chang Zhou",
        "Hongxia Yang",
        "Longbo Huang"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "15",
      "title": "Mitigating modality collapse in multimodal VAEs via impartial optimization",
      "authors": [
        "Adri√°n Javaloy",
        "Maryam Meghdadi",
        "Isabel Valera"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "17",
      "title": "Revisiting disentanglement and fusion on modality and context in conversational multimodal emotion recognition",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Lizi Liao",
        "Yu Zhao",
        "Chong Teng",
        "Tat-Seng Chua",
        "Donghong Ji",
        "Fei Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "GraphMFT: A graph network based multimodal fusion technique for emotion recognition in conversation",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "CTNet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Zetian Wu",
        "Yun Cheng",
        "Jason Wu",
        "Leslie Yufan Chen",
        "Peter Wu",
        "Michelle Lee",
        "Yuke Zhu"
      ],
      "year": "2021",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "21",
      "title": "Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions",
      "authors": [
        "Paul Pu Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2022",
      "venue": "Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions",
      "arxiv": "arXiv:2209.03430"
    },
    {
      "citation_id": "22",
      "title": "Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing",
      "authors": [
        "Xun Lin",
        "Shuai Wang",
        "Rizhao Cai",
        "Yizhong Liu",
        "Ying Fu",
        "Zitong Yu",
        "Wenzhong Tang",
        "Alex Kot"
      ],
      "year": "2024",
      "venue": "Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing",
      "arxiv": "arXiv:2402.19298"
    },
    {
      "citation_id": "23",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "24",
      "title": "Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction",
      "authors": [
        "Cam-Van Thi Nguyen",
        "Tuan Mai",
        "Son The",
        "Dang Kieu",
        "Duc-Trong Le"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.937"
    },
    {
      "citation_id": "25",
      "title": "Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition",
      "authors": [
        "Cam-Van Thi Nguyen",
        "Cao-Bach Nguyen",
        "Duc-Trong Le",
        "Quang-Thuy Ha"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024"
    },
    {
      "citation_id": "26",
      "title": "Balanced multimodal learning via on-the-fly gradient modulation",
      "authors": [
        "Xiaokang Peng",
        "Yake Wei",
        "Andong Deng",
        "Dong Wang",
        "Di Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "28",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "29",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition",
      "arxiv": "arXiv:1904.05862"
    },
    {
      "citation_id": "30",
      "title": "Directed Acyclic Graph Network for Conversational Emotion Recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.123"
    },
    {
      "citation_id": "31",
      "title": "Multilogue-Net: A Context-Aware RNN for Multi-modal Emotion Detection and Sentiment Analysis in Conversation",
      "authors": [
        "Aman Shenoy",
        "Ashish Sardana"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)",
      "doi": "10.18653/v1/2020.challengehml-1.3"
    },
    {
      "citation_id": "32",
      "title": "MMT: Multi-Way Multi-Modal Transformer for Multimodal Learning",
      "authors": [
        "Jiajia Tang",
        "Kang Li",
        "Ming Hou",
        "Xuanyu Jin",
        "Wanzeng Kong",
        "Yu Ding",
        "Qibin Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, LD Raedt"
    },
    {
      "citation_id": "33",
      "title": "Adaptive Graph Learning for Multimodal Conversational Emotion Detection",
      "authors": [
        "Geng Tu",
        "Tian Xie",
        "Bin Liang",
        "Hongpeng Wang",
        "Ruifeng Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Normface: L2 hypersphere embedding for face verification",
      "authors": [
        "Feng Wang",
        "Xiang Xiang",
        "Jian Cheng",
        "Alan Loddon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM international conference on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "What makes training multimodal classification networks hard",
      "authors": [
        "Weiyao Wang",
        "Du Tran",
        "Matt Feiszli"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "37",
      "title": "Unlocking the Power of Multimodal Learning for Emotion Recognition in Conversation",
      "authors": [
        "Yunxiao Wang",
        "Meng Liu",
        "Zhe Li",
        "Yupeng Hu",
        "Xin Luo",
        "Liqiang Nie"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "38",
      "title": "MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "39",
      "title": "Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks",
      "authors": [
        "Nan Wu",
        "Stanislaw Jastrzebski",
        "Kyunghyun Cho",
        "Krzysztof Geras"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "40",
      "title": "MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning",
      "authors": [
        "Ruize Xu",
        "Ruoxuan Feng",
        "Shi-Xiong",
        "Di Zhang",
        "Hu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Modeling both intra-and inter-modal influence for real-time emotion detection in conversations",
      "authors": [
        "Dong Zhang",
        "Weisheng Zhang",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Qiao Yu"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "43",
      "title": "Multimodal fusion on low-quality data: A comprehensive survey",
      "authors": [
        "Qingyang Zhang",
        "Yake Wei",
        "Zongbo Han",
        "Huazhu Fu",
        "Xi Peng",
        "Cheng Deng",
        "Qinghua Hu",
        "Cai Xu",
        "Jie Wen",
        "Di Hu"
      ],
      "year": "2024",
      "venue": "Multimodal fusion on low-quality data: A comprehensive survey",
      "arxiv": "arXiv:2404.18947"
    },
    {
      "citation_id": "44",
      "title": "Tensor ring decomposition",
      "authors": [
        "Qibin Zhao",
        "Guoxu Zhou",
        "Shengli Xie",
        "Liqing Zhang",
        "Andrzej Cichocki"
      ],
      "year": "2016",
      "venue": "Tensor ring decomposition",
      "arxiv": "arXiv:1606.05535"
    },
    {
      "citation_id": "45",
      "title": "Ring loss: Convex feature normalization for face recognition",
      "authors": [
        "Yutong Zheng",
        "Dipan Pal",
        "Marios Savvides"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "46",
      "title": "Xuguang Duan, and Wenwu Zhu. 2023. Intra-and Inter-Modal Curriculum for Multimodal Learning",
      "authors": [
        "Yuwei Zhou",
        "Xin Wang",
        "Hong Chen"
      ],
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    }
  ]
}