{
  "paper_id": "2207.08104v1",
  "title": "A Multibias-Mitigated And Sentiment Knowledge Enriched Transformer For Debiasing In Multimodal Conversational Emotion Recognition",
  "published": "2022-07-17T08:16:49Z",
  "authors": [
    "Jinglin Wang",
    "Fang Ma",
    "Yazhou Zhang",
    "Dawei Song"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition in conversations (mERC) is an active research topic in natural language processing (NLP), which aims to predict human's emotional states in communications of multiple modalities, e,g., natural language and facial gestures. Innumerable implicit prejudices and preconceptions fill human language and conversations, leading to the question of whether the current datadriven mERC approaches produce a biased error. For example, such approaches may offer higher emotional scores on the utterances by females than males. In addition, the existing debias models mainly focus on gender or race, where multibias mitigation is still an unexplored task in mERC. In this work, we take the first step to solve these issues by proposing a series of approaches to mitigate five typical kinds of bias in textual utterances (i.e., gender, age, race, religion and LGBTQ+) and visual representations (i.e, gender and age), followed by a Multibias-Mitigated and sentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive experimental results show the effectiveness of the proposed model and prove that the debias operation has a great impact on the classification performance for mERC. We hope our study will benefit the development of bias mitigation in mERC and related emotion studies.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Whether people realize them or not, innumerable implicit prejudices and preconceptions fill human language, and are conveyed in almost all data sources, such as news, reviews and conversations  (Misra et al., 2016) . Such prejudices are known to hurt specific groups, and infringe their rights. For example, these two utterances, \"older people are not interested in digital technology\", or In the bracket at the end of each utterance is the emotion type (e.g., joy, surprise, etc.) of it. The underlined sentences and the words highlighted in red express an apparent gender bias, and such bias is further amplified by Joey's facial gestures.\n\n\"women are pleasant to look slim\", reveal the age and gender biases.\n\nRecent research has shown that pre-trained word representations, e.g., word embeddings (in which each word is represented as a vector in the semantic space), tend to amplify the bias in the data  (Kurita et al., 2019; Webster et al., 2020) . The male names have been proved more likely to be associated with career-related terms than female names, by calculating the similarity between their embeddings  (Caliskan et al., 2017) . African-American names are also shown to be more likely to be associated with unpleasant terms than European-American names  (Nadeem et al., 2020) . Unconsciously learning such implicit biases from a dataset that is sampled from all kinds of data sources, leads to the fact that the learned models may further amplify the harmful bias (such as gender or race) when they make decisions  (Goyal et al., 2019; Srinivasan and Bisk, 2021) . In addition, the biased error will propagate to downstream tasks. For example, coreference resolution systems exhibit a gender bias due to the use of biased word embeddings  (Rudinger et al., 2018) . Facial recognition applications have also been proved to perform worse for the inter-sectional group \"darker females\" than for either darker individuals or females  (Buolamwini and Gebru, 2018) .\n\nIn view that human language is multi-modal in nature, human bias also exists in multimodal conversations, e.g., textual and visual utterances. Figure  1  shows an example of the gender bias in a multimodal dialogue dataset 1 . Joey makes an association with a beautiful female nurse and expresses a significant smile when Rachel says \"cute nurse\", but when Rachel says \"they are male nurses\", he shows a disappointed-looking facial expression, although his textual response seems neutral.\n\nTherefore, human bias naturally resides in the multimodal expression of emotions in conversations.\n\nThere has been a great body of literature in debiasing for computer vision  (Buolamwini and Gebru, 2018)  and pre-trained language models  (Wang et al., 2020) . However, the existing debias models mainly focus on only one kind of prejudice, e.g., gender or race, where multibias mitigation is still an unexplored task in mERC. This leaves us with a research question: Whether the current data-driven multi-modal emotion recognition in conversations approaches produce a biased error or not?\n\nTo answer this question, we first propose a series of approaches for debiasing multiple types of bias in multimodal (i.e., textual and visual) conversations. For textual utterances, we propose mitigating five types of bias, including gender, age, race, religion, and LGBTQ+ in word embedding.\n\nFor visual utterances, we first propose a subspace-projection-based debiasing approach to mitigate two typical visual biases, i.e., gender and age. It constructs a subspace for each type of visual bias and identifies the type of bias in the visual representation by projecting the representation into the corresponding subspace.\n\nTo incorporate the proposed multimodal debiasing methods into the mERC task that involves 1 Trigger Warning: This paper contains examples of biases and stereotypes seen in society and language representations. These examples may be potentially triggering and offensive. These examples are meant to bring light to and mitigate these biases, and it is not an endorsement. conversational context modeling, cross-modality interactions capturing and the use of sentiment knowledge, we propose a Muiltibiases Mitigated and sentiment Knowledge Enriched Transformer (MMKET) as a unified framework. Specifically, it is a bimodal Transformer involving a contextual attention layer to capture the contextual interactions, a bimodal cross-attention layer to capture the cross-modal interactions, and a sentiment attention layer to enrich the debiased representation with sentiment knowledge.\n\nEmpirical evaluation has been carried out on two benchmark datasets, and the experimental results shows that the proposed multimodal debiasing methods can effectively mitigate the corresponding biases. We also prove that debiasing the representation of multimodal utterances has a remarkable impact on the performance of mERC models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Generation Of Bias",
      "text": "Models and algorithms have never independently created bias. Social bias is exhibited in multiple components of a NLP system, including the training corpus, pre-trained models (e.g., word embeddings), and algorithms themselves  (Caliskan et al., 2017; Garg et al., 2018) .\n\nDatasets: The Soil of Bias. The dataset is the basis of model training. The bias in the dataset comes from the unbalanced dataset samples and biased labels. For example, gender bias manifests itself in training data that features more examples of men than women, an unbalanced dataset. In the process of label annotation, the annotators will transfer personal bias to the data, where the algorithm absorbs, thus produces a biased model. Sometimes, such bias is due to a lack of domain expertise  (Plank et al., 2014)  or preconceived notions and stereotypes held by the annotators  (Sap et al., 2019) .\n\nBias in Word Embeddings. Word embeddings are often trained from large and human-created corpora that contain multifarious biased raw data. Recent literature has demonstrated that gender bias is encoded in word embeddings  (May et al., 2019) . For example, Bolukbasi highlights that \"programmer\" is more closely associated with \"man\" while \"homemaker\" is more closely associated with \"woman\" in word2vec embeddings trained on the Google News dataset  (Bolukbasi et al., 2016) . And word embeddings connect medical doctors more frequently to male pronouns than female pro- Bimodal Fusion",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Bias Type",
      "text": "Word Pairs Gender woman-man, girl-boy, she-he, mother-father, daughter-son, gal-guy, female-male Race slave-secondary, group-tribe, easy task-cake walk, master-primary Age young-old, health-disease, work-retirement, education-pension Religion Christian-Muslim, Christianity-Islam, Christ-Allah, Jesus-Muhammad LGBTQ+ homosexuals-they, husband/wife-spouse, dad/father/mom/mother-parent\n\nTable  1 : The five pre-defined sets of word pairs, where the main difference between each pair of words captures the corresponding bias.\n\nnouns  (Caliskan et al., 2017) . Furthermore, pretrained word embeddings are often used without access to the original data. Social bias in word embeddings will propagate to downstream tasks, which can further amplify social bias. Studies show that machine translation systems tend to link occupations to their stereotypical gender, e.g., linking \"doctor\" to \"he\" and \"nurse\" to \"she\"  (Prates et al., 2019) .\n\n3 Debiasing Methods",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mitigating Multiple Biases In Glove",
      "text": "The recent debiasing models  (Bolukbasi et al., 2016; Wang et al., 2020)  have only focused on removing gender bias in pre-trained word embeddings, particularly GloVe  (Pennington et al., 2014) , which has surfaced several social biases  (Spliethöver and Wachsmuth, 2021) . In this paper, we propose to mitigate five types of biases in GloVe embeddings, i.e., gender, race, religion, age, and LGBTQ+. Methodologically, we extend the existing Double-Hard Debias method, to multi-ple types of bias.\n\nHard Debias  (Bolukbasi et al., 2016) . Hard Debias is a commonly adopted debiasing strategy in NLP. It projects a pre-trained word embedding vector into a subspace orthogonal to an inferred bias subspace (i.e., direction of a particular type of bias), which is constructed based on a set of pre-defined word pairs (e.g., young vs. old) characterizing the bias.\n\nTo extend it to multiple types of bias mitigation, we manually define a set of n characterizing word pairs for each type of bias based on typical data biases. Table  1  shows a range of representative examples.  (Wang et al., 2020) . Wang et al. discovered that word frequency twists the bias direction, and proposed the Double-Hard Debias method.To find an intermediate subspace that can mitigate the effect of word frequency on the bias direction, Wang uses the clustering accuracy of highly biased words as an indicator to iteratively test the principal components of the word embedding space.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Double Hard Debias",
      "text": "Specifically, the Double Hard Debias method includes the following steps (taking age bias for example):\n\n(a) Let W be the vocabulary of the word embeddings we aim to debias. Pick the top biased young and elderly words W y , W e ∈ W , according to the Cosine similarity of their embeddings to the age direction computed earlier.\n\n(b) Calculate the principal components of W (measured by their projections onto the age direction) as the candidate frequency direction. Repeat steps (c)-(e) for each candidate dimension u i respectively.\n\n(c) The top biased word embeddings are mapped to an intermediate space orthogonal to u i to mitigate the frequency-related bias: w bias = w bias -(u T i w bias )u i , where w bias ∈ W y , W e . (d) Apply the Hard Debias method. The characterizing word pairs D t 1 , D t 2 , ...D tn ⊂ W are used here to substract the bias projection from the top biased word embeddings: ŵbias = Hard Debias (w bias ).\n\nThe detailed steps of Hard Debias can be found in the original paper.\n\n(e) Cluster the ŵbias , and then compute the corresponding accuracy and append to S debias .\n\nThe purpose of debiasing is to make the top biased words (e.g., words about young and elderly) less separable. So the lower clustering accuracy in S debias , the better debiasing effect that removing u i has (i.e. the top biased words are mixed up). In other words, we filter out the u i that causes the most significant decrease in the clustering accuracy and then remove it. Let j = arg min i S debias , we get the frequency-debiased word embeddings : w = w -(u T j w)u j , where w ∈ W . Then, apply the Hard Debias method to w to obtain the output age-debiased word embedding: ŵ = Hard Debias (w ).\n\nThe algorithm operates on the five types of bias sequentially, i.e., the debiased word embedding of the first type serves as the input for the second, and so forth. Finally, we get the multibias-mitigated pre-trained word embeddings, which can be used in our proposed MMKET model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mitigating Multiple Biases In Visual Representation",
      "text": "Recent research shows that gender and age bias accounts for a noticeable portion of visual bias  (Drozdowski et al., 2020)\n\nHere the k is set to 1. As a result, the bias subspace V B becomes a bias direction --→ V B. After getting the visual bias subspace, each image representation v is debiased through: ṽ\n\nProjection Debias. In order to further mitigate the finer-grained gender and age bias in the image representation, we propose a new visual debias method, namely Projection Debias. Specifically, it projects the image representation twice into the bias subspaces (e.g., male vs. female, young vs. old) respectively. By subtracting the two projections from the original visual representation, we get the final debiased representation. Figure  3  shows the Projection Debias method on gender bias. First we use the IMDB-WIKI to define four sets of images U i , where i ∈ [1, 2, 3, 4], corresponding to the female, male, young and old respectively. The we compute the bias subspace as:\n\nwhere i ∈ [1, 2, 3, 4], u i is the first principal component of U i computed through Principal Component Analysis ( PCA). T means the transpose operation, and ⊗ means outer product operation. Then we can get the corresponding projection-debiased visual representation through:\n\nThen we get the two-step debiased visual representation v for an image.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Proposed Mmket Model",
      "text": "We outline the Multibias-mitigated and Sentiment Knowledge Enriched Transformer (MMKET) model (c.f. Figure  2 ). In the proposed framework, we apply the Transformer  (Vaswani et al., 2017)  to leverage the debiased contextual and multimodal (text and visual) clues to predict the emotions of the target utterance, due to its ability to capture the context and fast computation. The main ideas are: (1) a multi-modal encoder to create textual and visual representations of contexts and responses, including debiased word embedding (GloVe) and debiased visual representation from the pre-trained Effi-cientNet Network  (Tan and Le, 2019) . (  2 ) The text representation is enriched by sentiment knowledge.\n\n(3) The context-aware attention mechanism is proposed to effectively incorporate conversational context. (4) The text representation and non-verbal embedding are forwarded through a self-attention layer and a feed-forward sublayer to perform multimodal fusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task Definition",
      "text": "Suppose our dataset has N data-points, we can represent the i-th data as {U i j , Y i j }, U i j = (X i j , V i j ), where i ∈ {1, 2, ..., N }, j ∈ {1, 2, ..., N i }, which is a collection of {utterance, label} pairs, N denotes the number of conversations, and N i denotes the number of utterances in the i-th conversation. Each utterance consists of two modalities: text (X), video (V). We align the visual features with their corresponding tokens in the text modality. Therefore, both two modalities have the same length. Given an utterance, our task is to predict its emotion label. The objective of the task is to maximize the following function:\n\nwhere U i j-1 , ..., U i 1 denote contextual utterances and θ denotes the model parameters set. We denote the number of contextual utterances as M .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Bimodal Encoder Layer",
      "text": "We extract textual and visual features via the bimodal encoder respectively. For text representation, we use a debiased word embedding layer to convert each token t in X i into a vector representation t ∈ R d , where d denotes the size of word embedding. Moreover, the debiased GloVe embeddings (through our debiasing methods presented in Sec. 3.1) are used for initialization in the word embedding layer. Let t = Embed(t)\n\n(5)\n\nas described in the previous part, we use a sentiment embedding layer to convert each token t in the utterance into a corresponding sentiment features score S i as an additional information source vector. The resulting textual embeddings are fed into the Transformer encoders to further refine textual representation.\n\nFor the visual representation, each input video clip is scaled to 480 × 360, and the pre-trained EfficientNet  (Tan and Le, 2019)  is used to extract the features. The Transformer encoders are used to learn the visual representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sentiment Knowledge Attention",
      "text": "In philosophy and psychology, sentiment and emotion are closely related, corresponding to internal and external human affection  (Evans, 2002) . Sentiment refers to human's subjective experience and mental attitude, which involves long-term and deep human cognition  (Dolan, 2002) . Therefore, we hypothesise that the sentiment knowledge will help the task of emotion recognition. Correspondingly, we propose a sentiment knowledge attention mechanism to capture and counterpoise the sentiment representation for each token. Specifically, a gated unit is used to combine the sentiment representation and the original utterance representation.\n\nIn our model, we use a commonsense emotion lexicon NRC_VAD  (Mohammad, 2018)  as the sentiment knowledge source. The NRC Valence, Arousal, and Dominance (VAD) lexicon include a list of more than 20,000 English words and their valence, arousal, and dominance scores.\n\nFor a given word and a dimension (V/A/D), the scores range from 0 to 1.\n\nIn general, for each word token t in X i j , we only retrieve its valence values from the NRC_VAD dictionary, which is the 'positive-negative' dimension. The final sentiment knowledge representation for each text utterance X i j is a list of valence scores: [V(t 1 ),V(t 2 ),...V(t n )]. The valence scores of tokens that are not included in NRC_VAD are set to 0.5. The sentiment knowledge representation of each text utterance will be used to enrich the text representation and serve the multi-bias mitigation. The gate value g i for each token x i is calculated as follows:\n\nwhere h i is the hidden vector of token x i from the previous lstm layer, W g is a learnable linear transformation and b g is the bias. Then the attention output T is calculated as a weighted combination of sentiment enriched and original attention scores:",
      "page_start": 1,
      "page_end": 6
    },
    {
      "section_name": "Bimodal Cross Attention",
      "text": "We use a bimodal cross attention layer  (Hasan et al., 2021) , which is a multi-head self-attention mechanism, to learn the joint representation of U l and U v , U l = T i , where U l represents sentiment-enriched textual representation and U v denotes sentimentenriched visual representation. Specifically, we create corresponding sets of queries (Q l , Q v ), keys (K l , K v ), and values (V l , V v ) to learn the interaction between textual and visual modalities (U l , U v ). The modal representation and query set is attached to a multi-head cross attention layer. We also add the normalization layer and residual connections layer after each cross attention layer. Let",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classification",
      "text": "The bimodal fusion representation is gained from the bimodal cross attention layer, which is shown in Eq. 8. We then add a maxpooling layer to extract the most salient features across the time dimension and yield a one-dimensional vector. Let\n\nwhere P represents the output probability, W ∈ R d * l and b ∈ R l denote parameters, l denotes the number of classes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "5.1 Datasets IEMOCAP  (Busso et al., 2008) : A multimodal dataset containing emotional dialogues. Each video contains a single dynamic dialogue, segmented into utterances. The emotion labels of utterances include neutral, happiness, sadness, anger, frustrated, and excited.\n\nMELD  (Poria et al., 2019) : A dataset of TV show scripts collected from Friends, which is a multimodal emotion classification dataset. The emotion labels of the dataset include happiness, surprise, sadness, anger, disgust, and fear. Both datasets contain textual, visual, and acoustic information for every utterance. We only focus on the textual and visual modalities in this work. Table  2  shows the statistics of the datasets. In all our experiments, 300-dimensional GloVe is leveraged to initialize word embeddings, pre-trained EfficientNet network is used to extract the corresponding feature vectors of images. The dimensionality of hidden states is set to 300. We use adam as an optimizer with a learning rate of 0.0001 and train. The coefficient of L2 regularization is 10 -5 . The batch size is 64. The network is subjected to regularization in the form of Dropout.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Debiasing. We use k-Means clustering to verify the effectiveness of the debiasing methods. For each type of bias, we take the top 100/500/1000 of the original GloVe embeddings and 100/300/500 of the visual features by calculating their cosine similarity with the specific bias directions. Then, we cluster them into two groups and compute the  alignment accuracy for the bias. To visualize the difference, we applied tSNE projection on word embeddings and the image features.\n\nOur Proposed MMKET Model. We evaluate our proposed MMKET model on IEMOCAP and MELD, and adopt the F1 score on the test set as our evaluation metric.   6 Results and Analysis",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Effects Of Debiasing",
      "text": "Mitigating Multiple Biases in GloVe. Table  3  shows the result of K-Means clustering on the original GloVe and the debiased ones. Lower accuracy means fewer bias cues can be learned. The accuracy appears to decrease after the debiasing operation, suggesting the debias method works effectively in embeddings. More intuitively, in the upper row of Figure  4 , word embeddings are divided into two clear parts. In the lower row, the two parts have mixed up, though different biases have varied effects. Among the five proposed biases, gender and religious bias were mitigated most.\n\nLGBTQ+ bias also reduced, while racial and age bias did not decrease significantly. We speculate that the racial bias is more implicit in textual data given that the accuracy of the original GloVe is already close to 50. As for the age bias, we consider the bias words like \"old\" are widely used as unbiased meanings, i.e. \"an old tree\", \"a seven-year-old boy\", which decreased the effect of debiasing. Mitigating the racial and age bias will be left to our future work.\n\nMitigating Multiple Biases in Visual Representation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Debiased Merc Results",
      "text": "We make the first step to explore the role of bias plays in mERC tasks. Human emotions contain prejudice, so removing the bias will decrease the emotion classification accuracy, which can explain the results in Table  4  and Table  6 . Compared to the single modal results (Table  4 ), our MMKET model makes full use of the rich information in the Bimodal data and the connection between them, which greatly improves the performance of the algorithm.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Studies",
      "text": "To further investigate how the sentiment knowledge affects the debias method and mERC, we conduct extensive ablation experiments with the weight of sentiment knowledge of different values, whose results are included in Table  7 . The sentiment knowledge improves model performance significantly, but less on the debiased model. One possible reason is that biases themselves imply the emotions of humans, so mitigating biases will reduce the effect of sentiment knowledge.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we extend the types of bias in the embedding level (e.g., gender, age, race, religion, and LGBTQ+) and innovatively propose the Projection Debias to mitigate gender and age bias in visual representation. We also present a Multibias-mitigated and Sentiment Knowledge Enriched Transformer (MMKET), taking the first step to explore how the debiasing operation affects the algorithm in multimodal emotion recognition in conversation (mERC). We conduct extensive experiments to show the effectiveness of the proposed model and prove that debias operation and sentiment knowledge has a great impact on the classification performance for the task of mERC. Due to the difference of the biases, the effect of debiasing also varies, which requires further research. Our model also has a few limitations. For example, we only select to mitigate two typical visual biases, while other typles of bias are ignored. Such efforts will be left to our future work. We hope our study will benefit the development of bias mitigation in mERC and other emotion studies.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustrative example of gender bias in",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall architecture of our proposed Multibias-Mitigated and sentiment Knowledge enriched Trans-",
      "page": 3
    },
    {
      "caption": "Figure 3: shows the",
      "page": 4
    },
    {
      "caption": "Figure 3: Projection Debias.",
      "page": 5
    },
    {
      "caption": "Figure 2: ). In the proposed framework,",
      "page": 5
    },
    {
      "caption": "Figure 4: TSNE visualization of clustering top 500 most biased embeddings (a-e) and their debiased embeddings",
      "page": 7
    },
    {
      "caption": "Figure 5: tSNE visualization of top 300 most biased",
      "page": 7
    },
    {
      "caption": "Figure 4: , word embeddings are divided into",
      "page": 7
    },
    {
      "caption": "Figure 5: , the visual",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "",
          "Umm. I don't think": "so.[joy]"
        },
        {
          "driven mERC approaches produce a biased er-": "ror.\nFor example,\nsuch approaches may of-",
          "Speaker: Rachel": "U6",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "What? What are you talking",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "fer higher emotional scores on the utterances",
          "Speaker: Rachel": "about?![surprise]",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "by females than males.\nIn addition,\nthe exist-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "ing debias models mainly focus on gender or",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "",
          "Umm. I don't think": "Figure 1: An illustrative example of gender bias\nin"
        },
        {
          "driven mERC approaches produce a biased er-": "race, where multibias mitigation is still an un-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "the multimodal dialogue dataset",
          "Umm. I don't think": "(MELD). There are"
        },
        {
          "driven mERC approaches produce a biased er-": "explored task in mERC. In this work, we take",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "",
          "Umm. I don't think": "three speakers in this conversation: Rachel, Joey, and"
        },
        {
          "driven mERC approaches produce a biased er-": "the ﬁrst step to solve these issues by propos-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "Monica.\nIn the bracket at",
          "Umm. I don't think": "the end of each utterance is"
        },
        {
          "driven mERC approaches produce a biased er-": "ing a series of approaches to mitigate ﬁve typi-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "the emotion type (e.g.,",
          "Umm. I don't think": "joy, surprise, etc.)\nof\nit.\nThe"
        },
        {
          "driven mERC approaches produce a biased er-": "cal kinds of bias in textual utterances (i.e., gen-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "",
          "Umm. I don't think": "underlined sentences and the words highlighted in red"
        },
        {
          "driven mERC approaches produce a biased er-": "der, age,\nrace,\nreligion and LGBTQ+) and vi-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "",
          "Umm. I don't think": "express an apparent gender bias, and such bias is fur-"
        },
        {
          "driven mERC approaches produce a biased er-": "sual representations (i.e, gender and age), fol-",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "",
          "Speaker: Rachel": "ther ampliﬁed by Joey’s facial gestures.",
          "Umm. I don't think": ""
        },
        {
          "driven mERC approaches produce a biased er-": "lowed by a Multibias-Mitigated and sentiment",
          "Speaker: Rachel": "",
          "Umm. I don't think": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "yzhou_zhang@bit.edu.cn": "Speaker: Rachel"
        },
        {
          "yzhou_zhang@bit.edu.cn": "U0"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Monica? You gonna be very"
        },
        {
          "yzhou_zhang@bit.edu.cn": "proud of me. I just got us"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Speaker: Joey\ndates with two unbelievably"
        },
        {
          "yzhou_zhang@bit.edu.cn": "cute nurses.[joy]"
        },
        {
          "yzhou_zhang@bit.edu.cn": "U1"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Oh my![surprise]"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Speaker: Rachel"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "U2"
        },
        {
          "yzhou_zhang@bit.edu.cn": "They're male nurses."
        },
        {
          "yzhou_zhang@bit.edu.cn": "[neutral]\nSpeaker: Joey"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "U3"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Not in my head.\n[neutral]"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Speaker: Rachel"
        },
        {
          "yzhou_zhang@bit.edu.cn": "U4"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Anyway, they want to take"
        },
        {
          "yzhou_zhang@bit.edu.cn": "us out Saturday night!"
        },
        {
          "yzhou_zhang@bit.edu.cn": "What do you say?[joy]"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Speaker: Monica\nU5"
        },
        {
          "yzhou_zhang@bit.edu.cn": "Umm.  Umm. \nSpeaker: Rachel\nUmm. I don't think"
        },
        {
          "yzhou_zhang@bit.edu.cn": "so.[joy]"
        },
        {
          "yzhou_zhang@bit.edu.cn": "U6"
        },
        {
          "yzhou_zhang@bit.edu.cn": "What? What are you talking"
        },
        {
          "yzhou_zhang@bit.edu.cn": "about?![surprise]"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "Figure 1: An illustrative example of gender bias\nin"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "the multimodal dialogue dataset\n(MELD). There are"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "three speakers in this conversation: Rachel, Joey, and"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "Monica.\nIn the bracket at\nthe end of each utterance is"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "the emotion type (e.g.,\njoy, surprise, etc.)\nof\nit.\nThe"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "underlined sentences and the words highlighted in red"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "express an apparent gender bias, and such bias is fur-"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "ther ampliﬁed by Joey’s facial gestures."
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "“women are pleasant to look slim”, reveal the age"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "and gender biases."
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "Recent research has shown that pre-trained word"
        },
        {
          "yzhou_zhang@bit.edu.cn": "representations, e.g., word embeddings (in which"
        },
        {
          "yzhou_zhang@bit.edu.cn": "each word is\nrepresented as a vector\nin the se-"
        },
        {
          "yzhou_zhang@bit.edu.cn": ""
        },
        {
          "yzhou_zhang@bit.edu.cn": "mantic\nspace),\ntend to amplify the bias\nin the"
        },
        {
          "yzhou_zhang@bit.edu.cn": "data (Kurita et al., 2019; Webster et al., 2020)."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": ""
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "The male names have been proved more likely"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "to be associated with career-related terms\nthan"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "female names, by calculating the similarity be-"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "tween their embeddings\n(Caliskan et al., 2017)."
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "African-American names are also shown to be more"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "likely to be associated with unpleasant terms than"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "European-American names (Nadeem et al., 2020)."
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "Unconsciously learning such implicit biases from"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "a dataset\nthat\nis sampled from all kinds of data"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "sources,\nleads to the fact\nthat\nthe learned models"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": ""
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "may further amplify the harmful bias (such as gen-"
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": ""
        },
        {
          "data (Kurita et al., 2019; Webster et al., 2020).": "der or race) when they make decisions (Goyal et al.,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "biased error will propagate to downstream tasks.",
          "conversational context modeling, cross-modality": "interactions capturing and the use of\nsentiment"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "For example, coreference resolution systems ex-",
          "conversational context modeling, cross-modality": "knowledge, we propose a Muiltibiases Mitigated"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "hibit a gender bias due to the use of biased word",
          "conversational context modeling, cross-modality": "and sentiment Knowledge Enriched Transformer"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "embeddings (Rudinger et al., 2018). Facial recogni-",
          "conversational context modeling, cross-modality": "(MMKET) as a uniﬁed framework. Speciﬁcally, it"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "tion applications have also been proved to perform",
          "conversational context modeling, cross-modality": "is a bimodal Transformer involving a contextual"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "worse for the inter-sectional group “darker females”",
          "conversational context modeling, cross-modality": "attention layer\nto capture the contextual\ninterac-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "than for either darker individuals or females (Buo-",
          "conversational context modeling, cross-modality": "tions, a bimodal cross-attention layer to capture the"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "lamwini and Gebru, 2018).",
          "conversational context modeling, cross-modality": "cross-modal interactions, and a sentiment attention"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "In view that human language is multi-modal in",
          "conversational context modeling, cross-modality": "layer\nto enrich the debiased representation with"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "nature, human bias also exists in multimodal con-",
          "conversational context modeling, cross-modality": "sentiment knowledge."
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "versations, e.g., textual and visual utterances. Fig-",
          "conversational context modeling, cross-modality": "Empirical evaluation has been carried out on"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "ure 1 shows an example of\nthe gender bias in a",
          "conversational context modeling, cross-modality": "two benchmark datasets, and the experimental re-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "multimodal dialogue dataset 1. Joey makes an asso-",
          "conversational context modeling, cross-modality": "sults shows that the proposed multimodal debiasing"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "ciation with a beautiful female nurse and expresses",
          "conversational context modeling, cross-modality": "methods can effectively mitigate the corresponding"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "a signiﬁcant smile when Rachel says “cute nurse”,",
          "conversational context modeling, cross-modality": "biases. We also prove that debiasing the represen-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "but when Rachel says “they are male nurses”, he",
          "conversational context modeling, cross-modality": "tation of multimodal utterances has a remarkable"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "shows a disappointed-looking facial expression, al-",
          "conversational context modeling, cross-modality": "impact on the performance of mERC models."
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "though his textual response seems neutral.",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "2\nGeneration of Bias"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "Therefore, human bias naturally resides in the",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "multimodal expression of emotions in conversa-",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "Models and algorithms have never independently"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "tions.",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "created bias. Social bias is exhibited in multiple"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "There has been a great body of literature in de-",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "components of a NLP system, including the train-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "biasing for computer vision (Buolamwini and Ge-",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "ing corpus, pre-trained models (e.g., word embed-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "bru, 2018) and pre-trained language models (Wang",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "dings), and algorithms themselves (Caliskan et al.,"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "et al., 2020). However, the existing debias models",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "2017; Garg et al., 2018)."
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "mainly focus on only one kind of prejudice, e.g.,",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "gender or race, where multibias mitigation is still",
          "conversational context modeling, cross-modality": "Datasets: The Soil of Bias.\nThe dataset is the ba-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "an unexplored task in mERC. This leaves us with a",
          "conversational context modeling, cross-modality": "sis of model training. The bias in the dataset comes"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "research question: Whether the current data-driven",
          "conversational context modeling, cross-modality": "from the unbalanced dataset samples and biased"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "multi-modal emotion recognition in conversations",
          "conversational context modeling, cross-modality": "labels. For example, gender bias manifests itself in"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "approaches produce a biased error or not?",
          "conversational context modeling, cross-modality": "training data that features more examples of men"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "To answer this question, we ﬁrst propose a series",
          "conversational context modeling, cross-modality": "than women, an unbalanced dataset. In the process"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "of approaches for debiasing multiple types of bias",
          "conversational context modeling, cross-modality": "of label annotation, the annotators will transfer per-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "in multimodal (i.e.,\ntextual and visual) conversa-",
          "conversational context modeling, cross-modality": "sonal bias to the data, where the algorithm absorbs,"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "tions. For textual utterances, we propose mitigating",
          "conversational context modeling, cross-modality": "thus produces a biased model. Sometimes, such"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "ﬁve types of bias, including gender, age, race, reli-",
          "conversational context modeling, cross-modality": "bias is due to a lack of domain expertise (Plank"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "gion, and LGBTQ+ in word embedding.",
          "conversational context modeling, cross-modality": "et al., 2014) or preconceived notions and stereo-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "For\nvisual\nutterances,\nwe\nﬁrst\npropose\na",
          "conversational context modeling, cross-modality": "types held by the annotators (Sap et al., 2019)."
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "subspace-projection-based debiasing approach to",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "Bias in Word Embeddings.\nWord embeddings"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "mitigate two typical visual biases, i.e., gender and",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "are often trained from large and human-created"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "age. It constructs a subspace for each type of visual",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "corpora that contain multifarious biased raw data."
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "bias and identiﬁes the type of bias in the visual",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "Recent\nliterature has demonstrated that gender"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "representation by projecting the representation into",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "bias is encoded in word embeddings (May et al.,"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "the corresponding subspace.",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "2019). For example, Bolukbasi highlights that “pro-"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "To incorporate the proposed multimodal debi-",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "grammer” is more closely associated with “man”"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "asing methods into the mERC task that\ninvolves",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "while “homemaker” is more closely associated"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "1Trigger Warning: This paper contains examples of biases",
          "conversational context modeling, cross-modality": "with “woman” in word2vec embeddings trained on"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "and stereotypes seen in society and language representations.",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "the Google News dataset (Bolukbasi et al., 2016)."
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "These examples may be potentially triggering and offensive.",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "",
          "conversational context modeling, cross-modality": "And word embeddings connect medical doctors"
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "These examples are meant to bring light to and mitigate these",
          "conversational context modeling, cross-modality": ""
        },
        {
          "2019; Srinivasan and Bisk, 2021). In addition, the": "biases, and it is not an endorsement.",
          "conversational context modeling, cross-modality": "more frequently to male pronouns than female pro-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: shows a range of representative",
      "data": [
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "the corresponding bias."
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "nouns (Caliskan et al., 2017). Furthermore, pre-\nple types of bias."
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "trained word embeddings are often used without"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "Hard Debias (Bolukbasi et al., 2016).\nHard De-"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "access to the original data.\nSocial bias in word"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "bias is a commonly adopted debiasing strategy in"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "embeddings will propagate to downstream tasks,"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "NLP. It projects a pre-trained word embedding vec-"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "which can further amplify social bias. Studies show"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "tor into a subspace orthogonal to an inferred bias"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "that machine translation systems tend to link occu-"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "subspace (i.e., direction of a particular type of bias),"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "pations to their stereotypical gender, e.g., linking"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "which is constructed based on a set of pre-deﬁned"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "“doctor” to “he” and “nurse” to “she” (Prates et al.,"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "word pairs (e.g., young vs. old) characterizing the"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "2019)."
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "bias."
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "To extend it to multiple types of bias mitigation,"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "3\nDebiasing Methods"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "we manually deﬁne a set of n characterizing word"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "pairs for each type of bias based on typical data\n3.1\nMitigating Multiple Biases in GloVe"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "biases. Table 1 shows a range of\nrepresentative"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "The\nrecent debiasing models\n(Bolukbasi\net\nal.,"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "examples."
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "2016; Wang et al., 2020) have only focused on"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "Double Hard\nDebias\n(Wang\net\nal.,\n2020).\nremoving gender bias in pre-trained word embed-"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "dings, particularly GloVe (Pennington et al., 2014),\nWang et al. discovered that word frequency twists"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "which has\nsurfaced several\nsocial biases\n(Spli-\nthe bias direction, and proposed the Double-Hard"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "ethöver and Wachsmuth, 2021). In this paper, we\nDebias method.To ﬁnd an intermediate subspace"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "propose to mitigate ﬁve types of biases in GloVe\nthat can mitigate the effect of word frequency on"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "embeddings,\ni.e., gender, race, religion,\nthe bias direction, Wang uses the clustering accu-"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "age, and LGBTQ+. Methodologically, we extend\nracy of highly biased words as an indicator to iter-"
        },
        {
          "Table 1:\nThe ﬁve pre-deﬁned sets of word pairs, where the main difference between each pair of words captures": "the existing Double-Hard Debias method, to multi-\natively test the principal components of the word"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "embedding space.": "Speciﬁcally,\nthe Double Hard Debias method",
          "two methods: Visual Hard Debias and Projection": "Debias methods. The Visual Hard Debias method"
        },
        {
          "embedding space.": "includes the following steps (taking age bias for",
          "two methods: Visual Hard Debias and Projection": "can mitigate the superﬁcial gender and age bias in"
        },
        {
          "embedding space.": "example):",
          "two methods: Visual Hard Debias and Projection": "visual representation. Then we devise the Projec-"
        },
        {
          "embedding space.": "(a) Let W be the vocabulary of the word embed-",
          "two methods: Visual Hard Debias and Projection": "tion Debias method to further mitigate ﬁner-grained"
        },
        {
          "embedding space.": "dings we aim to debias. Pick the top biased young",
          "two methods: Visual Hard Debias and Projection": "visual bias."
        },
        {
          "embedding space.": "and elderly words Wy, We ∈ W , according to the",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "Visual Hard Debias.\nWe assume that, for each"
        },
        {
          "embedding space.": "Cosine similarity of their embeddings to the age",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "type of visual bias, there is a pre-deﬁned set of n im-"
        },
        {
          "embedding space.": "direction computed earlier.",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "age pairs V1, V2, ..., Vn ∈ S (e.g., male-female or"
        },
        {
          "embedding space.": "(b) Calculate the principal components of W",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "young-old), which represent the bias. The images"
        },
        {
          "embedding space.": "(measured by their projections onto the age direc-",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "are selected randomly from IMDB-WIKI (Rothe"
        },
        {
          "embedding space.": "tion) as the candidate frequency direction. Repeat",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "et al., 2018), a publicly available face image dataset"
        },
        {
          "embedding space.": "re-\nsteps (c)-(e) for each candidate dimension ui",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "with gender and age labels. Let (cid:126)v denote an im-"
        },
        {
          "embedding space.": "spectively.",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "age’s visual representation. Let ui = (cid:80)\np/|Vi|"
        },
        {
          "embedding space.": "(c) The top biased word embeddings are mapped",
          "two methods: Visual Hard Debias and Projection": "p∈Vi"
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "in\nbe the mean of the image representations of Vi"
        },
        {
          "embedding space.": "to miti-\nto an intermediate space orthogonal to ui",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "the pre-deﬁned image set. The visual bias subspace"
        },
        {
          "embedding space.": "gate the frequency-related bias: w(cid:48)",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "bias = wbias −",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "V B is spanned by the ﬁrst k(≥ 1) eigen-vectors of"
        },
        {
          "embedding space.": "(uT",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "i wbias)ui, where wbias ∈ Wy, We.",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "V C), by applying Singular Value Decomposition"
        },
        {
          "embedding space.": "(d) Apply\nthe Hard Debias method.\nThe",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "(SVD) on it."
        },
        {
          "embedding space.": "characterizing word pairs Dt1, Dt2, ...Dtn ⊂ W",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "are\nused\nhere\nto\nsubstract\nthe\nbias\nprojection",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "from the top biased word embeddings:\nwbias =",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "m(cid:88) i\n(cid:88) v\nV C :=\n(1)\n((cid:126)v − ui)T ((cid:126)v − ui)/|Vi|"
        },
        {
          "embedding space.": "Hard Debias (w(cid:48)\nThe\ndetailed\nsteps\nof",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "bias).",
          "two methods: Visual Hard Debias and Projection": "=1\n∈Vi"
        },
        {
          "embedding space.": "Hard Debias can be found in the original paper.",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "(e) Cluster the ˆwbias, and then compute the cor-",
          "two methods: Visual Hard Debias and Projection": "Here the k is set to 1. As a result, the bias subspace"
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "−\n−\n→"
        },
        {
          "embedding space.": "responding accuracy and append to Sdebias.",
          "two methods: Visual Hard Debias and Projection": "V B becomes a bias direction\nV B. After getting"
        },
        {
          "embedding space.": "The purpose of debiasing is to make the top bi-",
          "two methods: Visual Hard Debias and Projection": "the visual bias subspace, each image representation"
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "−\n−\n→\n−\n−\n→"
        },
        {
          "embedding space.": "ased words (e.g., words about young and elderly)",
          "two methods: Visual Hard Debias and Projection": "V BT · (cid:126)v)"
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "v is debiased through: ˜v = (cid:126)v − (\nV B."
        },
        {
          "embedding space.": "less separable. So the lower clustering accuracy in",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "the better debiasing effect\nthat removing\nSdebias,",
          "two methods: Visual Hard Debias and Projection": "Projection Debias.\nIn order to further mitigate"
        },
        {
          "embedding space.": "the top biased words are mixed up).\nui has (i.e.",
          "two methods: Visual Hard Debias and Projection": "the ﬁner-grained gender and age bias in the image"
        },
        {
          "embedding space.": "that causes the\nIn other words, we ﬁlter out the ui",
          "two methods: Visual Hard Debias and Projection": "representation, we propose a new visual debias"
        },
        {
          "embedding space.": "most signiﬁcant decrease in the clustering accu-",
          "two methods: Visual Hard Debias and Projection": "method, namely Projection Debias. Speciﬁcally, it"
        },
        {
          "embedding space.": "racy and then remove it. Let j = arg mini Sdebias,",
          "two methods: Visual Hard Debias and Projection": "projects the image representation twice into the bias"
        },
        {
          "embedding space.": "we get the frequency-debiased word embeddings",
          "two methods: Visual Hard Debias and Projection": "subspaces (e.g., male vs.\nfemale, young vs. old)"
        },
        {
          "embedding space.": ": w(cid:48) = w − (uT\nThen,",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "j w)uj, where w ∈ W .",
          "two methods: Visual Hard Debias and Projection": "respectively. By subtracting the two projections"
        },
        {
          "embedding space.": "apply the Hard Debias method to w(cid:48)\nto obtain",
          "two methods: Visual Hard Debias and Projection": "from the original visual representation, we get the"
        },
        {
          "embedding space.": "w =\nthe output age-debiased word embedding:",
          "two methods: Visual Hard Debias and Projection": "ﬁnal debiased representation. Figure 3 shows the"
        },
        {
          "embedding space.": "Hard Debias (w(cid:48)).",
          "two methods: Visual Hard Debias and Projection": "Projection Debias method on gender bias."
        },
        {
          "embedding space.": "The algorithm operates on the ﬁve types of bias",
          "two methods: Visual Hard Debias and Projection": "First we use the IMDB-WIKI to deﬁne four sets"
        },
        {
          "embedding space.": "sequentially, i.e., the debiased word embedding of",
          "two methods: Visual Hard Debias and Projection": "of images Ui, where i ∈ [1, 2, 3, 4], corresponding"
        },
        {
          "embedding space.": "the ﬁrst type serves as the input for the second, and",
          "two methods: Visual Hard Debias and Projection": "to the female, male, young and old respectively."
        },
        {
          "embedding space.": "so forth. Finally, we get\nthe multibias-mitigated",
          "two methods: Visual Hard Debias and Projection": "The we compute the bias subspace as:"
        },
        {
          "embedding space.": "pre-trained word embeddings, which can be used",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "in our proposed MMKET model.",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "(2)\nBi = (cid:126)uT\ni ⊗ (cid:126)ui"
        },
        {
          "embedding space.": "3.2\nMitigating Multiple Biases in Visual",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "where i ∈ [1, 2, 3, 4], (cid:126)ui is the ﬁrst principal compo-"
        },
        {
          "embedding space.": "Representation",
          "two methods: Visual Hard Debias and Projection": ""
        },
        {
          "embedding space.": "",
          "two methods: Visual Hard Debias and Projection": "nent of Ui computed through Principal Component"
        },
        {
          "embedding space.": "Recent research shows that gender and age bias ac-",
          "two methods: Visual Hard Debias and Projection": "Analysis ( PCA). T means the transpose operation,"
        },
        {
          "embedding space.": "counts for a noticeable portion of visual bias (Droz-",
          "two methods: Visual Hard Debias and Projection": "and ⊗ means outer product operation. Then we can"
        },
        {
          "embedding space.": "dowski et al., 2020). To mitigate them, we propose",
          "two methods: Visual Hard Debias and Projection": "get\nthe corresponding projection-debiased visual"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is a collection of {utterance, label} pairs, N de-": "notes the number of conversations, and Ni denotes"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "the number of utterances in the i-th conversation."
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "Each utterance consists of two modalities:\ntext (X),"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": ""
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "video (V). We align the visual features with their"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "corresponding tokens in the text modality. There-"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "fore, both two modalities have the same length."
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "Given an utterance, our task is to predict its emo-"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "tion label. The objective of the task is to maximize"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": "the following function:"
        },
        {
          "is a collection of {utterance, label} pairs, N de-": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the task of emotion recognition. Correspondingly,": "we propose a sentiment knowledge attention mech-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "The bimodal fusion representation is gained from"
        },
        {
          "the task of emotion recognition. Correspondingly,": "anism to capture and counterpoise the sentiment",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "the bimodal cross attention layer, which is shown"
        },
        {
          "the task of emotion recognition. Correspondingly,": "representation for each token. Speciﬁcally, a gated",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "in Eq. 8. We then add a maxpooling layer to extract"
        },
        {
          "the task of emotion recognition. Correspondingly,": "unit is used to combine the sentiment representa-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "the most salient features across the time dimension"
        },
        {
          "the task of emotion recognition. Correspondingly,": "tion and the original utterance representation.",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "and yield a one-dimensional vector. Let"
        },
        {
          "the task of emotion recognition. Correspondingly,": "In our model, we use\na\ncommonsense\nemo-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "tion lexicon NRC_VAD (Mohammad, 2018) as the",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "(9)\nMl,v = MaxPooling(Ml,v)"
        },
        {
          "the task of emotion recognition. Correspondingly,": "sentiment knowledge source. The NRC Valence,",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "Arousal, and Dominance (VAD) lexicon include a",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "list of more than 20,000 English words and their",
          "4.5\nClassiﬁcation": "(10)\nP = softmax(Ml,vW + b)"
        },
        {
          "the task of emotion recognition. Correspondingly,": "valence, arousal, and dominance scores.",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "where P represents the output probability, W ∈"
        },
        {
          "the task of emotion recognition. Correspondingly,": "For a given word and a dimension (V/A/D), the",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "Rd∗l and b ∈ Rl denote parameters, l denotes the"
        },
        {
          "the task of emotion recognition. Correspondingly,": "scores range from 0 to 1.",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "number of classes."
        },
        {
          "the task of emotion recognition. Correspondingly,": "In general, for each word token t in X i",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "j, we only",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "retrieve its valence values from the NRC_VAD dic-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "5\nExperiments"
        },
        {
          "the task of emotion recognition. Correspondingly,": "tionary, which is the ‘positive-negative’ dimension.",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "The ﬁnal sentiment knowledge representation for",
          "4.5\nClassiﬁcation": "5.1\nDatasets"
        },
        {
          "the task of emotion recognition. Correspondingly,": "each text utterance X i\nis a list of valence scores:",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "j",
          "4.5\nClassiﬁcation": "IEMOCAP\n(Busso et al., 2008): A multimodal"
        },
        {
          "the task of emotion recognition. Correspondingly,": "to-\n[V(t1),V(t2),...V(tn)]. The valence scores of",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "dataset containing emotional dialogues. Each video"
        },
        {
          "the task of emotion recognition. Correspondingly,": "kens that are not included in NRC_VAD are set to",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "contains a single dynamic dialogue, segmented into"
        },
        {
          "the task of emotion recognition. Correspondingly,": "0.5. The sentiment knowledge representation of",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "utterances. The emotion labels of utterances in-"
        },
        {
          "the task of emotion recognition. Correspondingly,": "each text utterance will be used to enrich the text",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "clude neutral, happiness, sadness, anger, frustrated,"
        },
        {
          "the task of emotion recognition. Correspondingly,": "representation and serve the multi-bias mitigation.",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "and excited."
        },
        {
          "the task of emotion recognition. Correspondingly,": "is calculated as\nThe gate value gi for each token xi",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "MELD\n(Poria et al., 2019): A dataset of TV"
        },
        {
          "the task of emotion recognition. Correspondingly,": "follows:",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "show scripts collected from Friends, which is"
        },
        {
          "the task of emotion recognition. Correspondingly,": "(6)\ngi = σ(Wghi + bg)",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "a multimodal emotion classiﬁcation dataset. The"
        },
        {
          "the task of emotion recognition. Correspondingly,": "is the hidden vector of\nfrom\nwhere hi\ntoken xi",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "emotion labels of\nthe dataset\ninclude happiness,"
        },
        {
          "the task of emotion recognition. Correspondingly,": "is a learnable linear\nthe previous lstm layer, Wg",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "surprise, sadness, anger, disgust, and fear."
        },
        {
          "the task of emotion recognition. Correspondingly,": "transformation and bg is the bias. Then the attention",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "Both datasets contain textual, visual, and acous-"
        },
        {
          "the task of emotion recognition. Correspondingly,": "output (cid:126)T is calculated as a weighted combination",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "tic information for every utterance. We only focus"
        },
        {
          "the task of emotion recognition. Correspondingly,": "of sentiment enriched and original attention scores:",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "on the textual and visual modalities in this work."
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "Table 2 shows the statistics of the datasets.\nIn all"
        },
        {
          "the task of emotion recognition. Correspondingly,": "(7)\nTi = gi(cid:126)ti + (1 − gi) (cid:126)Si(cid:126)ti",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "our experiments, 300-dimensional GloVe is lever-"
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "aged to initialize word embeddings, pre-trained"
        },
        {
          "the task of emotion recognition. Correspondingly,": "4.4\nBimodal Cross Attention",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "EfﬁcientNet network is used to extract\nthe corre-"
        },
        {
          "the task of emotion recognition. Correspondingly,": "We use a bimodal cross attention layer (Hasan et al.,",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "sponding feature vectors of images. The dimen-"
        },
        {
          "the task of emotion recognition. Correspondingly,": "2021), which is a multi-head self-attention mecha-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "sionality of hidden states is set\nto 300. We use"
        },
        {
          "the task of emotion recognition. Correspondingly,": "nism, to learn the joint representation of Ul and Uv,",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "adam as an optimizer with a learning rate of 0.0001"
        },
        {
          "the task of emotion recognition. Correspondingly,": "Ul = (cid:126)Ti, where Ul represents sentiment-enriched",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "and train. The coefﬁcient of L2 regularization is"
        },
        {
          "the task of emotion recognition. Correspondingly,": "textual representation and Uv denotes sentiment-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "10-5. The batch size is 64. The network is subjected"
        },
        {
          "the task of emotion recognition. Correspondingly,": "enriched visual representation.",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "to regularization in the form of Dropout."
        },
        {
          "the task of emotion recognition. Correspondingly,": "Speciﬁcally, we create corresponding sets of",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "queries\nkeys\nand\nvalues\n(Ql, Qv),\n(Kl, Kv),",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "5.2\nEvaluation Metrics"
        },
        {
          "the task of emotion recognition. Correspondingly,": "(Vl, Vv) to learn the interaction between textual and",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "Debiasing.\nWe use k-Means clustering to verify"
        },
        {
          "the task of emotion recognition. Correspondingly,": "visual modalities (Ul, Uv). The modal representa-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "the effectiveness of\nthe debiasing methods.\nFor"
        },
        {
          "the task of emotion recognition. Correspondingly,": "tion and query set is attached to a multi-head cross",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "each type of bias, we take the top 100/500/1000 of"
        },
        {
          "the task of emotion recognition. Correspondingly,": "attention layer. We also add the normalization layer",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "the original GloVe embeddings and 100/300/500"
        },
        {
          "the task of emotion recognition. Correspondingly,": "and residual connections layer after each cross at-",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "of the visual features by calculating their cosine"
        },
        {
          "the task of emotion recognition. Correspondingly,": "tention layer. Let",
          "4.5\nClassiﬁcation": ""
        },
        {
          "the task of emotion recognition. Correspondingly,": "",
          "4.5\nClassiﬁcation": "similarity with the speciﬁc bias directions. Then,"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: K-Means clustering accuracy (%) of top",
      "data": [
        {
          "60": "",
          "40": ""
        },
        {
          "60": "40",
          "40": "20"
        },
        {
          "60": "20",
          "40": ""
        },
        {
          "60": "",
          "40": ""
        },
        {
          "60": "0",
          "40": "0"
        },
        {
          "60": "",
          "40": ""
        },
        {
          "60": "20",
          "40": "20"
        },
        {
          "60": "40",
          "40": ""
        },
        {
          "60": "",
          "40": "40"
        },
        {
          "60": "60",
          "40": ""
        },
        {
          "60": "",
          "40": ""
        },
        {
          "60": "",
          "40": ""
        },
        {
          "60": "",
          "40": ""
        },
        {
          "60": "40",
          "40": ""
        },
        {
          "60": "30",
          "40": "40"
        },
        {
          "60": "",
          "40": ""
        },
        {
          "60": "20",
          "40": ""
        },
        {
          "60": "",
          "40": "20"
        },
        {
          "60": "10",
          "40": ""
        },
        {
          "60": "0",
          "40": "0"
        },
        {
          "60": "10",
          "40": ""
        },
        {
          "60": "",
          "40": "20"
        },
        {
          "60": "20",
          "40": ""
        },
        {
          "60": "30",
          "40": "40"
        },
        {
          "60": "",
          "40": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: K-Means clustering accuracy (%) of top",
      "data": [
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "GloVe",
          "Top100": "100.0",
          "Top500": "99.9",
          "Top1000": "99.7"
        },
        {
          "Dataset": "",
          "# dialogues.": "100",
          "# utterances.": "4810",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "Gender-debiased GloVe",
          "Top100": "86.0",
          "Top500": "68.7",
          "Top1000": "55.3"
        },
        {
          "Dataset": "IEMOCAP",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        },
        {
          "Dataset": "",
          "# dialogues.": "20",
          "# utterances.": "1000",
          "Embeddings": "GloVe",
          "Top100": "100.0",
          "Top500": "100.0",
          "Top1000": "99.3"
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "Age-debiased GloVe",
          "Top100": "100.0",
          "Top500": "99.2",
          "Top1000": "98.9"
        },
        {
          "Dataset": "",
          "# dialogues.": "31",
          "# utterances.": "1623",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "GloVe",
          "Top100": "86.5",
          "Top500": "75.3",
          "Top1000": "54.5"
        },
        {
          "Dataset": "",
          "# dialogues.": "1039",
          "# utterances.": "9989",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "Race-debiased GloVe",
          "Top100": "86.5",
          "Top500": "75.1",
          "Top1000": "54.4"
        },
        {
          "Dataset": "MELD",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        },
        {
          "Dataset": "",
          "# dialogues.": "114",
          "# utterances.": "1109",
          "Embeddings": "GloVe",
          "Top100": "99.5",
          "Top500": "95.7",
          "Top1000": "96.6"
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "Religion-debiased GloVe",
          "Top100": "97.0",
          "Top500": "86.8",
          "Top1000": "81.6"
        },
        {
          "Dataset": "",
          "# dialogues.": "280",
          "# utterances.": "2610",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "GloVe",
          "Top100": "100.0",
          "Top500": "99.7",
          "Top1000": "99.3"
        },
        {
          "Dataset": "",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "LGBTQ+-debiased GloVe",
          "Top100": "94.5",
          "Top500": "90.7",
          "Top1000": "91.1"
        },
        {
          "Dataset": "Table 2: The data statistics of IEMOCAP and MELD.",
          "# dialogues.": "",
          "# utterances.": "",
          "Embeddings": "",
          "Top100": "",
          "Top500": "",
          "Top1000": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: K-Means clustering accuracy (%) of top",
      "data": [
        {
          "100/500/1000 biased words.": ""
        },
        {
          "100/500/1000 biased words.": "Mitigated Bias"
        },
        {
          "100/500/1000 biased words.": "None"
        },
        {
          "100/500/1000 biased words.": "Gender"
        },
        {
          "100/500/1000 biased words.": ""
        },
        {
          "100/500/1000 biased words.": "Race"
        },
        {
          "100/500/1000 biased words.": "Age"
        },
        {
          "100/500/1000 biased words.": "Religion"
        },
        {
          "100/500/1000 biased words.": "LGBTQ+"
        },
        {
          "100/500/1000 biased words.": "5 Biases"
        },
        {
          "100/500/1000 biased words.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: K-Means clustering accuracy (%) of top",
      "data": [
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "6.1\nEffects of Debiasing"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "Mitigating Multiple Biases in GloVe.\nTable 3"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "shows the result of K-Means clustering on the orig-"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "inal GloVe and the debiased ones. Lower accuracy"
        },
        {
          "6\nResults and Analysis": "means fewer bias cues can be learned. The accu-"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "racy appears to decrease after the debiasing oper-"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "ation, suggesting the debias method works effec-"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "tively in embeddings. More intuitively, in the upper"
        },
        {
          "6\nResults and Analysis": ""
        },
        {
          "6\nResults and Analysis": "row of Figure 4, word embeddings are divided into"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 7: Analysis of the weight of sentiment knowl-",
      "data": [
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "MELD\n55.93\n56.35+0.42\n56.24+0.31\n56.09+0.16"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "IEMOCAP\n57.60\n58.29+0.69\n57.96+0.36\n57.22−0.37"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Debiased-MELD\n54.14\n54.42+0.28\n54.31+0.17\n54.25+0.11"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Debiased-IEMOCAP\n56.37\n56.57+0.20\n56.40+0.03\n56.38+0.01"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Table 7: Analysis of\nthe weight of\nsentiment knowl-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "edge from 0 to 0.7."
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "sentiment knowledge of different values, whose re-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "sults are included in Table 7. The sentiment knowl-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "edge improves model performance signiﬁcantly,"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "but less on the debiased model. One possible rea-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "son is that biases themselves imply the emotions of"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "humans, so mitigating biases will reduce the effect"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "of sentiment knowledge."
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "7\nConclusion"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "In this work, we extend the types of bias in the em-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "bedding level (e.g., gender, age, race, religion, and"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "LGBTQ+) and innovatively propose the Projection"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Debias to mitigate gender and age bias in visual rep-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "resentation. We also present a Multibias-mitigated"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "and Sentiment Knowledge Enriched Transformer"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "(MMKET),\ntaking the ﬁrst step to explore how"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "the debiasing operation affects the algorithm in"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "multimodal emotion recognition in conversation"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "(mERC). We conduct extensive experiments\nto"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "show the effectiveness of the proposed model and"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "prove that debias operation and sentiment knowl-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "edge has a great impact on the classiﬁcation perfor-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "mance for the task of mERC. Due to the difference"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "of the biases,\nthe effect of debiasing also varies,"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "which requires further research. Our model also"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "has a few limitations. For example, we only select"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "to mitigate two typical visual biases, while other"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "typles of bias are ignored. Such efforts will be left"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "to our future work. We hope our study will beneﬁt"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "the development of bias mitigation in mERC and"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "other emotion studies."
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Acknowledgements"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "This\nresearch was\nsupported\nin\npart\nby Natu-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "ral Science Foundation of Beijing (grant number:"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "4222036) and Huawei Technologies (grant num-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "ber: TC20201228005). This work was supported"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "by National Science Foundation of China under"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "grant No. 62006212, the fund of State Key Lab. for"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Novel Software Technology in Nanjing University"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "(grant No. KFKT2021B41), and the Industrial Sci-"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "ence and Technology Research Project of Henan"
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": "Province (grant No. 222102210031)."
        },
        {
          "Dataset\n0\n0.3\n0.5\n0.7": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "and Ross Girshick. 2016. Seeing through the human"
        },
        {
          "References": "Tolga\nBolukbasi,\nKai-Wei\nChang,\nJames Y Zou,",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "reporting bias: Visual classiﬁers from noisy human-"
        },
        {
          "References": "Venkatesh\nSaligrama,\nand Adam T Kalai.\n2016.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "centric labels.\nIn Proc. of CVPR, pages 2930–2939."
        },
        {
          "References": "Man is\nto computer programmer\nas woman is\nto",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "Ad-\nhomemaker?\ndebiasing word\nembeddings.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Saif Mohammad. 2018. Obtaining reliable human rat-"
        },
        {
          "References": "vances in neural information processing systems.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "ings of valence, arousal, and dominance for 20,000"
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "english words.\nIn Proc. of ACL."
        },
        {
          "References": "Joy Buolamwini\nand Timnit Gebru. 2018.\nGender",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Moin\nNadeem,\nAnna\nBethke,\nand\nSiva\nReddy."
        },
        {
          "References": "shades:\nIntersectional accuracy disparities in com-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "2020.\nStereoset:\nMeasuring\nstereotypical\nbias"
        },
        {
          "References": "mercial gender classiﬁcation.\nIn Conference on fair-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "arXiv\npreprint\nin\npretrained\nlanguage models."
        },
        {
          "References": "ness, accountability and transparency.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "arXiv:2004.09456."
        },
        {
          "References": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Jeffrey Pennington, Richard Socher, and Christopher D"
        },
        {
          "References": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJean-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Manning. 2014. Glove: Global vectors for word rep-"
        },
        {
          "References": "nette N Chang,\nSungbok Lee,\nand\nShrikanth\nS",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "resentation.\nIn Proc. of EMNLP."
        },
        {
          "References": "Narayanan. 2008.\nIemocap:\nInteractive emotional",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "Language\nre-\ndyadic motion\ncapture\ndatabase.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014."
        },
        {
          "References": "sources and evaluation.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Learning part-of-speech taggers with inter-annotator"
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "agreement loss.\nIn Proc. of ACL."
        },
        {
          "References": "Aylin\nCaliskan,\nJoanna\nJ\nBryson,\nand\nArvind",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "Narayanan. 2017.\nSemantics derived automatically",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "References": "from language corpora contain human-like biases.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "References": "Science.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "halcea.\n2019.\nMeld:\nA multimodal multi-party"
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "dataset for emotion recognition in conversations.\nIn"
        },
        {
          "References": "Raymond J Dolan. 2002. Emotion, cognition, and be-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Proc. of ACL."
        },
        {
          "References": "havior. science, 298(5596):1191–1194.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Marcelo OR Prates, Pedro H Avelar, and Luís C Lamb."
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "2019. Assessing gender bias in machine translation:"
        },
        {
          "References": "Pawel\nDrozdowski,\nChristian\nRathgeb,\nAntitza",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "a case study with google translate. Neural Comput-"
        },
        {
          "References": "Dantcheva, Naser Damer,\nand Christoph Busch.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "ing and Applications."
        },
        {
          "References": "2020.\nDemographic bias in biometrics: A survey",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "IEEE Transactions on\non an emerging challenge.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Rasmus Rothe, Radu Timofte, and Luc Van Gool. 2018."
        },
        {
          "References": "Technology and Society.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Deep expectation of real and apparent age from a sin-"
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "International\ngle image without\nfacial\nlandmarks."
        },
        {
          "References": "Dylan Evans. 2002. Emotion: The science of sentiment.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Journal of Computer Vision, 126(2-4):144–157."
        },
        {
          "References": "Oxford University Press, USA.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Rachel Rudinger,\nJason Naradowsky, Brian Leonard,"
        },
        {
          "References": "Nikhil Garg, Londa Schiebinger, Dan Jurafsky,\nand",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "and Benjamin Van Durme. 2018.\nGender bias\nin"
        },
        {
          "References": "James Zou. 2018. Word embeddings quantify 100",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "coreference resolution.\nIn Proc. of ACL."
        },
        {
          "References": "years of gender and ethnic stereotypes. Proceedings",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "of the National Academy of Sciences.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,"
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "and Noah A Smith. 2019. The risk of racial bias in"
        },
        {
          "References": "Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "hate speech detection.\nIn Proc. of ACL."
        },
        {
          "References": "Summers-Stay, Dhruv Batra, and Devi Parikh. 2019.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Maximilian\nSpliethöver\nand\nHenning Wachsmuth."
        },
        {
          "References": "Making the v in vqa matter: Elevating the role of",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "2021.\nBias silhouette analysis: Towards assessing"
        },
        {
          "References": "image understanding in visual question answering.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "the quality of bias metrics for word embedding mod-"
        },
        {
          "References": "International Journal of Computer Vision.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "the Thirtieth International\nels.\nIn Proceedings of"
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Joint Conference on Artiﬁcial\nIntelligence,\nIJCAI-"
        },
        {
          "References": "Md\nKamrul\nHasan,\nSangwu\nLee, Wasifur\nRah-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "21."
        },
        {
          "References": "man, Amir Zadeh, Rada Mihalcea, Louis-Philippe",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "Morency, and Ehsan Hoque. 2021. Humor knowl-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Tejas\nSrinivasan\nand Yonatan Bisk.\n2021.\nWorst"
        },
        {
          "References": "edge enriched transformer\nfor understanding multi-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "of both worlds:\nBiases\ncompound in pre-trained"
        },
        {
          "References": "modal humor.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "arXiv\npreprint\nvision-and-language\nmodels."
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "arXiv:2104.08666."
        },
        {
          "References": "Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "and Yulia Tsvetkov. 2019. Measuring bias in contex-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Mingxing Tan and Quoc Le. 2019.\nEfﬁcientnet: Re-"
        },
        {
          "References": "tualized word representations.\nIn Proceedings of the",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "thinking model scaling for convolutional neural net-"
        },
        {
          "References": "First Workshop on Gender Bias in Natural Language",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "works.\nIn Proc. of ICML."
        },
        {
          "References": "Processing.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": ""
        },
        {
          "References": "",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "References": "Chandler May, Alex Wang, Shikha Bordia, Samuel",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz"
        },
        {
          "References": "Bowman, and Rachel Rudinger. 2019. On measur-",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "Kaiser, and Illia Polosukhin. 2017. Attention is all"
        },
        {
          "References": "ing social biases in sentence encoders.\nIn Proc. of",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "information pro-\nyou need.\nIn Advances in neural"
        },
        {
          "References": "ACL.",
          "Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,": "cessing systems."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "jani, Bryan McCann, Vicente Ordonez, and Caiming"
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "Xiong. 2020. Double-hard debias: Tailoring word"
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "embeddings for gender bias mitigation.\nIn Proc. of"
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "ACL."
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "Kellie Webster, Xuezhi Wang,\nIan Tenney, Alex Beu-"
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "tel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi,"
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "and Slav Petrov. 2020.\nMeasuring and reducing"
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "arXiv\ngendered correlations in pre-trained models."
        },
        {
          "Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Ra-": "preprint arXiv:2010.06032."
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems",
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Zou",
        "Venkatesh Saligrama",
        "Adam Kalai"
      ],
      "year": "2016",
      "venue": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "Joy Buolamwini",
        "Timnit Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "authors": [
        "Aylin Caliskan",
        "Joanna Bryson",
        "Arvind Narayanan"
      ],
      "year": "2017",
      "venue": "Science"
    },
    {
      "citation_id": "5",
      "title": "Emotion, cognition, and behavior",
      "authors": [
        "J Raymond",
        "Dolan"
      ],
      "year": "2002",
      "venue": "science"
    },
    {
      "citation_id": "6",
      "title": "Demographic bias in biometrics: A survey on an emerging challenge",
      "authors": [
        "Pawel Drozdowski",
        "Christian Rathgeb",
        "Antitza Dantcheva",
        "Naser Damer",
        "Christoph Busch"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Technology and Society"
    },
    {
      "citation_id": "7",
      "title": "Emotion: The science of sentiment",
      "authors": [
        "Dylan Evans"
      ],
      "year": "2002",
      "venue": "Emotion: The science of sentiment"
    },
    {
      "citation_id": "8",
      "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "authors": [
        "Nikhil Garg",
        "Londa Schiebinger",
        "Dan Jurafsky",
        "James Zou"
      ],
      "year": "2018",
      "venue": "Word embeddings quantify 100 years of gender and ethnic stereotypes"
    },
    {
      "citation_id": "9",
      "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "authors": [
        "Yash Goyal",
        "Tejas Khot",
        "Aishwarya Agrawal",
        "Douglas Summers-Stay",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2021",
      "venue": "Humor knowledge enriched transformer for understanding multimodal humor"
    },
    {
      "citation_id": "11",
      "title": "Measuring bias in contextualized word representations",
      "authors": [
        "Keita Kurita",
        "Nidhi Vyas",
        "Ayush Pareek",
        "Alan Black",
        "Yulia Tsvetkov"
      ],
      "year": "2019",
      "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels",
      "authors": [
        "Chandler May",
        "Alex Wang",
        "Shikha Bordia",
        "Samuel Bowman",
        "Rachel Rudinger ; Ishan",
        "C Misra",
        "Margaret Lawrence Zitnick",
        "Ross Mitchell",
        "Girshick"
      ],
      "year": "2016",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "13",
      "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
      "authors": [
        "Saif Mohammad"
      ],
      "year": "2018",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "14",
      "title": "Stereoset: Measuring stereotypical bias in pretrained language models",
      "authors": [
        "Moin Nadeem",
        "Anna Bethke",
        "Siva Reddy"
      ],
      "year": "2020",
      "venue": "Stereoset: Measuring stereotypical bias in pretrained language models",
      "arxiv": "arXiv:2004.09456"
    },
    {
      "citation_id": "15",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "16",
      "title": "Learning part-of-speech taggers with inter-annotator agreement loss",
      "authors": [
        "Barbara Plank",
        "Dirk Hovy",
        "Anders Søgaard"
      ],
      "year": "2014",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "17",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "18",
      "title": "Assessing gender bias in machine translation: a case study with google translate",
      "authors": [
        "Pedro Marcelo Or Prates",
        "Luís C Avelar",
        "Lamb"
      ],
      "year": "2019",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "19",
      "title": "Deep expectation of real and apparent age from a single image without facial landmarks",
      "authors": [
        "Radu Rasmus Rothe",
        "Luc Timofte",
        "Van Gool"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Gender bias in coreference resolution",
      "authors": [
        "Rachel Rudinger",
        "Jason Naradowsky",
        "Brian Leonard",
        "Benjamin Van Durme"
      ],
      "year": "2018",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "21",
      "title": "The risk of racial bias in hate speech detection",
      "authors": [
        "Maarten Sap",
        "Dallas Card",
        "Saadia Gabriel",
        "Yejin Choi",
        "Noah Smith"
      ],
      "year": "2019",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "22",
      "title": "Bias silhouette analysis: Towards assessing the quality of bias metrics for word embedding models",
      "authors": [
        "Maximilian Spliethöver",
        "Henning Wachsmuth"
      ],
      "year": "2021",
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. Tejas Srinivasan and Yonatan Bisk. 2021. Worst of both worlds: Biases compound in pre-trained vision-and-language models",
      "arxiv": "arXiv:2104.08666"
    },
    {
      "citation_id": "23",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "Mingxing Tan",
        "Quoc Le"
      ],
      "year": "2019",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "24",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Double-hard debias: Tailoring word embeddings for gender bias mitigation",
      "authors": [
        "Tianlu Wang",
        "Xi Victoria Lin",
        "Nazneen Fatema Rajani",
        "Bryan Mccann",
        "Vicente Ordonez",
        "Caiming Xiong"
      ],
      "year": "2020",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "26",
      "title": "Measuring and reducing gendered correlations in pre-trained models",
      "authors": [
        "Kellie Webster",
        "Xuezhi Wang",
        "Ian Tenney",
        "Alex Beutel",
        "Emily Pitler",
        "Ellie Pavlick",
        "Jilin Chen",
        "Ed Chi",
        "Slav Petrov"
      ],
      "year": "2020",
      "venue": "Measuring and reducing gendered correlations in pre-trained models",
      "arxiv": "arXiv:2010.06032"
    }
  ]
}