{
  "paper_id": "2308.11673v1",
  "title": "Wears: Wearable Emotion Ai With Real-Time Sensor Data",
  "published": "2023-08-22T11:03:00Z",
  "authors": [
    "Dhruv Limbani",
    "Daketi Yatin",
    "Nitish Chaturvedi",
    "Vaishnavi Moorthy",
    "Pushpalatha M",
    "Harichandana BSS",
    "Sumit Kumar"
  ],
  "keywords": [
    "heart rate",
    "accelerometer",
    "gyroscope",
    "smartwatch",
    "emotion",
    "prediction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion prediction is the field of study to understand human emotions. Existing methods focus on modalities like text, audio, facial expressions, etc., which could be private to the user. Emotion can be derived from the subject's psychological data as well. Various approaches that employ combinations of physiological sensors for emotion recognition have been proposed. Yet, not all sensors are simple to use and handy for individuals in their daily lives. Thus, we propose a system to predict user emotion using smartwatch sensors. We design a framework to collect ground truth in real-time utilizing a mix of English and regional language-based videos to invoke emotions in participants and collect the data. Further, we modeled the problem as binary classification due to the limited dataset size and experimented with multiple machine-learning models. We also did an ablation study to understand the impact of features including Heart Rate, Accelerometer, and Gyroscope sensor data on mood. From the experimental results, Multi-Layer Perceptron has shown a maximum accuracy of 93.75 percent for pleasant-unpleasant (high/low valence classification) moods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "One of the ways to understand user emotion is to observe and model the physiological data of the user. Physiologically and mentally, feelings regulate the state of humans. Plutchik's model of emotion, known as the \"wheel of emotions\" from  [1] , describes eight basic emotions (joy, trust, fear, surprise, sadness, disgust, anger, and anticipation). He proposed that emotions have both a positive and negative valence and that the intensity of an emotion can vary. Valence and arousal are two emotional characteristics that are frequently utilized in video tagging. Valence refers to an emotion's positive or negative quality, while arousal refers to the level of emotional activation. It commonly uses a number scale to express the valence and arousal levels of the emotions depicted. James Russell developed a circumplex model and used a statistical approach to group similar emotions together in a circle  [2] .\n\nVarious approaches have been implemented for mood detection. Among them, few record speech, facial expressions, tweet messages, human walks, and many other aspects to identify mood using machine learning  [3, 4, 5] .\n\nPhysiological reactions can be used to build emotion identification systems and can provide important details about a person's emotional state. Physiological sensors are tools that can gauge the body's many physiological responses, including temperature, skin conductance, heart rate, and breathing rate. These sensors can be applied to many different tasks, such as emotion identification, where they might reveal information about a person's emotional state. ElectroCardioGram (ECG) and ElectroEncephaloGram (EEG) devices may assess the electrical activity of the heart and brain, respectively. Skin conductance, which is correlated with variations in sweat gland activity, can be measured by Galvanic Skin Response (GSR) and ElectroDermal Activity (EDA) sensors. Changes in skin temperature can be detected by temperature sensors, while variations in blood flow can be detected using Blood Volume Pulse (BVP) sensors.\n\nAlthough using any or a combination of the physiological sensors mentioned above has been shown to be the best option for developing highly accurate emotion recognition models, the sole goal of this paper is to understand the feasibility of emotion prediction from daily use devices like smartwatches non-intrusively. In this paper, we attempt to identify mood from heart rate, accelerometer, and gyroscope sensor data. These sensors can be used to create models with lower computational complexity and battery consumption and can enable real-time mood detection.\n\nA heart rate sensor often entails the use of a device, in this case, a smartwatch, that is equipped with a sensor capable of measuring the electrical activity of the heart. This sensor namely the Photoplethysmography (PPG) sensor, detects changes in blood volume in the capillaries of the wrist and uses this data to compute the heart rate. Data collection utilizing 979-8-3503-3439-5/23 /$31.00 © 2023 IEEE an accelerometer and gyroscope sensor often entails using a device equipped with these sensors, such as a smartphone, tablet, or wearable device. The accelerometer and gyroscope both measure acceleration and tilt, whereas the gyroscope also measures angular velocity. The sensors' data can be used to track physical activity.\n\nAdditionally, since all three of these sensors are readily accessible through any edge device, such as a smartwatch or fitness tracker, adding mood detection functionality to such a device can also assist users in addressing mental health issues at an early stage. The aim of this work is to deploy an application that accurately predicts the emotion of the person wearing it. Since there is no publicly available dataset with all the sensor readings of the simple smartwatch, data was collected from volunteers and utilized.\n\nIn this work, we have used a Samsung smartwatch (Galaxy Watch 4) to capture data from volunteers. The watch is capable of capturing heart rate data, accelerometer, and gyroscope readings. The data is captured while showing the volunteers a set of videos from the FilmStim  [6]  dataset in an attempt to trigger a particular emotion in them. During the initial round of data collection, it was noticed that some of the clips were already been seen by the volunteers and that films acquired directly from the open-source dataset were not evoking the desired emotions. Therefore a video dataset was generated with a collection of different regional and English clips from YouTube and FilmStim  [6] , which could elicit eight of Robert Plutchik's key emotion dimensions, based on the demographic. A total of 78 volunteer data has been collected. For data collection and mood prediction, two different application pipelines have been developed and deployed on the watch.\n\nThe paper is organized as follows: The literature review of earlier relevant publications is found in Section II. The data collection process is described in Section III. The creation of datasets and binary categorization are covered in Section IV. The main findings from the data analysis are outlined in Section V. The methods for creating and testing models are described in Section VI. The outcomes of various models and inferences are displayed in Section VII. In Section VIII, the model deployment is explained. The study is concluded in Section IX, while Section X proposes future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Recently, a number of works on emotional intelligence have been put into practice. Some of these varied efforts use inputs including text  [4] , voice  [7] , facial expressions  [8] , and other characteristics to identify emotions  [5] . While the use of speech and text as input has been found to produce approaches that are more accurate than those that rely on facial expressions  [8] , many studies have proposed a physiological sensors-based approach for mood recognition.\n\nQuiroz, Juan Carlos et al. utilized smartwatches in their work  [9]  to identify emotion from their walk and were able to achieve a maximum accuracy of 78% in the binary classification of happiness and sadness. HealthyOffice  [10]  uses the Toshiba SilmeeTM Bar Type, a prototype wearable sensor, to collect physiological information and predict employee mood. Additionally, it provides a smartphone app and a method for gathering ground truth, creates different mood models for customized and generalized mood detection, and achieves an average classification accuracy of 70.6% across 8 moods and 5 categories. Work done by Shu, L.; Yu, Y. et al.  [11]  utilizes only heart rate variable data where the mood was attempted to classify into happy, sad, and neutral and was able to achieve an accuracy of 70.4% and 52% for binary and five class classifications respectively. EmoSense  [12]  presents an ECG signal-based technique and builds a typical machine learning pipeline for 9-class emotion classification with 92.5% accuracy using the DREAMER  [23]  dataset. In a study by Merve Erknay zdemir et al.  [13] , ANN is used to classify high/low valence with a true positive rate of 69%. This work differs from other works in terms of the dataset used to trigger volunteers' emotions and the sensor data collected. Sensor data such as gyroscope reading is unique. The way the data was pre-processed for modeling is different. Also, considering the battery life, which is a very important factor for devices like a smartwatch, our solution does not require continuous data collection and can predict emotion whenever heart rate is measured by the user.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Data Collection",
      "text": "The data is collected using a Samsung Galaxy Watch across different classes of volunteers. An android wearable application with the flow mentioned in Fig.  1  has been developed and deployed on the watch to collect the user's age, gender, heart rate, accelerometer, and gyroscope sensor data, and the emotions are evaluated through videos.\n\n• Initially the volunteers were informed about the process and were given some time to be comfortable with the system. • The volunteer was made aware that his/her data was being collected via sensors. • The emotion was initially selected by the team (which the volunteer is not aware of), and the gender and age is entered by the volunteer. • Moving ahead, the data collection process is initiated, and all the accessible sensors start working except for the heart rate sensor. The heart rate sensor takes around 15-20s to start. • Once the heart rate starts off, the video is displayed and sensor data is stored in the database in real-time simultaneously. Post this, the user takes three self-assessments:\n\n• \"How much pleasant are you feeling after watching the video? Rate on a scale of 0-10\". This gives us valence value.\n\n• \"How intense is your feeling? Rate on a scale of 0-10\". This gives us arousal value. • \"Please select the emotion felt by you after watching the video\". This gives us the actual mood felt by the volunteer. The valence arousal scale's purpose is to estimate the sense of pleasure and displeasure which can be used for justification of the mood  [14] . Data from volunteers ranging from various age groups of 16-30, 31-45, and then above 45 for each gender were collected. The main aim for collecting the data from such a wide range of age groups was to primarily know the scale at which different emotions vary in terms of age. A total of 78 volunteer data has been recorded.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Data Processing A. Data Sets Formation",
      "text": "Taking motivation from the paper  [15] , the collected data was further formatted into statistical data with more features resulting in two types of datasets for ML model building, non-statistical and statistical. The non-statistical dataset has the sensor readings of that particular volunteer at a particular instance. The statistical dataset has the statistical data of heart rate, accelerometer, gyroscope meter, and other columns like age and gender. Along with statistical data such as mean, mode, median, and number of peaks recorded, heart rate variable metrics  [16]  were also determined and utilized. The following metrics were considered alongside statistical data:\n\n• SDNN, the standard deviation of time intervals between peaks(cleaned data). • RMSSD, root mean square of successive time interval difference between the peaks(raw data). • NN50, the total number of time intervals between peaks that differ by more than 50ms • pNN50, percentage of time intervals between peaks that differ by more than 50ms. • HR-range, difference between maximum and minimum heart rate readings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Binary Classification -Pleasant Vs Unpleasant",
      "text": "Since only 78 people's data was collected and used, it was not enough for the classification of 8 moods from Robert Plutchik's Wheel of Emotions. To deal with this, the data tagged with anger, sadness, disgust, and fear i.e. low valence moods, and the one tagged with joy, surprise, anticipation, and trust i.e. high valence moods was segregated as unpleasant and pleasant moods respectively  [4] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Insights From Data Analysis",
      "text": "The 16-30 age group has recorded the greatest average accelerometer, gyroscope, and heart rate readings of any age group, it has been noted. Males recorded the greatest average accelerometer and gyroscope readings, while females recorded the highest average heart rate.\n\nThe volunteer's mood was compared with the averages of the accelerometer, gyroscope and heart rate readings, and the following key insights were found:\n\n• \"Sadness\" and \"Surprise\" showed the highest and lowest accelerometer average respectively. • \"Anticipation\" and \"Trust\" showed the highest and lowest gyroscope average respectively. • \"Joy\" and \"Trust\" showed the highest and lowest mean heart rate. The overall unpleasant mood category was observed at the higher end of mean accelerometer and gyroscope readings while the pleasant category was observed at the higher end of mean heart rate readings.\n\nPost this, every data point on the valence-arousal graph in the left part of Fig.  2  was labeled with its corresponding mood type category. Two distinct clusters can be roughly distinguished.\n\nApplying the K-means algorithm, with k = 2, two welldefined clusters were identified (illustrated in the right part of Fig.  2 ). Comparing the left part of Fig.  2  and the right part of Fig.  2 , the algorithm was able to identify two clusters, which were able to represent an unpleasant and pleasant mood, respectively. Thus justifying the use of valence and arousal scales for categorizing moods.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Ml Model Building And Testing A. Train-Test Data Splitting",
      "text": "In order to avoid data imbalance, train-test splits of the data were made with an equal proportion of each target class in both",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Models",
      "text": "Logistic Regression, Decision Tree Classifier, Random Forest Classifier, Gaussian Naïve Bayes, K Nearest Neighbors, AdaBoost Classifier, Multi-Layer Perceptron(MLP), XGBoost for the non-statistical dataset, and same models in addition to SVM and Gradient Boost for statistical dataset were trained with different sets of input features after normalizing the dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Custom Accuracy Function For Non-Statistical Dataset",
      "text": "Since the models using non-statistical data were trained to predict mood at every instance of sensor reading for a particular ID (volunteer), it was identified that a conventional accuracy function was not suitable to test these models. So, a custom accuracy function was built with a naive approach. The logic was that the mood which was predicted most of the time was taken as the final predicted mood.  Table  I  shows the performance of the binary classifiers trained and tested on different sets of input features. The average accuracies of 44% and 46.5% were observed with MLP and XGBoost respectively. When using only Accelerometer, Gyroscope and Heart rate as a feature, KNN with n value = 5 showed the highest average accuracy of 56.5%.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vii. Results And Inference A. Using Non-Statistical Dataset",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Using Statistical Dataset",
      "text": "From all the binary classifiers trained and tested using statistical data, the highest accuracy performance was recorded by Random Forest as shown in Table  II . An accuracy of 70.625% was achieved when all statistical data, heart rate variables while Gyroscope reading dropped, were given as input features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Using Mlp With Statistical Data",
      "text": "MLP is a small neural network with fully connected dense layers. Fig.  3  shows the architecture of our MLP model, it had an input layer with 32 units followed by a dropout layer, whose rate is 0.5 followed by a fully connected dense layer of 8 units and finally an output layer. Similar to the Machine Learning model approach in the previous section, the MLP model was tested with various input features. The highest accuracy achieved was 93.75% while the input features were statistical data of accelerometer and gyroscope meter readings while gender was dropped. Fig.  4  and Fig.  5  indicate a decrease in loss and an increase in accuracy respectively with an increase in epochs.\n\nWhile making sure either Heart Rate data and Heart Rate Variable data or both were present, the highest accuracy recorded was 87.5% while using 1) all statistical data but dropping age and gender and 2) all statistical data but dropping age alone were input features. The results with other sets of input features are mentioned in Table  III . As depicted in Fig.  6 , higher accuracy was observed for each model using the statistical dataset as compared to that of the non-statistical dataset. EEG and ECG data have been found to be helpful for identifying emotions  [12] . Though these physiological sensors were not employed, our work performed better than a few others who did. While some works aim to categorize all opposing emotions in a binary fashion, others tend to do it in a way that is similar to ours  [13, 17, 18, 19] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Viii. Deployment",
      "text": "After all the models were successfully studied, a complete TensorFlow pipeline was built, incorporated into a different application that was identical to the one discussed earlier, and it was then deployed on the Samsung Galaxy watch. When the user initiates the app, it takes user input and records sensor data to determine the user's mood type and then displays it.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ix. Conclusion",
      "text": "As proposed, our work was able to predict the user's mood as pleasant or unpleasant for a specific duration without any use of ECG, EEG, GSR, EDA, BVP, etc., sensors data. Nine Supervised Machine Learning models and a neural network were trained on two different dataset formats and the results have been compared. Taking into consideration of being restricted to gathering and using heart rate, gyroscope, and accelerometer data through a single smartwatch on our own, our work has achieved a maximum accuracy of 93.75% with MLP and outperformed binary classifiers from few other works which are either trained on readily available EEG data and multi peripheral physiological signals or on their own physiological signals data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "X. Future Works",
      "text": "In this work, we demonstrated that with daily use devices like smartwatches, it is possible to detect user emotion nonintrusively with just the use of existing sensors. The proposed machine learning models classify data into pleasant and unpleasant categories using binary classification, with four pleasant emotions (joy, anticipation, surprise, trust) and four unpleasant emotions (anger, sadness, disgust, and fear). In the future, an extension to the model can be developed that further classifies each category into individual emotions. The majority of emotion classification or recognition makes use of EEG and ECG data. Our current model has been built on heart rate, accelerometer, and gyroscope sensor data and still achieves comparable performance. The Deep Neural Network(DNN) model, can be researched further for understanding more accurate patterns and identifying the primary emotions. This can be supported with an extended data set along with EEG and ECG data under medical supervision to improve the classification accuracy.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: has been developed",
      "page": 2
    },
    {
      "caption": "Figure 1: Data Collection App flow diagram",
      "page": 3
    },
    {
      "caption": "Figure 2: was labeled with its corresponding",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Comparing the left part of Fig. 2 and the right",
      "page": 3
    },
    {
      "caption": "Figure 2: , the algorithm was able to identify two clusters,",
      "page": 3
    },
    {
      "caption": "Figure 2: True values of valence and arousal and Clusters identified by K-means",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the architecture of our MLP model, it",
      "page": 4
    },
    {
      "caption": "Figure 3: The MLP model summary.",
      "page": 4
    },
    {
      "caption": "Figure 4: and Fig. 5 indicate a decrease in loss and an increase in",
      "page": 4
    },
    {
      "caption": "Figure 4: The training and testing losses are plotted against the epochs.",
      "page": 4
    },
    {
      "caption": "Figure 6: , higher accuracy was observed for",
      "page": 4
    },
    {
      "caption": "Figure 5: The training and testing accuracies are plotted against the epochs.",
      "page": 5
    },
    {
      "caption": "Figure 6: Bar graph representation of model performance on both datasets.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Input Features": "Acc, Gyro, Hr",
          "Logistic\nRegression": "46",
          "Decision\nTree": "35",
          "Random\nForest": "25",
          "Gaussian\nNB": "40",
          "KNN": "30",
          "AdaBoost": "45"
        },
        {
          "Input Features": "Acc, Gyro, Hr\n(without age)",
          "Logistic\nRegression": "44.5",
          "Decision\nTree": "50.5",
          "Random\nForest": "49.5",
          "Gaussian\nNB": "54",
          "KNN": "50",
          "AdaBoost": "47.5"
        },
        {
          "Input Features": "Acc, Gyro, Hr\n(without gender)",
          "Logistic\nRegression": "50",
          "Decision\nTree": "50.5",
          "Random\nForest": "47",
          "Gaussian\nNB": "47.5",
          "KNN": "55",
          "AdaBoost": "45.5"
        },
        {
          "Input Features": "Acc, Gryo, Hr\n(without age\n& gender)",
          "Logistic\nRegression": "48",
          "Decision\nTree": "50.5",
          "Random\nForest": "52.5",
          "Gaussian\nNB": "51.5",
          "KNN": "56.5",
          "AdaBoost": "48.5"
        },
        {
          "Input Features": "Acc, Gyro\n(without age\n& gender)",
          "Logistic\nRegression": "52",
          "Decision\nTree": "40",
          "Random\nForest": "44",
          "Gaussian\nNB": "51",
          "KNN": "41.5",
          "AdaBoost": "43"
        },
        {
          "Input Features": "Hr\n(without age\n& gender)",
          "Logistic\nRegression": "44.5",
          "Decision\nTree": "52.5",
          "Random\nForest": "52.5",
          "Gaussian\nNB": "52.5",
          "KNN": "53.5",
          "AdaBoost": "52.5"
        },
        {
          "Input Features": "PCA\n(3 components)",
          "Logistic\nRegression": "46.5",
          "Decision\nTree": "42.5",
          "Random\nForest": "42",
          "Gaussian\nNB": "51.5",
          "KNN": "44.5",
          "AdaBoost": "48"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "INPUT FEATURES": "Hrv, Hr ,Acc, Gyro",
          "Training Accuracy": "96.77",
          "Testing Accuracy": "68.75"
        },
        {
          "INPUT FEATURES": "Hr, Acc, Gyro",
          "Training Accuracy": "98.38",
          "Testing Accuracy": "68.75"
        },
        {
          "INPUT FEATURES": "Hrv, Acc, Gyro",
          "Training Accuracy": "93.54",
          "Testing Accuracy": "75"
        },
        {
          "INPUT FEATURES": "Hrv, Hr, Acc",
          "Training Accuracy": "88.70",
          "Testing Accuracy": "56.25"
        },
        {
          "INPUT FEATURES": "HRV, Hr, Gyro",
          "Training Accuracy": "93.54",
          "Testing Accuracy": "81.25"
        },
        {
          "INPUT FEATURES": "Hrv, Hr",
          "Training Accuracy": "87.09",
          "Testing Accuracy": "56.25"
        },
        {
          "INPUT FEATURES": "Hrv",
          "Training Accuracy": "77.41",
          "Testing Accuracy": "31.25"
        },
        {
          "INPUT FEATURES": "Hr",
          "Training Accuracy": "82.25",
          "Testing Accuracy": "56.25"
        },
        {
          "INPUT FEATURES": "Hrv, Hr, Acc, Gyro\n(without age)",
          "Training Accuracy": "91.93",
          "Testing Accuracy": "87.5"
        },
        {
          "INPUT FEATURES": "Hrv, Hr, Acc, Gyro\n(without gender)",
          "Training Accuracy": "93.54",
          "Testing Accuracy": "81.25"
        },
        {
          "INPUT FEATURES": "Hrv, Hr, Acc, Gyro\n(without age & gender)",
          "Training Accuracy": "96.77",
          "Testing Accuracy": "87.5"
        },
        {
          "INPUT FEATURES": "Hrv, Hr, Acc, Gyro\n(without median & mode)",
          "Training Accuracy": "96.77",
          "Testing Accuracy": "81.25"
        },
        {
          "INPUT FEATURES": "Hrv, Hr, Acc, Gyro\n(without age, gender,\nmedian & mode)",
          "Training Accuracy": "91.93",
          "Testing Accuracy": "62.5"
        },
        {
          "INPUT FEATURES": "Acc, Gyro",
          "Training Accuracy": "90.32",
          "Testing Accuracy": "81.25"
        },
        {
          "INPUT FEATURES": "Acc, Gyro (without gender)",
          "Training Accuracy": "85.48",
          "Testing Accuracy": "93.75"
        },
        {
          "INPUT FEATURES": "Acc, Gyro (without age)",
          "Training Accuracy": "87.09",
          "Testing Accuracy": "87.5"
        },
        {
          "INPUT FEATURES": "Acc, Gyro (without age & gender)",
          "Training Accuracy": "85.48",
          "Testing Accuracy": "87.5"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions make cities live: towards mapping emotions of older adults on urban space",
      "authors": [
        "Radoslaw Nielek",
        "Ciastek",
        "Wiesław Miroslaw & Kopeć"
      ],
      "year": "2017",
      "venue": "Emotions make cities live: towards mapping emotions of older adults on urban space",
      "doi": "1076-1079.10.1145/3106426.3109041"
    },
    {
      "citation_id": "2",
      "title": "Independence and bipolarity in the structure of current affect",
      "authors": [
        "L Feldman Barrett",
        "J Russell"
      ],
      "year": "1998",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.74.4.967"
    },
    {
      "citation_id": "3",
      "title": "Multi-Modal Emotion Recognition From Speech and Facial Expression Based on Deep Learning",
      "authors": [
        "L Cai",
        "J Dong",
        "M Wei"
      ],
      "year": "2020",
      "venue": "2020 Chinese Automation Congress (CAC)",
      "doi": "10.1109/CAC51589.2020.9327178"
    },
    {
      "citation_id": "4",
      "title": "Mining Emotions on Plutchik's Wheel",
      "authors": [
        "A Mondal",
        "S Gokhale"
      ],
      "year": "2020",
      "venue": "2020 Seventh International Conference on Social Networks Analysis, Management and Security (SNAMS)",
      "doi": "10.1109/SNAMS52053.2020.9336534"
    },
    {
      "citation_id": "5",
      "title": "Motion Reveal Emotions: Identifying Emotions From Human Walk Using Chest Mounted Smartphone",
      "authors": [
        "M Hashmi",
        "Q Riaz",
        "M Zeeshan",
        "M Shahzad",
        "M Fraz"
      ],
      "year": "2020",
      "venue": "IEEE Sensors Journal",
      "doi": "10.1109/JSEN.2020.3004399"
    },
    {
      "citation_id": "6",
      "title": "year = 2016, month = 11, pages = , title = English versions of the emotional stimuli tested by",
      "authors": [
        "Alexandre Schaefer",
        "Dexter Rong"
      ],
      "year": "2010",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "7",
      "title": "Word-Level Emotion Embedding Based on Semi-Supervised Learning for Emotional Classification in Dialogue",
      "authors": [
        "Y. -J Lee",
        "C. -Y Park",
        "H. -J Choi"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Big Data and Smart Computing (BigComp)",
      "doi": "10.1109/BIG-COMP.2019.8679196"
    },
    {
      "citation_id": "8",
      "title": "An intelligent system for facial emotion recognition",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "J Taylor",
        "S Ioannou",
        "M Wallace",
        "S Kollias"
      ],
      "year": "2005",
      "venue": "2005 IEEE International Conference on Multimedia and Expo",
      "doi": "10.1109/ICME.2005.1521570"
    },
    {
      "citation_id": "9",
      "title": "Emotion Recognition Using Smart Watch Sensor Data: Mixed-Design Study",
      "authors": [
        "Juan Quiroz",
        "Carlos"
      ],
      "year": "2018",
      "venue": "JMIR mental health",
      "doi": "10.2196/10153"
    },
    {
      "citation_id": "10",
      "title": "HealthyOffice: Mood recognition at work using smartphones and wearable sensors",
      "authors": [
        "A Zenonos",
        "A Khan",
        "G Kalogridis",
        "S Vatsikas",
        "T Lewis",
        "M Sooriyabandara"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)",
      "doi": "10.1109/PER-COMW.2016.7457166"
    },
    {
      "citation_id": "11",
      "title": "Wearable Emotion Recognition Using Heart Rate Data from a Smart Bracelet",
      "authors": [
        "L Shu",
        "Y Yu",
        "W Chen",
        "H Hua",
        "Q Li",
        "J Jin",
        "X Xu"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20030718"
    },
    {
      "citation_id": "12",
      "title": "EmoSens: Emotion Recognition based on Sensor data analysis using LightGBM",
      "year": "2022",
      "venue": "2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",
      "doi": "10.1109/CONECCT55679.2022.9865753"
    },
    {
      "citation_id": "13",
      "title": "Classification of emotional valence dimension using artificial neural networks",
      "authors": [
        "M Özdemir",
        "E Yıldırım",
        "S Yıldırım"
      ],
      "year": "2015",
      "venue": "2015 23nd Signal Processing and Communications Applications Conference (SIU)",
      "doi": "10.1109/SIU.2015.7130404"
    },
    {
      "citation_id": "14",
      "title": "Affect and Exercise written by Sandro dos Santos Ferreira Submitted",
      "year": "2019",
      "venue": "Affect and Exercise written by Sandro dos Santos Ferreira Submitted",
      "doi": "10.5772/intechopen.90056"
    },
    {
      "citation_id": "15",
      "title": "Multiclass emotion prediction using heart rate and virtual reality stimuli",
      "authors": [
        "A Bulagang",
        "J Mountstephens",
        "J Teo"
      ],
      "year": "2021",
      "venue": "J Big Data",
      "doi": "10.1186/s40537-020-00401-x"
    },
    {
      "citation_id": "16",
      "title": "An Overview of Heart Rate Variability Metrics and Norms\" Front Public Health",
      "authors": [
        "F Shaffer",
        "J Ginsberg"
      ],
      "year": "2017",
      "venue": "An Overview of Heart Rate Variability Metrics and Norms\" Front Public Health",
      "doi": "10.3389/fpubh.2017.00258"
    },
    {
      "citation_id": "17",
      "title": "Identifying Rules for Electroencephalograph (EEG) Emotion Recognition and Classification",
      "authors": [
        "E Pane",
        "M Hendrawan",
        "A Wibawa",
        "M Purnomo"
      ],
      "year": "2017",
      "venue": "2017 5th International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering (ICICI-BME)",
      "doi": "10.1109/ICICI-BME.2017.8537731"
    },
    {
      "citation_id": "18",
      "title": "Classification of emotion primitives from EEG signals using visual and audio stimuli",
      "authors": [
        "Y Das ¸demir",
        "S Yıldırım",
        "E Yıldırım"
      ],
      "year": "2015",
      "venue": "2015 23nd Signal Processing and Communications Applications Conference (SIU)",
      "doi": "10.1109/SIU.2015.7130325"
    },
    {
      "citation_id": "19",
      "title": "Robust EEG emotion classification using segment level decision fusion",
      "authors": [
        "V Rozgić",
        "S Vitaladevuni",
        "R Prasad"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2013.6637858"
    },
    {
      "citation_id": "20",
      "title": "Arousal-Valence Classification from Peripheral Physiological Signals Using Long Short-Term Memory Networks",
      "authors": [
        "M Zitouni",
        "C Park",
        "U Lee",
        "L Hadjileontiadis",
        "A Khandoker"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)",
      "doi": "10.1109/EMBC46164.2021.9630252"
    },
    {
      "citation_id": "21",
      "title": "Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-González",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/AC-CESS.2018.2883213"
    },
    {
      "citation_id": "22",
      "title": "A Comparative Study of Arousal and Valence Dimensional Variations for Emotion Recognition Using Peripheral Physiological Signals Acquired from Wearable Sensors",
      "authors": [
        "F Alskafi",
        "A Khandoker",
        "H Jelinek"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)",
      "doi": "10.1109/EMBC46164.2021.9630759"
    },
    {
      "citation_id": "23",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics",
      "doi": "10.1109/JBHI.2017.2688239"
    }
  ]
}