{
  "paper_id": "2506.15754v1",
  "title": "Explainable Speech Emotion Recognition Through Attentive Pooling: Insights From Attention-Based Temporal Localization",
  "published": "2025-06-18T07:22:47Z",
  "authors": [
    "Tahitoa Leygue",
    "Astrid Sabourin",
    "Christian Bolzmacher",
    "Sylvain Bouchigny",
    "Margarita Anastassova",
    "Quoc-Cuong Pham"
  ],
  "keywords": [
    "speaker emotion recognition",
    "affective computing",
    "multimodal learning",
    "multihead attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "State-of-the-art transformer models for Speech Emotion Recognition (SER) rely on temporal feature aggregation, yet advanced pooling methods remain underexplored. We systematically benchmark pooling strategies, including Multi-Query Multi-Head Attentive Statistics Pooling, which achieves a 3.5 percentage point macro F1 gain over average pooling. Attention analysis shows 15 percent of frames capture 80 percent of emotion cues, revealing a localized pattern of emotional information. Analysis of high-attention frames reveals that non-linguistic vocalizations and hyperarticulated phonemes are disproportionately prioritized during pooling, mirroring human perceptual strategies. Our findings position attentive pooling as both a performant SER mechanism and a biologically plausible tool for explainable emotion localization. On Interspeech 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, our approach obtained a macro F1 score of 0.3649.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech serves as humanity's primary communication medium, conveying both linguistic content and paralinguistic cues about emotions, intentions, and context. The complexities of vocal modulations (pitch, speech rate, variance) provide crucial insights into speakers' emotional states  [1] . Emotions-complex psychological and physiological responses to stimuli-are fundamental to human communication and social interaction  [2] .\n\nTraditional SER systems relied on handcrafted acoustic features-combining prosodic elements (pitch, energy, duration) and spectral descriptors (MFCCs, formants)  [3] -and used conventional classifiers such as SVMs  [4] , HMMs  [5] , and GMMs  [6] . While effective for acted datasets, these approaches underperformed with spontaneous emotions in real-world scenarios  [7] . Early integration of basic textual features  [8]  was also limited in capturing complex semantic relationships.\n\nDeep learning has revolutionized SER with end-to-end architectures that automatically learn features. Early innovations used CNNs for spectrogram analysis  [9]  and attention-enhanced LSTMs  [10] , leading to advanced models like Wav2Vec2.0  [11]  and HuBERT  [12] . Originally for speech recognition, these models now excel in various audio tasks including SER by learning powerful representations from unlabeled data. Recent developments like W2V-BERT 2.0 (600M parameters, trained on 4.5M hours of audio)  [13]  further underscore their potential for SER tasks.\n\nFor textual analysis, BERT-based architectures  [14]  excel at capturing contextual information, making them a popular choice for tasks that involve processing text derived from audio transcripts. Compared to large language models (LLMs) such as LLaMA  [15] , BERT offers a compelling trade-off between performance and computational efficiency.\n\nMultimodal approaches which combine audio representations with their corresponding textual transcripts have shown improved robustness  [16] , though determining optimal strategies for aggregating these features remains an open area of research. The integration of lexical information alongside acoustic features has consistently demonstrated superior performance in emotion recognition tasks compared to unimodal approaches. Recent architectures leverage pre-trained models for both modalities, such as Wav2Vec 2.0 for speech and BERT variants or LLMs for text, thereby benefiting from transfer learning. These approaches have proven particularly effective in the Odyssey 2024 Challenge  [17] , where multimodal systems consistently outperformed their unimodal counterparts  [18, 19] .\n\nThe transition from frame-level to utterance-level representations is a pivotal step in SER architectures, particularly in the era of self-supervised speech models. Models like Wav2Vec 2.0 have proven adept at extracting rich frame-wise features, but the challenge lies in aggregating these fine-grained temporal representations into effective utterance-level embeddings. While the temporal dynamics of speech are inherently complex, the choice of aggregation mechanisms significantly impacts the performance of downstream emotion recognition tasks.\n\nHistorically, simple pooling methods like mean, max, and downsampling gained popularity from text embeddings  [20] , but struggled with speech's complex temporal dependencies. Statistics Pooling  [21]  advanced this by incorporating higherorder moments, capturing richer patterns crucial for selfsupervised SER.\n\nA significant advancement came with Attentive Statistics (AS) Pooling  [22] , which introduced learnable parameters to dynamically weight frames, enabling better capture of longrange dependencies. This innovation led to Multi-Head variants including Self-Attentive (SA) pooling  [23]  and Multi-Head Attention (MHA) Pooling  [24] . The most recent advancement, Multi-Query Multi-Head Attentive (MQMHA) Pooling  [25] , can be viewed as a generalization of these approaches, combining multiple attention mechanisms with statistical aggregation.\n\nSpeaker identification determines who is speaking, while SER analyzes emotional content in speech. Both examine vocal characteristics, with MQMHA effectively capturing subtle acoustic features for each. For identification, attention heads focus on consistent traits like timbre; for SER, they track dynamic features such as prosody and speech rate indicating emotions. Both tasks benefit from MQMHA's detailed analysis while targeting different speech characteristics.\n\nWe present a multimodal framework for SER that leverages the MSP-Podcast corpus dataset  [26] . Developed for the Task 1 on the Interspeech 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, our framework introduces three key innovations. We first introduce a flexible architecture paired with a training procedure that gradually unfreezes pretrained audio and text encoders. This approach enables the integration of diverse pooling strategies, mitigates overfitting, and ensures computational efficiency. Secondly, we systematically evaluate various pooling mechanisms and adopt MQMHA Pooling, which consistently improves performance on the speech emotion recognition task. Finally, through attention weight analysis, we provide empirical evidence that emotional markers are temporally localized in speech, with attention patterns aligning with specific speech characteristics such as emphasized syllables. These findings advance our understanding of how temporal dynamics contribute to emotion recognition in naturalistic conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2. Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "2.1. Model Architecture",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio And Text Encoders",
      "text": "Our architecture leverages state-of-the-art pretrained models for both audio and text processing, is represented in Figure  1 . For audio encoding, we employ W2V-BERT 2.0  [13] , which has demonstrated superior performance in paralinguistic tasks through its self-supervised pretraining on large-scale speech data. We use the pretrained checkpoint available at https: //huggingface.co/facebook/w2v-bert-2.0.\n\nFor text encoding, we utilize DeBERTa v3  [27] , which offers several advantages over traditional BERT-based models for our emotion recognition task. The model's disentangled attention mechanism and enhanced positional encoding improve the capture of nuanced emotional content in transcribed speech by representing each word with two distinct vectors: one for position and one for content. While large language models like LLaMA 3  [15]  have shown impressive results in various NLP tasks, we opted for DeBERTa v3 due to its computational efficiency and proven effectiveness in emotion-specific tasks  [28] . The pretrained model is available at https:// huggingface.co/microsoft/deberta-v3-base.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Fusion",
      "text": "The fusion module concatenates the pooled embeddings from both modalities, followed by a Multi-Layer Perceptron (MLP) classifier. This approach, while simple, has shown robust performance in multimodal emotion recognition tasks and during Odyssey 2024 challenge  [17, 18, 19] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pooling Strategies",
      "text": "A key insight from our analysis of the dataset highlights the significant disparity in length between audio data and its text transcriptions. Specifically, audio sequences contain a substantially higher number of frames compared to the token count in their corresponding textual representations, as illustrated by the histograms of data lengths in Figure  2 . This stark difference underscores the unique challenges posed by the high temporal resolution of audio data, motivating our investigation into advanced pooling strategies for effectively aggregating these fine-grained frame-level features into robust utterance-level representations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Basic Pooling Methods",
      "text": "To ensure a transparent basis for evaluating the performance of our custom pooling layers, we implement standard aggregation methods that account for masking to handle variablelength sequences. This masking mechanism ensures robustness by avoiding any influence from padding tokens, thereby eliminating sources of error.\n\nEach method operates on a batch of input feature matrices X ∈ R B×T ×K , where B is the batch size, T the maximum sequence length, and K the feature size, with corresponding masking matrices M ∈ {0, 1} B×T to denote valid frames. 1. Max Pooling can be expressed as Y ∈ R B×K where:\n\n2. Average Pooling can be expressed as Y ∈ R B×K where:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Advanced Pooling Strategies",
      "text": "We implement MQMHA Pooling  [25] , which generalizes existing attention-based pooling methods. Using the same notations as above, we also introduce Q ∈ N, H ∈ N respectively the number of queries and heads.\n\nLet K ′ = K/H ∈ N, we partition X into H equal parts such that X = [X (1) , X (2) , . . . ,\n\ndenote either a linear mapping (for n = 1) or a two-layer MLP with a hidden layer of dimension p and a ReLU activation (for n = 2); q and h are the query and head indices, respectively. We sequentially apply the F (q,h) n,p (•) and discard masked layers: e\n\nẽ(q,h)\n\nWe finally apply softmax to get the attention weights, with ω (q,h) b,t being the weight associated to the score ẽ(q,h) b,t . Representation of weighted mean and standard deviation can be computed from the following equations.\n\nThe output Y ∈ R B×2QK is obtained by concatenating the outputs from all queries and heads. This formulation encompasses several existing attention mechanisms as special cases.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Protocol",
      "text": "Our training strategy employs gradual unfreezing, a technique that balances the stability-flexibility trade-off in transfer learning  [29] . Fully unfreezing all layers initially risks catastrophic forgetting (where over-parameterized models overwrite pretrained knowledge and overfit to sparse emotion labels), while freezing all but the last layers limits adaptation capacity. Gradual unfreezing navigates this compromise by incrementally exposing pretrained parameters to task-specific features, enabling controlled evolution of multimodal representations. This preserves generalizable acoustic-linguistic patterns while adapting to emotional semantics, facilitating robust convergence through curriculum-inspired parameter updates.\n\nOur protocol consists of three phases. First, we freeze both encoders and train only the classification head. This initial phase allows the fusion layer to learn cross-modal relationships while preserving the pretrained representations. Subsequently, we selectively unfreeze the upper transformer layers of both encoders, following the intuition that higher layers capture more task-specific features. Finally, we unfreeze all parameters, including the embedding layers and feature extractors. Transition between steps are triggered by an early stopping module.\n\nTo address class imbalance we employ a class-weighted focal loss:\n\nwhere pc is the model's estimated probability for the target class, αc is the class weight computed as the inverse of class frequency, and γ = 2.\n\nFor optimization, we use AdamW with weight decay λ = 0.01 and implement a linear learning rate warm-up followed by cosine decay. The learning rates are scaled progressively across the phases: η1 = 1 × 10 -5 in phase 1, η2 = 3 × 10 -6 in phase 2, and η3 = 1 × 10 -6 in phase 3.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this section, we outline our experimental design, detailing the dataset and evaluation protocol. We evaluate our approach on the challenge dataset-derived from the MSP-Podcast corpus  [26]  and featuring naturalistic emotional speech-by addressing class imbalance. To mitigate frequency bias while preserving diverse examples to learn emotions, we randomly subsample the majority classes in the training set to a maximum imbalance ratio of 8:1. This choice balances the reduction of frequency bias present in the original training set with a maximum imbalance ratio of 26:1 with the need to preserve sufficient diversity for effective emotion learning. Our final training subset comprises 49,248 utterances, 1,994 speakers, and 78 hours of recording. For robust model selection, we create a balanced development set by sampling an equal number of instances per emotion category following the approach proposed by Härm et al.  [19] , yielding 2608 utterances, 473 speakers, and 5 hours of audio. We used the original test set for final evaluation.\n\nSince test set transcripts are not provided, we generate consistent transcriptions across all splits using Whisper  [30] . We used pretrained weights available at https: //huggingface.co/openai/whisper-large-v3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Dicussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Approach",
      "text": "Initial experiments demonstrate the effectiveness of our multimodal approach compared to unimodal baselines. As shown in Table  1 , our multimodal architecture achieves a macro F1 score of 0.3559 on the development set, outperforming both the audio-only and transcript-only models. This suggests that audio and textual modalities provide complementary information for emotion recognition, with each modality capturing distinct emotional cues that contribute to better overall performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparative Pooling Analysis",
      "text": "As shown in Table  2 , attention-based pooling strategies surpass static pooling (average/statistical) baselines when optimally parameterized, though improper configurations degrade performance due to their architectural complexity. In what is following, we set parameter p = 256. Multi-head approaches are consistently configured with n = 1, whereas alternative methods employ n = 2.\n\nMQMHA with Q = 2, H = 2 achieves the highest validation performance, indicating its capacity to disentangle multimodal emotion patterns effectively. Due to time constraints, however, only the AS strategy was submitted for evaluation on the test set, despite MQMHA's superior performance. We attribute this decision to diminishing returns observed at higher query counts (Q > 2), where increased complexity risks overfitting without commensurate gains in generalization-an outcome aligned with sparse emotional label distributions.\n\n0.3514",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Localization Of Emotional Cues",
      "text": "While attention-driven classification gains suggest that models localize emotionally salient regions, it remains unclear whether such areas holistically capture emotional content. We analyze attention heatmaps (from an AS pooling layer) and their temporal dynamics to scrutinize what the model learns-specifically, whether high-attention frames align with interpretable acoustic or linguistic cues, and how this informs emotion recognition efficacy. Figure  3  visualizes these weights for a sample utterance that was correctly classified as expressing sadness. On the balanced development set, an averaged correlation of ρ = 0.20 ± 0.13 between attention weights and audio energy suggests that the attention mechanism effectively focuses on emotionally salient regions rather than merely reflecting raw acoustic intensity. Figure  4  presents the aggregated cumulative distribution of attention weights across temporal frames, revealing the model's attention allocation patterns in sequential data. The distribution exhibits a characteristic steep initial slope, where on average, 15% of frames account for 80% of the cumulative attention weight. This highly concentrated distribution suggests that emotional cues are primarily localized within specific temporal regions rather than being uniformly distributed across the utterance. The observed pattern follows a Pareto-like distribution, where a small subset of temporal frames captures the majority of the model's focus. This finding suggests an efficient information extraction mechanism, where the attention layer successfully identifies and emphasizes the most emotionally salient segments.\n\nFigure  5  shows the results of our Bayesian likelihood analysis, which calculated the frequency of attended phonemes relative to their prior corpus frequency. The analysis revealed sys- tematic patterns in the most attention-grabbing phonemes at the utterance level, which were strongly correlated with phonetic prominence and acoustic salience. We leveraged the phoneme annotations available in the MSP-Podcast corpus  [26] . The nonlinguistic marker spn (spoken noise) was exceptionally overrepresented, indicating attention weights disproportionately prioritize non-speech vocalizations (e.g., breath sounds, laughter) during pooling. Primary-stressed vowels (AW1, AY1) and diphthongs (aw) exhibited elevated ratios, consistent with their acoustic markedness (longer duration, higher intensity). Secondary stress (AE2) also showed heightened salience, corroborating the role of syllabic prominence in perceptual weighting. The attention mechanism's prioritization of hyperarticulated vowels and non-canonical phones aligns with human perceptual strategies for decoding speech in noise  [31, 32] , reinforcing its biological plausibility in SER systems.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we introduced MQMHA to Speech Emotion Recognition, demonstrating its advantages over traditional attention mechanisms in the context of the Interspeech 2025 Speech Emotion Recognition Challenge in Naturalistic Conditions Challenge. Our comprehensive evaluation showed that MQMHA outperforms Attentive Statistics and other pooling strategies while maintaining computational efficiency. This advancement is integrated into a multimodal architecture combining W2V-BERT 2.0 and DeBERTa v3 into a lightweight multimodal architecture (less than 1B parameters), supported by considerations of class imbalance and gradual unfreezing strategies. Through attention analysis, we revealed that attentionbased pooling methods effectively identifie emotionally salient regions independently of signal energy, contributing to both performance improvement and interpretability in SER systems.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: For audio encoding, we employ W2V-BERT 2.0 [13], which",
      "page": 2
    },
    {
      "caption": "Figure 1: Multimodal model architecture. “Custom Pooler”",
      "page": 2
    },
    {
      "caption": "Figure 2: Distribution of the number of frames per dataset item",
      "page": 2
    },
    {
      "caption": "Figure 2: This stark difference under-",
      "page": 2
    },
    {
      "caption": "Figure 3: visualizes these weights for a sample",
      "page": 4
    },
    {
      "caption": "Figure 3: Attention heatmap on sample 5244 0119.",
      "page": 4
    },
    {
      "caption": "Figure 4: presents the aggregated cumulative distribution of",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the results of our Bayesian likelihood anal-",
      "page": 4
    },
    {
      "caption": "Figure 4: Attention weight distribution across frames.",
      "page": 4
    },
    {
      "caption": "Figure 5: Attention phoneme salience as bayesian likelihood",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , our multimodal architecture achieves a macro F1",
      "page": 3
    },
    {
      "caption": "Table 2: , attention-based pooling strategies surpass",
      "page": 3
    },
    {
      "caption": "Table 1: Macro F1 Scores models on MSP-Podcast corpus.",
      "page": 3
    },
    {
      "caption": "Table 2: Macro F1 Scores on MSP-Podcast corpus using vari-",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akc",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "Y Chavhan",
        "M Dhore",
        "P Yesaware"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "7",
      "title": "Robust text-independent speaker identification using Gaussian mixture speaker models",
      "authors": [
        "D Reynolds",
        "R Rose"
      ],
      "year": "1995",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "8",
      "title": "The INTERSPEECH 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "ISCA"
    },
    {
      "citation_id": "9",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "Chul Min",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "10",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato"
    },
    {
      "citation_id": "13",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Seamlessm4t-massively multilingual & multimodal machine translation",
      "authors": [
        "L Barrault",
        "Y.-A Chung",
        "M Meglioli",
        "D Dale",
        "N Dong",
        "P.-A Duquenne",
        "H Elsahar",
        "H Gong",
        "K Heffernan",
        "J Hoffman"
      ],
      "year": "2023",
      "venue": "Seamlessm4t-massively multilingual & multimodal machine translation",
      "arxiv": "arXiv:2308.11596"
    },
    {
      "citation_id": "15",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "M.-W Kenton",
        "L Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of naacL-HLT"
    },
    {
      "citation_id": "16",
      "title": "The llama 3 herd of models",
      "authors": [
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Yang",
        "A Fan"
      ],
      "year": "2024",
      "venue": "The llama 3 herd of models"
    },
    {
      "citation_id": "17",
      "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Learning Alignment for Multimodal Emotion Recognition from Speech"
    },
    {
      "citation_id": "18",
      "title": "Odyssey 2024 -Speech Emotion Recognition Challenge: Dataset, Baseline Framework, and Results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Naini",
        "L Moro-Velázquez",
        "T Thebaud",
        "P Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "19",
      "title": "1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem",
      "authors": [
        "M Chen",
        "H Zhang",
        "Y Li",
        "J Luo",
        "W Wu",
        "Z Ma",
        "P Bell",
        "C Lai",
        "J Reiss",
        "L Wang",
        "P Woodland",
        "X Chen",
        "H Phan",
        "T Hain"
      ],
      "year": "2024",
      "venue": "1st Place Solution to Odyssey Emotion Recognition Challenge Task1: Tackling Class Imbalance Problem"
    },
    {
      "citation_id": "20",
      "title": "TalTech Systems for the Odyssey 2024 Emotion Recognition Challenge",
      "authors": [
        "H Härm",
        "T Alumäe"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "21",
      "title": "Self-Supervised Speech Representation Learning: A Review",
      "authors": [
        "A Mohamed",
        "H -Y. Lee",
        "L Borgholt",
        "J Havtorn",
        "J Edin",
        "C Igel",
        "K Kirchhoff",
        "S.-W Li",
        "K Livescu",
        "L Maaløe",
        "T Sainath",
        "S Watanabe"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Deep Neural Network Embeddings for Text-Independent Speaker Verification",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2017",
      "venue": "ISCA"
    },
    {
      "citation_id": "23",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Interspeech 2018, ser. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Self-attentive speaker embeddings for text-independent speaker verification",
      "authors": [
        "Y Zhu",
        "T Ko",
        "D Snyder",
        "B Mak",
        "D Povey"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Self Multi-Head Attention for Speaker Recognition",
      "authors": [
        "M India",
        "P Safari",
        "J Hernando"
      ],
      "year": "2019",
      "venue": "Self Multi-Head Attention for Speaker Recognition"
    },
    {
      "citation_id": "26",
      "title": "Multi-Query Multi-Head Attention Pooling and Inter-Topk Penalty for Speaker Verification",
      "authors": [
        "M Zhao",
        "Y Ma",
        "Y Ding",
        "Y Zheng",
        "M Liu",
        "M Xu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing",
      "authors": [
        "P He",
        "J Gao",
        "W Chen"
      ],
      "year": "2023",
      "venue": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"
    },
    {
      "citation_id": "29",
      "title": "Emotion Classification In Software Engineering Texts: A Comparative Analysis of Pre-trained Transformers Language Models",
      "authors": [
        "M Imran"
      ],
      "year": "2024",
      "venue": "Emotion Classification In Software Engineering Texts: A Comparative Analysis of Pre-trained Transformers Language Models"
    },
    {
      "citation_id": "30",
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "authors": [
        "J Howard",
        "S Ruder"
      ],
      "year": "2018",
      "venue": "Universal Language Model Fine-tuning for Text Classification"
    },
    {
      "citation_id": "31",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision"
    },
    {
      "citation_id": "32",
      "title": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2014",
      "venue": "Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing"
    },
    {
      "citation_id": "33",
      "title": "Communication of emotions in vocal expression and music performance: Different channels, same code?",
      "authors": [
        "P Juslin",
        "P Laukka"
      ],
      "year": "2003",
      "venue": "Psychological Bulletin"
    }
  ]
}