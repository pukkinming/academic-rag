{
  "paper_id": "2312.06034v1",
  "title": "Modeling Uncertainty In Personalized Emotion Prediction With Normalizing Flows",
  "published": "2023-12-10T23:21:41Z",
  "authors": [
    "Piotr Miłkowski",
    "Konrad Karanowski",
    "Patryk Wielopolski",
    "Jan Kocoń",
    "Przemysław Kazienko",
    "Maciej Zięba"
  ],
  "keywords": [
    "artificial neural networks",
    "natural language processing",
    "human profile modelling",
    "probabilistic technique"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Designing predictive models for subjective problems in natural language processing (NLP) remains challenging. This is mainly due to its non-deterministic nature and different perceptions of the content by different humans. It may be solved by Personalized Natural Language Processing (PNLP), where the model exploits additional information about the reader to make more accurate predictions. However, current approaches require complete information about the recipients to be straight embedded. Besides, the recent methods focus on deterministic inference or simple frequency-based estimations of the probabilities. In this work, we overcome this limitation by proposing a novel approach to capture the uncertainty of the forecast using conditional Normalizing Flows. This allows us to model complex multimodal distributions and to compare various models using negative log-likelihood (NLL). In addition, the new solution allows for various interpretations of possible reader perception thanks to the available sampling function. We validated our method on three challenging, subjective NLP tasks, including emotion recognition and hate speech. The comparative analysis of generalized and personalized approaches revealed that our personalized solutions significantly outperform the baseline and provide more precise uncertainty estimates. The impact on the text interpretability and uncertainty studies are presented as well. The information brought by the developed methods makes it possible to build hybrid models whose effectiveness surpasses classic solutions. In addition, an analysis and visualization of the probabilities of the given decisions for texts with high entropy of annotations and annotators with mixed views were carried out.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human affective states, including emotions, strongly depend on the individual, the stimulant eliciting them, and the associated context  [1] . Therefore, the reasoning of a person's perception based on machine learning bears a significant degree of uncertainty. It refers to the reaction to any content, including text reading. We can say that disagreements in human textual inferences are inherent  [2] . Most solutions to subjective problems in natural language processing (NLP), like recognition of emotions, hate speech, sarcasm, sense of humor, sentiment, and many others, rely on generalized perspectives. They consider only text and its single generalized interpretation. Then, the commonly used solution is to simplify multiple distinct views, i.e., annotations provided by many annotators using majority voting or other methods to achieve † These authors contributed equally to this work. a sole perception. Overall, we can identify two sources of uncertainty: (1) humans, who are unsure and imprecise in their annotations (this is a hidden factor), and (2) a community of annotators. The latter refers to discrepancies between people in understanding the problem, and perception of a given text  [3] -  [5] . The standard measures for inter-rater agreement are Krippendorff's alpha  [6]  or Fleiss' kappa  [7] . However, they provide only a single value characterizing the set of all annotations for all texts. Yet another (3) source of uncertainty: the trained model itself. It means that the model is not capable of precisely learning about concepts (what is joy or hate speech?) and relations from the available learning samples. This leads to errors and proximate reasoning. Simultaneously, emotions can be considered multidimensional objects, which requires multi-task learning  [8]  and further complicates the problem of uncertainty modeling. Most of the proposed approaches for subjective modeling in the NLP domain focus on deterministic predictions. In this work, we propose to enrich the family of emotional methods by introducing Emotional Normalizing Flow -an entirely probabilistic framework that utilizes conditional Normalizing Flows to model uncertainty. We postulate to represent the considered tasks as multivariate regression problems and represent the distribution of the outputs with conditional flows. This approach allows us to model complex multimodal distributions of multidimensional outputs. The experiments and validation were carried out on emotion detection (ten tasks) and hate speech (two tasks). We examine various choices of flow models and compare their performance with the mixture of Gaussians, showing the superiority of Emotional Normalizing Flow compared to the selected baseline. Moreover, we show that incorporating personalization into our model leads to better distribution adjustment measured with negative log-likelihood (NLL) value.\n\nTo summarize, the contributions of this work are as follows:\n\n• We introduce a novel approach for probabilistic modeling in subjective NLP-based problems; • We examine the impact of personalization on the quality of the model and show that in most of the considered experimental cases, additional information about the reader leads to better probability adjustment; • We show that our approach outperforms the standard baseline that utilizes a mixture of Gaussians;\n\n• We propose a hybrid approach utilizing Normalizing Flows and personalization that outperform previous models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Initial work on emotion recognition in the text was based mainly on frequency analysis of words defined in lexicons of emotions  [9] ,  [10] . These lexicons contained words with assigned categories of basic emotions, e.g., joy, anger, sadness  [11] ,  [12] . Emotions occurring most frequently at the lexical level were then assigned to the entire text. With the development of text classification methods based on machine learning, datasets containing texts manually annotated with emotions began to emerge  [13] -  [20] . Due to annotators' subjective perception of emotions, and thus low inter-annotator agreement, it was common to assign emotion labels to text based on majority voting  [13] ,  [18] . Based on such prepared data, text classification models were trained. Initially, such models as SVM  [21] , BiLSTM, and GRU  [22]  were used. Currently, transformer-based models such as BERT perform best in the task of emotion recognition  [18] ,  [23] . The aforementioned approaches require data for which the inter-annotator agreement is high. However, there are some data sets such as Wiki-Detox  [24] , Sentimenti  [16]  or Measuring Hate Speech  [25] , which contain an annotator identifier linked to their affective annotation. They also include multiple annotations for a given text from multiple annotators. For such data, new personalized approaches have recently been developed, in which the context of the annotator is taken into account in the model learning process  [26] -  [36] . This makes it possible, for example, to answer the question of what emotions a particular text evokes for a particular user. Recent method proposals also focus on neuro-symbolic approaches to explain decisions made  [37] , usage of large-scale pre-trained language model (PLM) for prompt-based classification tasks such as sentiment analysis and emotion detection  [38] , using recently popular large language models (LLMs)  [39] , or methods of complex persona attribute extraction  [40] . However, the methods mentioned above do not model the uncertainty associated with the community's subjective perception of emotions and the degree of indecision of the annotators themselves.\n\nIn this paper, to model uncertainty described in the Introduction we adapt the concept of Normalizing Flows. The best-known Normalizing Flow models such as NICE  [41] , RealNVP  [42] , MAF  [43] , and CNF  [44]  were originally used for density estimation and image generation tasks. These models were further extended and used as components for more sophisticated tasks or even for other domains of applications. In Computer Vision, there were proposed models such as RegFlow  [45]  for probabilistic future location prediction, Flow Plugin Network  [46] , PluGeN  [47] , and StyleFlow  [48]  models for conditional image generation. For the tabular data, recently, TreeFlow  [49]  was proposed that utilizes a combination of tree-based models with conditional Normalizing Flows to estimate uncertainty for uni-and multivariate regression problems. In terms of Natural Language Processing and Normalizing Flows, only Discrete Flow  [50]  was proposed to model character-level datasets using Normalizing Flows dedicated to the discrete data. To the best of our knowledge, no probabilistic approach has been proposed to model distributions of uncertainty in personalized natural language processing, and our Emotional Normalizing Flow is the first probabilistic model proposed for multi-task prediction of personalized emotions. III. BACKGROUND a) Generalized and Personalized Approach to Subjective NLP Problems.: In the classic approach to the task of text classification or regression, we assume a training set of the form D = {(t i , y i )} N i=1 , where t i ∈ T is the i-th text document and y i is its annotation. However, many NLP tasks, such as recognizing emotions in a text or detecting hate speech, can be subjective because each person perceives these phenomena. This leads to a situation when we can have more than one annotation y for the same text t, as different people may annotate the same texts differently. Therefore, a training set is in the form of D = {(t i , p i , y i )} N i=1 , where y i is the annotation given by person p i ∈ P for text t i ∈ T .\n\nOne approach to subjective tasks in NLP is the so-called generalized. It assumes that the model predicts the result based solely on the text and returns the same prediction for every user. Generalized models usually consist of two parts: text encoder (language model), which creates text representation e t and classifier or regressor (usually fully-connected layer) that gives prediction ŷ. However, recent studies  [28] ,  [51] ,  [52]  show that this approach should not be considered correct, as adding information about the annotator significantly improves model quality and yields better results. The approach that combines information about the text and the human is socalled personalized. Compared to the generalized, personalized model adds another component called profile extractor, that creates human representation e p . The comparison of generalized and personalized approaches is shown in Fig  1 .\n\nThere are few existing architectures  [28] ,  [51]  utilizing this fact. Still, all of them are deterministic, meaning none model uncertainty as a direct optimization of negative log-likelihood.\n\nb) Normalizing Flows.: Normalizing Flows  [53]  are a class of generative models that enables estimation of the uncertainty of prediction thanks to the access to log probability function and thus enable direct optimization of negative loglikelihood (NLL). The goal of the model is to transform base distribution p U (u) (usually Gaussians with independent components) to the complex distribution of the data p Y (y) using a series of K invertible functions that can be written as\n\nFor that purpose, Normalizing Flows utilize the change-of-variable formula and then the NLL y is given by\n\nTo specify the exact Normalizing Flow model, we need to define transformations f 1 , . . . , f K . Here, multiple models were  proposed such as NICE  [41] , RealNVP  [42] , MAF  [43]  or Continuous Normalizing Flows  [44] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Our Approach",
      "text": "In this section, we introduce Emotional Normalizing Flow -the probabilistic model for subjective uncertainty modeling in the NLP domain. The general schema of the proposed approach is provided in Fig.  2 . The model is composed of Profile extractor that is responsible for creating the representation of the person, e p , and Text encoder that creates embedding e t directly from the input text. Both components can be represented by various models (trainable and fixed), and we elaborate on this further in this section.\n\nThe extracted vectors e p and e t are further delivered to the conditional flow represented by the complex transformation function f (•). The role of the function is to transform multivariate regression outputs y to z that represents the variable in the base space, assuming given vectors, e p and e t . Formally, we have z = f (y, e p , e t ), where f is invertible with respect to  y, y = f -1 (z, e p , e t ). Moreover, the complex transformation f can be decomposed into a sequence of simple functions,\n\n, where the K is number discrete transformations. With such assumptions, the probability distribution for y that represents the distribution over the regression outputs can be calculated using the formula:\n\nwhere z 0 , . . . , z K are intermediate steps after discrete transformations, assuming z 0 := y, and z K := z. p Z (z) is the assumed base distribution for z with the known density function, usually represented by Gaussian. Consequently, we have direct access to the density function for that conditional distribution. Therefore we can calculate the likelihood function for a set of input-output pairs to evaluate the quality of the model. We can sample an infinite number of output values assuming given inputs and interpret the results. The proposed model can quickly adapt to the problems without personalization, simply skipping e p conditioning in the flow. Our approach is independent of the conditional Normalizing Flow type, and we experimentally compare the performances of the most popular models. We follow the methodology of incorporating conditional components described in  [46] . a) Profile extractor.: Vector e p contains information about the user specific to the personalization architecture used. This can include information such as the deviation of responses from the majority voice, metadata about the user, user identifier  [28] , the correlation of the text's context with historical evaluations, or other features unique to the recipient of the text. It also can be randomly initialized and tuned during the learning process by backpropagation  [51] .\n\nb) Text encoder.: In the case of e t vector, text representation is implemented using Transfomer language models. An attentional weight is assigned for a given text input, divided into individual tokens. The assigned values are then used to calculate the weighted sum of the resulting vectors  [54] . It is possible to fine-tune the language model using the loss function of the final model.\n\nc) Training the model.: To trained Emotional Normalizing Flow we use the dataset D = {(t n , p n , y n )} N n=1 , composed of t n textual input, p n features of the person, and corresponding subjective annotation y n given by the person p n for text t n . We train our model directly by optimizing the negative log-likelihood function:\n\nwhere e n,p is a vector, that represents profile of the person p n , and e n,t is an embedding of the text t n . The model can be trained in a two-stage mode or end-to-end paradigm depending on the form of Profile extractor and Text encoder.\n\nIn the first case, the embeddings e p and e t are extracted in the first stage, and parameters of the flow are trained while optimizing L. Alternatively, suppose the Profile extractor or Text encoder are represented by differentiable architectures.\n\nIn that case, the entire system can be optimized end-to-end, directly minimizing the negative log-likelihood function.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Experiments",
      "text": "In this subsection, we evaluate our approach on a set of challenging datasets, investigating the impact of adding contextual information about the person in the model. Moreover, we compare flow-based probabilistic models to a simple Gaussian Mixture Model. Then, we compare our solution to the deterministic models using sampling from flow and discretization. Finally, we mix deterministic and probabilistic approaches to create a hybrid model.\n\nA. Datasets a) Wikipedia-Detox.: The Wikipedia Detox project has created a crowd-sourced dataset that contains one million annotations covering 100,000 discussions of page edits on Wikipedia  [24] . These were often filled with toxic statements, verbal aggression, and even personal attacks. Each comment was annotated by about ten annotators provided by the Crowdflower service.\n\nThe collection containing toxic statements consists of 160,000 texts. It includes a binary determination of toxicity (where: 0 = non-toxic, 1 = toxic), as well as a rating from -2 to 2 (where: 2 = very healthy, 0 = neutral, and -2 = very toxic).\n\nSets for personal attacks and verbal aggression consist of 100,000 of the same comments. In addition to the binary marks for aggression (0 = neutral or friendly comment, 1 = aggressive or attacking), aggression is put on a scale analogous to toxicity from -2 to 2 (where: 2 = very friendly, 0 = neutral, and -2 = very aggressive). Personal attacks are divided into types: quoting, recipient, third party, or another type of attack. In addition to texts shared between these collections, the same applies to annotators. Thus, we can use knowledge from one collection to benefit from it in another or a collective approach. Those willing to participate in the study also completed questionnaires so that we have demographic information about them available.\n\nb) Emotion Simple.: This collection consists of 100 texts marked on 10 scales by 5,365 annotators  [55] . Texts are opinions posted on websites. This gives 53.65 annotations per text and 1.69 markings from a single user. The texts were rated for eight basic emotions (sadness, anticipation, joy, fear, surprise, disgust, trust, and anger) and emotional arousal on a scale from 0 to 4 for each dimension. In addition, the tenth aspect rated is the valence expressed on a scale from -3 to 3 (where -3 = negative, 0 = neutral, and 3 = positive). In the set of individuals with two marks, those with three or more annotations also appear. c) Emotion Meanings.: In  [56] , a huge collection containing 6,000 assessed word collocations was prepared and published. It contains dimensions and scales analogous to the Emotion Simple collection -the basic emotions from Robert Plutchik's Wheel of Emotions  [57] .\n\nThe scale of the collection makes it one of the most interesting and, simultaneously, the most difficult for personalizing emotion detection. It has 303,143 annotations from 16,101 people who participated in the study. Each collocation has been evaluated 50.67 times, and a single annotator has an average of 18.83 annotations.\n\nThe difficulty in working with these data is also because these are not full-fledged textual statements containing context but just two words. An example item from the collection: \"colorful beads\". Annotator data include information such as gender, age, education, size of residence, relationship status, income, or political views.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Setups",
      "text": "The dataset was divided into training, validation, and testing splits. Users and texts were not mixed between sets to bring the evaluation as close to the real-world scenario. Each experiment consisted of 10-fold cross-validation, and obtained results were averaged. Statistical significance tests were performed: t-test with Bonferroni correction to address the problem of multiple comparisons. In the tables within the rows, comparisons were made between models without and with personalization. Bold indicates the best result, and underline indicates the absence of statistically significant differences for each dataset. Within the \"Type\" column, the best probabilistic model type or no significant difference between the two was similarly marked for each dataset separately. a) Baselines.: We have three reference points. To check the impact of personalization, we compared personalized models with a baseline that uses only textual information (TXT-Baseline); it is a generalized approach. To investigate the impact of normalizing flows, we compared them with a Gaussian Mixture Model to have a reference point in the form of another, less complex probabilistic method. Finally, we compared our method with deterministic approaches.\n\nb) Models for conditional normalizing flows.: In our experimental evaluation, we consider Emotional Normalizing Flow with various types of conditional normalizing flows. For single-dimensional datasets: Wikipedia Detox: Toxicity, Wikipedia Detox: Aggression and Wikipedia Detox: Attack, we used MAF (maf) and CNF (cnf). For multi-dimensional Emotions Meanings and Emotions Simple, we used two extra flows: RealNVP (real_nvp) and NICE (nice). We compared the results against the baseline that uses mixtures of Gaussians to model the probability (gmm).\n\nc) Models for personalization.: We investigate three approaches to respect the personalization context: OneHot, HuBi-Formula, and HuBi-Medium  [51] . They are confronted with TXT-Baseline (generalized, non-personalized) that does not contain any information about the annotator. We exploit LaBSE  [58]  as a language model in every experiment.\n\nC. Experimental scenarios a) Experiment 1 -Comparison of generalized and personalized solutions in the probabilistic approach.: The first approach verifies the performance of Emotional Normalizing Flow with fixed hyperparameters on multiple data sets and tasks: Wikipedia Detox: Toxicity, Wikipedia Detox: Aggression, Wikipedia Detox: Attack, Emotion Meanings and Emotion Simple. We also verified the ability of the proposed Emotional Normalizing Flow to transfer knowledge between thematically similar multidimensional text labels. For this purpose, the Wikipedia Detox: Aggression and Wikipedia Detox: Attack datasets were joined, as they contain annotations for the same texts performed by the same annotators. As a result, we obtained a dataset with multi-dimensional labels. This experiment aimed to examine the effect of personalization models on the prediction of probability distributions, thus verifying whether the additional information provided to the model reduces its uncertainty and comparing Normalizing Flows to Gaussian Mixture Model.\n\nb) Experiment 2 -Investigating the effect of hyperparameters selected for personalization and Emotional Normalizing Flow methods on the most difficult dataset.: The second approach was to verify the maximum possible reduction of model uncertainty by tuning the model hyperparameters to a given set and checking which normalizing-flow model obtained the best results. Due to limited resources, we decided to perform this experiment using only Emotion Meanings dataset. The parameters that we tuned were: the number of hidden features, number of layers, number of blocks per layer, dropout probability, batch normalization within layers, batch normalization between layers, learning rate, the size of hidden layers used to prepare user embeddings, and the size of the output of these embeddings.\n\nc) Experiment 3 -Comparison of probabilistic and deterministic approaches.: To compare with classical methods  [8] , which are deterministic, it was necessary to prepare conversions of the Emotional Normalizing Flow output to the form of exact values. Included in the body of the paper is the application of two best normalizing flows (RealNVP and CNF) for multidimensional datasets (Aggression & Attack [classification task] and Emotion Simple [regression task]).\n\nFor the first type of task, each text or text-user pair was sampled using an iterative method. In the preparation step, we increased the number of samples in the test part of the dataset so that the value from 0 to 1 with a step of 0.1 for the class was tested as a possible context. Iterations were done twice for values of 0 and 1 in the opposite class. Next, an exponential was applied to the 44 probabilities of the resulting sample (22 per class for each text). Within the values for the opposite sampling, (e.g., [0.5, 0] and [0.5, 1]) of a given dimension were summed, and then for each stopper (0.0, 0.1, ..., 1.0) divided by the sum of all values for the dimension. If the probability mass prevailed on the side from 0.0 to 0.5, it was considered that the class was not assigned and vice versa for the other part of the axis.\n\nIt was impossible for a 10-dimensional set for the regression task to sample each possible dimension in all values separately because of the number of possible combinations. Each item from the test subset was replicated 100 times containing random real values from 0 to 1 in each class. Majority voting was then conducted to determine the most likely response for the scale of each dimension. In the collection, each dimension had a value analogous to the slider setting during annotation. For this reason, the task was treated as an ordinary regression, and the resulting values were rounded to the nearest possible position. This assumption was used for both values from the deterministic and probabilistic approaches.\n\nd) Experiment 4 -Hybrid approach (utilizing knowledge from the text and uncertainty modeling).: The combined approach, hereafter referred to as hybrid, was done in two steps. In the first, the learned Emotional Normalizing Flow models were sampled in the same way as in Experiment 3, but for all the texts in the collection. Then, the network input was extended to the deterministic model with an additional feature. A vector containing the resulting probabilities for each text was entered along with its embeddings and, in the case of approaches with personalization, the user profile. This vector contained all the values from the sampling, and no additional mathematical operations were performed on it.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Results",
      "text": "a) Results of Experiment 1.: The first experiment proved that adding personalization reduces the uncertainty of probabilistic models, Tab. I. For Wikipedia Detox datasets (Aggression, Attack and Toxicity), all personalized models received significantly lower negative log-likelihood values compared to the non-personalized TXT-Baseline. For all three tasks, the best architecture was HuBi-Medium combined with CNF. For Aggression & Attack dataset, personalization improved most cases' results. The best results were obtained by OneHot combined with RealNVP and HuBi-Formula combined with CNF. In the case of Emotion Simple and Emotion Meaning, personalization also reduced model uncertainty in most of the cases. For both datasets, the best results were obtained by the HuBi-Medium model combined with RealNVP. It is worth noting that compared to Gaussian Mixture Model, Normalizing Flows always obtain lower negative log-likelihood values. It suggests that target variables, i.e., emotions, have complex distributions, and using a simple probabilistic approach is not enough.\n\nb) Results of Experiment 2.: In the second experiment, we carried out hyperparameter tuning on the most challenging dataset: Emotion Meanings, Tab. II. All possible combinations of hyperparameters were considered when performing the grid search. The results seem to confirm earlier speculations about MAF's better ability to deal with multidimensional problems compared to other approaches. Moreover, none of the variants indicated the benefit of using text alone as input.\n\nc) Results of Experiment 3.: In the third experiment, we compared results obtained by deterministic models and Emotional Normalizing Flow, Tab. III. It was carried out on two datasets: combined Wikipedia Detox: Aggression & Attack and Emotions Simple. We also decided to use only two Normalizing Flow Models that performed the best on both of these datasets: RealNVP and CNF.\n\nIn the case of Aggression & Attack dataset, the results obtained by deterministic models were better for every architecture, including the non-personalized one. In the case of Emotion Simple dataset, the results obtained by probabilistic models significantly outperformed deterministic models. The best model was a combination of HuBi-Medium and CNF. This result seems to confirm that the probabilistic approach is especially effective on complex multi-dimensional tasks. d) Results of Experiment 4.: In the fourth experiment, we mixed the deterministic approach with the probabilistic, to create a hybrid model, Tab. IV. In both Aggression & Attack",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "In this paper, we proposed a novel Emotional Normalizing Flow approach to personalized NLP that opens up new perspectives on predicting reader behavior in a non-deterministic way. From the perspective of psychology and the variability of emotion sensation over time, the problem of emotion recognition is one of the most difficult and subjective tasks facing NLP. People do not perceive their emotions as zeroone, and most of the attempts so far classified their feelings in this way. The presented probabilistic approach based on normalizing flows provides more complex information about the uncertainty and diversity of possible emotional states. A comparative analysis of models for emotion recognition without and with personalization indicated that new methods are also effectively applicable in a non-deterministic setup. The generalized, non-personalized solution generates a completely different concentration of probability mass, directed toward a   quantitative approach. Personalization can shift the view of the problem in a contextual way by dedicating reasoning to a single user. Finally, we showed that adding information about model uncertainty significantly improves the ability to predict complex and subjective behaviors such as recognizing hate speech or emotions in a text. The hybrid model we created significantly outperformed the previous methods, becoming a new state-of-the-art on two very challenging tasks. Our future work will focus on applications of our approach to some other tasks such as active or reinforcement learning.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Limitations",
      "text": "One important issue related to the nature of normalizing flows is their ability to convert probabilities to disambiguate uncertain answers. At the moment, there are no reference datasets available that contain text and annotator information simultaneously with multiple markings of the same text by the same person. This is due to cost constraints in preparing such data. However, we have conducted experiments on datasets with different characteristics in which (1) one person marked several hundred texts [Wikipedia Detox Datasets] and (2) one text was evaluated dozens of times by different people [Emotions Simple and Emotion Meanings datasets]. In order to address the problem mentioned in the introduction, one text should have N annotations from the same person, e.g., a few days apart. If we gain access to or prepare such a dataset, we would be happy to conduct in-depth studies on it.\n\nDue to the language of one of the datasets being different from English, a multilingual model was used to embed the text. This decision was made in order to allow for direct comparisons and cross-referencing. This would have added an unnecessary layer to the already relatively complex problem that was addressed. It is possible to experiment with other language models as well using the source codes provided † .",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: There are few existing architectures [28], [51] utilizing this",
      "page": 2
    },
    {
      "caption": "Figure 1: Comparison of (a) generalized and (b) personalized",
      "page": 3
    },
    {
      "caption": "Figure 2: The model is composed of",
      "page": 3
    },
    {
      "caption": "Figure 2: Comparison of (a) generalized and (b) personalized",
      "page": 3
    },
    {
      "caption": "Figure 3: ) - the input is just an embedding of text",
      "page": 9
    },
    {
      "caption": "Figure 4: ) - the user represented as a one-hot vector",
      "page": 9
    },
    {
      "caption": "Figure 5: ) - the deviation of the user’s",
      "page": 9
    },
    {
      "caption": "Figure 6: ) - annotation-based learned user",
      "page": 9
    },
    {
      "caption": "Figure 3: TXT-Baseline architecture utilizing normalizing flows.",
      "page": 9
    },
    {
      "caption": "Figure 4: OneHot architecture utilizing normalizing flows.",
      "page": 9
    },
    {
      "caption": "Figure 7: and Fig. 8.",
      "page": 9
    },
    {
      "caption": "Figure 5: HuBi-Formula architecture utilizing normalizing flows.",
      "page": 10
    },
    {
      "caption": "Figure 6: HuBi-Medium architecture utilizing normalizing flows.",
      "page": 10
    },
    {
      "caption": "Figure 7: Visualizations of the waveform of the probability",
      "page": 10
    },
    {
      "caption": "Figure 8: Visualizations of the waveform of the probability",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hyperparameter": "hidden features",
          "Values": "[2, 4, 6, 8]"
        },
        {
          "Hyperparameter": "num layers",
          "Values": "[1, 2, 3, 4, 5]"
        },
        {
          "Hyperparameter": "num. blocks per\nlayer",
          "Values": "[1, 2, 3, 4]"
        },
        {
          "Hyperparameter": "dropout probability",
          "Values": "[0.0, 0.1, 0.2, 0.4]"
        },
        {
          "Hyperparameter": "batch norm within layers",
          "Values": "[True, False]"
        },
        {
          "Hyperparameter": "batch norm between layers",
          "Values": "[True, False]"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "2",
      "title": "Inherent Disagreements in Human Textual Inferences",
      "authors": [
        "E Pavlick",
        "T Kwiatkowski"
      ],
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "3",
      "title": "Representation problems in linguistic annotations: Ambiguity, variation, uncertainty, error and bias",
      "authors": [
        "C Beck",
        "H Booth",
        "M El-Assady",
        "M Butt"
      ],
      "year": "2020",
      "venue": "Proceedings of the 14th Linguistic Annotation Workshop"
    },
    {
      "citation_id": "4",
      "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
      "authors": [
        "A Davani",
        "M Díaz",
        "V Prabhakaran"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Emotion ratings: How intensity, annotation confidence and agreements are entangled",
      "authors": [
        "E Troiano",
        "S Padó",
        "R Klinger"
      ],
      "year": "2021",
      "venue": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "6",
      "title": "Computing krippendorff's alpha-reliability",
      "authors": [
        "K Krippendorff"
      ],
      "year": "2011",
      "venue": "Annenberg School for Communication Departmental Papers: Philadelphia"
    },
    {
      "citation_id": "7",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "J Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "8",
      "title": "Multitask personalized recognition of emotions evoked by textual content",
      "authors": [
        "P Miłkowski",
        "S Saganowski",
        "M Gruza",
        "P Kazienko",
        "M Piasecki",
        "J Kocoń"
      ],
      "year": "2022",
      "venue": "EmotionAware 2022: Sixth International Workshop on Emotion Awareness for Pervasive Computing Beyond Traditional Approaches at PerCom 2022"
    },
    {
      "citation_id": "9",
      "title": "Learning to identify emotions in text",
      "authors": [
        "C Strapparava",
        "R Mihalcea"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 ACM symposium on Applied computing"
    },
    {
      "citation_id": "10",
      "title": "Comparison of emotion lexicons",
      "authors": [
        "F Tabak",
        "V Evrim"
      ],
      "year": "2016",
      "venue": "Comparison of emotion lexicons"
    },
    {
      "citation_id": "11",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "13",
      "title": "An analysis of annotated corpora for emotion classification in text",
      "authors": [
        "L Oberländer",
        "R Klinger"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Context-sensitive sentiment propagation in wordnet",
      "authors": [
        "J Kocoń",
        "A Janz",
        "M Piasecki"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th global wordnet conference"
    },
    {
      "citation_id": "15",
      "title": "Propagation of emotions, arousal and polarity in wordnet using heterogeneous structured synset embeddings",
      "authors": [
        "J Kocoń",
        "A Janz"
      ],
      "year": "2019",
      "venue": "Proceedings of the 10th Global Wordnet Conference"
    },
    {
      "citation_id": "16",
      "title": "Recognition of emotions, valence and arousal in large-scale multi-domain text reviews",
      "authors": [
        "J Kocoń",
        "A Janz",
        "P Miłkowski",
        "M Riegel",
        "M Wierzba",
        "A Marchewka",
        "A Czoska",
        "D Grimling",
        "B Konat",
        "K Juszczyk"
      ],
      "year": "2019",
      "venue": "Recognition of emotions, valence and arousal in large-scale multi-domain text reviews"
    },
    {
      "citation_id": "17",
      "title": "Multilingual and language-agnostic recognition of emotions, valence and arousal in large-scale multi-domain text reviews",
      "authors": [
        "J Kocoń",
        "P Miłkowski",
        "M Wierzba",
        "B Konat",
        "K Klessa",
        "A Janz",
        "M Riegel",
        "K Juszczyk",
        "D Grimling",
        "A Marchewka"
      ],
      "year": "2019",
      "venue": "Language and Technology Conference"
    },
    {
      "citation_id": "18",
      "title": "Goemotions: A dataset of fine-grained emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Aspectemo: multi-domain corpus of consumer reviews for aspect-based sentiment analysis",
      "authors": [
        "J Kocoń",
        "J Radom",
        "E Kaczmarz-Wawryk",
        "K Wabnic",
        "A Zaj",
        "M Zaśko-Zielińska"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Data Mining Workshops (ICDMW)"
    },
    {
      "citation_id": "20",
      "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
      "authors": [
        "A Srivastava",
        "A Rastogi",
        "A Rao",
        "A Shoeb",
        "A Abid",
        "A Fisch",
        "A Brown",
        "A Santoro",
        "A Gupta",
        "A Garriga-Alonso"
      ],
      "year": "2023",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "21",
      "title": "# emotional tweets",
      "authors": [
        "S Mohammad",
        "; * Sem"
      ],
      "year": "2012",
      "venue": "Proceedings of the Sixth International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "22",
      "title": "Emonet: Fine-grained emotion detection with gated recurrent neural networks",
      "authors": [
        "M Abdul-Mageed",
        "L Ungar"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "23",
      "title": "Socialnlp 2018 emotionx challenge overview: Recognizing emotions in dialogues",
      "authors": [
        "C.-C Hsu",
        "L.-W Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the sixth international workshop on natural language processing for social media"
    },
    {
      "citation_id": "24",
      "title": "Ex machina: Personal attacks seen at scale",
      "authors": [
        "E Wulczyn",
        "N Thain",
        "L Dixon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 26th international conference on world wide web"
    },
    {
      "citation_id": "25",
      "title": "Constructing interval variables via faceted rasch measurement and multitask deep learning: a hate speech application",
      "authors": [
        "C Kennedy",
        "G Bacon",
        "A Sahn",
        "C Vacano"
      ],
      "year": "2020",
      "venue": "Constructing interval variables via faceted rasch measurement and multitask deep learning: a hate speech application",
      "arxiv": "arXiv:2009.10277"
    },
    {
      "citation_id": "26",
      "title": "Modeling annotator perspective and polarized opinions to improve hate speech detection",
      "authors": [
        "S Akhtar",
        "V Basile",
        "V Patti"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing"
    },
    {
      "citation_id": "27",
      "title": "Offensive, aggressive, and hate speech analysis: From data-centric to human-centered approach",
      "authors": [
        "J Kocoń",
        "A Figas",
        "M Gruza",
        "D Puchalska",
        "T Kajdanowicz",
        "P Kazienko"
      ],
      "year": "2021",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "28",
      "title": "Useridentifier: implicit user representations for simple and effective personalized sentiment analysis",
      "authors": [
        "F Mireshghallah",
        "V Shrivastava",
        "M Shokouhi",
        "T Berg-Kirkpatrick",
        "R Sim",
        "D Dimitriadis"
      ],
      "year": "2021",
      "venue": "Useridentifier: implicit user representations for simple and effective personalized sentiment analysis",
      "arxiv": "arXiv:2110.00135"
    },
    {
      "citation_id": "29",
      "title": "Controversy and conformity: from generalized to personalized aggressiveness detection",
      "authors": [
        "K Kanclerz",
        "A Figas",
        "M Gruza",
        "T Kajdanowicz",
        "J Kocoń",
        "D Puchalska",
        "P Kazienko"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Personal bias in prediction of emotions elicited by textual opinions",
      "authors": [
        "P Milkowski",
        "M Gruza",
        "K Kanclerz",
        "P Kazienko",
        "D Grimling",
        "J Kocon"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021): Student Research Workshop"
    },
    {
      "citation_id": "31",
      "title": "Studemo: A non-aggregated review dataset for personalized emotion recognition",
      "authors": [
        "A Ngo",
        "A Candri",
        "T Ferdinan",
        "J Kocoń",
        "W Korczynski"
      ],
      "year": "2022",
      "venue": "Proceedings of the 1st Workshop on Perspectivist Approaches to NLP@ LREC2022"
    },
    {
      "citation_id": "32",
      "title": "What if ground truth is subjective? personalized deep neural hate speech detection",
      "authors": [
        "K Kanclerz",
        "M Gruza",
        "K Karanowski",
        "J Bielaniewicz",
        "P Miłkowski",
        "J Kocoń",
        "P Kazienko"
      ],
      "year": "2022",
      "venue": "Proceedings of the 1st Workshop on Perspectivist Approaches to NLP@ LREC2022"
    },
    {
      "citation_id": "33",
      "title": "Deep-sheep: Sense of humor extraction from embeddings in the personalized context",
      "authors": [
        "J Bielaniewicz",
        "K Kanclerz",
        "P Miłkowski",
        "M Gruza",
        "K Karanowski",
        "P Kazienko",
        "J Kocoń"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Data Mining Workshops (ICDMW)"
    },
    {
      "citation_id": "34",
      "title": "Personalized models resistant to malicious attacks for human-centered trusted ai",
      "authors": [
        "T Ferdinan",
        "J Kocoń"
      ],
      "year": "2023",
      "venue": "The AAAI-23 Workshop on Artificial Intelligence Safety"
    },
    {
      "citation_id": "35",
      "title": "Capturing human perspectives in nlp: Questionnaires, annotations, and biases",
      "authors": [
        "W Mieleszczenko-Kowszewicz",
        "K Kanclerz",
        "J Bielaniewicz",
        "M Oleksy",
        "M Gruza",
        "S Woźniak",
        "E Dzięcioł",
        "P Kazienko",
        "J Kocoń"
      ],
      "year": "2023",
      "venue": "The ECAI 2023 2nd Workshop on Perspectivist Approaches to NLP, CEUR Workshop Proceedings"
    },
    {
      "citation_id": "36",
      "title": "Differential dataset cartography: Explainable artificial intelligence in comparative personalized sentiment analysis",
      "authors": [
        "J Kocoń",
        "J Baran",
        "K Kanclerz",
        "M Kajstura",
        "P Kazienko"
      ],
      "year": "2023",
      "venue": "International Conference on Computational Science"
    },
    {
      "citation_id": "37",
      "title": "Senticnet 7: A commonsense-based neurosymbolic ai framework for explainable sentiment analysis",
      "authors": [
        "E Cambria",
        "Q Liu",
        "S Decherchi",
        "F Xing",
        "K Kwok"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "38",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "A wide evaluation of chatgpt on affective computing tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "A wide evaluation of chatgpt on affective computing tasks",
      "arxiv": "arXiv:2308.13911"
    },
    {
      "citation_id": "40",
      "title": "Paed: zeroshot persona attribute extraction in dialogues",
      "authors": [
        "L Zhu",
        "W Li",
        "R Mao",
        "V Pandelea",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "41",
      "title": "Nice: Non-linear independent components estimation",
      "authors": [
        "L Dinh",
        "D Krueger",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "arXiv"
    },
    {
      "citation_id": "42",
      "title": "Density estimation using Real NVP",
      "authors": [
        "L Dinh",
        "J Sohl-Dickstein",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "43",
      "title": "Masked autoregressive flow for density estimation",
      "authors": [
        "G Papamakarios",
        "T Pavlakou",
        "I Murray"
      ],
      "year": "2018",
      "venue": "Masked autoregressive flow for density estimation"
    },
    {
      "citation_id": "44",
      "title": "Ffjord: Free-form continuous dynamics for scalable reversible generative models",
      "authors": [
        "W Grathwohl",
        "R Chen",
        "J Bettencourt",
        "I Sutskever",
        "D Duvenaud"
      ],
      "year": "2018",
      "venue": "Ffjord: Free-form continuous dynamics for scalable reversible generative models",
      "arxiv": "arXiv:1810.01367"
    },
    {
      "citation_id": "45",
      "title": "RegFlow: Probabilistic Flow-based Regression for Future Prediction",
      "authors": [
        "M Zieba",
        "M Przewieźlikowski",
        "M Śmieja",
        "J Tabor",
        "T Trzcinski",
        "P Spurek"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "46",
      "title": "Flow plugin network for conditional generation",
      "authors": [
        "P Wielopolski",
        "M Koperski",
        "M Zieba"
      ],
      "year": "2021",
      "venue": "Flow plugin network for conditional generation",
      "arxiv": "arXiv:2110.04081"
    },
    {
      "citation_id": "47",
      "title": "Plugen: Multi-label conditional generation from pre-trained models",
      "authors": [
        "M Wolczyk",
        "M Proszewska",
        "L Maziarka",
        "M Zieba",
        "P Wielopolski",
        "R Kurczab",
        "M Smieja"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event"
    },
    {
      "citation_id": "48",
      "title": "Styleflow: Attributeconditioned exploration of stylegan-generated images using conditional continuous normalizing flows",
      "authors": [
        "R Abdal",
        "P Zhu",
        "N Mitra",
        "P Wonka"
      ],
      "year": "2021",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "49",
      "title": "Treeflow: Going beyond tree-based gaussian probabilistic regression",
      "authors": [
        "P Wielopolski",
        "M Zieba"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "50",
      "title": "Discrete flows: Invertible generative models of discrete data",
      "authors": [
        "D Tran",
        "K Vafa",
        "K Agrawal",
        "L Dinh",
        "B Poole"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "51",
      "title": "Learning personal human biases and representations for subjective tasks in natural language processing",
      "authors": [
        "J Kocoń",
        "M Gruza",
        "J Bielaniewicz",
        "D Grimling",
        "K Kanclerz",
        "P Miłkowski",
        "P Kazienko"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "52",
      "title": "Human-centred neural reasoning for subjective content processing: Hate speech, emotions, and humor",
      "authors": [
        "P Kazienko",
        "J Bielaniewicz",
        "M Gruza",
        "K Kanclerz",
        "K Karanowski",
        "P Miłkowski",
        "J Kocoń"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "53",
      "title": "Variational Inference with Normalizing Flows",
      "authors": [
        "D Rezende",
        "S Mohamed"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015"
    },
    {
      "citation_id": "54",
      "title": "Analyzing the structure of attention in a transformer language model",
      "authors": [
        "J Vig",
        "Y Belinkov"
      ],
      "year": "2019",
      "venue": "Analyzing the structure of attention in a transformer language model",
      "arxiv": "arXiv:1906.04284"
    },
    {
      "citation_id": "55",
      "title": "Personal bias in prediction of emotions elicited by textual opinions",
      "authors": [
        "P Miłkowski",
        "M Gruza",
        "K Kanclerz",
        "P Kazienko",
        "D Grimling",
        "J Kocoń"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop"
    },
    {
      "citation_id": "56",
      "title": "Emotion norms for 6000 polish word meanings with a direct mapping to the polish wordnet",
      "authors": [
        "M Wierzba",
        "M Riegel",
        "J Kocoń",
        "P Miłkowski",
        "A Janz",
        "K Klessa",
        "K Juszczyk",
        "B Konat",
        "D Grimling",
        "M Piasecki"
      ],
      "year": "2021",
      "venue": "Emotion norms for 6000 polish word meanings with a direct mapping to the polish wordnet"
    },
    {
      "citation_id": "57",
      "title": "The emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1991",
      "venue": "The emotions"
    },
    {
      "citation_id": "58",
      "title": "Languageagnostic BERT sentence embedding",
      "authors": [
        "F Feng",
        "Y Yang",
        "D Cer",
        "N Arivazhagan",
        "W Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}