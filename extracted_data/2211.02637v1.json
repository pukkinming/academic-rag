{
  "paper_id": "2211.02637v1",
  "title": "Emotion Recognition With Temporarily Localized 'Emotional Events' In Naturalistic Context *",
  "published": "2022-10-25T10:01:40Z",
  "authors": [
    "Mohammad Asif",
    "Sudhakar Mishra",
    "Majithia Tejas Vinodbhai",
    "Uma Shanker Tiwary"
  ],
  "keywords": [
    "Affective Computing",
    "CNN",
    "DEAP",
    "DENS",
    "EEG",
    "Emotion Dataset",
    "Emotion Recognition",
    "LSTM",
    "SEED"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition using EEG signals is an emerging area of research due to its broad applicability in BCI. Emotional feelings are hard to stimulate in the lab. Emotions don't last long, yet they need enough context to be perceived and felt. However, most EEG-related emotion databases either suffer from emotionally irrelevant details (due to prolonged duration stimulus) or have minimal context doubting the feeling of any emotion using the stimulus. We tried to reduce the impact of this trade-off by designing an experiment in which participants are free to report their emotional feelings simultaneously watching the emotional stimulus. We called these reported emotional feelings \"Emotional Events\" in our Dataset on Emotion with Naturalistic Stimuli (DENS). We used EEG signals to classify emotional events on different combinations of Valence(V) and Arousal(A) dimensions and compared the results with benchmark datasets of DEAP and SEED. STFT is used for feature extraction and used in the classification model consisting of CNN-LSTM hybrid layers. We achieved significantly higher accuracy with our data compared to DEEP and SEED data. We conclude that having precise information about emotional feelings improves the classification accuracy compared to long-duration EEG signals which might be contaminated by mind-wandering.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition has been a challenging task in artificial intelligence. Several methods are available for measuring the participants' emotions, such as behavioural changes and subjective experiences self-reported by the participants; peripheral and central nervous system measures  [1] , but brain activities are among the most robust dimensions of detecting human affect as it is difficult for the users to manipulate innate brain activity during the process. Accordingly, EEG is considered a suitable and convenient method to record electrical activities to measure brain activities as it is a non-invasive method, i.e. there are no scalpel incisions. Many studies have already been conducted to measure human affect with the help of EEG and other peripheral responses  [2] [3] [4] [5] . In the previous studies, the focus of the study was to develop a database that is labelled and suitable for emotion detection by intelligent systems and has contributed to affective computing. There is a typical method in these studies to elicit emotion in the participants by presenting them with video clips as stimuli. In the process of emotion recognition and other classification tasks, all the EEG data for that stimulus has to be considered for the classification model as there is no information about the precise temporal location at which a participant may experience the emotion. Models must consider all the data presented for that label, which is unnecessarily computationally expensive and decreases the system's efficiency by feeding not-so-essential data in the input.\n\nIn our approach, we have presented a novel method to overcome this issue by providing precise information about the emotion elicitation, self-reported by the participants. We call it an 'Emotional Event'. In this method, an additional task is given to the participants to mention precise temporal information by clicking on their computer screens while watching the emotional clips if they feel some emotion. To the best of our knowledge, there are also no EEG affective datasets available for the Indian subcontinent population. Hence we tried to fulfil this research gap also in our work. We have considered DEAP dataset  [2]  and SEED dataset  [3]  to be benchmark. We tried to follow the format similar to the benchmark datasets and compared our dataset's results with these datasets based on statistical significance.\n\nEEG measures the electrical signals from the scalp with temporal details. To detect emotions from brain activity using EEG, participants wear an EEG cap that captures the brain's electrical activity from the scalp through the electrodes. These electrodes are fixed with the help of an elastic cap at a specified distance from each other. Different EEG devices vary with the number of electrodes; that is called the number of channels of the EEG. Thirty-two or fewer EEG channels are especially notable in affective computing research  [6] . There are also a few studies with up to 64 electrodes. In this work, we used 128 channels EEG device to detect emotions. This EEG cap follows the International 10-10 system's standards  [7] .\n\nEmotions are complex and challenging to understand as many theories exist about emotions, and there is a lack of a single consensus  [8] . Study of emotions has been an emerging topic that combines multidiscipline fields of psychology, neuroscience, computer science and medicines, not limited to only these fields. There are different aspects involved in determining emotions, such as behavioural, psychological and physiological aspects, cognitive appraisals, expression through facial or vocal responses, and the subjective experience, to be last but not the least. This study focuses on physiological aspects of emotion, which are considered into account by the brain signals captured through EEG while watching emotional video clips. Further, this study tries to collect a comprehensive list of subjective experiences through a self-assessment rating at the end of each clip.\n\nMany approaches could be used to assess the participants' emotional states. Earlier, some basic emotions were used that are universally recognised for study purposes  [9] . Later, more theories came about some complex emotions that are a combination of basic emotions  [10] . Multi-dimensional theories of emotions are among the most notable standards for assessing core affect  [11, 12] . According to these theories, emotions are considered a multi-dimensional array; one dimension for valence (experiencing positive or negative) and the other for arousal (experiencing the intensity) or dominance (controlling or feeling controlled). There are also a few more dimensions that make the spectrum broader, e.g., relevance (), familiarity () and liking (). Asking participants to report these experiences on a continuous scale is common in similar studies. Some theories deal with the physiological responses of feeling emotions, e.g., body temperature and heartbeat change  [13, 14] . As viewing different theories, it is obvious to consider that emotion is not a single phenomenon; instead, it is a combination of physiological responses and other information. Evidence is available that many brain regions are involved during emotion perception  [15] . We have collected ECG data of the participants along with EEG to consider these parameters.\n\nEmotional Event: Emotion is a complex phenomenon which is embedded within a context. Moreover, emotion is transient in nature and is not available throughout the stimulus duration. In fact, more than one aspect could be embedded within the stimulus context, and different participants can feel emotion at different points of time considering various aspects. However, most of the datasets recorded to date  [2]  [3] ignore the transient nature of emotions and provide a single emotional category for the whole stimulus duration. Although the stimulus has emotional information, it has some non-emotional aspects too, which could lead to mind wandering activity. Although there are some attempts to get continuous subjective feedback on emotional experience and neural activity, the experimental method involved multiple watching of the stimulus and retrospective collection of emotional experience  [16] [17] [18] [19] [20] . The retrospective collection depends on autobiographical memory and can raise biases across subjects depending on their capability to recall  [21] . Also, repetitive viewing effects can bias the ratings and underlying neural effects  [21] . Hence, an experimental paradigm is needed to record the participants' feedback dynamically, with minimal distraction during emotion processing and minimizing the memory recall biases. To the best of our knowledge, in this work, we are introducing a novel paradigm in which the time-stamp of emotional feelings can be marked online that can be further utilized to get the subjective feedback of emotional feelings and analyze brain signals temporally localized to the feeling of an emotion. We will refer to these time-stamped emotional feelings as \"emotional events\".\n\nEmotion recognition through EEG data follows a similar pattern as follows in various EEG signal analyses. First, the data is acquainted, and some preprocessing is applied to the signal. These preprocessing steps involve removing artefacts such as ocular activity, muscle activity, and powerline interference. Also, downsampling of the signal and band-pass filtering are used to make data more useful. Various dimensionality reduction techniques such as ICA and PCA are also used to prune the data to make it feature-rich. After pre-processing, features are extracted from the signal to feed into the model for the classification task. There are different kinds of features available to extract that mainly include time-domain (e.g., event-related potential (ERP), high-order crossing (HOC), etc.), frequency-domain (e.g., power spectral density (PSD), etc.); and time-frequency domain (e.g., STFT, wavelet analysis, etc.) EEG records multi-frequency non-stationary brain signals from various electrodes. Analyzing these signals is challenging because of the complex and irregular nature of EEG signals. The time-frequency domain analysis benefits both the time and frequency domains, e.g., better spatial and temporal information from EEG signals. One basic time-frequency domain feature extraction method is Short-Time Fourier Transform (STFT). STFT is a time-ordered sequence of spectral estimates and is one of the powerful and general-purpose signal processing techniques. It has been used in the field of spectral analysis of a signal. The STFT is used to compute spectrograms which are used extensively for signal processing. Spectrograms are visual representations of the spectrum of frequencies of a signal with varying times  [22] .\n\nCNN is the most frequently used architecture for EEG analysis and classification tasks, and DBN and RNN follow it  [23] . Hence we have also used a hybrid combination of the CNN-LSTM model to train our data as it should be suitable to compare our results with benchmark datasets. Using artificial intelligence for affective computing provides better learning capabilities to intelligent systems. With the advancement of computing power and the development of effective and advanced neural network research, the trend of using various machine learning and deep learning techniques has grown within the last few years  [24] . This article employs the widely used state-of-the-art deep learning methods used to detect emotions from EEG signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Details",
      "text": "The complete flow diagram of the experiment is given in Fig.  1 : Phase-1. We call our dataset 'Dataset on Emotion with Naturalistic Stimuli' (abbreviated as 'DENS').",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Stimuli",
      "text": "The selection of stimuli to induce participants' emotions also plays a vital role in emotion recognition. A careful selection of stimuli is critical, and for that, technical validation of the video clips is crucial to assess if the intended emotional experience is elicited by the stimuli. In our previous work we have validated set of multimedia stimuli []. We selected 15 stimuli to perform our EEG experiment from the multimedia dataset. The table-1 shows list of stimuli with the target emotions assigned during the stimuli validation by [].",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Stimulus Name",
      "text": "Target Emotion ashayen (199) Adventorous horror (214) Afraid Anacondas The Hunt for the Blood Orchid clip  (    1 : Selected stimuli for EEG study from the stimuli dataset  [25] ). The time duration of each stimulus is 60 s. Bracket contains stimulus Id (available in open science framework repository).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Recording",
      "text": "We recorded the EEG activity of forty participants (23.3±1.25, F=3) while they were watching emotional film stimuli. The complete experimental procedure is available elsewhere  [26] . We are describing here the unique information which is relevant to this study. Each participant saw nine emotional stimuli, which were randomly extracted from the set of 16 stimuli. While watching the emotional film stimuli participants were instructed to perform a mouse click the moment they felt any emotion. At the end of each video stimuli, participants are provided six self-assessment scales, including valence, arousal, dominance, liking, familiarity, and relevance. Following these responses, for each click, participants were provided with a list of emotions pooled into four quadrants of V-A space (HVHA, LVHA, LVLA, HVLA) in the drop-down menu. Participants were also given a choice to enter the emotional category which suits their emotional experience but is unavailable in the provided emotion list. After that, an inter-trial-interval next trial started.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ratings",
      "text": "Subjective ratings are one of the well-known methods to evaluate the personal emotional experience of the participants. Emotional pictures/videos or audio clips are presented to the participants, and they are asked to rate these clips on different scales based on their personal experiences. These scales include Valence, Arousal, Dominance, Liking, Familiarity and Relevance. Although, in this analysis we considered only valence and arousal scales (represented in figure-2).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Preprocessing And Artifact Removal Of The Eeg Data",
      "text": "The procedure followed to perform the pre-processing is described elsewhere  [26] . The critical step which should be described here includes filtering and artifact removal. We had 128-channel EEG raw data with a sampling rate of 250 Hz. The raw signal is filtered using a Butterworth fifth-order bandpass filter with the passband 1-40 Hz. Independent component analysis (ICA) is used to remove artifacts, including heart rate, muscle movement, and eye blink-related artifacts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Other Datasets Used",
      "text": "We have used DEAP dataset (a dataset for emotion analysis using eeg, physiological and video signals) and SEED dataset (A dataset collection for various purposes using EEG signals) for comparing the reults with our dataset (DENS).\n\nThe DEAP dataset consisted of 40 videos/trials, and for each trial there are 40 channels of EEG including peripheral signals are available and data is given foe each channel. We have used only 32 channels (i.e., discarded peripheral signals) for the experiment as we only want to use data from the brain only. This data was already pre-processed as 128 Hz downsampled, bandpass frequency of 4-45 Hz and EOG removed. For each trial, there are 4 labels available-Valence (V), Arousal (A), Dominance and Linking. We have used only V-A space for the experiment purpose.\n\nThe SEED dataset was recorded for 15 participants and emotions were presented to the participants into three categories-positive, negative and neutral emotions. We have used only V-space in DENS dataset to match the number of classes for both the datsets. The data was recorded using 62 channels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extraction",
      "text": "EEG Signals are non-stationary, meaning the signal's statistical characteristics change over time. If these signals are transformed to the frequency domain using Fourier Transform, it provides the frequency information, which is averaged over the entire EEG signal. So, information on different frequency events is not analyzed properly. If a signal is cut into minor segments such that it could be considered as stationary and focus on signal properties at a particular section which is called a windowing section and apply Fourier transform on it, it is called as Short-Time Fourier Transform (STFT). It will move to the entire signal length and apply Fourier transform to find the spectral content of that section and display the coefficient as a function of both time and frequency. It provides insight into the nature of the time-varying spectral characteristics of the signal. Before STFT, let's look at the discrete Fourier transform. Consider x: [0: L-1] = {0, 1, . . . , L -1} → R be a discrete-time signal where L is the signal length which is acquired by equidistance sample points with respect to the fixed sampling frequency. Mathematically DFT equation is,\n\nWhere k [0, K] and K is the frequency index with respect to Nyquist frequency. N is the duration of the section. The equation returns the complex Fourier coefficients for the k th . These coefficients provide two parameters: phase and magnitude. For STFT, consider the additional parameter hop size (H), which is the step size of the window to be shifted. ω be a sampling window function which is ω: [0, N-1] → R. STFT can be defined as,\n\nWhere m [0,M] and M is the maximum frame index mathematically M = L-N",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "H",
      "text": ". The Short-Time Fourier Transform is not only a function of k but also m which is a proxy time representation. Here, the function returns Fourier coefficients for the k th proxy frequency at the m th temporal bin. Spectrograms are nothing but squared magnitude of STFT of the signal.\n\nIt is a 2D image where the horizontal axis represents time, and the vertical axis represents frequency bins. Number of frequency bins is (framesize / 2) + 1. No of time frames are ((size of signal -framesize) / hopsize) + 1. χ(m, k) represent intensity or color at (m, k).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Input Preprocessing To Feed Data Into The Classifier",
      "text": "It is essential to convert the data into meaningful format that can be fed into our classifier model. As all the three datasets are available in different formats, we have provided information of the input preprocessing for each datasets as following:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "For Deap Data:",
      "text": "Each subject in the DEAP dataset is given by a tensor that is in the form of X ∈ R 40×40×8064 , representing 40 videos, 40 EEG-channels (including peripheral channels), and 8064 EEG data samples for each channel. For labels, DEAP data provided a matrix in the form of X ∈ R 40×4 ; i.e, for each subject, there 40 videos and 4 scales. From the DEAP dataset, the first 15 subjects are picked. For the label we have used Valence-Arousal space and divided it into four classes-HVHA, HVLA, LVLA, LVHA (abbreviations-H:High, L:Low, V:Valence, A:Arousal). We have converted 15 subjects data tensor into a matrix of X ∈ R 19200×8064 (i.e., 15subjects × 40videos × 32channels, 8064samples). Moreover, this data was processed for feature extraction using STFT with a window size of 0.5s and an overlap of 0.25s of data samples. Using STFT, we have converted every 8064 size of EEG data samples into a spectrogram image size of (33,251), as mentioned in the feature extraction section. Then, a hybrid CNN-LSTM classifier was implemented for multi-class classification with an input tensor of X ∈ R 33×251×3 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "For Seed Data:",
      "text": "SEED dataset contains 45 .mat files for 15 subjects for each subject with 3 trials. The label file contains 3 emotional labels -1 for negative, 0 for neutral, and 1 for positive on the valence scale. After renaming, the labels become 0 for neutral, 1 for positive, and 2 for negative. For classification, we have considered 15 .mat files, one trial per subject. Due to the different sizes of data length in each channel, the first 16000 sample for each data which is the first 80s of data, is considered for further processing. EEG cap includes 62 channels according to the 10-20 international system. So, 15 subjects, 15 trials, 62-channels, and 16000 EEG data are converted into a tensor of X ∈ R 13950×16000 (i.e., 15subjects × 15trials × 62channels, 16000samples)for feature extraction. As mentioned in the DEAP dataset experiment, using STFT with window size 0.5s and overlap of 0.25s, each 16000 EEG data is converted into a spectrogram with the shape of (51,319). Then, a hybrid CNN-LSTM classifier was implemented for multi-class classification with input tensor shape X ∈ R 51×319×3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "For Dens Data:",
      "text": "For the DENS dataset, we have 465 .mat files which contain emotional events. All the 465 files are picked for the experiment. Each .mat file is a matrix of X ∈ R 128×1751 , where 128 is the number of EEG channels and 1751 is the sample data for each channel. Then we have converted the data tensor of X ∈ R 465×128×1751 into the form of R 59520×1751 (i.e., 465emotionalevents × 128channels, 1751samples) for feature extraction with window size 0.5s and overlap is 0.25s. After feature extraction, we have 59520 spectrograms, and each spectrogram is in the shape of (63, 26). Now to compare with the DEAP dataset DENS dataset with 4 label classification is performed with a hybrid CNN-LSTM classifier. For the label, we have used the same V-A space (HVHA, HVLA, LVLA, LVHA) with input tensor of X ∈ R 63×26×3 . For SEED and DENS datasets comparison, we have used 3-label classification. For the DENS dataset on the valence scale rating below 4.5 is marked as negative (0 labelled), and ratings above 5.5 is marked as positive (2 labelled). For neutral labels, in the DENS dataset, we have non-emotional files; we have marked neutral (1 labelled) for those files' data. Then with classifier, input tensor of X ∈ R 63×26×3 is used for classification.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Architecture For The Classification Task",
      "text": "Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) are one of the most widely used deep learning techniques. CNNs are used to extract meaningful patterns and features from the data. The key element in CNN is the convolution operation using kernels that automatically learn the local patterns from data. These local features are then combined into more complex features when multiple CNN layers are stacked. Filters (i.e, weights trained) in this process are also known as feature detector matrices. Input data will be convoluted with a filter map by sliding the kernel window. At the same time, LSTM networks can capture the sequential pattern as LDTMs are best suited for time-series data. LSTMs are designed to work for temporal correlations. Therefore, to exploit the benefits of both CNN and LSTM, a hybrid CNN-LSTM architecture is used for the classification of emotions. The hybrid Figure  3 : Model Architecture: It is consisted of two 2D-convolution layers with 3 × 3 kernels and 32 filters and 64 filters respectively, followed by a max pooling layer followed by a dropout layer and flattening layer. A repeat vector layer of size 4 is used before sending the data to the LSTM layers. Two LSTM layers are used of sizes 256 units and 128 units respectively, each followed by a dropout layer. At the end two dense layers are used of sizes 64 (followed by a dropout layer) and 4 or 3 (equals the number of the output classes). The Pattern learning block consists of two 2D-convolutional blocks, each with a kernel size of (3 × 3). The feature map, which is the output of convolutional layers, keeps track of the location of the features in the input. A max-Pooling layer is added in between two consecutive convolutional layers. A pooling layer is added after the convolutional layer to reduce the feature-map dimension; hence it reduces the computational cost, and the activation function is applied to enhance the capability of the model. Rectified Linear Unit (ReLU) activation function which has been widely used to resilient vanishing gradient problem. In between, the dropout layer is used in some places to avoid the overfitting problem. The flattening layer transforms these feature maps into one-dimensional vectors. Repeat vector gives extra dimension for LSTM layer. The sequential learning block consists of 2 LSTM layers which capture the long-term temporal dependencies from the feature map extracted by CNN layers. 1 st LSTM layer consists of 256 cells with a return sequence is set to 'True' while 2 nd LSTM consists of 128 cells and as it is the last LSTM layer return sequence is 'False'. Between LSTM layers, dropout layers with rate = 0.2 are added to avoid overfitting issues. Finally, two fully-connected layers where 1 st layer with 64 neurons and 2 nd layer with the number of classes as neurons are added for further processing. As we have the multi-class classification, the SoftMax activation function is used in the output layer as it outputs a vector representing the probability distributions of a list of potential classes.\n\nThe parameter setting for developed deep learning model is mentioned in the Table  2 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "The confusion matrix for DEAP, SEED and DENS datasets are shown in 4a, 5a, 4b and 5b. In the confusion matrix shown, each cell contains data on the number of population and percentage of the population. The X-axis represents actual labels and the Y-axis represents predicted labels by the classifier. There is an additional row and a column in this confusion matrix, where the last column (sum_lin) data mention precision for that label, and the last row data (sum_col) mention the recall. The intersection cell of the last column and the last row shows accuracy and the total number of supports. The diagonal of the matrix represents the correctly identified label.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparison Between Deap And Dens:",
      "text": "We have used repeated K-Fold cross-validation with K = 5 and number of repeats = 5 so generated 25 accuracies for DENS and DEAP. For label classification we have used V-A space (HVHA, HVLA, LVLA, LVHA). Comparison between DEAP and DENS is mentioned in Table  3 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset",
      "text": "Mean F1 score (in %) DEAP 95.65 (± 0.38) DENS 96.82 (± 0.18) Table  3 : DEAP vs DENS with mean F1 scores Figure  6a  shows a F1 score comparison between the DEAP and DENS dataset per trial. Using t-test statistical testing, the 25 F1 scores of DEAP dataset (M = 95.65%, SD = 0.38%) compared with the 25 F1 scores of DENS dataset (M = 96.82%, SD = 0.18%), DENS dataset shows better results with absolute t (35) = 13.54, p < 0.0001, d estimate: -13.70 (large), 95 percent confidence interval: [-16.51 -10.89].",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison Between Seed And Dens:",
      "text": "For SEED vs DENS comparison, label classification we have used 3 labels on valence scale. Comparison between SEED and DENS result is mentioned in Table  4 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Dataset",
      "text": "Mean F1 scores (in %) SEED 95.65 (± 0.37) DENS 97.68 (± 0.13) Table  4 : SEED vs DENS with mean F1 scores Figure  6b  shows a F1 score comparison between the SEED and DENS dataset per trial. Using t-test statistical testing, the 25 F1 score of SEED dataset (M = 95.65%, SD = 0.37%) compared with the 25 F1 score of DENS dataset (M = 97.68%, SD = 0.13%), DENS dataset shows better results with absolute t (31) = 25.466, p < 0.0001, Cohen's d estimate: -11.37 (large), 95 percent confidence interval: [-13.73 -9.02].",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "In this work, we captured emotional experiences within the ecologically valid naturalistic environment with a precise temporal marker than any study to date. As per the recent theories, emotional experience is a constructing phenomenon which involves networks of the brain, including the default mode network, salience network, and fronto-parietal network. These networks are not specific to emotional experiences. In fact, these networks are domain-general networks which are involved in perception (in general). Though, the connectivity among these networks might not be the same in different perceptions which are apparently shown in our previous work  [27] . In addition, different from normal perception, emotional experiences involve changes in body physiology  [26] . Putting together the above-mentioned ideas from recent results hints that the emotional experiences can be easily confused with the other perceptions, which might not be an emotional experience.\n\nOne of the major concerns is the mind-wandering activity while using the film stimuli. In the previous research, the whole stimulus is considered to elicit a single emotional experience. And the duration of the stimulus varied from  This space is divided into three classes (SEED dataset provided data with three classes, while DENS data is divided into three classes based on the valence ratings provided by the participants)and assigned a label to it as follows: For SEED:0 for neutral, 1 for positive and 2 for negative For DENS: 0 for low-valence (valence ratings ranges from 1-4.5), 1 for non-emotional data (valence ratings ranges from 4.5-5.5) and 2 for high-valence (valence ratings ranges from 5.5-9). seconds to minutes. Research shows that averaging the participant's feedback for the whole duration of the stimulus might not be correctly capturing emotional experience (in particular)  [28] . Hence, it is important to know the duration of the emotional experience without compromising the ecological validity of the stimuli.\n\nThe main idea behind this work is that if we can capture the temporal marker of emotional experience within the reallife resembling environment, we might achieve better accuracy than the accuracy achieved to date with other datasets lacking the information about time. Although, due to the limited number of subjects, we didn't go for the subjectindependent classification for now. Though, in future, we will be collecting more data to mitigate this limitation.\n\nIn our results, we observed that the same hybrid deep learning model on our dataset outperformed other datasets, including benchmark datasets like DEAP and SEED. Classification of DEAP data into four labels, including HVHA, LVHA, LVLA, and HVLA, resulted in 94.74% accuracy. At the same time, the classification of DENS data into four labels resulted in 96.38% accuracy. The significance testing showed that even with the multiple iterations, the classification accuracy was significantly higher for our data.\n\nTo the date, most of the work on emotion recognition applied different shallow machine learning and deep learning techniques using many different configurations of input data including, spectrogram, raw signals, statistical features, variational mode decomposition (VMD), empirical mode decomposition (EMD), functional connectivity based features, fractal features and so on. However, still, the recognition of emotion from EEG stands as a problem. Most of the works on emotion recognition have used some benchmark datasets, including DEAP, SEED, AMIGO, MAHNOB-HCI and so on. Though, most of the emotion classification works revolves around DEAP and SEED datasets  [2] .\n\nIn  [29] , emotional states are classified by means of EEG-based functional connectivity patterns. Forty participants viewed audio-visual film clips to evoke neutral, positive (one amusing and one surprising) or negative (one fear and one disgust) emotions. Correlation, coherence, and phase synchronization are used for estimating the connectivity indices. They stated significant differences among emotional states. A maximum classification rate of 82% was reported when the phase synchronization index was used for connectivity measure.\n\nThe classes considered in the study are elementary. We suspect that with the increasing number of emotional classes, which includes not only basic classes but complex emotions as well, taking the long duration signal without a temporal marker may not be able to categorize emotional classes. The reason is that there are fewer chances for a movie stimulus to have positive as well as a negative emotional experience in the same stimuli, but it is certainly possible that it can have more than one positive or more than one negative feeling in the movie.",
      "page_start": 8,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "The work presented in this article is based on the concept that emotion is a short-lived phenomenon which might last for very few seconds. Hence, using long-duration EEG signals recorded during emotional stimulus watching might not contain emotional information for the whole duration. Therefore, we hypothesized that using only the duration of the signal where an emotional event is reported without compromising the ecological validity of the stimuli will contain more emotional information. To test the hypothesis, we designed an EEG experiment which uniquely marks the duration of the emotional event in the continuous recording of brain waves using EEG. We performed deep learning analysis using a hybrid CNN and LSTM model and found results that significantly favoured our hypothesis. In this work, we saw the problem with a different aspect which have not attracted the attention of the researcher. We suggest that future research on emotion recognition should adapt our approach to collect more such kinds of data so that emotion recognition using EEG can go beyond the recognition of basic emotions using facial features or body physiology toward more complex emotions.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Complete Flowgram of the Experiment",
      "page": 3
    },
    {
      "caption": "Figure 1: Phase-1. We call our dataset ’Dataset on Emotion",
      "page": 3
    },
    {
      "caption": "Figure 2: Valence and arousal rating scales used in the experiment.",
      "page": 5
    },
    {
      "caption": "Figure 3: Model Architecture: It is consisted of two 2D-convolution layers with 3 × 3 kernels and 32 ﬁlters and 64",
      "page": 7
    },
    {
      "caption": "Figure 3: CNN are often placed in the initial layers as it helps in local pattern learning from spectrogram or in general input data.",
      "page": 7
    },
    {
      "caption": "Figure 6: a shows a F1 score comparison between the DEAP and DENS dataset per trial. Using t-test statistical testing,",
      "page": 8
    },
    {
      "caption": "Figure 6: b shows a F1 score comparison between the SEED and DENS dataset per trial. Using t-test statistical testing,",
      "page": 8
    },
    {
      "caption": "Figure 4: Comparison of Confusion matrices for DEAP and DENS datasets over Valence-Arousal space. This space",
      "page": 9
    },
    {
      "caption": "Figure 5: Comparison of Confusion matrices for SEED and DENS datasets over Valence space. This space is divided",
      "page": 10
    },
    {
      "caption": "Figure 6: F1 scores of DENS comparing with DEAP and SEED datasets for all the 25 trials.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Selected stimuli for EEG study from the stimuli dataset [25]). The time duration of each stimulus is 60 s.",
      "data": [
        {
          "Stimulus Name": "ashayen (199)\nhorror (214)\nAnacondas The Hunt for the Blood Or-\nchid clip (10)\nLage Raho Munnabhai Only the Funny\nScenes\nanger legend of baghat singh (198)\nDivergent Kiss scene clip\nButterﬂy Nets (40)\nBest Horror Kills Ghost Ship Opening\nscene (26)\nJai Ho (92)\nSaddaHaq (152)\nMASOOM (109)\ncheerfulRang (201)\nCrash Saddest scene (51)\nhate lbs (210)\nMadari movie of best scene (113)\nFinal Race of Milkha Singh Career (67)",
          "Target Emotion": "Adventorous\nAfraid\nAlarmed\nAmused (98)\nAngry\nAroused (54)\nCalm\nDisgust\nEnthusiastic\nExcited\nHappy\nJoyous\nMelancholic\nMiserable\nSad\nTriumphant"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Parameter": "Optimizer",
          "Setting": "Adam"
        },
        {
          "Parameter": "Loss function",
          "Setting": "Categorical Cross-entropy"
        },
        {
          "Parameter": "Learning rate",
          "Setting": "0.001"
        },
        {
          "Parameter": "Adjustment",
          "Setting": "Early Stopping criteria: monitor - ’val_loss’, patience = 30\nModel Checkpoint: monitor - ’val_accuracy’"
        },
        {
          "Parameter": "Batch size",
          "Setting": "256"
        },
        {
          "Parameter": "Epochs",
          "Setting": "100"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Measures of emotion: A review",
      "authors": [
        "B Iris",
        "Mauss",
        "Michael D Robinson"
      ],
      "year": "2009",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "2",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "3",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "4",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "5",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "Juan Abdon",
        "Mojtaba Khomami Abadi",
        "Nicu Sebe",
        "Ioannis Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Recognition of human emotions using eeg signals: A review",
      "authors": [
        "Ajay Md Mustafizur Rahman",
        "Md Sarkar",
        "Md Amzad Hossain",
        "Md Rabiul Selim Hossain",
        "Md Islam",
        "Julian Mw Biplob Hossain",
        "Mohammad Quinn",
        "Moni Ali"
      ],
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "7",
      "title": "Determination of the hydrocel geodesic sensor nets' average electrode positions and their 10-10 international equivalents. Inc, Technical Note",
      "authors": [
        "Phan Luu",
        "Thomas Ferree"
      ],
      "year": "2005",
      "venue": "Determination of the hydrocel geodesic sensor nets' average electrode positions and their 10-10 international equivalents. Inc, Technical Note"
    },
    {
      "citation_id": "8",
      "title": "Human emotions",
      "authors": [
        "Carroll E Izard"
      ],
      "year": "2013",
      "venue": "Human emotions"
    },
    {
      "citation_id": "9",
      "title": "Basic emotions, natural kinds, emotion schemas, and a new paradigm",
      "authors": [
        "Carroll E Izard"
      ],
      "year": "2007",
      "venue": "Perspectives on psychological science"
    },
    {
      "citation_id": "10",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "11",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "Russell James"
      ],
      "year": "2003",
      "venue": "Psychological review"
    },
    {
      "citation_id": "12",
      "title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals",
      "authors": [
        "K Gyanendra",
        "Uma Verma",
        "Tiwary Shanker"
      ],
      "year": "2014",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "13",
      "title": "The theory of emotion: I: Emotional attitudes",
      "authors": [
        "John Dewey"
      ],
      "year": "1894",
      "venue": "Psychological review"
    },
    {
      "citation_id": "14",
      "title": "The james-lange theory of emotions: A critical examination and an alternative theory",
      "authors": [
        "Walter B Cannon"
      ],
      "year": "1927",
      "venue": "The American journal of psychology"
    },
    {
      "citation_id": "15",
      "title": "The emotional brain",
      "authors": [
        "Tim Dalgleish"
      ],
      "year": "2004",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Neural, electrophysiological and anatomical basis of brain-network variability and its characteristic changes in mental disorders",
      "authors": [
        "Jie Zhang",
        "Wei Cheng",
        "Zhaowen Liu",
        "Kai Zhang",
        "Xu Lei",
        "Ye Yao",
        "Benjamin Becker",
        "Yicen Liu",
        "Keith Kendrick",
        "Guangming Lu"
      ],
      "year": "2016",
      "venue": "Brain"
    },
    {
      "citation_id": "17",
      "title": "Dynamic shifts in large-scale brain network balance as a function of arousal",
      "authors": [
        "Christina B Young",
        "Gal Raz",
        "Daphne Everaerd",
        "Indira Christian F Beckmann",
        "Talma Tendolkar",
        "Guillén Hendler",
        "Erno Fernández",
        "Hermans"
      ],
      "year": "2017",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "18",
      "title": "Functional connectivity dynamics during film viewing reveal common networks for different emotional experiences",
      "authors": [
        "Gal Raz",
        "Alexandra Touroutoglou",
        "Christine Wilson-Mendenhall",
        "Gadi Gilam",
        "Tamar Lin",
        "Tal Gonen",
        "Yael Jacob",
        "Shir Atzil",
        "Roee Admon",
        "Maya Bleich-Cohen"
      ],
      "year": "2016",
      "venue": "Cognitive, Affective, & Behavioral Neuroscience"
    },
    {
      "citation_id": "19",
      "title": "Dynamic intersubject neural synchronization reflects affective responses to sad music",
      "authors": [
        "Assal Matthew E Sachs",
        "Antonio Habibi",
        "Jonas Damasio",
        "Kaplan"
      ],
      "year": "2020",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "20",
      "title": "Default and control network connectivity dynamics track the stream of affect at multiple timescales",
      "authors": [
        "Giada Lettieri",
        "Giacomo Handjaras",
        "Francesca Setti",
        "Elisa Cappello",
        "Valentina Bruno",
        "Matteo Diano",
        "Andrea Leo",
        "Emiliano Ricciardi",
        "Pietro Pietrini",
        "Luca Cecchetti"
      ],
      "year": "2022",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "21",
      "title": "Repeated movie viewings produce similar local activity patterns but different network configurations",
      "authors": [
        "Michael Andric",
        "Susan Goldin-Meadow",
        "Steven Small",
        "Uri Hasson"
      ],
      "year": "2016",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "22",
      "title": "Applications of the short time fourier transform to speech processing and spectral analysis. In ICASSP'82",
      "authors": [
        "Allen"
      ],
      "year": "1982",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Deep learning-based electroencephalography analysis: a systematic review",
      "authors": [
        "Yannick Roy",
        "Hubert Banville",
        "Isabela Albuquerque",
        "Alexandre Gramfort",
        "H Tiago",
        "Jocelyn Falk",
        "Faubert"
      ],
      "year": "2019",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "24",
      "title": "Deep learning with Python",
      "authors": [
        "Nikhil Ketkar",
        "Eder Santana"
      ],
      "year": "2017",
      "venue": "Deep learning with Python"
    },
    {
      "citation_id": "25",
      "title": "Affective film dataset from india (afdi): Creation and validation with an indian sample",
      "authors": [
        "Sudhakar Mishra",
        "Narayanan Srinivasan",
        "Uma Shanker"
      ],
      "year": "2021",
      "venue": "Affective film dataset from india (afdi): Creation and validation with an indian sample"
    },
    {
      "citation_id": "26",
      "title": "Cardiac-brain dynamics depend on context familiarity and their interaction predicts experience of emotional arousal",
      "authors": [
        "Sudhakar Mishra",
        "Narayanan Srinivasan",
        "Uma Shanker"
      ],
      "year": "2022",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "27",
      "title": "Dynamic functional connectivity of emotion processing in beta band with naturalistic emotion stimuli",
      "authors": [
        "Sudhakar Mishra",
        "Narayanan Srinivasan",
        "Uma Shanker"
      ],
      "year": "2022",
      "venue": "Brain sciences"
    },
    {
      "citation_id": "28",
      "title": "Naturalistic stimuli in affective neuroimaging: a review",
      "authors": [
        "Heini Saarimäki"
      ],
      "year": "2021",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "29",
      "title": "Classifying different emotional states by means of eeg-based functional connectivity patterns",
      "authors": [
        "You-Yun Lee",
        "Shulan Hsieh"
      ],
      "year": "2014",
      "venue": "PloS one"
    }
  ]
}