{
  "paper_id": "2409.09282v1",
  "title": "Turbo Your Multi-Modal Classification With Contrastive Learning",
  "published": "2024-09-14T03:15:34Z",
  "authors": [
    "Zhiyu Zhang",
    "Da Liu",
    "Shengqiang Liu",
    "Anna Wang",
    "Jie Gao",
    "Yali Li"
  ],
  "keywords": [
    "contrastive learning",
    "multi-modal representation learning",
    "audio-text classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Contrastive learning has become one of the most impressive approaches for multi-modal representation learning. However, previous multi-modal works mainly focused on cross-modal understanding, ignoring in-modal contrastive learning, which limits the representation of each modality. In this paper, we propose a novel contrastive learning strategy, called T urbo, to promote multi-modal understanding by joint in-modal and crossmodal contrastive learning. Specifically, multi-modal data pairs are sent through the forward pass twice with different hidden dropout masks to get two different representations for each modality. With these representations, we obtain multiple inmodal and cross-modal contrastive objectives for training. Finally, we combine the self-supervised Turbo with the supervised multi-modal classification and demonstrate its effectiveness on two audio-text classification tasks, where the state-ofthe-art performance is achieved on a speech emotion recognition benchmark dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recently, as an effective self-supervised strategy of learning representations, contrastive learning has been successfully applied in multi-modal tasks. By learning the alignment between different modalities, contrastive learning projects different modal representations into a joint semantic space. For example, CLIP  [1]  uses large-scale image text pairs to learn unimodal representations and bring the representations into the joint multi-modal space through contrastive learning. Similarly, CLAP  [2]  learns to associate language and audio modalities by contrastive learning. They have achieved outstanding results comparable to supervised learning.\n\nThe general steps of previous multi-modal contrastive learning methods can be summarized as follows. First, the two modalities are modeled by their respective encoders to obtain the corresponding representation. Then the respective representations from the same multi-modal pair constitute the positive samples, and the rest in the mini-batch constitute negative samples. Finally, contrastive learning is used to learn representations by pulling positive samples closer together and pushing negative samples farther away. However, this approach only focuses on the contrast of cross-modal representations, ignoring in-modal contrastive learning. The success of uni-modal contrastive learning methods, such as SimCLR  [3]  and SimCSE  [4] , has proven that in-modal contrastive learning can effectively en-⋆ Equal contributions † Corresponding author hance representation learning. Accordingly, we propose simultaneously incorporating in-modal and cross-modal contrastive learning to promote multi-modal understanding. On the other hand, previous multi-modal contrastive learning approaches are usually used in pre-training, which requires enormous amounts of pair-wise multi-modal data. However, collecting and cleaning pair-wise data is difficult, especially in specific domains. Therefore, we adopt a simple approach that directly combines the supervised multi-modal classification task with contrastive learning.\n\nIn this paper, we propose a novel self-supervised contrastive learning method called T urbo and apply it to multi-modal classification. Motivated by the idea of R-Drop  [5]  and SimCSE  [4] , we use dropout  [6]  twice to get multiple representations for two modalities and utilize these representations to compare and learn in-modal and cross-modal information. Dropout is usually a way to regularize a network, but here we are using its randomness to get different representations for data augmentation. Specifically, in the training step, we let each multi-modal data pair go through the forward pass twice and get two different representations for each modality by randomly dropping out some hidden units. Then we carry out in-modal contrastive learning for the two representations from the same modality and cross-modal contrastive learning for the representation of different modalities. The loss of the Turbo we defined is the sum of several in-modal and cross-modal contrastive losses. Finally, we train Turbo as an auxiliary task to improve the performance of the multi-modal classification tasks. Comparing to previous multi-modal methods, Turbo first propose to incorporate in-modal contrastive learning and obtains multiple in-modal and cross-modal contrastive objectives from the same input pair, enhancing the generalization of representation learning.\n\nBy combining the Turbo training method, we have achieved significant improvements in audio-text classification tasks. Compared to the baseline, we observed accuracy improvements of 5.59% for the speech emotion recognition task and 3.83% for the device-directed speech detection task. At the same time, we outperform the previous systems on the emotion recognition benchmark dataset IEMOCAP  [7]  and achieve state-ofthe-art performance. Further analysis of experimental results shows that the Turbo method can improve the alignment and unif ormity  [8]  of modal representations, thus improving the performance of the system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "The overall framework of our model with the Turbo method is shown in Fig.  1 . We take acoustics and language modalities as an example to introduce the proposed multi-modal framework. Audio-text pairs are firstly sent into an audio encoder,  e.g., wav2vec2  [9]  or Whisper  [10] , and a text encoder, e.g., BERT  [11]  or GPT  [12] , respectively. Then we construct multiple in-modal and cross-modal contrastive objectives for audiotext pairs in a mini-batch using the Turbo method. Concurrently, representations of audio-text pairs will be concatenated and sent to a linear classifier. Finally, we jointly optimize the classifier's supervised loss and the Turbo's self-supervised loss. We show numpy-like pseudocode for the Turbo in Listing 1.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Encoder With Dropout Mask",
      "text": "For each input audio-text pair {xa, xt}, we first feed the audio and text to the encoder respectively to get the utterancelevel embedding. Let ea ∈ R d1 denote the audio embedding, and et ∈ R d2 denote the text embedding after the encoder. Each audio-text pair can be denoted as {ea, et}i in a mini-batch, where i ∈ [0, N ] and N is the batch size. Then we send audio and text embeddings, ea and et, into a joint multi-modal space of dimension d by using a trainable fully-connected layer:\n\nwhere ha ∈ R d , ht ∈ R d , F Ca and F Ct are the fully-connected layers for audio and text respectively. After the above process, we can obtain {ha, ht}i for the audio-text pair {xa, xt}i through a training forward pass. Inspired by SimCSE  [4]  and R-Drop  [5] , dropout can be used in data augmentation and regularizing the output predictions. We apply it to multi-modal contrastive learning. Concretely, we place dropout masks on the encoders and fullyconnected layers. Dropout will randomly drop part of units in each neural network layer. We feed the same input audio-text pair {xa, xt}i to the forward pass of the network twice. Because of different hidden dropout masks, we can get two representation pairs: {h 1 a , h 1 t }i and {h 2 a , h 2 t }i.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "In-Modal And Cross-Modal Contrastive Learning",
      "text": "Previous works, such as CLIP and CLAP, usually focus on cross-modal contrastive learning and ignore in-modal information, weakening the ability to extract information from a single modality. Consequently, we add the extra in-modal contrastive learning in the training step. Taking the audio modality as an example, h 1 a and h 2 a are similar audio representations obtained from the two forward passes. Then we take them as \"positive pairs,\" and other audio representations in the same mini-batch as \"negatives\" during contrastive learning. The specific training objective for in-modal contrastive learning can be defined as follows:\n\nwhere ℓ a in is in-modal InfoNCE loss  [13]  of the audio modality, N is the batch size, τ is a temperature hyper-parameter, sim(.) denotes the cosine similarity calculation and h cross , ℓ 1,2 cross , ℓ 2,1 cross and ℓ 2,2 cross . Taking ℓ 1,1 cross as an example, it means the contrastive loss between h 1 a and h 1 t in the mini-batch, which can be defined as:\n\nThen, incorporating in-modal and cross-modal contrastive learning, the loss of Turbo is formulated as:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Supervised Classification With Turbo",
      "text": "For each labeled audio-text pair, we obtain two representation pairs {h 1 a , h 1 t }i and {h 2 a , h 2 t }i after two forward passes. Each pair can be chosen to be trained for supervised classification. Taking {h 1 a , h 1 t }i as an example, we concatenate it and send it through a simple linear classifier. After that, the predicted probability distribution ŷ for the target class is as follows:\n\nwhere W and b are trainable parameters, and ⊕ denotes the concatenation operation. Multi-class cross-entropy (CE) is chosen as the loss function of this classifier: ℓce. Finally, we treat Turbo as an auxiliary task for classification. The overall objective will be:\n\nwhere λ is a balancing hyperparameter.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "To evaluate the proposed multi-modal model, we conduct experiments on two audio-text classification tasks, i.e., speech emotion recognition (SER) and device-directed speech detection (DSD). Speech Emotion Recognition: The purpose of the SER task is to identify whether the audio recording belongs to one of the categories, such as happy, sad, angry, or neutral. We use the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [7] , an open multi-modal emotion recognition benchmark dataset. Following previous works  [14]  [15], the dataset contains 5,531 utterances in total (1,636 happy, 1,084 sad, 1,103 angry and 1,708 neutral). In the training process, we perform 10-fold cross-validation where each 8, 1, and 1 folds are used for the train, validation, and test sets, respectively. The performance of this task is measured in widely used evaluation metrics: weighted accuracy (WA) which is the overall classification accuracy and unweighted accuracy (UA) which is the average accuracy over the emotion categories. Device-directed Speech Detection: The DSD task aims to distinguish voice queries intended for a virtual voice assistant device from background speech  [16] [17] . Since this is no publicly available data for this task, we evaluate it on the real-life inhouse dataset, named REJ, which consists of audio utterances with ground truth annotations of device-directed or non-devicedirected. The dataset contains roughly 50K utterances and is split into train, validation, and test partitions with 40K, 5K, and 5K utterances, respectively. The class split is roughly 1:1 to balance the two classes in each partition. We utilize the equal error (EER) and accuracy (ACC) to measure the classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use wav2vec 2.0-base and BERT-base models from Hug-gingFace repositories  1  as the audio and text encoders, which both have 768-dimensional embeddings. The optimizer for the model is Adam, with a learning rate of 1e-5. The training batch size is 64, and we set the early stopping setting as ten epochs.\n\nDropout is applied with a probability of 0.2 after every feedforward layer except the output layer. The hyperparameter λ is set to 0.5 according to experimental results. All experiments are conducted on a single Nvidia RTX 3090 GPU.\n\nFor comparison, we build two multi-modal models as our baselines. The first one, the vanilla model, directly concatenates the output of audio and text encoders for classification. It forwards only once and without any contrastive learning loss. The second one is based on the vanilla model with a simple cross-modal contrastive learning loss, similar to CLAP  [2] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "Table  1  shows the results of the mentioned approaches for the IEMOCAP and REJ datasets. +CLcross denotes the second baseline with cross-modal contrastive loss. Overall, the proposed framework with the Turbo method substantially outperforms the vanilla model, a strong baseline based on the pretrained wav2vec and BERT in both tasks. For the IEMOCAP dataset, the proposed Turbo achieves improvements of 5.59% and 5.32% on UA and WA. Equally, Turbo improves the accuracy by 3.83% and reduces EER by 2.62% for the REJ dataset, confirming the proposed method's effectiveness and universality. In addition, comparing the second baseline with the vanilla model, we can find that cross-modal contrastive learning improves performance for multi-modal classification. This is because the contrastive loss will guide the classification loss during training and affect the representations to improve audio-text alignment. Compared with the two baselines, Turbo yields stable improvements no matter what dataset, which shows the superiority of our special in-modal and cross-modal contrastive learning method to understand multi-modal information.\n\nThe comparison results of our method with some previous multi-modal methods on the IEMOCAP are listed in Table  2 . These methods have learned modal information by directly calculating the correlation between different modalities based on the attention mechanism, which can generate extra noise information and damages the performance. They concentrated on the complex strategy of fusing multiple modalities, without considering the distribution of the representations within each modality. In contrast, we use Turbo contrastive learning method to guide the representations of different modalities to aligned feature space. The experimental results indicate that the proposed method can improve the ability of the fusion of modalities without any complex fusion methods (we only use the method of concatenating features directly) and achieve state-of-the-art performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analysis",
      "text": "To better understand the effectiveness of Turbo, we use alignment and unif ormity  [8]  to measure the quality of learned representations. The pseudocode for these two metric calculations is shown in listing 2, which is slightly different from the uni-modal computing method in  [8] . Alignment means the alignment (closeness) between corresponding crossmodal audio-text pairs, and unif ormity means the uniformity (coverage) of the uni-modal representation space on the unit hypersphere. In general, models with better alignment and uniformity can achieve better performance.\n\nWe show the alignment and uniformity plot of three models evaluated on the IEMOCAP in Fig.  2 . Lower numbers of alignment and uniformity are better. For each method, we train three times and visualize the embeddings in the figure. There are several observations. Compared with the vanilla model, the \"+CLcross\" model dramatically improves the alignment because of the cross-modal contrastive learning. Turbo can further improve the alignment since it has multiple cross-modal contrastive objectives for the same audio-text pair. In terms of uniformity, the Turbo method is substantially superior to the \"+CLcross\" and vanilla models. That is because Turbo performs additional in-modal contrastive learning. This phenomenon is also reflected in Fig.  3 , in which we visualize the distributions of audio representation space for three models. Results indicate that the Turbo method achieves the most uniform distribution, which again proves the superiority of our method.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we propose Turbo, a contrastive learning strategy for multi-modal classification tasks. Unlike previous works that only focus on cross-modal contrastive learning, Turbo performs in-modal contrastive learning simultaneously. By forwarding twice, Turbo obtains multiple in-modal and cross-modal contrastive objectives for the same input pair, which enhances the generalization of representation learning. We demonstrate the effectiveness of our method on two audio-text classification tasks. Compared with the vanilla method, we achieved a 5.59% improvement in weighted accuracy on the SER task and a 3.83% improvement in weighted accuracy on the DSD task. Further analysis indicates that Turbo can effectively enhance the uniformity and alignment of multi-modal representations.\n\nIn the future, we will explore extending our proposed Turbo to audio-text pre-trained learning and improve performance for the downstream tasks. We also intent to expand our proposed multi-modal learning method to include the visual domain and integrate three modalities for classification.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our proposed classification framework with Turbo",
      "page": 2
    },
    {
      "caption": "Figure 2: The align-uniform plot of models",
      "page": 4
    },
    {
      "caption": "Figure 3: Feature distributions with Gaussian kernel density",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "yali.li@nio.com": "hance representation learning. Accordingly, we propose simul-"
        },
        {
          "yali.li@nio.com": "taneously incorporating in-modal and cross-modal contrastive"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "learning to promote multi-modal understanding. On the other"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "hand, previous multi-modal contrastive learning approaches are"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "usually used in pre-training, which requires enormous amounts"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "of pair-wise multi-modal data. However, collecting and clean-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "ing pair-wise data is difficult, especially in specific domains."
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "Therefore, we adopt a simple approach that directly combines"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "the supervised multi-modal classification task with contrastive"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "learning."
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "In this paper, we propose a novel self-supervised contrastive"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "learning method called T urbo and apply it to multi-modal clas-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "sification. Motivated by the idea of R-Drop [5] and SimCSE"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "[4], we use dropout [6]\ntwice to get multiple representations"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "for two modalities and utilize these representations to compare"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "and learn in-modal and cross-modal\ninformation. Dropout\nis"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "usually a way to regularize a network, but here we are using its"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "randomness to get different representations for data augmenta-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "tion. Specifically,\nin the training step, we let each multi-modal"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "data pair go through the forward pass twice and get\ntwo dif-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "ferent representations for each modality by randomly dropping"
        },
        {
          "yali.li@nio.com": "out some hidden units. Then we carry out in-modal contrastive"
        },
        {
          "yali.li@nio.com": "learning for the two representations from the same modality and"
        },
        {
          "yali.li@nio.com": "cross-modal contrastive learning for the representation of differ-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "ent modalities. The loss of the Turbo we defined is the sum of"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "several\nin-modal and cross-modal contrastive losses.\nFinally,"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "we train Turbo as an auxiliary task to improve the performance"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "of\nthe multi-modal classification tasks.\nComparing to previ-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "ous multi-modal methods, Turbo first propose to incorporate"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "in-modal contrastive learning and obtains multiple in-modal and"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "cross-modal contrastive objectives from the same input pair, en-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "hancing the generalization of representation learning."
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "By combining the Turbo training method, we have achieved"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "significant\nimprovements\nin\naudio-text\nclassification\ntasks."
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "Compared to the baseline, we observed accuracy improvements"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "of 5.59% for\nthe speech emotion recognition task and 3.83%"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "for the device-directed speech detection task. At the same time,"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "we outperform the previous systems on the emotion recogni-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "tion benchmark dataset\nIEMOCAP [7] and achieve state-of-"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "the-art performance.\nFurther analysis of experimental\nresults"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "shows that\nthe Turbo method can improve the alignment and"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "unif ormity [8] of modal representations,\nthus improving the"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "performance of the system."
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "2. Method"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "The overall\nframework of our model with the Turbo method"
        },
        {
          "yali.li@nio.com": ""
        },
        {
          "yali.li@nio.com": "is shown in Fig.1. We take acoustics and language modalities"
        },
        {
          "yali.li@nio.com": "as an example to introduce the proposed multi-modal\nframe-"
        },
        {
          "yali.li@nio.com": "work. Audio-text pairs are firstly sent\ninto an audio encoder,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "example, h1\nand h2\nare similar audio representations obtained"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "from the two forward passes. Then we take them as “positive"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "pairs,” and other audio representations in the same mini-batch"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "as “negatives” during contrastive learning. The specific train-"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "ing objective for\nin-modal contrastive learning can be defined"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "as follows:"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "exp (cid:0)sim (cid:0)h1\n(cid:1) /τ (cid:1)\na,i, h2"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "a,i"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "N(cid:88) i\nℓa\n(2)\nin = − log"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "(cid:80)N\n(cid:1) /τ (cid:1)"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "a,i, h2\na,j\nj=1 exp (cid:0)sim (cid:0)h1\n=1"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "where ℓa\nin is in-modal InfoNCE loss [13] of the audio modal-"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "ity, N is\nthe batch size, τ\nis a temperature hyper-parameter,"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "sim(.) denotes the cosine similarity calculation and h1\na,i, h2\na,i"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "represent the audio representations for first and second forward"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "passes respectively.\nFor\nthe text modality, we apply the same"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "process to get ℓt\nin."
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "Since\nthere\nare\ntwo\naudio-text\nrepresentation\npairs"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "{h1\nfor\nthe same pair {xa, xt}i, we can\na , h1\nt }i and {h2\na , h2\nt }i"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "build\nfour\ndifferent\ncross-modal\ncontrastive\nobjectives,\ni.e."
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "{h1\nThen we\ncan\na , h1\nt }i, {h1\na , h2\nt }i, {h2\na , h1\nt }i, {h2\na , h2\nt }i."
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "respectively calculate the corresponding cross-modal InfoNCE"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "Loss: ℓ1,1\ncross, ℓ1,2\ncross, ℓ2,1\ncross and ℓ2,2\ncross. Taking ℓ1,1\ncross as an ex-"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "ample,\nit means the contrastive loss between h1\nand h1\nin the"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "mini-batch, which can be defined as:"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "exp (cid:0)sim (cid:0)h1\n(cid:1) /τ (cid:1)\na,i, h1\nt,i"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "ℓ1,1"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "N(cid:88) i\n(3)\ncross = − log"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "(cid:80)N\n(cid:1) /τ (cid:1)"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "a,i, h1\nt,j\nj=1 exp (cid:0)sim (cid:0)h1\n=1"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "Then,\nincorporating in-modal and cross-modal contrastive"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "learning, the loss of Turbo is formulated as:"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "(4)\nℓT urbo = ℓa\nin + ℓt\nin + ℓ1,1\ncross + ℓ1,2\ncross + ℓ2,1\ncross + ℓ2,2\ncross"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "2.3.\nSupervised Classification with Turbo"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "For each labeled audio-text pair, we obtain two representation"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "pairs {h1\na , h1\nt }i and {h2\na , h2\nt }i after two forward passes. Each"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "pair can be chosen to be trained for supervised classification."
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "Taking {h1\na , h1\nt }i as an example, we concatenate it and send"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "it\nthrough a simple linear classifier. After\nthat,\nthe predicted"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "probability distribution ˆy for the target class is as follows:"
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": ""
        },
        {
          "Figure 1: Overview of our proposed classification framework with Turbo": "y = softmax[ W · (h1\n(5)\na ⊕ h1\nt ) + b ]"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "teractive Emotional Dyadic Motion Capture (IEMOCAP)\n[7],",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "an open multi-modal emotion recognition benchmark dataset.",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "Following previous works [14] [15],\nthe dataset contains 5,531",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "utterances\nin total\n(1,636 happy, 1,084 sad, 1,103 angry and",
          "Table 2: Comparison with previous methods on the IEMOCAP": "3.2. Experimental Setup"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "1,708 neutral).\nIn the training process, we perform 10-fold",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "We use wav2vec 2.0-base and BERT-base models from Hug-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "cross-validation where each 8, 1, and 1 folds are used for\nthe",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "gingFace repositories1 as the audio and text encoders, which"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "train, validation, and test sets,\nrespectively.\nThe performance",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "both have 768-dimensional embeddings. The optimizer for the"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "of\nthis\ntask is measured in widely used evaluation metrics:",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "model is Adam, with a learning rate of 1e-5. The training batch"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "weighted accuracy (WA) which is the overall classification ac-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "size is 64, and we set\nthe early stopping setting as ten epochs."
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "curacy and unweighted accuracy (UA) which is the average ac-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "Dropout\nis applied with a probability of 0.2 after every feed-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "curacy over the emotion categories.",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "forward layer except the output layer. The hyperparameter λ is"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "Device-directed Speech Detection: The DSD task aims to dis-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "set to 0.5 according to experimental results. All experiments are"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "tinguish voice queries intended for a virtual voice assistant de-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "conducted on a single Nvidia RTX 3090 GPU."
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "vice from background speech [16] [17]. Since this is no publicly",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "For comparison, we build two multi-modal models as our"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "available data for\nthis task, we evaluate it on the real-life in-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "baselines.\nThe first one,\nthe vanilla model, directly concate-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "house dataset, named REJ, which consists of audio utterances",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "nates the output of audio and text encoders for classification.\nIt"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "with ground truth annotations of device-directed or non-device-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "forwards only once and without any contrastive learning loss."
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "directed.\nThe dataset contains roughly 50K utterances and is",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "The second one is based on the vanilla model with a simple"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "split into train, validation, and test partitions with 40K, 5K, and",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "cross-modal contrastive learning loss, similar to CLAP[2]."
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "5K utterances, respectively. The class split is roughly 1:1 to bal-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "ance the two classes in each partition. We utilize the equal error",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "3.3. Results"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "rate (EER) and accuracy (ACC) to measure the classification.",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "Table 1 shows the results of the mentioned approaches for the"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "# audio_encoder\n-\nWav2vec2 or\nWhisper",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "IEMOCAP and REJ datasets. +CLcross denotes the second"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "# text_encoder\n-\nBert\nor\nGPT",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "baseline with cross-modal contrastive loss. Overall,\nthe pro-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "# extract\nfeature\nrepresentations",
          "Table 2: Comparison with previous methods on the IEMOCAP": "posed framework with the Turbo method substantially outper-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "A_1st\n=\naudio_encoder(audio_input)",
          "Table 2: Comparison with previous methods on the IEMOCAP": "forms\nthe vanilla model, a strong baseline based on the pre-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "T_1st\n=\ntext_encoder(text_input)",
          "Table 2: Comparison with previous methods on the IEMOCAP": "trained wav2vec and BERT in both tasks. For the IEMOCAP"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "dataset,\nthe proposed Turbo achieves improvements of 5.59%"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "# second\nforward\ncalculation",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "and 5.32% on UA and WA. Equally, Turbo improves the accu-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "A_2nd\n=\naudio_encoder(audio_input)",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "racy by 3.83% and reduces EER by 2.62% for the REJ dataset,"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "T_2nd\n=\ntext_encoder(text_input)",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "confirming the proposed method’s effectiveness and universal-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "# in-modal\ncontrastive\nlearning",
          "Table 2: Comparison with previous methods on the IEMOCAP": "ity.\nIn addition, comparing the second baseline with the vanilla"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "loss\n+=\nInfoNce(np.dot(A_1st,\nA_2nd))",
          "Table 2: Comparison with previous methods on the IEMOCAP": "model, we can find that cross-modal contrastive learning im-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "loss\n+=\nInfoNce(np.dot(T_1st,\nT_2nd))",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "proves performance for multi-modal classification. This is be-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "cause the contrastive loss will guide the classification loss dur-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "# cross-modal\ncontrastive\nlearning",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "ing training and affect the representations to improve audio-text"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "loss\n+=\nInfoNce(np.dot(A_1st,\nT_1st))",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "alignment. Compared with the two baselines, Turbo yields sta-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "loss\n+=\nInfoNce(np.dot(A_1st,\nT_2nd))",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "loss\n+=\nInfoNce(np.dot(A_2nd,\nT_1st))",
          "Table 2: Comparison with previous methods on the IEMOCAP": "ble improvements no matter what dataset, which shows the su-"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "loss\n+=\nInfoNce(np.dot(A_2nd,\nT_2nd))",
          "Table 2: Comparison with previous methods on the IEMOCAP": "periority of our special\nin-modal and cross-modal contrastive"
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "",
          "Table 2: Comparison with previous methods on the IEMOCAP": "learning method to understand multi-modal information."
        },
        {
          "categories, such as happy, sad, angry, or neutral. We use the In-": "Listing 1: Numpy-like pseudocode for the core of an implemen-",
          "Table 2: Comparison with previous methods on the IEMOCAP": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Method"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "concatenation operation. Multi-class cross-entropy (CE) is cho-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "WA\nUA\nACC\nEER"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "sen as the loss function of this classifier: ℓce.",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "Finally, we treat Turbo as an auxiliary task for classifica-",
          "IEMOCAP\nREJ": "V anilla\n0.7076\n0.7177\n0.8944\n0.0938"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "tion. The overall objective will be:",
          "IEMOCAP\nREJ": "0.7238\n0.7265\n0.9152\n0.0786\n+CLcross"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "+T urbo\n0.7635\n0.7709\n0.9327\n0.0676"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "(6)\nℓtotal = λℓce + (1 − λ)ℓT urbo",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Table 1: Comparison of baselines and our method"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "where λ is a balancing hyperparameter.",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "3. Experiments",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Method\nYear\nWA\nUA"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "3.1. Datasets",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Xu et al. [18]\n2019\n0.725\n0.709"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Liu et al. [15]\n2020\n0.724\n0.701"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "To evaluate the proposed multi-modal model, we conduct ex-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Krishna et al. [14]\n2020\n-\n0.728"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "periments on two audio-text classification tasks,\ni.e.,\nspeech",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Li et al. [19]\n2020\n0.727\n0.735"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "emotion recognition (SER) and device-directed speech detec-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Makiuchi et al. [20]\n2021\n0.735\n0.730"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "tion (DSD).",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Chen et al. [21]\n2022\n0.743\n0.753"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "Speech Emotion Recognition: The purpose of\nthe SER task",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "0.764\n0.771\nOurs\n2023"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "is to identify whether the audio recording belongs to one of the",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "categories, such as happy, sad, angry, or neutral. We use the In-",
          "IEMOCAP\nREJ": "Table 2: Comparison with previous methods on the IEMOCAP"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "teractive Emotional Dyadic Motion Capture (IEMOCAP)\n[7],",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "an open multi-modal emotion recognition benchmark dataset.",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "Following previous works [14] [15],\nthe dataset contains 5,531",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "utterances\nin total\n(1,636 happy, 1,084 sad, 1,103 angry and",
          "IEMOCAP\nREJ": "3.2. Experimental Setup"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "1,708 neutral).\nIn the training process, we perform 10-fold",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "We use wav2vec 2.0-base and BERT-base models from Hug-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "cross-validation where each 8, 1, and 1 folds are used for\nthe",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "gingFace repositories1 as the audio and text encoders, which"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "train, validation, and test sets,\nrespectively.\nThe performance",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "both have 768-dimensional embeddings. The optimizer for the"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "of\nthis\ntask is measured in widely used evaluation metrics:",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "model is Adam, with a learning rate of 1e-5. The training batch"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "weighted accuracy (WA) which is the overall classification ac-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "size is 64, and we set\nthe early stopping setting as ten epochs."
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "curacy and unweighted accuracy (UA) which is the average ac-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Dropout\nis applied with a probability of 0.2 after every feed-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "curacy over the emotion categories.",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "forward layer except the output layer. The hyperparameter λ is"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "Device-directed Speech Detection: The DSD task aims to dis-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "set to 0.5 according to experimental results. All experiments are"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "tinguish voice queries intended for a virtual voice assistant de-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "conducted on a single Nvidia RTX 3090 GPU."
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "vice from background speech [16] [17]. Since this is no publicly",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "For comparison, we build two multi-modal models as our"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "available data for\nthis task, we evaluate it on the real-life in-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "baselines.\nThe first one,\nthe vanilla model, directly concate-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "house dataset, named REJ, which consists of audio utterances",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "nates the output of audio and text encoders for classification.\nIt"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "with ground truth annotations of device-directed or non-device-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "forwards only once and without any contrastive learning loss."
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "directed.\nThe dataset contains roughly 50K utterances and is",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "The second one is based on the vanilla model with a simple"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "split into train, validation, and test partitions with 40K, 5K, and",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "cross-modal contrastive learning loss, similar to CLAP[2]."
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "5K utterances, respectively. The class split is roughly 1:1 to bal-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "ance the two classes in each partition. We utilize the equal error",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "3.3. Results"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "rate (EER) and accuracy (ACC) to measure the classification.",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "Table 1 shows the results of the mentioned approaches for the"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "# audio_encoder\n-\nWav2vec2 or\nWhisper",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "IEMOCAP and REJ datasets. +CLcross denotes the second"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "# text_encoder\n-\nBert\nor\nGPT",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "baseline with cross-modal contrastive loss. Overall,\nthe pro-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "# extract\nfeature\nrepresentations",
          "IEMOCAP\nREJ": "posed framework with the Turbo method substantially outper-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "A_1st\n=\naudio_encoder(audio_input)",
          "IEMOCAP\nREJ": "forms\nthe vanilla model, a strong baseline based on the pre-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "T_1st\n=\ntext_encoder(text_input)",
          "IEMOCAP\nREJ": "trained wav2vec and BERT in both tasks. For the IEMOCAP"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "dataset,\nthe proposed Turbo achieves improvements of 5.59%"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "# second\nforward\ncalculation",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "and 5.32% on UA and WA. Equally, Turbo improves the accu-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "A_2nd\n=\naudio_encoder(audio_input)",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "racy by 3.83% and reduces EER by 2.62% for the REJ dataset,"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "T_2nd\n=\ntext_encoder(text_input)",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "confirming the proposed method’s effectiveness and universal-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "# in-modal\ncontrastive\nlearning",
          "IEMOCAP\nREJ": "ity.\nIn addition, comparing the second baseline with the vanilla"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "loss\n+=\nInfoNce(np.dot(A_1st,\nA_2nd))",
          "IEMOCAP\nREJ": "model, we can find that cross-modal contrastive learning im-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "loss\n+=\nInfoNce(np.dot(T_1st,\nT_2nd))",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "proves performance for multi-modal classification. This is be-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "cause the contrastive loss will guide the classification loss dur-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "# cross-modal\ncontrastive\nlearning",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "ing training and affect the representations to improve audio-text"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "loss\n+=\nInfoNce(np.dot(A_1st,\nT_1st))",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "alignment. Compared with the two baselines, Turbo yields sta-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "loss\n+=\nInfoNce(np.dot(A_1st,\nT_2nd))",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "loss\n+=\nInfoNce(np.dot(A_2nd,\nT_1st))",
          "IEMOCAP\nREJ": "ble improvements no matter what dataset, which shows the su-"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "loss\n+=\nInfoNce(np.dot(A_2nd,\nT_2nd))",
          "IEMOCAP\nREJ": "periority of our special\nin-modal and cross-modal contrastive"
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "",
          "IEMOCAP\nREJ": "learning method to understand multi-modal information."
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "Listing 1: Numpy-like pseudocode for the core of an implemen-",
          "IEMOCAP\nREJ": ""
        },
        {
          "where W and b are trainable parameters, and ⊕ denotes the": "tation of Turbo.",
          "IEMOCAP\nREJ": "1https://huggingface.co"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Thesemethodshavelearnedmodalinformationbydirectlycal-",
      "data": [
        {
          "means the alignment (closeness) between corresponding cross-": "modal audio-text pairs, and unif ormity means the uniformity",
          "-0.5\n-0.5\n-0.5": "-1.0\n-1.0\n-1.0"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "(coverage) of the uni-modal representation space on the unit hy-",
          "-0.5\n-0.5\n-0.5": "-1.0-0.5 0.0 0.5 1.0\n-1.0-0.5 0.0 0.5 1.0\n-1.0-0.5 0.0 0.5 1.0"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "persphere. In general, models with better alignment and unifor-",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "mity can achieve better performance.",
          "-0.5\n-0.5\n-0.5": "Figure 3: Feature distributions with Gaussian kernel density"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "estimation (KDE) in R2."
        },
        {
          "means the alignment (closeness) between corresponding cross-": "We show the alignment and uniformity plot of three mod-",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "els evaluated on the IEMOCAP in Fig.2.\nLower numbers of",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "alignment and uniformity are better. For each method, we train",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "three times and visualize the embeddings in the figure. There",
          "-0.5\n-0.5\n-0.5": "4. Conclusions"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "are several observations. Compared with the vanilla model, the",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "In this work, we propose Turbo, a contrastive learning strategy"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "the alignment be-\n”+CLcross” model dramatically improves",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "for multi-modal classification tasks. Unlike previous works that"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "cause of\nthe cross-modal contrastive learning. Turbo can fur-",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "only focus on cross-modal contrastive learning, Turbo performs"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "ther\nimprove the alignment since it has multiple cross-modal",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "in-modal contrastive learning simultaneously.\nBy forwarding"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "contrastive objectives\nfor\nthe same audio-text pair.\nIn terms",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "twice, Turbo obtains multiple in-modal and cross-modal con-"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "of uniformity,\nthe Turbo method is\nsubstantially superior\nto",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "trastive objectives\nfor\nthe\nsame\ninput pair, which enhances"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "That\nis because Turbo\nthe ”+CLcross” and vanilla models.",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "the generalization of representation learning. We demonstrate"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "performs additional\nin-modal contrastive learning.\nThis phe-",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "the effectiveness of our method on two audio-text classifica-"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "nomenon is also reflected in Fig.3,\nin which we visualize the",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "tion tasks. Compared with the vanilla method, we achieved a"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "distributions of audio representation space for three models. Re-",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "5.59% improvement in weighted accuracy on the SER task and"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "sults indicate that the Turbo method achieves the most uniform",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "a 3.83% improvement\nin weighted accuracy on the DSD task."
        },
        {
          "means the alignment (closeness) between corresponding cross-": "distribution, which again proves the superiority of our method.",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "Further analysis indicates that Turbo can effectively enhance the"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "uniformity and alignment of multi-modal representations."
        },
        {
          "means the alignment (closeness) between corresponding cross-": "# A:\nthe\ndistribution\nof\naudio\nrepresentation",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "In the future, we will explore extending our proposed Turbo"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "# T:\nthe\ndistribution\nof\ntext\nrepresentation",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "to audio-text pre-trained learning and improve performance for"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "# the\ncalculation\nof\nalignment",
          "-0.5\n-0.5\n-0.5": "the downstream tasks. We also intent\nto expand our proposed"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "def\nlalign(A,\nT,\nalpha=2):",
          "-0.5\n-0.5\n-0.5": "multi-modal\nlearning method to include the visual domain and"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "return\n(A-T).norm(dim=1).pow(alpha).mean()",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "integrate three modalities for classification."
        },
        {
          "means the alignment (closeness) between corresponding cross-": "#\nthe\ncalculation\nof\nuniformity",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "def\nlunif(A,\nt=2):",
          "-0.5\n-0.5\n-0.5": "5. Acknowledgements"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "sq_pdist\n=\ntorch.pdist(A,\np=2).pow(2)",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "The INTERSPEECH 2023 organisers would like to thank ISCA"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "return\nsq_pdist.mul(-t).exp().mean().log()",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "and the organising committees of past INTERSPEECH confer-"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "ences for their help and for kindly providing the previous ver-"
        },
        {
          "means the alignment (closeness) between corresponding cross-": "Listing 2: Pytorch-like pseudocode for calculating alignment",
          "-0.5\n-0.5\n-0.5": ""
        },
        {
          "means the alignment (closeness) between corresponding cross-": "",
          "-0.5\n-0.5\n-0.5": "sion of this template."
        },
        {
          "means the alignment (closeness) between corresponding cross-": "and uniformity.",
          "-0.5\n-0.5\n-0.5": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: Thesemethodshavelearnedmodalinformationbydirectlycal-",
      "data": [
        {
          "The comparison results of our method with some previous": "multi-modal methods on the IEMOCAP are listed in Table 2.",
          "2.0": "Vanilla3\nVanilla2"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "Vanilla1"
        },
        {
          "The comparison results of our method with some previous": "These methods have learned modal information by directly cal-",
          "2.0": "1.8"
        },
        {
          "The comparison results of our method with some previous": "culating the correlation between different modalities based on",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "the attention mechanism, which can generate extra noise infor-",
          "2.0": "1.6"
        },
        {
          "The comparison results of our method with some previous": "mation and damages the performance. They concentrated on the",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "complex strategy of fusing multiple modalities, without consid-",
          "2.0": "Cross-modal align\n1.4"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "+CLcross2"
        },
        {
          "The comparison results of our method with some previous": "ering the distribution of the representations within each modal-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "+CLcross1"
        },
        {
          "The comparison results of our method with some previous": "ity.\nIn contrast, we use Turbo contrastive learning method to",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "1.2"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "+CLcross3"
        },
        {
          "The comparison results of our method with some previous": "guide the representations of different modalities to aligned fea-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "ture space. The experimental results indicate that\nthe proposed",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "1.0"
        },
        {
          "The comparison results of our method with some previous": "method can improve the ability of the fusion of modalities with-",
          "2.0": "+Turbo1, 2, 3"
        },
        {
          "The comparison results of our method with some previous": "out any complex fusion methods (we only use the method of",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "0.8"
        },
        {
          "The comparison results of our method with some previous": "concatenating features directly) and achieve state-of-the-art per-",
          "2.0": "3.0\n2.5\n2.0\n1.5\n1.0\n0.5"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "In-modal uniform"
        },
        {
          "The comparison results of our method with some previous": "formance.",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "Figure 2: The align-unif orm plot of models"
        },
        {
          "The comparison results of our method with some previous": "3.4. Analysis",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "To\nbetter\nunderstand\nthe\neffectiveness\nof\nTurbo,\nwe\nuse",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "Vanilla\n+Trubo\n+CLcross"
        },
        {
          "The comparison results of our method with some previous": "alignment\nand unif ormity [8]\nto measure\nthe quality of",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "1.0\n1.0\n1.0"
        },
        {
          "The comparison results of our method with some previous": "learned representations.\nThe pseudocode for\nthese two met-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "0.5\n0.5\n0.5"
        },
        {
          "The comparison results of our method with some previous": "ric calculations is shown in listing 2, which is slightly differ-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "0.0\n0.0\n0.0"
        },
        {
          "The comparison results of our method with some previous": "ent from the uni-modal computing method in [8]. Alignment",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "means the alignment (closeness) between corresponding cross-",
          "2.0": "-0.5\n-0.5\n-0.5"
        },
        {
          "The comparison results of our method with some previous": "modal audio-text pairs, and unif ormity means the uniformity",
          "2.0": "-1.0\n-1.0\n-1.0"
        },
        {
          "The comparison results of our method with some previous": "(coverage) of the uni-modal representation space on the unit hy-",
          "2.0": "-1.0-0.5 0.0 0.5 1.0\n-1.0-0.5 0.0 0.5 1.0\n-1.0-0.5 0.0 0.5 1.0"
        },
        {
          "The comparison results of our method with some previous": "persphere. In general, models with better alignment and unifor-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "mity can achieve better performance.",
          "2.0": "Figure 3: Feature distributions with Gaussian kernel density"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "estimation (KDE) in R2."
        },
        {
          "The comparison results of our method with some previous": "We show the alignment and uniformity plot of three mod-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "els evaluated on the IEMOCAP in Fig.2.\nLower numbers of",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "alignment and uniformity are better. For each method, we train",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "three times and visualize the embeddings in the figure. There",
          "2.0": "4. Conclusions"
        },
        {
          "The comparison results of our method with some previous": "are several observations. Compared with the vanilla model, the",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "In this work, we propose Turbo, a contrastive learning strategy"
        },
        {
          "The comparison results of our method with some previous": "the alignment be-\n”+CLcross” model dramatically improves",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "for multi-modal classification tasks. Unlike previous works that"
        },
        {
          "The comparison results of our method with some previous": "cause of\nthe cross-modal contrastive learning. Turbo can fur-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "only focus on cross-modal contrastive learning, Turbo performs"
        },
        {
          "The comparison results of our method with some previous": "ther\nimprove the alignment since it has multiple cross-modal",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "in-modal contrastive learning simultaneously.\nBy forwarding"
        },
        {
          "The comparison results of our method with some previous": "contrastive objectives\nfor\nthe same audio-text pair.\nIn terms",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "twice, Turbo obtains multiple in-modal and cross-modal con-"
        },
        {
          "The comparison results of our method with some previous": "of uniformity,\nthe Turbo method is\nsubstantially superior\nto",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "trastive objectives\nfor\nthe\nsame\ninput pair, which enhances"
        },
        {
          "The comparison results of our method with some previous": "That\nis because Turbo\nthe ”+CLcross” and vanilla models.",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "the generalization of representation learning. We demonstrate"
        },
        {
          "The comparison results of our method with some previous": "performs additional\nin-modal contrastive learning.\nThis phe-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "the effectiveness of our method on two audio-text classifica-"
        },
        {
          "The comparison results of our method with some previous": "nomenon is also reflected in Fig.3,\nin which we visualize the",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "tion tasks. Compared with the vanilla method, we achieved a"
        },
        {
          "The comparison results of our method with some previous": "distributions of audio representation space for three models. Re-",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "5.59% improvement in weighted accuracy on the SER task and"
        },
        {
          "The comparison results of our method with some previous": "sults indicate that the Turbo method achieves the most uniform",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "a 3.83% improvement\nin weighted accuracy on the DSD task."
        },
        {
          "The comparison results of our method with some previous": "distribution, which again proves the superiority of our method.",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "Further analysis indicates that Turbo can effectively enhance the"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "uniformity and alignment of multi-modal representations."
        },
        {
          "The comparison results of our method with some previous": "# A:\nthe\ndistribution\nof\naudio\nrepresentation",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "In the future, we will explore extending our proposed Turbo"
        },
        {
          "The comparison results of our method with some previous": "# T:\nthe\ndistribution\nof\ntext\nrepresentation",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "to audio-text pre-trained learning and improve performance for"
        },
        {
          "The comparison results of our method with some previous": "# the\ncalculation\nof\nalignment",
          "2.0": "the downstream tasks. We also intent\nto expand our proposed"
        },
        {
          "The comparison results of our method with some previous": "def\nlalign(A,\nT,\nalpha=2):",
          "2.0": "multi-modal\nlearning method to include the visual domain and"
        },
        {
          "The comparison results of our method with some previous": "return\n(A-T).norm(dim=1).pow(alpha).mean()",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "integrate three modalities for classification."
        },
        {
          "The comparison results of our method with some previous": "#\nthe\ncalculation\nof\nuniformity",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "def\nlunif(A,\nt=2):",
          "2.0": "5. Acknowledgements"
        },
        {
          "The comparison results of our method with some previous": "sq_pdist\n=\ntorch.pdist(A,\np=2).pow(2)",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "The INTERSPEECH 2023 organisers would like to thank ISCA"
        },
        {
          "The comparison results of our method with some previous": "return\nsq_pdist.mul(-t).exp().mean().log()",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "and the organising committees of past INTERSPEECH confer-"
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "ences for their help and for kindly providing the previous ver-"
        },
        {
          "The comparison results of our method with some previous": "Listing 2: Pytorch-like pseudocode for calculating alignment",
          "2.0": ""
        },
        {
          "The comparison results of our method with some previous": "",
          "2.0": "sion of this template."
        },
        {
          "The comparison results of our method with some previous": "and uniformity.",
          "2.0": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "2.0: A framework for self-supervised learning of speech repre-"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "sentations,” Advances in Neural Information Processing Systems,"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "vol. 33, pp. 12 449–12 460, 2020."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[10] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "I. Sutskever,\n“Robust\nspeech recognition via\nlarge-scale weak"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "supervision,” Technical\nreport, OpenAI, 2022. URL https://cdn."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "openai. com/papers/whisper. pdf, Tech. Rep., 2022."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[11]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "training of deep bidirectional\ntransformers\nfor\nlanguage under-"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "standing,” arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[12] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nI. Sutskever"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "et al., “Language models are unsupervised multitask learners,”"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "OpenAI blog, vol. 1, no. 8, p. 9, 2019."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[13] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "contrastive predictive coding,” arXiv preprint arXiv:1807.03748,"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "2018."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[14] D. Krishna and A. Patil, “Multimodal emotion recognition using"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "cross-modal attention and 1d convolutional neural networks,” in"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Interspeech, 2020, pp. 4243–4247."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[15]\nP. Liu, K. Li, and H. Meng, “Group gated fusion on attention-"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "based bidirectional alignment\nfor multimodal emotion recogni-"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "tion,” Proc. Interspeech 2020, pp. 379–383, 2020."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[16]\nS. H. Mallidi, R. Maas, K. Goehner, A. Rastrow, S. Matsoukas,"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "and B. Hoffmeister, “Device-directed utterance detection,” Proc."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "Interspeech 2018, pp. 1225–1228, 2018."
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "[17] A. Norouzian, B. Mazoure, D. Connolly,\nand D. Willett,\n“Ex-"
        },
        {
          "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec": "ploring attention mechanism for acoustic-based classification of"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "ing alignment for multimodal emotion recognition from speech,”"
        },
        {
          "6. References": "[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "Proc. Interspeech 2019, pp. 3569–3573, 2019."
        },
        {
          "6. References": "wal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "transferable visual models from natural language supervision,” in",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "[19] H. Li, W. Ding, Z. Wu,\nand Z. Liu,\n“Learning fine-grained"
        },
        {
          "6. References": "International Conference on Machine Learning.\nPMLR, 2021,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "cross modality excitement for speech emotion recognition,” arXiv"
        },
        {
          "6. References": "pp. 8748–8763.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "preprint arXiv:2010.12733, 2020."
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "[20] M. R. Makiuchi, K. Uto, and K. Shinoda, “Multimodal emotion"
        },
        {
          "6. References": "[2] B. Elizalde, S. Deshmukh, M. A.\nIsmail, and H. Wang, “Clap:",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "recognition with high-level\nspeech and text\nfeatures,”\nin 2021"
        },
        {
          "6. References": "Learning\naudio\nconcepts\nfrom natural\nlanguage\nsupervision,”",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "IEEE Automatic Speech Recognition and Understanding Work-"
        },
        {
          "6. References": "arXiv preprint arXiv:2206.04769, 2022.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "shop (ASRU).\nIEEE, 2021, pp. 350–357."
        },
        {
          "6. References": "[3]\nT. Chen, S. Kornblith, M. Norouzi,\nand G. Hinton,\n“A simple",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "[21] W. Chen, X. Xing, X. Xu, J. Yang, and J. Pang, “Key-sparse trans-"
        },
        {
          "6. References": "framework for contrastive learning of visual representations,” in",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "former\nfor multimodal speech emotion recognition,” in ICASSP"
        },
        {
          "6. References": "International conference on machine learning.\nPMLR, 2020,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "2022-2022 IEEE International Conference on Acoustics, Speech"
        },
        {
          "6. References": "pp. 1597–1607.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": "and Signal Processing (ICASSP).\nIEEE, 2022, pp. 6897–6901."
        },
        {
          "6. References": "[4]\nT. Gao, X. Yao, and D. Chen, “Simcse: Simple contrastive learn-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "the 2021 Con-\ning of sentence embeddings,” in Proceedings of",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "ference on Empirical Methods in Natural Language Processing,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "2021, pp. 6894–6910.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[5]\nL. Wu, J. Li, Y. Wang, Q. Meng, T. Qin, W. Chen, M. Zhang, T.-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "Y\n. Liu et al., “R-drop: Regularized dropout for neural networks,”",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "Advances in Neural Information Processing Systems, vol. 34, pp.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "10 890–10 905, 2021.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[6] N.\nSrivastava, G. Hinton, A. Krizhevsky,\nI.\nSutskever,\nand",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "R. Salakhutdinov,\n“Dropout:\na\nsimple way to prevent neural",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "networks from overfitting,” The journal of machine learning re-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "search, vol. 15, no. 1, pp. 1929–1958, 2014.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[7] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "Interactive emotional dyadic motion capture database,” Language",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "resources and evaluation, vol. 42, no. 4, pp. 335–359, 2008.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[8]\nT. Wang and P.\nIsola, “Understanding contrastive representation",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "learning through alignment and uniformity on the hypersphere,”",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "in International Conference on Machine Learning.\nPMLR, 2020,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "pp. 9929–9939.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[9] A. Baevski, Y. Zhou, A. Mohamed,\nand M. Auli,\n“wav2vec",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "2.0: A framework for self-supervised learning of speech repre-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "sentations,” Advances in Neural Information Processing Systems,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "vol. 33, pp. 12 449–12 460, 2020.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[10] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "I. Sutskever,\n“Robust\nspeech recognition via\nlarge-scale weak",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "supervision,” Technical\nreport, OpenAI, 2022. URL https://cdn.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "openai. com/papers/whisper. pdf, Tech. Rep., 2022.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[11]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "training of deep bidirectional\ntransformers\nfor\nlanguage under-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "standing,” arXiv preprint arXiv:1810.04805, 2018.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[12] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nI. Sutskever",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "et al., “Language models are unsupervised multitask learners,”",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "OpenAI blog, vol. 1, no. 8, p. 9, 2019.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[13] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "contrastive predictive coding,” arXiv preprint arXiv:1807.03748,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "2018.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[14] D. Krishna and A. Patil, “Multimodal emotion recognition using",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "cross-modal attention and 1d convolutional neural networks,” in",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "Interspeech, 2020, pp. 4243–4247.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[15]\nP. Liu, K. Li, and H. Meng, “Group gated fusion on attention-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "based bidirectional alignment\nfor multimodal emotion recogni-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "tion,” Proc. Interspeech 2020, pp. 379–383, 2020.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[16]\nS. H. Mallidi, R. Maas, K. Goehner, A. Rastrow, S. Matsoukas,",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "and B. Hoffmeister, “Device-directed utterance detection,” Proc.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "Interspeech 2018, pp. 1225–1228, 2018.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "[17] A. Norouzian, B. Mazoure, D. Connolly,\nand D. Willett,\n“Ex-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "ploring attention mechanism for acoustic-based classification of",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "speech utterances into system-directed and non-system-directed,”",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "in ICASSP 2019-2019 IEEE International Conference on Acous-",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        },
        {
          "6. References": "7310–7314.",
          "[18] H. Xu, H. Zhang, K. Han, Y. Wang, Y. Peng, and X. Li, “Learn-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Clap: Learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2022",
      "venue": "Clap: Learning audio concepts from natural language supervision",
      "arxiv": "arXiv:2206.04769"
    },
    {
      "citation_id": "4",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "5",
      "title": "Simcse: Simple contrastive learning of sentence embeddings",
      "authors": [
        "T Gao",
        "X Yao",
        "D Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "R-drop: Regularized dropout for neural networks",
      "authors": [
        "L Wu",
        "J Li",
        "Y Wang",
        "Q Meng",
        "T Qin",
        "W Chen",
        "M Zhang",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "9",
      "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
      "authors": [
        "T Wang",
        "P Isola"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "10",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Tech. Rep"
    },
    {
      "citation_id": "12",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "13",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "14",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "15",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "D Krishna",
        "A Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks"
    },
    {
      "citation_id": "16",
      "title": "Group gated fusion on attentionbased bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "17",
      "title": "Device-directed utterance detection",
      "authors": [
        "S Mallidi",
        "R Maas",
        "K Goehner",
        "A Rastrow",
        "S Matsoukas",
        "B Hoffmeister"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Exploring attention mechanism for acoustic-based classification of speech utterances into system-directed and non-system-directed",
      "authors": [
        "A Norouzian",
        "B Mazoure",
        "D Connolly",
        "D Willett"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "authors": [
        "H Li",
        "W Ding",
        "Z Wu",
        "Z Liu"
      ],
      "year": "2020",
      "venue": "Learning fine-grained cross modality excitement for speech emotion recognition",
      "arxiv": "arXiv:2010.12733"
    },
    {
      "citation_id": "21",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "22",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}