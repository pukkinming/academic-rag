{
  "paper_id": "2408.03650v2",
  "title": "Towards Multimodal Emotional Support Conversation Systems",
  "published": "2024-08-07T09:25:17Z",
  "authors": [
    "Yuqi Chu",
    "Lizi Liao",
    "Zhiyuan Zhou",
    "Chong-Wah Ngo",
    "Richang Hong"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The integration of conversational artificial intelligence (AI) into mental health care promises a new horizon for therapist-client interactions, aiming to closely emulate the depth and nuance of human conversations. Despite the potential, the current landscape of conversational AI is markedly limited by its reliance on single-modal data, constraining the systems' ability to empathize and provide effective emotional support. This limitation stems from a paucity of resources that encapsulate the multimodal nature of human communication essential for therapeutic counseling. To address this gap, we introduce the Multimodal Emotional Support Conversation (MESC) dataset, a first-of-its-kind resource enriched with comprehensive annotations across text, audio, and video modalities. This dataset captures the intricate interplay of user emotions, system strategies, system emotion, and system responses, setting a new precedent in the field. Leveraging the MESC dataset, we propose a general Sequential Multimodal Emotional Support framework (SMES) grounded in Therapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES framework incorporates an LLM-based reasoning model that sequentially generates user emotion recognition, system strategy prediction, system emotion prediction, and response generation. Our rigorous evaluations demonstrate that this framework significantly enhances the capability of AI systems to mimic therapist behaviors with heightened empathy and strategic responsiveness. By integrating multimodal data in this innovative manner, we bridge the critical gap between emotion recognition and emotional support, marking a significant advancement in conversational AI for mental health support. This work not only pushes the boundaries of AI's role in mental health care but also establishes a foundation for developing conversational agents that can provide more empathetic and effective emotional support.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "The integration of conversational artificial intelligence (AI) into mental health care introduces a promising frontier for enhancing therapist-client interactions  [1] ,  [2] . Aiming at replicating the rich nuances of human dialogue, conversational AI seeks to broaden the accessibility and depth of mental health support  [3] ,  [4] . This innovation stands to revolutionize the therapeutic landscape, offering the potential for more nuanced, empathetic interactions that bridge the gap between technology and the essential human elements of therapy, making effective mental health care more accessible to a wider audience.\n\nExisting works has primarily focused on Emotion Recognition, using key multimodal benchmarks like the IEMOCAP  [11]  and MELD  [10]  datasets to recognize emotions in conversations. This area of study primarily identifies and tracks speakers' emotional states throughout a dialogue. For example, Li et al.  [12]  designed a Graph-based Cross-modal Feature Complementation (G-CFC) module to enhance modeling of contextual and interactive information across different modalities. Nie et al.  [13]  developed an incremental graph convolution network (I-GCN) to capture both semantic correlations and temporal changes in utterances. Ma et al.  [14]  proposed a transformer-based model equipped with self-distillation (SDT) that effectively captures intra-and inter-modal interactions. However, these efforts are limited to identifying emotional states and do not generate dialogues that respond to these emotions, thus failing to provide mental health support. Furthermore, research on Emotional Support Conversation (ESC) has been solely based on text. Liu et al.  [5]  introduced the ESC task along with the ESConv dataset to alleviate emotional distress through conversation. Tu et al.  [15]  and Peng et al.  [16]  advocated for the integration of commonsense knowledge into dialogue models to enhance their effectiveness. Cheng et al.  [17]  developed the PAL method, which employs persona information and dynamically models conversation history to generate responses. Nonetheless, these approaches rely solely on text, overlooking other modalities and recognizing users' emotional states.\n\nDespite these developments, to mimic the interactions between a client and therapist, particularly for addressing emotional distress, two critical challenges persist:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Text Video Audio Emotion Recognition Strategy Prediction Response Generation",
      "text": "Esconv  [5]  ✓\n\n• Challenge 1: The absence of a comprehensive multimodal dataset tailored for emotional support conversations. This gap significantly hinders the development of AI systems that can understand and respond to the complex emotional states of users  [3] ,  [4] . As shown in Table  I , existing datasets for emotional support conversations mainly concentrate on a single modality. For example, Esconv includes only text-based data, whereas multimodal datasets like MELD focus on identifying emotions in daily life dialogues. The datasets lack strategy and are more suited for emotion recognition than for generating therapeutic responses.\n\n• Challenge 2: The lack of a streamlined methodological framework that integrates multimodal data for emotion recognition and generates empathetic and strategic responses in AI-driven therapy sessions  [18] . Existing methods typically treat emotion recognition, strategy formulation, and response generation as distinct, disjointed tasks. This approach fails to capture the interconnected nature of these components, which are considered in actual counseling conversations. Therapists need to consider the client's emotions when generating responses and formulating treatment plans, which is crucial for empathy response and helpful for addressing emotion block.\n\nAddressing these challenges requires a comprehensive multimodal dataset and a general framework. (1) For Challenge 1, we have constructed the MESC dataset 1 . This first-ofits-kind dataset is enriched with comprehensive annotations, including emotions and strategies, across text, audio, and video modalities. As shown in Table  I , the MESC dataset is versatile, supporting not only emotion recognition but also emotional support. These capabilities are essential for integrating conversational artificial intelligence (AI) into mental health applications. (2) For Challenge 2, We propose a general Sequential Multimodal Emotional Support Framework (SMES), a multitask method grounded in Therapeutic Skills Theory  [19] -  [22] . The SMES framework leverages the strengths of multimodal foundation models to extract emotional cues from video and audio. It employs an LLM-based Reasoning model to sequentially generate multi-task results, encompassing user emotion recognition, strategy prediction, system emotion prediction, 1 https://github.com/chuyq/MESC and response generation. By utilizing multi-task maximum likelihood training, the SMES framework adeptly models the interdependencies among these tasks, optimizing the therapeutic dialogue process in a comprehensive end-to-end manner.\n\nTo sum up, our main contributions are threefold:\n\n• We introduce the first comprehensive multimodal conversation dataset for mental health care, combining text, audio, and video to capture the complex interplay of user emotions, agent strategies, and responses. • We develop a general Sequential Multimodal Emotional Support Framework (SMES) based on Therapeutic Skills Theory, enabling AI to mimic therapist behavior more accurately through a sequential multi-task approach for emotion recognition and response generation. • We demonstrate significant improvements in AI's empathy and strategic responsiveness for mental health support, establishing a new benchmark that bridges the gap between emotion recognition and emotional support.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ii. Related Work A. Related Datasets For Emotional Support",
      "text": "Emotional support has garnered attention due to its potential applications in psychology and emotional artificial intelligence systems. There are two mainly data-driven tasks: emotion recognition and emotional support conversations. For emotion recognition, Li et al.  [7]  developed the DailyDialog dataset, a text-based collection designed to mirror everyday communication styles and encompass various topics about daily life. Chen et al.  [8]  constructed the EmotionLines dataset, while Zahiri et al.  [9]  annotated the EmoryNLP dataset. Both datasets are text-based and derived from the TV show Friends with each utterance in these datasets annotated with one of seven emotion-categorical labels. The main distinction lies in the emotional classes assigned, and additionally, the EmotionLines dataset contains a larger number of utterances and dialogues compared to the EmoryNLP dataset. Besides, Busso et al.  [11]  annotated the IEMOCAP database, a multimodal dataset comprising dyadic sessions in which actors engage in improvisations or scripted scenarios. This dataset features six emotion labels and encompasses a total of only 151 dialogues. Poria et al.  [10]  introduced the MELD dataset as an extension and enhancement of the EmotionLines dataset, which contains audio, visual, and textual modalities. MELD revisited the emotion labeling of the EmotionLines dataset, considering dynamic changes in emotional states observed in video data. While the EmotionLines dataset comprises 2000 dialogues larger than MELD contains 1400 dialogues. The inclusion of multiple modalities in MELD poses additional challenges for labeling, rendering the task more complex than textonly datasets. While the existing datasets offer rich emotional labels, they lack emotional strategies. It only has the capable of analyzing the speaker's emotional state but can not provide corresponding emotional support tailored to that state.\n\nIn the realm of emotional support conversations, Sharma et al  [23]  annotated post-response pairs from TalkLife and mental health subreddits, with only the data from Reddit being publicly available. Hosseini et al.  [24]  collected similar pairs from online support groups, although these dialogues are restricted to single-turn or brief interactions. Liu et al.  [5]  developed the ESConv dataset, comprising 1,053 dialogues from daily interactions and featuring eight types of support strategies. However, these datasets are limited as they are exclusively text-based, which inadequately captures the interaction in human counseling and diminishes the potential effectiveness of emotional support. Traditional therapists often utilize multimodal cues, such as changes in facial expressions and voice tone, which are absent in these datasets.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Related Tasks For Emotional Support",
      "text": "Emotional conversation systems are comprised of key tasks such as emotion recognition in conversation (ERC), emotional conversation, and empathetic conversation. Emotion Recognition in Conversation (ERC) aims to automatically identify and track the emotional states of speakers in dialogues by leveraging multimodal cues such as facial expressions, vocal tonality, and gestures. Research in this area has primarily focused on three methodologies: commonsense reasoning  [25] ,  [26] , attention-recurrent networks  [27] ,  [28] , and Graph Neural Network approaches  [29] ,  [30] . A notable work is by Nie et al.  [13] , which used a dynamic graph structure to capture semantic correlations and temporal changes in utterances. These methods aim to enhance the accuracy of emotion recognition in conversations. However, they stop short of generating responses based on identified emotions. Emotional and empathetic conversation tasks are designed to produce responses aligned with pre-specified emotional cues  [31] -  [33] . Gao et al.  [34]  concentrated on detecting these cues within conversations to generate contextually and emotionally coherent responses. Furthering this approach, Sabour et al.  [35]  have incorporated external commonsense knowledge to enhance the system's understanding of users' emotions. These methods collectively aim to comprehend and appropriately respond to users' emotional states. Besides, another key area is emotional support conversation (ESC), which seeks to offer emotional support through social interaction, not professional counseling. A significant contribution by Liu et al.  [5]  introduced the multi-turn ESC dataset. Building on this work, Peng et al.  [16]  implemented a graph-based method, while Deng et al.  [36]  further enhanced the approach by integrating knowledge for improved context comprehension",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Mesc Dataset",
      "text": "Facial expressions, vocal tonality, and body language are crucial for analyzing a user's psychological state, enabling conversational systems to enhance their capacity to mimic human mental health support. Current datasets for emotional support, however, are predominantly text-based and fail to capture these crucial cues. Furthermore, existing multimodal emotion datasets are generally focused on daily scenarios and do not incorporate therapeutic strategies. To fill this gap, we construct a multimodal emotional support dataset (MESC).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Data Construction",
      "text": "The MESC dataset is derived from the TV show In Treatment, specifically covering seasons 1 to 3. This series chronicles the weekly sessions of psychotherapist Paul Weston with his patients, as well as his own counseling with a therapist. The data source is highly professional and the case within has been analyzed by the Director of Clinical Psychology at the Shanghai Mental Health Center. Additionally, The New York Times has praised the series for providing a compelling insight into the psychopathology of everyday life. To ensure the dataset is adaptable for multiple emotional tasks, we have annotated each utterance with its corresponding emotion. Moreover, we have annotated utterances spoken by the therapist with the strategy employed. Each dialogue is segmented into counseling scenarios, complete with detailed descriptions of each scenario, as shown in Fig.  2 . More annotation details will be provided in the later subsection.\n\nGiven the complexity of this multimodal emotional support task, we have invested considerable effort to ensure the quality, effectiveness, and comprehensiveness of the dataset. Our efforts are concentrated on the following aspects:\n\n(1) Multimodal Unified Timestamps: We synchronize the timestamps across all modalities to solve the discrepancies in timestamps between text and video modalities. After manual filtering of chatting segments that are irrelevant to emotion support counseling in the TV series, we extract the starting and ending points from videos and accurately correlate them with dialogue utterances and vocal tonality to ensure seamless alignment across different forms of data.\n\n(2) Annotation Quality Control: To ensure the quality of the annotations, we have written a tutorial that includes definitions of the strategies, examples of emotion classification, and a three-hour training session for annotators. Additionally, each annotator must pass a preliminary test before beginning official annotations, and only those who pass are permitted to proceed with the annotating process.\n\n(3) Comprehensive Coverage of Emotional Support: We have segmented the videos based on the client's experiences and scenarios. The dataset features a diverse range of 15 scenarios, 10 therapeutic strategies, and 7 emotion categories, providing a thorough overview of the emotional support process. This dataset can be applied to a range of tasks, including emotion recognition, strategy prediction, and response generation, offering comprehensive data for emotional support in mental health care.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Dataset Annotation",
      "text": "Our dataset focuses on multimodal emotional support. We annotate emotional states and emotional support strategies by taking into account the interplay among three modalities, considering facial expressions, vocal tonality, language, and gestures. This complex task requires a significant investment of time and labor.\n\nTo enhance labor efficiency and reduce costs, we employ a large model like GPT-3.5 for coarse-grained annotation, followed by manual fine-grained calibration. The overall annotation accuracy of GPT-3.5 is about 25%, which is low. Therefore, we employ three graduate students specializing in emotional support research as annotators for fine-grained calibration. They undergo training in our labeling methodologies and must pass a rigorous annotation test. The annotators are required to watch video clips to identify and calibrate three key elements: the client's emotional state, the therapist's emotional state, and the therapeutic strategy employed. Throughout this process, annotators consider not just the textual content but also the facial expressions, gestures, and vocal nuances presented by both clients and therapists. Each piece of data is calibrated by two annotators to ensure consistency. In cases of discrepancies between the annotators' assessments, a third annotator would review the video clip and decide on the most accurate interpretation.\n\nEmotion Annotation: we require the annotators to identify and calibrate the emotional states of both clients and therapists through detailed observation of video clips. During this process, annotators are instructed to consider not only the text content but also facial expressions, gestures, and vocal nuance cues presented by both clients and therapists. They need to annotate each utterance with emotional labels chosen from a predefined set of classes, encompassing seven emotions: anger, sadness, disgust, depression, fear, neutral, and joy, as detailed in Table  III .\n\nStrategy Annotation: To teach the annotators to label emotional support strategies, we have written a tutorial that includes definitions of the strategies and a three-hour training session for annotators. Drawing inspiration from the online emotional support platform  [37] , we develop ten sub-tasks. These sub-tasks are designed to help annotators learn the definitions of the ten professional therapeutic support strategies. Each sub-task is structured around an example conversation excerpt, accompanied by a quiz question tailored to cement the annotator's understanding of each therapeutic strategy. This educational approach ensures that annotators are acquainted with theoretical concepts while watching the video.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Quality Control",
      "text": "We employ a variety of methods to ensure that the videos and dialogues selected for our multimodal dataset are of high quality and tailored for emotional support conversations.\n\nTimestamp Alignment and Content Validity: To ensure the alignment of timestamps across the three modalities and maintain the validity of the dialogue content, we first write a script to manually calibrate each episode, aligning the subtitles closely with the videos. We then utilize the transcription alignment tool Gentle to achieve precise timestamp alignment for each sentence. This tool automatically aligns the transcript text with the audio and extracts word-level timestamps for accuracy. Furthermore, to maintain content relevance, we remove segments unrelated to emotional counseling, such as interactions between the therapist and his family members, thus focusing the content solely on patient interactions.\n\nAnnotation Correction: To ensure data quality, we implement a two-tier annotation strategy. Initially, GPT-3.5 is employed for coarse-grained labeling of emotional states and therapeutic strategies. This is followed by meticulous checks and calibrations by our annotators. This approach not only reduces costs and labor but also aims to minimize labeling bias, providing a more balanced and nuanced understanding of the data. For emotion annotation, we require concordance between the labels from two annotators and manually calibrate 20,133 utterances-over 70% of the data initially annotated by GPT. After these manual calibrations, we achieve an emotion Fleiss kappa score of 0.57, compared to a score of 0.43 for the MELD dataset. For strategy annotation, considering the complexity of the task and the requirement for at least two annotators to agree, over 83% of the labels undergo manual calibration. After these calibrations, we achieve a strategy Fleiss kappa score of 0.69.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Dataset Characteristics A. Statistics",
      "text": "Our multimodal dataset MESC comprises 28,762 utterances, 1,019 dialogues, and each utterance is annotated with emotion labels from seven categories and ten therapeutic strategies. Additionally, the dataset includes a corresponding set of video and audio clips, matched in quantity to the utterances, to provide a comprehensive multimodal resource, as detailed in Table  II . The dataset reveals an average dialogue length of 28.2 utterances, pointing out that effective Emotional Support (ES) needs a relatively long and multi-turn conversation. It highlights that clients need to share personal experiences fully. Therapists, in turn, need the information to explore where the emotional wounds originate from, thereby enabling them to formulate and apply targeted therapeutic strategies to distress the clients' stress.\n\nIn our study, emotions are classified into seven distinct categories for annotation: anger, sadness, disgust, depression, neutral, joy, and fear. Different from the prior dataset MELD  [10]  and IEMOCAP  [11] , our dataset concentrates on the emotional states encountered in therapeutic counseling. The focus on dynamic emotion recognition is maintained throughout all stages of our research, including the training, validation, and testing phases. Our analysis of the dataset's emotional distribution uncover a non-uniform pattern, with neutral emotions emerging as the predominant category. This is attributed to therapists often keeping a neutral emotional state to create a trustworthy and communicative environment, encouraging clients to open up more freely. The second emotional state is depression, indicating that most of clients are experiencing emotional blocks, they are in a low mood coming from the relationship with friends and family, or a life-changing event, the distribution is shown in Fig.  2 . It comprises 15 scenarios from clients face in life, including PTSD, dream analysis, childhood shadow, and other issues typically addressed in professional therapeutic settings. Among these, clients most frequently express concerns related to their familial relationships and their therapeutic interactions. This suggests that emotional blocks often originate from the following areas: clients may fear rejection or endure negative social interactions, prompting them to hide and suppress their pain. Therefore, providing effective emotional support requires creating a trusting and empathetic environment that helps clients express their feelings and thoughts, and understand their true emotions.\n\nBeyond emotion annotation, we detail the statistics of strategy annotations in Table  III . The MESC dataset sets itself apart from existing datasets by not only focusing on emotion recognition but also on emotional support strategies, and emotional response generation. The MESC dataset is endowed with the most comprehensive annotations for emotional  support tasks currently available. Consequently, it acts as a valuable resource for bolstering emotional support within AI conversational systems and can be applied to a wide array of emotional tasks.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Strategy Analysis",
      "text": "In our study, we aim to analyze the strategy employed by the therapist at different phases of emotional therapeutic counseling. To achieve this, we consider a conversation with N utterances in total, where the k-th utterance from the therapist employs strategy S. The position of this utterance within the conversation is defined as the conversation phase and represented as k/N . To visually display the changes in strategy employed during the dialogue process, we divide the progression of the conversation into four phases for analysis. Fig.  3  shows the distribution of ten strategies across the conversation progress, derived from professional therapeutic theories  [19] -  [22] .\n\nIt is noteworthy that the choice of strategy is influenced by changes in the client's emotional state. For instance, therapists might use \"open questions\" to explore underlying issues if the client displays a low mood, or \"approval\" to provide positive reinforcement, thus fostering more open communication. Additionally, significant shifts in the client's mood LLM-based Reasoning during a session may prompt therapists to engage in \"selfdisclosure\" or \"guide the pace and depth of the conversation,\" aimed at managing the client's emotional state and enhancing the therapeutic relationship. Importantly, a therapist can employ multiple strategies within a single phase of progress.\n\nAs depicted in Fig.  3 , the strategy of \"Open Questions\" is consistently observed across all four stages, with a relatively high frequency and in combination with other strategies. In the initial stage, \"Open Questions\" are paired with \"Restatement,\" enabling therapists to probe into the origins of the clients' emotional distress. As the therapy progresses to the middle and later stages, this strategy is complemented by \"Interpretation\", designed to help clients process their emotions, uncover the root causes of their issues, and reduce stress.\n\nV. METHODOLOGY",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Task Definition",
      "text": "To replicate the functions of a human therapist in providing emotional support, there are four critical tasks: User Emotion Recognition: This task involves identifying the emotional state of the client using multimodal inputs such as video, audio, and text. By analyzing facial expressions, body language, and voice intonations, alongside textual analysis of dialogue, the system dynamically models the client's psychological condition. This comprehensive understanding is crucial for tailoring the conversation to the client's emotional needs.\n\nSystem Strategy Prediction: Based on the recognized emotions and the context of the conversation, the system predicts a therapeutic strategy. This involves choosing the most appropriate conversational approach, such as asking open questions, engaging in self-disclosure, or employing specific communication skills to address the client's underlying issues and alleviate stress. The strategy adapts to changes in the client's mood and emotional state throughout the session.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "System Emotion Prediction:",
      "text": "This task requires the system to predict its own emotional tone in responses, to align with the therapeutic strategy. By generating empathetic responses that reflect understanding and concern, the system fosters a supportive environment conducive to emotional healing.  System Response Generation: The final task is generating responses that are not only contextually appropriate but also therapeutically effective. These responses are designed to resonate with the client's emotional state and therapeutic needs, helping to explore emotional wounds and promote psychological recovery. The dialogue generated by the system aims to support the client's process of identifying and addressing emotional issues, contributing actively to their path toward emotional well-being.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Smes Framework",
      "text": "In this section, we propose a general sequential Multimodal Emotional Support framework (SMES) designed specifically for emotional support. The framework is designed to leverage multimodal data to deliver rich emotional insights, supporting a range of tasks related to emotional support. It consists of four primary tasks: user emotion recognition, strategy prediction, system emotion prediction, and response generation. Illustrated in Fig.  4 , during each turn t of the dialogue process, the user provides inputs including an utterance U t , video V t , and audio A t . After processing these multimodal inputs, they are transformed into a textual sequence M t , which captures the emotional state descriptions from this turn. The multimodal dialogue history can be represented as\n\nOur goal is to sequentially generate results for the four emotion-related tasks to provide better emotional support.\n\nTo achieve this, In a turn t, we first employ an Audio-Visual Large Language Model (Video-LLaMA)  [38]  to extract emotion-related cues from video V t and audio A t . we construct the prompt to query LLM as: Video [Vt]; Audio [At]: Question 1: \"What is the emotional state of the speaker?\" Question 2: \"What life distress might explain the speaker's emotional expression and posture in this video?\" These questions enable the Video-LLaMA to detect emotional changes in visual scenes and audio signals, the clues are like \"The speaker seems to be in a state of contemplation or thoughtfulness, as she is looking directly into the camera with a serious expression on her face.\" The emotion clue can be denoted as C t , These are then concatenated with the user's utterance U t to form M t .\n\nTo sequentially generate the four task results, the LLMbased Reasoning first reads all previous turns history H t . It then generates user emotion recognition result E t ,\n\nSubsequently, the LLM-based Reasoning takes it as input to generate strategy prediction S t . S t represents the strategy predicted by the system, which determines the therapeutic approach for generating responses-whether to inquire further about the situation or to offer sympathy and comfort. the LLM-based Reasoning then takes the concatenated sequence of H t , E t , and S t to decide the system emotion, SE t which influences the style of the responses generated by the system. The response R t is generated based on all prior information concatenated into a single sequence:\n\nFig.  5  shows the training of the LLM-based Reasoning model. To capitalize on the strengths of pre-trained language models (PLMs) like BlenderBot, which has demonstrated a superior ability to generate high-quality responses in dialogue systems  [39] , We integrate the generative PLMs into our framework and we reformulate a single training sequence as Y = [H t , E t , S t , SE t , R t ], the model is trained to minimize the loss function L over the dataset D, where I is the sequence length:\n\nVI. EXPERIMENTS SMES serves as a versatile framework for a variety of emotional conversational tasks, including emotion recognition, strategy prediction, and response generation. We have evaluated several methods related to these emotional tasks on the MESC dataset.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Experimental Setups",
      "text": "Evaluation Metrics: As for automatic evaluation, we utilize Accuracy and Weighted F1 as metrics for emotion recognition, strategy prediction, and system emotion prediction. In line with prior studies on response generation, our evaluation includes BLEU-n (B-2), ROUGE-L (R-L), and BERTScore to assess the quality of generated responses. These metrics collectively provide a comprehensive overview of model performance across different tasks. Baselines: We evaluate methods across four tasks: emotion recognition, strategy prediction, system emotion prediction, and response generation. For emotion recognition, we assess DialogueGCN  [40] , which utilizes Graph Convolutional Networks to enhance Emotion Recognition in Conversation; MMGCN  [41] , employing a graph structure to capture both intra-and inter-modality features; and MMDFN  [42] , which leverages speaker features and integrates multimodal contexts while minimizing redundancy. For strategy prediction and response generation, we compare two methods: Blenderbot-Joint  [5] , an open-domain agent with developed communication skills, and BBMHR (BlenderBot for Mental Health with Reasoning)  [43] , which uses GPT-3 as an expert tailored for mental health and reasoning enhancement. As a baseline for these four tasks, we utilize two method: GPT-3.5, a large language model known for its strong communication abilities, and GPT-4.0, the latest and advanced large language model.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Overall Performance",
      "text": "Table IV presents the principal outcomes of our proposed method in comparison to the baseline models across four distinct tasks. Our method, the SMES framework, distinguishes itself by its versatility, demonstrating aptitude across all tasks. This contrasts with other models that specialize in specific areas. The SMES framework exhibits a robust performance, not only in emotion recognition and strategy prediction but also in system emotion prediction and response generation. We analyze the results from four aspects: Emotion Recognition: The SMES achieves an accuracy of 54.6% in emotion recognition, outperforming the specialized DialogueGCN model and closely following the state-ofthe-art MMDFN. This slight deviation in performance can be attributed to the SMES's expansive capabilities, which, unlike models solely concentrated on emotion recognition, are designed to excel across a spectrum of tasks. Diverging from methods focused exclusively on identifying emotions, the SMES adopts a comprehensive framework, with the dual ability to understand and engage with users. It is engineered to understand and dynamically interact with users, thereby facilitating the generation of responses that are not only  contextually appropriate but also emotionally resonant. For the SMES, identifying emotions is the critical first step towards its overarching objective: capturing the user's emotional state to deliver tailored and efficient emotional support.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Strategy Prediction:",
      "text": "The SMES distinguishes itself with a 49.0% accuracy in strategy prediction, significantly outperforming GPT-3.5's 19.9% and slightly besting Blenderbot-Joint's 48%. This performance highlights the SMES's capability to handle strategic aspects of emotional support tasks.\n\nSystem Emotion Prediction: Previous methods often neglected the system's emotional state in response generation, a key factor in creating empathetic interactions. In contrast, the SMES framework effectively incorporates this aspect, achieving a remarkable 96.1% accuracy in predicting system emotions-significantly surpassing GPT-3.5. This precision enables the SMES to generate more empathetic responses, providing enhanced support and comfort to users dealing with distressing issues.\n\nResponse Generation: The SMES outperforms both BlenderBot-based methods and large language models methods across all generation metrics. These results not only affirm the effectiveness of the SMES algorithm in delivering emotional support but also underscore the necessity of the entire framework for multitasking. By leveraging the inherent dependencies between tasks, SMES optimizes their interactions, thus producing responses that consider the user's emotional state, strategic needs, and system-predicted emotions. This approach adeptly models the interdependencies among these elements, enhancing the therapeutic dialogue process in a comprehensive, end-to-end manner.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Human Evaluation",
      "text": "Following previous studies  [5] ,  [16] , we conduct a human evaluation to compare the generated responses of two models across five dimensions: (1) Fluency: Which bot's responses are more fluent and easy to understand? (2) Identification: Which bot more accurately explores your experiences and provides responses relevant to your problems? (3) Comfort: Which bot's responses are more comforting? (4) Suggestions: Which bot offers more helpful and empathetic suggestions for your problems? (5) Overall: Which bot provides better emotional support for dealing with life's distressing problems?\n\nWe randomly select 100 dialogues from the MESC dataset and involve three human participants to determine the Win/Tie/Lose outcome for each dialogue comparison. We compare the responses generated by MESC with those from two other baselines, BlenderBot-Joint and BlenderBot (without fine-tuning on MESC), as detailed in Table  V . The results show that: (a) SMES demonstrates significant improvement in the identification metric, achieving a success rate of 74% against BlenderBot and 57% against BlenderBot-Joint. This improvement is likely attributed to the emotional cues from multimodal data, which enhances the model's ability to accurately recognize the user's state. (b) SMES performs better in the suggestion metric, outperforming both models by over 50%. This may suggest that generating responses based on emotional recognition and strategic prediction effectively identifies user confusion, leading to more targeted advice. (c) SMES achieved a 70% success rate in overall metrics, demonstrating its ability to offer better emotional support. Furthermore, it may be beneficial to optimize the inherent interdependencies among the four tasks, which is essential for helping users accurately identify their issues and receive effective suggestions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Ablation Study",
      "text": "To evaluate the impact of each modality and sub-task on final performance, we conducted the ablation study, the results of which are presented in Tables VI and VII. Effect of multimodality information: We first explore the impact of each modality's data on four tasks, as detailed in Table  VI . Since we use video-llama for multimodality processing, where video and audio are bound together, removing video Case Id: Laura, Problem: Childhood Shadow, Situation: As a child, my father was unaware of the incident that David harmed me. My therapist advised that I should discuss this matter with him.\n\nClient: He didn't hurt me. It didn't hurt at all. Video Emotion Cues: The speaker seems to be in a state of shock or disbelief. The speaker's facial expression suggests that she is trying to process the information she is receiving.   also means removing audio. We can observe that removing the text modality significantly impacts the four tasks, with emotional recognition dropping by 6.1% and system emotion prediction by 25%. Similarly, the video and audio modalities are crucial; their removal results in a 4.8% decrease in strategy prediction and a 0.19 increase in the perplexity metric of generated responses. This demonstrates the importance of multimodal information in generating the four tasks and enhancing the effectiveness of emotional support. Effect of multi-task: We conduct the impact of emotion and strategy tasks on response generation within an LLM-based reasoning framework. As detailed in Table VII, removing the emotion task results in a 0.52 decrease in B-2 metric and a 1.27 reduction in R-L; Moreover, eliminating the strategy task has a greater impact on the response generation, leading to a 1.07 decrease in B-2 and a 1.30 reduction in R-L. This analysis reveals that a multi-task framework effectively harnesses the interconnected of these tasks to optimize response generation, making the responses more empathetic and supportive.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "E. Case Study",
      "text": "Fig.  6  presents a selection of dialogues from the MESC dataset along with responses generated by SMES, based on multimodal inputs. Initially, emotionally relevant clues are extracted from the video and audio inputs, as indicated by the green text in Fig.  6 . These modalities primarily focus on analyzing the speaker's emotional state and facial expressions. SMES then utilizes these clues, along with the user query, to perform four key tasks: recognizing user emotions, predicting strategies, forecasting system emotions, and generating responses. By integrating these four emotional tasks, the responses generated by SMES not only empathize with the user's feelings but also effectively alleviate the user's concerns, providing efficient emotional support.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this work, we introduce the comprehensive multimodal MESC dataset for mental health care, along with the Sequential Multimodal Emotional Support Framework (SMES)-a general approach designed to enhance AI-driven conversation systems in mental health care. Developed using the MESC dataset and informed by Therapeutic Skills Theory, the SMES Framework skillfully extracts and integrates emotional cues from text, audio, and video modalities. By employing a sequential multi-task strategy that spans user emotion recognition, system strategy prediction, system emotion prediction, and response generation, this framework effectively captures the complex interplay of these elements to optimize therapeutic dialogues. Our MESC dataset and SMES Framework address two critical gaps: the lack of a comprehensive multimodal dataset for emotional support conversations and the absence of a cohesive framework for integrating multimodal data in conversational systems. Our extensive evaluation shows that the SMES significantly boosts the empathetic and strategic capabilities of AI, setting a new benchmark for conversational AI in mental health support.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example chat from the MESC dataset, where the client’s and",
      "page": 1
    },
    {
      "caption": "Figure 2: More annotation details",
      "page": 3
    },
    {
      "caption": "Figure 2: It comprises 15 scenarios from",
      "page": 5
    },
    {
      "caption": "Figure 2: The proportion of scenarios of MESC.",
      "page": 5
    },
    {
      "caption": "Figure 3: The distribution of strategies at different conversation progress.",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the distribution of ten strategies across the",
      "page": 5
    },
    {
      "caption": "Figure 4: The SMES framework uses multimodality information as inputs to improve mental health support. It employs Video-Llama to extract emotional cues",
      "page": 6
    },
    {
      "caption": "Figure 3: , the strategy of “Open Questions” is",
      "page": 6
    },
    {
      "caption": "Figure 5: The LLM-based reasoning modal consolidates all emotion-related",
      "page": 6
    },
    {
      "caption": "Figure 4: , during each turn t of the dialogue",
      "page": 6
    },
    {
      "caption": "Figure 5: shows the training of the LLM-based Reasoning",
      "page": 7
    },
    {
      "caption": "Figure 6: Case Study. Green text: The emotional cues are extracted from the video and audio. Red text: The client’s emotion is generated by the SMES",
      "page": 9
    },
    {
      "caption": "Figure 6: presents a selection of dialogues from the MESC",
      "page": 9
    },
    {
      "caption": "Figure 6: These modalities primarily focus on",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Case Id: Laura,\nProblem: Childhood Shadow,\nSituation:  As a child, my father was unaware of the incident that David harmed me. My therapist advised that I should \ndiscuss this matter with him.": ""
        },
        {
          "Case Id: Laura,\nProblem: Childhood Shadow,\nSituation:  As a child, my father was unaware of the incident that David harmed me. My therapist advised that I should \ndiscuss this matter with him.": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The use and promise of conversational agents in digital health",
      "authors": [
        "T Dingler",
        "D Kwasnicka",
        "J Wei",
        "E Gong",
        "B Oldenburg"
      ],
      "year": "2021",
      "venue": "Yearbook of Medical Informatics"
    },
    {
      "citation_id": "2",
      "title": "Evaluating conversational agents for mental health: Scoping review of outcomes and outcome measurement instruments",
      "authors": [
        "A Jabir",
        "L Martinengo",
        "X Lin",
        "J Torous",
        "M Subramaniam",
        "L Car"
      ],
      "year": "2023",
      "venue": "Journal of Medical Internet Research"
    },
    {
      "citation_id": "3",
      "title": "Systematic review and meta-analysis of ai-based conversational agents for promoting mental health and well-being",
      "authors": [
        "H Li",
        "R Zhang",
        "Y.-C Lee",
        "R Kraut",
        "D Mohr"
      ],
      "year": "2023",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "4",
      "title": "The growing field of digital psychiatry: current evidence and the future of apps, social media, chatbots, and virtual reality",
      "authors": [
        "J Torous",
        "S Bucci",
        "I Bell",
        "L Kessing",
        "M Faurholt-Jepsen",
        "P Whelan",
        "A Carvalho",
        "M Keshavan",
        "J Linardon",
        "J Firth"
      ],
      "year": "2021",
      "venue": "World Psychiatry"
    },
    {
      "citation_id": "5",
      "title": "Towards emotional support dialog systems",
      "authors": [
        "S Liu",
        "C Zheng",
        "O Demasi",
        "S Sabour",
        "Y Li",
        "Z Yu",
        "Y Jiang",
        "M Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "6",
      "title": "empathetic open-domain conversation models: A new benchmark and dataset",
      "authors": [
        "H Rashkin",
        "E Smith",
        "M Li",
        "Y.-L Boureau"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "C Hsu",
        "S Chen",
        "C Kuo",
        "T Huang",
        "L Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018"
    },
    {
      "citation_id": "9",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "10",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "11",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "12",
      "title": "Graphcfc: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "I-gcn: Incremental graph convolution network for conversation emotion detection",
      "authors": [
        "W Nie",
        "R Chang",
        "M Ren",
        "Y Su",
        "A Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "A transformerbased model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "MISC: A mixed strategy-aware model integrating COMET for emotional support conversation",
      "authors": [
        "Q Tu",
        "Y Li",
        "J Cui",
        "B Wang",
        "J Wen",
        "R Yan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Control globally, understand locally: A global-to-local hierarchical graph network for emotional support conversation",
      "authors": [
        "W Peng",
        "Y Hu",
        "L Xing",
        "Y Xie",
        "Y Sun",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "PAL: personaaugmented emotional support conversation generation",
      "authors": [
        "J Cheng",
        "S Sabour",
        "H Sun",
        "Z Chen",
        "M Huang"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "18",
      "title": "Multimodal biomedical ai",
      "authors": [
        "J Acosta",
        "G Falcone",
        "P Rajpurkar",
        "E Topol"
      ],
      "year": "2022",
      "venue": "Nature Medicine"
    },
    {
      "citation_id": "19",
      "title": "Therapist skills associated with client emotional expression in psychodynamic psychotherapy",
      "authors": [
        "M Anvari",
        "C Hill",
        "D Kivlighan"
      ],
      "year": "2020",
      "venue": "Psychotherapy Research"
    },
    {
      "citation_id": "20",
      "title": "Helpful and nonhelpful events in brief counseling interviews: An empirical taxonomy",
      "authors": [
        "R Elliott"
      ],
      "year": "1985",
      "venue": "Journal of Counseling Psychology"
    },
    {
      "citation_id": "21",
      "title": "Effects of therapist response modes in brief psychotherapy",
      "authors": [
        "C Hill",
        "J Helms",
        "V Tichenor",
        "S Spiegel",
        "K O'grady",
        "E Perry"
      ],
      "year": "2001",
      "venue": "Effects of therapist response modes in brief psychotherapy"
    },
    {
      "citation_id": "22",
      "title": "Psychoanalysis and psychoanalytic psychotherapy: Is there a meaningful distinction in the process?",
      "authors": [
        "J Fosshage"
      ],
      "year": "1997",
      "venue": "Psychoanalytic Psychology"
    },
    {
      "citation_id": "23",
      "title": "A computational approach to understanding empathy expressed in text-based mental health support",
      "authors": [
        "A Sharma",
        "A Miner",
        "D Atkins",
        "T Althoff"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "It takes two to empathize: One to seek and one to provide",
      "authors": [
        "M Hosseini",
        "C Caragea"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "25",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "26",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "27",
      "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Supervised adversarial contrastive learning for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "Y Bao",
        "L Wei",
        "W Zhou",
        "S Hu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "29",
      "title": "Structure aware multi-graph network for multi-modal emotion recognition in conversations",
      "authors": [
        "D Zhang",
        "F Chen",
        "J Chang",
        "X Chen",
        "Q Tian"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Challenges in building intelligent opendomain dialog systems",
      "authors": [
        "M Huang",
        "X Zhu",
        "J Gao"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Information Systems (TOIS)"
    },
    {
      "citation_id": "32",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "H Zhou",
        "M Huang",
        "T Zhang",
        "X Zhu",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Exploiting emotion-semantic correlations for empathetic response generation",
      "authors": [
        "Z Yang",
        "Z Ren",
        "Y Wang",
        "X Zhu",
        "Z Chen",
        "T Cai",
        "Y Wu",
        "Y Su",
        "S Ju",
        "X Liao"
      ],
      "year": "2024",
      "venue": "Exploiting emotion-semantic correlations for empathetic response generation",
      "arxiv": "arXiv:2402.17437"
    },
    {
      "citation_id": "34",
      "title": "Improving empathetic response generation by recognizing emotion cause in conversations",
      "authors": [
        "J Gao",
        "Y Liu",
        "H Deng",
        "W Wang",
        "Y Cao",
        "J Du",
        "R Xu"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "35",
      "title": "Cem: Commonsense-aware empathetic response generation",
      "authors": [
        "S Sabour",
        "C Zheng",
        "M Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Knowledge-enhanced mixed-initiative dialogue system for emotional support conversations",
      "authors": [
        "Y Deng",
        "W Zhang",
        "Y Yuan",
        "W Lam"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Online emotional support delivered by trained volunteers: users' satisfaction and their perception of the service compared to psychotherapy",
      "authors": [
        "A Baumel"
      ],
      "year": "2015",
      "venue": "Journal of mental health"
    },
    {
      "citation_id": "38",
      "title": "Video-LLaMA: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "H Zhang",
        "X Li",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "39",
      "title": "Recipes for building an open-domain chatbot",
      "authors": [
        "S Roller",
        "E Dinan",
        "N Goyal",
        "D Ju",
        "M Williamson",
        "Y Liu",
        "J Xu",
        "M Ott",
        "E Smith",
        "Y.-L Boureau",
        "J Weston"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter"
    },
    {
      "citation_id": "40",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019"
    },
    {
      "citation_id": "41",
      "title": "MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "42",
      "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "43",
      "title": "Ask an expert: Leveraging language models to improve strategic reasoning in goal-oriented dialogue models",
      "authors": [
        "Q Zhang",
        "J Naradowsky",
        "Y Miyao"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    }
  ]
}